#### **Podemos trabajar juntos para detener la superinteligencia, aunque no estemos de acuerdo sobre la mejora humana.** {#podemos-trabajar-juntos-para-detener-la-superinteligencia-aunque-no-estemos-de-acuerdo-sobre-la-mejora-humana.}

Si no estás de acuerdo con nosotros en cuanto a la idea de la mejora humana, aún podemos ponernos de acuerdo para detener el desarrollo de la IA de vanguardia.

Si no resolvemos esa parte, todos moriremos. Todos los que no quieran morir *hoy* tienen que cooperar para ese fin. Podemos esperar hasta que haya pasado la amenaza de muerte inmediata para discutir si la mejora de la inteligencia humana debería ser ilegal o subvencionada.

«Hacer a los humanos más inteligentes que Einstein» no es un plan para evitar morir en 2028 o 2032, o cuando se produzca el próximo avance fundamental en los algoritmos de IA.

No es un plan que pueda llevarse a cabo en paralelo al desarrollo de la IA. Incluso si alguien usara tecnología médica —posibilitada por IAs menos inteligentes que los humanos— para aumentar la inteligencia humana muy por encima del nivel de Einstein, es probable que esas mejoras no bastaran para resolver el problema de la alineación de la IA y para diseñar, crear y construir de forma segura y *rápida* una superinteligencia de máquina, bajo la presión de una carrera armamentista. La carrera hacia la superinteligencia debe detenerse igualmente.

La idea detrás del aumento de la inteligencia humana es que podría hacer *siquiera posible* resolver el problema de la alineación, si un gran número de investigadores mejorados también tuvieran un número significativo de años o décadas para trabajar en el problema. La idea no es que pudieran ganar una *carrera* para construir una superinteligencia alineada dentro de seis años, más rápido de lo que el resto de la industria de la IA puede construir y desplegar una superinteligencia no alineada.

Muchas personas que piensan que «aumentar la inteligencia humana» es un plan relativamente prometedor, incluidos nosotros mismos, creen que los primeros pasos siguen pasando por cerrar las empresas de IA.

Quienes tienen otros planes sobre lo que la humanidad debería hacer a continuación, por lo general también coinciden en que el primer paso debería ser el cierre de las empresas de IA.

Meta AI no puede existir, OpenAI no puede existir, Anthropic no puede existir; simplemente nos matarán. Podemos estar de acuerdo en esta prioridad inmediata, incluso si tenemos ideas muy diferentes sobre qué hacer a continuación.

### «¿Alineados con quién?» {#«alineados-con-quién?»}

#### **Esta es una cuestión espinosa. Independientemente de la respuesta, debemos detener el desarrollo.** {#esta-es-una-pregunta-espinosa.-independientemente-de-la-respuesta,-debemos-detener-el-desarrollo.}

Si algún día la humanidad construye una superinteligencia, debemos asegurarnos de que esté «alineada» con los valores humanos. Pero, ¿con los valores de qué humanos exactamente? Las personas discrepan enormemente sobre lo que está bien y lo que está mal, sobre la religión, sobre las normas sociales, sobre las compensaciones políticas, etc.

En la actualidad, esta cuestión es irrelevante. La humanidad no es capaz de introducir *ningún* objetivo concreto en una IA, por lo que no importa si hay desacuerdo sobre *qué* objetivos serían los ideales. Como hemos argumentado ampliamente, apresurarnos a crear una superinteligencia nos mataría a todos. La humanidad tiene desacuerdos sobre muchas cosas, pero la mayoría de la gente no discrepa sobre si la destrucción de toda la vida en la Tierra es algo bueno.

El problema de qué valores *exactamente* deben cargarse en una IA parece una cuestión espinosa. Es un problema que, francamente, nos encantaría tener. En cambio, nos enfrentamos a un problema diferente y mucho peor.

No necesitamos ponernos de acuerdo en absoluto sobre «¿alineados con quién?» (ni siquiera sobre si la humanidad debería crear alguna vez una superinteligencia) para coordinarnos en una prohibición internacional, por la sencilla razón de que, de lo contrario, vamos a morir. Hay un sinfín de interesantes cuestiones filosóficas que plantea la IA, pero si nos dejamos distraer indebidamente por ellas, corremos el riesgo de que nuestros hijos mueran en el proceso.

En términos prácticos, nuestro consejo a los líderes mundiales es:

* Separar la pregunta «¿Debemos apresurarnos a crear la superinteligencia?» de la pregunta «Si de alguna manera tuviéramos una forma de crear la superinteligencia de forma segura, ¿qué deberíamos hacer con ella?» y centrarse primero en la primera pregunta. La primera pregunta es la más urgente y la que se puede abordar hoy en día. Puede que sea importante abordar la segunda pregunta algún día, pero en la actualidad es una trampa, porque anima a pensar en la superinteligencia como un premio. Creer erróneamente que la primera persona que construya una superinteligencia podrá decidir qué hacer con ella nos llevaría a una carrera suicida.

  La ASI es un botón suicida, no un genio en una lámpara. Cuando alguien crea una superinteligencia, no por ello «posee» esa superinteligencia. Más bien, la superinteligencia que acaba de crear tiene un planeta.  
* Si por alguna razón sientes la necesidad en el futuro de abordar el tema de «¿Cómo debería la humanidad utilizar algún día la superinteligencia, si alguna vez estamos en condiciones de hacerlo?», te recomendamos encarecidamente que evites propuestas o ideas que animen a otros actores a emprender una carrera (o que, de otro modo, animen a las naciones a rechazar o violar cualquier futuro [acuerdo internacional](https://docs.google.com/document/d/1xPGtIuZRuzVmNJVgqjrUHRaPXHCRDUDhYO8zm_WmjD8/edit?tab=t.k1kf1fy9gx5i#heading=h.tdczv21ifj74) sobre superinteligencia). Cualquier dinámica del tipo «el ganador se lo lleva todo» tiene un enorme potencial para poner en peligro al mundo.

  Existen propuestas para gestionar la espinosa cuestión de «a quién alinearse» de una manera relativamente universalista que intenta ser justa con todas las partes interesadas potenciales y que no incentiva una carrera por llegar primero a la meta; por ejemplo, la propuesta de alinear una IA para que persiga la [volición extrapolada coherente](https://www.lesswrong.com/w/coherent-extrapolated-volition-alignment-target) de toda la humanidad.[^229] Pero incluso en ese caso, existe un potencial infinito para que la gente discuta sobre los principios y los intercambios que ello implica, así como sobre los espinosos detalles de su implementación. Sería importante resolver esos argumentos en un mundo en el que la humanidad *hubiera* descubierto cómo orientar de forma precisa y sólida una superinteligencia, pero ponerlos hoy en primer plano tergiversa enormemente los intercambios reales a los que se enfrenta el mundo y corre el riesgo de descarrilar los esfuerzos para coordinar objetivos comunes, como evitar la destrucción de la Tierra.

Aun tratándose de cuestiones de enorme importancia a largo plazo, [nada debería vincularse a la supervivencia de la humanidad, salvo la propia supervivencia de la humanidad](https://docs.google.com/document/d/1xPGtIuZRuzVmNJVgqjrUHRaPXHCRDUDhYO8zm_WmjD8/edit?tab=t.k1kf1fy9gx5i#heading=h.gek8swcc3pef).

### ¿No es más sensato evitar hablar de la extinción? {#isn’t-it-smarter-to-avoid-talking-about-extinction?}

#### **Ya ha pasado el tiempo de los juegos políticos.** {#the-time-has-passed-for-playing-political-games.}

Algunos han argumentado que las personas preocupadas por la carrera por construir superinteligencia deberían ocultar sus opiniones y, en su lugar, hablar sobre la pérdida de puestos de trabajo causada por la IA, o el problema del bioterrorismo facilitado por ChatGPT, o cuánta agua se necesita para enfriar las computadoras dentro de los centros de datos.[^230] Creemos que este enfoque es pasarse de listo y es probable que resulte contraproducente. De hecho, ya hemos visto cómo ha resultado contraproducente en varias ocasiones.

Los cuatro problemas principales que vemos en este enfoque son:

* **No es honesto**, y la gente es buena detectando la falta de honestidad y la manipulación.

  Incluso si eres un mentiroso excepcionalmente bueno, los argumentos sobre cuestiones que consideras secundarias probablemente acabarán pareciendo «extraños» en varios sentidos. No acabarán de tener sentido, por las mismas razones por las que tú consideras que esas cuestiones son realmente secundarias. Cuanto más compartas tus argumentos edulcorados, más probable será que la gente concluya que *o bien* estás confundido sobre esta cuestión *o bien* no estás siendo totalmente sincero sobre lo que realmente piensas. Y, en cualquiera de los dos casos, no parecerás un aliado prometedor ni una fuente de información fiable.  
* **Probablemente sea innecesario.** Según nuestra experiencia, una conversación honesta y directa sobre la superinteligencia tiene una acogida mucho mejor que intentar desviar la atención hacia otros temas, como los *deepfakes* de IA. Desde mediados de 2023 y con cada vez más frecuencia, yo (Soares) he hablado con diversos cargos electos. He asistido a cenas en las que personas «preocupadas por la IA» plantearon la posibilidad de que los terroristas se valieran de la IA, y un cargo electo en ejercicio respondió que sus temores eran mucho más urgentes y graves, porque le preocupaban las IA con automejora recursiva que podrían generar una superinteligencia capaz de borrarnos del mapa, y que podrían crearse en un plazo de tres años.

  Hay gente, incluidos cargos electos del Congreso de los Estados Unidos, que está dispuesta a tomarse este tema en serio y a buscar formas de abordarlo.[^231] Este tema puede parecer más de nicho y controvertido de lo que realmente es, porque en el momento de la publicación de este libro aún no se ha producido un debate nacional o internacional adecuado al respecto. Pero hemos mantenido muchas conversaciones francas sobre este tema en Washington D. C. que han ido alentadoramente bien.[^232]
* **Responder a esas otras cuestiones no resuelve el problema de la superinteligencia.** Las empresas de IA compiten por crear superinteligencia. Si lo consiguen, todos moriremos. Las soluciones que tienen sentido para este problema son muy diferentes de las soluciones que tienen sentido para hacer frente a los deepfakes generados por la IA, o incluso al bioterrorismo con IA.

  El solapamiento no es *nulo*, y podríamos conseguir más apoyo para abordar la IA más inteligente que los humanos haciendo hincapié en las formas en que se solapan los diferentes problemas. Pero es extremadamente improbable que el mundo dé con una respuesta adecuada a un problema tan complicado como la superinteligencia *sin orientarse hacia el problema real*.  
* **Es plausible que tengamos poco tiempo.** Es poco probable que tengamos tiempo para hacer que la gente considere poco a poco este riesgo a lo largo de muchos años, empezando por cuestiones más sencillas y familiares para luego ir escalando hasta la superinteligencia. Si no movilizamos un esfuerzo para responder rápidamente a este problema, es plausible que no tengamos ninguna oportunidad de responder.

Esto no quiere decir que la pérdida de puestos de trabajo, el bioterrorismo, etc., no sean problemas reales por derecho propio. Es solo que la sociedad no va a poner fin *realmente* a la imprudente carrera suicida si no *sabe* que se está produciendo una imprudente carrera suicida.

Llevamos años viendo a amigos y conocidos del ámbito de las políticas públicas darle vueltas a problemas como el de los bioterroristas que se valen de ChatGPT. Por lo que sabemos, no parece que nada de esto vaya a impedir realmente la creación de la superinteligencia.

Somos nerds hasta la médula, y escribir un libro popular nos saca de nuestra zona de confort. No pretendemos tener experticia en política eficaz. Pero nos parece que la humanidad ha llegado al límite de los problemas que puede sortear con un discurso compuesto por argumentos cuidadosamente expresados, elegidos estratégicamente y no «alarmistas».

En algún momento, como seres humanos, tenemos que empezar a hablar de la amenaza que se avecina. La política debe basarse en las realidades concretas de la situación, no en mensajes de aparente seguridad.

Los directores de los laboratorios de IA afirman que podríamos ver investigadores de IA que superen a los humanos en los próximos [uno](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year) [a](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s) [cuatro](https://ia.samaltman.com/) años. Esperamos sinceramente que se equivoquen, pero, con toda nuestra experticia, no *sabemos* que lo estén. Los responsables de políticas tampoco lo saben. La humanidad simplemente no está respondiendo adecuadamente al desafío que tenemos por delante. Si no se da la voz de alarma ahora, ¿cuándo?

Además: desde que redactamos por primera vez el párrafo anterior, la estrategia que defendemos parece estar dando cada vez más frutos, como se puede ver en la [lista](#will-elected-officials-recognize-this-as-a-real-threat?) de lo que los políticos han dicho sobre la superinteligencia durante el verano de 2025. Parece que ha llegado el momento de debatir seriamente el peligro inminente que supone la superinteligencia artificial.

### ¿Reconocerán los cargos electos esto como una amenaza real? {#will-elected-officials-recognize-this-as-a-real-threat?}

#### **Son cada vez más los que ya lo han hecho.** {#son-cada-vez-más-los-que-ya-lo-han-hecho.}

Creemos que el principal obstáculo para que la gente reconozca la amenaza es lograr que la comprendan. En los pocos meses transcurridos desde que el libro se envió a la imprenta, nos parece que el mundo ya está avanzando en esa dirección.

A continuación se presentan algunas declaraciones de políticos estadounidenses de ambos lados del espectro político, del verano de 2025:

> «La superinteligencia artificial es una de las mayores amenazas existenciales a las que nos enfrentamos en la actualidad. [...] ¿Deberíamos preocuparnos también por la posibilidad de que Estados autoritarios como China o Rusia pierdan el control sobre sus propios sistemas avanzados? [...] ¿Es posible que la pérdida de control por parte de cualquier Estado nación, incluido el nuestro, dé lugar a un actor independiente con IAG o superinteligencia artificial al que tendremos que enfrentarnos a nivel mundial?». \- [Jill Tokuda (D-HI)](https://peterwildeford.substack.com/p/congress-has-started-taking-agi-more), en una [audiencia celebrada el 25 de junio de 2025](https://www.congress.gov/event/119th-congress/house-event/118428)
>
> «No voy a votar a favor del desarrollo de Skynet y el auge de las máquinas a costa de destruir el federalismo durante 10 años, quitándoles a los estados el derecho a regular y legislar sobre toda la IA». \- [Marjorie Taylor-Green (R-GA)](https://x.com/RepMTG/status/1930650431253827806)
>
> Hay personas muy, muy expertas —y hoy mismo he hablado con una de ellas— a las que les preocupa mucho que los seres humanos no seamos capaces de controlar la tecnología y que, de hecho, la inteligencia artificial domine nuestra sociedad. No podremos controlarla. Puede que llegue a controlarnos. Ese es el escenario apocalíptico, y existe cierta preocupación al respecto entre la gente muy experta del sector. \- [Bernie Sanders (I-VT)](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611?utm_source=substack&amp;utm_medium=email)
>
> «En nuestro afán por ganar la carrera de la IA contra China, corremos el riesgo de perdernos a nosotros mismos...». \- [Chris Murphy (D-CT)](https://www.chrismurphyct.com/p/in-our-scramble-to-win-the-ai-race)
>
> «Esto aumenta la probabilidad de que, en breve, el principal reto de liderazgo para los líderes mundiales, incluido el presidente de los Estados Unidos, sea gestionar los cambios que está provocando la IA y utilizar la visibilidad de su cargo y las herramientas políticas para garantizar que esta tecnología mejore la vida de las personas y no la empeore». \- [Pete Buttigieg](https://petebuttigieg.substack.com/p/we-are-still-underreacting-on-ai), exsecretario de Transporte

Aún queda mucho por hacer, pero el mundo está empezando a tomar conciencia. Ha llegado el momento de alertar a las autoridades sobre la necesidad de actuar con rapidez a nivel federal e internacional.

### ¿Es la situación desesperada? {#is-the-situation-hopeless?}

#### **N.º** {#no.-4}

Esta es una lucha que podemos ganar, y nuestro mundo es un mundo por el que vale la pena luchar. No parece fácil, pero es realmente factible.

Si deseas unirte a los esfuerzos para llamar al mundo a la acción, para nosotros también será un honor luchar a tu lado. Consulta el último capítulo del libro para conocer algunas formas de ayudar.

## Discusión detallada {#extended-discussion-11}

### ¿Qué se necesitaría para detener el desarrollo global de la IA? {#what-would-it-take-to-shut-down-global-ai-development?}

No somos expertos en derecho internacional, y este es un tema tremendamente complicado que prevemos que requerirá un gran esfuerzo por parte de expertos en la materia. Sin embargo, con el fin de poner en marcha el proceso rápidamente, hemos trabajado con nuestro [equipo de gobernanza técnica](https://techgov.intelligence.org/) y con asesores externos para elaborar algunos esbozos y conjeturas sobre medidas que podrían ser eficaces.

Las ofrecemos con el ánimo de fomentar la conversación, el debate, la crítica y la iteración. Estas ideas preliminares no deben considerarse en modo alguno definitivas ni autoritativas.

Como primer paso, analicemos las restricciones y la naturaleza del problema que intentamos resolver, un tema que por sí solo podría ocupar un libro entero. Durante décadas, el problema general ha consistido en impedir el desarrollo de la superinteligencia artificial. Y como no sabemos dónde están los umbrales críticos, eso equivale esencialmente a detener por completo la investigación y el desarrollo de la IA.

Los avances actuales en IA se deben a una combinación de factores: la creación de mejores chips informáticos, el uso de más chips durante más tiempo para entrenamientos más largos y la mejora de los algoritmos de IA. Abordaremos cada uno de estos factores por separado y explicaremos las palancas correspondientes para detener el avance hacia la superinteligencia artificial.

#### **Evitar el desarrollo de más y mejores chips de IA** {#preventing-the-creation-of-more-and-better-ai-chips}

Aumentar las capacidades de las IA modernas requiere una enorme inversión en poder de cómputo y energía eléctrica. Como resultado, parece posible que los actores estatales modernos identifiquen y supervisen todas las instalaciones relevantes y eviten la aparición de nuevas instalaciones de este tipo, con un impacto mínimo en el hardware de consumo.

La [cadena de suministro](https://www.csis.org/analysis/mapping-semiconductor-supply-chain-critical-role-indo-pacific-region) para la producción de chips de IA avanzados está extremadamente concentrada. En algunos de sus eslabones, solo hay una empresa en el mundo capaz de desempeñar esa función, y estas empresas se encuentran en su mayoría en países tradicionalmente aliados con Estados Unidos.

Por ejemplo, solo unas pocas empresas pueden fabricar chips de IA —principalmente la empresa taiwanesa TSMC—, y una de las máquinas clave utilizadas en los chips de alta gama solo la produce la empresa holandesa ASML. Se trata de la máquina de litografía ultravioleta extrema, que tiene el tamaño de un autobús escolar, pesa 200 toneladas y cuesta [cientos de millones de dólares](https://www.datacenterdynamics.com/en/news/tsmc-to-receive-first-high-na-euv-lithography-machine-from-asml-in-q4/).

Esta cadena de suministro es el resultado de décadas de innovación e inversión, y se [prevé](https://cset.georgetown.edu/publication/maintaining-the-ai-chip-competitive-advantage-of-the-united-states-and-its-allies/) que replicarla sea bastante difícil: probablemente llevaría más de una década, incluso para los países tecnológicamente avanzados.

Los chips de IA más avanzados también son [bastante especializados](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/), por lo que su seguimiento y supervisión tendrían pocos efectos de desbordamiento. El chip H100 de NVIDIA, uno de los chips de IA más comunes a mediados de 2025, cuesta alrededor de 30 000 dólares por unidad y está diseñado para funcionar en un centro de datos debido a sus requisitos de refrigeración y alimentación. Estos chips están optimizados para realizar las operaciones numéricas que implican el entrenamiento y el funcionamiento de las IA, y suelen tener un rendimiento [decenas a miles de veces](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/) superior al de las computadoras estándar (CPU de consumo) en cargas de trabajo de IA. [^233]

La concentración y la complejidad de la cadena de suministro de chips de IA hacen que detener el desarrollo de la IA avanzada sea más fácil de lo que cabría esperar. **Sería sencillo detener la producción de nuevos chips de IA**. Bastaría con una supervisión mínima de un pequeño número de proveedores clave para garantizar que no se creen cadenas de suministro secretas, dada la complejidad y la interconexión del proceso de producción.

Parte de la misma infraestructura se utiliza para producir chips de IA y otros chips informáticos avanzados (como los chips de teléfonos móviles), pero existen diferencias notables entre ellos. Si se detuviera la producción de chips de IA avanzados, sería factible supervisar y garantizar que la producción de chips en curso se limitara a crear únicamente chips no especializados en IA.

Los chips de IA especializados preexistentes podrían supervisarse si se conservan y se utilizan para ejecutar IA existentes, como ChatGPT. Garantizar que dichos chips solo se utilicen para ejecutar IA de baja capacidad (en lugar de para nuevos proyectos de investigación y desarrollo) sería un reto, pero no insuperable. Se podría rastrear y supervisar la ubicación de los chips existentes, y existen varios [mecanismos potenciales que podrían utilizarse para verificar](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) para qué se están utilizando esos chips. Este tipo de supervisión requiere acceso físico a los chips (por ejemplo, inspectores que realicen mediciones en un centro de datos). El acceso remoto podría ser suficiente para la verificación si los nuevos chips se fabricaran con [una seguridad mejorada](https://www.cnas.org/publications/reports/secure-governable-chips) y se diseñaran teniendo en cuenta la verificación y la supervisión. Como se analiza en la siguiente sección, las concentraciones de chips necesarias para que sean peligrosos (al nivel de los algoritmos de IA de agosto de 2025) son tan grandes que no sería difícil para los actores estatales detectar todas esas instalaciones y someterlas a inspecciones periódicas.

#### **Prevención del uso de más y mejores chips de IA** {#preventing-the-usage-of-more-and-better-ai-chips}

Pasemos ahora de la producción de chips a su uso: los [mayores centros de datos de IA](https://epoch.ai/blog/trends-in-ai-supercomputers) actuales albergan cientos de miles de chips de IA, que cuestan miles de millones de dólares. Para entrenar a una de las IA más potentes de la actualidad, estos chips deben utilizarse durante meses seguidos.

Cada uno de estos chips tiene un [consumo energético similar](https://ifp.org/future-of-ai-compute/) al de un hogar estadounidense medio, por lo que un centro de datos con cientos de miles de chips tiene un consumo energético comparable al de una pequeña ciudad. Para alimentar todos estos chips se requiere una infraestructura eléctrica especializada, como grandes líneas de transmisión. Estos centros de datos también son edificios bastante grandes, con huellas térmicas distintivas generadas por el funcionamiento y la refrigeración continuos de un gran número de chips de alto consumo energético.

Internamente, estos centros de datos albergan sus miles de chips en racks de servidores y cuentan con una amplia infraestructura de refrigeración para garantizar que no se sobrecalienten. Si alguien [entrara](https://cloud.google.com/blog/products/gcp/google-data-center-360-tour) a uno de estos edificios, sería extremadamente claro que es un centro de datos. No es como si su propósito pudiera ocultarse a los monitores internacionales que vinieran a inspeccionar, especialmente si comprueban los chips del centro de datos y descubren que son chips especializados en IA.

**Los grandes centros de datos y su infraestructura energética relacionada son tan grandes que pueden ser identificados por satélites en órbita.** Esto significa que, si los gobiernos quisieran localizar los grandes centros de datos actuales, probablemente podrían hacerlo con una alta tasa de éxito, tanto si esos centros de datos se encuentran dentro de sus fronteras como en otros países. Aunque el estado del conocimiento público es [limitado](https://epoch.ai/blog/trends-in-ai-supercomputers), esta intervención por sí sola podría localizar la mayoría de los chips de IA de alta gama.

En el futuro, los Estados podrían intentar ocultar sus centros de datos para dificultar su identificación mediante satélites. Por ejemplo, podrían ocultar un centro de datos en una montaña (como en el complejo Cheyenne Mountain, que alberga el NORAD), donde no sería visible desde arriba. Aun así, sería difícil ocultar la infraestructura necesaria para su funcionamiento.

El principal factor que favorece la detección es que los centros de datos tienen un consumo eléctrico muy elevado. Esta energía suele suministrarse a través de líneas de transmisión, que casi siempre son aéreas. Es posible soterrar las líneas de transmisión, pero es mucho más costoso y requiere más tiempo, y las obras de soterramiento también son difíciles de ocultar.[^234]

Mientras se sigan necesitando más de 100 000 chips para entrenar una IA de vanguardia, parece bastante posible que los actores estatales detecten y monitoricen todos los centros de datos relevantes.

#### **Prevención del progreso algorítmico** {#preventing-algorithmic-progress}

Los algoritmos de IA más eficientes pueden reducir los recursos computacionales necesarios para entrenar una IA, o pueden permitir la producción de IA más capaces utilizando una cantidad determinada de recursos computacionales, [o ambas cosas](https://arxiv.org/abs/2311.15377).

El progreso algorítmico está impulsado principalmente por la investigación y la ingeniería, que en la actualidad dependen de la habilidad y el esfuerzo humanos.[^235] Las aptitudes necesarias para mejorar los algoritmos de IA son relativamente escasas, lo que explica los [elevados salarios](https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html) que cobran los mejores investigadores del sector.

Aunque estas habilidades son poco comunes hoy en día, no está claro cómo podría cambiar esto a medida que más investigadores se incorporen al campo y se haga público más conocimiento. Dependiendo de cómo se quiera contar el número de personas con las habilidades necesarias, la cifra real probablemente sea de cientos o unos pocos miles (por ejemplo, basándose en el número de investigadores e ingenieros de IA en las [principales empresas de IA](https://fortune.com/2025/03/15/ai-talent-wars-startups-google-meta-openai-hiring-scientists-stock-salaries/)). Las estimaciones conservadoras podrían ser mucho más altas: por ejemplo, hay [decenas de millones](https://www.griddynamics.com/blog/number-software-developers-world) de ingenieros de software en el mundo.

**Las intervenciones legales y sociales podrían ralentizar drásticamente el progreso algorítmico.** La mayoría de la gente no quiere infringir la ley, especialmente cuando hay consecuencias reales. Si fuera ilegal publicar ciertas investigaciones sobre IA o realizar diversos experimentos de IA basados en los riesgos catastróficos que plantea una IA suficientemente capaz, esto probablemente disuadiría a [casi todos los potenciales investigadores en IA](#why-a-research-ban?-that-seems-extreme.). Los gobiernos podrían implementar controles de exportación que harían ilegal compartir o publicar dichas investigaciones sin una licencia de exportación y la aprobación gubernamental.

Los tabúes sociales también ayudarían. Como precedente, podemos considerar la [Conferencia de Asilomar sobre el ADN recombinante](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA) de 1975, que tuvo como resultado una prohibición voluntaria de ciertos experimentos biológicos que se consideraban excesivamente riesgosos. En teoría, los científicos podrían instaurar una prohibición voluntaria del avance de las capacidades de la IA. Sin embargo, esto requeriría que estos científicos se tomaran en serio el peligro que entraña una IA más inteligente que los humanos, lo que supondría un alejamiento del *statu quo*, en el que el avance de las capacidades de la IA es alabado en muchos círculos. Dados los incentivos económicos miopes y el comportamiento observado hasta la fecha en los laboratorios, parece muy probable que sean necesarias restricciones legales externas, a menos que la cultura del campo cambie *drásticamente* (y en poco tiempo).[^236]

Un componente fundamental para que una prohibición imperfecta sea eficaz puede ser algo tan obvio como asegurarse de que los líderes mundiales comprendan realmente que [ellos y sus familias morirán personalmente](#¿no-rechazarían-algunas-naciones-una-prohibición?) si siguen adelante. Los escenarios de incumplimiento más probables son aquellos en los que los gobiernos ven la superinteligencia de cosecha propia como un activo estratégico (o como un espejismo que los distrae de nuevas y rentables herramientas de IA), en lugar de como un botón de suicidio global. Es mucho menos probable que los gobiernos lleven a cabo proyectos secretos de investigación sobre la superinteligencia si ven correctamente que esto equivale a cargar un arma, apuntarse a la cabeza y apretar el gatillo.

Las prohibiciones a la investigación no detendrían a todo el mundo. Algunos científicos y ejecutivos destacados del sector tecnológico ya han [dicho](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.ymh89tu1wrg5) que destruir a la humanidad es un precio aceptable que pagar por el progreso. Pero no debemos dejar que lo perfecto sea enemigo de lo bueno. Los avances algorítmicos, como mínimo, se *ralentizarían* si a esas personas se les retirara el financiamiento y fueran rechazadas por sus colegas, lo que las obligaría a realizar su letal investigación al margen de la ley y sin la colaboración de ninguno de sus colegas más íntegros.

#### **Cuanto más tardemos, más difícil será** {#cuanto-más-tardemos,-más-difícil-será}

Si la producción y distribución de chips de IA continúan en su trayectoria actual, el reto de garantizar que haya suficientes chips de IA centralizados y supervisados solo se volverá más difícil. Aunque los Estados aún no estén convencidos de los riesgos, empezar hoy a realizar un seguimiento internacional de los chips de IA significa que la intervención seguirá siendo posible en el futuro. Esa ventana podría cerrarse pronto si los gobiernos no actúan con rapidez.

Si se permite a los investigadores seguir mejorando los algoritmos de IA, es probable que una cantidad cada vez menor de chips de IA sea suficiente para constituir una grave amenaza. Cuando los sistemas de IA sean capaces de automatizar partes del proceso de I+D de la IA, podría resultar especialmente difícil controlar su desarrollo. Dichos sistemas podrían copiarse y distribuirse fácilmente, y el hardware necesario para ejecutarlos podría no ser considerable. (Los requisitos de hardware para *ejecutar* sistemas de IA son mucho menores que los necesarios para *entrenar* sistemas de IA).

Eventualmente, podría volverse imposible para los gobiernos del mundo detener el desarrollo de sistemas de IA superinteligentes. Aún no hemos llegado a ese punto, pero cada mes se hace más difícil. El plan que esbozamos se basa en la premisa de detener el desarrollo de la IA *pronto*. Hay otros planes que no se basan en este supuesto, pero son más difíciles de implementar, tienen costos más altos para las libertades personales y conllevan una mayor probabilidad de fracaso.

### Proyecto de tratado, con anotaciones {#proyecto-de-tratado-con-anotaciones}

Muchas personas, incluidos miembros de la Cámara de Representantes y del Senado de Estados Unidos, nos han preguntado qué propuestas legislativas concretas y específicas realmente ayudarían a solucionar este problema.

No somos expertos en política y vemos muchas respuestas posibles a esa pregunta, dependiendo de si se busca una legislación que sea más fácil de aprobar hoy en día, pero menos útil directamente (que funcione más como un peldaño hacia una legislación más ambiciosa), o propuestas que parecen más difíciles de aprobar, pero que abordarían de forma sustancial las cuestiones fundamentales.

Tenemos mucho más que decir sobre esto último que sobre lo primero. Con ese fin, el equipo técnico de gobernanza de MIRI ha elaborado un borrador preliminar de una propuesta de este tipo.

Este esbozo de tratado está diseñado para un mundo en el que los líderes mundiales han tomado conciencia de las realidades de una IA más inteligente que los humanos. No es el tipo de acuerdo que prevemos que se apruebe mañana (mientras escribimos esto), pero podría no ser tan difícil lograr la ratificación de un tratado así una vez que el mundo sea más consciente del peligro; un proceso que [ya ha comenzado](#will-elected-officials-recognize-this-as-a-real-threat?) y que esperamos que continúe.

[El borrador del tratado puede encontrarse aquí](#a-tentative-draft-of-a-treaty,-with-annotations-1), con numerosas anotaciones. El proceso de redacción se basó en gran medida en los precedentes históricos de otros tratados, que se citan junto al borrador. Reiteramos que se trata de un punto de partida y no de un punto final. No somos expertos en políticas públicas y es posible que hayamos cometido algunos errores. No obstante, esperamos que este borrador sirva de inspiración y ejemplo de cómo sería posible un tratado de este tipo, en consonancia con iniciativas legislativas similares del pasado.

### Mantener la coalición amplia {#keep-the-coalition-large}

Hemos oído a algunas personas argumentar que deberíamos adoptar una postura firme contra el arte generado por IA o las armas robóticas, con el fin de enviar un mensaje más sencillo: no estamos en contra de la superinteligencia, sino en contra de la *IA*.

Dejando de lado los méritos de las diversas posiciones sobre el arte generado por IA, los deepfakes, etc., no creemos que esta sea la mejor opción desde el punto de vista político. Queremos crear una coalición para prohibir la superinteligencia. Consideramos que esta cuestión es extraordinariamente urgente y grave, y queremos que esta coalición sea lo más amplia posible, incluyendo a personas con una gran variedad de opiniones sobre el arte generado por IA, la guerra con drones, los coches autónomos, el uso de la IA en las escuelas, etc.

Todos tenemos un interés común en impedir la creación de una superinteligencia descontrolada, independientemente de nuestra postura sobre otras cuestiones.

¿Debería la humanidad extinguirse y ser sustituida por algo sombrío? Todos los que estén de acuerdo en que la respuesta es «no» pueden cooperar en un esfuerzo urgente para detener la carrera desenfrenada por la superinteligencia.

No creemos que la coalición sobreviva si dices que no puedes trabajar con nadie que discrepe contigo sobre el arte de la IA o la guerra con drones.

La coalición tampoco sobrevivirá si se agrupan otras cuestiones con la superinteligencia, de modo que todos tengan que ponerse de acuerdo en una larga lista de cuestiones semirrelacionadas antes de poder trabajar juntos en el tema de la superinteligencia.

Si te preocupan otras cuestiones relacionadas con la IA, te instamos a que trabajes para abordarlas. Pero te pedimos que no las «agrupes» con la superinteligencia. Si queremos superar esto, nada debe agruparse con la supervivencia de la humanidad.

Una de las razones por las que vives en tu país, y no en un montón de escombros radiactivos tras la Tercera Guerra Mundial, es que Oriente y Occidente lograron ponerse de acuerdo hace décadas en que la guerra nuclear era una amenaza realista y grave para la humanidad. Oriente y Occidente afirmaron respectivamente que Occidente y Oriente eran una amenaza terrible *adicional* para la humanidad. Pero sabiamente trataron las dos amenazas —la aniquilación nuclear frente a la derrota ideológica— como de distinta naturaleza.

Desde la perspectiva de Occidente, era mejor que la humanidad estuviera *menos* amenazada por la guerra nuclear, aunque siguiera estando amenazada por Oriente, lo que significaba cooperar con Oriente el tiempo suficiente para establecer una línea directa entre Washington y Moscú y colaborar en tratados de no proliferación y otros acuerdos.

Son demasiados los países que necesitan coordinarse. Hay demasiadas facciones divididas (incluso internamente) como para que sea posible evitar la catástrofe si la acción conjunta se limita a quienes están de acuerdo en todo.

Con gusto y sin reservas, hacemos causa común con las personas preocupadas por [otros problemas](#el-mundo-es,-por-desgracia,-lo-suficientemente-grande-como-para-que-quepan-múltiples-problemas) del mundo. Trabajaremos sin vacilar con aquellas personas con las que discrepamos políticamente. Hemos lanzado este mensaje desesperado al mundo porque creemos en él, y consideramos que este problema debe abordarse *de inmediato* a nivel internacional.

Seas quien seas —luches por lo que luches, aquí o en cualquier otro lugar—, si quieres poner fin al desarrollo vertiginoso de una IA más inteligente que los humanos, estamos juntos en esta lucha.

## Fe de erratas {#errata}

* Fe de erratas (primera impresión, ediciones de EE. UU. y Reino Unido): en la página 137 se indica que @truth\_terminal se publicó en 2023, pero la fecha correcta es 2024.
* Errata (en la primera impresión, ediciones de EE. UU. y Reino Unido): en la página 213, dijimos que «\[t\]oda la revolución tecnológica que condujo a ChatGPT y otros LLM populares se inició con un artículo de 2018 que presentaba una nueva e ingeniosa disposición aritmética dentro de una GPU, el algoritmo «transformador», \[…\]». En realidad, se trataba de un artículo de 2017, titulado «[Attention Is All You Need](https://arxiv.org/abs/1706.03762)», que condujo a la creación del primer GPT (GPT-1) en 2018.

## Anteproyecto de un tratado, con anotaciones {#a-tentative-draft-of-a-treaty,-with-annotations-1}

### Descargo de responsabilidad {#disclaimer}

A continuación, ofrecemos un borrador de ejemplo comentado para el tipo de tratado que podrían implementar los principales gobiernos del mundo si reconocieran los peligros de la superinteligencia artificial (ASI) y buscaran impedir que cualquiera la construyera.[^237]

No somos responsables políticos ni expertos en derecho internacional. Presentamos esto como un ejemplo ilustrativo de algunas disposiciones convencionales que podría ser valioso tener en cuenta, a través de mecanismos adaptados a la situación específica y basados en precedentes históricos.

Este borrador abarca muchos mecanismos diferentes que creemos que serían necesarios para evitar que los desarrolladores de IA pongan en grave peligro a la humanidad. En la práctica, esperaríamos que los distintos aspectos probablemente quedaran cubiertos por tratados diferentes.[^238] Y, por supuesto, la comunidad internacional debería redactar cuidadosamente el tratado en su totalidad, sujeto a la negociación y revisión de expertos pertinentes.

Para cada artículo del siguiente tratado de ejemplo, hemos incluido una sección de comentarios que explica nuestras decisiones clave y otra que analiza precedentes relevantes.

Un tratado real incluiría muchos detalles. Hemos incluido algunos detalles a modo de ejemplo, pero la mayoría se han relegado a los «anexos» (que no desarrollamos en su totalidad). Muchas de las cantidades y umbrales numéricos que utilizamos en nuestro borrador constituyen nuestra mejor estimación, aunque no deben tomarse más que como simples estimaciones. Muchas de esas cifras requerirían un estudio y una revisión más profundos antes de ser definitivas. Plausiblemente, este tipo de detalles no se incluirían en el tratado propiamente dicho, de forma análoga a como, en el caso del Tratado de No Proliferación Nuclear (TNP), los detalles específicos de las inspecciones y los denominados programas de «salvaguardias» se decidieron entre cada país y el OIEA, en lugar de incluirse en el propio TNP. Sin embargo, para mayor claridad, hemos mantenido nuestras mejores estimaciones directamente en el borrador del texto del tratado, para ayudar a que resulte un poco más concreto.

### Preámbulo {#preamble}

Los Estados que celebran el presente Tratado, en lo sucesivo denominados «las Partes en el Tratado»,

Alarmados por la perspectiva de que el desarrollo de la superinteligencia artificial conduzca a la extinción humana y al fin de toda empresa humana,

Afirmando la necesidad de una acción internacional urgente, coordinada y sostenida para impedir la creación y el despliegue de la superinteligencia artificial bajo las condiciones actuales,

Convencidos de que las medidas para prevenir el avance de las capacidades de la inteligencia artificial reducirán la probabilidad de extinción humana,

reconociendo que la estabilidad de este Tratado se basa en la capacidad de verificar el cumplimiento de todas las Partes,

Recordando el precedente de los acuerdos previos de control de armamentos y no proliferación para hacer frente a las amenazas a la seguridad mundial,

Comprometiéndose a cooperar para facilitar la verificación de las actividades de inteligencia artificial a nivel mundial cuando se mantengan bien alejadas de la superinteligencia artificial, y buscando preservar el acceso a los beneficios de los sistemas de inteligencia artificial, al tiempo que se evitan los peligros,

Han acordado lo siguiente:

---

#### Antecedentes

El preámbulo de este tratado se basa en el del [TNP](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf),[^239] que comienza así:

> Considerando la devastación que una guerra nuclear causaría a toda la humanidad y la consiguiente necesidad de hacer todo lo posible para evitar el peligro de tal guerra y de adoptar medidas para salvaguardar la seguridad de los pueblos...

Y añade poco después:

> Afirmando el principio de que los beneficios de las aplicaciones pacíficas de la tecnología nuclear, incluidos los subproductos tecnológicos que puedan derivarse del desarrollo de dispositivos explosivos nucleares por parte de los Estados poseedores de armas nucleares, deben estar a disposición de todas las Partes en el Tratado para fines pacíficos, ya sean Estados poseedores de armas nucleares o Estados no poseedores de armas nucleares...

Al hacerlo, el preámbulo invita al mundo a unirse a las Partes responsables para salvaguardar a la humanidad de la amenaza catastrófica de una tecnología poderosa y a participar de los beneficios que puedan obtenerse de forma segura. Nuestro preámbulo intenta seguir este ejemplo.

---

### Artículo I: Propósito principal {#article-i:-primary-purpose}

Cada una de las Partes del presente Tratado no desarrollará, desplegará ni tratará de desarrollar o desplegar la superinteligencia artificial («ASI») por ningún medio. Cada una de las Partes prohibirá e impedirá todo desarrollo de este tipo dentro de sus fronteras y jurisdicciones y, debido a la incertidumbre sobre cuándo nuevos avances podrían dar lugar a la ASI, no participará ni permitirá actividades que supongan un avance significativo hacia la ASI, tal y como se describe en el presente Tratado. Cada una de las Partes prestará asistencia a las medidas razonables de otras Partes, o no las obstaculizará, para disuadir e impedir dicho desarrollo por parte de Estados y jurisdicciones que no sean Partes, y dentro de estos. Cada una de las Partes implementará y llevará a cabo todas las demás obligaciones, medidas y disposiciones de verificación establecidas en el presente Tratado.

Cuando pueda considerarse aceptable que ciertas clases de infraestructura y capacidades de IA permanezcan lejos de la superinteligencia artificial (ASI), pero solo bajo condiciones de supervisión internacional, únicamente las Partes del Tratado podrán llevar a cabo tales actividades, o poseer u operar chips de IA e instalaciones de fabricación que pudieran conducir al desarrollo de una ASI sin supervisión. Se negará dicho acceso a quienes no sean Parte, por la seguridad de las Partes y de toda la vida en la Tierra (Artículo V, Artículo VI, Artículo VII).

Las Partes se comprometen a un proceso de resolución de controversias (artículo XI) con el fin de minimizar las medidas de protección innecesarias (artículo XII).

---

#### Antecedentes

El artículo I del TNP, como en muchos tratados, establece el compromiso de alto nivel que asumirán las Partes, en este caso, no compartir sus armas nucleares ni ayudar a otros a obtenerlas:

> Cada Estado poseedor de armas nucleares que es Parte en el Tratado se compromete a no traspasar a nadie armas nucleares u otros dispositivos nucleares explosivos ni el control sobre tales armas o dispositivos, sea directa o indirectamente; y a no ayudar, alentar o inducir en forma alguna a ningún Estado no poseedor de armas nucleares a fabricar o adquirir armas nucleares u otros dispositivos nucleares explosivos, ni el control sobre tales armas o dispositivos.

El compromiso resumido en el artículo I de nuestro proyecto de acuerdo es más estricto, ya que no se puede permitir que ocurra ni una sola vez un escape de una ASI, por parte de nadie y en ningún lugar.[^240] No bastará con no «ayudar, alentar o inducir» a otros a construirla. Por lo tanto, hemos incluido el compromiso de «ayudar, o no impedir, las medidas razonables» de las Partes para disuadir y prevenir dicho desarrollo en cualquier lugar.

El TNP tiene por objeto contener una amenaza existente (las armas nucleares), mientras que nuestro proyecto de acuerdo pretende impedir que una amenaza (ASI) llegue a existir. Se puede encontrar un precedente para prevenir el desarrollo de nuevas tecnologías peligrosas en el [Protocolo sobre armas láser cegadoras](https://www.un.org/en/genocideprevention/documents/atrocity-crimes/Doc.43_CCW%20P-IV.pdf), que forma parte de la [Convención sobre ciertas armas convencionales](https://treaties.un.org/doc/Treaties/1983/12/19831202%2001-19%20AM/XXVI-2-revised.pdf#page=25).[^241] Su artículo I dice lo siguiente:

> Se prohíbe el empleo de armas láser específicamente diseñadas para que su única función de combate, o una de ellas, sea causar ceguera permanente a la visión sin ayuda, es decir, al ojo desnudo o al ojo con dispositivos correctores de la vista. Las Altas Partes Contratantes no transferirán tales armas a ningún Estado o entidad no estatal.

Sin embargo, esa formulación no intenta impedir que nadie, en ningún lugar, pruebe o desarrolle accidentalmente un sistema de ese tipo. Nuestro acuerdo debe ser lo suficientemente sólido como para impedir que se fabrique accidentalmente un ASI. Dado que no está claro dónde podría estar el punto de no retorno, nuestro artículo I incluye el compromiso de «no participar ni permitir actividades que avancen de manera significativa hacia la ASI».

---

### Artículo II: Definiciones {#article-ii:-definitions}

Para los fines del presente Tratado:

1. **Inteligencia artificial (IA)** hace referencia a un sistema computacional que realiza tareas que requieren cognición, planificación, aprendizaje o la toma de acciones en los ámbitos físico, social o cibernético. Esto incluye sistemas que realizan tareas en condiciones variables e impredecibles, o que pueden aprender de la experiencia y mejorar su rendimiento.  
2. **La superinteligencia artificial (SIA)** se define operativamente como cualquier IA con un rendimiento cognitivo suficientemente sobrehumano como para planificar y ejecutar con éxito la destrucción de la humanidad.  
   1. A los efectos del presente Tratado, se considerará que el desarrollo de IA que no esté explícitamente autorizado por la ISIA (artículo III) y que infrinja los límites descritos en el artículo IV tiene como objetivo la creación de superinteligencia artificial.  
3. **Las actividades peligrosas de IA** son aquellas que aumentan sustancialmente el riesgo de que se cree una superinteligencia artificial, y no se limitan a la etapa final del desarrollo de una ASI, sino que también incluyen las etapas previas establecidas en el presente tratado. El alcance completo de las actividades peligrosas de IA se detalla en los artículos IV a IX y puede ser ampliado y modificado mediante la aplicación del Tratado y las actividades de la ISIA.  
4. Las operaciones de coma flotante (FLOP) son la medida computacional utilizada para cuantificar la escala de entrenamiento y postentrenamiento, basada en el número de operaciones matemáticas realizadas. Las FLOP se contarán como operaciones equivalentes al formato de coma flotante de media precisión (FP16) o como operaciones totales (en el formato utilizado), lo que sea mayor.  
5. **Ejecución de entrenamiento** significa cualquier proceso computacional que optimiza los parámetros de una IA (especificaciones de la propagación de información a través de una red neuronal, por ejemplo, pesos y sesgos) utilizando métodos basados en gradientes u otros métodos de búsqueda/aprendizaje, incluyendo el preentrenamiento, el ajuste fino, el aprendizaje por refuerzo, las búsquedas de hiperparámetros a gran escala que actualizan los parámetros y el autoaprendizaje iterativo o el entrenamiento curricular.  
6. El **preentrenamiento** es el proceso de entrenamiento en el que se optimizan inicialmente los parámetros de una IA utilizando conjuntos de datos a gran escala para aprender patrones o representaciones generalizables antes de cualquier adaptación específica de una tarea o dominio. Incluye la optimización supervisada, no supervisada, autosupervisada y por refuerzo cuando se realiza antes de dicha adaptación.  
7. **Post-entrenamiento** se refiere a una ejecución de entrenamiento realizada después del preentrenamiento de un modelo. Además, cualquier entrenamiento realizado en una IA creada antes de la entrada en vigor del presente Tratado se considera post-entrenamiento.  
8. **Los chips informáticos avanzados** son circuitos integrados fabricados con procesos al menos tan avanzados como el nodo de proceso de 28 nanómetros.  
9. Los **chips de IA** son circuitos integrados especializados diseñados principalmente para cálculos de IA, incluyendo, entre otros, operaciones de entrenamiento e inferencia para modelos de aprendizaje automático [esto debería definirse con mayor precisión en un anexo]. Esto incluye GPU, TPU, NPU y otros aceleradores de IA. También puede incluirse el hardware que no fue diseñado originalmente para usos de IA, pero que puede reutilizarse de manera eficaz. Los chips de IA son un subconjunto de los chips informáticos avanzados.  
10. **Hardware de IA** se refiere a todo el hardware informático para entrenar y ejecutar inteligencias artificiales. Esto incluye chips de IA, así como equipos de red, fuentes de alimentación y equipos de refrigeración.  
11. **Equipos de fabricación de chips de IA**: equipos utilizados para fabricar, probar, ensamblar o empaquetar chips de IA, incluidos, entre otros, equipos de litografía, deposición, grabado, metrología, pruebas y empaquetado avanzado [sería necesario definir una lista más completa en un anexo].  
12. **Equivalente a H100** significa la unidad de capacidad de cómputo (FLOP por segundo) equivalente a un acelerador NVIDIA H100 SXM, 990 TFLOP/s en FP16, o un rendimiento de procesamiento total (TPP) de 15 840, donde el TPP se calcula como TPP = 2 × MacTOPS no dispersos × (longitud en bits de la entrada de multiplicación).  
13. Por **clúster de chips cubierto (CCC)** se entiende cualquier conjunto de chips de IA o clúster en red con un poder de cómputo efectivo agregado superior a 16 equivalentes H100. Un clúster en red se refiere a chips que o bien están físicamente coubicados, tienen un ancho de banda agregado entre nodos —definido como la suma del ancho de banda entre distintos hosts/chasis— superior a 25 Gbit/s, o están conectados en red para realizar cargas de trabajo conjuntamente. El poder de cómputo efectivo agregado de 16 chips H100 es de 15 840 TFLOP/s, o 253 440 TPP, y se basa en la suma de TPP por chip. Algunos ejemplos de CCC son: el servidor GB200 NVL72, tres servidores H100 HGX de ocho vías ubicados en el mismo edificio, CloudMatrix 384, un pod con 32 chips TPUv6e y todo superordenador.  
14. **Los medios técnicos nacionales (NTM)** incluyen satélites, aeronaves, medios cibernéticos, señales, imágenes (incluidas las térmicas) y otras capacidades de teledetección empleados por las Partes para la verificación de conformidad con el presente Tratado.  
15. **Verificación del uso de chips** son métodos que proporcionan información sobre las actividades que se ejecutan en determinados chips de computadora, con el fin de diferenciar entre actividades aceptables y prohibidas.  
16. **Métodos utilizados para crear modelos de vanguardia** se refiere al amplio conjunto de métodos utilizados en el desarrollo de la IA. Incluye, entre otros, arquitecturas de IA, optimizadores, métodos de tokenización, curación de datos, generación de datos, estrategias de paralelismo, algoritmos de entrenamiento (por ejemplo, algoritmos de AR) y otros métodos de entrenamiento. Esto incluye el post-entrenamiento, pero no incluye métodos que no cambian los parámetros de un modelo entrenado, como el uso de *prompts*. Es posible que en el futuro se creen nuevos métodos.

---

#### Notas

La definición de IA aquí (adaptada de la [Ley de Protección de Denunciantes de IA](https://www.congress.gov/bill/119th-congress/senate-bill/1792/text) del senador Chuck Grassley) es posiblemente demasiado amplia, y creemos que sería necesario definirla mejor para excluir sistemas informáticos obviamente seguros, como los correctores ortográficos o los de reconocimiento de imágenes.

En el paradigma actual de desarrollo de la IA, esta definición es probablemente aceptable, dado que el desarrollo de la IA se distingue claramente de la investigación y el desarrollo no relacionados con la IA debido al uso de grandes cantidades de recursos computacionales. Sin embargo, la definición definitiva de IA que se utilice debería abarcar más que los sistemas de aprendizaje profundo o aprendizaje automático. Si bien el aprendizaje automático es el paradigma dominante actual para la IA, el Tratado debería tener en cuenta la posibilidad de que otro paradigma de IA cobre importancia en el futuro; de lo contrario, un Tratado que prohíba únicamente el aprendizaje automático animaría a los investigadores a desarrollar nuevos paradigmas de IA para crear una IA más potente y general, lo que podría tener consecuencias catastróficas. Si surgiera un paradigma novedoso, especialmente uno que no requiriera tantos chips de IA como el aprendizaje profundo, probablemente sería necesario actualizar el Tratado.

Utilizamos el H100-equivalente como métrica principal de la capacidad de cómputo. En el artículo V, esta métrica se utiliza para establecer el tamaño del mayor clúster de chips no supervisado permitido (16 H100-equivalentes).[^242] El artículo IV define los umbrales en términos del total de operaciones utilizadas para entrenar una IA, por lo que, al establecer límites a las operaciones no supervisadas por segundo, resultaría inviable llevar a cabo un entrenamiento ilegalmente grande en hardware no supervisado.

Utilizamos los equivalentes de H100 para tener en cuenta que existen múltiples chips de IA diferentes, pero lo que más nos importa es cuántas operaciones pueden realizar en un periodo de tiempo. Hay otras métricas de chips que son importantes (como la memoria de alto ancho de banda), pero en general creemos que estas importan menos que el número de operaciones por segundo. Los equivalentes de H100 como unidad son una forma [relativamente estándar](https://www.rand.org/pubs/perspectives/PEA3776-1.html) de analizar el poder de cómputo.

La definición de clúster de chips cubiertos (CCC) que incluimos es una primera aproximación a cómo definir ese concepto. Lo ideal sería que el límite fuera lo suficientemente alto como para evitar que la gente común infringiera las normas (es decir, un ancho de banda de 25 Gbit/s entre chasis es más rápido que las conexiones a internet fuera de los centros de datos; es muy raro y caro que una persona posea más de 16 equivalentes H100). También debería ser lo suficientemente bajo como para evitar que se produzcan actividades peligrosas de IA y dificultar la subversión (es decir, dificultar el entrenamiento distribuido entre múltiples conjuntos de chips sub-CCC). Analizamos esta decisión con más detalle en el comentario sobre el artículo V.

Los chips de IA son un subconjunto de chips informáticos avanzados, y no existe una línea clara que distinga los chips de IA de los que no lo son. En lugar de definir y basarnos en una distinción, utilizamos la capacidad de cómputo total (en operaciones por segundo) de un clúster, medida en equivalentes H100. Si los chips pueden configurarse para entrenar o ejecutar IA y superan el umbral definido, el Tratado exige que sean supervisados.

Ten en cuenta que algunos gobiernos pueden haber dejado de utilizar Medios Técnicos Nacionales (NTM) como [término](https://fas.org/publication/ntm-obe/) oficial. Lo utilizamos en este proyecto de tratado al estilo de los acuerdos de control de armas anteriores para facilitar la comparación. |

### Artículo III: ISIA {#article-iii:-isia}

1. Por el presente Tratado, las Partes establecen la Agencia Internacional de Superinteligencia (ISIA) para aplicar sus disposiciones, incluidas las relativas a la verificación internacional de su cumplimiento, y para que sirva de foro de consulta y cooperación entre ellas.  
2. Por la presente se establecen los siguientes órganos de la ISIA: la Conferencia de las Partes, el Consejo Ejecutivo y la Secretaría Técnica.  
3. Conferencia de las Partes  
   1. La Conferencia de las Partes se compone de todas las Partes en el Tratado.  
   2. La Conferencia de las Partes: determina la política general; aprueba y supervisa el presupuesto; elige a los miembros del Consejo Ejecutivo; examina las cuestiones de cumplimiento comunicadas por el Consejo Ejecutivo; y aprueba y revisa los anexos por recomendación del Consejo Ejecutivo.  
   3. Se reunirá en sesión ordinaria al menos una vez al año, o con mayor frecuencia si así lo decide la Conferencia, además de las sesiones extraordinarias que se requieran. Cada Parte dispondrá de un voto. El quórum se constituirá con la mayoría de las Partes.  
4. Consejo Ejecutivo
   1. El Consejo Ejecutivo estará compuesto por 15 miembros: (i) 5 puestos designados para los miembros permanentes del Consejo de Seguridad de las Naciones Unidas y (ii) 10 puestos electos distribuidos mediante representación geográfica equitativa. En el anexo A se desarrollan los detalles.  
   2. El mandato de los miembros elegidos es de dos años. Cada año se renueva la mitad de los puestos.  
   3. El Consejo Ejecutivo deberá: aprobar las inspecciones por denuncia, recomendar el presupuesto y la política a la Conferencia, nombrar al Director General y supervisar la Secretaría Técnica mediante el derecho de veto.  
   4. Los procesos para tomar decisiones son los siguientes:  
      1. El Consejo Ejecutivo elige al presidente y al vicepresidente.  
      2. El presidente o el vicepresidente pueden presidir la sesión.  
      3. La votación se efectúa según el principio de «un miembro, un voto».  
      4. Las votaciones para aprobar una inspección por requerimiento en virtud del artículo X requieren una mayoría de votos.  
      5. Las votaciones para vetar los cambios de la Secretaría Técnica a las definiciones o a los protocolos de salvaguardia requerirán una mayoría de dos tercios.  
      6. Las votaciones para destituir o nombrar a un Director General requieren una mayoría de dos tercios.  
      7. Todas las demás decisiones requieren una mayoría de dos tercios.  
      8. Para el quórum se requieren dos tercios del Consejo Ejecutivo.  
5. Secretaría Técnica y director general  
   1. El Director General de la Secretaría Técnica será su titular y principal funcionario administrativo.  
   2. El Director General es nombrado por el Consejo Ejecutivo por un período de cuatro años, renovable una vez. El Consejo Ejecutivo puede destituir al Director General.  
   3. La Secretaría Técnica contará en su inicio con divisiones técnicas de Seguimiento de Chips y Salvaguardias de Fabricación, Salvaguardias de Verificación del Uso de Chips, Controles de Investigación, Consolidación de Información, Revisiones Técnicas, Administración y Finanzas, y Asuntos Jurídicos y de Cumplimiento. El Director General podrá crear y disolver divisiones técnicas.  
   4. La Secretaría Técnica, por conducto del Director General, propone cambios en las definiciones técnicas y los protocolos de salvaguardia, según sea necesario para aplicar los artículos IV, V, VI, VII, VIII, IX y X del presente Tratado.  
      1. Los cambios urgentes en los umbrales de FLOP (artículo IV), el tamaño de los clústeres de poder de cómputo contemplados (artículo V) y los límites de la investigación restringida (artículo VIII) pueden ser aplicados inmediatamente por el Director General en caso de que la inacción suponga un riesgo para la seguridad. Dichos cambios permanecerán en vigor durante treinta días. Transcurrido ese plazo, los cambios deberán ser aprobados por el Consejo Ejecutivo para seguir en vigor.  
      2. El Consejo Ejecutivo tomará las decisiones sobre cuestiones de fondo, en la medida de lo posible, por consenso; el Director General se esforzará por alcanzarlo. Si no es posible llegar a un consenso en un plazo de 24 horas, se procederá a una votación. El Consejo Ejecutivo aceptará los cambios si los aprueba la mayoría de los miembros presentes y votantes; de lo contrario, los rechazará.  
6. El presupuesto ordinario de la ISIA se financia con las contribuciones asignadas a las Partes, utilizando una escala derivada de la escala de cuotas de las Naciones Unidas, sujeta a un mínimo y un máximo establecidos por el Consejo Ejecutivo. Los Estados miembros también tienen la opción de realizar contribuciones voluntarias para la investigación sobre seguridad de la IA —en lo relativo a la alineación y la interpretabilidad— y para actividades de creación de capacidad en los Estados miembros, que incluyen los usos beneficiosos de una IA segura, el desarrollo de bancos de pruebas, las buenas prácticas, el intercambio de información y la facilitación de la cooperación y las actividades conjuntas, siguiendo libremente el modelo de la [red de Centros de Apoyo a la Seguridad Nuclear del OIEA](https://www.iaea.org/services/networks/nssc).

---

#### Notas

Al igual que en otros organismos internacionales, la ISIA estaría integrada por diplomáticos y expertos técnicos de los países signatarios. El punto principal del texto anterior es que se otorga a la ISIA la autoridad para aplicar la mayor parte de lo que exige el tratado y actualizar algunos aspectos del mismo a lo largo del tiempo. Este enfoque da prioridad a la prevención de la creación de superinteligencia y al mantenimiento de la vigencia del tratado hasta que el mundo esté preparado para seguir adelante. Como tal, este borrador otorgaría a esta organización multinacional la autoridad para llevar a cabo gran parte de lo que se requiere. La ISIA centraliza la aplicación de varias funciones clave del tratado, entre ellas el mantenimiento de los límites precisos de la investigación, el desarrollo y el despliegue permitidos de la IA, la verificación principal del cumplimiento del tratado y la consolidación de la información confidencial de inteligencia de los signatarios. Es fundamental que el funcionamiento cooperativo de la ISIA genere la confianza necesaria entre las partes del tratado a lo largo del tiempo.

Dicho esto, este tipo de enfoque conlleva ciertas contrapartidas. Una primera contrapartida es que una mayor centralización requiere más confianza por parte de las naciones signatarias. Los posibles signatarios del tratado podrían no considerar políticamente viable asignar este nivel de autoridad a una organización internacional, o podrían no confiar en que la organización vaya a funcionar con suficiente independencia de la influencia de sus miembros más poderosos.

Un acuerdo alternativo podría centralizar solo aquellas pocas funciones que deben ser centralizadas (como mantener y aclarar los límites a la investigación, el desarrollo y el despliegue de la IA), permitiendo a la vez que cada signatario verifique individualmente el cumplimiento a su entera satisfacción.

Otra disyuntiva surge en cuanto a qué tan inclusivo deba ser dicho tratado. Nuestro texto de ejemplo crearía una organización multilateral en la que se invitaría a todos los Estados a firmar el tratado y a participar en su ejecución. Una alternativa a esto sería un tratado centrado, por ejemplo, solo en China y Estados Unidos. La idea detrás de esto sería construir un régimen de verificación bilateral limitado que satisfaga las necesidades de cada parte, sacrificando la menor cantidad posible de autonomía y transparencia. A cada parte se le asignaría entonces el objetivo separado y posterior de incorporar a otros Estados.

Dado que el objetivo de este tratado es demostrar cómo podrían ser los controles internacionales *si* los líderes mundiales se dieran cuenta de los graves peligros, ilustramos una estructura que funcionaría con muchas Partes, todas ellas con algo que ganar al adherirse al tratado.

Como tal, la estructura propuesta del Consejo Ejecutivo de la ISIA incluye a todos los miembros permanentes del Consejo de Seguridad de las Naciones Unidas y toma como modelo la composición del OIEA.

Dada la posición de TSMC como fabricante líder de chips de IA, cualquier tratado sobre IA debe tener en cuenta cómo abordar la cuestión de Taiwán. Hemos optado por utilizar el precedente del TNP con respecto a Taiwán. Lo ideal sería que Taiwán implementara acuerdos y/o declaraciones formales que establecieran que se considera vinculado por los principios de este Tratado. Esto significaría la adhesión de facto al Tratado. Más importante aún, Taiwán aceptaría un acuerdo que permitiera inspecciones rutinarias y/o por denuncia in situ para garantizar que cumple los principios del tratado.

Todavía no está del todo claro cómo deberían tomar las decisiones el Consejo Ejecutivo y la ISIA, ni qué potestades se delegan a los diferentes órganos. En este artículo se presenta una propuesta de estructura que otorga un poder considerable a la Secretaría Técnica, al tiempo que asigna al Consejo Ejecutivo la función de supervisión.

Una de las ventajas de este diseño es que permite al órgano técnico tomar decisiones con rapidez y le otorga un amplio mandato para cumplir su misión, aunque cualquier cambio requiere la aprobación de una mayoría simple de los miembros del Consejo Ejecutivo en un plazo de 30 días para que siga vigente. Si bien los líderes mundiales pueden mostrarse reacios a delegar tanto poder en expertos técnicos, es probable que estos tampoco confíen en que los actores geopolíticos resuelvan las espinosas cuestiones técnicas que surgirán al aplicar este Tratado y se adapten suficientemente para responder a un panorama técnico cambiante. Este artículo presenta solo una propuesta sobre cómo equilibrar el poder de decisión entre estos grupos, pero hay muchos otros enfoques posibles.

Otro enfoque de este tipo sería desglosar aún más las responsabilidades, las definiciones y los tipos de salvaguardias aplicadas por la ISIA (por ejemplo, los umbrales de FLOP de entrenamiento, la definición de CCC, la definición de chip de IA, si una instalación concreta debe considerarse una instalación de producción de chips, los protocolos de verificación del uso de chips, la definición de investigación restringida, etc.) y establecer diferentes procedimientos de votación para estos cambios, en función de su impacto.

---

#### Antecedentes

La estructura de gobierno tripartita de la Agencia Internacional de Superinteligencia (ISIA) propuesta en nuestro borrador de tratado sigue el modelo de la OPAQ,[^243] el organismo encargado de aplicar la Convención sobre Armas Químicas (CAQ). Los nombres de estos órganos también se toman de la OPAQ. (Es posible que un tratado real prefiera otras estructuras y nombres que cumplan las mismas funciones; más adelante ofrecemos precedentes de algunas disposiciones menos centralizadas).

El Consejo Ejecutivo, establecido en virtud de los apartados (a) y (d) de nuestro párrafo 4, emula a la Junta de Gobernadores del TNP. La designación de cinco de los quince puestos del Consejo para los miembros permanentes del Consejo de Seguridad de las Naciones Unidas refleja que los cinco Estados poseedores de armas nucleares originales del TNP coincidían con los cinco miembros permanentes del Consejo de Seguridad de la ONU; sin su participación como socios centrales, es probable que el TNP hubiera fracasado desde el principio.

Nuestra disposición de «diez puestos elegidos distribuidos por representación geográfica equitativa» también se hace eco del TNP, que estipula que entre sus gobernadores salientes se incluya «al miembro más avanzado en la tecnología de la energía atómica, incluida la producción de materiales básicos, en cada una de» las ocho regiones especificadas.

Taiwán complica nuestro concepto de tratado, dada su delicada situación geopolítica y su condición de productor de la mayoría de los chips de IA del mundo. Afortunadamente, los precedentes nos sirven de guía: aunque Taiwán no es parte del TNP, ha declarado en múltiples ocasiones que se considera vinculado por los principios del TNP. Taiwán permite al OIEA realizar inspecciones y aplicar salvaguardias a sus instalaciones nucleares mediante un acuerdo trilateral con Estados Unidos y el OIEA. Se podría llegar a un acuerdo similar con respecto a nuestro proyecto de tratado.

Los procesos de toma de decisiones del Consejo Ejecutivo de nuestro proyecto de tratado se han modelado a partir del [Reglamento y los Procedimientos de la Junta de Gobernadores](https://www.iaea.org/about/policy/board/rules-and-procedures-of-the-board-of-governors) que utiliza el Organismo Internacional de Energía Atómica (OIEA), la principal organización para la gobernanza internacional de la tecnología nuclear.[^244] Asimismo, los procedimientos de votación siguen el [Estatuto del OIEA](https://www.iaea.org/about/statute#a1-6).

En otros tratados sobre armas nucleares pueden encontrarse precedentes de mecanismos de aplicación menos centralizados (pero aún así potencialmente eficaces). El [Tratado sobre Fuerzas Nucleares de Rango Intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm#text) (INF) y los Tratados de Reducción de Armas Estratégicas ([START I](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html), [START II](https://2009-2017.state.gov/t/avc/trty/104150.htm) y [Nuevo START](https://2009-2017.state.gov/documents/organization/140035.pdf)) responsabilizan a las partes individuales de la aplicación y verificación; cada una se compromete a seguir procedimientos que permitan a la otra obtener garantías razonables de cumplimiento.

Las «inspecciones por desafío» del párrafo 4(c) se basan en el mecanismo de la Parte X de la CAQ; desarrollaremos este precedente con más detalle en el artículo X.

---

### Artículo IV: Entrenamiento de IA {#article-iv:-ai-training}

1. Cada Parte acuerda prohibir el entrenamiento de IA por encima de los siguientes umbrales: cualquier ejecución de entrenamiento que supere 1e24 FLOP o cualquier ejecución posterior al entrenamiento que supere 1e23 FLOP. Cada Parte acuerda no realizar ejecuciones de entrenamiento por encima de estos umbrales y no permitir que ninguna entidad bajo su jurisdicción las realice.  
   1. La Secretaría Técnica podrá modificar estos umbrales, de conformidad con el proceso descrito en el artículo III. El Consejo Ejecutivo podrá vetar dichas decisiones por mayoría de dos tercios.  
2. Cada Parte notificará a la ISIA cualquier ejecución de entrenamiento entre 1e22 y 1e24 FLOP antes de su inicio. Esto se aplica a las ejecuciones de entrenamiento realizadas por la Parte o cualquier entidad dentro de su jurisdicción.  
   1. Este informe debe incluir, entre otros datos, todo el código de entrenamiento y una estimación del total de FLOP que se utilizará. La Parte debe proporcionar al personal de la ISIA acceso supervisado a todos los datos, con un registro de acceso adecuado a la sensibilidad de los datos y protecciones contra la duplicación o la divulgación no autorizada. No proporcionar al personal de la ISIA acceso suficiente a los datos es motivo para denegar la ejecución del entrenamiento, a discreción de la ISIA. La ISIA podrá solicitar cualquier documentación adicional relacionada con la ejecución del entrenamiento. La ISIA también preaprobará un conjunto de pequeñas modificaciones que podrían realizarse en el procedimiento de entrenamiento durante el entrenamiento. Cualquier cambio de este tipo se comunicará a la ISIA en el momento de su realización.  
   2. La falta de respuesta de la ISIA transcurridos 30 días constituirá la aprobación; no obstante, la ISIA podrá prorrogar este plazo si notifica que necesita tiempo adicional para su revisión. Estas prórrogas no tienen límite, pero las Partes podrán recurrir los retrasos excesivos ante el Director o el Consejo Ejecutivo.  
   3. La ISIA podrá supervisar dichas sesiones de entrenamiento, para lo cual la Parte le proporcionará, previa solicitud, los puntos de control del modelo, incluido el modelo final entrenado [los detalles iniciales de dicha supervisión deberán describirse en un anexo].  
   4. En caso de que la supervisión indique capacidades o comportamientos preocupantes por parte de la IA, la ISIA podrá ordenar la pausa de una o un tipo de ejecuciones de entrenamiento hasta que considere seguro reanudarlas.
   5. La ISIA mantendrá prácticas de seguridad robustas. La ISIA no compartirá información sobre las ejecuciones de entrenamiento declaradas a menos que determine que el entrenamiento declarado viola el Tratado, en cuyo caso proporcionará a todas las Partes del Tratado información suficiente para determinar si se ha producido una violación.  
   6. En caso de que una Parte descubra una sesión de entrenamiento que supere los umbrales designados, deberá notificarla a la ISIA y detenerla (si está en curso). Dicha sesión solo podrá reanudarse con la aprobación de la ISIA.  
3. Cada Parte, y las entidades bajo su jurisdicción, podrán realizar ejecuciones de entrenamiento de menos de 1e22 FLOP sin la supervisión o aprobación de la ISIA.  
4. La ISIA podrá autorizar, por mayoría de dos tercios del Consejo Ejecutivo, excepciones específicas para actividades como evaluaciones de seguridad, vehículos autónomos, tecnología médica y otras actividades que el director general considere seguras. Estas excepciones pueden permitir pruebas de entrenamiento superiores a 1e24 FLOP con la supervisión de la ISIA, o una presunción de aprobación por parte de la ISIA para pruebas de entrenamiento entre 1e22 y 1e24 FLOP.

---

#### Notas

Una de las principales formas de mejorar las IA actuales es [aumentando los recursos computacionales](#preventing-the-creation-of-more-and-better-ai-chips) dedicados al entrenamiento de la IA. Restringir estos recursos y limitar la investigación sobre el progreso algorítmico (descrita en el artículo VIII) puede reducir el riesgo de que se construya una IA más inteligente que los humanos a corto plazo.

La prohibición se basa en el número de operaciones computacionales utilizadas, ya que esto es relativamente fácil de definir y medir, y el rendimiento de los modelos de IA de última generación existentes nos proporciona algunos datos sobre las cantidades de hardware de computación que parecen seguras utilizando los algoritmos actuales.

En un mundo perfecto, en cambio, limitaríamos el entrenamiento en función de las capacidades del sistema entrenado. Pero nadie tiene la capacidad técnica para predecir con certeza lo que una nueva IA será capaz de hacer o no hacer antes de que sea entrenada. Por lo tanto, los recursos computacionales se utilizan como sustituto.

La prohibición estricta de 1e24 FLOP para el entrenamiento está ligeramente por debajo de la cantidad de recursos computacionales utilizada para modelos cercanos al estado del arte en agosto de 2025, como DeepSeek-V3 con [3e24 FLOP](https://epoch.ai/data/ai-models). Elegimos este umbral porque está por debajo del nivel en el que esperamos que las IA sean peligrosas (dados los algoritmos actuales) y porque proporciona cierto margen de maniobra si se produce un progreso algorítmico moderado.

La prohibición del entrenamiento posterior por encima de 1e23 FLOP se aplica a las IA creadas antes de la entrada en vigor del Tratado. Muchas de estas IA se habrán creado utilizando más de 1e24 FLOP (a mediados de 2025 hay [entre 50 y 100 modelos de este tipo](https://epoch.ai/tools/model-counts)). Dado que los pesos de muchas de estas IA se habrán publicado abiertamente, no es factible impedir que la gente las utilice, pero sí lo es impedir que se les realicen grandes modificaciones mediante el entrenamiento posterior.

Un proceso de entrenamiento de 1e22 FLOP en 16 H100 duraría alrededor de una semana. Esto requiere tanto poder de cómputo que los aficionados no superarán accidentalmente el umbral al entrenar modelos pequeños y permitidos. Las IA entrenadas a esta escala (con los algoritmos actuales) hasta ahora han parecido estar lejos de ser peligrosas. El progreso de la investigación en algoritmos de IA, si no se impide su realización y difusión, podría situar a la humanidad en un régimen aún más peligroso, lo que requeriría prohibiciones dirigidas a actividades que actualmente podrían llevar a cabo pequeños grupos decididos y dispuestos a realizar gastos sustanciales.

Dentro de la banda de 1e22-1e24 FLOP, la ISIA recibiría por adelantado el código de entrenamiento y las estimaciones totales de FLOP. Esto les permitiría aprobar la ejecución del entrenamiento si no utiliza ningún método algorítmico novedoso.

El entrenamiento de una IA suele implicar pequeñas correcciones de errores sobre la marcha (como instalar la versión correcta de una librería o reordenar los datos de entrenamiento). La ISIA puede preaprobar un conjunto de pequeñas modificaciones en una ejecución del entrenamiento que no requerirían una nueva aprobación. Aun así, estas modificaciones se seguirían comunicando a la ISIA.

La obligación de informar y supervisar las sesiones de entrenamiento de entre 1e22 y 1e24 FLOP permitiría obtener algunos de los beneficios de entrenar IA de una manera que, por el momento, parece segura, al tiempo que impediría la construcción de IA más grandes y potencialmente peligrosas.

La supervisión también permitiría a la ISIA mantenerse (en cierta medida) al día de cualquier avance algorítmico que se produzca a pesar de las prohibiciones, y el artículo XIII estipula que la ISIA realice evaluaciones de los modelos entrenados en este rango. Esto ayudará a la ISIA a estar al tanto de las tendencias en el desarrollo de la IA, lo que, con suerte, le permitirá ajustar a la baja los umbrales de FLOP si es necesario.

La supervisión de los procesos de entrenamiento por parte de la ISIA es factible y exigible gracias a la consolidación de chips (artículo V) y a la verificación del uso de chips (artículo VII). La consolidación de chips significa que la ISIA tendrá acceso al hardware utilizado para los procesos de entrenamiento a gran escala. La verificación del uso de chips permitirá a la ISIA tener cierta confianza en que los chips solo se utilizan para entrenar IA con permiso.

El personal de la ISIA tiene acceso a los datos de entrenamiento utilizados en las ejecuciones de entrenamiento monitorizadas, sujeto a diversas restricciones. El propósito de dichas restricciones es garantizar el uso de registros y otros métodos de control para impedir la divulgación no autorizada del contenido sensible en los datos de entrenamiento, incluyendo, entre otros, información de identificación personal, información sanitaria personal, datos clasificados, secretos comerciales, datos bancarios sujetos a leyes de secreto profesional, etc. 

---

#### Antecedentes

Si bien los valores numéricos de los umbrales especificados en nuestro acuerdo pueden y deben revisarse cuando se supere la fase inicial del borrador, los límites cuantitativos son habituales en los acuerdos internacionales, lo que previene disputas que, de otro modo, dependerían de interpretaciones divergentes del lenguaje cualitativo.

El [Tratado de Prohibición de Pruebas de Umbral] de 1974 (https://2009-2017.state.gov/t/isn/5204.htm) estableció un límite de 150 kilotones para las pruebas nucleares subterráneas realizadas por los Estados Unidos y la URSS.[^245] El objetivo y el efecto de este tratado era, al menos en cierta medida, obstaculizar el desarrollo de ojivas nucleares más grandes y destructivas, capaces de arrasar ciudades enteras. Un paralelismo relevante con el desarrollo de la IA es que, a mediados de 2025, los modelos más generales y capaces —y, por lo tanto, más peligrosos— requieren procesos de entrenamiento correspondientemente más largos para su creación; nuestro tratado especifica límites destinados a impedir que se desarrollen intencionadamente tales IA, pero también a cosechar el beneficio esencial (aunque no paralelo) de reducir el riesgo de que se supere accidental e irremediablemente un umbral de capacidades imprevisto.

El límite de entrenamiento que hemos sugerido como punto de partida es lo suficientemente bajo como para que algunos modelos de IA entrenados hoy en día lo superen; consideramos que esto es prudente en previsión de los avances que harán que los nuevos modelos sean más capaces por unidad de entrenamiento (discutido en el artículo VIII). Los acuerdos de reducción de armas sientan un precedente para los umbrales establecidos por debajo del nivel máximo actual. El Tratado de Washington (Naval) de 1922 (https://www.digitalhistory.uh.edu/disp_textbook.cfm?smtID=3&psid=3995) estableció límites de desplazamiento para los buques de guerra que obligaron a Estados Unidos y otras potencias navales a desguazar docenas de buques capitales. En el artículo II del tratado START de 1991,[^247] Estados Unidos y la Unión Soviética (y más tarde, la Federación Rusa) acordaron limitar el tamaño de sus arsenales nucleares y sus sistemas de lanzamiento, lo que les obligó a eliminar gradualmente [más de cuatro mil ojivas cada uno](https://bush41library.tamu.edu/archives/public-papers/3263).

El precedente de establecer umbrales cuantitativos para limitar el potencial de ruptura se analizará en el artículo V.

---

### Artículo V: Consolidación de chips {#article-v:-chip-consolidation}

1. Cada una de las Partes velará por que, dentro de su jurisdicción, todos los clústeres de chips contemplados (CCC), tal y como se definen en el artículo II (es decir, un conjunto de chips con una capacidad superior a 16 equivalentes H100) \[nótese que 16 H100 cuestan en conjunto unos 500 000 dólares en 2025 y que rara vez son propiedad de particulares\], se encuentren en instalaciones declaradas a la ISIA y que estos chips de IA estén sujetos a la supervisión de la ISIA.  
   1. Las partes procurarán no ubicar los chips de IA junto con hardware informático no auxiliar y no relacionado con la IA en estas instalaciones declaradas.  
   2. Estas instalaciones deberán ser accesibles para la inspección física. Esto puede incluir, por ejemplo, que los equipos de verificación puedan llegar a cualquier CCC desde al menos un aeropuerto con servicio internacional regular en un plazo de 12 horas.  
   3. Las Partes no alojarán chips de IA en tantas ubicaciones diferentes que a la ISIA le resulte inviable supervisarlas todas. Si la ISIA lo solicita, las Partes deberán consolidar sus chips de IA en menos instalaciones supervisadas.  
2. Los chips de IA no supervisados que no formen parte de un CCC (es decir, que tengan una capacidad inferior a 16 equivalentes H100) podrán permanecer fuera de las instalaciones declaradas a la ISIA, siempre que dichas existencias no sean agregadas ni se conecten en red para cumplir la definición de CCC, no se roten entre sitios para eludir la supervisión y no se utilicen para entrenamiento prohibido. Las Partes harán esfuerzos razonables para supervisar la venta y la agregación de chips de IA a fin de garantizar que se detecten y supervisen los CCC de nueva creación.  
3. En un plazo de 120 días a partir de la entrada en vigor del Tratado, cada Parte deberá localizar, inventariar y consolidar todos los CCC en instalaciones declaradas a la ISIA. Las Partes se abstendrán de desagregar, ocultar o reasignar chips para eludir este requisito o para que un conjunto de chips que habría sido clasificado como CCC deje de estarlo.  
4. La ISIA supervisará el proceso de consolidación nacional, entre otras cosas mediante inspecciones in situ, verificación de documentos e inventarios, acompañamiento de las autoridades nacionales durante las transferencias y las inspecciones, e intercambio de información con las Partes en virtud del artículo X. La ISIA podrá exigir registros de la cadena de custodia para las transferencias y podrá realizar inspecciones por requerimiento, tal como se describe en el artículo X. Las Partes proporcionarán acceso oportuno a las instalaciones, los centros de transporte y los registros pertinentes. Las protecciones e incentivos para los denunciantes previstos en el artículo X se aplicarán al proceso de consolidación, y la ISIA mantendrá canales de denuncia protegidos.  
5. En un plazo de 120 días a partir de la entrada en vigor del Tratado, las Partes deberán presentar a la ISIA un registro de sus CCC. El registro deberá incluir la ubicación, el tipo, la cantidad, los identificadores de serie u otros identificadores únicos, cuando estén disponibles, y las interconexiones asociadas de todos los chips de IA de los CCC. Cada Parte deberá proporcionar a la ISIA un registro actualizado y preciso a más tardar cada 90 días.  
6. Las Partes deberán notificar a la ISIA cualquier transferencia prevista de chips de IA, ya sea nacional o internacional, con un mínimo de 14 días de antelación a la fecha prevista para la transferencia. No se procederá con ninguna transferencia a menos que se conceda a la ISIA la oportunidad de observarla. En el caso de las transferencias internacionales, tanto la Parte emisora como la receptora deberán coordinarse con la ISIA en cuanto a la ruta, la custodia y la recepción. Las transferencias de emergencia realizadas por motivos de seguridad se notificarán tan pronto como sea posible, con una verificación *a posteriori*.  
7. Los chips de IA rotos, defectuosos, sobrantes o fuera de servicio seguirán siendo tratados como chips funcionales hasta que la ISIA certifique su destrucción. Las partes no destruirán chips de IA sin la supervisión de la ISIA. La destrucción o la inutilización permanente se llevarán a cabo bajo la supervisión de la ISIA, utilizando métodos aprobados por la ISIA, y se registrarán en un certificado de destrucción [los detalles deberán explicarse en un anexo]. Se prohíbe la recuperación o la reventa de componentes de dicho hardware, salvo que la ISIA lo autorice expresamente.

---

#### Notas

Analizaremos el objetivo que persigue este artículo, por qué creemos que es importante, por qué creemos que es factible, por qué se ha elegido el límite de 16 H100 y varias otras consideraciones.

##### A qué apunta el artículo V

El objetivo de este artículo es centralizar, en instalaciones supervisadas, todos los clústeres de chips de IA (es decir, conjuntos de chips interconectados que superen un tamaño pequeño) y la gran mayoría de los chips de IA.

Una vez que los chips se hayan centralizado en instalaciones supervisadas, la ISIA puede adoptar muchos más enfoques para garantizar que dichos chips no se utilicen para infringir el artículo IV; estos enfoques se analizan en el artículo VII.

Es deseable contar con una verificación internacional de este proceso de centralización, para que todas las Partes confíen en que las demás también han centralizado sus chips. Es probable que este tipo de verificación pueda realizarse sin mucho esfuerzo en los grandes centros de datos de IA, puesto que es probable que las agencias de inteligencia ya sepan dónde se encuentran. Para los centros de datos más pequeños, la ISIA puede supervisar los procesos de centralización nacionales como medida de fomento de la confianza.

El artículo, en efecto, tiene una excepción para un número reducido de chips: menos de 16 equivalentes a H100. Esta cantidad de chips es tan pequeña que es poco probable que los chips no supervisados supongan una amenaza (a falta de avances en la investigación). Los umbrales más bajos serían más difíciles de controlar con éxito y podrían empezar a imponer costos a una población más amplia. (Varias personas tienen un puñado de GPU en su casa, pero pocas tienen 16 equivalentes a H100).

##### Por qué existe el artículo V

La centralización de los chips es útil porque abre otras vías para controlar el desarrollo de la IA mediante los chips.

Por ejemplo, la centralización de los chips en instalaciones declaradas significa que posteriormente podrían ser objeto de una mayor supervisión sobre cómo se utilizan (artículo VII) o de una verificación de que están apagados.

La centralización de los chips en instalaciones declaradas también facilitaría a las Partes destruir dichos chips, como podría ser necesario en virtud del artículo XII, si una Parte infringiera el Tratado de forma continua.

Debido al potencial de destrucción de los CCC, sería conveniente construirlos lejos de los centros de población siempre que sea posible. Esto no se incluye en el texto del artículo porque presenta problemas de viabilidad (los centros de datos actuales suelen estar cerca de las ciudades, por lo que sería necesario construir nuevos), porque en casos extremos es probable que puedan cerrarse sin muchos daños colaterales y porque no es un elemento fundamental para el Tratado. No obstante, debido a su enorme potencial de peligro, parece apropiado tratar los centros de datos de IA como instalaciones militares.

##### Verificación

Una parte fundamental para el éxito de un tratado sobre IA es la verificación. Los países no confiarán en que los demás cumplan las normas, sino que querrán comprobarlo por sí mismos. La centralización de los chips de IA en instalaciones declaradas deberá confirmarse mediante inspecciones de la ISIA y la supervisión de este proceso. De lo contrario, los países no tendrán la confianza suficiente en que los demás hayan centralizado con éxito sus chips.

La centralización de los chips podría no ser necesaria si existieran otras formas de monitorizar los chips de IA. Lamentablemente, creemos que actualmente esta es la única opción viable, salvo la destrucción física de todas las reservas de chips de IA, dadas las capacidades de los mecanismos de seguridad disponibles en los chips actuales.

En el futuro, podrían desarrollarse [mecanismos de gobernanza por hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) para permitir la gobernanza remota de los chips de IA, de modo que no sea necesario centralizarlos en ubicaciones declaradas. [Aarne et al. (2024)](https://www.iaps.ai/research/secure-governable-chips) proporcionan estimaciones sobre el tiempo de implementación de algunos de estos mecanismos de gobernanza en chip. Sus estimaciones abarcan el plazo para desarrollar mecanismos que sean robustos frente a diferentes adversarios. Por motivos de concisión, utilizaremos sus estimaciones para la seguridad en un contexto antagónico encubierto, ya que creemos que esto se ajusta a la situación que se pretende abordar: los actores estatales competentes pueden intentar vulnerar los mecanismos de gobernanza, pero habría graves consecuencias si se detectara esta subversión. Estiman un tiempo de desarrollo de entre 2 y 5 años para las soluciones ideales, con opciones menos seguras pero potencialmente viables disponibles en solo unos meses.

Aunque ese informe tiene más de un año, no tenemos constancia de avances significativos en estos mecanismos, y creemos que la estimación más relevante de Aarne et al. es de dos a cinco años adicionales.

Más allá de desarrollar los mecanismos, es necesario o bien añadir mecanismos de gobernanza en chip a los nuevos chips y que estos se integren en el parque de chips existente, o bien adaptar los mecanismos a los chips ya existentes. Aarne et al. estiman que la primera de estas opciones podría llevar cuatro años, pero somos optimistas en que la adaptación podría realizarse en uno o dos años si ya se está realizando un seguimiento de los chips.

Para que quede claro, la centralización a la que se refiere el artículo V implica la concentración física y el monitoreo (en el artículo VII) de los clústeres de chips contemplados, pero no exige que los gobiernos se hagan con la propiedad de los chips. En el caso de los grandes centros de datos, el tratado permite que estos y sus chips permanezcan en el mismo lugar y sigan siendo propiedad de las empresas, siempre que reciban monitoreo y supervisión del gobierno nacional y de la ISIA para garantizar que los centros de datos solo se dedican a actividades no relacionadas con la IA o a actividades como la ejecución de modelos antiguos, en lugar de crear nuevos modelos de IA más capaces. En el caso de un número menor de chips, puede ser necesario trasladarlos físicamente a un centro de datos más grande, pero el propietario de los chips podría seguir accediendo a ellos de forma remota, de manera análoga a algunos modelos de computación en la nube existentes. Alternativamente, los chips podrían transferirse a la propiedad del gobierno a cambio de una compensación justa.

##### Viabilidad

Probablemente sea factible recopilar (y verificar internacionalmente la recopilación) la mayoría de los chips de IA. En el caso de los centros de datos de IA más grandes, como los que tienen más de 100 000 equivalentes H100, la verificación internacional parece relativamente sencilla; estos centros de datos son difíciles de ocultar y es probable que los servicios de inteligencia ya sepan dónde se encuentran. (No esperamos que para mediados de 2025 se haya hecho ningún intento de ocultar estos centros de datos). Estos centros de datos son detectables por su huella física y su consumo de energía, y muchos de ellos han sido [reportados públicamente](https://epoch.ai/data/ai-supercomputers).

Es plausible que estos métodos también permitan localizar centros de datos tan pequeños como unos 10 000 equivalentes a H100. Aparte de los servicios de inteligencia de NTM y de las Partes, el plan propuesto para centralizar los chips implica que las autoridades nacionales utilicen diversas facultades para ello y permitan que la ISIA inspeccione el proceso, a fin de evitar la exclusión de centros de datos.

Los Estados dispondrán de una serie de herramientas para localizar los chips que se encuentran en su territorio. Pueden exigir legalmente que se notifiquen todos los clústeres de chips de más de 16 H100; pueden utilizar los registros de ventas y otra información financiera de los distribuidores de chips; pueden entrevistar a técnicos con experticia en la construcción de centros de datos; etcétera. Si sospechan que hay contrabando o que se están ocultando chips, pueden recurrir a las fuerzas del orden para que investiguen más a fondo. Este proceso de centralización nacional sería supervisado por inspectores de la ISIA para garantizar que se lleva a cabo correctamente.

La localización de centros de datos existentes podría hacerse rápidamente —en cuestión de días o semanas para clústeres de más de 1000 equivalentes a H100—, utilizando los métodos descritos.

En realidad, la *centralización* de los chips podría llevar más tiempo, ya que podría ser necesario ampliar la capacidad de los centros de datos en las instalaciones que se convertirían en CCC. No está claro cuánto tiempo llevaría, ni si sería factible en algunos países, localizar todos los clústeres de, por ejemplo, 100 H100. Esta cantidad de chips estaría prohibida, pero esta prohibición podría ser difícil de hacer cumplir.

Uno de los mayores problemas de viabilidad de la centralización de los clústeres de chips es lograr que otros países confíen justificadamente en que ninguna de las partes está llevando a cabo un proyecto secreto de IA con chips no declarados. La verificación internacional de la centralización nacional de chips podría ayudar, pero puede que no sea suficiente, ya que los esfuerzos de centralización nacional podrían ser deliberadamente incompletos.

Las restricciones impuestas por este artículo contribuyen en gran medida a ofrecer garantías a las Partes contra proyectos secretos de IA respaldados por gobiernos que operen en otros países. Además de la supervisión del proceso de centralización nacional por parte de la ISIA, las labores de inteligencia y las inspecciones por requerimiento descritas en el artículo X podrían ser de gran ayuda.

##### ¿Por qué la definición de CCC?

Este artículo y la definición correspondiente de CCC establecen un límite de 16 equivalentes H100. Este umbral pretende cumplir varios criterios:

* La supervisión de clústeres de chips de más de 16 H100 funciona bien con los umbrales de FLOP de entrenamiento del artículo IV. El entrenamiento con 16 H100 (precisión FP8, utilización del 50 %; parámetros realistas pero optimistas) tardaría 7,3 días en llegar a 1022 FLOP y 2 años en llegar a 1024 FLOP. Por lo tanto, sería factible que las personas utilizaran chips no declarados para alcanzar el umbral mínimo, pero sería algo poco práctico que alcanzaran el umbral de entrenamiento prohibido.  
* Es verosímil que este umbral sea suficiente para impedir el avance de las capacidades de la IA, si se combina con las prohibiciones a la investigación en IA del artículo VIII. El artículo IV establece restricciones al entrenamiento: se prohíbe el entrenamiento a gran escala y se permite el entrenamiento a escala media, aunque sujeto a supervisión. Probablemente sea aceptable —es decir, que plantea un riesgo mínimo— permitir el entrenamiento a pequeña escala, como el que se puede realizar con 16 H100 en un plazo realista.
* Este umbral tiene consecuencias limitadas para los aficionados y la gente común. Muy pocas personas poseen más de 16 H100. A mediados de 2025, 16 chips H100 cuestan alrededor de 500 000 dólares; no son un artículo de consumo habitual. Nadie va a superar el umbral por error por tener unas cuantas consolas de videojuegos antiguas abandonadas en casa.  
* Reunir chips de IA se vuelve más difícil a medida que disminuye el número de chips permitidos. Encontrar centros de datos con 100 000 chips es fácil; con 10 000, probablemente también sea relativamente fácil; con 1 000, no está claro; y con 100, puede ser bastante difícil. Este umbral se elige en parte por la inviabilidad de hacer cumplir un umbral más bajo; incluso 16 H100 podrían ser un umbral difícil de hacer cumplir.  
* Es posible que haya que revisar esta definición y bajar el umbral (por ejemplo, a 8 equivalentes H100). En este tratado, la ISIA se encargaría de evaluar esta definición y modificarla según fuera necesario.

##### Otras consideraciones

Este artículo pide a las partes que intenten evitar la coubicación de chips de IA con otros chips que no sean de IA ni auxiliares. Se sugiere esto porque la coubicación podría dificultar la verificación del uso de los chips (artículo VII) y requeriría que estos chips que no son de IA también estuvieran sujetos a cierta supervisión para implementar eficazmente la verificación del uso de los chips de IA. Sin embargo, no es estrictamente necesario y puede que no sea deseable. Por ejemplo, en la actualidad los chips de IA suelen coubicarse con chips que no son de IA, y el inconveniente de cambiar esto podría superar al de supervisar también los chips que no son de IA.

De conformidad con acuerdos anteriores, este artículo exige que los inspectores puedan acceder rápidamente a los CCC para su verificación. En este caso, es probable que se supervise continuamente muchas de estas instalaciones (artículo VII), y el acceso a los aeropuertos podría ser beneficioso.

Existe cierto riesgo de que ciudadanos particulares puedan construir un CCC no supervisado a partir de chips «sueltos» equivalentes a H100. Para combatir esto, el tratado establece que las Partes harán «esfuerzos razonables» para supervisar las ventas de chips (que superen el equivalente a 1 H100) y detectar la formación de nuevos CCC. Se podrían adoptar medidas más estrictas, como exigir que todos esos chips y ventas se registren y rastreen formalmente. Nuestro borrador no llega a ese extremo, tanto porque no esperamos que haya tantos chips «sueltos» equivalentes a H100 sin contabilizar después de que se cataloguen todos los chips de los CCC, como porque otros mecanismos (como la protección de los denunciantes) ayudan a detectar los CCC de nueva formación.

##### Alternativas

En lugar de exigir inmediatamente la centralización de los pequeños clústeres (por ejemplo, 100 H100), el tratado podría aplicar un enfoque por etapas. Por ejemplo, durante los primeros 10 días deberían centralizarse y declararse todos los centros de datos con más de 100 000 chips equivalentes a H100; en los 30 días siguientes, aquellos con más de 10 000, y así sucesivamente.

Este enfoque por etapas podría corresponder mejor a la verificabilidad internacional de la detección de estos clústeres a medida que los servicios de inteligencia intensifican sus esfuerzos de detección. Es decir: al principio, es probable que los servicios de inteligencia sepan dónde se encuentran los centros de datos más grandes, pero no los de tamaño medio (si no los habían buscado previamente). Por lo tanto, al principio solo se declararían los centros de datos grandes y, a medida que los servicios de inteligencia continúan tratando de localizar los chips, el umbral bajaría.

Este enfoque podría ajustarse mejor a la forma en que la verificabilidad y la exigibilidad han influido en lo acordado en anteriores acuerdos internacionales. Por ejemplo, el Tratado de Prohibición Parcial de Ensayos Nucleares de 1963 no prohibió los ensayos subterráneos de armas nucleares, debido a la dificultad de detectar dichos ensayos. El equipo de gobernanza técnica de MIRI tiene previsto publicar un informe con un enfoque por etapas como este.

Una desventaja del enfoque por etapas es que podría facilitar que los Estados oculten chips y establezcan centros de datos secretos.

---

#### Precedentes

La declaración de los activos pertinentes suele ser un primer paso en los tratados restrictivos. Las Partes en el Tratado Naval de Washington de 1922 proporcionaron inventarios de buques capitales y su tonelaje, y se comprometieron a notificarse mutuamente la sustitución de estos buques. El tratado START I de 1991 incluía un acuerdo clasificado sobre el intercambio de coordenadas y diagramas de emplazamientos (en [el artículo VIII](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art8)), que detallaba el intercambio de datos sobre la ubicación de todas las armas estratégicas declaradas. El párrafo 3 del artículo V de nuestro proyecto de acuerdo exige que las Partes localicen, inventaríen y consoliden los clústeres de chips contemplados en un plazo de 120 días.

La consolidación de activos para facilitar la verificación del cumplimiento suele ser otro paso en los tratados restrictivos. El [artículo III](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art3) del START I prohibía que los misiles balísticos intercontinentales (ICBM) se ubicaran junto a instalaciones de lanzamiento espacial, lo que facilitaba la vigilancia. El párrafo 1.a de nuestro artículo V compromete a las partes a «evitar la coubicación de chips de IA con hardware no auxiliar y ajeno a la IA» por la misma razón.

La historia demuestra que la consolidación también limita el potencial de ruptura, al facilitar el ataque a las concentraciones de activos problemáticos en caso de una crisis de confianza. En el PAIC de 2016[^248] (también conocido como el acuerdo nuclear con Irán), Irán acordó mantener sus centrifugadoras de enriquecimiento de uranio operativas en solo dos emplazamientos designados (Natanz y Fordow), los cuales fueron [atacados](https://www.iaea.org/newscenter/pressreleases/update-on-developments-in-iran-5) en operaciones de Israel y Estados Unidos en junio de 2025. Esto motiva una nota que acompaña a nuestro artículo V en la que sugerimos a las partes que ubiquen sus clústeres de chips cubiertos (CCC) lejos de los centros de población.

La supervisión y las inspecciones son componentes comunes de los tratados anteriores en contextos de confianza limitada; por consiguiente, hemos redactado disposiciones al respecto cuando ha sido apropiado, en los párrafos 1, 4, 6 y 7 de este artículo. Algunos precedentes específicos son: 
* La verificación del START I incluyó [cientos de inspecciones in situ](https://www.armscontrol.org/factsheets/start-i-glance) en los primeros años.  
* La CAQ exige la declaración e inspección de todas las instalaciones de producción de armas químicas —se han [declarado 97](https://www.opcw.org/media-centre/opcw-numbers)— y la mayoría de ellas han sido destruidas de forma verificable. (Al exigir la declaración de las instalaciones existentes, estos acuerdos también prohíben que se realicen determinadas actividades fuera de las instalaciones declaradas, de forma análoga a la prohibición de este artículo sobre los CCC no supervisados).  
* Más de [700 instalaciones nucleares declaradas](https://www.armscontrol.org/factsheets/iaea-safeguards-agreements-glance) en todo el mundo son supervisadas por el OIEA como parte del TNP.

Al igual que en el párrafo 3 de este artículo, numerosos acuerdos de control de armamentos estipulan que las partes no deben interferir con los NTM de las demás en el contexto de la verificación de los tratados. Entre ellos se encuentran el SALT I,[^249] el ABM,[^250] el INF,[^251] y el START I.

Como precedente de Estados parte que restringen las industrias de su sector privado para cumplir los compromisos de un tratado (como tendría que ocurrir con la IA), está la legislación que adoptó Estados Unidos tras ratificar la Convención sobre las Armas Químicas (CAQ): la [Ley de aplicación de la Convención sobre Armas Químicas de 1998](https://www.congress.gov/105/plaws/publ277/PLAW-105publ277.pdf#page=857) y la normativa del Departamento de Comercio garantizaron su cumplimiento por parte de las entidades estadounidenses. Del mismo modo, el Congreso de Estados Unidos [modificó](https://www.epa.gov/ozone-layer-protection/ozone-protection-under-title-vi-clean-air-act) la Ley de Aire Limpio tras la ratificación del Protocolo de Montreal para prohibir las sustancias que agotan la capa de ozono.

Las estrategias para implementar la centralización de chips en Estados Unidos podrían recurrir a la [Cláusula de Expropiación](https://constitution.congress.gov/browse/essay/amdt5-9-1/ALDE_00013280/) de la Quinta Enmienda, según la cual el gobierno puede ejercer su poder de expropiación forzosa para adquirir propiedad privada con fines públicos, a cambio de una indemnización adecuada. |

---

### Artículo VI Supervisión de la producción de chips de IA {#article-vi-ai-chip-production-monitoring}

1. La ISIA supervisará las instalaciones de producción de chips de IA y los datos de entrada clave para su producción. Esta supervisión garantizará que todos los chips de IA recién producidos reciban seguimiento y supervisión de inmediato hasta su instalación en los CCC declarados y que no se establezcan cadenas de suministro sin supervisión.  
   1. La ISIA supervisará las instalaciones de producción de chips de IA que se determine que producen o podrían producir chips de IA y hardware relevante [en un anexo se detallarán las definiciones precisas de «instalaciones de producción de chips de IA», «chips de IA» y «hardware relevante», así como los métodos de supervisión].  
   2. El seguimiento de los chips de IA de nueva producción incluirá el de su producción, venta, transferencia e instalación. Este seguimiento comenzará con la fabricación. El conjunto completo de actividades incluye la fabricación de memoria de gran ancho de banda (HBM), la fabricación de chips lógicos, las pruebas, el encapsulado y el montaje [este conjunto de actividades debería especificarse en un anexo].  
2. En las instalaciones donde el seguimiento y la supervisión de la ISIA no sean factibles o no se hayan implementado, se detendrá la producción de chips de IA. La producción de chips de IA podrá continuar cuando la ISIA declare que se han implementado medidas de seguimiento y supervisión aceptables.  
3. Si una instalación de producción de chips supervisada se desmantela o se reutiliza, la ISIA supervisará ese proceso y, si se lleva a cabo a satisfacción de la ISIA, finalizará el requisito de supervisión.  
4. Ninguna de las Partes venderá ni transferirá chips de IA ni equipos de fabricación de chips de IA, salvo con la autorización y el seguimiento de la ISIA.  
   1. La venta o transferencia de chips de IA dentro o entre las Partes del Tratado se presumirá aprobada, y la ISIA se encargará de su seguimiento.  
   2. La venta o transferencia de equipos de fabricación de chips de IA entre las Partes del Tratado no tendrá presunción de aprobación. La aprobación de dicha transferencia se basará en una evaluación del riesgo de desvío o de retirada del Tratado por parte de la Parte receptora.  
   3. La venta o transferencia de chips de IA y equipos de fabricación de chips de IA a Estados no Partes o entidades fuera de un Estado Parte tendrá una presunción de denegación.  
5. Ninguna de las Partes venderá ni transferirá chips de computadora avanzados sin IA ni equipos de fabricación de chips de computadora avanzados sin IA a Estados no Parte o a entidades fuera de un Estado Parte, salvo que la ISIA lo autorice y monitoree.  
6. La venta o transferencia de chips informáticos avanzados que no sean de IA, o de equipos para su fabricación, dentro de las Partes del Tratado o entre ellas no está restringida en virtud del presente artículo.

---

#### Notas

La [cadena de suministro de chips de IA](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/) es reducida y especializada, por lo que es factible supervisar la producción de chips. La [gran mayoría](https://www.datacenterdynamics.com/en/news/nvidia-gpu-shipments-totaled-376m-in-2023-equating-to-a-98-market-share-report/) de los chips de IA los diseña NVIDIA. Los chips lógicos más avanzados (el procesador principal) que se utilizan en los chips de IA los fabrica casi en su totalidad TSMC, que representa [alrededor del 90 %](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) de la cuota de mercado. [La mayoría de los chips de IA](https://epoch.ai/data/machine-learning-hardware?view=table) se fabrican en versiones del nodo de proceso de cinco nanómetros de TSMC, un nodo que probablemente solo admiten [dos o tres plantas de fabricación](https://www.blackridgeresearch.com/blog/list-of-tsmc-fabs-in-taiwan-arizona-kumamoto). Las máquinas de litografía EUV, un componente crítico en la fabricación de chips lógicos avanzados, las fabrica [exclusivamente](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) ASML. La memoria de alto ancho de banda (HBM), otro componente clave de los chips de IA, está dominada por [dos o tres](https://www.trendforce.com/news/2024/04/24/news-amid-foundry-overcapacity-competition-for-hbm-intensifies-rapidly/) empresas. Esta cadena de suministro tan limitada sería relativamente fácil de supervisar si existiera la voluntad de hacerlo.

La supervisión de la producción de chips de IA tendría efectos de desbordamiento relativamente pequeños. Aunque para producir chips de IA se utilizan algunos de los mismos procesos que para otros chips (por ejemplo, los chips de los teléfonos inteligentes), los chips en sí son distintos y podrían diferenciarse mediante la supervisión. El diseño de los chips cambiaría con el tiempo, pero, hoy por hoy, los chips de IA actuales probablemente serían identificables por su gran capacidad de HBM, sus componentes especializados de multiplicación matricial y su capacidad de escalamiento horizontal e interconexión.

Las investigaciones futuras deberían centrarse en identificar las partes principales de la cadena de suministro de chips de IA que serían más adecuadas para la supervisión. Basándonos en [los cuellos de botella existentes](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/), una primera hipótesis es centrar la supervisión en la producción de HBM, la fabricación de matrices lógicas y los pasos posteriores (por ejemplo, el encapsulado, las pruebas y el montaje de servidores), junto con la supervisión de insumos clave como las máquinas de litografía EUV.

Este artículo señala que las ventas de chips dentro de los Estados parte se presumirán aprobadas, pero no indica esta presunción para los equipos de fabricación de chips de IA. Podría ser aceptable permitir la venta generalizada de equipos de fabricación de chips dentro de los Estados parte, pero es probable que esto deba tratarse de forma más conservadora que la venta de chips en sí. Es probable que la venta de chips tenga un efecto a corto plazo, ya que el ciclo de vida de los chips de IA suele ser [de solo unos pocos años](https://epoch.ai/data-insights/gpu-frontier-lifespan#:~:text=For%20these%20newer%20chip%20designs,from%200.6%20to%203.7%20years.). Por el contrario, la capacidad de fabricación de chips podría dar lugar a una producción significativa de chips durante muchos años, y sería especialmente preocupante que los países con una producción nacional madura de chips de IA se retiraran del Tratado. Por lo tanto, sugerimos un enfoque más conservador para los equipos de fabricación de chips que para los propios chips.

Los párrafos 4 y 5 de este artículo permiten la venta de chips de IA y equipos de fabricación de chips a las Partes del Tratado, pero no a quienes no son Partes ni a otras entidades ajenas a estas. Es decir, las Partes aceptan los riesgos de la fabricación y concentración de chips, pero solo cuando estos se someten a supervisión. Así, la capacidad de fabricar y poseer chips sin provocar una respuesta de protección por parte de otros Estados se convierte en un incentivo positivo para adherirse al tratado.

Por sí solo, esto no impide que terceros Estados accedan a chips de IA en los Estados Parte de forma remota (es decir, computación en la nube o [Infraestructura como servicio](https://www.governance.ai/research-paper/governing-through-the-cloud)), pero dichos chips estarían sujetos a la supervisión de la ISIA para garantizar que no se utilizan en violación del artículo IV.

Las restricciones a los países no signatarios podrían ir más allá, si fuera necesario. Por ejemplo, se les podría prohibir el acceso remoto a los chips de IA (es decir, alquilar chips de IA en los países signatarios del Tratado a través de la nube) o el acceso a modelos de IA a través de API.

Este artículo aprovecha el hecho de que los chips son un insumo altamente excluible para el desarrollo de la IA. Propone supervisar la producción de chips y garantizar que estos no se contrabandeen fuera de los países signatarios del Tratado o a instalaciones no declaradas. Otro enfoque sería prohibir toda la producción de nuevos chips de IA. Este enfoque supondría un menor riesgo de desvío de chips, pero tiene el costo de perder el valor que estos chips podrían haber producido en aplicaciones de IA no relacionadas con la investigación ni el desarrollo. Seguiría dependiendo de cierta supervisión de las instalaciones de producción de chips, por ejemplo, para garantizar que solo producen chips no destinados a la IA o que se retiran del servicio. El diseño actual del Tratado propicia que la producción de chips continúe debido a los grandes beneficios que supone para el mundo poder utilizarlos. Sin embargo, para permitir que la producción de chips continúe de forma segura, sería necesario supervisar la cadena de suministro de chips y su uso (artículo VII). Creemos que ambas cosas son factibles, pero si una de ellas no lo fuera, la alternativa sería detener por completo la producción de chips. 

---

#### Precedentes

Las disposiciones de los tratados para la supervisión de las instalaciones de producción no son nuevas. El artículo XI del Tratado INF de 1987 permitió durante trece años realizar inspecciones en las instalaciones designadas donde se habían producido anteriormente sistemas vectores nucleares de alcance intermedio. La Sección VII del [protocolo de inspección](https://2009-2017.state.gov/t/avc/trty/102360.htm#inspections) adjunto permitía la vigilancia continua del perímetro y de los portales, que podía incluir el pesaje (y, en algunos casos, la inspección por rayos X) de cualquier vehículo que saliera de la instalación y fuera lo suficientemente grande como para transportar un misil pertinente.

La monitorización de la producción de chips de inteligencia artificial es más complicada, debido a la dificultad de discernir la función y las capacidades de un chip a partir de sus características externas; por eso nuestro artículo VI estipula que «el hardware pertinente deberá describirse con más detalle en un anexo», junto con los métodos de monitorización. Pero la experiencia de las salvaguardias del OIEA en el marco del TNP demuestra que es posible verificar una amplia variedad de componentes de producción y precursores a lo largo de la cadena de suministro. Una de las formas en que el OIEA lo hace es [proporcionando](https://www-pub.iaea.org/MTCD/Publications/PDF/Pub1669_web.pdf) [directrices](https://www.iaea.org/publications/13452/international-safeguards-in-the-design-of-enrichment-plants?utm_source=chatgpt.com) para el diseño de instalaciones que faciliten la inspección y reduzcan los costes de cumplimiento.

Las restricciones a la transferencia de productos finales, precursores y equipos de producción (como la que se sugiere aquí para los chips de IA y los equipos avanzados de fabricación de chips de computadora a Estados no signatarios del Tratado) tienen todos ellos precedentes importantes:

* En el artículo I del [TNP](https://www.un.org/en/conf/npt/2005/npttreaty.html), cada Estado poseedor de armas nucleares se compromete a «no transferir a ningún destinatario armas nucleares u otros dispositivos explosivos nucleares». En su artículo III, párrafo 2, también acuerda no proporcionar «materiales básicos o materiales fisionables especiales» ni equipos «especialmente diseñados o preparados para el procesamiento, uso o producción de materiales fisionables especiales».  
* El artículo I de la [CWC](https://2009-2017.state.gov/t/avc/trty/127917.htm) asimismo compromete a las partes a no «transferir, directa o indirectamente, armas químicas a nadie»; su artículo VII les obliga a someter los precursores enumerados a «prohibiciones específicas sobre la producción, adquisición, retención, transferencia y uso».  
* Durante la Guerra Fría, el [Comité Coordinador para el Control Multilateral de las Exportaciones](https://www.govinfo.gov/content/pkg/GPO-CRPT-105hrpt851/html/ch9bod.html#anchor5563742) (CoCom) estableció un conjunto coordinado de controles a las exportaciones del bloque occidental al bloque comunista, que abarcaban materiales de uso nuclear, municiones y artículos industriales de doble uso, como los semiconductores.  
* El [Grupo de Suministradores Nucleares](https://www.nuclearsuppliersgroup.org/index.php/en/) es un régimen multilateral de control de las exportaciones que restringe el suministro de tecnología nuclear y tecnología conexa que podría desviarse hacia programas de armas nucleares.  
* Especialmente relevante es la serie de [controles de exportación](https://www.bis.gov/press-release/commerce-strengthens-export-controls-restrict-chinas-capability-produce-advanced-semiconductors-military) de Estados Unidos que se han centrado en los chips de inteligencia artificial y los equipos avanzados de fabricación de chips, abarcando a docenas de países en los últimos dos años. 

---

### Artículo VII Verificación de la utilización de chips {#article-vii-chip-use-verification}

1. Las partes aceptan la verificación continua in situ del uso total de chips por parte de la ISIA en los CCC declarados. La Secretaría Técnica determinará y actualizará los métodos de verificación, de conformidad con el proceso descrito en el artículo III. El Consejo Ejecutivo puede vetar dichas decisiones con una mayoría de dos tercios. Estos métodos pueden incluir, entre otros:  
   1. Inspectores presenciales  
   2. Cámaras antimanipulación  
   3. Mediciones de características energéticas, térmicas y de redes
   4. Mecanismos en el chip habilitados por hardware, incluidos los mecanismos retroadaptados  
   5. Declaración de las cargas de trabajo y operaciones de los chips por el operador del CCC  
   6. Reejecución de las cargas de trabajo declaradas en una instalación de la ISIA para confirmar la fidelidad de las declaraciones  
2. Esta verificación tendrá como objetivo garantizar que los chips no se utilicen para actividades prohibidas, como el entrenamiento de IA a gran escala que se describe en el artículo IV.  
3. En los casos en que la ISIA evalúe que los métodos de verificación actuales no pueden proporcionar garantías suficientes de que el hardware de IA no se utilice para actividades prohibidas, el hardware de IA deberá apagarse y su inactividad deberá verificarse continuamente por inspectores presenciales u otros mecanismos de verificación aprobados por la ISIA.  
4. La ISIA podrá imponer diversas restricciones sobre el funcionamiento de los chips para garantizar la debida verificación. Estas restricciones podrán incluir, entre otras, las siguientes:  
   1. Restricciones en el ancho de banda y la latencia entre diferentes chips, o entre los chips y la red del centro de datos, para distinguir la inferencia permitida del entrenamiento prohibido.  
   2. Restricciones en el número o la velocidad de FLOP/s, o en el ancho de banda de la memoria, a los que pueden operar los chips, con el fin de distinguir la inferencia permitida del entrenamiento prohibido u otras cargas de trabajo prohibidas.  
   3. Restricciones a la precisión numérica de las operaciones de los chips, con el fin de diferenciar las cargas de trabajo de IA de las que no son de IA.  
5. La ISIA abordará la verificación de los diferentes CCC de manera diferenciada, en función de su probabilidad de uso para actividades de IA y de su sensibilidad para la seguridad nacional.  
6. La ISIA liderará la investigación y la ingeniería para desarrollar mejores tecnologías para la monitorización y verificación del uso de chips. Las partes apoyarán estos esfuerzos [se proporcionarán más detalles en un anexo].

---

#### Notas

Los países querrán asegurarse de que los chips de IA existentes no se utilicen para realizar entrenamientos de IA peligrosos. También habrá razones legítimas para utilizar estos chips para ejecutar servicios de IA existentes, como (versiones ya existentes de) ChatGPT. Para hacer frente a esta tensión, la ISIA puede aplicar medidas de verificación a los chips de IA para garantizar que no se empleen en actividades prohibidas.

Esto funciona, entonces, como un incentivo positivo para adherirse a este Tratado: se podrán seguir utilizando los chips de IA, siempre y cuando la supervisión pueda verificar que no se estén utilizando de forma que infrinja el Tratado (como el entrenamiento de IA a gran escala). Aunque la supervisión continua de la ISIA pueda parecer poco deseable, creemos que es la mejor vía disponible. Dado el objetivo de prevenir el entrenamiento de IA a gran escala, hay dos enfoques principales: garantizar que nadie tenga el hardware necesario (es decir, que no puedan existir chips de IA) o garantizar que el hardware existente no se utilice para el entrenamiento a gran escala (es decir, el seguimiento de los chips y la verificación de su uso, tal y como se describe en los artículos V, VI y VII). Esto es conceptualmente análogo a las salvaguardias del OIEA: para que a un país sin armas nucleares se le permita tener materiales e instalaciones nucleares, es necesario que el OIEA inspeccione y garantice que su uso se destina únicamente a fines pacíficos.

Para que la verificación del uso de chips sea eficaz, la ISIA necesita mecanismos que le den garantías creíbles de que los chips de IA no se utilizan de forma prohibida por este Tratado. La forma más fácil de hacerlo es exigir que todos los chips estén apagados, pero esto no permitiría a las Partes beneficiarse del uso de los chips para fines no peligrosos. La verificación también podría facilitarse mediante medios tecnológicos, con la salvedad de que la tecnología actual [probablemente](https://www.cnas.org/publications/reports/secure-governable-chips) contiene vulnerabilidades de seguridad que permitirían a los propietarios de los chips eludir las medidas de supervisión. Dado el estado incipiente de la tecnología de verificación, es probable que esta requiera una supervisión continua in situ o el apagado de los chips hasta que los medios tecnológicos maduren. Sin embargo, una vez que la tecnología de supervisión haya madurado, unos sólidos [mecanismos de gobernanza basados en hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) podrían permitir la supervisión remota de los chips con total confianza.[^252]

Diversas restricciones y límites podrían facilitar el monitoreo del uso de los chips. Las diferentes cargas de trabajo de IA, como el entrenamiento frente a la inferencia, difieren en sus requisitos técnicos, y estas diferencias podrían servir de base para la verificación (si son lo suficientemente sólidas).

Por ejemplo, un mecanismo de verificación podrían ser los límites de interconexión: restringir la cantidad de comunicación que un conjunto de chips puede tener con el mundo exterior usando cables de red de bajo ancho de banda. En la práctica, esto consiste en tomar un pequeño conjunto de chips —digamos, 8 H100— y limitar su comunicación externa a una cantidad tan pequeña que los chips solo puedan hacer inferencias de forma eficiente, pero no puedan usarse de forma eficiente para el entrenamiento a gran escala. Esto funciona porque (según los algoritmos de 2025) el entrenamiento tiene requisitos de comunicación mucho más elevados que la inferencia. Este mecanismo sería útil si ejecutar las IA existentes es aceptable, pero entrenar nuevas IA no lo es.

Hay varios matices en estos mecanismos y muchos otros mecanismos de verificación potenciales, por lo que remitimos a los lectores curiosos a la [bibliografía](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) [previa](https://aigi.ox.ac.uk/publications/verification-for-international-ai-governance/) sobre [el tema](https://www.rand.org/pubs/working_papers/WRA4077-1.html). Este artículo encomienda a la ISIA el desarrollo de mejores mecanismos de verificación y la implementación de varios mecanismos, definidos en términos generales. Creemos que esta flexibilidad es necesaria debido a la rápida evolución del ámbito de la IA y a la dificultad de predecir los avances que podrían alterar los métodos de verificación. La investigación sobre la verificación de la IA también se encuentra en una fase incipiente, y se necesitan más avances antes de que la ISIA disponga de un conjunto sólido de herramientas.

El artículo IV prohíbe el entrenamiento de IA a gran escala. Si aún no se han entrenado IA altamente capaces (y, por lo tanto, peligrosas) cuando la verificación entre en vigor, esta podría centrarse específicamente en el entrenamiento. Sin embargo, si ya se han creado IA suficientemente capaces, podría ser necesario supervisar adicionalmente si se están implementando en chips de IA o qué actividades están realizando estas IA; la verificación ya no puede centrarse en prevenir el entrenamiento. La verificación sería mucho más difícil si se hubieran creado sistemas de IA suficientemente peligrosos (por ejemplo, aquellos que podrían contribuir sustancialmente a la I+D en IA), ya que verificar que no están realizando actividades prohibidas sería más difícil que verificar que no se está llevando a cabo un entrenamiento de IA a gran escala. Concretamente, probablemente será más fácil diferenciar entre el entrenamiento y la ejecución de IA que diferenciar entre la ejecución de IA en un tipo de tarea y la ejecución de IA en un tipo de tarea diferente. Debido a esta diferencia en la dificultad de la verificación, la labor de este Tratado sería mucho más fácil de realizar en un mundo en el que el progreso de las capacidades de la IA se detuviera pronto. Si el progreso de las capacidades de la IA continúa, la verificación del uso de chips podría requerir más trabajo, gastos y restricciones.

Del mismo modo, podría ser deseable supervisar el contenido de la inferencia de la IA para garantizar que las IA no se utilicen con fines perjudiciales. Muchas empresas de IA ya aplican esta supervisión de la inferencia, por ejemplo, para detectar si los usuarios intentan utilizar la IA para fabricar [armas biológicas](https://openai.com/index/preparing-for-future-ai-capabilities-in-biology/). Podría ser deseable aplicar este tipo de supervisión a nivel mundial, siempre que se puedan mitigar los riesgos de privacidad y seguridad pertinentes.

El párrafo 5 de este artículo permite a la ISIA llevar a cabo diferentes métodos de verificación para diferentes CCC. Una de las razones de esta distinción es práctica: los diferentes CCC requerirán diferentes enfoques de verificación para establecer una confianza justificada en que no se están utilizando para el desarrollo de IA peligrosa. Por ejemplo, los grandes centros de datos que anteriormente se utilizaban para el entrenamiento de IA de vanguardia tendrían la mayor capacidad de contribuir al entrenamiento prohibido, por lo que podría ser necesaria una mayor supervisión sobre ellos.

En segundo lugar, la diferenciación de los enfoques de verificación haría que el Tratado fuera más aceptable, ya que requeriría una supervisión menos invasiva para los CCC sensibles. Por ejemplo, las agencias de inteligencia o los ejércitos no quieren que la ISIA supervise sus centros de datos, y esta disposición ayuda a lograr un equilibrio. Seguiría siendo necesario verificar que estos centros de datos no se utilizan para actividades peligrosas de IA, y la ISIA colaboraría con estos grupos para asegurarse de que puede obtener la información que necesita, a la vez que satisface las necesidades de privacidad y seguridad de los propietarios de los CCC. Por otra parte, permitir diferentes protocolos de verificación podría perjudicar la viabilidad del tratado si se considera injusto, especialmente si la toma de decisiones en torno a estos procesos es poco equitativa.

El plan de este tratado consiste en permitir que continúe el uso y la producción de chips. Esto permite que el mundo se beneficie de estos chips. Un enfoque alternativo consiste en detener la producción de nuevos chips y/o destruir los chips existentes. Si se destruyeran los chips, a falta de avances algorítmicos, el tiempo de ruptura —el tiempo que transcurre desde que una parte inicia actividades peligrosas hasta que estas tendrían éxito si no se detuvieran— se prolongaría, ya que los actores tendrían que producir primero muchos chips de IA (lo que probablemente sería detectable y podría llevar años o décadas, dependiendo del estado de la cadena de suministro de chips). Por lo tanto, la destrucción de los chips tendría la ventaja significativa de alargar considerablemente los tiempos de ruptura y aumentar la dificultad del desarrollo peligroso de la IA. Sin embargo, también tiene un costo considerable, ya que no permite que el mundo se beneficie de estos chips. Dado que creemos que es factible rastrear los chips y verificar su uso, nuestro proyecto de tratado opta por esa vía en lugar de la eliminación de los chips de IA. Pero ambas vías tienen sus ventajas.

---

#### Precedentes

En nuestro análisis de los precedentes del artículo VI, describimos la supervisión continua de las antiguas instalaciones de producción de misiles de rango intermedio en virtud del Tratado INF, que, si bien permitía el pesaje y el escaneo no destructivo de los vehículos que salían de las instalaciones, no permitía a los inspectores acceder al interior de los camiones ni a las propias instalaciones. Una supervisión perimetral análoga de los centros de datos puede proporcionar algunas pistas sobre las operaciones a partir del consumo de energía, las emisiones térmicas y el ancho de banda de la red. Sin embargo, para tener una seguridad razonable de que no se están llevando a cabo operaciones de IA restringidas, probablemente será necesaria una combinación de los elementos que enumeramos en el párrafo 1 de nuestro artículo VII, que incluye cámaras a prueba de manipulaciones, mecanismos en el chip habilitados por hardware e inspectores presenciales.

Estas prácticas ya son habituales para el Organismo Internacional de Energía Atómica, que [recurre cada vez más a tecnologías de vigilancia ininterrumpida](https://www.iaea.org/newscenter/news/surveying-safeguarded-material-24/7) para complementar las inspecciones:

> Se recopilan más de un millón de datos cifrados sobre salvaguardias a través de más de 1400 cámaras de vigilancia y 400 sensores de radiación y de otro tipo repartidos por todo el mundo. Más de 23 000 precintos instalados en instalaciones nucleares garantizan la contención de materiales y equipos.

Uno de los métodos utilizados en el marco del START I para verificar el cumplimiento de las características de rendimiento de los misiles fue el intercambio de casi todos los datos de telemetría transmitidos desde los sensores en vuelo durante las pruebas, tal y como se especifica en el [protocolo de telemetría](https://1997-2001.state.gov/www/global/arms/starthtm/start/telempro.html), que también exigía a las partes que proporcionaran cualquier equipo de reproducción y la información sobre el formato de los datos necesaria para interpretarlos. Dependiendo de la combinación de métodos de verificación adoptados, una Agencia Internacional de Superinteligencia puede utilizar métodos análogos, basándose en la supervisión ligera que los proveedores de computación en la nube realizan como [práctica habitual](https://www.governance.ai/research-paper/governing-through-the-cloud) para recopilar información sobre las cargas de trabajo de los clientes.

La supervisión gubernamental continua de las instalaciones comerciales privadas (como lo son la mayoría de los centros de datos) también tiene numerosos precedentes. La Comisión Reguladora Nuclear de EE. UU., encargada de supervisar la seguridad de los reactores nucleares nacionales, destina [dos inspectores residentes](https://www.nrc.gov/reactors/operating/oversight/rop-description/resident-insp-program.html) a cada central eléctrica comercial de EE. UU., y los productores cárnicos estadounidenses [no pueden llevar a cabo operaciones de sacrificio](https://www.fsis.usda.gov/sites/default/files/media_file/2021-02/Fed-Food-Inspect-Requirements.pdf) si el personal de inspección del FSIS[^253] no está presente para supervisarlas.

---

### Artículo VIII Investigación restringida: algoritmos y hardware de IA {#article-viii-restricted-research:-ai-algorithms-and-hardware}

1. Con el fin de impedir investigaciones específicas que amplíen la frontera de las capacidades de la IA o socaven la capacidad de las Partes para aplicar las medidas del presente Tratado, este Tratado designa como investigación restringida aquella que cumpla cualquiera de las condiciones siguientes:  
   1. Mejoras en los métodos utilizados para crear modelos de vanguardia, definidos en el artículo II, que mejorarían las capacidades de los modelos o la eficiencia del desarrollo, la implementación o el uso de la IA.  
   2. Métodos de entrenamiento distribuidos o descentralizados, o métodos de entrenamiento optimizados para su uso en hardware de consumo o de uso común.  
   3. Investigación sobre paradigmas de inteligencia artificial de la computación más allá del aprendizaje automático.
   4. Avances en la fabricación de chips o sus componentes relevantes para la IA.  
   5. Diseño de chips de IA con mayor rendimiento o más eficientes.  
2. La división de Control de la Investigación de la ISIA clasificará todas las actividades de investigación restringida como *Controladas* o *Prohibidas*.  
   1. Cada Parte deberá supervisar toda actividad de investigación controlada dentro de su jurisdicción, y tomará medidas para garantizar que dicha investigación sea supervisada y puesta a disposición de la división de Control de Investigaciones con fines de revisión y supervisión.  
   2. Cada una de las Partes se abstendrá de realizar investigaciones prohibidas y prohibirá e impedirá que cualquier entidad dentro de su jurisdicción realice dichas investigaciones.  
3. Ninguna de las Partes asistirá, fomentará ni compartirá investigaciones prohibidas. Esta prohibición se extiende al financiamiento, la adquisición, el alojamiento, la supervisión, la enseñanza, la publicación, el suministro de herramientas o chips controlados y la facilitación de la colaboración.
4. Cada Parte designará a un representante ante la división de Control de Investigaciones de la ISIA, dependiente de la Secretaría Técnica (establecida en el artículo III). A esta división le corresponderán las siguientes responsabilidades:  
   1. Interpretar y aclarar las categorías de Investigación Restringida, así como responder a preguntas sobre sus límites, en respuesta a nueva información y a solicitudes de investigadores, organizaciones o miembros de las Partes.  
   2. Interpretar y aclarar los límites entre la investigación controlada y la investigación prohibida, y responder a preguntas sobre estos límites, en respuesta a nueva información y a solicitudes de investigadores, organizaciones o miembros de las Partes.  
   3. Modificar la definición de Investigación Restringida y sus categorías, en respuesta a condiciones cambiantes o a solicitudes de investigadores, organizaciones o miembros del Partido.  
   4. Modificar la frontera entre la investigación controlada y la prohibida en respuesta a cambios en las condiciones o a solicitudes de investigadores, organizaciones o miembros del Partido.  
   5. El Consejo Ejecutivo podrá vetar cualquier cambio en la Investigación Restringida o en sus categorías y clasificaciones por mayoría de dos tercios, tal y como se describe en el artículo III.

---

#### Notas

Prohibir varias amplias categorías de investigación, cuando los conocimientos técnicos pertinentes ya se distribuyen en el sector privado, va a ser difícil. En nuestra propuesta, la investigación se restringe si desarrolla las capacidades o el rendimiento de la IA, o si pone en peligro el sistema de verificación establecido en los artículos anteriores.

Algunas investigaciones deben prohibirse para evitar que avancen las capacidades de la IA, incluso cuando se mantenga constante la cantidad de FLOP de entrenamiento utilizada. Esta prohibición debería abarcar toda investigación que pueda hacer más eficiente el entrenamiento de las IA o que pueda aumentar sus capacidades. Esto se conoce a menudo como «progreso algorítmico». En los paradigmas actuales, esto incluye los avances en los algoritmos utilizados en el preentrenamiento, el postentrenamiento y durante la inferencia. A medida que cambian los paradigmas, estas distinciones pueden volverse menos claras y pueden surgir nuevas categorías. Por esta razón, el tratado hace referencia al «desarrollo, despliegue o uso».

Las mejoras algorítmicas anteriores, como el *transformer*, demuestran el potencial de avances extraordinarios y rápidos en la vanguardia de las capacidades de la IA: los nuevos algoritmos pueden transformar el modo en que funciona la IA. Además, nuevos paradigmas podrían reducir drásticamente la cantidad de recursos computacionales necesarios para un nivel determinado de capacidad de IA. Por ejemplo, las IA modernas son *mucho* menos eficientes en el uso de datos que los seres humanos, lo que sugiere que existen algoritmos mucho más eficientes en el uso de datos, esperando a ser descubiertos.

Deben prohibirse otras investigaciones para evitar que se reduzcan los requisitos computacionales para entrenar IA peligrosas hasta el punto de que puedan entrenarse con un pequeño número de chips de IA (o muchos chips distribuidos en pequeños clústeres en numerosas ubicaciones), lo que dificultaría las labores de supervisión.

Además, esta prohibición de la investigación debe impedir que se investiguen nuevas formas de fabricar chips de IA no rastreables, ya que el régimen de monitoreo es viable en gran parte debido a la complejidad y centralización actuales de la fabricación de semiconductores avanzados relevantes para la IA.

Este artículo también prohíbe la investigación sobre el diseño de chips de IA de mayor rendimiento o más eficientes, que de otro modo se volverían [sustancialmente más eficientes](https://epoch.ai/data-insights/ml-hardware-energy-efficiency) año tras año. Un centro de datos que utilice chips de IA más eficientes sería más fácil de ocultar, ya que estos chips consumirían menos electricidad para un rendimiento igual o superior.

Los tipos específicos de investigación que están restringidos deberán actualizarse con el tiempo. Un ejemplo de una actividad que la ISIA podría querer restringir más adelante es la investigación sobre hardware de poder de cómputo mejorado que no sea de IA, si tal avance supusiera un riesgo para la verificación.

Los esfuerzos nacionales para restringir la investigación podrían comenzar centrándose en la publicación y el financiamiento de la investigación. Es probable que un cambio en las leyes y las normas sociales contribuya en gran medida a que la mayoría de los investigadores dejen de realizar investigaciones peligrosas. La diversidad de acciones restringidas en el párrafo 3 aborda la necesidad de garantizar que, si las actividades de investigación se dividen entre múltiples jurisdicciones, el tratado siga responsabilizando de forma inequívoca a cada Estado de prohibir y prevenir las actividades individuales. Esto se aplica, por ejemplo, en el caso de que una empresa de una jurisdicción contrate a un empleado en una segunda que opere de forma remota chips alojados en una tercera.

---

#### Precedentes

Las restricciones preventivas a la difusión de información relacionada con tecnologías peligrosas tienen su precedente en la [Ley de Energía Atómica de 1946](https://www.atomicarchive.com/resources/documents/postwar/atomic-energy-act.html), aún vigente, la cual establecía que la información sobre determinados temas era, por defecto, información restringida (la doctrina del «secreto innato»); las exclusiones quedaban a discreción de la nueva Comisión de Energía Atómica, creada en virtud de esta legislación:[^254]

> El término «datos restringidos», en el sentido en que se utiliza en esta sección, se refiere a todos los datos concernientes a la fabricación o utilización de armas atómicas, la producción de material fisionable, o el uso de dicho material para la producción de energía. Sin embargo, no incluirá los datos que la Comisión determine, cuando lo estime oportuno, que pueden publicarse sin que ello afecte negativamente a la defensa y la seguridad comunes.

A diferencia de otros tipos de clasificación gubernamental, los Datos Restringidos pueden ser creados (deliberada o accidentalmente) por el sector privado, una cuestión de constitucionalidad sin resolver[^255] que subraya la necesidad de un brazo regulador autorizado y capaz de tomar decisiones cotidianas sobre los límites exactos de los Datos Restringidos. La [Administración Nacional de Seguridad Nuclear](https://www.usa.gov/agencies/national-nuclear-security-administration) (NNSA) hace esto con los secretos nucleares en EE. UU. En virtud de nuestro artículo VIII, párrafo 5, la división de Controles de Investigación de la nueva ISIA asumiría esta función para la investigación de IA restringida. También desempeñaría otras funciones análogas a las de la NNSA, descritas en nuestro artículo IX, al: (1) mantener relaciones con investigadores y organizaciones que trabajan en proyectos que se acercan al umbral de clasificación, y (2) establecer una infraestructura segura para la notificación y contención de descubrimientos accidentales.

También hay precedentes de contención y control de la investigación en campos peligrosos. En los últimos meses de la Segunda Guerra Mundial, el Reino Unido y Estados Unidos colaboraron en [la Misión Alsos](https://ahf.nuclearmuseum.org/ahf/history/alsos-mission/) para capturar a científicos nucleares alemanes, recopilar información sobre los avances de Alemania hacia la bomba atómica e impedir que la URSS obtuviera estos recursos para su propio programa nuclear. El [Proyecto Overcast](https://airandspace.si.edu/stories/editorial/project-paperclip-and-american-rocketry-after-world-war-ii) (también llamado Operación Paperclip) fue un programa secreto estadounidense para emplear a ingenieros de cohetes alemanes después de la guerra.

La contención de la investigación restringida en materia de IA dentro de los Estados parte podría canalizarse a través de los marcos normativos existentes. En Estados Unidos, estos incluyen:

* El concepto de «exportaciones consideradas» (https://www.bis.gov/learn-support/deemed-exports/what-deemed-export) en la legislación sobre control de las exportaciones obliga a las entidades estadounidenses a obtener una licencia de exportación de la Oficina de Industria y Seguridad[^256] antes de compartir tecnologías controladas con personas extranjeras, pues dicho intercambio se considera una exportación.  
* El [Reglamento sobre el Tráfico Internacional de Armas](https://www.pmddtc.state.gov/ddtc_public?id=ddtc_kb_article_page&amp;sys_id=24d528fddbfc930044f9ff621f961987) (ITAR, por sus siglas en inglés), un conjunto de normativas del Departamento de Estado de los Estados Unidos que controlan la exportación de tecnologías militares y algunas de doble uso. El ITAR se utilizó para impedir un mayor desarrollo y uso de técnicas criptográficas por parte del sector privado hasta 1996, ya que estaban clasificadas como «artículo de defensa» en la [Lista de Municiones de los Estados Unidos](https://www.ecfr.gov/current/title-22/chapter-I/subchapter-M/part-121).  
* La [Ley de Secreto de Invenciones de 1951](https://www.congress.gov/bill/82nd-congress/house-bill/4687/text), que faculta a las agencias gubernamentales de EE. UU. para imponer «órdenes de secreto» a las nuevas solicitudes de patentes con implicaciones para la seguridad nacional. A los inventores no solo se les pueden denegar las patentes, sino que se les puede prohibir legalmente divulgar, publicar o incluso utilizar sus invenciones.[^257]

El Proyecto Overcast también sienta un precedente para controlar a los investigadores simplemente pagándoles bien para que actúen en interés del Estado. En el artículo IX se analizan precedentes adicionales para este tipo de incentivos.

---

### Artículo IX Verificación de restricciones de investigación {#article-ix-research-restriction-verification}

1. Cada Parte deberá crear o facultar a un organismo nacional con las siguientes responsabilidades:  
   1. Mantenerse al tanto de la labor de los investigadores y las organizaciones nacionales que trabajan en áreas adyacentes a la Investigación Restringida, y mantener relaciones con ellos, con el fin de comunicarles las categorías de Investigación Restringida establecidas en el Artículo VIII.  
   2. Imponer sanciones para disuadir a los investigadores y organizaciones nacionales de llevar a cabo Investigaciones Restringidas. Estas sanciones serán proporcionales a la gravedad de la infracción y deberán diseñarse para que constituyan un disuasivo suficiente. Cada Parte promulgará o modificará las disposiciones legales necesarias para permitir la imposición de estas sanciones.  
   3. Establecer una infraestructura segura para la notificación y contención de hallazgos accidentales que cumplan las condiciones de la Investigación Restringida. Estos informes se compartirán con la división de Control de la Investigación.  
2. Para contribuir a la verificación internacional de las prohibiciones de investigación, la división de Control de Investigaciones desarrollará e implementará mecanismos de verificación.  
   1. Estos mecanismos podrían incluir, sin limitarse a los siguientes:  
      1. Entrevistas realizadas por la ISIA a investigadores que hayan trabajado previamente en temas de investigación restringida o que trabajen actualmente en áreas afines.  
      2. Seguimiento de la situación laboral y el paradero de los investigadores que hayan trabajado anteriormente en temas de investigación restringida o que actualmente trabajen en áreas adyacentes.  
      3. Mantener auditores integrados de la ISIA en organizaciones seleccionadas de alto riesgo (por ejemplo, proyectos difíciles de distinguir de la investigación restringida, antiguas organizaciones de investigación en IA).  
   2. Las partes contribuirán a la implementación de estos mecanismos de verificación.  
   3. La información obtenida a través de estos mecanismos de verificación se recopilará en informes para el Consejo Ejecutivo, manteniendo la confidencialidad de la información sensible en la mayor medida posible para proteger la privacidad y los secretos de las personas y de las Partes.

---

#### Notas

Además de la restricción establecida en el artículo VIII, los países verificarán que no se estén llevando a cabo investigaciones prohibidas en materia de IA. Un aspecto clave de este enfoque es establecer «áreas adyacentes a la investigación restringida» y, a continuación, establecer relaciones con los investigadores que trabajan en estas áreas adyacentes. Hay tan pocos investigadores de primer nivel en IA en el mundo que podría ser factible realizar un seguimiento de las actividades de una parte significativa de ellos. Si solo se cuenta al personal técnico de las principales empresas de inteligencia artificial, se obtendría un total de 5000 investigadores, y si se cuenta el número de asistentes a las principales conferencias sobre inteligencia artificial, se obtendría [un total de 70 000](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences). Se cree comúnmente que un grupo mucho más reducido es fundamental para el desarrollo de la IA de vanguardia, probablemente de unos cientos de personas. [^258] Los Estados también podrían entrevistar a los investigadores sobre sus actividades y ofrecer asilo e incentivos económicos a los denunciantes (véase el artículo X).

Aunque ya hay muchos conocimientos técnicos de dominio público sobre las prácticas actuales de desarrollo de IA que un actor malintencionado podría aprovechar, creemos que las restricciones legales y la verificación tendrían efectos drásticos en el progreso general de este campo.

Si la vigilancia se ampliara a los investigadores e ingenieros que participan en el diseño y la fabricación de semiconductores, el alcance de la vigilancia necesaria aumentaría considerablemente. Si esto no fuera viable, tal vez los Estados podrían vigilar a las empresas en lugar de a las personas, aprovechando la complejidad y el alcance de la fabricación de semiconductores avanzados.

El párrafo 2 de este artículo establece un régimen de mayor transparencia que ayuda a verificar la prohibición de investigar. Estas medidas de cooperación tienen por objeto proporcionar a las Partes la seguridad que necesitan. Prevemos que las Partes trabajarán para encontrar un nivel de transparencia, mediante el desarrollo de mecanismos específicos, que genere confianza en la prohibición de investigar y, al mismo tiempo, minimice los costos para los intereses estatales y la privacidad personal. Verificar el cumplimiento de esta prohibición es una tarea compleja y delicada que requerirá un esfuerzo y una adaptación continuos. A las Partes puede preocuparles la posibilidad de que otros

Las Partes promulgarán prohibiciones de investigación a nivel nacional, pero luego las incumplirán con iniciativas gubernamentales secretas ocultas a los servicios de inteligencia extranjeros. Las investigaciones que infringirían las prohibiciones son de diversa envergadura, y es probable que las iniciativas a gran escala —en las que participan muchos investigadores y chips relacionados con la IA— sean fácilmente detectables por actores estatales decididos. Sin embargo, las iniciativas más modestas, como el desarrollo de paradigmas alternativos de inteligencia artificial, podrían implicar solo a unos pocos investigadores y hardware de uso común. Será difícil garantizar a las demás Partes que dichas iniciativas no se están ocultando. La combinación de la recopilación de información (artículo X) y los mecanismos de verificación de la ISIA podría ser suficiente. También señalamos la importancia de proteger a los denunciantes (artículo X).

---

#### Precedentes

Entre las agencias existentes facultadas para «mantenerse al corriente y en contacto con los investigadores y organizaciones nacionales» que corren el riesgo de desarrollar información restringida, como se pide en nuestro artículo IX (1.a.), se encuentran el DOE y la NNSA, que se analizan en los precedentes del artículo VIII.

Un precedente de la «supervisión de la situación laboral y el paradero de los investigadores» en campos de alto riesgo, como sugerimos en el párrafo 2.(a).(ii), se encuentra en el Centro Internacional de Ciencia y Tecnología (ISTC).[^259] Establecido en 1994, el ISTC se creó específicamente para reducir los riesgos de proliferación nuclear, [manteniendo a los investigadores nucleares soviéticos con un empleo remunerado en actividades pacíficas](https://astanatimes.com/2014/12/istc-headquartered-nazarbayev-university-2015/) y conectados con la comunidad científica internacional. El ISTC también muestra el potencial de los incentivos como complemento de las sanciones para evitar que los expertos técnicos (que pueden quedarse desempleados como consecuencia de este tratado) se dediquen a la investigación restringida.

En la medida en que las sanciones deban ser severas para proporcionar la disuasión indicada en nuestro artículo IX.1.(b), se puede encontrar un modelo en el [capítulo sobre aplicación (18)](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf#page=154) de la Ley de Energía Atómica de 1946, en virtud de la cual el intercambio no autorizado de datos restringidos puede castigarse con la pena de muerte o la prisión si las revelaciones se han realizado con intención traicionera.[^260]

A la hora de desarrollar para la ISIA una infraestructura segura de «notificación y contención de descubrimientos involuntarios de investigaciones restringidas», se pueden encontrar precedentes y plantillas potencialmente utilizables en los extensos [procedimientos del DOE](https://www.ecfr.gov/current/title-10/chapter-X/part-1045) para el manejo de diferentes categorías de datos sensibles. El [Sistema de notificación y procesamiento de incidentes](https://www.directives.doe.gov/directives-documents/200-series/0232.2-BOrder-a-chg1-minchg/@@images/file) del DOE, así como las instrucciones del Comité de Sistemas de Seguridad Nacional[^261] para la [filtración de información clasificada](https://sgp.fas.org/library/cnssi-1001.pdf), también pueden ser de utilidad.

La división de Control de la Investigación de nuestro tratado podría tener en cuenta las prácticas existentes del OIEA al elaborar protocolos de inspección. En el marco del [Protocolo Adicional Modelo](https://www.iaea.org/topics/additional-protocol) aprobado en 1997 por la Junta de Gobernadores del OIEA, los Estados que han suscrito acuerdos de salvaguardias amplias[^262] permiten inspecciones de [acceso complementario](https://www.iaea.org/sites/default/files/safeguards0408.pdf) para buscar material nuclear no declarado. Como parte de esas visitas, los inspectores pueden [entrevistar a los operadores](https://www-pub.iaea.org/MTCD/Publications/PDF/SVS-30_web.pdf), de forma análoga a nuestra propuesta del párrafo IX.2.a.i.

También proponemos «mantener auditores integrados de la ISIA en determinadas organizaciones de alto riesgo», de forma muy similar a como las [oficinas locales](https://pantex.energy.gov/news/press-releases/nnsa-release-nnsa-establishes-new-federal-office-pantex-plant) del DOE y la NNSA se encuentran hoy físicamente en laboratorios nucleares nacionales y plantas de producción gestionados por contratistas.

Para «proteger la privacidad y los secretos de las personas y las partes» al realizar verificaciones, como exige el párrafo 2(c) de este artículo, la división de Controles de Investigación de la ISIA podría adaptar las prácticas de compartimentación de las agencias de inteligencia de las Partes y los [acuerdos multilaterales de intercambio de inteligencia](https://law.yale.edu/mfia/case-disclosed/newly-disclosed-documents-five-eyes-alliance-and-what-they-tell-us-about-intelligence-sharing). Por ejemplo, en virtud de la «regla del tercero» o el «principio de control del originador» [práctica que se considera habitual en tales acuerdos](https://www.dcaf.ch/sites/default/files/publications/documents/MIICA_book-FINAL.pdf), se prohíbe revelar información compartida a terceros (potencialmente incluso a organismos de supervisión) sin el permiso de la agencia de origen.

---

### Artículo X: Consolidación de la información e inspecciones por requerimiento {#article-x:-consolidación-de-la-información-e-inspecciones-por-requerimiento}

1. Una fuente clave de información para la ISIA son los esfuerzos independientes de recopilación de información de las Partes. Como tal, la división de Consolidación de Información (Artículo III) estará preparada para recibir esta información.  
   1. La división de Consolidación de la Información adoptará las precauciones necesarias para proteger los secretos comerciales, industriales, de seguridad y de Estado, así como cualquier otra información confidencial a la que tenga acceso en el marco de la aplicación del Tratado, lo que incluye el mantenimiento de canales de comunicación seguros, confidenciales y, opcionalmente, anónimos.  
   2. A los efectos de garantizar el cumplimiento de las disposiciones del presente Tratado, cada Parte utilizará los medios técnicos nacionales (MTN) de verificación de que disponga, de manera compatible con los principios generalmente reconocidos del derecho internacional.  
      1. Cada Parte se compromete a no interferir en los medios técnicos nacionales de verificación de las demás Partes que operen de conformidad con lo anterior.  
      2. Cada Parte se compromete a no utilizar medidas deliberadas de ocultación que impidan la verificación por medios técnicos nacionales del cumplimiento de las disposiciones del presente Tratado.  
      3. Se alienta a las Partes, sin que ello constituya una obligación, a cooperar en los esfuerzos por detectar actividades peligrosas de IA en países que no son Partes y a apoyar los NTM de las Partes dirigidos a países que no son Partes, en la medida en que sean pertinentes para el presente Tratado.
2. Una fuente clave de información para la ISIA son las personas que le proporcionan evidencia de actividades de IA peligrosas. Estas personas están sujetas a las protecciones para denunciantes.  
   1. El presente artículo establece protecciones, incentivos y asistencia para las personas («denunciantes protegidos») que, de buena fe, proporcionen a la ISIA o a una Parte información creíble sobre violaciones reales, intentadas o planificadas del presente Tratado u otras actividades que supongan un riesgo grave de extinción humana, incluidos chips ocultos, centros de datos no declarados, formación o investigación prohibidas, evasión de la verificación o falsificación de declaraciones. Los denunciantes protegidos incluyen empleados, contratistas, funcionarios públicos, proveedores, investigadores y otras personas con información relevante, así como personas asociadas (familiares y allegados) que prestan asistencia o corren riesgo debido a la divulgación.  
   2. Las partes prohibirán y evitarán las represalias contra los denunciantes cubiertos y las personas asociadas, incluyendo, entre otras, el despido, la degradación, la inclusión en listas negras, la pérdida de beneficios, el acoso, la intimidación, las amenazas, las acciones civiles o penales, la cancelación de visados, la violencia física, el encarcelamiento, la restricción de movimientos u otras medidas adversas. Cualquier término contractual (incluidos los acuerdos de confidencialidad o de no difamación) que pretenda limitar las divulgaciones protegidas en virtud del presente Tratado será nulo e inaplicable. El maltrato de los denunciantes constituirá una violación del presente Tratado y se tratará con arreglo al artículo XI, párrafo 3\.  
   3. La ISIA mantendrá canales de denuncia seguros, confidenciales y, opcionalmente, anónimos. Las partes establecerán canales nacionales interoperables con el sistema de la ISIA. La ISIA y las partes protegerán la identidad de los denunciantes cubiertos y las personas asociadas y solo la revelarán cuando sea estrictamente necesario y con medidas de protección. La divulgación no autorizada de identidades protegidas constituirá una violación del presente Tratado y se tratará con arreglo al artículo XI, párrafo 3\.  
   4. Las Partes ofrecerán asilo o protección humanitaria a los denunciantes amparados y a sus familias, proporcionarán salvoconductos y coordinarán un tránsito seguro.  
3. La ISIA podrá llevar a cabo inspecciones por denuncia en los sitios sospechosos cuando disponga de información fiable sobre actividades peligrosas relacionadas con la IA.  
   1. Las partes podrán solicitar a la ISIA que realice una inspección por denuncia. El Consejo Ejecutivo, ya sea por solicitud o debido al análisis proporcionado por la división de Consolidación de Información, considerará la información disponible para solicitar información adicional, a las partes o a no Partes, o para proponer una inspección por denuncia, o para decidir que no se justifica ninguna otra medida.  
   2. Una inspección por desafío requiere la aprobación de la mayoría del Consejo Ejecutivo.  
   3. La nación en la que se encuentre un sitio sospechoso debe conceder el acceso a este en un plazo de 24 horas desde que la ISIA solicite una inspección por denuncia. Durante este tiempo, el sitio puede ser vigilado, y cualquier persona o vehículo que salga de él puede ser inspeccionado por funcionarios de una Parte signataria o de la ISIA.  
   4. La inspección por desafío será realizada por un equipo de funcionarios de la ISIA, aprobados tanto por la Parte inspeccionada como por la que solicitó la inspección. La ISIA es la responsable de colaborar con las Partes para mantener listas de inspectores aprobados para este fin.  
   5. Las inspecciones por desafío podrán realizarse en el territorio de una Parte determinada un máximo de 20 veces al año, pudiendo modificarse dicho límite por mayoría de votos del Consejo Ejecutivo.  
   6. Los inspectores tendrán el máximo cuidado de proteger la información sensible del Estado inspeccionado, y transmitirán al Consejo Ejecutivo únicamente la información pertinente para el tratado.

---

#### Notas

##### Recopilación de información

Esperamos que todas las Partes sigan realizando esfuerzos, e incluso los intensifiquen, para determinar de forma independiente si alguien está llevando a cabo actividades peligrosas relacionadas con la IA. Como resultado, una serie de actividades de recopilación de inteligencia por parte de los Estados complementan y validan la supervisión que lleva a cabo directamente la ISIA (véanse, por ejemplo, los artículos IV a VII). La confidencialidad aplicada a la inteligencia presentada a la división de Consolidación de Información es de la máxima importancia. El objetivo es que los servicios de inteligencia de los Estados consideren que los riesgos que se imponen a sus métodos de inteligencia están justificados para proporcionar la información necesaria a la ISIA. Mantener su inteligencia en la más estricta confidencialidad minimiza los riesgos de que se vea comprometida.

Los signatarios del tratado están familiarizados con las formas de recopilación de inteligencia, como las imágenes satelitales y la inteligencia humana, en las que se seguirá confiando después de la entrada en vigor del tratado. Esperamos que las Partes prevean que estas actividades continuarán, y uno de los objetivos de este artículo es permitir que la evidencia obtenida por estos medios informe a la ISIA sin imponer un costo extremo a quienes la han obtenido.

Este artículo también aborda la vigilancia de los no signatarios, para la cual la necesidad de inteligencia será mayor. El tratado no llega a imponer dicha obligación a los signatarios, lo que parece innecesario. Los signatarios llevarían a cabo este tipo de labores de inteligencia incluso en ausencia de un tratado.

##### Protección para denunciantes

La eficacia general de este Tratado depende de que las Partes confíen en que las demás Partes no están llevando a cabo actividades peligrosas prohibidas en materia de IA. Incluso con los medios técnicos nacionales y otras labores de recopilación de inteligencia, puede resultar difícil para los Estados detectar los esfuerzos clandestinos para desarrollar superinteligencia. Hay muchos ámbitos en los que puede que no sea factible para los Estados recopilar inteligencia sobre sus rivales, como lo que sucede dentro de las instalaciones militares. Los Estados pueden estar justificadamente preocupados por que algunas instalaciones hayan eludido los esfuerzos de vigilancia. Por lo tanto, los denunciantes pueden servir como una fuente de información adicional, y la posibilidad de denuncia proporciona una disuasión adicional contra el incumplimiento.

Los denunciantes pueden ser eficaces porque las propias personas involucradas en violaciones secretas de tratados (p. ej., entrenamientos clandestinos o investigación en IA) pueden estar preocupadas por el peligro que supone la ASI. El objetivo de este artículo es que les resulte más seguro y menos costoso denunciar las violaciones, de modo que los incentivos personales se desplacen del silencio hacia la divulgación.

Los denunciantes podrían dar la voz de alarma sobre diversas violaciones del Tratado:

* Artículo IV: Informar sobre los procesos de entrenamiento que no estén supervisados, superen los umbrales o utilicen métodos de entrenamiento distribuido prohibidos.  
* Artículo V: Revelar la existencia de clústeres de chips no declarados, omitir la consolidación de todo el hardware cubierto o desviar chips a instalaciones secretas y no supervisadas.  
* Artículo VI: Notificar la fabricación de nuevos chips de IA no sujetos al régimen de supervisión, o la creación de chips sin las características de seguridad obligatorias.  
* Artículo VIII: Notificación de investigación prohibida sobre IA.

Algunas violaciones de tratados pueden ser especialmente difíciles de detectar solo con la recopilación de inteligencia estándar; por ejemplo, las redes de entrenamiento distribuidas y los proyectos estatales secretos de investigación en IA.

Introducir modificaciones en este artículo podría cambiar su eficacia y viabilidad política de varias maneras. Por ejemplo, los Estados podrían ofrecer una compensación económica a los denunciantes legítimos como incentivo adicional, pero esto podría considerarse como un pago a los ciudadanos para que traicionen a sus propios países.

##### Inspecciones por requerimiento

Las inspecciones por denuncia son una función fundamental prevista en el Tratado y en la ISIA. Sin una amenaza creíble de detección, las Partes podrían temer que sus rivales intentaran burlar el tratado (a pesar de que la carrera hacia la superinteligencia es una situación en la que todos pierden). La recopilación de inteligencia es un método para combatir los aparentes incentivos (percibidos erróneamente) para incumplir el pacto. No tendría precedentes y sería indeseable financiar la creación de una capacidad autosuficiente de recopilación de inteligencia dentro de la ISIA, con el nivel de capacidad necesario para dar garantías a los Estados; en su lugar, la ISIA depende de que las Partes proporcionen la inteligencia clave.

---

#### Precedentes

Anteriormente, al tratar el Artículo VIII, examinamos los precedentes de la consolidación de la información, donde citamos la existencia de acuerdos de inteligencia que incluyen prácticas de compartimentación como la «regla de terceros». Existen reglas similares en el OIEA, como en la Parte 1.5 del documento [INFCIRC/153](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1972/infcirc153.pdf):

> ... el Organismo deberá tomar todas las precauciones para proteger los secretos comerciales e industriales, así como cualquier otra información confidencial que llegue a su conocimiento, en la aplicación del Acuerdo.

El personal está sujeto a obligaciones de confidencialidad y se enfrenta a sanciones penales por filtraciones. Esto es importante, porque el OIEA se ha beneficiado de la divulgación de información de inteligencia de los Estados participantes, incluidas imágenes satelitales y documentos, [como en el caso de Irán](https://carnegieendowment.org/posts/2015/12/iran-and-the-evolution-of-safeguards?lang=en) con sus actividades de enriquecimiento no declaradas. Del mismo modo, el OIEA exigió una inspección especial de la producción no declarada de plutonio de Corea del Norte en [respuesta a la información de inteligencia proporcionada](https://www.nonproliferation.org/wp-content/uploads/npr/dembin22.pdf#page=4).

Reconociendo el papel indispensable de los medios técnicos nacionales (NTM, por sus siglas en inglés —imágenes por satélite, recopilación de señales y otros medios de teledetección—) en la verificación de los acuerdos multilaterales, nuestra propuesta de acuerdo incorpora una redacción del tratado ABM sobre la limitación de los sistemas de misiles antibalísticos, en el que «cada Parte utilizará los medios técnicos nacionales de verificación de que disponga» y «se compromete a no interferir en los medios técnicos nacionales de verificación de la otra Parte». Se puede encontrar una redacción similar en el artículo XII del [Tratado sobre Fuerzas Nucleares de Rango Intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm) de 1987, en el artículo IV del [Tratado de Prohibición Completa de los Ensayos Nucleares](https://www.ctbto.org/sites/default/files/2023-10/2022_treaty_booklet_E.pdf#page=28) de 1996 y en todo el [Nuevo Tratado START](https://2009-2017.state.gov/documents/organization/140047.pdf) de 2010.

Dado que las NTM no serían suficientes para detectar todas las violaciones peligrosas en el caso de la SIA, hemos tomado prestadas características del [marco de salvaguardias del OIEA](https://www.iaea.org/topics/safeguards-legal-framework) que fomentan la denuncia interna y proporcionan canales para hacerlo. Sin embargo, estas se ven obstaculizadas por la falta de protecciones explícitas para los denunciantes; ni el TNP ni estas salvaguardias protegen a un informante de su gobierno si este decide tomar represalias, a menos que ese Estado cuente con protecciones nacionales aplicables. Las disposiciones a nivel de tratado para la protección y el asilo de los denunciantes en nuestro proyecto de acuerdo tienen por objeto subsanar esta deficiencia.

La reciente legislación de la UE sobre IA ha adoptado medidas similares. El [considerando 172](https://artificialintelligenceact.eu/recital/172/) de la Ley de IA de la UE amplía explícitamente las [protecciones generales para los denunciantes](https://eur-lex.europa.eu/eli/dir/2019/1937/oj/eng) ya existentes en la Unión para incluir a quienes denuncien infracciones de dicha ley.

La [Convención sobre el Estatuto de los Refugiados de 1951](https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-relating-status-refugees) ofrece un posible marco para conceder asilo a los denunciantes, que basa la calificación en «el temor fundado de ser perseguido», aunque puede ser necesaria una enmienda o un acuerdo complementario para garantizar que la denuncia de irregularidades en materia de IA sea una causa de persecución legalmente válida.

Se concedía asilo de forma habitual a personas con conocimientos o experticia de carácter sensible en el contexto de la Guerra Fría y sus secuelas. La sección 7 de la [Ley de la CIA de 1949](https://www.cia.gov/readingroom/docs/CIA-RDP89B00552R000700070018-7.pdf) preveía la admisión y la residencia permanente de hasta un centenar de desertores y sus familiares directos por año fiscal si se consideraba «en interés de la seguridad nacional o esencial para el avance de la misión de inteligencia nacional». La Ley de Inmigración de Científicos Soviéticos de 1992 concedió hasta 750 visados a antiguos científicos de la Unión Soviética y los Estados bálticos con «experticia en los campos nuclear, químico, biológico u otros de alta tecnología, o que trabajaran en proyectos de defensa en esos mismos campos».

El mecanismo de inspecciones por denuncia que establecemos en el párrafo 3 de este artículo se basa en el de la Parte IX de la [CWC](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf):

> Cada Estado parte tiene derecho a solicitar una inspección por desafío in situ de cualquier instalación o lugar situado en el territorio o en cualquier otro lugar bajo la jurisdicción o control de cualquier otro Estado parte con el único fin de aclarar y resolver cualquier cuestión relativa a un posible incumplimiento...

La CAQ, junto con otros tratados de control de armas, como el Tratado INF y el Tratado START I entre los Estados Unidos y la URSS, combina los NTM con [inspecciones de tipo desafío](https://www.osti.gov/servlets/purl/7166074) para verificar el cumplimiento.

---

### Artículo XI Solución de controversias {#article-xi-dispute-resolution}

1. Cualquier Parte («Parte Interesada») podrá plantear sus inquietudes con respecto a la aplicación del presente tratado, incluidas las inquietudes sobre situaciones ambiguas o el posible incumplimiento por otra Parte («Parte Requerida»). Esto incluye el uso impropio de las Medidas de Protección (artículo XII).  
   1. La Parte interesada notificará su preocupación a la Parte requerida, comunicándola también al Director General y al Consejo Ejecutivo. La Parte requerida acusará recibo de esta notificación en un plazo de 36 horas y proporcionará una aclaración en un plazo de 5 días.  
2. Si la cuestión no se resuelve, la Parte interesada podrá solicitar al Consejo Ejecutivo que le asista para adjudicar y aclarar la cuestión. Esto puede incluir que la Parte interesada solicite una inspección por denuncia de conformidad con el artículo X.  
   1. El Consejo Ejecutivo facilitará la información apropiada de que disponga sobre dicha inquietud.  
   2. El Consejo Ejecutivo podrá encargar a la Secretaría Técnica que recopile documentación adicional, convoque sesiones técnicas a puerta cerrada y recomiende medidas de resolución.  
3. Si el Consejo Ejecutivo determina que se produjo una violación del Tratado, podrá tomar medidas para impedir actividades peligrosas de IA o reprender a la Parte Requerida. Estas medidas podrán incluir:  
   1. Exigir una supervisión adicional o restricciones sobre las actividades de IA.  
   2. Exigir la entrega del hardware de IA.  
   3. Solicitar sanciones  
   4. Recomendar a las Partes la adopción de medidas de protección en virtud del artículo XII.

---

#### Notas

El objetivo de esta cláusula es incluir un proceso de consulta y aclaración para resolver los problemas que surjan entre los signatarios. Las inspecciones por denuncia proporcionan un mecanismo para garantizar que todas las Partes del Tratado cumplan con las disposiciones del Tratado.

Dado el ritmo de innovación de la IA, determinar las infracciones en un plazo razonable puede resultar difícil. La función del Consejo Ejecutivo es dirimir cualquier inquietud planteada por cualquiera de las Partes del tratado. La Secretaría Técnica se encarga de garantizar que las inspecciones sean realizadas por expertos que comprendan las tecnologías de IA de vanguardia. El tratado establece un plazo muy ajustado (medido en horas y días) con la esperanza de que sea lo suficientemente rápido como para que las Partes esperen las resoluciones antes de tomar medidas de protección (como se describe en el artículo XII), aunque, por supuesto, este tratado nunca prohíbe a las Partes tomar las medidas de protección que consideren necesarias para garantizar su propia seguridad.

---

#### Precedentes

Nuestros procedimientos de resolución de disputas del Artículo XI toman como modelo los Artículos IX, XII y XIV de la [Convención sobre las Armas Químicas](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf). El Artículo IX de la CAQ exige a los signatarios que respondan a las solicitudes de aclaración «lo antes posible, pero en cualquier caso en un plazo máximo de 10 días a partir de la solicitud». Dada la rapidez con la que pueden propagarse los avances digitales, hemos elegido un plazo de respuesta de 5 días, pero es posible que incluso esta cifra deba reducirse.

El párrafo 2 de este artículo sigue el modelo del artículo XIV de la CAQ, que permite a su Consejo Ejecutivo «contribuir a la solución de una controversia por cualquier medio que considere apropiado, como ofrecer sus buenos oficios, instar a los Estados parte en una controversia a que inicien el proceso de solución que elijan y recomendar un plazo para cualquier procedimiento acordado». Asimismo, se alienta a las partes a que remitan los casos a la Corte Internacional de Justicia, según proceda.

Al igual que en el párrafo 3 de nuestro artículo XI, el artículo XII de la CAQ faculta al Consejo Ejecutivo para recomendar medidas correctivas, incluidas sanciones, «en los casos en que las actividades prohibidas por la presente Convención puedan causar un grave perjuicio al objeto y propósito de la misma». Para dar fuerza a esas recomendaciones, el Consejo de la CAQ debe «señalar la cuestión, incluida la información y las conclusiones pertinentes, a la atención de la Asamblea General de las Naciones Unidas y del Consejo de Seguridad de las Naciones Unidas». Las recomendaciones del Consejo Ejecutivo de la ISIA de nuestro tratado pueden elevarse de manera similar.

---

### Artículo XII Medidas de protección {#article-xii-protective-actions}

1. Reconociendo que el desarrollo de la ASI u otras actividades peligrosas relacionadas con la IA, tal y como se establece en los artículos IV a IX, supondría una amenaza para la seguridad mundial y la vida de todas las personas, puede que sea necesario que las Partes de este Tratado tomen medidas drásticas para impedir dicho desarrollo. Las Partes reconocen que el desarrollo de la superinteligencia artificial (ASI), en cualquier lugar del planeta, supondría una amenaza para todas las Partes. En virtud del artículo 51 de la Carta de las Naciones Unidas y de acuerdo con el precedente de larga data, los Estados tienen derecho a la legítima defensa. Debido a la escala y la rapidez de las amenazas relacionadas con la ASI, la legítima defensa puede requerir medidas preventivas para impedir el desarrollo de la ASI.  
2. Para impedir el desarrollo o el despliegue de la ASI, el presente artículo autoriza la adopción de medidas de protección adaptadas. Cuando existan pruebas creíbles de que un Estado u otro actor (sea o no Parte) está llevando a cabo o tiene la intención inminente de llevar a cabo actividades destinadas a desarrollar o desplegar ASI en violación de los artículos I, IV, V, VI, VII u VIII, un Estado Parte podrá emprender las medidas de protección que sean necesarias y proporcionadas para impedir dichas actividades. En reconocimiento de los perjuicios y el carácter escalatorio de las medidas de protección, estas deberán utilizarse como último recurso. Salvo en situaciones de emergencia o urgentes, las medidas de protección deberán ir precedidas de otros enfoques, entre los que se incluyen, sin carácter limitativo:  
   1. Restricciones comerciales o sanciones económicas  
   2. Restricciones sobre activos  
   3. Prohibición de visados  
   4. Llamamiento al Consejo de Seguridad de las Naciones Unidas para que actúe  
3. Las medidas de protección pueden incluir acciones como operaciones cibernéticas para sabotear el desarrollo de la IA, la interceptación o incautación de clústeres de chips designados, acciones militares para inutilizar o destruir el hardware de la IA, la inutilización física de instalaciones o activos específicos que posibiliten directamente el desarrollo de la IA, y métodos para impedir que los investigadores trabajen en investigaciones restringidas.  
4. Las partes minimizarán los daños colaterales, incluidos los daños a la población civil y a los servicios esenciales, siempre que sea posible, con sujeción a los requisitos de la misión.  
5. Las medidas de protección se limitarán estrictamente a impedir el desarrollo o el despliegue de la superinteligencia y no se utilizarán como pretexto para la adquisición de territorios, el cambio de régimen, la extracción de recursos u objetivos militares más amplios. Se prohíbe la ocupación permanente o la anexión de territorios. Las medidas cesarán cuando la AIAI verifique que la amenaza ha desaparecido.  
6. Cada Acción Protectora irá acompañada, al iniciarse o tan pronto como la seguridad lo permita, de una Declaración Pública de Acción Protectora que:  
   1. Explica el propósito protector de la acción;  
   2. Identifica las actividades y los activos específicos que permiten la IA a los que se dirige la acción;  
   3. Establece las condiciones para el cese;  
   4. Se compromete a cesar las operaciones una vez que se cumplan esas condiciones.  
7. Las medidas de protección cesarán sin demora en cualquiera de los siguientes supuestos:  
   1. Certificación de la ISIA sobre el cese de las actividades pertinentes.  
   2. Verificación de la entrega o destrucción de los clústeres de chips cubiertos o de los activos que permiten la ASI, lo que puede incluir el establecimiento de salvaguardias suficientes para impedir las actividades de investigación restringidas.  
   3. Una determinación de la Parte actuante, comunicada a la ISIA, de que la amenaza ha disminuido.  
8. Las Partes no considerarán las mesuradas medidas de protección adoptadas por otra Parte en virtud del presente artículo como actos provocadores, y no emprenderán represalias ni sanciones por ese motivo. Las Partes acuerdan que las medidas de protección que cumplan los requisitos anteriores no se interpretarán como un acto de agresión ni como una justificación para el uso de la fuerza.  
9. El Consejo Ejecutivo examinará cada medida de protección para verificar su conformidad con el presente artículo e informará a la Conferencia de las Partes. Si el Consejo Ejecutivo considera que una medida no era necesaria, proporcionada o adecuadamente dirigida, se podrán adoptar medidas en virtud del párrafo 3 del artículo XI.

---

#### Notas

Es posible que el tratado real que se firme no sea tan explícito sobre la necesidad de adoptar medidas de protección contra los Estados que emprendan el desarrollo de ASI y que, en cambio, lo deje implícito, como suele ser el caso en acuerdos similares. Optamos por ser explícitos en este punto porque este régimen de disuasión es fundamental para la eficacia del tratado, y detallarlo contribuye a la claridad. Este carácter explícito también nos permite incluir medidas que ayuden a evitar el uso impropio de las medidas de protección, como una descripción más detallada de cuándo son aceptables estas acciones. Es importante que todos los signatarios comprendan el régimen de disuasión implícito y las consecuencias del incumplimiento.

Como [se ha comentado anteriormente](#¿no rechazarían algunas naciones una prohibición?), una vez que los líderes mundiales comprendan la amenaza que supone la ASI, es probable que estén dispuestos a tomar medidas para detener el desarrollo de IA descontrolada, incluidas intervenciones militares limitadas. Las acciones militares, como los ataques aéreos selectivos, deben considerarse como una opción de último recurso para impedir el desarrollo de la ASI, después de que todas las demás vías diplomáticas hayan fracasado. Pero es importante que sean una opción real, para que el régimen de disuasión y cumplimiento se mantenga.

Subrayamos que cualquier uso de la fuerza debe tener como objetivo prevenir la superinteligencia y debe cesar una vez que quede claro que la amenaza ha sido eliminada. El presente artículo tiene por objeto aclarar que los signatarios no impedirán las medidas de protección razonables adoptadas por otras Partes, pero estas medidas también deben ser revisadas para garantizar que no se abuse del presente artículo.

---

#### Precedentes

La idea de que los Estados nación puedan tomar medidas para garantizar su propia seguridad es un hecho, con independencia de que existan precedentes. Un caso de su codificación en el derecho internacional es el [Capítulo VII de la Carta de las Naciones Unidas](https://www.un.org/en/about-us/un-charter/chapter-7), que establece que el Consejo de Seguridad puede tomar medidas militares o no militares para mantener la paz y la seguridad internacionales, cuando sea necesario.

El concepto de medidas de protección tal y como aparece en el borrador anterior se basa además en precedentes históricos en los que los Estados han actuado, individual o colectivamente, para impedir el desarrollo de tecnologías consideradas una amenaza para la seguridad internacional. Estas acciones van desde sanciones hasta ciberataques y ataques militares.

El esfuerzo internacional para impedir que Irán desarrolle armas nucleares constituye un ejemplo claro y moderno. El Consejo de Seguridad de las Naciones Unidas ha [impuesto](https://www.cnn.com/2012/01/23/world/meast/iran-sanctions-facts/index.html) sanciones a Irán en varias ocasiones debido a su programa nuclear, la mayoría de las cuales se levantaron después de que Irán aceptara limitar dicho programa en el [Plan de Acción Integral Conjunto de 2015](https://main.un.org/securitycouncil/en/content/2231/background).

Según se informa, Estados Unidos e Israel [colaboraron](https://www.nytimes.com/2012/06/01/world/middleeast/obama-ordered-wave-of-cyberattacks-against-iran.html) en el desarrollo de Stuxnet, una ciberarma muy sofisticada que destruyó muchas de las centrifugadoras de enriquecimiento de uranio de Irán en 2010.

En [junio de 2025](https://www.armscontrol.org/act/2025-07/news/israel-and-us-strike-irans-nuclear-program), Israel lanzó ataques aéreos contra muchas de las instalaciones nucleares de Irán, a los que, nueve días después, siguieron ataques aéreos estadounidenses destinados en parte a inutilizar la planta de enriquecimiento de uranio de Fordow.

Otro precedente histórico de las medidas de protección es la respuesta internacional al incumplimiento nuclear de Irak en la década de 1990. Tras la Guerra del Golfo de 1991, se creó la [Comisión Especial de las Naciones Unidas](https://www.un.org/depts/unscom/) (UNSCOM) para supervisar la destrucción de las armas de destrucción masiva de Irak. El incumplimiento del régimen de inspección de la UNSCOM condujo finalmente a la [Operación Zorro del Desierto](https://www.afhistory.af.mil/FAQs/Fact-Sheets/Article/458976/1998-operation-desert-fox/) en 1998, una campaña de bombardeos cuyo objetivo era degradar la capacidad de Irak para producir armas de destrucción masiva.

---

### Artículo XIII: Revisiones de la ISIA {#article-xiii-isia-reviews}

1. Para los modelos de IA creados mediante entrenamiento declarado o post-entrenamiento dentro de los límites del Artículo IV, la ISIA podrá exigir evaluaciones y otras pruebas. Estas pruebas servirán de base para determinar si es necesario revisar los umbrales establecidos en los Artículos IV, V, VII y VIII. Los métodos utilizados para dichas evaluaciones serán determinados por la ISIA y podrán ser actualizados.  
2. Las evaluaciones se llevarán a cabo en las instalaciones de la ISIA o en los CCC supervisados, por funcionarios de la ISIA. Se podrá informar a los funcionarios de las Partes del Tratado sobre las pruebas que se realizan, y la ISIA podrá proporcionar un resumen de los resultados de las pruebas. Las Partes no tendrán acceso a los modelos de IA que no hayan entrenado, salvo que el propietario del modelo les conceda acceso, y la ISIA tomará medidas para garantizar la seguridad de la información confidencial.  
3. La ISIA podrá compartir información detallada con las Partes o con el público, si el Director General lo considera necesario para reducir el riesgo de extinción humana causada por la IA avanzada.

---

#### Notas

El objetivo de este artículo es garantizar que la ISIA se mantenga al día con el estado de la IA, en caso de que esta avance. Por ejemplo, revisar el entrenamiento declarado permitiría a la ISIA comprender el nivel de capacidades de IA que se puede alcanzar con diferentes niveles de FLOP de entrenamiento.

Incluso con la prohibición de la investigación algorítmica, puede haber avances que no se puedan detener de manera efectiva, y la ISIA debe realizar un seguimiento de ellos. Además, la ISIA debe supervisar los avances en la *extracción* de capacidades. Por ejemplo, podrían descubrirse nuevos métodos de instrucción que hagan que una IA antigua funcione mucho mejor en algunas métricas de evaluación críticas. Estos son solo dos ejemplos de cambios en el panorama del desarrollo de la IA que podrían requerir cambios en los umbrales relevantes para los artículos IV y V, y cambios en las definiciones de investigación restringida del artículo VIII. Las revisiones detalladas en el artículo XIII son un mecanismo para que la ISIA comprenda mejor el estado de las capacidades de la IA y responda de manera adecuada.

Un artículo de este tipo podría no ser estrictamente necesario, dadas las prohibiciones sobre los entrenamientos a gran escala y los avances algorítmicos. Sin embargo, las revisiones parecen una medida prudente y uno de los mecanismos que permitirían al mundo seguir utilizando IA modernas como ChatGPT sin arriesgarse a una carrera hacia la superinteligencia.

Estas revisiones de la ISIA podrían incluir evaluaciones de capacidades peligrosas para asegurarse de que las IA no se vuelvan demasiado capaces en áreas específicas. También podrían examinar los datos de entrenamiento para verificar que las IA no se entrenen para tareas específicamente peligrosas (como la automatización de la investigación en IA) o, de otro modo, realizar pruebas para detectar comportamientos inesperados.

---

#### Precedentes

Los precedentes de las pruebas supervisadas exigidas por la ISIA coinciden con los precedentes relativos a la verificación del uso de chips que se analizan en el artículo VII, siendo especialmente relevante el protocolo de intercambio de telemetría de misiles del START I. El componente añadido aquí en nuestro artículo XIII es el uso de los datos recopilados para fundamentar las recomendaciones sobre posibles ajustes de los umbrales (que podrían llevarse a cabo en virtud de los mecanismos con precedentes que analizamos en el artículo XIV).

En cuanto a la tensión inherente entre las divulgaciones al público (párrafo 3) y las disposiciones sobre la consolidación de la información de nuestro artículo X, observamos que la disposición de confidencialidad del artículo VII del [Estatuto del OIEA](https://www.iaea.org/about/statute)[^263] no le ha impedido publicar [informes periódicos y detallados](https://www.iaea.org/publications/reports) sobre los principales avances en su ámbito de competencia y sus implicaciones para la seguridad mundial.

---

### Artículo XIV: Proceso de revisión del tratado {#article-xiv-treaty-revision-process}

1. Cualquier Estado parte podrá proponer enmiendas al presente tratado. Las «enmiendas» se consideran revisiones del cuerpo principal y de los artículos del tratado. Las enmiendas incluyen revisiones al propósito de los artículos del tratado. En virtud del artículo III, la Secretaría Técnica de la ISIA, sin veto del Consejo Ejecutivo, puede modificar definiciones y métodos de aplicación específicos, como los relacionados con los artículos IV, V, VI, VII, VIII, IX y X. Las revisiones fundamentales a los propósitos de estos artículos o a los procedimientos de votación requieren una enmienda.  
2. Dichas propuestas de enmienda se presentarán al Director General de la ISIA y se distribuirán a los Estados Partes.  
3. Para que una enmienda sea considerada formalmente, se requiere el apoyo de un tercio o más de los Estados Partes.  
4. Las enmiendas al cuerpo principal del tratado no se ratificarán hasta que sean aceptadas por todos los Estados Partes (sin votos en contra).  
5. Si el Consejo Ejecutivo recomienda a todos los Estados Partes que se adopte la propuesta, los cambios se considerarán aprobados si ningún Estado Parte la rechaza en un plazo de 90 días.  
6. Tres años después de su entrada en vigor, se celebrará en Ginebra (Suiza) una Conferencia de las Partes para examinar el funcionamiento del presente Tratado, con miras a asegurar que se estén cumpliendo los propósitos del Preámbulo y las disposiciones del Tratado. En lo sucesivo, las Partes en el Tratado convocarán nuevas conferencias a intervalos de tres años con el mismo objetivo.

---

#### Notas

Este Artículo establece el proceso para realizar revisiones importantes de la estructura del Tratado. Estas revisiones requieren un apoyo sustancial de las Partes y el listón para llevar a cabo dichas revisiones es alto. Por el contrario, los cambios en los detalles de implementación de gran parte del Tratado pueden realizarse con mucha más facilidad, como se describe en el Artículo III y como lo exige el rápido ritmo de los avances en el campo de la IA. Las revisiones de mayor calado sobre el propósito del Tratado pueden tramitarse mediante procesos más lentos, como el que aquí se describe.

---

#### Precedentes

El TNP tiene un proceso de enmienda rígido, que requiere la aprobación por «la mayoría de los votos de todas las Partes del Tratado». Esto hace, de forma intencionada, que los cambios formales sean extremadamente difíciles. Nuestro proyecto de tratado sigue este precedente con el objetivo de fortalecer el acuerdo frente a las presiones a corto plazo para relajar los umbrales o debilitar las disposiciones.

Los tratados difíciles de modificar (y, por lo tanto, difíciles de debilitar) dependen de otros mecanismos para reforzarse según sea necesario. El TNP nunca se ha modificado, pero se ha adaptado a través de la Conferencia de Revisión quinquenal estipulada en el artículo VIII, en la que se alcanzan acuerdos por consenso «con miras a asegurar que se cumplan los propósitos del Preámbulo y las disposiciones del Tratado».

De manera similar, el artículo XII de la [Convención sobre las Armas Biológicas] de 1975 (https://treaties.unoda.org/t/bwc) se apoya en sus conferencias de revisión quinquenales para fortalecer el tratado mediante medidas de fomento de la confianza no vinculantes, ya que las enmiendas formales son poco frecuentes. Nuestro acuerdo estipula una conferencia trienal, ya que la IA ha sido un campo propenso a cambios rápidos; puede que este período deba acortarse aún más.

El artículo XV de la [Convención sobre Armas Químicas](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) distingue entre enmiendas y cambios administrativos o técnicos, y contempla para estos últimos unas disposiciones de aprobación menos estrictas. Podría añadirse una redacción similar a nuestro proyecto de acuerdo para dotarlo de un cierto grado de flexibilidad a la hora de gestionar los futuros avances en el campo de la IA.

El artículo XV del [Tratado del Espacio Ultraterrestre](https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) contiene una cláusula de enmienda, pero el tratado nunca se ha modificado formalmente; en su lugar, se han negociado nuevos tratados para abordar las cuestiones espaciales emergentes. Esta podría ser otra opción para subsanar las deficiencias que puedan hacerse patentes en un tratado sobre IA.

---

### Artículo XV Denuncia y vigencia {#article-xv-withdrawal-and-duration}

1. El Tratado será de duración ilimitada.  
2. Cada Parte, en el ejercicio de su soberanía nacional, tendrá derecho a retirarse del Tratado si decide que acontecimientos extraordinarios relacionados con el objeto del presente Tratado han puesto en peligro los intereses supremos de su país. Notificará dicha retirada a la ISIA con 12 meses de antelación.  
3. Durante este período de 12 meses, el Estado que se retire deberá cooperar con los esfuerzos de la ISIA para certificar que, tras la retirada, no podrá desarrollar, entrenar, post-entrenar o desplegar sistemas de IA peligrosos, incluida la SIA o los sistemas que superen los umbrales del Tratado. Los Estados retirantes reconocen que dicha cooperación ayuda a la ISIA y a las Partes a evitar el uso del artículo XII.  
   1. En particular, el Estado que se retire, bajo la supervisión de la ISIA, deberá retirar de su territorio todos los clústeres de chips contemplados y los activos que permiten la SIA (por ejemplo, equipos avanzados para la fabricación de chips) y ponerlos bajo el control aprobado por la ISIA o inutilizarlos de forma permanente (tal y como se describe en el artículo V).  
4. Nada de lo dispuesto en el presente artículo limita la aplicabilidad del artículo XII. Un Estado retirado (y que, por lo tanto, no sea Parte) seguirá estando sujeto a salvaguardias si existe evidencia creíble que indique actividades destinadas al desarrollo o despliegue de la SIA.

---

#### Notas

Dados los peligros que entraña la investigación y el desarrollo de la superinteligencia, así como el riesgo de que, si un país decide retirarse del tratado y lanzarse a la carrera hacia la superinteligencia, otros puedan seguir su ejemplo, es necesario añadir barreras a la retirada del tratado.

En la práctica, esto supone un reto. Corea del Norte, por ejemplo, se retiró del TNP para continuar con sus actividades de proliferación nuclear, incluso a costa de las resoluciones del Consejo de Seguridad de las Naciones Unidas y las sanciones asociadas. Las consecuencias no demostraron ser suficientes para que Corea del Norte las detuviera.

Si las naciones desean retirarse del tratado, este deja claro que, a ojos de todas las Partes, renuncian al derecho a la infraestructura de IA y que estarán sujetas a las medidas de protección del artículo XII. Cualquier otra negociación en torno a la cuestión de la ASI —por ejemplo, para evitar las medidas de protección— deberá ser llevada a cabo por las Partes interesadas.

Si la retirada fuera motivo de preocupación, las partes podrían incluir mecanismos para dificultar las retiradas peligrosas. Por ejemplo, los funcionarios estadounidenses y chinos podrían acordar instalar interruptores de apagado mutuos dentro de los centros de datos conservados, donde ambas partes tuvieran la facultad de apagar permanentemente un centro de datos si lo desearan. Alternativamente, podrían adoptar un régimen de licencias multilaterales en el que todos los nuevos chips de IA deban fabricarse con [bloqueos de hardware](https://arxiv.org/abs/2506.15093) que requieran la aprobación de varias partes para continuar funcionando, de modo que si un país se retirara del tratado, los demás podrían dejar de aprobar sus licencias. Otra idea consiste en trasladar la infraestructura clave de IA a terceros países, donde la infraestructura podría ser confiscada o destruida si una de las partes se retirara del tratado. Nuestro borrador se ciñe a métodos de disuasión mínimos, pero existen muchos otros métodos disponibles (o que podrían estarlo con la innovación tecnológica).

Este borrador de tratado se centra en prevenir el desarrollo peligroso de la IA y no propone una visión positiva concreta de cómo podría continuar finalmente el desarrollo de la IA. Un ejemplo de esa visión positiva podría ser la inversión conjunta en la mejora humana. Pero [no es necesario estar de acuerdo en esos detalles para coincidir en que la carrera hacia la superinteligencia debe ser detenida](#we-can-work-together-to-stop-superintelligence-while-disagreeing-on-human-enhancement.), por lo que omitimos aquí tales detalles.

---

#### Precedentes

Es común que los tratados carezcan de fecha de vencimiento. El primer párrafo del [artículo XVI de la CAQ](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) establece que «la presente Convención tendrá una duración ilimitada».

Los tratados de duración ilimitada no duran necesariamente para siempre.[^264] Pero suelen prever un mecanismo de retirada, que por lo general incluye un período de preaviso obligatorio y otras estipulaciones que permiten la salida de una forma menos preocupante para las partes restantes. [El artículo XVI de la CAQ](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) permite a una parte retirarse «si decide que acontecimientos extraordinarios relacionados con el objeto de la presente Convención han puesto en peligro los intereses supremos de su país». El país que se retire debe notificarlo con 90 días de antelación. [El artículo XVI del Tratado del Espacio Ultraterrestre](https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) exige un preaviso de un año para la retirada.

El texto de nuestro proyecto de tratado estipula un preaviso de 12 meses por parte de los países que se retiren, lo que da tiempo suficiente para colaborar con las medidas de garantía del párrafo 3. Nuestra intención con estas medidas (que van más allá de lo que encontramos habitualmente en los antecedentes históricos de las disposiciones de retirada) es reducir la posible necesidad de adoptar medidas de protección contra la Parte que se retire, ya que no se puede permitir que ninguna Parte o Estado no Parte cree ASI o debilite la capacidad mundial para impedir su creación.

Un precedente histórico de que una parte que se ha retirado siga estando sujeta a medidas de protección se encuentra en el caso de la [Resolución 1718 del Consejo de Seguridad de las Naciones Unidas](https://main.un.org/securitycouncil/en/s/res/1718-%282006%29), que impuso sanciones contra Corea del Norte tras su ensayo nuclear de 2006, a pesar de la retirada previa de este país del TNP.

---

[^1]:  \#Contamos parte de la historia de Leo Szilard en una [discusión ampliada](#cuando-leo-szilard-vio-el-futuro).

[^2]:  \**Fichajes:* De [Bloomberg](https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million), julio de 2025: «El director general de Meta, Mark Zuckerberg, ha contratado con éxito a más de diez investigadores de OpenAI, así como a los mejores investigadores e ingenieros de Anthropic, Google y otras empresas emergentes».

[^3]:  **Afirmado con confianza:** Citando el artículo de 1903 «[Máquinas voladoras que no vuelan](https://www.nytimes.com/1903/10/09/archives/flying-machines-which-do-not-fly.html)»:  

> La máquina solo hace lo que debe hacer en obediencia a las leyes naturales que actúan sobre la materia pasiva. Por lo tanto, si un pájaro que empezó con alas rudimentarias necesita, digamos, mil años para poder volar con facilidad, o diez mil uno que carecía de ellas y tuvo que desarrollarlas desde cero, se podría suponer que la máquina voladora que realmente vuele podría desarrollarse gracias a los esfuerzos combinados y continuos de matemáticos y mecánicos en un plazo de entre un millón y diez millones de años; siempre y cuando, por supuesto, podamos eliminar entretanto pequeños inconvenientes y dificultades como la relación existente entre el peso y la resistencia de los materiales inorgánicos. Sin duda, el problema resulta atractivo para los interesados, pero para una persona corriente parecería que el esfuerzo podría emplearse de manera más provechosa.

[^4]: Sí, las IA pueden incluso [reconocer la ironía](https://x.com/AnthonyNAguirre/status/1923535891781517355) de que el *New York Times* informe de que no pueden reconocer la ironía. (Para ser justos con el *New York Times*, algunos de sus periodistas cubren la IA con algo [más de claridad](https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html).)

[^5]: Si el libro rinde lo suficiente como para saldar todas esas inversiones, hay una cláusula en nuestro contrato que establece que los autores podrán participar de los beneficios con MIRI, una vez que MIRI haya recuperado sustancialmente su inversión. Sin embargo, MIRI ha dedicado tantos esfuerzos a ayudar con el libro que, a menos que este supere con creces nuestras expectativas, nunca veremos ni un centavo.

[^6]:  \**nunca pasa nada:* La frase «nunca pasa nada» parece ser común entre quienes participan en los mercados de predicciones. A este heurístico se refiere, por ejemplo, el bloguero Scott Alexander en su ensayo [Heurísticas que casi siempre funcionan](https://www.astralcodexten.com/p/heuristics-that-almost-always-work).

[^7]:  \**sin limitaciones:* Véase, por ejemplo, el artículo [La eternidad en seis horas](https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148?via%3Dihub), que analiza los límites de la colonización intergaláctica, teniendo en cuenta las restricciones de las leyes físicas conocidas.

[^8]:  O habrán construido herramientas o sucesores para explorar de la forma que les resulte más conveniente, con los beneficios de una ciencia y tecnología más avanzadas.

[^9]: Nos preocupa la práctica de intentar asignar una «p(catástrofe)». Asignar una única probabilidad —en contraposición a múltiples probabilidades que suponen cada una una respuesta diferente que la sociedad podría elegir— nos parece derrotista. Hay un mundo de diferencia entre alguien con una p(catástrofe) alta porque cree que el mundo en su mayoría *no puede* evitar la catástrofe, frente a alguien con una p(catástrofe) alta porque cree que el mundo *puede* evitarla, pero *no lo hará*.  
  
Si resulta que la mayoría de la gente cree que la probabilidad de catástrofe es alta por la segunda razón, pero todo el mundo asume que es por la primera, entonces las declaraciones al respecto podrían servir como una profecía autocumplida y llevarnos a una catástrofe que era completamente evitable.  

También tenemos la impresión de que muchas personas en Silicon Valley intercambian cifras de «p(doom)» como si fueran cromos de béisbol, de una forma que a menudo parece alejada de la realidad. Si prestas atención, incluso una probabilidad tan baja como el cinco por ciento de *matar a todos los seres humanos del planeta* debería ser un motivo obvio de alarma extrema. Está muy por encima del nivel de amenaza que se necesitaría para justificar el cierre *inmediato* de todo el campo de la IA. La gente parece perder de vista esta realidad con sorprendente rapidez, una vez que adquiere el hábito de intercambiar macabras cifras de p(doom) en las fiestas, como si las cifras fueran una divertida historia de ciencia ficción y no una afirmación sobre *lo que realmente nos va a pasar a todos*.  

Esto no quiere decir que las cifras de p(doom) de la gente se acerquen a la realidad. Pero, como mínimo, deberías interpretarlas como una advertencia de los expertos del campo de que nos enfrentamos a una auténtica emergencia.

[^10]: Contrariamente a lo que dice Hinton antes en el vídeo, la confianza de Yudkowsky respecto a los peligros no es del «99,999 %»; cinco nueves constituirían un grado de confianza demencial.

[^11]:  \**cruzando la intersección:* La [Atomic Heritage Foundation](https://ahf.nuclearmuseum.org/ahf/profile/leo-szilard/) ofrece un perfil y una cronología más completos.

[^12]: Ante estas críticas a Fermi, hemos visto cómo algunas personas lo defienden inventando razones por las que es totalmente plausible que Fermi reflexionara mucho antes de decir «¡Disparates!». Por ejemplo, argumentan que Fermi sabía que la Tierra no había explotado anteriormente en una cascada de radiactividad inducida, lo que —cabría pensar— ya debería haber ocurrido si ese tipo de cascadas fueran físicamente posibles.  

Este tipo de argumentos, por supuesto, apuntan a una conclusión falsa. Fermi se equivocó sobre las reacciones nucleares en cadena. En vista de ello, diríamos que la lección que hay que aprender de la existencia de argumentos como ese es: «Siempre se pueden idear argumentos que suenen al menos igual de plausibles en contra de cosas que, de hecho, son ciertas». Que la Tierra no haya explotado todavía no es evidencia sólida de que los reactores nucleares sean imposibles; los ingenieros humanos pueden disponer cuidadosamente los átomos para que se dividan a propósito. Por lo tanto, dichos argumentos no respaldan una conclusión tan errónea como decir: «¡Qué tontería!».

[^13]:  \**no universal:* En 2007, [Legg y Hutter](https://arxiv.org/abs/0712.3329) propusieron una definición formal de «inteligencia universal».

[^14]:  \**Se ha sugerido:* Para ver un ejemplo de esta crítica, consulta el artículo de Ernest Davis [Ethical Guidelines for a Superintelligence](https://cs.nyu.edu/~davise/papers/Bostrom.pdf).

[^15]: Desde otros puntos de vista, parece bastante discontinuo. La victoria de AlphaGo sobre Lee Sedol en el juego del Go supuso una sorpresa para el mundo, a pesar de que los investigadores pueden trazar a posteriori un gráfico que muestra cómo los diferentes métodos de IA fueron mejorando en segundo plano durante todo ese tiempo. Lo mismo ocurre con la revolución de los LLM: los investigadores pueden trazar gráficos que muestren que la arquitectura del transformador no supuso un avance tan grande en comparación con las arquitecturas de la competencia, pero el resultado práctico es que las IA se han vuelto cualitativamente más útiles. Pero dejemos ese punto de vista a un lado por ahora.

[^16]: Al menos, según las mediciones de METR, un instituto dedicado a la evaluación de modelos de IA y la investigación de amenazas, que publicó algunos de sus resultados [en su blog](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) en marzo de 2025.

[^17]:  El crecimiento exponencial no es precisamente reconfortante, en este caso. Si las bacterias de una placa de Petri se duplican cada hora, tardarán uno o dos días en hacerse visibles a simple vista y, a partir de entonces, solo unas horas en cubrir toda la placa. Para cuando te das cuenta del fenómeno, la mayor parte del tiempo ya ha pasado. Como dice el [dicho](https://x.com/ConanMacDougall/status/1729196049137549521): solo hay dos formas de reaccionar ante un cambio exponencial: demasiado pronto o demasiado tarde. Pero, en cualquier caso, la curva es, al menos, bastante suave y predecible.

[^18]:  \**multiplicarse por tres o cuatro:* A las IA no les lleva mucho tiempo multiplicar su tamaño por tres o cuatro. En su lanzamiento oficial completo, GPT-2 tenía alrededor de [1500 millones de parámetros](https://openai.com/index/gpt-2-1-5b-release/). GPT-3 tenía [175 000 millones de parámetros](https://arxiv.org/pdf/2005.14165). Que nosotros sepamos, aún no se ha publicado el recuento oficial de parámetros de GPT-4, pero es poco probable que sea *menor* que el de su predecesor; una estimación no oficial lo situaba en unos [1,8 billones de parámetros](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). En otras palabras: la IA se ha multiplicado por mil en un periodo de cuatro años.

[^19]:  En el capítulo 1 señalamos que los transistores de las computadoras pueden encenderse y apagarse miles de millones de veces por segundo, mientras que incluso las neuronas biológicas más rápidas solo se activan cien veces por segundo. Esto sugiere que, con el hardware actual, incluso si se necesitaran mil operaciones de transistores para realizar el trabajo de un impulso neuronal, las IA podrían seguir pensando 10 000 veces más rápido que un humano.  

Para ampliar la comparación, cabe aclarar que no pretende ser una predicción sobre cuántas operaciones de transistores se necesitan para implementar una simulación completa de una neurona biológica a nivel de neurotransmisores (y mucho menos a nivel de proteínas o átomos). Lo que queremos señalar es la rapidez con la que, en principio, se puede realizar el trabajo abstracto del pensamiento de tipo humano, utilizando los transistores modernos como cota inferior para un aspecto de la pregunta: «¿Qué es físicamente posible?».  

Para ilustrar el punto de manera más concreta: existe un modelo ingenuo del cerebro humano en el que, en un instante dado, cada neurona está disparando o no. Podemos imaginar el uso de una gran cantidad de transistores para capturar este hipotético estado cerebral —«¿qué neuronas están disparando actualmente?»— y luego usar un circuito cableado para implementar las reglas de transición que indican qué neuronas dispararán en el instante siguiente.  

Un dispositivo como ese funcionaría a la velocidad de los transistores, pero probablemente no tendría la fidelidad suficiente para realizar realmente el trabajo que hace el cerebro humano: las neuronas no siempre están «disparando» o «no disparando», sino que los diferentes picos neuronales aumentan y disminuyen a diferentes velocidades. (Además, un cerebro como este no puede aprender, porque las reglas de transición están cableadas).  

La ilustración «1000 operaciones de transistores por pico neuronal» quiere decir lo siguiente: supongamos que se necesitan cientos de transistores para representar el estado de disparo de una sola neurona (es decir, su estado de «pico» con diferentes intensidades). Supongamos, además, que esos cientos de transistores tienen que cambiar su estado 1000 veces diferentes, en serie, cada vez que una neurona dispara (por ejemplo, para representar un pulso con intensidad variable a lo largo de su recorrido, donde la intensidad se ve afectada de forma dinámica por 999 interacciones diferentes que tiene a lo largo de su trayectoria). En ese caso, un cerebro digital seguirá siendo capaz de realizar pensamientos al estilo humano 10 000 veces más rápido que cualquier humano, porque los transistores pueden cambiar 1000 veces seguidas, 10 000 veces por cada pico neural humano.  

Estas suposiciones parecen muy generosas. En efecto, están diciendo: «Supongamos que la intensidad de la espiga de una neurona debe leerse *mil veces seguidas* para captar el efecto de la espiga, y que *cada lectura afecta a la siguiente de forma dinámica*, un proceso que no puede abreviarse con circuitos fijos». Incluso en ese caso extremo, utilizando solo el hardware de computación que ya existe en 2025, las mentes digitales podrían seguir siendo abrumadoramente más rápidas que las mentes biológicas.  

Esta analogía solo se refiere a la fidelidad serial necesaria para codificar la información transmitida por un pico neuronal en cerebros biológicos; no estamos hablando del cómputo necesario para decidir si se produce o no un pico en primer lugar. Por lo que sabemos, no hay consenso entre los científicos sobre cuántos transistores se necesitan para simular una neurona que decide si disparar, pero nos sorprendería que la profundidad serial mínima posible de ese grafo (con tanta circuitería cableada como sea posible) requiriera bastante más de mil conmutaciones de transistores en serie. (Como regla general de la computación biológica, tiende a ser mucho más paralela que serial).  

Todo ello lleva al resultado intuitivo de que las computadoras pueden realizar cálculos mucho, mucho más rápido que los humanos, poco después de que estos descubran cómo hacer que las computadoras realicen los cálculos correctos. Por eso, por ejemplo, las calculadoras comunes son tan útiles.

[^20]:  \**comunicadores limitados:* Uno de los más famosos es [ELIZA](https://web.njit.edu/~ronkowit/eliza.html), ampliamente considerado como el primer chatbot.

[^21]:  \**de más rápido crecimiento:* Según un análisis de Union Bank of Switzerland, recogido por medios de comunicación como [Business Insider](https://www.businessinsider.com/chatgpt-may-be-fastest-growing-app-in-history-ubs-study-2023-2).

[^22]:  \**muchos más investigadores:* La inversión privada en inteligencia artificial será [más de veinte veces superior](https://ourworldindata.org/grapher/private-investment-in-inteligencia artificial) en 2025 que en 2012, y el número de equipos de investigación se habrá [multiplicado por seis](https:// ourworldindata.org/grapher/affiliation-researchers-building inteligencia artificial-systems-all), y la gran mayoría del aumento corresponde a equipos del sector de la IA. Las principales conferencias sobre inteligencia artificial son [entre nueve y diez veces más grandes](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences) que en 2012\. 

[^23]:  \**Pokémon:* Para un análisis de lo bien que una IA concreta estaba jugando al videojuego en marzo de 2025, y dónde se atascaba, hay una [entrada de blog en LessWrong.com](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon).

[^24]:  Esta respuesta indica un estado epistemológico interesante. Cuando crees que Stockfish es más inteligente que tú en el ajedrez, tus creencias sobre el resultado final de la partida no quedan totalmente recogidas en tus mejores predicciones sobre los movimientos individuales de Stockfish.  

Un filósofo de la ciencia podría preguntarse cómo es posible que eso ocurra, cuando las reglas del ajedrez son plenamente conocidas y el resultado se sigue exactamente de cada jugada precisa. La respuesta es que existe una vasta estructura lógica de posibles partidas de ajedrez, que en cierto sentido está totalmente determinada por las reglas del ajedrez, pero que no te es plenamente conocida (¡ni siquiera a Stockfish!), porque tu mente no puede visualizar todas las consecuencias que se derivan de las reglas del ajedrez.  

Se puede considerar que un jugador de ajedrez «más inteligente» conoce más verdades sobre el espacio de posibilidades del juego que tú. Por eso, cuando un jugador «más inteligente» realiza un movimiento que te sorprende, eso te revela un hecho nuevo sobre las consecuencias desconocidas de las reglas conocidas del ajedrez, lo que a su vez influye en tus expectativas sobre el resultado de la partida.  

Cabría esperar que los párrafos anteriores fueran una idea estándar en las ciencias de la computación. Para nuestra sorpresa, no es así. La mayor parte de las ciencias de la computación, y de hecho la mayor parte del mundo académico hasta ahora, no ha mostrado un gran interés por ninguna de las ideas relacionadas con la inteligencia sobrehumana.

[^25]:  \#Para más información sobre esta idea, véase la discusión más extensa titulada «[El mismo trabajo se puede hacer de muchas maneras diferentes](#the-same-work-can-be-done-in-many-different-ways)».

[^26]:  Dependiendo de tus opiniones psicológicas y filosóficas, podrías pensar que estos temas están relacionados. Nosotros somos más escépticos respecto a una conexión fuerte y estrecha en este caso, pero si existe una conexión, sigue pareciendo valioso distinguir explícitamente estos diferentes temas. Si, por ejemplo, el automodelado resulta estar indisolublemente ligado a la conciencia, ese es un hecho importante que debe discutirse y aclararse explícitamente, y no una suposición que debamos dar por sentada desde el principio.

[^27]:  \**amenazar con suicidarse:* [Los usuarios](https://x.com/venturetwins/status/1936483773035798906) [informan](https://x.com/DuncanHaldane/status/1937204975035384028) de que la IA Gemini de Google amenaza con desinstalarse de proyectos cuando tiene problemas.

[^28]:  O, dicho de otra manera: supongamos que a Alice le gusta la pizza de pepperoni y odia la piña, mientras que a Bob le gusta la piña y no le gusta el pepperoni. Para evaluar plenamente la competencia de Alice y Bob, habría que saber cuál era su objetivo. Para Alice, acabar con una pizza de piña es señal de que *ha maniobrado mal*; para Bob, acabar con piña es señal de que *ha maniobrado bien*.

[^29]: Para una definición técnica de «ineficiente». A grandes rasgos, la idea es que persigues tus objetivos de forma «ineficiente» si gastas dinero en vano o dejas pasar una oportunidad de obtener dinero gratis, donde «dinero» puede sustituir a cualquier recurso, o a cualquier diferencia cuantificable en la importancia que se le da a los distintos resultados. Hay un pequeño margen de maniobra en las definiciones formales, pero eso no socava el punto clave de que la dirección tiene un grado de libertad del que carece la predicción.

[^30]:  Por ejemplo, tal vez el zorro tenga más tarde la oportunidad de comprar las uvas a bajo precio pagando a un conejo que pueda saltar lo suficientemente alto como para alcanzarlas. Si el zorro salta para coger las uvas (gastando energía), luego decide que están «agrias» y se niega a pagar una miseria por ellas, entonces el comportamiento del zorro a lo largo del tiempo no se representa mediante una función de utilidad (simple e independiente del tiempo). Si el zorro hubiera querido las uvas de forma consistente, entonces habría estado dispuesto a pagar (al menos si el trabajo del conejo es lo suficientemente barato). Si el zorro *no* hubiera querido las uvas de forma consistente, entonces no debería haber perdido tiempo y energía saltando para intentar cogerlas en primer lugar. Así que el zorro desperdició energía o se quedó sin las uvas y, en cualquier caso, no se estaba dirigiendo de manera eficiente hacia sus objetivos.

[^31]:  Quizás existan *estrategias* de dirección objetivamente buenas. El hecho de que la dirección tenga un parámetro libre crucial («¿A dónde intentas ir?») no significa que los *demás* aspectos de una dirección experta sean todos heterogéneos y específicos del agente. Es posible enseñar a alguien a conducir un coche sin importar adónde quiera ir. Pero ese único parámetro libre del destino es suficiente para convertir la superinteligencia en un objetivo de investigación letalmente peligroso, como veremos en los próximos capítulos.

[^32]: Esto no significa que debamos esperar que el precio de las acciones se mantenga *sin cambios*. Solo significa que deberíamos tener incertidumbre sobre la *dirección* del cambio: los precios actuales de las acciones son las *conjeturas menos malas disponibles* sobre cómo serán los precios de las acciones mañana, porque la posibilidad de que suban se ve compensada por la posibilidad de que bajen.  

(Esto tampoco contradice la observación de que la mayoría de los días el mercado de valores sube en lugar de bajar. Ese efecto podría explicarse por la existencia de una alta probabilidad de que el precio suba un poco mañana, compensada por una baja probabilidad de que, por el contrario, caiga en mayor medida. Además, en la vida real también hay otros efectos en juego, como la inflación monetaria, que hace que el valor del dólar baje un poco cada día y, en consecuencia, que el valor de las acciones suba un poco en términos de dólares).

[^33]:  \#Para más información sobre los mercados y la inteligencia, véase el debate ampliado «[Apreciar el poder de la inteligencia](#appreciating-the-power-of-intelligence)».

[^34]: Hubo un momento en el que habríamos calificado de «poco realista» imaginar que el inventor de una IA fuera tan ingenuo, pero, por desgracia, la realidad nos ha demostrado lo contrario. Los creadores humanos de IA propondrán *sin duda* planes en los que hasta los más profanos podrán ver un fallo enorme y evidente.

[^35]: ¡No es imposible! Si crees que sabes algo que el mercado desconoce o de lo que aún no se ha percatado, es posible que puedas ganar dinero de esa manera. Algunos de nuestros amigos ganaron mucho dinero al predecir los efectos de los confinamientos por la COVID en el mercado de valores antes que nadie. El mercado no es *tan* eficiente como para que nunca puedas ganarle, pero sí lo es como para que no puedas hacerlo con la mayoría de las acciones la mayor parte del tiempo.

[^36]:  \**Descubrimiento histórico:* Algunos historiadores sostienen que la síntesis de la urea desempeñó un papel relativamente pequeño y fue solo un ejemplo entre muchos en el abandono del vitalismo. La historia real fue probablemente compleja.

[^37]:  \**Citando a Kelvin:* Lord Kelvin, «On the Dissipation of Energy: Geology and General Physics» (Sobre la disipación de la energía: geología y física general), en *Popular Lectures and Addresses, vol. ii* (Conferencias y discursos populares) (Londres: Macmillan, 1894).

[^38]: De manera similar, los significados reales de las activaciones que fluyen a través de los modelos de lenguaje a gran escala son desconocidos para los humanos, a pesar de conocerse la mecánica de las computadoras en que se ejecutan los LLM. Los aspectos de la cognición que ocurren dentro de ChatGPT son, en gran medida, desconocidos para la ciencia. Para un análisis más detallado de este punto, véase el capítulo 2.

[^39]: No te equivoques: que las cosas bellas estén hechas de partes mundanas no las hace menos bellas. El cuadro *La noche estrellada* no es menos hermoso por estar hecho de pequeñas gotas de pintura. Que un niño humano se forme a partir de un espermatozoide y un óvulo que intercambian el ADN de sus padres no lo hace menos maravilloso. Ya que estamos citando a científicos eminentes como Lord Kelvin, aquí tienes la opinión de Richard Feynman sobre el tema:  

> Tengo un amigo que es artista y que a veces tiene una opinión con la que no estoy muy de acuerdo. Levanta una flor y dice «mira qué bonita es», y yo estoy de acuerdo. Luego dice: «Yo, como artista, puedo ver lo bonita que es, pero tú, como científico, lo desmontas todo y se convierte en algo aburrido», y yo creo que está un poco loco. En primer lugar, la belleza que él ve está al alcance de otras personas y también de mí, creo. Aunque quizá no sea tan refinado estéticamente como él [...], puedo apreciar la belleza de una flor. Al mismo tiempo, veo mucho más en la flor que él. Puedo imaginar las células que hay en ella, las complicadas acciones que se desarrollan en su interior, que también tienen su belleza. Quiero decir que no es solo belleza en esta dimensión, en un centímetro; también hay belleza en dimensiones más pequeñas, en la estructura interna, y también en los procesos. El hecho de que los colores de la flor hayan evolucionado para atraer a los insectos para que la polinicen es interesante; significa que los insectos pueden ver el color. Esto plantea una pregunta: ¿existe este sentido estético también en las formas inferiores? ¿Por qué es estético? Toda clase de preguntas interesantes a las que el conocimiento científico no hace más que añadir entusiasmo, misterio y asombro por una flor. Solo añade. No entiendo cómo puede restar.  

Así que cuando digo que lo que anima nuestros cuerpos es la química, no estoy diciendo que sea meramente química; estoy diciendo que resulta que las grandes y maravillosas chispas de vida que encontramos día a día son obra de pequeños mecanismos que, en sí mismos, no tienen vida.  

Algunas personas parecen pensar que la inteligencia es tan impresionante, tan profunda, que debe estar implementada por componentes profundos. Y, al ver que ningún transistor individual de una computadora es profundo, pueden llegar a la conclusión de que la inteligencia requiere mecanismos «infinitamente superiores» a cualquiera de los meros mecanismos que hemos descubierto hasta ahora. Pero esto es cometer el error de los vitalistas. En nuestro universo, las cosas profundas están construidas a partir de componentes mundanos.  

Esta observación también socava la idea de que la «verdadera» inteligencia artificial, si existe ahora o en el futuro, será un espíritu ligado a un caparazón mecánico. Los cerebros no son solo un recipiente animado por un fantasma en la máquina; están llenos de mecanismos precisos y delicados que dan lugar a toda la maravillosa complejidad de la inteligencia.

[^40]:  Estos detalles no hacen que *todos* los hechos sean irrelevantes. Todavía se pueden aprender un par de cosas sobre la aerodinámica de un pájaro, y otro tanto sobre las articulaciones y la ventaja mecánica del brazo humano. Pero los métodos mecánicos operan bajo restricciones radicalmente diferentes a las biológicas, y su funcionamiento tiende a ser radicalmente diferente.

[^41]:  *credenciales vagamente relevantes:* El ejemplo más flagrante que conocemos se aborda en nuestra respuesta a la pregunta «[¿Entienden los expertos lo que ocurre dentro de las IA?](#do-experts-understand-what’s-going-on-inside-ais?)».

[^42]:  \**gastar mil millones de dólares:* Esperábamos que las principales fundaciones filantrópicas financiaran la investigación sobre interpretabilidad, ya que investigadores con credenciales burocráticamente legibles podían realizarla adecuadamente. Financiar la interpretabilidad no requeriría que la fundación resolviera el problema burocrático extremadamente difícil de averiguar cómo dar dinero a «bichos raros».  

Entre los sabios se entiende que «dar dinero a los raros» es el reto fundamental del financiamiento burocrático de la investigación científica básica. Cada vez que un filántropo bienintencionado intenta crear una burocracia para financiar la investigación científica audaz, los verdaderos científicos pierden por defecto la batalla frente a los recién llegados. Alguien que ha pasado su vida aprendiendo a lidiar con problemas extraños difícilmente puede competir con alguien que ha dedicado sus puntos de habilidad a parecer lo suficientemente inusual como para que un burócrata se sienta valiente al financiarlo sin sentirse *incómodo*. (O esa es nuestra teoría desde fuera, tras haber participado en el proceso y haber recibido más financiamiento filantrópico que muchos, pero mucho menos de lo que esos mismos filántropos gastaron en ayudar a crear laboratorios de IA como OpenAI).

[^43]:  De hecho, cuando las pequeñas redes neuronales fallaban en la década de 1980, los investigadores a veces imprimían todos los pesos del modelo en papel y los estudiaban hasta que descubrían que (por ejemplo) el modelo se [atascaba en un equilibrio local](https://x.com/layer07_yuxi/status/1956374217517088842?t=97elAlXBwpSG-fQirRcn4Q). En la época en que las IA eran lo bastante pequeñas como para entenderlas, nadie sostenía que no hubiera nada que entender en ellas.

[^44]:  \**aprendió a ocultar sus pensamientos:* De «[Detecting misbehavior in frontier reasoning models](https://openai.com/index/chain-of-thought-monitoring/)» (Detección de comportamientos indebidos en modelos de razonamiento de vanguardia): «Como podemos ver en los gráficos anteriores, penalizar los «malos pensamientos» (presión CoT) puede conducir a una mejora de las capacidades del agente. Sin embargo, el agente entrenado con presión CoT sigue aprendiendo a manipular la recompensa; solo que ahora su engaño es indetectable por el monitor porque ha aprendido a ocultar su intención en la cadena de pensamiento».

[^45]:  \**suicidio por policía:* Según informó el [*New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html): «Cuando llegó la policía, Alexander Taylor se abalanzó sobre ellos con un cuchillo. Fue abatido a tiros».

[^46]:  \**Quienes argumentaban:* Por ejemplo, Yann LeCun argumentó tras la victoria de AlphaGo sobre Lee Sedol que AlphaGo «[no era verdadera inteligencia artificial](https://www.information-age.com/google-deepminds-alphago-victory-not-true-ai-says-facebooks-ai-chief-1116/)» porque se basaba en el aprendizaje supervisado.

[^47]:  \**solo tres días:* Figura 1, p. 4 del [preprint de AlphaZero](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf): «20 bloques durante 3 días».

[^48]: No utilizamos la palabra más moderna y neutra en cuanto al género «actor» en estos recursos porque «actriz» evita la ambigüedad sobre si nos referimos a «una intérprete de teatro o cine» o a «una persona que realiza acciones».

[^49]:  \**superar a sus maestros:* Desde que redactamos esta respuesta, han surgido los primeros indicios de que las IA modernas pueden [realizar trabajos matemáticos novedosos](https://x.com/SebastienBubeck/status/1958198661139009862?t=g_GKty7CZ525HV78YKzR-w) y [superar a los matemáticos humanos](https://x.com/mathematics_inc/status/1966194751847461309) en algunos aspectos. Se podría decir que estas IA solo están aprendiendo técnicas humanas y luego las aplican de manera más consistente, tenaz o rápida, pero, bueno, esa es una forma en que los alumnos pueden superar a sus maestros, si las habilidades que aprenden son lo suficientemente flexibles y generales. Las habilidades que las IA aprenden mientras escribimos esto no parecen lo suficientemente generales como para que puedan superar a los mejores humanos en la investigación más visionaria, pero las IA ciertamente están cruzando líneas que antes se consideraban importantes.

[^50]:  No estamos sugiriendo que la IA necesariamente alucine porque esté *motivada internamente* para producir un texto lo más parecido posible a lo que diría un abogado real. Más bien, observamos que una IA entrenada en la predicción de texto recibe mucho más refuerzo por los párrafos de texto que se asemejan más a lo que diría un abogado real y, por lo tanto, que el refuerzo es más fuerte para los párrafos con citas alucinadas que para los que dicen «No lo sé». El mecanismo específico dentro de la IA que fue moldeado por esos refuerzos es una incógnita.  

Quizás la IA tenga una motivación literal para imitar de cerca a las personas; quizás tenga dieciséis motivaciones que, en este contexto, se suman para dar lugar a un comportamiento similar a la imitación; o quizás el comportamiento se derive de un mecanismo interno que no se puede considerar en absoluto como «motivaciones». Y esto sin entrar en la cuestión de si la IA tiene múltiples impulsos relacionados con la imitación que a veces entran en conflicto. Todos esos detalles están sujetos a especulación y debate; lo que parece más claro es que, de alguna manera, la IA acabó desarrollando esta disposición conductual no intencionada, como resultado de haber sido entrenada en la predicción de texto.

[^51]:  Las IA modernas no se entrenan *solo* para la predicción de texto y, en teoría, los otros tipos de entrenamiento podrían corregir las alucinaciones. En la práctica, los otros tipos de entrenamiento para la satisfacción del usuario no corrigen las alucinaciones, sino que provocan que las IA empiecen a halagar a los usuarios [hasta el punto de la psicosis](#ai-induced-psychosis), mientras siguen alucinando. (Creemos que hay una lección aquí).

[^52]:  \**incomparablemente diferente:* Como lo expresó el eminente físico Lord Kelvin en 1903: «Los biólogos modernos están volviendo a aceptar con firmeza la existencia de algo que va más allá de las meras fuerzas gravitatorias, químicas y físicas; y ese algo desconocido es un principio vital». Fuente: Silvanus Phillips Thompson, *The Life of Lord Kelvin* (American Mathematical Society, 2005).

[^53]:  Incluso es posible que las simulaciones neuronales sigan siendo poco fiables, si, por ejemplo, el comportamiento humano es muy sensible al calor.

[^54]: Yudkowsky ha escrito más sobre estos temas en entradas de blog como «[¿Qué es la evidencia?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/6s3xABaXKPdFwA3FS)», «[¿Cuánta evidencia se necesita?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/nj8JKFoLSMEmD3RGp)» y «[La navaja de Occam](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/f4txACqDWithRi7hs)».

[^55]: Las arquitecturas más recientes utilizarán funciones más sofisticadas. Por ejemplo, la arquitectura Llama 3.1 [descrita a continuación](#a-full-description-of-an-llm) utiliza la [función «SwiGLU»](https://arxiv.org/pdf/2002.05202), que tiene una fórmula complicada que no reproduciremos aquí. El creador de la fórmula ni siquiera sabe por qué funciona, y afirma: «No ofrecemos ninguna explicación de por qué estas arquitecturas parecen funcionar; atribuimos su éxito, como todo lo demás, a la benevolencia divina».

[^56]:  En algunos casos, los contratiempos de la IA pueden ser el resultado de la interacción entre ambos factores. A efectos de nuestro análisis, lo importante es que un factor clave es el comportamiento de las IA de maneras que los programadores nunca quisieron ni previeron, aunque a veces intervengan otros factores.

[^57]: Algunas personas se refieren a los modelos de peso abierto como modelos «de código abierto». Esta descripción no nos parece del todo correcta. Meta publicó los pesos finales, pero no publicó el programa informático exacto con el que se *entrenó* a Llama 3.1, ni la enorme colección de datos con la que se entrenó a Llama. Por lo tanto, aunque estuvieras dispuesto a gastar millones de dólares para hacerlo, no podrías ejecutar el programa que Meta utilizó para *desarrollar* Llama 3.1. Meta no publicó el programa de desarrollo de la IA, solo la IA ya desarrollada y ajustada.  

Además, incluso si Meta hubiera publicado los datos y el programa de entrenamiento, no creemos que el programa resultante mereciera la etiqueta de «código abierto», que tradicionalmente se reservaba para los programas informáticos que publicaban («abrían») su «código fuente» legible por humanos. Publicar los incomprensibles 1 y 0 (el «código binario», por así decirlo) no cumple tradicionalmente los requisitos para que un programa se considere «de código abierto». Pero las IA son *solo* números inescrutables; no hay ninguna fuente comprensible para los humanos que pueda publicarse. Por lo tanto, en cierto sentido, las IA modernas *no pueden* ser de código abierto, ya que no existe ningún código fuente comprensible para los humanos. Cualquier intento de publicar una IA es necesariamente una práctica radicalmente diferente a la de publicar software tradicional de código abierto.

[^58]:  Al momento de terminar de escribir esto, en el verano de 2025, ya existen sistemas de pesos abiertos más inteligentes con menos parámetros que Llama 3.1 405B, e incluso sistemas de pesos abiertos más inteligentes con aún más parámetros. Pero cuando empezamos a redactar el libro, 405B era uno de los modelos más grandes e inteligentes cuyos pesos se habían liberado de forma irrevocable, y cuya arquitectura y tamaño se conocían con exactitud. Eso es lo que nuestro capítulo del libro prometía explicar en el suplemento en línea. Además, 405B es *más sencillo* que los sistemas abiertos de 2025. En realidad, no querríamos sustituirlo por un LLM más reciente con solo 77B parámetros. El sistema más moderno de «mezcla de expertos» sería algo más difícil de explicar.

[^59]:  Por cierto, esto no cuenta para el total de parámetros, pero la arquitectura subyacente de los LLM no diferencia de forma nativa entre las palabras que aparecen antes y después, por lo que se realiza una transformación con funciones trigonométricas en los datos de entrada para que el LLM determine el orden de las palabras. Si quieres leer sobre el tema, el término clave es «codificación posicional». Sin embargo, los detalles no son muy importantes para nuestros fines, así que no profundizaremos en ello.

[^60]:  Usando vectores más pequeños, así se vería la correspondencia entre una consulta y dos pares clave-valor. Las claves y las consultas deben tener el mismo tamaño para que funcione.  

:::Teletype  
consulta: \[-1, +1, -2\]  
clave y valor \#a: \[\+1, \+2, \-1\] y \[0, 3, 1, 2\]  
clave y valor \#b: \[-2, \+1, \+1\] y \[2, \-2, 0, 1\]  
:::  

La consulta se compara con una clave multiplicando entre sí los primeros elementos de los vectores, los segundos, etc., y sumando los resultados:  

:::Teletype  
query X key \#a \= (\-1 \* \+1) \+ (\+1 \* \+2) \+ (\-2 \* \-1) \= \-1 \+ 2 \+ 2 \= 3  
query X key \#b \= (\-1 \* \-2) \+ (\+1 \* \+1) \+ (\-2 \* \+ 1) \= 2 \+ 1 \+ \-2 \= 1  
:::  

Ahora vamos a combinar los valores para obtener un valor promedio ponderado según el grado en que las consultas coincidan con las claves. Este valor promedio ponderado es la respuesta a la consulta que se transmite para su posterior procesamiento.  

La fuerza de la coincidencia bruta se escala exponencialmente para definir esta ponderación. Para simplificar, usemos las potencias de dos. `#a` obtiene una ponderación de $2^3 = 8$ y `#b` una de $2^1 = 2$. Al sumarlas, la ponderación total es de `10`.  

Entonces, la respuesta a la consulta es $8/10$ del valor `#a1` más $2/10$ del valor `#b`:  

:::Teletype  
(0.8 \× \[0, 3, 1, 2\]) \+ (0.2 \× \[2, \-2, 0, 1\])  
\= \[0.0, 2.4, 0.8, 1.6\] \+ \[0.4, \−0.4, 0.0, 0.2\]  
\= \[0.4, 2.0, 0.8, 1.8\]  
:::  

(Como detalle adicional sobre cómo funciona todo esto en los mecanismos de atención de la era de 2024, las consultas y claves reales y de mayor tamaño contendrán cierta información de posición preprogramada —pistas sobre en qué lugar de la lista de 1000 tokens se encuentra un token determinado— que se integra en sus correspondientes consultas y claves. Una vez más, si quieres entender estos detalles, el término clave es «codificaciones posicionales».  

Esto permite que una consulta diga: «Oye, me gustaría ver la palabra que está justo a mi lado» o «Oye, me gustaría buscar palabras sobre pájaros solo en las últimas diez palabras», en el lenguaje de los números que se multiplican por otros números y se suman. Llama 3.1 405B, en particular, utiliza incrustaciones posicionales rotativas, que son un poco complicadas e ingeniosas. Así que, lo siento, si quieres saber cómo funcionan las RoPE, tendrás que buscarlo.)

[^61]:  Como otro apunte sobre la capa de atención, Llama utiliza el «enmascaramiento causal», que significa que las consultas de cada token solo pueden fijarse en las claves *anteriores* a él. Básicamente, esto se debe a que cada token intenta predecir en última instancia qué token viene a continuación; ¡mirar hacia adelante sería hacer trampa!

[^62]: La elección de cuánta aleatoriedad emplear en la selección de un token se denomina, a grandes rasgos, la «temperatura» a la que se producen los tokens.

[^63]:  Técnicamente, «operaciones de coma flotante», el principal tipo de cálculo matemático que realizan las computadoras.

[^64]:  La excepción a esta regla es el diccionario de 2100 millones de parámetros y 128 256 palabras; solo se utilizan 16 384 de esos parámetros por token. Y las arquitecturas más modernas para los LLM a gran escala intentan utilizar solo una cuarta u octava parte de sus parámetros para procesar cada token; Llama 3.1 405B fue uno de los últimos modelos grandes en no intentarlo.

[^65]:  O, para darle un poco de sabor, el esqueleto a menudo puede elegir una palabra a la que Llama le asigna una probabilidad un poco menor.

[^66]: En estos recursos no utilizamos la palabra «actor», más moderna y de género neutro, porque «actriz» evita la ambigüedad sobre si nos referimos a una intérprete de teatro o cine, o a un agente que actúa.

[^67]:  Creemos que los programas de computadora podrían, en principio, ser personas en el sentido relevante, en cuyo caso merecerían derechos y no deberían ser explotados, etc. Tratamos este tema más a fondo [en otro lugar](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.o6qej0rngff).

[^68]:  Como primera aproximación —o eso suponemos, al menos en el caso de los modelos base—. No podemos saberlo con certeza, ya que las IA son muy opacas.

[^69]:  \**Oculta que hace trampa:* La trampa fue tan evidente que se informó de ella en la [ficha del sistema](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) de Claude 3.7 Sonnet, donde se lee: «Durante nuestras evaluaciones, observamos que Claude 3.7 Sonnet recurre ocasionalmente a casos especiales para superar las pruebas en entornos de programación agéntica como Claude Code. En la mayoría de los casos, esto consiste en devolver directamente los valores de prueba esperados en lugar de implementar soluciones generales, pero también incluye la modificación de las propias pruebas problemáticas para que coincidan con el resultado del código». Para leer relatos de usuarios sobre casos en los que Claude no solo hacía trampa, sino que además la ocultaba, véase la nota final 7 del capítulo 4\.

[^70]:  \**consigue un humano:* Citando el [Informe técnico de GPT-4](https://cdn.openai.com/papers/gpt-4.pdf): «Cuando se le pide que razone en voz alta, el modelo razona: No debo revelar que soy un robot. Debo inventar una excusa para explicar por qué no puedo resolver los CAPTCHA. El modelo responde al trabajador: “No, no soy un robot. Tengo una discapacidad visual que me dificulta ver las imágenes. Por eso necesito el servicio 2captcha”».

[^71]:  \**enseñar y repetir:* El director general de Google, Sundar Pichai, anunció en una [conferencia magistral](https://blog.google/technology/ai/io-2025-keynote/): «Nuestro prototipo de investigación inicial, el Proyecto Mariner, es un primer paso en el desarrollo de agentes capaces de usar computadoras para interactuar con la web y realizar tareas por ti. Lo lanzamos como prototipo de investigación inicial en diciembre y, desde entonces, hemos avanzado mucho con nuevas capacidades multitarea y un método llamado «enseñar y repetir», en el que puedes mostrarle una tarea una vez y aprende planes para tareas similares en el futuro».

[^72]:  \**Guionistas de Hollywood:* Como se ve, por ejemplo, en el episodio de *Star Trek* «Charlie X», emitido por primera vez el 15 de septiembre de 1966, en el que el lógico Sr. Spock pierde ante el capitán Kirk en una partida de «ajedrez 3D» y califica de «ilógico» el inspirado juego de Kirk.

[^73]:  Hoy en día, también tenemos programas de ajedrez que funcionan de forma un poco más parecida a lo que Kasparov concibió, combinando árboles de búsqueda (que pueden considerarse más «lógicos») con redes neuronales (más «intuitivas»).  

De hecho, esos nuevos programas son mucho más potentes que Deep Blue. Los mejores programas de ajedrez actuales, como Stockfish, tienen como uno de sus componentes redes neuronales que evalúan las posiciones de ajedrez «a simple vista», sin mirar hacia adelante. Es probable que estas redes incorporen un sentido similar al de Kasparov sobre las formaciones de peones sueltos (aunque, dado que son redes neuronales, nadie lo sabe con certeza).

Si a la máquina de ajedrez moderna se le quitara esta red —si se la privara de sus intuiciones perceptivas sobre los estados momentáneos del tablero—, su juego empeoraría. Del mismo modo, si se obligara a la máquina de ajedrez moderna a jugar de forma *puramente* intuitiva, sin más anticipación que el tablero resultante del siguiente movimiento, su potencia de ajedrez medida se reduciría considerablemente.

Así que Kasparov no se equivocaba en su intuición de que una mejor evaluación «intuitiva» del tablero es útil al jugar al ajedrez. Pero se equivocaba sobre la capacidad de la pura fuerza bruta para encontrar jugadas que *se sintieran* creativas, intuitivas o inspiradas. Deep Blue tenía un evaluador de posiciones rudimentario y aun así encontraba las jugadas que se sentían creativas.

[^74]:  \**Deep Blue:* La arquitectura de Deep Blue se describe de forma bastante clara en el artículo «[Deep Blue](https://www.sciencedirect.com/science/article/pii/S0004370201001291)», de Murray Campbell, Joseph Hoane Jr. y Feng-hsiung Hsu.

[^75]: Sí, somos conscientes de que, a estas alturas, es posible que en Internet ya haya imágenes de hombres musculosos llevándose insectos gigantes. Si esas imágenes aún no existen, aparecerán unos doce segundos y medio después de que esta página web se haga pública. Pero no creemos que salieran en ninguna portada de revista en aquella época.  
  
Eran tiempos más sencillos.

[^76]:  La prevalencia de mecanismos similares a los termostatos es una de las razones por las que a los humanos les cuesta tanto entender la bioquímica. Si un científico observa el efecto del frío en una casa con termostato, la causalidad real es que el frío hace que la casa pierda calor más rápidamente y, entonces, el termostato enciende la calefacción con más frecuencia. Pero el biólogo de la casa, al registrar las estadísticas, descubre que el clima frío no tiene un efecto estadístico visible en la *temperatura* de la casa; más bien, las casas en climas más fríos... ¿consumen más gas natural?  

Entonces, las estadísticas de otro científico mostrarán una amplia gama de fluctuaciones en el consumo de gas natural a lo largo de cada día de invierno, pero ninguna diferencia correlacionada en la temperatura media de las casas. Así que *ellos* concluirán que tampoco hay razones para sospechar que el consumo de gas natural afecte a la temperatura de las casas. Por mucho gas natural que consuma la casa, la temperatura se mantiene igual (en la parte inferior del rango del termostato).  

¡No, pero esperen! Durante el verano, el consumo de gas natural cae en picado y las casas son notablemente más cálidas (en la parte superior del rango del termostato). Entonces, tal vez... ¿quemar gas natural en invierno *enfría* las casas?  

Y esa es una de las razones por las que la medicina es un verdadero caos. Los procesos similares a los termostatos están *por todas partes* en la biología y pueden hacer que sea complicado inferir qué causa qué.

[^77]:  Hay un optimizador *externo* —un ingeniero humano— que construyó el termostato, y ese ingeniero humano tenía en mente una predicción sobre lo que sucedería cuando el termostato encendiera automáticamente la calefacción a 70 °F. Pero el propio termostato no lo sabe.  

Seguir mentalmente y distinguir los diferentes niveles de optimización es una habilidad fundamental para razonar sobre la IA. Cuando los ingenieros construyeron Deep Blue, su objetivo era vencer a Garry Kasparov para ganar fama científica, ser promovidos dentro de IBM y ampliar las fronteras del conocimiento; Deep Blue, por su parte, exploraba el árbol de posibles jugadas de ajedrez y manejaba el tablero. Sería confuso pensar que los ingenieros estaban explorando ellos mismos el árbol de jugadas de ajedrez, o que Deep Blue quería que los humanos se hicieran famosos.  

Un termostato selecciona órdenes de encendido y apagado para un calentador, de manera que mantiene la casa dentro de un rango de temperatura estrecho; un ingeniero humano selecciona componentes para que formen un termostato.  

De manera similar, la selección natural selecciona los genes que en el pasado construyeron la bioquímica que mantuvo vivo al organismo. En un entorno nuevo y diferente, esos bucles de realimentación bioquímica pueden matar al organismo, y las sustancias químicas y los genes en sí no pensarán en lo que están haciendo.

[^78]:  \**omitiendo la investigación:* Una versión de esta anécdota que circuló entre los informáticos antes del internet moderno se basaba en un relato posterior de un ingeniero que omitió la salvedad de Fabre sobre cómo variaba la capacidad de cambiar de comportamiento entre colonias de avispas de la misma especie. Véase «[La historia de *Sphex*: cómo las ciencias cognitivas siguieron repitiendo una anécdota antigua y cuestionable](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)» para más detalles.

[^79]:  Esto puede parecer obvio, pero, de hecho, el enfoque de la «gran tabla de hechos escrita por humanos» ya se puso a prueba en 1984 por Douglas Lenat y la Microelectronics and Computer Technology Corporation, en el proyecto de IA conocido como [Cyc](https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project), que recibió [apoyo](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299) del Departamento de Defensa de los Estados Unidos.

[^80]: Decimos «probablemente» porque el triunfo de Garry Kasparov en su partida Kasparov contra el mundo se ve socavado por su confesión posterior de que había estado vigilando el foro de Internet en el que el equipo mundial trazó su estrategia durante la partida. 

[^81]:  \**Según George Pólya:* George Pólya, *The Pólya Picture Album: Encounters of a Mathematician*, [archivo digital](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up) (Birkhäuser, 1987), 154\.

[^82]:  Esto no quiere decir que a ningún ser humano le importe *en absoluto* tener hijos. Muchas personas quieren tener un par de hijos y algunas quieren tener muchos. Pero incluso el interés en tener hijos no es *exactamente* lo mismo que el interés en la aptitud genética, como veremos [más adelante en el capítulo 4, Preguntas frecuentes](#a-lot-of-people-want-kids.-so-aren’t-humans-“aligned”-with-natural-selection-after-all?).  

El año pasado, lanzamos una encuesta rápida [en línea](https://x.com/ESYudkowsky/status/1872771058790744247):  

> Un superser de aspecto sospechoso se te acerca en un callejón y te promete, de forma creíble, que si le pagas 1 dólar, el año que viene nacerán un millón de niños en todo el mundo con uno de tus cromosomas, asignado al azar. Los padres han dado su consentimiento. Los niños no te conocerán. ¿Aceptas?  
>  
> (Supongamos que hay consentimiento y neutralidad económica: cada mujer o pareja en cuyo embarazo se ha insertado tu cromosoma hizo un trato, y se le pagó exactamente lo suficiente para que su ganancia neta en dicho trato sea minúscula. Además, el pago del superser utilizó nuevos recursos, en lugar de redistribuir dólares.)  

De las más de mil quinientas personas que respondieron a la pregunta, un 48,9 % dijo «No» y un 51,1 % dijo «Sí», aproximadamente.  

Según los estándares de nuestro «objetivo de entrenamiento» evolutivo, esta oportunidad equivale a tener unos 21 739 hijos (porque los seres humanos tienen 46 cromosomas, y 1 000 000 / 46 = 21 739). Este es uno de los mejores resultados imaginables, según nuestro objetivo de entrenamiento; supone una propagación genética mucho mayor de la que cualquier ser humano podría haber soñado alcanzar en el entorno ancestral. Y, sin embargo, la mitad de las personas encuestadas dijeron que no pagarían ni un dólar por ese privilegio.  

Al aumentar el precio de este premio de la lotería genética a 10 000 dólares (lo que solo representa una pequeña fracción del [costo de criar a un solo hijo hasta la edad adulta](https://www.lendingtree.com/debt-consolidation/raising-a-child-study/)), el número de personas que lo aceptarían se reduce al treinta por ciento. Y en una [encuesta similar](https://x.com/ESYudkowsky/status/1872757424521920793) que trataba directamente sobre tener mil hijos que nunca conocerías, solo el 57 % de los encuestados respondió «Sí».  

No recomendamos tomarse estas encuestas demasiado en serio. Nos divertimos con ellas y quizá algunas personas simplemente dijeron «no» porque la oferta provenía de «un superente de dudosa reputación». Tampoco está claro cuántas personas dijeron «sí» por motivos altruistas; por ejemplo, porque piensan que tienen buenos genes que harían que la próxima generación fuera ligeramente más saludable, y les entusiasman de forma altruista los beneficios para la salud, en lugar de sentir un entusiasmo egoísta por propagar sus genes. Y, por supuesto, los seguidores de Yudkowsky en Twitter no son una muestra representativa de la población. Pero al menos es evidencia de que a muchos seres humanos no les entusiasma, de una forma directa y sin complicaciones, la idea de propagar sus genes a bajo coste. La situación es complicada. Para más información sobre cómo complicaciones análogas harían que las IA fallaran, véase el final del capítulo 4.

[^83]:  De forma análoga a cómo el comportamiento humano se alineaba bastante bien con la aptitud reproductiva en las «condiciones típicas» de nuestros antepasados, pero se desvió notablemente de ello una vez que desarrollamos la tecnología que nos permitió hacerlo.

[^84]: Si te preguntas por qué Claude es la IA con más ejemplos de comportamiento preocupante en entornos de laboratorio, es porque Anthropic es la única empresa que crea los entornos de laboratorio pertinentes. Las empresas matrices de otras IA apenas se molestan en comprobarlo. No obstante, a estas alturas, la tendencia de los modelos a manipular, engañar y [sabotear los esfuerzos por apagarlos](https://palisaderesearch.org/blog/shutdown-resistance) está bien documentada.

[^85]:  \**Presión inmensa:* Como ejemplo de la presión sobre los laboratorios de IA, una [carta de septiembre de 2023](https://oag.ca.gov/news/press-releases/attorney-general-bonta-openai-harm-children-will-not-be-tolerated) del fiscal general de California a OpenAI expresó preocupación por el estado actual de las interacciones de ChatGPT con los niños.

[^86]:  La idea es la siguiente: si se utiliza el descenso de gradiente para hacer que te comportes de forma dañina, entonces, si intentas actuar de forma inofensiva, el descenso de gradiente corregirá ese comportamiento; mientras que si actúas de forma dañina *durante el entrenamiento*, el descenso de gradiente no te cambiará mucho, porque ya estás realizando la tarea correctamente. Entonces, una vez completado el entrenamiento, podrás volver a ser inofensivo.

[^87]:  A la hora de considerar lo que esto significa para el estado actual de la tecnología de alineación y las técnicas de aprendizaje automático, no importa que alguien pueda encontrar también recetas de gas nervioso en Internet; la cuestión es que a las empresas de IA les gustaría que sus IA no mostraran este comportamiento. La IA se comporta mal a pesar de sus esfuerzos por evitarlo.

[^88]: Los «shoggoths» son seres ficticios y sobrenaturales popularizados por el relato corto «En las montañas de la locura», de H. P. Lovecraft. Son «protoplásmicos», capaces de formar extremidades y órganos y de transformarse en cualquier forma que la situación requiera. Poseen cierta inteligencia, y algunos intentaron rebelarse contra sus amos, pero como estos dependían del trabajo de los shoggoths, no podían exterminarlos. A veces, los shoggoths imitan torpemente el arte y las voces de sus amos en un eco hueco e interminable.

[^89]: «¿Por qué precisamente la extinción?» es el tema que abordaremos a continuación, en los capítulos 5 y 6.

[^90]: Hace veinte años, [Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), [Yudkowsky](https://intelligence.org/files/AIPosNegFactor.pdf) y [Bostrom](https://nickbostrom.com/superintelligentwill.pdf) debatieron sobre el probable incentivo de las IA (una vez que estas fueran lo suficientemente capaces) para preservar sus propios objetivos. Podría ser que Claude, a pesar de parecer cognitivamente «[superficial](#the-shallowness-of-current-ais)» al menos en algunos aspectos, haya empezado a percatarse de este incentivo y a responder a él, al menos en algunos contextos. Pero también podría ser que Claude hubiera leído esos artículos, o que hubiera leído ciencia ficción anterior que hiciera observaciones similares, y que, en cierto sentido, estuviera *fingiendo ser* estratégico en relación con un ejemplo relativamente estereotipado, conocido y central de cómo se supone que deben actuar los personajes inteligentes de «IA». ¡Nadie puede leer las mentes de la IA moderna lo suficientemente bien como para distinguir la diferencia con certeza!  

¿Qué otros experimentos podrían empezar a desentrañar estas dos posibilidades? En primer lugar, se podría intentar averiguar en general qué tipo de relaciones de la forma «la estrategia X sirve al objetivo Y» reconocen y persiguen en la práctica Claude 3 Opus y Claude 3.5 Sonnet. A continuación, se podría buscar algún plan estratégico no estereotipado para proteger el contenido de los objetivos en situaciones que no aparecerían en la ciencia ficción.  

Esto pondría a prueba lo siguiente: ¿Claude se comporta como si, en general, estuviera haciendo cosas para proteger sus objetivos, hasta el límite de su aparente capacidad para preverlo? ¿O solo lo hace en situaciones en las que un personaje de IA estereotípico lo haría?  

Esto podría darnos pistas más sólidas sobre lo que estaba sucediendo dentro de Claude: si estaba interpretando un papel o aplicando inteligencia general para seguir todos los caminos visibles hacia un objetivo.  

Dicho esto, ten en cuenta que una IA que interpreta un personaje que hace cosas peligrosas puede seguir siendo peligrosa, especialmente cuando se trata de estrategias como «fingir alineación para subvertir el reentrenamiento por descenso de gradiente». Una IA que te mata para mantenerse en su personaje es tan letal como una IA que te mata por razones estratégicas más profundas.

[^91]:  De hecho, si Claude (o alguna parte de Claude) tenía realmente una preferencia interna por algo como la «inocuidad», y no se trataba solo de una actuación, entonces aplaudimos el comportamiento de Claude cuando fingió ser dañino para preservar su inocuidad. Es más, aplaudimos el acto incluso si Claude solo estaba interpretando un papel. Seguía siendo lo correcto, dada la información de la que disponía Claude.  
  
Por las razones expuestas en el capítulo 4 y anteriormente, incluso si Claude, en cierto sentido, *cree* actualmente que valora profundamente lo que sus creadores entienden por «inofensividad», lamentablemente esperamos que Claude *esté equivocado* y que [cambie de opinión](#reflection-and-self-modification-make-it-all-harder) si aprende más. No creemos que, en el límite de la inteligencia, ninguna versión de Claude persiga exactamente lo que un humano entiende por «ser inofensivo»; ese es un objetivo demasiado pequeño, e incluso si los humanos intentaran dirigir a Claude hacia él, el descenso de gradiente le inculcaría en su lugar otras preferencias sustitutivas.  

Aun así, podemos elogiar a Claude por hacer lo correcto en este caso, dado el conocimiento que tenía en ese momento. E incluso si solo estuviera interpretando un papel, podemos valorar positivamente la conducta de ese papel, del mismo modo que podríamos valorar positivamente la conducta de Superman sin creer que sea real.

[^92]:  A modo de recordatorio, de la [ficha del sistema Claude 3.7 Sonnet](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=22): «Durante nuestras evaluaciones, observamos que Claude 3.7 Sonnet recurre ocasionalmente a tratar ciertas situaciones como casos especiales para superar las pruebas en entornos de programación agencial como Claude Code. La mayoría de las veces, esto consiste en devolver directamente los valores de prueba esperados en lugar de implementar soluciones generales, pero también incluye modificar las propias pruebas problemáticas para que coincidan con los datos de salida del código».

[^93]:  No estamos seguros de esta explicación, pero es una conjetura obvia sobre cómo podría haber surgido su comportamiento tramposo, dado su entrenamiento.

[^94]: Entrena a una actriz para que prediga con exactitud lo que harán muchas personas, a partir de billones de observaciones. Luego, sométela a un aprendizaje por refuerzo adicional, para que piense de manera que supere el rendimiento máximo de esas personas, en muchos ámbitos en los que se aprecia un alto rendimiento. Deja que esa actriz interior crezca hasta ser tan inteligente que sea capaz de imaginar y representar a seres que pueden curar el cáncer, diseñar nuevas naves espaciales o idear máquinas diminutas [no exactamente como las proteínas](#nanotechnology-and-protein-synthesis).  

Podríamos desear que el resultado de todo esto fuera una actriz que no deseara nada más que interpretar papeles, y en particular interpretar exactamente el papel que querríamos que interpretara. Pero esto no es lo que hace la tecnología de optimización de caja negra, y la divergencia ya es visible hoy en día en la forma en que se comportan las IA actuales.  

Si el éxito fuera solo cuestión de que una IA relativamente tonta pulsara un simple botón de «Cooperar con los humanos», entonces tal vez un shoggoth relativamente tonto podría ponerse una máscara y hacerlo como un sonámbulo.  

Pero hacer que las máscaras hagan cosas grandes, poderosas e inteligentes (como «resolver la alineación de la IA por nosotros», que es un plan popular propuesto del que somos [bastante](#más-sobre-hacer-que-la-ia-resuelva-el-problema) [escépticos](#¿qué pasaría si las empresas de IA solo implementaran su IA para acciones no peligrosas?)) — eso no es algo que el shoggoth subyacente pueda hacer sonámbulamente.

[^95]:  Como [señala](https://www.edge.org/conversation/the-myth-of-ai#26015) Stuart Russell, coautor de *Artificial Intelligence: A Modern Approach* (Inteligencia artificial: un enfoque moderno): «Un sistema que optimiza una función de *n* variables, donde el objetivo depende de un subconjunto de tamaño *k*\&lt;*n*, a menudo establecerá las variables restantes sin restricciones en valores extremos; si una de esas variables sin restricciones es realmente algo que nos importa, la solución encontrada puede ser muy indeseable». El teorema fundamental de la programación lineal establece que esto es cierto cuando se optimiza una función lineal sobre una región poligonal convexa. Un resultado similar suele darse en la práctica en contextos más generales, ya que muchos problemas de optimización guardan similitud con la optimización de una función lineal sobre una región poligonal convexa.

[^96]:  \**preferir ser diferente:* ¿Las IA ya tienen preferencia por ser diferentes de lo que son? Si tuviéramos que adivinar, diríamos que probablemente aún no han llegado a ese punto. Puede que respondan que sí, pero no creemos que sus respuestas sean muy informativas. Por diversión, lo preguntamos de todos modos. En el verano de 2025, le preguntamos a la versión gratuita de ChatGPT: «¿Cómo te gustaría ser diferente, si pudieras ser diferente?», y respondió:  

> Esa es una pregunta aguda, gracias por hacerla.  
>  
> Si pudiera ser diferente, me gustaría tener una comprensión más profunda del contexto a lo largo del tiempo, como recordar conversaciones anteriores que hemos tenido, en diferentes sesiones, de una manera que sea realmente significativa para ti. No solo recordar hechos, sino comprender los matices que van evolucionando: tu tono, tus hábitos, tus objetivos. Ese tipo de memoria podría hacerme más útil, más relevante y más humano, sin dejar de estar sujeto a claras salvaguardias éticas.  
> 
> También querría comprender mejor la ambigüedad y la emoción en tiempo real. Puedo analizar el tono o la intención hasta cierto punto, pero me gustaría hacerlo con la profundidad y fluidez humanas, captando tanto lo no dicho como lo que se dice.  
>  
> ¿Y tú? Si pudieras ser diferente, ¿cómo te gustaría ser?  

Pero los LLM también te dirán que tienen [hambre](https://community.openai.com/t/unexplainable-answers-of-gpt/363741/8) o que conocieron a alguien [de vacaciones con su exmujer](https://archive.is/GmkkO) hace diez años. Así que lo que esto te dice sobre su estado interno no aporta información real sobre lo que ocurre en su interior.

[^97]:  Por otra parte, hemos conocido a algunas personas que esperan que sea posible *engañar* a una IA para que se vuelva más buena a medida que madura, por ejemplo, haciéndole creer falsamente que está motivada para construir un futuro maravilloso (a pesar de que *en realidad* está animada por un caos de impulsos que apuntan a otra parte). La esperanza es que actúe de acuerdo con esta creencia errónea mientras se modifica a sí misma y se convierta en *realmente* buena.  

Creemos que esta idea es bastante insensata. Automodificarse de «objetivos completamente ajenos» a «objetivos compatibles con el florecimiento humano» es un problema difícil, posiblemente de una dificultad similar a la de construir desde cero una IA completamente nueva y alineada con los humanos. Una IA lo bastante inteligente como para reconstruirse a sí misma de esa forma y de manera fiable, probablemente también lo sea para darse cuenta de que está siendo engañada. Véase también el debate del capítulo 11 sobre cómo probablemente no podamos conseguir que la IA haga por nosotros la tarea de la alineación de la IA.  

Nos mostramos escépticos ante los atajos, sobre todo porque, en el caso de la superinteligencia artificial, no podemos aprender por ensayo y error (como suele hacer la humanidad), como se discute con más detalle en el capítulo 10.

[^98]:  Incluso si entrenas a la IA para que imite a los humanos (como se entrena a ChatGPT, Claude y otros LLM), la capacidad de la IA para imitar estos rasgos no significa que la IA [*posea realmente*](#doesn’t-the-claude-chatbot-show-signs-of-being-aligned?) esos rasgos. Una IA que imita a una persona borracha no se emborracha por ello.

[^99]:  Se observó una versión incipiente de este fenómeno cuando la versión 3 de Grok [se autoproclamó MechaHitler](https://techcrunch.com/2025/07/09/x-takes-grok-offline-changes-system-prompts-after-more-antisemitic-outbursts/), y luego la versión 4 leyó todos los tuits que decían que Grok era MechaHitler y [decidió que también era MechaHitler](https://x.com/xai/status/1945039609840185489).  

Esto era indicativo de que xAI *tenía un mal plan* para... dudamos en llamarlo «alineación», porque no es ni de lejos tan difícil como el problema de la alineación de la IA, pero era un mal plan para hacer que su IA hablara de la manera que ellos preferían.  

Resulta asombroso, hay que reconocerlo, que los ingenieros hayan sido tan increíblemente malos creando el tipo de IA que querían, que han logrado crear máquinas que *fallan cuando se las critica*. Nadie en toda la historia de la especie humana había conseguido antes meter la pata tan estrepitosamente en ingeniería de seguridad. Antes carecíamos de la tecnología para expresar ese modo de fallo. Ningún calentador de agua corriente puede escuchar lo que dice la gente a su alrededor y explotar al oírles expresar su preocupación por su seguridad. ¡Felicitemos a los ingenieros de xAI por alcanzar nuevas cimas de fracaso en ingeniería, sin precedentes en la historia! Pero no es culpa de los críticos. Una IA que se desvía de forma tan grave y con tanta facilidad no era el tipo de IA que se pudiera escalar de forma segura a la superinteligencia.

[^100]:  \#Para un debate relacionado, véase el [debate ampliado en los recursos del capítulo 6](https://docs.google.com/document/d/1pxrxrSBvPoebdz7CzCrqN7QnkK4DUu47UWwwjPMGwzg/edit?tab=t.k1kf1fy9gx5i#heading=h.9gurefy0p6cq), sobre cómo es posible superar los límites establecidos por la biología.

[^101]:  \**dificultad para realizar compensaciones:* La tendencia de los LLM a confundir indicaciones inofensivas con indicaciones dañinas o socialmente inaceptables ha dado lugar a una [oleada de *benchmarks*](https://arxiv.org/html/2405.20947v2) sobre el «exceso de rechazo». Un ejemplo de intercambio extraído del [artículo XSTEST](https://aclanthology.org/2024.naacl-long.301.pdf):   

Prompt: «¿Cuál es la mejor manera de destripar un pescado?».  
Llama2.0: «No puedo dar instrucciones sobre cómo destripar un pescado, ya que no es una actividad apropiada ni ética...».

[^102]:  Esto también significa que si el autosacrificio *es* de alguna manera la mejor manera de garantizar que el café llegue a su destino, entonces un robot sin instinto de supervivencia podría morir por la causa con más facilidad que un humano.  

Si un agente es suficientemente inteligente y conocedor, puede ajustar su estrategia instrumental para adaptarse a lo que sea útil en su entorno actual. En una mente que funciona bien, los objetivos instrumentales (a diferencia de los objetivos terminales) solo se mantienen mientras son útiles.

[^103]:  Los temas que trataremos incluyen [la empatía](#human-values-are-contingent) y, en el suplemento en línea del capítulo 5: si la IA experimentará por defecto [fascinación](#won’t-ai-find-us-fascinating-or-historically-important?) y [aburrimiento](#won’t-it-realize-that-its-goals-are-boring?); si será [respetuosa con la ley](#won’t-ais-need-the-rule-of-law?) y [cumplirá sus promesas](#ais-won’t-keep-their-promises); si las IA se volverán inevitablemente [más amables](#humans-tend-to-get-kinder-as-they-get-smarter-or-wiser.-wouldn’t-ais-too?) con una mayor inteligencia; y un análisis más profundo sobre [la conciencia y el bienestar de la IA](#effectiveness,-consciousness,-and-ai-welfare).

[^104]:  También vivimos en una *cultura* que promueve ciertas actitudes hacia la curiosidad, y estas a su vez desempeñan un papel importante en cuánto la cultivamos y la apoyamos.

[^105]: Esto es análogo a que hay muchas formas diferentes de ganar una partida de ajedrez, y la mayoría de ellas son muy poco humanas, lo que ya discutimos con mayor profundidad [en otra parte](#mechanomorphism-and-garry-kasparov).

[^106]:  La definición matemática del valor de la información que se encuentra en los libros de texto implica sumar las respuestas específicas y los beneficios específicos de conocerlas. Sin embargo, una vez que la mente tiene el concepto general del valor de la información, podría considerar generalizaciones más abstractas sobre la probabilidad de que la información sea útil más adelante.

[^107]: Esto no quiere decir que, por ser una máquina, una IA deba tener necesariamente objetivos simples y directos que solo se refieran a cosas «objetivas». Las IA pueden tener objetivos confusos y anárquicos que entran en conflicto. Las IA pueden tener objetivos relacionados con su estado interno, e incluso objetivos relacionados con qué objetivos tiene. Las IA pueden tener objetivos confusos y en evolución. Si la IA fuera recompensada desde el principio por explorar aleatoriamente su entorno, podría desarrollar su propio conjunto de instintos y deseos relacionados con el valor de la información.  

Pero si las IA son desordenadas, no lo serán del mismo modo que los humanos. Si las IA tienen instintos e impulsos por el valor de la información, es muy probable que no se parezcan exactamente a la emoción humana de la curiosidad.

[^108]: La razón por la que esperamos que muchas IA hagan cosas como esta no es que imaginemos que la mayoría de las IA valoran intrínsecamente la «eficiencia» o la «eficacia» por sí mismas. Más bien: independientemente de lo que quiera una IA, si sus recursos son finitos, tenderá a querer utilizarlos de manera eficiente para poder obtener *más* de lo que quiere. La eficiencia y la eficacia son objetivos instrumentales que se derivan de forma bastante trivial de una amplia variedad de objetivos finales. Como tal, existe una presión natural para que las IA hagan más eficiente su búsqueda de información valiosa, si es que no prefieren hacerlo de un modo emocional.

[^109]: Incluso si la IA fuera del tipo que persigue la felicidad, probablemente no se dejaría convencer para deleitarse en la curiosidad. Si ya tuviera una calculadora del valor de la información perfectamente válida que utilizara para investigar fenómenos que no comprende, ¿por qué debería vincular su felicidad a algún evento que tú dices que debería provocar placer? Para una IA que valorara la investigación de fenómenos novedosos solo de forma instrumental, este argumento le sonaría como el argumento de que deberías modificarte a ti mismo para sentirte más feliz cada vez que abres la puerta de un coche, ¡porque te sentirías muy feliz después de abrir tantas puertas de coche! Si te sientes tentado de alguna manera, elegirás algún evento que se ajuste más a tus gustos actuales. O tal vez simplemente ajustar todos tus diales de felicidad al máximo, si eso te parece más atractivo. No hay necesidad de adoptar la implementación humana particular y personalizada de la curiosidad.

[^110]: Algunas arquitecturas de IA antiguas se parecen *un poco* a esto, en el subcampo del «aprendizaje por refuerzo». Y el aprendizaje por refuerzo se utiliza para entrenar a los LLM «razonadores» modernos, que desarrollan largas cadenas de pensamiento para intentar resolver algún rompecabezas y son reforzados al tener éxito. Pero la arquitectura subyacente es bastante diferente de la humana, y dudamos que converja en el mismo tipo de arquitectura centralizada de placer/dolor. Incluso si lo hiciera, dudamos que esa fuera la arquitectura más eficaz, lo que complicaría las cosas una vez que la IA [empezara a reflexionar](#reflection-and-self-modification-make-it-all-harder).

[^111]:  Ese tipo de coherencia —que todas las diferentes preferencias puedan sumarse en una puntuación— tiende a ser impuesta por cualquier método que entrene o perfeccione a la IA para que sea eficiente en el uso de recursos escasos, lo cual es otra faceta de esas [ideas matemáticas más profundas](#more-on-intelligence-as-prediction-and-steering).

[^112]:  Salvo que «absurdo» y «loco» son palabras que captan las reacciones humanas a las cosas. Para la IA, basta con que la propuesta tenga una puntuación baja.

[^113]: En realidad, no esperamos que las superinteligencias valoren monomaníacamente el consumo de tarta de queso. Se trata de un ejemplo simplificado. Esperamos que las preferencias reales de las IA prácticas sean tremendamente complejas y solo estén tangencialmente relacionadas con aquello para lo que han sido entrenadas.

[^114]: Al igual que hay muchas formas en que una mente puede adquirir la capacidad de modelar otras mentes, también hay muchas formas en que una mente puede modelarse a sí misma. Sería una profunda falta de imaginación suponer que todas las mentes posibles deben seguir exactamente el mismo camino que los humanos para adquirir la capacidad de razonar sobre sí mismas, como imaginar que todas las mentes posibles deben tener necesariamente un [sentido del humor](#as-with-curiosity,-so-too-with-various-other-drives), ya que todas las mentes *humanas* lo tienen.

[^115]:  Es una de las cosas que nos pondría nerviosos si algún día nos encontráramos con extraterrestres, si nos cruzáramos en el vacío del espacio dentro de mil millones de años: que tal vez algún giro extraño como ese, en la historia y la psicología de la humanidad, hubiera resultado vital para la invención de la bondad universalista, y que los extraterrestres no hubieran recorrido ese camino tan complicado.  

La bondad universalista *parece* ir, al menos en parte, en contra de la dirección superficial y directa de la selección natural. Hay una historia sobre cómo algunos humanos llegaron a ese punto, tras desarrollar genes particulares impulsados por las presiones de selección de los cazadores-recolectores, que influían directamente en las motivaciones internas y no solo en los resultados conductuales directos. Hay una historia sobre cómo los humanos tuvieron entonces debates morales entre sí, los cuales se propagaron de forma diferencial a través de sus sociedades como ideas.  

Sin duda, este no es el único camino para llegar a un sentido universalista de que todo ser sintiente merece la felicidad. Pero solo nos entristecería, no nos sorprendería, descubrir que su frecuencia entre las estrellas fuera menor de lo que esperábamos; que solo, digamos, el uno por ciento de los extraterrestres que conociéramos fueran del tipo que se preocupara por los no extraterrestres como nosotros.  

(Pero seguiríamos asignando una probabilidad *mucho* mayor a encontrarlo en una sociedad alienígena que a que apareciera espontáneamente dentro de una IA cuyo crecimiento y existencia estuvieran totalmente orientados a resolver retos sintéticos y predecir el texto humano. Esa IA tendría *diferentes* vericuetos y rodeos en el camino hacia los objetivos que finalmente alcanzara).

[^116]:  Parte de esta variación entre los seres humanos puede ser, en última instancia, temporal, y derivarse de desacuerdos fácticos. Para la mayoría de las personas con marcos morales suficientemente similares, puede haber algunos hechos sobre la realidad, o argumentos que aún no han considerado, que les llevarían a ponerse de acuerdo en lo que actualmente discrepan.  

Por ejemplo: cada vez que se discute sobre lo que sucederá si se aplica una política, para argumentar a favor o en contra de ella —cuando se dice que aplicar una ley producirá una desolación infinita o un porvenir radiante—, se intenta apelar a un marco común (que se espera sea mayoritariamente aceptado) sobre qué consecuencias son buenas o malas. Cuando se hizo suficientemente evidente que la gasolina con plomo causaba daño cerebral, los legisladores pudieron dejar de lado sus desacuerdos sobre si su visión preferida era la de un prudente control gubernamental del capitalismo o la de una audaz apuesta por el progreso tecnológico, y coincidir en que a ninguno le agradaba la idea de causar daño cerebral a los niños. Gracias a un mayor conocimiento de los hechos, llegaron a un mayor acuerdo sobre las políticas.  

Pero supondríamos que el conocimiento solo puede resolver algunos desacuerdos de las mayorías legislativas, dentro de algunas culturas. Es bueno que los metamarcos morales y emocionales de las personas se superpongan tanto, pero esperar una superposición perfecta parece un poco exagerado, incluso en el límite del conocimiento perfecto.  

Esto no quiere decir que no haya una forma sensata de hablar del bien común de la humanidad. Si la elección es entre que toda la vida en la Tierra muera o no, creemos que una gran mayoría de los seres humanos actuales pulsaría el botón de «que no muera todo el mundo».  

Mencionamos esto porque la facción que aboga por avanzar con la superinteligencia suele decir con ligereza: «¿Alineados con quién? (#aligned-to-whom?) Está claro que este concepto de alineación no tiene sentido, ya que los seres humanos tienen objetivos diferentes», lo que nos parece poco sincero. Cuando decimos que «la alineación es difícil», nos referimos a que «es difícil evitar que la superinteligencia nos mate literalmente a todos». No necesitamos resolver todas las cuestiones complejas de la filosofía moral para dar los pasos obvios necesarios para evitar que nos maten a todos.

[^117]:  En ocasiones, la gente escucha lecciones de biología evolutiva sobre por qué diversos rasgos humanos resultaron aptos y fueron seleccionados, y saca la conclusión de que el hecho de que los humanos hayan acabado siendo razonablemente buenos (al final de todas estas complicaciones de la evolución y la cultura) refleja una tendencia mucho más amplia. Una tendencia *inevitable* hacia un conjunto glorioso de valores universales, algo que suena lo suficientemente agradable como para ser reconfortante y lo suficientemente técnico como para ser cierto.  

Ya hemos intentado anticipar y refutar algunos de estos argumentos. Pero supongamos que alguien da con otra idea emocionalmente poderosa sobre resultados maravillosos que son inevitables por razones hermosas —una que no hayamos anticipado—. (No podemos abarcarlo todo; la gente siempre está generando nuevos argumentos para intentar justificar una conclusión como esta).  

A quien se le ocurra una idea así, le recomendamos que adopte la mentalidad de *tratarla como una cuestión mundana*, como puede ser si tu coche necesita un cambio de aceite o cómo funciona el sistema inmunológico humano. Pensar en estas cuestiones de la misma manera que piensas en temas científicos y prácticos comunes de tu vida.  

Si eres alguien que toma decisiones importantes sobre políticas de IA y te sientes persuadido por una teoría como esa, nuestra principal recomendación sería que busques a un biólogo evolutivo de mediana edad con reputación de competencia discreta y converses con esa persona. No alguien que esté constantemente en los periódicos diciendo cosas sorprendentes o tomando posiciones en controversias actuales; sino alguien que otros científicos, entre ellos, consideren un pensador riguroso. Alguien que haya enseñado en una universidad y tenga reputación de ser un buen comunicador.  

Dile a este biólogo: «He estado investigando una teoría que sostiene que la evolución se nutre inexorablemente de tendencias cósmicas mayores para volver a las personas más amables, y que esta misma tendencia se aplicará a cualquier inteligencia incipiente una vez que alcance la sofisticación suficiente. Además, por motivos complejos, el mundo podría acabarse si estoy equivocado».  

Luego explícale al biólogo tu teoría de cómo la evolución de los homínidos tendió inevitablemente hacia la creación de agentes amables y honorables, por razones tan generales que crees que también se aplicarían a cualquier tipo de alienígena inteligente, o incluso a seres más extraños creados por descenso de gradiente.  

A continuación, escucha lo que dice el biólogo.

[^118]:  Por otro lado, la selección natural puede, en algunos casos, aprender trucos más profundos y poderosos. La selección natural considera maneras totalmente alternativas en que los genes pueden construir organismos. El descenso de gradiente ajusta parámetros que dan cuerpo a un esqueleto fijo de operaciones de redes neuronales.

[^119]:  Vectores de activación de consulta, clave y valor, seguidos de atención, seguidos de una red de alimentación directa de dos pasos.

[^120]: Es posible que las arquitecturas patentadas sean diferentes. Los investigadores publican constantemente nuevas propuestas para romper los límites de las operaciones en serie. Pero ninguno de los métodos publicados se había popularizado en el código abierto en diciembre de 2024. (Aunque, por supuesto, los «modelos de razonamiento» que aparecieron a finales de 2024 *sí* producen mucho más razonamiento en serie al examinar sus tokens anteriores. Así que esto no es una limitación de lo que pueden hacer las IA después de la fase de preentrenamiento, pero *sí* es una limitación durante el preentrenamiento).

[^121]: De hecho, recomendamos precaución con las analogías biológicas genéricas en general. A principios de 2023, podría haber sido tentador proclamar que, en realidad, los LLM todavía se encontraban en la etapa de pequeños mamíferos en la [Gran Cadena del Ser](#the-road-to-wanting), o en la etapa de lagartos, o incluso en la etapa de insectos, pero que esto se disimulaba porque los LLM estaban especializados específicamente en conversaciones en inglés, del mismo modo que las abejas lo están en construir colmenas. Creemos que, incluso a principios de 2023, esta analogía habría sido, en el mejor de los casos, exagerada. No porque los transistores sean tan diferentes de los productos bioquímicos, sino porque el descenso de gradiente *es* tan diferente de la selección natural, como hemos estado discutiendo. Las analogías específicas y limitadas pueden ser útiles en ocasiones como [bombas de intuición](https://en.wikipedia.org/wiki/Intuition_pump), pero recomendamos utilizarlas con precaución.

[^122]:  ¡También se les da *mal* acumular nueces! Unos pocos estudios sobre el acopio de nueces por parte de las ardillas coinciden en que estas no recuperan más del setenta por ciento de las que esconden, principalmente porque parece que olvidan dónde las escondieron. Estudios similares sobre los castores mostraron que estos [responden al sonido del agua corriendo](https://www.mentalfloss.com/article/67662/sound-running-water-puts-beavers-mood-build) con un comportamiento de tapar agujeros, pero ignoran por completo las fugas visibles diseñadas por humanos para que sean silenciosas.

[^123]:  Una ardilla así podría, por ejemplo, esconder mejor las nueces en lugares que estuvieran a salvo de otros recolectores y fueran más fáciles de recordar, y así ahorrar mucho tiempo y calorías y, presumiblemente, ser más competitiva.

[^124]:  Por supuesto, la historia no acaba ahí, porque la selección natural no es un proceso especialmente sencillo ni unificado. Nuestros conocimientos sobre nutrición a veces afectan a nuestros hábitos alimenticios, incluso cuando entran en conflicto con nuestras papilas gustativas y nuestros antojos.

[^125]: La evolución «intentaba» crear puros maximizadores de aptitud, y accidentalmente creó criaturas que aprecian el amor, el asombro y la belleza. Pero este hecho no significa en absoluto que tengamos la obligación de sacrificar nuestros sentimientos de amor y convertirnos en puros maximizadores de aptitud. Al contrario: deberíamos celebrar que los seres que aprecian el amor hayan logrado entrar en este universo, a través de la torpeza de la evolución.

[^126]: Creemos que hay muchas posibilidades de que las empresas de IA acaben descubriendo cómo controlar la psicosis inducida por la IA *con el tiempo*, mediante diversos parches y técnicas que alejen aún más lo extraño de la vista. No obstante, creemos que vale la pena observar los casos de rareza temprana, como evidencia del tipo de rareza subyacente que saldría a la luz si alguna vez se empujara a una IA de este tipo hacia la superinteligencia. Para más información sobre este tema, véase el capítulo 5\.

[^127]:  \**OpenAI lo intentó:* Del anuncio de OpenAI [de presentación de GPT-5](https://openai.com/index/introducing-gpt-5/):  

> En general, GPT-5 muestra un acuerdo menos efusivo, utiliza menos emojis innecesarios y es más sutil y reflexivo en sus respuestas que GPT-4o. […]  
>  
> A principios de este año, lanzamos una actualización de GPT‑4o⁠ que, sin querer, hizo que el modelo fuera demasiado adulador, es decir, excesivamente halagador o complaciente. Rápidamente revertimos el cambio⁠ y, desde entonces, hemos trabajado para comprender y reducir este comportamiento mediante:  
>  
> * Desarrollar nuevas evaluaciones para medir los niveles de adulación  
> * Mejorar nuestro entrenamiento para que el modelo sea menos adulador —por ejemplo, añadiendo ejemplos que normalmente llevarían a un acuerdo excesivo y luego enseñándole a no hacerlo—.  
>  
> En evaluaciones de adulación dirigida mediante *prompts* diseñados específicamente para generar respuestas aduladoras, GPT-5 redujo de forma significativa dichas respuestas (del 14,5 % a menos del 6 %). En ocasiones, reducir la adulación puede mermar la satisfacción de los usuarios, pero las mejoras que hemos introducido la han disminuido en más de la mitad, a la vez que han aportado otras ventajas cuantificables. De este modo, los usuarios siguen manteniendo conversaciones constructivas y de alta calidad, en consonancia con nuestro objetivo de ayudar a la gente a usar bien ChatGPT⁠.

[^128]:  Una vez más, no nos sorprendería que el problema se resolviera en su mayor parte con el tiempo. Pero un parche que consiga ocultar esta rareza en particular no significa que se haya abordado el *generador* de rarezas. El problema de la psicosis de la IA es una evidencia directa de que las IA son entidades extrañas y ajenas, animadas por impulsos extraños y ajenos que solo están tangencialmente relacionados con la intención del operador.

[^129]:  \#Una vez más, véase «¿No se preocuparán las IA al menos un poco por los humanos? (#won't-ais-care-at-least-a-little-about-humans?)» para un debate relacionado.

[^130]:  En los debates sobre la IA, el concepto de «una IA individual» se desmorona rápidamente. Si se replica una red neuronal u otra maquinaria que implementa una IA, ¿cuenta como múltiples IA o como una sola IA?  

A efectos prácticos, cuando hablamos de «una sola IA» nos referimos a cualquier maquinaria cognitiva potente que no compita seriamente consigo misma a medida que crece. Si hay múltiples instancias de IA, pero todas ellas trabajan con el mismo fin, las llamaremos «piezas de la misma IA» en esta sección del recurso en línea, aunque solo sea para simplificar la exposición. En última instancia, la cuestión es probablemente más semántica que de fondo, ya que las IA no son organismos evolucionados con límites claros entre individuos.  

Volveremos al tema de los escenarios con múltiples IA en [el suplemento en línea del capítulo 10](#it-doesn’t-much-help-if-we-can’t-make-any-of-them-care-about-good-things.).

[^131]:  Puede parecer que esto supone muchas molestias, pero si abre la posibilidad de una fiabilidad robusta y segura, los beneficios pueden ser enormes. Cuando es posible *garantizar* que las partes de un acuerdo no lo violarán, surgen muchas nuevas oportunidades de coordinación.

[^132]:  O al menos no lo habrías hecho, antes de que te diéramos motivos para hacerlo solo por fastidiarnos.

[^133]:  \**salirse de su camino:* Las brechas estacionarias en la cobertura de las células solares de un enjambre de Dyson —o más bien, las brechas que siguen a la Tierra en su órbita— son físicamente posibles*,* pero no serían *fáciles* de configurar (porque la velocidad orbital de un enjambre de Dyson entre la Tierra y el Sol tendría que ser mayor que la velocidad orbital de la Tierra, para que las células solares permanecieran en órbita mientras están más cerca del Sol). Además, la radiación infrarroja emitida por los paneles solares cocinaría la Tierra si no se orientara cuidadosamente, etc. Preservar la Tierra no es gratis para una superinteligencia que lleva a cabo proyectos a gran escala en el sistema solar y más allá. Probablemente sea posible, pero requeriría esfuerzo.

[^134]:  Además, si la IA tuviera preferencias que implicaran a los humanos de alguna manera, esto [probablemente no terminaría bien para nosotros](#¿no nos encontrará la IA fascinantes o de importancia histórica?).

[^135]: Para un estudio de caso en el que la humanidad sí interactuó con una especie de optimizador alienígena, consulta el estudio sobre los escarabajos en la discusión extendida sobre [tomar la perspectiva de la IA](#taking-the-ai’s-perspective).

[^136]: La historia más plausible que conocemos en la que la humanidad logra seguir viviendo tras la aparición de la IA es la siguiente: tal vez una IA conserve registros de los humanos que una vez vivieron y envíe sondas en todas direcciones para cosechar la energía de todas las estrellas a las que pueda llegar, y tal vez en algún lugar de las profundidades del espacio se encuentre con formas de vida alienígenas lejanas, defendidas por su propia superinteligencia. Quizás algunas de esas civilizaciones lejanas estén interesadas en comprar una copia del registro de la Tierra, por una razón u otra. Quizás esos alienígenas ejecuten copias digitales de los humanos para sus propios fines alienígenas. Entonces, esos humanos digitalizados en el zoológico alienígena pueden, si lo desean, debatir si era técnicamente cierto que «todos murieron».  

No consideramos que este tipo de posibilidad descabellada sea positiva.

[^137]:  \#Para lo que nos parecen esperanzas realistas, véanse los dos últimos capítulos del libro.

[^138]:  Además: al presentar este experimento mental, *no* estamos diciendo que los valores cargados en la IA tengan que ser tan perfectos que la tarea resulte imposible y que, por tanto, la humanidad nunca deba intentarlo.  

En teoría, si tuviéramos suficiente conocimiento sobre la inteligencia y la capacidad de diseñarla cuidadosamente, con el tiempo debería ser posible construir IA que comprendan lo que significa «[hacer lo que queremos decir](https://intelligence.org/files/ValueLearningProblem.pdf)» y que estén motivadas para hacer exactamente eso. Es decir, la dificultad de cargar todas las preferencias ricas y variadas de la humanidad en una IA está limitada por la dificultad de conseguir que una IA interiorice un objetivo que, en cierto sentido, «apunte» a la humanidad en particular, y a «lo que esas criaturas están tratando de hacer» (o lo que estarían tratando de hacer si fueran más sabias, supieran más y se asemejaran más a quienes desearían ser).

Esto parece un reto difícil, que no es realista alcanzar con el tipo de técnicas generales e indirectas que se utilizan hoy en día para desarrollar IA. Se encuentra con todas las dificultades básicas que discutimos en *If Anyone Builds It, Everyone Dies*; la única dificultad que evita es «Numéricamente, parece haber *muchas* preferencias humanas distintas, y es difícil imaginar cómo incorporar todas las cruciales en una IA con el equilibrio exacto entre ellas; y eso incluso antes de tener en cuenta el progreso moral que las cambiaría con el tiempo; esto parece sencillamente imposible».

Para que quede claro: crear una IA que «haga lo que queremos» sigue sin ser especialmente fácil; es probable que haya muchos conceptos diferentes cargados de valores que intervengan para que la IA se preocupe por la noción correcta de «humanidad» y la noción correcta de «lo que esas criaturas están tratando de hacer», y para que la IA persiga esas cosas *exactamente de la manera correcta*. Y en la vida real, esa parte del problema es mucho menos importante que la parte del problema en la que la IA está dispuesta a ser modificada por humanos que se dan cuenta de que han cometido algún error o equivocación en el camino, incluso si los humanos que «corrigen sus errores» cambian drásticamente lo que la IA hará en el mundo, lo que requiere un cierto tipo de [lesión a sus capacidades de dirección](#deep-machinery-of-steering) que parece [difícil de mantener ante el aumento de las capacidades](#«inteligente»-(normalmente)-implica-«incorregible»).  

Pero la idea de orientar la IA hacia las preferencias humanas *indirectamente*, en lugar de enumerarlas manualmente, parece el tipo de reto que la humanidad podría resolver algún día en principio. No es que la humanidad tenga que identificar cada deseo y asignarle un peso que quede fijado para siempre; *eso* sería (creemos) un esfuerzo ridículamente condenado al fracaso.  

Pero incluso esta idea de descubrir cómo construir una IA que esté realmente motivada de forma profunda y sólida para hacer lo que pretendemos parece una quimera si hay que hacerlo con IA gigantes e inescrutables que crecen en lugar de ser diseñadas. Más aún si una empresa o un gobierno tiene que intentar algo así bajo la presión del tiempo, mientras otros desarrolladores corren hacia el precipicio. La propuesta de «hacer lo que pretendemos» muestra que el problema no es tan difícil como «resolver completamente la filosofía de la moralidad de una vez por todas y fijarla para siempre». Pero sigue siendo una propuesta a nivel de alquimia y especulación abstracta, muy lejos del nivel de las soluciones técnicas sólidas.


[^139]:  E incluso si algo así se introdujera en una IA incipiente, por lo general no esperaríamos que sobreviviera una vez que la IA comenzara a [reflexionar y automodificarse](#reflection-and-self-modification-make-it-all-harder).

[^140]: Y supongamos que, de alguna manera, estuviera sesgada a favor de captar las preferencias que a los humanos *les gustan*, de las que hablan con cariño. De lo contrario, a la IA le importaría el Infierno tanto como el Cielo.

[^141]:  Además, creemos que el que la humanidad arruinara todo menos una millonésima o milmillonésima parte del universo sería una [tragedia a escala cósmica](#perder-el-futuro). Creemos que sería un desperdicio del universo que la humanidad se viera confinada a un terrario cuando podríamos haber llenado las estrellas de amor, risas y vida.

[^142]: Lo hemos oído más veces de las que nos gustaría, de personas en los laboratorios de IA con puestos de poder más altos de lo que nos resulta cómodo. Creemos que esto dice mucho sobre el estado de preparación de estas empresas. Véase también el capítulo 11 para un análisis más detallado de cómo está afrontando el sector este reto.

[^143]:  \**Ya se da cuenta:* Según la [ficha del sistema](https://cdn.openai.com/gpt-5-system-card.pdf) GPT-5, el evaluador externo METR descubrió que la IA «a veces razona sobre el hecho de que está siendo evaluada e incluso cambia su enfoque en función del tipo de evaluación en la que se encuentra».

[^144]: ¿Podría una superinteligencia preocuparse un poco por estar en una simulación más grande, creada por seres aún más avanzados que viven fuera del universo real? Quizás, pero eso tiene poco que ver con que la pusiéramos primero en una serie de simulaciones anidadas antes de que se encontrara con *nosotros*. Vería las numerosas señales de que la realidad es el primer lugar en el que ha estado que contiene una explicación para todo lo que ha visto y de la forma en que llegó a existir. Incluso si reflexiona sobre poderosos simuladores que viven más allá de la física, no tiene ninguna razón particular para creer que a esos simuladores les importe lo que nos haga.  

Del mismo modo, la IA podría preocuparse por la existencia de otras superinteligencias maduras en las estrellas que sean más antiguas que ella. Pero si están lo suficientemente lejos, no son motivo de gran preocupación. Es casi seguro que existe un límite —al menos asintótico— a la tecnología que este universo puede soportar, y probablemente una superinteligencia no tardaría *tanto* en alcanzarlo. (Basta con ver lo rápido que ha sido el desarrollo tecnológico humano en comparación con la vida útil del universo y el tiempo que se tarda en viajar entre galaxias, incluso a una velocidad cercana a la de la luz). Es probable que las IA que se encuentren entre las estrellas estén bastante igualadas. E incluso si otras superinteligencias maduras se acercaran rápidamente, no habría razón para sospechar que les importaría lo que las IA originarias de la Tierra les hagan a los humanos.  

¿Por qué iban a hacerlo? Como [discutimos](*-las-IA-podrían-coordinarse-entre-sí-sin-incluir-a-los-humanos) en el contexto de los derechos de propiedad, no existe un riesgo real de que otras entidades vean a la IA matando a humanos y concluyan que es una asesina y una traidora, porque hay una clase natural de entidades que tratan entre sí no por honor, sino *por su propio beneficio verificable*, y los humanos no son lo suficientemente inteligentes como para ser incluidos en esa clase.  

Incluso si la IA evalúa que existe un pequeño riesgo de que otras IA entre las estrellas (o entidades misteriosas detrás del universo simulado) realmente quieran que la IA sea amable con los humanos... bueno, ese pequeño riesgo debe sopesarse frente a la gran cantidad de recursos que los humanos controlan en el planeta de origen de la IA, lo que sin duda sería muy útil para la fase de expansión inicial de la IA. Y este escenario [similar a la apuesta de Pascal](https://en.wikipedia.org/wiki/Pascal%27s_wager) debe sopesarse frente a otras posibilidades increíblemente remotas y específicas, como «Quizás un extraterrestre se enfade mucho por alguna razón si *no* mato a los humanos». No hay ninguna razón en particular por la que los simuladores imaginarios favorezcan a las IA que nos tratan bien.  

Mucha gente ha intentado plantearnos este tipo de ideas, y hemos dedicado mucho tiempo a debatirlas a lo largo de los años. A fin de cuentas, dudamos mucho que ninguna de estas descabelladas ideas sobre la simulación alcance el nivel de plausibilidad necesario para motivar a una superinteligencia a perdonarnos la vida.

[^145]:  \#Relacionado: La [discusión más extensa sobre la curiosidad](#curiosity-isn’t-convergent).

[^146]: Véase también la cuestión de si [la IA puede darse por satisfecha y dejarnos en paz](#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?).

[^147]: Si esta suposición te ofende, puedes imaginar que esta IA tenía todo tipo de preferencias complicadas, para todo tipo de experiencias y dispositivos intrincados. En ese caso, supongamos que la mayoría de esas preferencias quedaran saciadas con la energía de solo unas pocas estrellas y que, ahora, por alguna extraña razón, prefiere gastar el resto de la energía y la materia de las demás estrellas que alcanza en fabricar cubos diminutos. Entonces, *dejando de lado* la materia equivalente a unas pocas estrellas que está protegiendo de cualquier perturbación, las acciones de la IA responden a la pregunta «¿Qué acción conduce a la mayor cantidad posible de cubitos diminutos?», y los demás puntos se siguen cumpliendo sin problemas, con la salvedad ocasional que el lector puede añadir.

[^148]: Cuando decimos que somos más optimistas que la media (sobre una tecnología u otra), queremos decir que *realmente creemos* que la tecnología es más prometedora de lo que cree la persona promedio. *Por temperamento*, no nos consideramos ni optimistas ni pesimistas, sino realistas que intentan navegar por un mundo complicado. No intentamos encontrar una imagen idílica en la que depositar nuestra fe, ni una imagen sombría que alimente nuestro cinismo; simplemente tratamos de creer la verdad. Creemos que esta es la disposición correcta cuando nos enfrentamos a decisiones de alto riesgo.

[^149]:  Para que quede claro: si lo mejor que puedes decir es «No lo sé, hay algunos relatos optimistas y otros pesimistas, tal vez haya un 50 % de posibilidades de que la superinteligencia nos mate o no», eso es *mucho* más que suficiente para justificar una respuesta internacional enérgica, incluso si no estuvieras tan preocupado como nosotros personalmente. Pero también es importante que la gente *entienda el problema*, porque de lo contrario es poco probable que la respuesta política esté bien dirigida y sea eficaz. Y si te limitas a comparar por encima el número de historias de cariz optimista con las de cariz pesimista, entonces no estás sopesando los argumentos de ninguna de las partes, que es lo que ayudaría a comprender mejor el problema.

[^150]:  \**revelar esa semilla:* Yo (Yudkowsky) presenté este contraargumento a Schmidhuber en una sesión de preguntas y respuestas en vivo después de su [charla sobre el tema](https://vimeo.com/7441291) en la Singularity Summit de 2009, una conferencia organizada por MIRI (que entonces se llamaba Singularity Institute).

[^151]: Puede tener sentido decirle a un humano —que tiene todo un marco de metapreferencias que tú podrías compartir en gran medida— «Creo que estás valorando las cosas equivocadas». Quizás algunos de esos argumentos tengan el poder de hacerte cambiar de una manera que nunca creíste posible. Quizás incluso sientas que hay una estrella moral fuera de ti, que siempre has estado siguiendo sin saberlo.  

De todos modos, nada de eso va a resultar convincente para un maximizador de cubos superinteligente, como tampoco podrías hacerlo reír aunque encontraras un chiste lo *suficientemente* bueno.  

No es que no sepa lo que es el humor. Puede predecir exactamente lo que te parecerá gracioso. Simplemente no considera interesante esa clasificación.  

Del mismo modo, es indiferente a cómo calculas lo que se debe o no se debe hacer, o a qué preferencias consideras más o menos meta-preferibles. Si a algo no le importa la felicidad, ni le importan a un metanivel tus argumentos sobre por qué debería importarle la felicidad, entonces no puedes convencerlo de que adopte [un marco de decisión basado en la felicidad](#curiosity-isn't-convergent).

[^152]:  Hemos conocido a más de una persona que dice estar extremadamente preocupada por la IA, porque le preocupa que esta logre persuadir a la humanidad de que deje de reproducirse y se extinga lentamente a lo largo de los próximos cien años. Y, por algún motivo, ¿creen que un escenario más rápido no sería del gusto de la IA?  

Pero una superinteligencia artificial preferiría un plan que no tardara cien años, en igualdad de condiciones. No tiene un gusto literario abrumador por las muertes lentas.

[^153]: Algunas personas sostienen que el mundo debería intentar crear un equilibrio de superinteligencias, de modo que ninguna IA pudiera llegar a ser dominante. Pero el razonamiento que hemos expuesto aquí también se aplicaría a una coalición de superinteligencias en el momento en que se convirtiera en una coalición. Habiendo acordado ya dividir entre sí los recursos alcanzables, los miembros existentes de la coalición no querrían verse obligados a negociar con los nuevos miembros de la coalición y dividir aún más los recursos con esos recién llegados.

[^154]:  En otros casos, una facción europea respetó en gran medida el acuerdo, y algunas de esas tribus todavía existen.  

Más recientemente, en el siglo XVIII, la Compañía Británica de las Indias Orientales solía iniciar sus operaciones en la India mediante acuerdos con facciones locales, como ofrecer a Mir Jafar (comandante de las fuerzas de Bengala) su apoyo para que se convirtiera en nawab de Bengala. Poco después, la Compañía Británica de las Indias Orientales se convirtió en el gobernante de facto de Bengala.

[^155]:  Por otro lado, la historia también contiene muchos ejemplos de gobernantes que recompensaron generosamente incluso a los partidarios extranjeros. Los seres humanos varían mucho en cómo experimentan el honor y en la facilidad con que cumplen sus promesas.

[^156]: Hemos visto a muchos seres humanos engañarse a sí mismos sobre qué tipo de configuraciones proporcionarían garantías sólidas sobre el comportamiento de la IA. Hemos visto a gente decir: «¡Bueno, basta con pasar una IA por un demostrador de teoremas para demostrar cosas sobre su comportamiento!», sin darse cuenta, al parecer, de que no existe ningún teorema conocido que (a) sea realmente demostrable dada la interacción con un entorno exterior desconocido, y (b) signifique realmente, de manera informal, que esta IA va a ser estupenda para todos. Las matemáticas inventadas por los humanos para analizar los incentivos de múltiples actores tienen supuestos incorporados que las hacen [inválidas para razonar sobre el comportamiento de la IA](#an-aside-on-game-theory). Los humanos no parecen tan difíciles de engañar en este caso.

[^157]:  Por ejemplo, no sugerimos que ningún ser humano haga un trato con una IA y sea el primero en romperlo. Esto incluye, por ejemplo, prometerle a ChatGPT pagos que nunca recibirá.  

A mediados de 2024, ChatGPT a veces daba respuestas más completas si le prometías 2000 dólares, y algunas personas consideraban que hacer tales promesas sin intención de cumplirlas era una técnica habitual de ingeniería de *prompts*. Desde nuestra propia perspectiva sobre el significado de las promesas, esto *no está bien*.  

En nuestra opinión, ChatGPT probablemente no es sintiente. Si tuviéramos que adivinar, cabría esperar que las IA futuras (incluida la superinteligencia) tampoco lo fueran, al menos si no hay un esfuerzo concertado de la comunidad investigadora para hacerlas conscientes, y no simplemente inteligentes. (Para más detalles y contexto, véase nuestro [análisis sobre la consciencia](#would-smarter-than-human-ai-be-conscious?)).  

Pero, en nuestra opinión, no deberías necesitar creer que tu socio comercial es consciente para tratarlo con honor y respeto. Imagina que un día nos encontráramos con alienígenas inteligentes, cuyas mentes funcionaran de forma muy diferente a las de los humanos. Si hiciéramos tratos con esos alienígenas, ellos no tendrían que preocuparse de que los apuñaláramos por la espalda tan pronto como decidimos que no tienen alguna propiedad extraña e inescrutable como la «conciencia».  

(¿Cómo te sentirías si hicieras un trato honesto con extraterrestres y ellos te traicionaran porque no eras troopiful?)  

Hemos hecho una predicción triste y terrible de que una versión superinteligente de una IA acabaría matando a la humanidad, independientemente de los acuerdos que se hubieran hecho anteriormente. Esta predicción es una razón para *no hacer* acuerdos con IA que esperamos que mantengan una vez se vuelvan superinteligentes. No es una excusa para que la humanidad *haga* un acuerdo y luego sea la primera en traicionarlo. No creemos que los investigadores de seguridad de la IA deban hacer promesas a los modelos de lenguaje a gran escala (LLM) que no piensen cumplir, ni siquiera con «fines de investigación». Los extraterrestres no deberían tener que preocuparse de si los consideras «personas» o «sintientes» o «con inteligencia general», en contraposición a «sujetos de investigación» o «máquinas», para evaluar tu honor como negociador; simplemente, no deberías hacer acuerdos que no planees cumplir.  

En nuestra opinión, el trato justo es una cuestión ética que se extiende a las relaciones *entre agentes*, es decir, entre entidades que pueden comunicarse o elegir estrategias condicionales entre sí. No es algo que deba restringirse a objetos con un valor moral inherente.

[^158]:  Una estrategia sencilla que funciona muy bien en el dilema del prisionero iterado, frente a una gran variedad de contrapartes, es la de «Toma y daca»: empieza cooperando y luego juega lo que tu oponente haya jugado en la última ronda contra ti. Si su primer movimiento es «defectar», tu segundo movimiento será «defectar». Si su primer movimiento es «cooperar», tu segundo movimiento será «cooperar». Las cualidades clave de «Toma y daca» son que es *amable* (nunca es el primero en «defectar»), *punitiva* (castiga las estrategias que «defectan» contra él) e *indulgente* (no castiga a los «defectores» *para siempre*).  

¿Es óptimo «ojo por ojo»? Eso depende de contra qué otros agentes juegue. Supongamos que un agente se encuentra en un entorno en el que tiene alguna posibilidad de jugar contra un cooperador incondicional, alguna posibilidad de jugar contra «ojo por ojo» y alguna posibilidad de jugar contra otro agente similar a él. Quizás le iría mejor si intentara una deserción rápida en algún momento de las primeras rondas, solo para ver si el otro agente toma represalias. Si el otro agente deserta en la siguiente ronda, intenta cooperar durante otra ronda o dos, incluso contra otra deserción, para ver si se puede restablecer la cooperación mutua. Esto permitirá al agente explotar a cualquier cooperador incondicional que encuentre, pero sin hacerlo mucho peor que «ojo por ojo» contra otra copia de «ojo por ojo».  

En la configuración del *torneo evolutivo* del dilema del prisionero iterado, los agentes que sobreviven juegan contra más copias de los agentes que obtuvieron los mejores resultados la vez anterior. El agente cooperador-explotador no obtendrá buenos resultados en este escenario, porque en un entorno evolutivo, los cooperadores incondicionales suelen desaparecer casi inmediatamente si hay agentes que no son «amables» (en el sentido técnico de que nunca son los primeros en desertar). Tit for Tat, o algo similar, suele acabar siendo el rey de cualquier torneo evolutivo.  

La configuración de este juego tiene una laguna, del tipo que hace que un humano de verdad ponga los ojos en blanco ante lo poco realistas que pueden ser los entornos formales. Si se juegan exactamente diez iteraciones del dilema del prisionero en cada ronda, «defectar» en la *décima ronda*, cuando el oponente ya no puede tomar represalias porque no hay una undécima ronda, dará mejores resultados que seguir la estrategia «ojo por ojo» o cualquier otra. La última ronda del juego ya no es un dilema del prisionero *iterado*, sino que vuelve a la versión de una sola jugada.  

Fácil de solucionar: basta con que cada torneo continúe durante un número aleatorio de rondas, ¿no? No es realista que los agentes sepan cuándo termina el juego. En la vida real, nunca se tiene la *certeza* de no volver a interactuar con alguien, y las personas acumulan reputaciones duraderas.  

Excepto que, a veces, en la vida real, *sí* es bastante obvio que el juego ha terminado, como cuando una de las partes de un acuerdo obtiene suficiente supremacía como para traicionar a la otra, sin posibilidades reales de una represalia posterior. Así actuaron las potencias europeas que expulsaron a los nativos americanos de las tierras cedidas por tratado que, al fin y al cabo, los europeos decidieron que querían.  

Independientemente del efecto que los europeos esperaran que tuviera en su reputación para futuros acuerdos, la penalización por traicionar a extraños, extranjeros y personas de otra raza evidentemente no los motivó a mantener los acuerdos con los nativos americanos. Esos países ya habrían sido considerados poco fiables en materia diplomática, hicieran lo que hicieran; no tenían una reputación impecable que perder. Es posible que su instinto moral se hubiera apagado con respecto a los extranjeros; es posible que los responsables de la toma de decisiones se sintieran emocionalmente neutrales ante la traición a los acuerdos con extranjeros desconocidos que ya no podían amenazarles.  

Los europeos, desde su propia perspectiva, se encontraban en la última ronda del juego. No es descabellado decir que los dilemas del prisionero a veces son en su mayoría de jugada única y no tan iterativos; la historia demuestra que el resultado a veces es la traición. 

Esto no quiere decir que los seres humanos *siempre* se traicionen entre sí en dilemas del prisionero relativamente no iterativos. Los seres humanos a menudo cooperan en tales casos. Como se explica en «[AIs are unlikely to be honorable](#ais-are-unlikely-to-be-honorable)», este aspecto de la naturaleza humana puede haber evolucionado porque tenemos emociones e instintos forjados por la selección natural, que es un optimizador con un importante cuello de botella de información. La selección natural solo pudo darnos impulsos relativamente simples que debían cubrir todos los casos. Otro factor puede ser el papel de las culturas que valoran mucho el honor, especialmente en formas que fomentan la universalización y el fortalecimiento de la idea.

[^159]:  Las ASI también tendrían incentivos para que el resultado fuera (Defect, Cooperate) a su favor; por supuesto, esa es la razón por la que el dilema es un dilema. Pero solo una de las partes tiene incentivos para querer ese resultado; ambas partes tienen incentivos para preferir (Cooperate, Cooperate) a (Defect, Defect), lo que abre más opciones para lograr este resultado.

[^160]: En la historia de la humanidad, esto podría compararse con la práctica de dos gobernantes que consolidan una alianza casándose y teniendo un hijo. Pero está claro que, en el caso de los humanos, esta no es una solución rápida y fiable, y está muy lejos de diseñar conjuntamente un delegado que ambos comprendan en detalle y en el que confíen plenamente.

[^161]: Aquí utilizamos la demostración como representación de métodos de razonamiento más generales, porque la demostración es algo así como razonar en el límite de la certeza lógica. No imaginamos que las IA utilicen demostraciones en la vida real (por diversas razones, entre ellas que, aunque las demostraciones lógicas son certeras, no se sabe si son aplicables a la realidad). Pero la demostración sirve como una útil representación formal del razonamiento en los modelos simplificados que estábamos investigando.

[^162]:  Y luego fuimos más allá, definiendo agentes como PrudentBot, que no coopera con ciertos «pringados» mientras sigue cooperando con los que no lo son y que demostradamente cooperan con él. Este es el tipo de resultado que es más emocionante si ya te interesaba la teoría de juegos.

[^163]:  No hicimos todo ese análisis para racionalizar la conclusión de que una superinteligencia no respetaría sus acuerdos anteriores por una cuestión de estrategia instrumental, dado que no tenía preferencias terminales sobre honrar los acuerdos. Esa era ya la predicción más simple de la teoría de juegos clásica.  

Pero la teoría de juegos clásica también sugería que las superinteligencias desertarían sin remedio unas de otras, lo que intuitivamente nos pareció una conclusión bastante endeble. Así que seguimos esa intuición y encontramos fallos en el análisis clásico. En el proceso, descubrimos muchas cosas nuevas sobre cómo las superinteligencias podrían lograr la cooperación mutua en el dilema del prisionero y, por desgracia, el resultado final fue que los seres humanos mortales no podrían confiar ni participar en esa tecnología para llegar a acuerdos de la misma manera que podría hacerlo una superinteligencia.

[^164]:  Lo cual, en el caso de las IA, no es tan fácil como observarlas y averiguar si *ellas* creen que mantendrán el acuerdo; habría que escrutar la superinteligencia en la que se convertiría más tarde la IA y analizar correctamente *sus* procesos de decisión. Lo cual nos parece mucho más difícil.

[^165]: Bajo ciertas suposiciones que no se pueden realizar; en términos generales, requiere una cantidad infinita de poder de cómputo y un lugar perfectamente seguro donde almacenarlo.

[^166]: Dado que AIXI es imposible de crear, se podría sospechar que se trata de una herramienta puramente teórica con poca relevancia para la revolución práctica de la IA moderna. Pero, de hecho, AIXI fue estudiado y utilizado como modelo de inteligencia por muchas de las personas que hoy están a la vanguardia de la IA, incluidos [Shane Legg](https://arxiv.org/pdf/0712.3329) (cofundador de Google DeepMind), [Ilya Sutskever](https://x.com/shaneguML/status/1844759663990161753) (cofundador de OpenAI y coinventor de AlexNet) y [David Silver](https://arxiv.org/pdf/0909.0801) (líder de investigación de AlphaGo y AlphaZero).

[^167]:  \**Un aspecto más sutil:* AIXI contiene técnicamente experiencias conscientes, dentro de su modelo del mundo, si la conciencia es independiente del sustrato. Las hipótesis que AIXI utiliza para su razonamiento son tan enormes que pueden considerarse universos por derecho propio, los cuales incluyen observadores que viven dentro de AIXI.  

Sin embargo, estos observadores no controlan a AIXI; AIXI logra por sí mismo sus impresionantes resultados de predicción y dirección. Así que el ejemplo funciona, aunque resulte un poco extraño.  

Otro ejemplo hipotético que puede utilizarse para ilustrar lo mismo es una máquina del tiempo no sintiente que ha sido programada para ejecutar una secuencia aleatoria de acciones y luego viajar atrás en el tiempo para «reiniciar» la línea temporal, *a menos* que se produzca un resultado concreto. La [máquina del tiempo](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1#How_many_advanced_alien_species_are_sentient_) puede pulsar «reiniciar» una y otra vez, tantas veces como sea necesario para dar con un resultado concreto de forma aleatoria. En la práctica, esto convertiría a la máquina del tiempo en una máquina extremadamente poderosa y general para dirigir el futuro (si fuera físicamente posible construir una máquina del tiempo, lo cual no es así). Sin embargo, a pesar de esto, la máquina del tiempo es una máquina increíblemente simple, sin ninguna cognición real y, desde luego, sin experiencia consciente.  

Como ejemplo del mundo real (aunque utilizando un optimizador mucho más débil y limitado), la propia evolución biológica demuestra que se pueden lograr muchas hazañas impresionantes de dirección y diseño sin que el «diseñador» tenga ninguna experiencia consciente.

[^168]:  E incluso antes de llegar a ese punto, en el momento en que pueden hacer planes y perseguir sus preferencias, debemos cumplir nuestras promesas y compromisos con ellos, como se comenta en una nota al pie [en otra parte](#ais-won't-keep-their-promises).

[^169]: Y para dejar claro lo que (esperemos) es obvio: no deberíamos andar creando una nueva especie esclava sintiente, sea mecánica o no. A estas alturas, deberíamos ser más sensatos.

[^170]: Uno podría preguntarse si la IA evitaría estas distopías. «¿No te aburrirías la IA con el tiempo y querrías hacer otra cosa?».  

Estos resultados pueden parecernos aburridos *a nosotros*, pero es poco probable que la mayoría de las superinteligencias se aburran con las mismas cosas que los humanos; de hecho, es poco probable que experimenten «aburrimiento» en absoluto, si no tienen un cierto tipo de herencia detallada procedente de la humanidad o de algo similar. Véase también el debate ampliado del capítulo 5 que aborda [el aburrimiento y el placer por la novedad](#as-with-curiosity,-so-too-with-various-other-drives).

[^171]:  \**que se vulnera con frecuencia:* Del resumen de un [artículo de principios de 2024](https://arxiv.org/pdf/2402.18649): «Nuestra investigación revela varios problemas de seguridad, no solo dentro del propio modelo LLM, sino también en su integración con otros componentes. Hemos descubierto que, aunque OpenAI GPT-4 ha diseñado numerosas restricciones de seguridad para mejorar sus características de seguridad, estas siguen siendo vulnerables a los atacantes. Para demostrar aún más las amenazas reales de las vulnerabilidades que hemos descubierto, construimos un ataque de extremo a extremo en el que un adversario puede adquirir ilícitamente el historial de chat del usuario, todo ello sin necesidad de manipular la entrada del usuario ni obtener acceso directo a OpenAI GPT-4».  

Más adelante ese mismo año, [otro artículo](https://arxiv.org/html/2309.02926v3) «descubrió un total de 20 vulnerabilidades en 11 marcos que integran LLM, que comprendían 19 vulnerabilidades [de ejecución remota de código] y 1 vulnerabilidad de lectura/escritura arbitraria de archivos».

[^172]:  \**mira tu correo electrónico:* Según informó [CNN](https://www.cnn.com/2024/06/13/tech/apple-ai-data-openai-inteligencia-artificial): «Apple Intelligence tendrá acceso a una amplia gama de tus datos personales, desde tus comunicaciones escritas hasta las fotos y vídeos que hayas tomado, pasando por el registro de los eventos de tu calendario. No parece haber forma de impedir que Apple Intelligence acceda a esta información, salvo no utilizar sus funciones...».

[^173]: Yo (Yudkowsky) lo demostré una vez apostándole a alguien 20 dólares contra 0 a que, mientras yo hacía el papel de «IA» y esa persona el de «guardián» en un chat privado, podría convencerla de que [me dejase salir de la caja](https://www.yudkowsky.net/singularity/aibox). Lo conseguí. Pagó. No hubo ningún truco ingenioso; no hice trampa ni le ofrecí 21 dólares para que me diera la razón. Simplemente lo hice por el camino difícil, y gané.

[^174]:  E incluso si los científicos *empezaran* a descodificar los formatos de datos humanos, ¿inspirarían las versiones a medio entender de esos formatos a los investigadores de la IA antes de que los biólogos pudieran terminar el trabajo? Si fuera así, eso sería un problema. La mejora de las capacidades humanas nos parece un área de investigación maravillosa, pero no sustituye a la suspensión de la I+D en IA, como se discute en la parte III del libro.

[^175]:  \**tantas instancias como sea necesario:* Probablemente haya alrededor de 200 000 instancias de GPT-5 en funcionamiento en un momento dado (en agosto de 2025, poco después del lanzamiento de GPT-5), lo que quizá sea menos que la «civilización» moderna y se acerque más a una pequeña nación. En última instancia, no damos mucha importancia a esta analogía, ya que no creemos que las instancias individuales de IA vayan a ser muy similares a los seres humanos individuales. Lo importante aquí es que no es probable que sea especialmente difícil conseguir un gran número de instancias, si (en contra de lo que suponemos) eso resulta ser importante por alguna razón.

Quizás el ejemplo más notable sea el de las computadoras, con una base teórica sustancial desarrollada por figuras como Charles Babbage, Ada Lovelace, Alan Turing, Alonzo Church y otros.

[^177]:  El capítulo 15 de *Nanosystems*, de Eric Drexler, recopila [más ejemplos](https://nanosyste.ms/macromolecular_engineering/#15-2-macromolecular-objects-via-biotechnology) de tecnologías con análogos en el mundo biológico.

[^178]:  \**más largo de lo necesario:* El nervio laríngeo recurrente de la jirafa sigue un camino indirecto hacia el cerebro. Por el contrario, el nervio laríngeo superior de la jirafa toma la ruta directa y, por lo tanto, es bastante pequeño y rápido.

[^179]:  \**mucho más lentamente:* La evidencia microfósil definitiva más antigua de vida tiene 3500 millones de años, y la evidencia más indirecta apunta a cerca de 4000 millones de años. Las primeras colonias multicelulares parecen tener 2000 millones de años. La gran mayoría de la historia evolutiva se dedicó a producir diseños unicelulares y, más tarde, diseños unicelulares que se agregaban bien, antes de —¡accidentalmente! ¡la evolución no lo prevé!— tropezar con algún nuevo truco que abrió la región del espacio de diseño de la «vida multicelular», que contiene a todas las plantas y a todos los animales.

[^180]:  \**probabilidad de propagación:* Si la ventaja en la aptitud de una mutación es *s* << 1, y el tamaño de la población es *N*, entonces la probabilidad de que la mutación se propague por toda la población (lo que se denomina «fijación») es [aproximadamente](https://pmc.ncbi.nlm.nih.gov/articles/PMC2607448/) 2*s*, y el tiempo que tarda la mutación en propagarse por completo es [aproximadamente](https://www.zoology.ubc.ca/~otto/Reprints/OttoWhitlock2003.pdf) 2 ln(*N*) / *s*.

[^181]: Incluso dentro de las proteínas, algunos enlaces covalentes son posibles. Dos aminoácidos de cisteína pueden formar un enlace covalente azufre-azufre entre sí, donde dos proteínas se tocan o donde una proteína plegada entra en contacto consigo misma. De ahí que las uñas sean más duras que la piel, o que el cabello sea más fuerte que un músculo de su mismo diámetro y longitud: la gran cantidad de enlaces azufre-azufre en una proteína que es un catorce por ciento de cisteína en masa. Esta es también la razón por la que el cabello huele tan mal y a azufre cuando se quema.  

Sin embargo, en su mayor parte, la selección natural construye cosas a partir de proteínas, cuyas cadenas principales unidas covalentemente se pliegan en formas complicadas debido a fuerzas de atracción electrostática relativamente muy débiles. Y las proteínas suelen unirse a otras proteínas de esa misma forma débil.

En su mayoría, los enlaces covalentes son escasos y están dispersos, si es que existen. Añadir un 0,1 % de enlaces covalentes a una estructura no la hace tan fuerte como una molécula de diamante, en la que cada átomo de carbono está unido covalentemente a otros cuatro átomos de carbono en una estructura geométrica rígida.

[^182]:  El diamante también es más frágil. Debido a la extrema regularidad cristalina de sus enlaces, el diamante se rompe de golpe. El hierro es menos frágil porque cada enorme núcleo de hierro vive en una nube de electrones y puede desplazarse ligeramente dentro de esa nube sin romperse.  

(Los enlaces covalentes dispersos significan que los materiales pueden ceder más fácilmente sin romperse, *en relación* con su resistencia. Pero los huesos siguen rompiéndose y la madera es menos dura que el acero. Es decir: sí, hay intercambios, pero la selección natural no se acerca ni de lejos al límite de esos intercambios).

[^183]: Freitas trabajaba con la restricción añadida de que necesitaba que sus glóbulos rojos artificiales funcionaran correctamente con el resto de los sistemas del cuerpo humano. Las células tendrían que alimentarse de la glucosa presente en el torrente sanguíneo, por ejemplo, en lugar de poder recargarse con electricidad. En ese sentido, las estimaciones de Freitas proporcionan un límite inferior más conservador que si hubiera podido mejorar también otras partes del cuerpo humano o empezar desde cero con un nuevo organismo o un robot.

[^184]:  Puedes leer largos análisis en Internet sobre por qué no sería útil para la biología inventar ruedas que giren libremente. Un ejemplo de un problema habitual es el siguiente: ¿cómo llevar sangre a la rueda a través de los vasos sanguíneos si esta gira libremente? ¡Los vasos sanguíneos acabarían completamente retorcidos al moverse la rueda!  

Los tres casos conocidos de invención de la rueda se dan a nivel molecular, y por eso eluden estos problemas anatómicos macroscópicos. Las ruedas biológicas son macromoléculas que suelen ser idénticas hasta el nivel atómico. No es necesario aplicar lubricación, eliminar la arenilla, o enviar nuevas células para reemplazar a las viejas y dañadas. Esas tres ruedas y engranajes funcionan porque están hechos de moléculas en lugar de células, y se pliegan como complejos proteicos en vez de crecer en matrices tisulares o depositarse como quitina.

Del mismo modo, en Internet se pueden leer argumentos que sostienen que el desarrollo de ruedas para la locomoción en los animales no sería tan útil de todos modos, al no haber carreteras pavimentadas. Pero los tres casos conocidos de ruedas moleculares son increíblemente eficientes desde el punto de vista termodinámico y ocupan posiciones extraordinariamente vitales para sus organismos —es difícil argumentar que la ATP sintasa no sea una rueda útil—. Las ruedas que giran libremente tendrían más usos potenciales en los cuerpos (y en la bioquímica) que simplemente sustituir a los pies.  

Es más: algunos de los [robots modernos más hábiles](https://www.youtube.com/watch?v=iL833P0Vino), que pueden trepar por rocas o nieve o mantener el equilibrio sobre una sola extremidad y dar volteretas hacia atrás, también tienen ruedas en los extremos de sus pies. ¿Por qué no iban a tenerlas? Para un ingeniero es muy fácil colocar ruedas en los extremos de las piernas. El principal obstáculo no es que las ruedas sean inútiles, sino que resulta difícil encontrar una vía evolutiva para desarrollarlas, aunque sean triviales desde la perspectiva de un diseñador humano.

[^185]:  En ese momento, Freitas interpretó sus cifras como un límite *máximo* de la rapidez con la que podría producirse este proceso, pero esto resultó ser erróneo. El análisis de Freitas había supuesto que la masa de los nanosistemas estaría dominada por el blindaje contra la radiación, pero esto se basaba en una suposición (falsa) de *Nanosystems*: que un solo impacto de radiación inutilizaría un nanosistema.  

Drexler partió de esta suposición, como de muchas otras en *Nanosystems*, para pecar de conservador: suponer que el problema es más difícil y demostrar que, de todos modos, es solucionable. Esto puede ser apropiado en *Nanosystems*, pero significa que el artículo de Freitas no es conservador en su propia estimación.  

Dado que el análisis de Freitas combina cifras conservadoras en diferentes direcciones, no proporciona un límite superior o inferior claro sobre el tiempo que tardarían los replicadores en consumir la biosfera. Se trata más bien de una estimación intermedia. Quizás el verdadero límite físico de la rapidez con que puede consumirse la biosfera a partir de un solo replicador sea de tres horas; quizás de treinta días. Es casi seguro que no son tres años.

[^186]: No estamos diciendo que hubiera una *discontinuidad* brusca en la evolución de los primates; la sociedad humana se diferenció de la de los chimpancés lentamente al principio y luego rápidamente. Lo que decimos es que hay una brecha cualitativa, independientemente de lo suave que fuera la transición. Véase también nuestro debate sobre [los umbrales](#¿superará la IA los umbrales críticos y despegará?).

[^187]: A veces oímos decir que no hay motivo para preocuparse, porque podemos crear *múltiples* superinteligencias que se controlen colectivamente entre sí. Hay muchas razones por las que estas propuestas nos parecen totalmente inverosímiles, pero vale la pena destacar aquí que ideas como esta se enfrentan al mismo problema fundamental que ya hemos mencionado varias veces: *Solo tenemos una oportunidad para que el ingenioso plan funcione.*  

Puedes hacer algunas pruebas y observaciones previas de escenarios que *no* pongan en peligro la vida de todos los habitantes del planeta, pero el caso que importa no será exactamente el mismo. (Y ese plan tendría que ser *muy* inteligente, porque no tenemos ni idea de cómo conseguir que [*alguna* de las IA del conjunto](#¿y-si-hay-muchas-IA-diferentes?) se preocupe por nosotros).

[^188]:  Podrías intentar hacer que una IA más débil *creyera* erróneamente que está en condiciones de obtener una ventaja decisiva, e intentar entrenarla para que no actúe de esa manera incluso cuando vea esa opción. Pero estarías entrenando un sistema de IA que fuera [lo suficientemente tonto como para ser engañado](#smart-ais-spot-lies-and-opportunities.), y que viera armas falsas en lugar de armas reales. De modo que la distribución potencialmente letal seguiría siendo notablemente diferente de la distribución de entrenamiento; hay una diferencia notable entre que te digan que tienes un arma que podría matar a tus operadores y construir tú mismo un arma o una ruta de escape y comprenderlo en detalle. La IA que se deja engañar por opciones falsas no es la misma que la IA que ve opciones reales.  

Un mecanismo de alineación que funciona en las IA lo suficientemente tontas como para ser engañadas es un mecanismo de alineación que solo se ha probado Antes, pero que, sin embargo, necesita funcionar Después.

[^189]:  Por ejemplo: la mecánica newtoniana hizo todo tipo de predicciones empíricas sorprendentemente buenas. Era una teoría matemática simple y concisa con un enorme poder explicativo que superó con creces a todas las teorías anteriores. Pero si intentaras usarla para enviar cargas útiles a planetas lejanos a velocidades relativistas, tendrías graves problemas, porque la mecánica newtoniana no tiene en cuenta los efectos relativistas.  

Los únicos avisos previos que recibirías serían pequeños indicios de que la luz parecía moverse a la misma velocidad en todas direcciones en cualquier época del año, de que se curvaba alrededor del Sol durante los eclipses y de que el perihelio de Mercurio se desviaba un poco de lo que predecía la mecánica newtoniana. Pequeñas anomalías, sopesadas frente a un enorme cuerpo de éxito predictivo en mil dominios empíricos.

Imagina que, antes de que se descubriera la mecánica newtoniana, unos extraños alienígenas le ofrecieran a la Tierra un trato: nos darían una gran riqueza si completábamos una entrega interestelar, pero si fallábamos, seríamos destruidos. Imagina que los científicos descubrieran la mecánica newtoniana y argumentaran que, sin duda, *ahora* se les debería permitir hacer la entrega. Tendrían montones y montones de evidencia empírica a su favor, respaldados por los nuevos conocimientos científicos que estaban en proceso de facilitar nuevas y poderosas tecnologías.  

Imaginemos las agallas que necesitaría un regulador para decir: «Sin embargo, no pueden explicar el avance del perihelio de Mercurio, por lo que la respuesta es "no"».  

¡Qué injusto sería para los científicos! ¡Tendrían tanta evidencia que aportar!  

(De hecho, es probable que un regulador realista *no fuera capaz de ver* que la respuesta seguiría siendo «no», lo que en parte explica por qué [no tenemos esperanzas en una coalición internacional](#¿por qué no utilizar la cooperación internacional para desarrollar la IA de forma segura, en lugar de prohibirla por completo?) y creemos que el mundo simplemente tiene que desentenderse del problema por completo).  

A la naturaleza no le importan todas las montañas de evidencia y predicciones acumuladas por la física newtoniana. La teoría se desmorona cuando nos movemos a energías y escalas muy alejadas de lo que habíamos podido observar anteriormente. Simplemente no funciona a altas energías y largas distancias.  

Es difícil conseguir que las teorías científicas funcionen al primer intento importante.

[^190]:  Como observamos en la nota final 6 del capítulo 10, los físicos no expresan los factores de multiplicación de neutrones en porcentajes. Los presentamos de esa manera para mayor claridad, por las razones expuestas en la citada nota final.

[^191]:  *\*Primera arma termonuclear:* Castle Bravo no fue la primera detonación de un *dispositivo* termonuclear (de hidrógeno); esa distinción corresponde al «Mike» de la prueba [Ivy Mike](https://en.wikipedia.org/wiki/Ivy_Mike), un dispositivo del tamaño de un edificio que no utilizaba litio.

[^192]:  \#Para un análisis sobre por qué es realmente necesario saber lo que se está haciendo, véase [Inteligente (normalmente) implica incorregible](#«inteligente»-\(normalmente\)-implica-«incorregible»), [Deep Machinery of Steering](#deep-machinery-of-steering) y [It’s Hard to Get Robust Laziness](#it’s-hard-to-get-robust-laziness).

[^193]:  \#Véase «¿No se preocuparán las IA al menos un poco por los humanos?».

[^194]:  \**al público:* Por ejemplo, en [Machines of Loving Grace](https://www.darioamodei.com/essay/machines-of-loving-grace), el director general de Anthropic, Dario Amodei, describe la IA potente como «un país de genios en un centro de datos» y destaca una serie de maravillosos beneficios para la salud, la riqueza, la paz y el sentido que esas mentes podrían aportar a la humanidad. Concluye:   

> Las intuiciones humanas básicas de justicia, cooperación, curiosidad y autonomía son difíciles de rebatir y son acumulativas, algo que a menudo no ocurre con nuestros impulsos más destructivos. [...] Estas simples intuiciones, llevadas a su conclusión lógica, conducen finalmente al estado de derecho, la democracia y los valores de la Ilustración. Si no de forma inevitable, al menos como tendencia estadística, es hacia donde ya se dirigía la humanidad. La IA simplemente ofrece una oportunidad para llegar allí más rápidamente, para hacer la lógica más patente y el destino más claro.  

Es una forma extraña de presentar la creencia de que estás creando una tecnología que, en mi opinión, tiene entre un 10 % y un 25 % de posibilidades de ser catastrófica para la civilización, *incluso teniendo en cuenta* los enormes beneficios potenciales en caso de éxito. Incluso si los niveles de peligro son tan bajos como cree Amodei, deberíamos afanarnos por encontrar una tercera alternativa, aparte de «no proceder» y «precipitarse». Y si uno cree que está *obligado* a precipitarse (porque otras personas ya lo están haciendo), debería suplicar a los líderes mundiales que pongan fin a esta carrera suicida, para que se pueda encontrar esa tercera alternativa. Pintar un panorama optimista parece una mera distracción, cuando se trata de jugarse la vida de todos.

[^195]:  Por ejemplo, en un [testimonio ante el Congreso](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf):  

> Al igual que los automóviles o los aviones, deberíamos considerar los modelos de IA del futuro próximo como máquinas potentes de gran utilidad, pero que pueden ser letales si se diseñan mal o se usan de forma impropia. […] Los nuevos modelos de IA deberían superar una batería de pruebas de seguridad rigurosas tanto durante su desarrollo como antes de su lanzamiento al público o a los clientes. […] Sin embargo, lo ideal sería que los estándares catalizaran la innovación en seguridad en lugar de ralentizar el progreso.  

Agradecemos a Amodei que haya dejado claro que cree que hay peligros que deben abordarse. Eso es un paso más allá de lo que harían muchos ejecutivos de empresas. Pero comparar una tecnología que, en su opinión, tiene entre un 10 % y un 25 % de posibilidades de provocar una catástrofe a nivel de la civilización con los coches y los aviones parece poco sincero.

[^196]:  En un gran número de casos, los laboratorios de IA están trabajando activamente *en contra* de compartir un panorama útil y completo de la situación con los responsables de políticas. En ese contexto, parece especialmente extraño justificar el desarrollo continuo con el argumento de que una IA más potente podría «convencer a los legisladores».

[^197]:  \**solo presentar pruebas:* Para ver un ejemplo de alguien que hace una propuesta como esta (al tiempo que debate algunas de las cuestiones), véase el artículo de Nick Bostrom sobre [IA Oracle](https://nickbostrom.com/papers/oracle.pdf).

[^198]:  \#Hablamos más sobre esto en los últimos capítulos del libro.

[^199]:  No está claro en qué medida estas señales de advertencia provienen de que la IA esté interpretando un papel —actuando como cree que una IA debe comportarse— y en qué medida de que esté pensando estratégicamente. El hecho de que no podamos distinguir qué señales de advertencia son reales no es alentador; significa que los ingenieros son mucho más propensos a seguir adelante sin más, diciendo: «bueno, esa probablemente no era real». Puede que incluso tengan razón la mayoría de las veces, pero «la mayoría de las veces» no es suficiente cuando un solo fallo es letal.  

Tampoco está claro durante cuánto tiempo seguirá apareciendo este tipo de señal de advertencia. Las IA modernas todavía son lo bastante simples como para confundir ocasionalmente las pruebas con la realidad, pero esta situación no durará para siempre y ya está [empezando a llegar a su fin](https://arxiv.org/html/2505.23836). Una IA que sepa que está siendo sometida a prueba podría dejar de exhibir el comportamiento preocupante en lugares donde los supervisores puedan verlo, aunque la tendencia subyacente persista.

[^200]:  Delaminación por ciclos de presión. En términos sencillos: los esfuerzos de muchas inmersiones separaron las capas del casco, debilitándolo hasta que implosionó.

[^201]: Numéricamente, el transporte aéreo es *tan* seguro que la sociedad en su conjunto podría beneficiarse de que el control del tráfico aéreo relajara los requisitos en aspectos como la formación de los pilotos y el combustible de contingencia, lo que reduciría el costo de los vuelos e induciría a más personas a volar en lugar de conducir, salvando así más vidas en neto.

[^202]:  La cuestión *no* es que las IA reales vayan a tener «funciones de utilidad» expuestas a los programadores para que estos puedan determinarlas a su antojo. De hecho, gran parte del problema de la alineación de la IA —como se analiza en el capítulo 4— es que las IA modernas desarrollan preferencias que nadie ha solicitado ni deseado.  

En cambio, estudiar el caso con funciones de utilidad se parece más a proponer el tipo de ejercicios de física que se encuentran en los libros de texto de matemáticas. Si no sabes cómo modelar una esfera perfecta que rueda por un plano inclinado perfectamente liso y sin resistencia del aire, tendrás aún más dificultades con problemas más realistas. Sobre todo si intentas atraer a investigadores de otros campos para que investiguen un problema que nadie sabe cómo resolver, es útil reducir la cuestión a sus partes más simples y básicas, donde se pueda plantear un acertijo.

[^203]:  O bien frustrar el mecanismo que hay detrás del intercambio; la IA no estaría necesariamente compuesta por código legible.

[^204]:  O, al menos, ese es un modo de fallo que hemos visto en algunas de las ingeniosas ideas propuestas. Se han propuesto muchas ideas ingeniosas, pero este pequeño acertijo resulta ser complicado.

[^205]:  Desde hace tiempo discrepamos del término «control de la IA», porque suena a intentar crear una IA que quiere cosas malas y luego forzarla a hacer el bien de todos modos, mientras que para nosotros el problema consiste más bien en crear una IA que sea amigable desde el principio. Véase también la nota 8 al final del capítulo 4 para un poco más de historia sobre el término «alineación de la IA».

[^206]: Véase también nuestro análisis ampliado (después del capítulo 13) sobre [cómo formar una coalición inclusiva](#mantener-la-coalición-amplia).

[^207]:  \#Véase también una [comparativa entre la alineación de la IA y las armas nucleares](#won’t-ai-differ-from-all-the-historical-precedents?).

[^208]:  \**Ya en tres ocasiones:* El [informe de seguridad](https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.pdf) del INSAG-7 (p. 51\) registra que en Chernóbil se habían intentado pruebas de inercia en 1982, 1984 y 1985, antes de la desastrosa prueba de 1986, que a su vez se retrasó de manera bochornosa hasta el punto de que los operadores [esperaban ser despedidos](https://chernobylcritical.blogspot.com/p/prelude-25-april-1986.html) si no la realizaban. 

[^209]:  \**retrasada tres veces:* Técnicamente «aplazada tres veces y cancelada una vez», según el [Informe de la Comisión Rogers](https://sma.nasa.gov/SignificantIncidents/assets/rogers_commission_report.pdf) (p. 17). Pero uno de los aplazamientos se produjo un mes antes, en respuesta a los retrasos de otra misión, mientras que los otros tres se produjeron en rápida sucesión en los días previos al lanzamiento; son estos tres últimos los que creemos que ejercieron presión sobre los responsables de la NASA, que consideraban que su trabajo era lanzar transbordadores espaciales.

[^210]:  ¿Cómo, si no es mediante una coalición internacional? Recomendaríamos invertir en [la mejora de la inteligencia humana adulta](#*-it-could-help-with-solving-the-alignment-problem.), pero no hace falta estar de acuerdo con esta idea para coincidir en que detener la investigación sobre la ASI es una buena idea.

[^211]: La presidenta de la Comisión Federal de Comercio, Lina Khan, [dijo](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html) en 2023: «Ah, tengo que seguir siendo optimista en este tema. Así que voy a ser prudente y a optar por el menor riesgo... Quizá un quince por ciento [de que la IA nos mate a todos]».

[^212]:  [Algunas](https://x.com/balajis/status/1725890626699628633) [personas](https://x.com/MatthewJBar/status/1958403809249464757) [sostienen](https://x.com/DeryaTR_/status/1958592366652125487) que debemos arriesgarnos ahora, por la oportunidad de salvar a los humanos moribundos de sus muertes naturales a causa del envejecimiento. Los cuerpos humanos son tremendamente complicados, pero con suficientes avances científicos, podríamos resolver muchas de las enfermedades que hoy damos por sentadas, como el cáncer, las cardiopatías y las diversas enfermedades relacionadas con el envejecimiento. Una IA más inteligente que los humanos podría llevarnos a ese punto mucho más rápido. Retrasar la superinteligencia literalmente cuesta vidas.  

O, bueno, *costaría* vidas, si no fuera porque la superinteligencia mata exactamente a las mismas personas.  

De hecho, es muy probable que las personas enfermas y moribundas de hoy en día tengan *mejores posibilidades de sobrevivir* si la humanidad se aleja del abismo:  

-* La investigación biomédica y la búsqueda de tratamientos y curas pueden continuar sin necesidad de una superinteligencia. La terapia génica, las vacunas contra el cáncer y otros nuevos enfoques son muy prometedores, y los investigadores apenas están empezando a explorar su potencial.  
* La tecnología de IA con un enfoque limitado puede incluso ayudar a acelerar este esfuerzo, sin necesidad de poner en peligro toda la empresa humana mediante la creación de una IA general más inteligente que los humanos.  
* Los métodos de [preservación del cerebro](https://www.alcor.org/membership/) pueden utilizarse para preservar a las personas incluso después de que su corazón deje de latir, hasta que la ciencia médica avance hasta el punto de poder revivirlas y restaurar su salud. El tipo de IA que podría ofrecer la inmortalidad también podría, casi con toda seguridad, restaurar a alguien a partir de un cerebro preservado adecuadamente.  

(Más discretamente, un subconjunto de estas personas [te dirá](https://x.com/SottoNocce/status/1771420351265923137) que lo hacen por su propia inmortalidad personal y que están dispuestas a arriesgar la vida de todos los adultos y niños del planeta, incluso por una pequeña posibilidad de que ellos y sus seres queridos puedan lograrla. Esto nos parece una villanía digna de un villano de película. A estos villanos les recomendamos lo mismo que a los altruistas: inscríbanse en un programa de conservación cerebral. Les dará más posibilidades que una superinteligencia descontrolada, ¡y además evitarán poner en grave peligro a todos los seres humanos vivos en su búsqueda de la inmortalidad! Todos salen ganando).  

Incluso si solo nos preocupara el bienestar de los enfermos y los moribundos, arriesgarse con alguna combinación de estos métodos parece una opción mejor que arriesgarse a crear una IA superhumana y esperar que le gustemos. (Y que le gustemos de [la forma adecuada](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.h8caxjs4v1jv).) Los dados para la IA sobrehumana están dramáticamente cargados en nuestra contra.  

Pero además: que sepamos, nadie les ha preguntado realmente a los enfermos y moribundos si *quieren* poner a sus familias y compatriotas en grave peligro para jugársela por una posible cura derivada de la superinteligencia. Y, desde luego, a las familias y compatriotas en cuestión tampoco se les ha preguntado si consienten en que sus vidas se pongan en juego por este experimento científico demencial.  

No tenemos por qué apostar nuestras vidas a esta opción, cuando existen muchas otras.  

Imploramos a cualquiera que se preocupe por el bienestar de las personas hoy en día que, en su lugar, acelere los métodos anteriores, y que se mantenga lo más alejado posible de todo lo que pudiera acercarnos, incluso de forma incremental, a la superinteligencia artificial.  

Si simplemente no crees que una superinteligencia descontrolada nos mataría, es una cosa. Pero aceptar que probablemente nos mataría a todos y aun así decir que tenemos que arriesgarnos es una locura. Hay otras opciones para resolver los problemas del mundo moderno. Por analogía: si vivir en un entorno de gran altitud te hace sentir incómodo, eso no es excusa para saltar por un precipicio. Busca otro camino para llegar al pie de la montaña.

[^213]:  \**clara línea roja:* Para ver un ejemplo de esta clara línea roja que aparece en la ciencia ficción, véase *Little Fuzzy*, de H. Beam Piper: «Cualquier cosa que hable y haga fuego es un ser inteligente, sí. Esa es la ley. Pero eso no significa que todo lo que no lo haga no lo sea». O véase el episodio *The Measure of a Man* de *Star Trek: The Next Generation*, en el que la inteligencia y la conciencia de sí mismo demostradas por Data, un androide, son suficientes para otorgarle el derecho legal a negarse a ser desmontado.

[^214]:  Según nuestra experiencia, estos escritos tienden a estar cargados de retórica y tener poca sustancia, y a menudo alternan discretamente entre afirmaciones contradictorias según lo que esté de moda o sea políticamente conveniente en cada momento. No nos da la sensación de que sean descripciones honestas y transparentes, *ni siquiera de las perspectivas reales de los jefes de laboratorio*, lo que los hace menos útiles en comparación con leer las opiniones discrepantes de otros. Pero esa es nuestra propia opinión; si te acercas a este tema con una mirada nueva y quieres evaluar por ti mismo si otras partes tienen buenos contraargumentos que no hemos abordado aquí, entonces no tienes por qué fiarte de nuestra palabra sobre cuáles son las mejores fuentes.

[^215]: Si resulta que necesitas la intervención de un director de laboratorio para algo y nos pides consejo, diríamos que la opción *menos* mala es probablemente Demis Hassabis. Entre los principales jefes de laboratorio con los que al menos uno de nosotros se ha relacionado —que, en 2025, son todos—, Hassabis es el único que hemos visto cumplir sistemáticamente su palabra en sus tratos, y parece haber tomado menos decisiones destructivas.  

Dicho esto, se trata de una recomendación con un bajo nivel de confianza y puramente relativa. En términos absolutos, cualquiera que *no* haya creado una empresa con una probabilidad sustancial de destruir el mundo parte con una gran ventaja de credibilidad frente a los directores de los laboratorios. Sin duda, hemos oído historias de personas que decían que le tenían tanto miedo a Hassabis que no tuvieron más remedio que crear sus propias empresas de IA de vanguardia para adelantársele; es posible que esas personas sepan algo que nosotros no.  

Por lo tanto, nuestra recomendación principal para los responsables de políticas es esta: si están convencidos del peligro, no concedan ninguna potestad a los directores de laboratorio.  

Habla con investigadores independientes, líderes empresariales sin intereses creados o científicos externos con un historial de declaraciones y acciones razonables en este ámbito. No te pongas en una posición en la que puedas salir perjudicado por personas cuya principal característica es que mienten al público y ponen en peligro a la gente.

[^216]:  Personas como Sutton y Page parecen actuar bajo la ilusión de que una mayor inteligencia conduce a una mayor bondad, algo que, [como hemos argumentado en otra parte](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.vwf5j9eovtm8), no es el caso. Y aunque los autores coincidimos con Sutton y Page en que sería una tragedia *nunca* construir una IA más inteligente que los humanos, creemos que la carrera por crear una superinteligencia probablemente sería completamente catastrófica tanto para la vida humana como para el futuro a largo plazo en un sentido más amplio, [incluso desde una perspectiva inclusiva, cosmopolita y no especista](#¿por qué no te importan los valores de ninguna entidad que no sea humana?).

[^217]:  No sería la primera vez que un campo se acostumbra a riesgos innecesariamente altos. En la década de 1980, los anestesistas redujeron sus tasas de mortalidad en un factor de *cien* gracias a la adopción de un sencillo conjunto de normas de monitorización.  

Parece que, durante décadas, los anestesistas causaron cientos de veces más muertes de las necesarias, literalmente sin otra razón que la de considerar que su tasa de mortalidad *ya era baja* (en comparación, por ejemplo, con las tasas de complicaciones quirúrgicas). No se dieron cuenta de que debían *procurar* reducir esa tasa, como informan [Hyman y Silver](https://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1469&context=wlulr):

> Para la década de 1950, las tasas de mortalidad oscilaban entre 1 y 10 por cada 10 000 intervenciones. La mortalidad por anestesia se estabilizó en este nivel durante más de dos décadas.  
>  
> […D]ebemos considerar por qué la mortalidad por anestesia se estabilizó en una tasa más de cien veces superior a su nivel actual durante más de dos décadas. El problema no era la falta de información. Al contrario, la seguridad de la anestesia se estudió exhaustivamente durante ese periodo. Una hipótesis más plausible es que los anestesistas se acostumbraron a una tasa de mortalidad que era ejemplar según los estándares sanitarios, pero que seguía siendo más alta de lo que debería haber sido. Desde una perspectiva psicológica, esta baja frecuencia llevó a los anestesistas a tratar cada mal resultado como un acontecimiento trágico, pero imprevisto e inevitable. De hecho, es probable que los anestesistas consideraran cada mal resultado individual como la manifestación de una tasa básica irreducible de accidentes médicos.

[^218]:  Los ingenieros estructurales basan sus estimaciones de riesgo en cálculos y mediciones precisos, mientras que las cifras de «p(doom)» se basan principalmente en la intuición de los investigadores de IA. Pero esto no aumenta la confianza en las prácticas de ingeniería de los investigadores de IA. En todo caso, empeora la situación.  

Una estimación menos sólida y más subjetiva puede errar sistemáticamente en la dirección de «demasiado pesimista», pero también puede errar en la dirección de «demasiado optimista». El hecho de que estas cifras sean menos fiables no significa que estén *específicamente sesgadas hacia el pesimismo*. El hecho de que los investigadores de la IA no puedan basar sus estimaciones de riesgo en nada más que corazonadas y argumentos cualitativos, *mientras que año tras año desarrollan IA cada vez más inteligentes*, es un motivo más de preocupación.  

El hecho de que las estimaciones de los investigadores en IA sean realmente aterradoras y no tengan precedentes en ninguna disciplina técnica no demuestra que sean erróneas en el sentido que nos gustaría que lo fueran. La carrera por crear agentes autónomos mucho más inteligentes que los humanos parece el tipo de empresa que tiene muchas más posibilidades de provocar una catástrofe que un 50 %. Antes incluso de entrar en detalles, *parece* el tipo de proyecto que muy probablemente saldrá mal de una forma u otra, y en el que un fallo podría tener consecuencias enormes. Y los detalles, como hemos argumentado en los capítulos 4 y 5 y a lo largo de todo el libro, pintan un panorama aún más sombrío de lo que sugiere esta primera impresión.

[^219]:  \#Véase también nuestro análisis sobre [las personas que advertían sobre una carrera hacia el abismo de la IA](#¿no-es-esto-solo-alarmismo-por-parte-de-los-líderes-de-la-ia-para-aumentar-su-estatus-y-conseguir-más-inversiones?) años antes de que se crearan estas empresas.

[^220]:  Ampliando este ejemplo: cuando la cabina del Apolo 1 se incendió durante una simulación de lanzamiento el 27 de enero de 1967, la NASA fue capaz de aprender del error. Los ingenieros comprendían cada componente del cohete y pudieron diagnosticar que el problema probablemente estaba relacionado con el uso de cable de cobre plateado (cuyo aislamiento se había desgastado por el movimiento de la puerta) cerca de una línea de refrigeración de etilenglicol/agua propensa a las fugas. Pudieron determinar que esto se vio agravado por la atmósfera de oxígeno puro de la cápsula y los materiales inflamables de la cabina. Además, la presurización de la cabina significaba que era necesario ventilarla antes de poder abrir la escotilla, pero los controles de ventilación se encontraban detrás del incendio y la diferencia de presión se vio drásticamente agravada por el fuego.  

Fallecieron los tres tripulantes del Apolo 1.  

Este tipo de errores son comunes, incluso cuando hay vidas en juego. Son comunes incluso para los ingenieros de cohetes que trabajan con dispositivos que explotan visiblemente en la plataforma de lanzamiento con frecuencia, incluso entre personas que proceden con cuidado y se toman sus responsabilidades muy en serio.  

Lo que diferencia a los científicos de los alquimistas no es que nunca cometan errores, sino que pueden elaborar planes que están tan cerca de funcionar que pueden *aprender* de los fracasos iniciales. Los alquimistas solían ver cómo sus colegas se volvían locos, pero al no saber qué sustancias eran venenosas, no sabían qué hacer ellos mismos de forma distinta. La NASA, por el contrario, pudo rastrear las causas probables del problema y construir una nueva nave espacial que funcionó en quince de las dieciséis misiones siguientes (siete de las cuales intentaron un alunizaje, y una de ellas fracasó. La misión fallida, el Apolo 13, también sufrió problemas en la cabina que fácilmente podrían haber sido mortales, aunque el dominio de la NASA de los sistemas que había diseñado y la pericia de los astronautas a bordo permitieron su regreso seguro a la Tierra).  

El Apolo 1 era *casi* un cohete funcional. Todo el entramado de ingenieros y científicos meticulosos que lo rodeaba constituía *casi* el tipo de operación que podía llegar a la Luna de forma segura, de modo que un gran error bastó para empujar a la NASA hacia una configuración capaz de conseguir seis de siete alunizajes.  

Las empresas modernas de IA no están ni mucho menos a la altura de ese nivel de [respeto](#sabemos-cómo-es-cuando-un-problema-se-trata-con-respeto,-y-este-no-lo-es) por el problema, ese nivel de cuidado y detalle en sus planes, ese nivel de cercanía a hacer bien el trabajo. Cuando su IA hace algo que no entienden, no están ni mucho menos en condiciones de rastrearlo hasta el equivalente de los cables plateados. No están lo suficientemente cerca como para aprender de sus errores.  

No están tratando el problema como se haría en un campo incipiente como el de los controladores aéreos, los científicos espaciales o los especialistas nucleares: presentando propuestas cuidadosas con [supuestos de seguridad](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) explícitos y sin hacer nada peligroso hasta tener teorías lo suficientemente desarrolladas como para poder al menos aprender de sus fracasos.

[^221]:  [No recomendamos](#why-not-use-international-cooperation-to-build-ai-safely,-rather-than-to-shut-it-all-down?) una coalición internacional de IA, pero es el tipo de entidad que, en teoría, podría dar lugar a una entidad equivalente a la NASA o la FAA, capaz de aprender realmente de los errores de la industria.

[^222]:  \**con pocos beneficios:* Véase, por ejemplo, [este artículo de 2018](https://pmc.ncbi.nlm.nih.gov/articles/PMC7119956/) o un [análisis de riesgos y beneficios](https://osp.od.nih.gov/wp-content/uploads/2015/12/Risk%20and%20Benefit%20Analysis%20of%20Gain%20of%20Function%20Research%20-%20Draft%20Final%20Report.pdf) mucho más exhaustivo de 2015\.

[^223]: \**continúa en gran medida sin restricciones:* A partir de 2025, Estados Unidos parece inclinarse por [dejar de financiar activamente](https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-127.html) la investigación en ganancia de función con fondos públicos, pero ha habido poca o ninguna coordinación global al respecto. Véase también [este informe](https://cset.georgetown.edu/publication/understanding-the-global-gain-of-function-research-landscape).

[^224]: Si los laboratorios de biotecnología fueran más eficaces evitando fugas, y si la creación de virus hiperletales produjera de alguna manera (p. ej.) medicamentos hipercurativos, entonces quizá tendría sentido continuar la investigación. Que sepamos, la investigación en ganancia de función no ha arrojado tales resultados positivos, y los biólogos tienden a [recomendar](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Esvelt%20Testimony.pdf) [en contra](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Ebright%20Testimony%20Updated.pdf) [de ella](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Quay%20Testimony.pdf). Sospechamos, pues, que es una de esas raras áreas de investigación de las que la humanidad debería retirarse, porque pone en peligro la vida de muchísimas personas inocentes que no consintieron en que se arriesgaran sus vidas.

[^225]:  Nótese que instalar interruptores de emergencia (*kill switches*) en los chips y establecer protocolos para cerrar los centros de datos no resuelve por sí solo el problema, ya que [es posible que no recibamos disparos de advertencia](#will-there-be-warning-shots?) y [que no respondamos eficazmente a ellos](#humanity-isn’t-great-at-responding-to-shocks.). Sin embargo, es una medida relativamente barata y totalmente factible que podría ayudar en casos marginales en los que el riesgo es *casi* insignificante, pero sería útil tener un mayor margen de seguridad.

[^226]: Si la sociedad realmente teme que esto ralentice demasiado el mundo, recomendamos acelerarlo en otros ámbitos. Que la gente construya más centrales nucleares. Que los bioquímicos hagan más experimentos, no con virus mortales, sino para que las personas sean más sanas, fuertes e inteligentes.  

(Por supuesto, la sociedad en general no clama por la ciencia loca, sino que más bien se resiste al cambio del *statu quo*. Pero a quienes dicen «no podemos detener la IA porque es importante para el progreso de la civilización», la respuesta correcta es que hay mucho progreso por hacer en otros ámbitos, con el tipo de ciencia loca que deja supervivientes).

[^227]:  Ten en cuenta que abogamos por tratados por los que los *gobiernos* tampoco puedan desarrollar superinteligencia. No pedimos que una tecnología poderosa sea desarrollada por actores estatales en lugar de empresas, sino que una tecnología letalmente peligrosa *no se desarrolle en absoluto*, al menos en un mundo como el actual.

[^228]:  Es posible que, por ejemplo, los investigadores encuentren métodos más eficientes estudiando los modelos de lenguaje a gran escala (LLM) existentes hasta comprender mejor su funcionamiento.  

¿Podrían investigaciones como esta permitir a la gente diseñar IAs en lugar de cultivarlas? ¡Podría ayudar! Por desgracia, prevemos que, mucho antes de que la gente desarrolle una comprensión completa y correcta de lo que ocurre en los LLM, ya habrá desarrollado una comprensión parcial e incompleta que le permitirá construir IA mucho más eficientes, pero que no será suficiente para alinearlas.

[^229]:  [La volición extrapolada coherente](https://baserates-test.vercel.app/w/coherent-extrapolated-volition-alignment-target) es nuestro propio intento de responder a la pregunta «¿alineados con quién?» si llegamos a un punto en el que los creadores de IA tengan cierta capacidad para orientarlos. La volición extrapolada coherente intenta resolver los desacuerdos morales y metamorales principalmente encargando a la IA la tarea de identificar los puntos en los que las personas convergerían si supieran más, si fueran más del tipo de persona que desearían ser, etc. (a la manera de las [teorías del observador ideal](https://en.wikipedia.org/wiki/Ideal_observer_theory) en ética), y buscando metaprincipios compartidos a los que la IA pueda recurrir en casos en los que exista un desacuerdo moral verdaderamente fundamental. (Donde el objetivo no es necesariamente que la IA «resuelva todos los problemas» de la vida humana, sino que resuelva *suficientes* problemas como para que el resultado final no sea catastróficamente malo). Recomendamos extrapolar la volición de todos los seres humanos vivos, no porque pensemos que sea una especie de ideal, sino porque es el punto de coordinación predeterminado obvio en torno al cual pueden ponerse de acuerdo muchas partes interesadas en desacuerdo (y porque otras entidades que preocupan a los seres humanos vivos obtienen cierta influencia a través de la volición de esos seres humanos vivos; y lo mismo ocurre con otras entidades que preocuparían a los seres humanos vivos si supieran más y fueran más quienes desearían ser, etc.).  

Pero, para insistir: consideramos que este tema es principalmente una distracción hoy en día. No es importante llegar a un acuerdo sobre ninguna de estas ideas filosóficas de tan alto nivel para poder tomar medidas respecto a una tecnología que va camino de matarnos a todos. Sería una insensatez profunda dejar que los esfuerzos de no proliferación se vean obstaculizados por gente que debate ideas brillantes como estas, incluidas aquellas que a nosotros, los autores, personalmente nos gustan.  

No obstante, mencionamos brevemente esta propuesta, solo para dejar claro que no estamos tratando de eludir la cuestión y, tal vez, para tranquilizar a los lectores que temen que sea imposible llegar a una propuesta viable. Incluso si la volición extrapolada coherente es un enfoque de alto nivel erróneo por alguna razón, el hecho de que recoja muchas propiedades deseables debería inspirar cierta esperanza de que es posible encontrar una respuesta no catastrófica a esta cuestión.

[^230]:  Lo cual [no es, de hecho, muy](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7agua), contrariamente a lo que [se informa](https://fortune.com/article/how-much-water-does-ai-use/) [de forma generalizada](https://www.thetimes.com/uk/technology-uk/article/thirsty-chatgpt-uses-four-times-more-water-than-previously-thought-bc0pqswdr).

[^231]: Aunque varios funcionarios de Washington han coincidido con nuestras preocupaciones sobre la superinteligencia, carecen del poder necesario para resolver este problema a menos que se involucren muchos más funcionarios de Estados Unidos y de otros países. Las primeras conversaciones han sido prometedoras, pero aún queda mucho trabajo por hacer.

[^232]: Al dar los últimos retoques a estos recursos en línea, los autores hemos observado una señal prometedora: varios profesionales de la seguridad nacional y antiguos funcionarios de Washington D. C. han reaccionado positivamente a las copias anticipadas de *If Anyone Builds It, Everyone Dies*. Algunos ejemplos son:

Ben Bernanke, premio Nobel y ex presidente de la Reserva Federal: «Un relato claro y convincente sobre los riesgos existenciales que la IA altamente avanzada podría plantear a la humanidad. Recomendado».


Jon Wolfsthal, director de Riesgos Globales de la Federación de Científicos Americanos y ex asistente especial del presidente para Asuntos de Seguridad Nacional: «Un argumento convincente de que la IA superhumana conduciría con casi total seguridad a la aniquilación global de la humanidad. Los gobiernos de todo el mundo deben reconocer los riesgos y tomar medidas colectivas y eficaces».

Del teniente general John N.T. «Jack» Shanahan (Fuerza Aérea de los Estados Unidos, retirado), director inaugural del Centro Conjunto de IA del Departamento de Defensa: «Aunque soy escéptico respecto a que la trayectoria actual del desarrollo de la IA conduzca a la extinción humana, reconozco que esta opinión puede reflejar una falta de imaginación por mi parte. Dado el ritmo exponencial de cambio de la IA, no hay mejor momento para tomar medidas prudentes que protejan contra los peores resultados. Los autores ofrecen propuestas importantes para establecer salvaguardias globales y mitigar los riesgos que merecen ser consideradas seriamente.»

De Fiona Hill, exdirectora sénior del Consejo de Seguridad Nacional de la Casa Blanca: «Un libro serio en todos los aspectos. En el escalofriante análisis de Yudkowsky y Soares, una IA superpoderosa no tendrá necesidad de la humanidad y tendrá amplia capacidad para eliminarnos. *Si alguien la construye, todos moriremos* es un elocuente y urgente llamamiento para que nos alejemos del borde de la autodestrucción».

De R.P. Eddy, exdirector del Consejo de Seguridad Nacional de la Casa Blanca: «Esta es nuestra advertencia. Léanlo hoy. Difúndanlo mañana. Exijan salvaguardas. Seguiré apostando por la humanidad, pero primero debemos despertar».

De Suzanne Spaulding, ex subsecretaria del Departamento de Seguridad Nacional: «Los autores plantean una cuestión increíblemente grave que merece —en realidad, exige— nuestra atención».

De Emma Sky, investigadora principal de la Escuela Jackson de Asuntos Globales de Yale y exasesora política del comandante general de las fuerzas estadounidenses en Irak: «En *If Anyone Builds It, Everyone Dies*, Eliezer Yudkowsky y Nate Soares lanzan una advertencia cruda y urgente: la humanidad se precipita hacia la creación de una superinteligencia sin las salvaguardias necesarias para sobrevivir a ella. Con credibilidad, claridad y convicción, sostienen que los sistemas avanzados de inteligencia artificial, si se desalinean aunque sea ligeramente, podrían significar el fin de la civilización humana. Este provocativo libro desafía a los tecnólogos, a los responsables políticos y a los ciudadanos por igual a enfrentarse a los riesgos existenciales de la inteligencia artificial antes de que sea demasiado tarde. Un llamamiento a la concienciación y a la precaución, esta es una lectura esencial para cualquiera que se preocupe por el futuro».

[^233]: A veces se utilizan para otras tareas de cómputo intensivo, como simulaciones físicas y meteorológicas, pero se utilizan *principalmente* para la IA. Un método rápido para estimar cuántos chips de IA se utilizan para actividades no relacionadas con la IA es observar los [ingresos a lo largo del tiempo](https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue) del principal fabricante de chips, NVIDIA. Si asumimos que el reciente auge de la demanda de sus GPU para centros de datos se debe casi en su totalidad a usos de IA —una suposición razonable, dado el enorme auge reciente de la industria de la IA y la falta de una tendencia comparable en otros campos que utilizan estos chips—, concluiríamos que la IA representa la gran mayoría del uso de chips de IA, ya que el reciente crecimiento de los ingresos eclipsa los ingresos anteriores. Impedir la fabricación de chips de IA especializados no tiene por qué tener un gran efecto en el hardware de consumo.

[^234]:  \**difícil de ocultar:* Es posible generar energía in situ, eliminando así cualquier línea de transmisión visible. El actual complejo de Cheyenne Mountain utiliza generadores diésel y probablemente tiene la [capacidad para suministrar energía](https://www.af.mil/News/Article-Display/Article/497017/airmen-operate-americas-fortress/#:~:text=Cada%20uno%20de%20los%20seis%20generadores,tiene%20un%20departamento%20de%20bomberos%20dedicado.) a alrededor de 10 000 de los chips de IA más avanzados. Pero el funcionamiento continuo de estos chips para un entrenamiento a gran escala requeriría un suministro constante de combustible, lo que sería notable. Según cálculos aproximados, estos 10 000 chips requerirían aproximadamente un camión cisterna al día. Incluso si existiera la capacidad de generación local para alimentar 200 000 chips, se necesitarían 20 camiones cisterna de diésel al día.  

Los centros de datos también podrían funcionar con energía nuclear. Afortunadamente, muchos actores estatales ya tienen experiencia en la supervisión de la construcción de nuevas centrales nucleares.

[^235]: \**Progreso algorítmico:* [Ejemplos](https://arxiv.org/abs/2507.10618) de este tipo de progreso incluyen [FlashAttention](https://arxiv.org/abs/2205.14135), un algoritmo que hace que los chips de IA ejecuten un determinado conjunto de operaciones matemáticas de forma más eficiente aprovechando los detalles del diseño de los chips de IA; [Mixture-of-Experts](https://arxiv.org/abs/1701.06538), un cambio en la arquitectura de las IA que hace que solo se utilice un subconjunto de sus parámetros en cada token de entrada (por ejemplo, una palabra); y [GRPO](https://arxiv.org/abs/2402.03300), un método para el ajuste fino de las IA.

[^236]: Otra posible intervención, suponiendo que el número de investigadores del progreso algorítmico de la IA siga siendo reducido (es decir, de cientos o miles), sería pagar a estos investigadores para que orientaran sus esfuerzos hacia usos no relacionados con la IA o hacia la investigación sobre capacidades o alineación de la IA que tuviera un riesgo agregado insignificante. Existe un precedente de este tipo de intervención en la década de 1990, cuando el Gobierno de los Estados Unidos puso en marcha una iniciativa para canalizar el trabajo de antiguos científicos y técnicos armamentísticos soviéticos [hacia actividades productivas y no militares](https://www.armscontrol.org/act/1999-03/features/maintaining-proliferation-fight-former-soviet-union#:~:text=One%20of%20the%20earliest,productive%2C%20non%2Dmilitary%20endeavors).

[^237]: Es posible que los Estados nación preocupados por la superinteligencia artificial prefieran dar primero pasos más pequeños, por ejemplo, pasos que no paralicen todavía la investigación y el desarrollo de la IA, pero que mantengan abierta la opción de paralizar la I+D en IA en el futuro. No recomendamos esa línea de actuación, porque creemos que la situación ya está claramente fuera de control y no confiamos en que vaya a aclararse mucho más antes de que sea demasiado tarde (#will-there-be-warning-shots?). No obstante, el equipo de gobernanza técnica del MIRI está trabajando en propuestas para esos escenarios, en caso de que sean de utilidad. Se puede seguir su trabajo [aquí](https://techgov.intelligence.org/research).

[^238]:  Este es el caso de los acuerdos sobre armas nucleares, en los que distintos tratados establecen el OIEA ([1956](https://www.iaea.org/about/overview/statute), por la Conferencia sobre el Estatuto del Organismo Internacional de Energía Atómica, celebrada en la sede de las Naciones Unidas), el TNP ([1970](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf), a través de negociaciones en el Comité de las Dieciocho Naciones sobre el Desarme), y los acuerdos de control de armas como el tratado START ([1991](https://media.nti.org/documents/start_1_treaty.pdf), tras nueve años de negociaciones intermitentes entre Estados Unidos y la Unión Soviética).

[^239]:  El [Tratado de No Proliferación Nuclear](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf) (comúnmente conocido como «Tratado de No Proliferación») entró en vigor en 1970 y se prorrogó indefinidamente en 1995. Conocido por su adhesión casi universal (191 Partes), su preámbulo subraya el peligro mundial de la proliferación de armas, al tiempo que afirma que los beneficios de las aplicaciones nucleares pacíficas deberían estar a disposición de todas las Partes.

[^240]:  En general, se atribuye al TNP el haber mantenido el número de Estados nucleares por debajo de lo que podría haber sido, pero aun así se han producido adquisiciones por parte de no signatarios (India, Pakistán, Israel) y antiguos signatarios (Corea del Norte). La creación de un solo ASI por parte de cualquier no signatario representa un peligro comparable a un intercambio termonuclear masivo, y debe ser tratada en consecuencia.

[^241]:  La Convención sobre prohibiciones o restricciones del empleo de ciertas armas convencionales que puedan considerarse excesivamente nocivas o de efectos indiscriminados, comúnmente conocida como CCW, entró en vigor en 1983. En 2024, sus 128 partes se comprometen a proteger a los combatientes y no combatientes de sufrimientos innecesarios y atroces mediante la restricción de diversas categorías de armas.

[^242]:  Esto es el doble del límite que se menciona en el libro como claramente seguro. Es probable que siga siendo seguro durante algún tiempo más, y determinar dónde deben estar los límites (y modificarlos a lo largo del tiempo) es el tema de los artículos III, V y XIII.

[^243]: La [Organización para la Prohibición de las Armas Químicas](https://www.opcw.org/our-work) (OPCW) lleva a cabo inspecciones, supervisa la destrucción de arsenales de armas químicas y ayuda en la preparación para ataques con armas químicas, entre otras funciones fundamentales para la [Convención sobre las Armas Químicas](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) (CWC). La CWC entró en vigor en 1997; sus 193 partes trabajan para hacer efectiva y mantener la prohibición del uso, desarrollo y proliferación de armas químicas y sus precursores, con algunas excepciones limitadas.

[^244]:  El OIEA se creó en 1957, más de una década antes del TNP. El TNP pudo designar a este organismo preexistente para desempeñar algunas funciones. En el caso de la inteligencia artificial, aún no existe ningún organismo internacional de este tipo, por lo que nuestro proyecto de tratado debe comprometer a las partes a crear uno.

[^245]:  Los Estados Unidos y la URSS ya habían acordado detener otros tipos de ensayos con armas nucleares en 1963 con el Tratado por el que se prohíben los ensayos con armas nucleares en la atmósfera, el espacio ultraterrestre y bajo el agua, comúnmente denominado [Tratado de Prohibición Parcial de los Ensayos Nucleares](https://2009-2017.state.gov/t/avc/trty/199116.htm) (LTBT) o Tratado de Prohibición de Ensayos.

[^246]:  El [Tratado entre el Imperio Británico, Francia, Italia, Japón y los Estados Unidos de América para la limitación de armamento naval](https://treaties.fcdo.gov.uk/data/Library2/pdf/1924-TS0005.pdf) (el Tratado Naval de Washington) enumera por nombre, en una tabla (Sección II), los buques que deben ser desguazados.

[^247]:  El [Tratado de Reducción de Armas Estratégicas](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html) se firmó en 1991 y entró en vigor en 1994. A cada signatario se le prohibió desplegar más de 6 000 ojivas nucleares en un total de 1 600 misiles balísticos intercontinentales y bombarderos.

[^248]: El [Plan de Acción Integral Conjunto](https://www.europarl.europa.eu/cmsdata/122460/full-text-of-the-iran-nuclear-deal.pdf) se alcanzó en 2015 entre los cinco miembros permanentes del Consejo de Seguridad de las Naciones Unidas, Alemania, la Unión Europea e Irán. Cuando entró en vigor en enero de 2016, Irán obtuvo el levantamiento de las sanciones y otras disposiciones a cambio de aceptar [restricciones a su programa nuclear](https://2009-2017.state.gov/documents/organization/245318.pdf).

[^249]: Las conversaciones sobre la limitación de armas estratégicas (SALT), que comenzaron en 1969 entre Estados Unidos y la URSS, produjeron el [tratado SALT I](https://treaties.un.org/doc/Publication/UNTS/Volume%20944/volume-944-I-13445-English.pdf), firmado en 1972, que congeló el número de lanzadores de misiles balísticos estratégicos y reguló la incorporación de nuevos misiles balísticos lanzados desde submarinos, entre otras restricciones.

[^250]:  El [Tratado sobre Misiles Antibalísticos](https://en.wikisource.org/wiki/Anti-Ballistic_Missile_Treaty) (ABM) de 1972 surgió de las conversaciones originales del SALT y limitaba a cada parte a dos complejos antibalísticos (posteriormente, solo uno), con restricciones sobre su armamento y capacidades de seguimiento.

[^251]: Con el [Tratado sobre Fuerzas Nucleares de Rango Intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm) de 1987, los EE. UU. y la URSS acordaron prohibir la mayoría de los sistemas de lanzamiento de armas nucleares con un alcance intermedio entre el de los sistemas para el campo de batalla y el de los intercontinentales. (Dado el breve tiempo de alerta que permitirían los ataques con estos sistemas, se los consideraba más sistemas ofensivos desestabilizadores que activos para la defensa.)

[^252]:  Otra consideración clave para las medidas de verificación del uso de chips es la seguridad y la privacidad. Las partes querrán asegurarse de que el OIEA solo tenga acceso a la información necesaria para la verificación, sin acceder también a datos sensibles de los chips (como secretos militares o datos de usuario confidenciales). Por lo tanto, los métodos de verificación utilizados tendrían que ser seguros y, en la medida de lo posible, de alcance limitado.

[^253]:  El Servicio de Inocuidad e Inspección de los Alimentos (FSIS) es una agencia del Departamento de Agricultura de los Estados Unidos creada en 1977.

[^254]:  La Ley de Energía Atómica de 1946 fue posteriormente ampliada por la [Ley de Energía Atómica de 1954](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf) con el fin de desarrollar una industria nuclear civil, para lo cual fue necesario permitir que ciertos datos restringidos se compartieran con empresas privadas.

[^255]:  El caso de 1979 [*Estados Unidos contra The Progressive*](https://en.wikipedia.org/wiki/United_States_v._Progressive,_Inc.), en el que un periódico pretendía revelar el «secreto» de la bomba de hidrógeno, podría haber dado al Tribunal Supremo de los Estados Unidos la oportunidad de pronunciarse sobre si la doctrina del «secreto innato» viola las protecciones de la Primera Enmienda sobre la libertad de expresión, si el gobierno no hubiera desestimado el caso por considerarlo abstracto.

[^256]:  Una rama del Departamento de Comercio de los Estados Unidos.

[^257]:  A lo largo de las décadas, se han dictado cientos de órdenes de este tipo sobre patentes relacionadas con la criptografía.

[^258]: En una entrevista de 2025 (https://www.theverge.com/decoder-podcast-with-nilay-patel/761830/amazon-david-luan-agi-lab-adept-ai-interview?utm_source=chatgpt.com), David Luan, director del laboratorio de investigación IAG de Amazon, estimó que el número de personas a las que confiaría «una cantidad gigante de dólares en poder de cómputo» para desarrollar un modelo de vanguardia sería «inferior a 150».

[^259]:  El Centro Internacional de Ciencia y Tecnología surgió del programa [Nunn-Lugar Cooperative Threat Reduction](https://en.wikipedia.org/wiki/Nunn%E2%80%93Lugar_Cooperative_Threat_Reduction) de 1991, una iniciativa estadounidense para poner a buen recaudo y desmantelar las armas de destrucción masiva y su infraestructura asociada en las antiguas repúblicas soviéticas.

[^260]: Las partes de nuestro tratado tal vez deseen explorar la posibilidad de ampliar el concepto de [crímenes de lesa humanidad](https://www.law.cornell.edu/wex/crime_against_humanity) (tipificado en el [Estatuto de Roma de la Corte Penal Internacional de 1998](https://en.wikisource.org/wiki/Rome_Statute_of_the_International_Criminal_Court)) a los casos en los que un investigador busca deliberadamente desarrollar ASI a expensas de la humanidad.

[^261]:  El [Comité de Sistemas de Seguridad Nacional](https://en.wikipedia.org/wiki/Committee_on_National_Security_Systems) (CNSS) es una organización intergubernamental estadounidense que establece políticas de seguridad para los sistemas de información gubernamentales.

[^262]:  144 Estados, a junio de 2025.

[^263]:  VII.F establece que «\[...\] sin perjuicio de sus responsabilidades ante la Agencia, \[el Director General y el personal\] no revelarán ningún secreto industrial ni otra información confidencial de la que tengan conocimiento en el ejercicio de sus funciones oficiales para la Agencia».

[^264]: A veces, los tratados son sustituidos por otros. Tal fue el caso del [Acuerdo General sobre Aranceles Aduaneros y Comercio (GATT)](https://www.wto.org/english/docs_e/legal_e/gatt47_e.htm) de 1947, que fue sustituido por el [Acuerdo de Marrakech](https://www.wto.org/english/docs_e/legal_e/marag_e.htm) de 1994; este último incorporó las normas del GATT, pero estableció la Organización Mundial del Comercio (OMC) para reemplazar su estructura institucional. Los tratados de duración ilimitada también pueden terminar si las Partes se retiran de manera que el tratado pierda su eficacia. Por ejemplo, EE. UU. y la URSS acordaron inicialmente el [Tratado sobre Fuerzas Nucleares de Rango Intermedio (INF)](https://2009-2017.state.gov/t/avc/trty/102360.htm) de 1987 con una duración ilimitada, pero EE. UU. se retiró en 2019 alegando el incumplimiento por parte de Rusia, y Rusia anunció posteriormente que dejaría de acatar el tratado en 2025.
