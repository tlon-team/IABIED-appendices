---
original_path: IABIED - Online resources(1).md
---
# Introducción: Decisiones difíciles y decisiones fáciles {#introducción:-decisiones-difíciles-y-decisiones-fáciles}

Este es el primer suplemento en línea de [*If Anyone Builds It, Everyone Dies*](https://www.amazon.com/gp/product/0316595640) (Si alguien lo construye, todos mueren), de Yudkowsky y Soares. Cada capítulo responde a preguntas comunes e incluye debates ampliados que profundizan en los antecedentes y detalles que no eran esenciales para el libro.

Estos recursos contienen mucho contenido y no están diseñados para leerse de principio a fin. Escribimos el libro para que fuera autónomo y cubriera todo el argumento central. Pero si tienes alguna pregunta, objeción o curiosidad específica que el libro no haya podido satisfacer, es muy probable que encuentres más información al respecto aquí. Si echas en falta algo importante, solicítalo \[aquí\](TODO).

## Preguntas frecuentes {#preguntas-frecuentes}

### ¿Por qué escribir un libro sobre la IA superhumana como amenaza de extinción? {#por-qué-escribir-un-libro-sobre-la-ia-superhumana-como-amenaza-de-extinción?}

#### **Porque la situación parece realmente grave y urgente.** {#porque-la-situación-parece-realmente-grave-y-urgente.}

Si se analiza un tema con suficiente atención, a veces se puede prever uno de los giros que dará la historia.

En 1933, el físico Leo Szilard fue el primero en darse cuenta de que las reacciones nucleares en cadena eran posibles.[^1] Esto le permitió predecir uno de los giros de la historia antes que nadie.

Creemos que si se observa la IA desde la perspectiva adecuada, hoy es posible prever uno de los giros que dará la historia. Y creemos que las cosas irán mal si la humanidad no cambia de rumbo.

Los laboratorios de IA compiten por construir máquinas más inteligentes que cualquier ser humano y parecen estar logrando avances significativos en este campo. Como veremos en los próximos capítulos, las IA modernas son más «desarrolladas» que «creadas». Muestran comportamientos que nadie ha pedido ni deseado, y están en camino de superar la capacidad de cualquier ser humano. Esto nos parece una situación extremadamente peligrosa.

Los principales científicos del campo se reunieron para firmar una [carta abierta](https://aistatement.com/) en la que advertían al público que la amenaza de la IA debía tratarse como una «prioridad mundial, junto a otros riesgos a escala societal como las pandemias y la guerra nuclear». No se trata de una preocupación aislada, sino que es compartida por [casi la mitad de los expertos en la materia](#expertos-en-ia-sobre-escenarios-catastroficos). Aunque al principio se muestre escéptico ante estos peligros, esperamos que el nivel de preocupación de los expertos en IA, y lo mucho que está en juego si sus inquietudes son acertadas, deje claro por qué este es un tema que merece un debate serio.

Este es un tema en el que debemos sopesar los argumentos en lugar de simplemente seguir nuestra primera intuición. Si las cartas y advertencias son correctas, el mundo se encuentra en una situación increíblemente peligrosa. Dedicaremos el resto del libro a exponer los argumentos y las pruebas que respaldan tales advertencias.

No creemos que la situación sea desesperada. Hemos escrito este libro con la esperanza de cambiar la trayectoria que parece seguir la humanidad, porque creemos que es posible encontrar una solución.

El primer paso para resolver un problema es comprenderlo.

### ¿Están sugiriendo que ChatGPT podría matarnos a todos? {#¿están-sugiriendo-que-chatgpt-podría-matarnos-a-todos?}

#### **No. La preocupación se centra en los próximos avances en IA.** {#no.-la-preocupación-se-centra-en-los-próximos-avances-en-ia.}

Una de las razones por las que estás leyendo este libro ahora es que avances como ChatGPT han llevado la IA a los titulares. El mundo está empezando a debatir el progreso de la IA y su impacto en la sociedad. Esto representa una oportunidad natural para hablar sobre una IA más inteligente que los humanos y por qué la situación actual no parece muy prometedora.

Los autores de este libro llevamos mucho tiempo trabajando en este campo. Los recientes avances en IA influyen en nuestras opiniones, pero nuestras preocupaciones no surgieron a raíz de ChatGPT, ni de los modelos de lenguaje a gran escala que lo precedieron. Llevamos décadas realizando investigación técnica para intentar garantizar que una IA más inteligente que los humanos opere de forma segura (Soares desde 2013 y Yudkowsky desde 2001). Sin embargo, recientemente hemos visto indicios de que el mundo podría estar preparado para esta conversación. Una conversación que, con toda probabilidad, *necesitamos* tener ahora, o el mundo podría perder su oportunidad de reaccionar.

El campo de la IA progresa y, con el tiempo (no sabemos cuándo), avanzará hasta el punto de crear una IA más inteligente que los humanos. Ese es el objetivo explícito de las principales empresas de IA:

> Ahora estamos seguros de que sabemos cómo crear una IAG [inteligencia artificial general] tal y como la hemos entendido tradicionalmente. […] Estamos empezando a mirar más allá, hacia la superinteligencia en el pleno sentido de la palabra. Nos encantan nuestros productos actuales, pero estamos aquí por el glorioso futuro. Con la superinteligencia, podremos lograr todo lo demás.  
— [Sam Altman](https://blog.samaltman.com/reflections), director general de OpenAI

> Creo que la IA potente podría llegar ya en 2026. […] Por IA potente, me refiero a un modelo de IA \[…\] con las siguientes propiedades: en términos de inteligencia pura, es más inteligente que un ganador del Premio Nobel en la mayoría de los campos relevantes: biología, programación, matemáticas, ingeniería, escritura, etc. Esto significa que puede demostrar teoremas matemáticos sin resolver, escribir novelas extremadamente buenas, escribir códigos difíciles desde cero, etc.  
— [Dario Amodei](https://www.darioamodei.com/essay/machines-of-loving-grace), director general de Anthropic

> En general, nos centramos en crear inteligencia general completa. Todas las oportunidades que he comentado hoy son consecuencia de ofrecer inteligencia general y hacerlo de manera eficiente.  
— [Mark Zuckerberg](https://www.facebook.com/share/p/16STVBshtn/), director general de Meta (poco antes de que la empresa [anunciara](https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26) un [proyecto de «superinteligencia»](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta) por valor de 14 300 millones de dólares).

> Creo que en los próximos cinco a diez años, habrá quizás un 50 % de posibilidades de que tengamos lo que definiríamos como IAG.  
— [Demis Hassabis](https://youtu.be/CRraHg4Ks_g?feature=shared&amp;t=41), director general de Google DeepMind

> Wes: Entonces, Demis, ¿estás tratando de provocar una explosión de inteligencia?  
Demis: No, no una explosión descontrolada...  
— [Wes Roth (entrevistador) y Hassabis](https://x.com/WesRothMoney/status/1926669591163621789)

Están invirtiendo su dinero en lo que predican. [Microsoft](https://www.reuters.com/technology/artificial-intelligence/microsoft-plans-spend-80-bln-ai-enabled-data-centers-fiscal-2025-cnbc-reports-2025-01-03/), [Amazon](https://www.datacenterdynamics.com/en/news/amazon-2025-capex-to-reach-100bn-aws-revenue-hit-100bn-in-2024/) y [Google](https://www.datacenterdynamics.com/en/news/google-expects-2025-capex-to-surge-to-75bn-on-ai-data-center-buildout/) han anunciado planes para invertir entre 75 000 y 100 000 millones de dólares en centros de datos con IA en 2025. La empresa emergente xAI [compró la red social X.com](https://www.reuters.com/markets/deals/musks-xai-buys-social-media-platform-x-45-billion-2025-03-28/) con una valoración de 80 000 millones de dólares, aproximadamente el doble que la propia X, poco antes de [recaudar 10 000 millones de dólares](https://www.cnbc.com/2025/07/01/elon-musk-xai-raises-10-billion-in-debt-and-equity.html) para financiar un enorme centro de datos y seguir desarrollando su IA, Grok. OpenAI ha anunciado el [Proyecto Stargate](https://openai.com/index/announcing-the-stargate-project/), valorado en 500 000 millones de dólares, en colaboración con Microsoft y otras empresas.

El director general de Meta, Mark Zuckerberg, ha [declarado](https://www.datacenterdynamics.com/en/news/zuckerberg-says-meta-will-spend-hundreds-of-billions-of-dollars-on-ai-infrastructure-over-the-long-term/) que Meta [espera gastar 65 000 millones de dólares](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/) en infraestructura de IA este año, y «cientos de miles de millones» en proyectos de IA en los próximos años. Meta ya ha invertido 14 300 millones de dólares en ScaleAI y ha contratado a su director general para dirigir los nuevos [Meta Superintelligence Labs](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superinteligencia-effort-more-hires), fichando para ello a más de una docena de investigadores de primer nivel de laboratorios rivales[^2] con ofertas de hasta 200 millones de dólares por un solo investigador.

Nada de esto significa que la IA más inteligente que los humanos esté a la vuelta de la esquina. Pero sí significa que todas las grandes empresas están haciendo todo lo posible por crearla, y que las IA como ChatGPT son el resultado de este programa de investigación. Estas empresas no se han propuesto crear chatbots. Se han propuesto crear superinteligencias, y los chatbots son una parada en el camino.

Nuestra opinión, tras décadas de intentar comprender mejor esta cuestión y reflexionar seriamente sobre los avances futuros, es que no existe ninguna barrera fundamental que impida a los investigadores lograr un avance mañana y crear una IA más inteligente que los humanos.

No sabemos si ese umbral se alcanzará en el futuro cercano o si aún queda una década para ello, etc. La historia demuestra que es mucho más difícil predecir cuándo aparecerán las nuevas tecnologías que predecir que se desarrollarán. Pero creemos que la evidencia del peligro es mucho mayor de lo necesario para justificar una respuesta internacional contundente en la actualidad. Por supuesto, ese argumento se esboza en el libro.

### ¿No es cierto que la gente siempre entra en pánico y sobrerreacciona a las cosas? {#¿no-es-cierto-que-la-gente-siempre-entra-en-pánico-y-sobrerreacciona-a-las-cosas?}

#### **Sí. Pero eso no significa que nada sea *realmente* peligroso.** {#sí.-pero-eso-no-significa-que-nada-sea-realmente-peligroso.}

A veces la gente sobrerreacciona a los problemas. Algunas personas son catastrofistas. Algunos pánicos sociales son infundados. Nada de esto significa que vivamos en un mundo perfectamente seguro.

En 1935, Alemania no era un buen lugar para los judíos, los romaníes ni otros grupos de personas. Algunos vieron las señales de advertencia y se marcharon. Otros descartaron las advertencias por considerarlas alarmistas y murieron.

La amenaza de la aniquilación nuclear era real, pero la humanidad estuvo a la altura de las circunstancias y la Guerra Fría nunca estalló.

Los clorofluorocarbonos realmente estaban agujereando la capa de ozono, hasta que fueron prohibidos con éxito por un tratado internacional. Después, la capa de ozono se recuperó.

Algunos peligros de los que se advierte son falsos. Otros son reales.

La humanidad no siempre sobrerreacciona ante los desafíos. Tampoco siempre se queda corta en su reacción. En algunos casos, la humanidad incluso consigue hacer ambas cosas a la vez, por ejemplo, cuando los países construyen enormes acorazados para utilizarlos en la próxima guerra, cuando en realidad deberían haber construido portaaviones. No existe una solución sencilla como «simplemente ignorar todos los supuestos riesgos tecnológicos» o «simplemente asumir que todos los riesgos tecnológicos son reales». Para averiguar qué es cierto, hay que examinar los detalles de cada caso.

(Para más información sobre este tema, consulta la introducción del libro).

### ¿Cuándo se desarrollará este tipo de IA tan preocupante? {#cuándo-se-desarrollará-este-tipo-de-ia-tan-preocupante?}

#### **\* Saber que una tecnología está por llegar no garantiza saber exactamente cuándo lo hará.** {#*-saber-que-una-tecnología-está-por-llegar-no-garantiza-saber-exactamente-cuándo-lo-hará.}

Muchas de las cosas que la gente nos pide que intentemos predecir, en realidad no tenemos forma de saberlas. Cuando Leo Szilard escribió una carta advirtiendo a los Estados Unidos sobre las armas nucleares en 1939, no incluyó ni podía incluir ninguna nota del tipo: «La primera arma atómica estará lista para detonar en pruebas dentro de seis años».

¡Habría sido una información muy valiosa! Pero incluso cuando eres la primera persona en predecir correctamente las reacciones nucleares en cadena, como lo fue Szilard, incluso cuando eres el primero en ver que una tecnología es posible y tendrá consecuencias, no puedes predecir exactamente cuándo llegará esa tecnología.

Hay decisiones fáciles y decisiones difíciles. No pretendemos ser capaces de tomar decisiones difíciles, como cuándo se producirá exactamente el tipo peligroso de IA.

#### **Los expertos no dejan de sorprenderse por la rapidez con la que avanza la IA.** {#los-expertos-no-dejan-de-sorprenderse-por-la-rapidez-con-la-que-avanza-la-ia.}

No saber cuándo llegará la IA no es lo mismo que saber que aún queda mucho tiempo para ello.

En 2021, la comunidad de pronosticadores del sitio web Metaculus [estimó](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) que la primera «IA verdaderamente general» llegaría en 2049. Un año después, en 2022, esa predicción conjunta de la comunidad se había reducido en doce años, hasta 2037. Otro año más tarde, en 2023, se había reducido otros cuatro años, hasta 2033. [Una vez más](https://x.com/slow_developer/status/1947248501743599705) y [otra vez](https://forecastingresearch.org/near-term-xpt-accuracy), los pronosticadores se han visto sorprendidos por el rápido avance de la IA, y sus estimaciones temporales varían enormemente de un año a otro.

Este fenómeno no es exclusivo de Metaculus. Una organización llamada 80.000 Horas [documenta](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/) otros casos en los que muchos grupos de expertos en pronósticos han acortado rápidamente sus plazos. E incluso los superpronosticadores, que ganan constantemente torneos de pronósticos y a menudo superan a los expertos en la materia en su capacidad para pronosticar el futuro, solo asignaron un [2,3 % de probabilidad](https://forecastingresearch.org/near-term-xpt-accuracy) a que las IA consiguieran la medalla de oro de la Olimpiada Internacional de Matemática para el año 2025. Las IA [consiguieron](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) la medalla de oro de la Olimpiada Internacional de Matemática en julio de 2024. 

Intuitivamente, podría parecer que aún faltan décadas para que exista una IA más inteligente que los humanos, pero en 2021 también parecía que faltaban décadas para que existiera una IA del nivel de ChatGPT, y de repente llegó. ¿Quién sabe cuándo llegarán de repente nuevas mejoras cualitativas en la IA? Quizás se necesiten otros diez años. O quizás el avance se produzca mañana. No sabemos cuánto tiempo llevará, pero varios investigadores están cada vez más preocupados por que el tiempo se esté agotando. Sin pretender tener un conocimiento especial en este ámbito, creemos que la humanidad debería reaccionar pronto. No está claro cuántas más advertencias vamos a recibir.

Véase el capítulo 1 para más información sobre las formas en que las capacidades de la IA podrían propagarse con muy poca antelación. Y véase el capítulo 2 para más información sobre los paradigmas modernos de la IA y si serán o no capaces de llegar «hasta el final».

#### **Desconfía de las afirmaciones de los medios de comunicación sobre lo que puede suceder o no en un futuro próximo. (¡Puede que ya haya sucedido!)** {#desconfía-de-las-afirmaciones-de-los-medios-de-comunicación-sobre-lo-que-puede-o-no-suceder-en-un-futuro-próximo-(¡puede-que-ya-haya-sucedido!)}

Dos años después de la [desalentadora predicción](https://www.wright-brothers.org/History_Wing/Wright_Story/Inventing_the_Airplane/Not_Within_A_Thousand_Years/Not_Within_A_Thousand_Years.htm) de Wilbur Wright de que el vuelo a motor tardaría mil años en hacerse realidad, el *New York Times* afirmaba con seguridad que tardaría un millón.[^3] Dos meses y ocho días después, los hermanos Wright volaron.

Hoy en día, los escépticos siguen haciendo afirmaciones exageradas de que la IA nunca podría rivalizar con los humanos en alguna capacidad específica, incluso cuando los recientes avances en el aprendizaje automático demuestran que las IA igualan (o superan) el rendimiento humano en una lista cada vez mayor de *benchmark*. Por ejemplo, se sabe, al menos desde finales de 2024, que las IA modernas a menudo pueden identificar el sarcasmo y la ironía a partir de [texto](https://www.yomu.ai/resources/can-ai-essay-writers-understand-satire-irony-or-sarcasm-in-essays#) e incluso de [señales no verbales](https://dl.acm.org/doi/10.1145/3678957.3685723). Pero esto no impidió que el *New York Times* [repitiese](https://www.nytimes.com/2025/05/16/technology/what-is-iag.html) en mayo de 2025 la afirmación de que «los científicos no tienen evidencia concluyente de que las tecnologías actuales sean capaces de realizar siquiera algunas de las cosas más sencillas que puede hacer el cerebro, como reconocer la ironía».[^4]

Todo esto quiere decir que muchos afirmarán que la IA más inteligente que los humanos es inminente, o que está incalculablemente lejos en el futuro. Pero la incómoda realidad es que nadie lo sabe en este momento.

Peor aún, hay muchas posibilidades de que nadie lo sepa *nunca* hasta que sea demasiado tarde para que la comunidad internacional haga algo al respecto.

Predecir cuándo se producirá el próximo avance tecnológico es increíblemente difícil. Sabemos que una IA más inteligente que los humanos es letalmente peligrosa, pero si además necesitamos saber en qué día de la semana llegará, entonces no hay nada que hacer. Tenemos que ser capaces de actuar en condiciones de incertidumbre, o no actuaremos en absoluto.

### ¿Podemos utilizar los avances del pasado para extrapolar cuándo construiremos una IA más inteligente que los humanos? {#¿podemos-utilizar-los-avances-del-pasado-para-extrapolar-cuando-construiremos-una-ia-mas-inteligente-que-los-humanos?}

#### **No tenemos un conocimiento suficientemente bueno de la inteligencia para eso.** {#no-tenemos-un-conocimiento-suficientemente-bueno-de-la-inteligencia-para-eso.}

Un tipo de predicción acertada consiste en tomar una línea recta en un gráfico, que se ha mantenido estable durante muchos años, y predecir que esta continuará durante al menos uno o dos años más.

Esto no siempre funciona. Las líneas de tendencia a veces cambian. Pero a menudo funciona razonablemente bien; es un caso en el que se hacen predicciones acertadas en la práctica.

El gran problema de este método es que, a menudo, lo que realmente queremos saber no es «¿qué altura alcanzará esta línea en el gráfico en 2027?», sino más bien «¿qué sucederá, cualitativamente, si esta línea sigue subiendo?». ¿Qué altura de la línea se corresponde con resultados importantes en el mundo real?

Y en el caso de la IA, simplemente no lo sabemos. Es bastante fácil elegir alguna medida de inteligencia artificial que forme una línea recta en un gráfico (como la «perplejidad» [https://en.wikipedia.org/wiki/Perplexity]) y proyectar esa línea hacia afuera. Pero nadie sabe qué nivel futuro de «perplejidad» corresponde a qué nivel de habilidad cualitativa para jugar al ajedrez. No se puede predecir de antemano; solo queda ejecutar la IA y averiguarlo.

Nadie sabe dónde cae en ese gráfico la línea «ahora tiene la capacidad de matar a todo el mundo». Lo único que pueden hacer es ejecutar la IA y averiguarlo. Por lo tanto, extrapolar la línea recta del gráfico no nos ayuda. (Y eso incluso antes de que el gráfico se vuelva irrelevante por el progreso algorítmico).

Por esa razón, en el libro no dedicamos tiempo a extrapolar líneas en gráficos para predecir exactamente cuándo alguien dedicará 1027 operaciones de coma flotante al entrenamiento de una IA, o qué consecuencias tendría esto. Es una decisión difícil. El libro se centra en lo que nos parece fácil de predecir. Se trata de un abanico reducido de temas, y nuestra capacidad para hacer un pequeño número de predicciones importantes en ese ámbito limitado no justifica hacer pronósticos arbitrarios sobre el futuro.

### ¿Cuáles son tus incentivos y conflictos de intereses, como autores? {#¿cuáles-son-tus-incentivos-y-conflictos-de-intereses,-como-autores?}

#### En general, no esperamos ganar dinero con el libro. Por otra parte, nos encantaría estar equivocados sobre la tesis del libro.** {#no tenemos la expectativa de ganar dinero con el libro en el caso más habitual.-por otra parte, nos encantaría estar equivocados sobre la tesis del libro.}

Nosotros (Soares y Yudkowsky) cobramos nuestro salario del Machine Intelligence Research Institute (MIRI), que se financia con donaciones de personas que consideran que estas cuestiones son importantes. Quizás el libro impulse las donaciones.

Dicho esto, tenemos otras oportunidades de ganar dinero y no nos dedicamos a escribir libros por dinero. El anticipo que recibimos por este libro se destinó íntegramente a su publicidad, y los derechos de autor irán en su totalidad al MIRI para compensarle el tiempo y el esfuerzo que ha invertido su personal.[^5]

Y, por supuesto, ambos autores estarían encantados de llegar a la conclusión de que nuestra civilización no está en peligro. Nos encantaría simplemente jubilarnos o ganar más dinero en otro lugar.

No creemos que nos costaría cambiar de opinión, si de hecho la evidencia justificara un cambio. Ya ha ocurrido antes. El MIRI se fundó (bajo el nombre de «Singularity Institute») como un proyecto para *construir* superinteligencia. Yudkowsky tardó un año en darse cuenta de que esto no saldría bien *automáticamente*, y un par de años más en darse cuenta de que conseguir que saliera bien sería bastante complicado.

Ya hemos cambiado de rumbo una vez y nos encantaría volver a hacerlo. Simplemente no creemos que la evidencia lo justifique.

No creemos que la situación sea desesperada, pero sí nos parece que hay un problema real y que la amenaza es extrema si el mundo *no* está a la altura de las circunstancias.

También vale la pena destacar que, para averiguar si la IA está en camino de matarnos a todos, hay que pensar en la *IA*. Si solo piensas en las personas, puedes encontrar razones para descartar cualquier fuente: los académicos están desconectados de la realidad; las empresas intentan generar expectación; las organizaciones sin ánimo de lucro quieren recaudar fondos; los aficionados no saben de lo que hablan.

Pero si tomas ese camino, tus creencias finales estarán determinadas por a quién decidas descartar, sin dejar que los argumentos y las pruebas te hagan cambiar de opinión si estás equivocado. Para averiguar la verdad, es indispensable evaluar los argumentos y ver si se sostienen por sí mismos, independientemente de quién los haya planteado.

Nuestro libro no comienza con el fácil argumento de que los ejecutivos de las empresas que dirigen los laboratorios de IA tienen un incentivo para convencer a la población de que las IA son seguras. Comienza analizando la *IA*. Y más adelante en el libro, dedicamos un poco de tiempo a *repasar la historia de los científicos humanos que se han mostrado demasiado optimistas*, pero nunca decimos que debas ignorar el argumento de alguien porque trabaje en un laboratorio de IA. Analizamos algunos de los planes reales de los desarrolladores y por qué esos planes no funcionarían por sus propios méritos. Nos esforzamos por plantear un debate sobre los argumentos reales, porque son los argumentos reales los que importan.

Si crees que estamos equivocados, te invitamos a analizar nuestros argumentos y a señalar los puntos específicos en los que crees que nos hemos equivocado. Creemos que esa es una forma más fiable de averiguar qué es cierto que fijarse principalmente en el carácter y los incentivos de las personas. La persona más parcial del mundo puede decir que está lloviendo, pero eso no significa que haga sol.

### ¿No es todo esto de la IA solo ciencia ficción? {#¿no-es-todo-esto-de-la-ia-solo-ciencia-ficción?}

#### **\* No podemos aprender mucho de la prevalencia de un tema en la ficción.** {#*-no-podemos-aprender-mucho-de-la-prevalencia-de-un-tema-en-la-ficción.}

Aún no se ha creado una IA más inteligente que los humanos, pero sí se ha representado en la ficción. Sin embargo, recomendamos no anclarse en estas representaciones. Es probable que la IA real no se parezca mucho a la IA ficticia, por razones que analizaremos en profundidad en el capítulo 4\.

La IA no es la primera tecnología que se anticipó en la ficción. [El vuelo más pesado que el aire](https://www.weslpress.org/9780819577269/robur-the-conqueror/) y [los viajes a la Luna](https://www.imdb.com/title/tt0000417/) se representaron antes de su tiempo. Y la idea general de las armas nucleares fue anticipada por H. G. Wells, uno de los primeros escritores de ciencia ficción, en una novela de 1914 titulada [*The World Set Free*](https://ahf.nuclearmuseum.org/ahf/key-documents/hg-wells-world-set-free/). Wells no acertó con los detalles; escribió sobre una bomba que ardía intensamente durante días, en lugar de una bomba que explotaba de golpe y dejaba tras de sí una muerte persistente. Pero Wells tenía la idea general de una bomba que funcionaba con energía nuclear en lugar de química.

En 1939, Albert Einstein y Leo Szilard enviaron una carta al presidente Roosevelt en la que pedían a Estados Unidos que intentara adelantarse a Alemania en la construcción de una bomba atómica. Podríamos imaginar un mundo en el que Roosevelt hubiera conocido por primera vez la noción de las bombas nucleares en la novela de Wells, lo que le habría llevado a descartar la idea como ciencia ficción.

De hecho, en la vida real, Roosevelt se tomó la idea en serio, al menos lo suficiente como para crear el Comité Asesor sobre el Uranio. Pero este caso demuestra el peligro de descartar ideas solo porque un escritor de ficción haya hablado de una idea parecida en el pasado.

La ciencia ficción puede inducirte a error porque das por sentado que es cierta, o puede inducirte a error porque das por sentado que es *falsa*. Los autores de ciencia ficción no son profetas, pero tampoco son antiprofetas con la garantía de que sus palabras serán erróneas. En la gran mayoría de los casos, es mejor ignorar la ficción y analizar las tecnologías y los escenarios en sus propios términos.

Para predecir lo que sucederá en la realidad, no hay sustituto para la reflexión sobre los argumentos y la ponderación de la evidencia.

#### **Las consecuencias de la IA serán inevitablemente extrañas.** {#las-consecuencias-de-la-ia-seran-inevitablemente-extrañas.}

Comprendemos la reacción de que la IA es *extraña* y que transformaría el mundo y violaría el *statu quo*. Todos tenemos intuiciones adaptadas, en cierta medida, a un mundo en el que los humanos son la única especie capaz de realizar hazañas como construir una central eléctrica. Todos tenemos intuiciones adaptadas a un mundo en el que las máquinas, a lo largo de toda la historia de la humanidad, siempre han sido herramientas carentes de inteligencia. Una cosa de la que podemos estar muy seguros es que un futuro con IA más inteligentes que los humanos sería *diferente*.

Los cambios grandes y duraderos en el mundo no ocurren todos los días. El heurístico «nunca pasa nada»[^6] funciona muy bien la mayoría de las veces, pero las ocasiones en que falla son algunos de los momentos más importantes de la historia para prestar atención. Gran parte del sentido de pensar en el futuro es anticipar esos momentos en los que ocurre algo grande, para que sea posible prepararse.

Una forma de superar el sesgo hacia el statu quo es recordar el registro histórico, como se comentó en la introducción.

A veces, ciertos inventos acaban revolucionando el mundo. Piensa en la máquina de vapor y en las muchas otras tecnologías que ayudó a desarrollar durante la Revolución Industrial, transformando rápidamente la vida humana:

![][image1]

¿Es la llegada de la IA verdaderamente general un avance igualmente trascendental? Parece que la inteligencia artificial sería *al menos* tan trascendental como la Revolución Industrial. Entre otras cosas:

* Es probable que la IA permita que el progreso tecnológico se desarrolle mucho más rápido. Como veremos en el capítulo 1, las máquinas pueden operar mucho más rápido que el cerebro humano. Y los humanos pueden mejorar la IA —y la IA acabará siendo capaz de mejorarse a sí misma— hasta que las máquinas sean mucho mejores que los humanos haciendo descubrimientos científicos, inventando nuevas tecnologías, etcétera.

  A lo largo de toda la historia de la humanidad, la maquinaria del cerebro humano se mantuvo fundamentalmente sin cambios, incluso mientras la humanidad producía logros de ingeniería cada vez más impresionantes. Cuando la maquinaria de la cognición comience a mejorar por derecho propio, cuando se vuelva capaz de mejorarse a sí misma, debemos esperar que *muchas cosas diferentes* empiecen a cambiar *muy rápidamente*.  
* Además, como veremos en el capítulo 3, las IA suficientemente capaces probablemente tendrán objetivos propios. Si las IA fueran esencialmente solo seres humanos más rápidos e inteligentes, eso ya sería algo enormemente importante por sí mismo. Pero las IA serán, en efecto, una especie totalmente nueva de vida inteligente en la Tierra —una con objetivos propios, que probablemente (como veremos en los capítulos 4 y 5) diverjan de manera importante de los objetivos humanos.

A primera vista, sería sorprendente que estos dos grandes desarrollos pudieran ocurrir *sin* revolucionar el orden mundial existente. Creer en un futuro «normal» parece requerir creer que la inteligencia artificial nunca superará en absoluto a la inteligencia humana. Esto nunca pareció una opción verdaderamente viable, y se ha vuelto mucho más difícil de creer en 2025 que en 2015 o 2005.

#### **El futuro a largo plazo también será extraño.** {#el-futuro-a-largo-plazo-también-será-extraño.}

Si miras demasiado lejos hacia el futuro, el resultado va a ser extraño de alguna manera. El siglo XXI se ve francamente extraño desde la perspectiva del siglo XIX, que se veía extraño desde la perspectiva del siglo XVII. La IA acelera este proceso y añade un jugador muy novedoso al tablero.

Un aspecto del futuro que hoy parece predecible es que las especies tecnológicamente avanzadas no permanecerán atrapadas en su propio planeta indefinidamente. En este momento, el cielo nocturno está lleno de estrellas que solo queman su energía. Pero nada impide que la vida construya la tecnología necesaria para viajar por las estrellas y aprovechar esa energía con algún fin.

Existen algunas limitaciones físicas en cuanto a la *rapidez* con la que se puede realizar ese viaje, pero parece que no hay limitaciones para hacerlo eventualmente.[^7] No hay nada que nos impida desarrollar eventualmente el tipo de sondas interestelares que puedan salir y extraer recursos del universo en general y convertir estos recursos en civilizaciones florecientes, con un complemento de sondas autorreplicantes para colonizar aún más regiones del espacio. Si nos sustituimos por IA, nada impedirá que esas IA hagan lo mismo, pero sustituyendo las «civilizaciones florecientes» por cualquier fin que persiga la IA.

Del mismo modo que la vida se extendió por las rocas áridas de la Tierra hasta que todo el mundo se llenó de organismos, podemos esperar que la vida (o las máquinas construidas por la vida) se extienda finalmente a partes deshabitadas del universo, hasta que sea tan extraño encontrar un sistema solar sin vida como lo sería encontrar hoy en día una isla sin vida en la Tierra, desprovista incluso de bacterias.

En la actualidad, la mayor parte de la materia del universo, como las estrellas, está dispuesta por casualidad. Pero es casi seguro que, en un futuro a largo plazo, la mayor parte de la materia estará dispuesta según algún diseño, es decir, según las preferencias de las entidades que logren cosechar y reutilizar las estrellas.

Incluso si nada de lo que hay en la Tierra se extiende por el cosmos, e incluso si la mayoría de las formas de vida inteligente que surgen en galaxias lejanas nunca abandonan su planeta natal, solo se necesita *una* inteligencia viajera espacial en cualquier parte del universo para encender la chispa y comenzar a propagarse por el universo, viajando a nuevos sistemas estelares y utilizando los recursos que allí se encuentran para construir más sondas con las que expandirse hacia aún más sistemas estelares, del mismo modo que solo se necesitó un microorganismo autorreplicante (y un poco de crecimiento exponencial) para convertir un planeta sin vida en un mundo repleto de vida en cada isla.

Así pues, el futuro será diferente del presente. De hecho, podemos esperar que sea radicalmente diferente. Es previsible que las propias estrellas se transformen, a largo plazo, por cualquier especie biológica o IA que busque más recursos, aunque hoy en día no podamos decir mucho sobre cómo será esa especie o sobre el fin al que se destinarán los recursos del universo.

Predecir los *detalles* parece difícil, casi imposible. Es difícil de determinar. Pero ¿predecir la transformación del universo en un lugar donde la mayor parte de la materia se recolecta y se destina a *algún* fin, sea cual sea? Eso es más fácil de predecir, aunque resulte contraintuitivo y extraño para una civilización que apenas ha comenzado a extraer recursos de las estrellas.

Dentro de un millón de años, no deberíamos esperar que el futuro se parezca al año 2025, con un montón de simios sin pelo deambulando por la superficie de la Tierra. Mucho antes de eso, o nos habremos extinguido, o nuestros descendientes habrán salido a explorar el cosmos por sí mismos.[^8]

Sin duda, la humanidad va a vivir una situación extraña. La pregunta es cuándo.

#### **El futuro nos llegará rápidamente.** {#el-futuro-nos-llegará-rápidamente.}

Tecnologías como la IA significan que el futuro puede llamar pronto a nuestra puerta, y sus efectos pueden afectarnos con fuerza.

La Revolución Industrial transformó el mundo muy rápidamente, según los estándares de la historia premoderna. El *Homo sapiens* remodeló el mundo muy rápidamente, según los estándares de los procesos evolutivos. La vida remodeló el mundo muy rápidamente, según los estándares de los procesos cosmológicos y geológicos. Los nuevos procesos para cambiar el mundo pueden remodelarlo muy rápidamente, según los antiguos estándares.

La humanidad parece estar al borde de otra transformación radical, en la que las máquinas pueden empezar a remodelar el mundo a velocidades de máquina, que superan con creces las velocidades biológicas. En los capítulos 1 y 6 hablaremos más sobre cómo se compararía la inteligencia de las máquinas con la inteligencia humana. Pero, como mínimo, debemos tomarnos en serio la posibilidad de que el desarrollo de máquinas más inteligentes que los humanos cambie radicalmente el mundo a gran velocidad. Este tipo de cosas han ocurrido una y otra vez a lo largo de la historia.

# 

## Debate ampliado {#debate-ampliado}

### Expertos en IA sobre escenarios catastróficos {#expertos-en-ia-sobre-escenarios-catastroficos}

En una [encuesta de 2022](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) realizada a 738 asistentes a las conferencias académicas sobre IA NeurIPS e ICML, el 48 % de los encuestados pensaba que había al menos un 10 % de posibilidades de que el resultado de la IA fuera «extremadamente negativo (por ejemplo, la extinción humana)». La preocupación por que la IA provoque desastres sin precedentes está muy extendida en este campo.

A continuación, hemos recopilado comentarios de destacados científicos e ingenieros de IA sobre los resultados catastróficos de la IA. Algunos de estos científicos dan su «p(doom)», es decir, su probabilidad de que la IA provoque la extinción humana o resultados igualmente desastrosos.[^9]

De **Geoffrey Hinton** ([2024](https://youtu.be/PTF5Up1hMhw?t=2285)), ganador del Premio Nobel y del Premio Turing por impulsar la revolución del aprendizaje profundo en la IA, hablando sobre sus estimaciones personales: [^10]

> De hecho, creo que el riesgo [de la amenaza existencial] es superior al cincuenta por ciento.

De **Yoshua Bengio** ([2023](https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340)), ganador del Premio Turing (junto con Hinton y Yann LeCun) y el científico vivo más citado:

> No sabemos cuánto tiempo tenemos antes de que se vuelva realmente peligroso. Lo que he estado diciendo durante las últimas semanas es: «Por favor, dadme argumentos, convencedme de que no debemos preocuparnos, porque así seré mucho más feliz». Y aún no ha sucedido. \[…\] Calculo que hay un veinte por ciento de probabilidades de que resulte catastrófico.

De **Ilya Sutskever** ([2023](https://openai.com/index/introducing-superalignment/)), coinventor de AlexNet, antiguo científico jefe de OpenAI y (junto con Hinton y Bengio) uno de los tres científicos más citados en IA:

> El enorme poder de la superinteligencia también podría ser muy peligroso y provocar la pérdida de poder de la humanidad o incluso la extinción humana. Aunque la superinteligencia parece algo lejano en este momento, creemos que podría llegar en esta década. \[…\]
>
> Actualmente, no tenemos una solución para dirigir o controlar una IA potencialmente superinteligente y evitar que se vuelva rebelde. Nuestras técnicas actuales para alinear la IA, como el aprendizaje por refuerzo a partir de realimentación humana⁠, se basan en la capacidad de los humanos para supervisar la IA. Pero los humanos no podréis supervisar de forma fiable sistemas de IA mucho más inteligentes que vosotros, por lo que nuestras técnicas de alineación actuales no escalarán a la superinteligencia. Necesitamos nuevos avances científicos y técnicos.

De **Jan Leike** ([2023](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)), codirector de ciencia de alineación en Anthropic y antiguo codirector del equipo de superalineación en OpenAI:

> \[entrevistador: «No dediqué mucho tiempo a intentar determinar con precisión mi p(doom) personal. Mi estimación es que es superior al diez por ciento e inferior al noventa por ciento».\]
>
> \[Leike:\] Probablemente ese sea el rango que yo también daría.

De **Paul Christiano** ([2023](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom)), jefe de Seguridad del Instituto de Seguridad de la IA de EE. UU. (con sede en el NIST) e inventor del aprendizaje por refuerzo a partir de realimentación humana (RLHF):

> Probabilidad de que la mayoría de los seres humanos mueran en los 10 años siguientes a la creación de una IA potente (lo suficientemente potente como para hacer obsoleta la mano de obra humana): **20 %** \[…\]
>
> Probabilidad de que la humanidad haya arruinado de alguna manera irreversiblemente nuestro futuro en los 10 años posteriores a la creación de una IA poderosa: **46 %**

De **Stuart Russell** ([2025](https://www.newsweek.com/deepseek-openai-race-human-extinction-2023482)), catedrático Smith-Zadeh de Ingeniería en UC Berkeley y coautor del principal libro de texto universitario de IA, *Artificial Intelligence: A Modern Approach*:

> La «carrera por la IAG» entre empresas y entre naciones es en cierto modo similar \[a la carrera de la Guerra Fría por construir bombas nucleares más grandes\], excepto que es peor: incluso los CEO que participan en la carrera han declarado que quien gane tiene una probabilidad significativa de causar la extinción humana en el proceso, porque no tenemos ni idea de cómo controlar sistemas más inteligentes que nosotros mismos. En otras palabras, la carrera por la IAG es una carrera hacia el borde de un precipicio.

De **Victoria Krakovna** ([2023](https://theinsideview.ai/victoria)), investigadora científica de Google DeepMind y cofundadora del Future of Life Institute:

> \[entrevistador: «No es algo muy agradable en lo que pensar, pero ¿cuál crees que es la probabilidad de que Victoria Krakovna muera a causa de la IA antes de 2100?»\]
>
> \[Krakovna:\] Bueno, 2100 está muy lejos, sobre todo teniendo en cuenta lo rápido que se está desarrollando la tecnología en la actualidad. De primeras, diría que un veinte por ciento o algo así.

De **Shane Legg** ([2011](https://baserates-test.vercel.app/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai)), cofundador y científico jefe de IAG en Google DeepMind:

> \[entrevistador: «¿Qué probabilidad le asignas a la posibilidad de consecuencias negativas/extremadamente negativas como resultado de una IA mal hecha? \[...\] Donde 'negativas' = extinción humana; 'extremadamente negativas' = sufrimiento humano»\]
>
> \[Legg:\] \[E\]n el plazo de un año desde la aparición de una IA de nivel humano\[…\] No lo sé. Quizás un cinco por ciento, quizás un cincuenta por ciento. No creo que nadie tenga una buena estimación al respecto. Si por sufrimiento te refieres a un sufrimiento prolongado, entonces creo que es bastante improbable. Si una máquina superinteligente (o cualquier tipo de agente superinteligente) decidiera deshacerse de nosotros, creo que lo haría de manera bastante eficiente.

De **Emad Mostaque** ([2024](https://x.com/EMostaque/status/1864266899170767105)), fundador de Stability AI, la empresa detrás de Stable Diffusion:

> Mi P(doom) es del 50 %. Dado un período de tiempo indefinido, la probabilidad de que sistemas más capaces que los humanos terminen controlando toda nuestra infraestructura crítica y nos eliminen por completo es como lanzar una moneda al aire, especialmente considerando el enfoque que estamos adoptando actualmente.

De **Daniel Kokotajlo** ([2023](https://www.lesswrong.com/posts/xDkdR6JcQsCdnFpaQ/adumbrations-on-agi-from-an-outsider?commentId=sHnfPe5pHJhjJuCWW)), especialista en gobernanza de la IA, denunciante de OpenAI y director ejecutivo del AI Futures Project:

> Creo que la probabilidad de doom por IA es del 70 %, y pienso que las personas que creen que es inferior al 20 %, por ejemplo, están siendo muy poco razonables\[.\]

De **Dan Hendrycks** ([2023](https://x.com/DanHendrycks/status/1642394635657162753)), investigador en aprendizaje automático y director del Centro para la Seguridad de la IA:

> \[M\]i p(doom) \> 80 %, pero en el pasado ha sido menor. Hace dos años era de ~20 %.

Todos los investigadores mencionados anteriormente firmaron la [Declaración sobre el riesgo asociado a la IA](https://aistatement.com/) con la que comenzamos el libro, que dice:

> Mitigar el riesgo de extinción causado por la IA debería ser una prioridad global junto con otros riesgos a escala societal, como las pandemias y la guerra nuclear.

Otros investigadores prominentes que firmaron la declaración incluyen: John Schulman, arquitecto de ChatGPT; Peter Norvig, exdirector de investigación de Google; Eric Horvitz, director científico de Microsoft; David Silver, líder de investigación de AlphaGo; Frank Hutter, pionero de AutoML; Andrew Barto, pionero del aprendizaje por refuerzo; Ian Goodfellow, inventor de las GAN; Ya-Qin Zhang, expresidente de Baidu; Martin Hellman, inventor de la criptografía de clave pública; y Alexey Dosovitskiy, líder de investigación de Vision Transformer. La lista continúa con otros firmantes que incluyen a Dawn Song, Jascha Sohl-Dickstein, David McAllester, Chris Olah, Been Kim, Philip Torr y cientos más.

### Cuando Leo Szilard vio el futuro {#cuando-leo-szilard-vio-el-futuro}

En septiembre de 1933, un físico llamado Leo Szilard cruzaba el cruce[^11] donde Southampton Row pasa por Russell Square cuando se le ocurrió la idea de una reacción nuclear en cadena, la idea clave detrás de las bombas atómicas.

A partir de ahí, comenzó toda una aventura en la que Szilard intentó averiguar qué hacer con esta idea trascendental. Acudió al prestigioso físico Isidor Rabi, y Rabi acudió al aún más prestigioso Enrico Fermi. Rabi le preguntó a Fermi si creía que las reacciones nucleares en cadena eran reales, y Fermi le respondió:

> ¡Una locura!

Rabi le preguntó a Fermi qué significaba «¡Una locura!», y Fermi respondió que era una posibilidad remota.

Rabi preguntó qué quería decir Fermi con «posibilidad remota», y Fermi respondió: «Un diez por ciento».

A lo que Rabi respondió: «Un diez por ciento no es una posibilidad remota si eso significa que podemos morir por ello».

Fermi se lo pensó mejor.

Hay varias moralejas que se pueden extraer de esta historia. Una moraleja que *no* extraemos es: «Vale la pena preocuparse por cualquier posibilidad remota si puede causarnos la muerte». No hay nada «remoto» en un diez por ciento, pero si la posibilidad *fuera* lo suficientemente remota, entonces simplemente no valdría la pena pensar en ella.

Una moraleja que *sí* extraemos de esta historia: a veces es posible darse cuenta de que una tecnología como una cascada de radiactividad es *posible* y, por lo tanto, saber (antes que nadie) que el mundo está destinado a algún tipo de cambio drástico.

Otra moraleja que extraemos de esta historia es que las intuiciones iniciales de uno a menudo no son una buena guía para anticipar y pensar en cambios drásticos. Ni siquiera si uno es un experto de renombre en el campo relevante, como lo era Enrico Fermi.

Piénsalo: ¿de dónde sacó Fermi esa «posibilidad remota» y ese «diez por ciento» en primer lugar?

¿Por qué Fermi pensaba que *no* se podía conseguir que la radiactividad indujera más radiactividad en una reacción en cadena? ¿Era simplemente porque la mayoría de las grandes ideas no funcionan?

Responder «¡Qué locura!» parece decir algo más fuerte que eso. Parece reflejar la sensación de que era *excesivamente* improbable que esa gran idea en particular funcionara. Pero ¿por qué? ¿En base a qué argumento físico?

¿Simplemente se sentía como una locura? Sí, la posibilidad de las armas nucleares tendría consecuencias radicales para el mundo. Pero la realidad no está organizada de manera que impida que ocurran acontecimientos con grandes consecuencias.[^12]

Cuando Fermi escuchó por primera vez la idea de Szilard, le sugirió que la publicara y la diera a conocer al mundo entero, incluyendo a Alemania y a su nuevo canciller, Adolf Hitler.

Fermi perdió esa discusión, y menos mal que fue así, porque al final resultó que las armas nucleares eran posibles. Fermi acabó uniéndose a la pequeña conspiración de Szilard, aunque siguió siendo escéptico casi hasta el momento en que él mismo supervisó la creación de la primera pila nuclear, la Chicago Pile-1.

A veces, las tecnologías cambian el mundo. Si das por sentado que las tecnologías radicalmente nuevas son «una locura», el progreso puede pillarte por sorpresa, incluso si eres uno de los científicos más inteligentes del mundo. Por lo tanto, es un gran mérito de Fermi que se sentara a discutir con Szilard. Y aún más mérito tiene que se dejara convencer para cambiar su comportamiento *antes* de que existiera la tecnología, antes de poder verla con sus propios ojos, cuando aún había tiempo para hacer algo al respecto.

A lo largo de la historia de la humanidad han ocurrido muchas cosas terribles, pero algunas de las cosas terribles que no han ocurrido se evitaron porque alguien se sentó y mantuvo una conversación. En algunos casos, forzó la conversación, como hizo Szilard con Fermi.

# 

# Capítulo 1: El poder especial de la humanidad {#capítulo-1:-el-poder-especial-de-la-humanidad}

Este es el suplemento en línea del capítulo 1 de [*Si alguien lo construye, todos mueren*](https://www.amazon.com/gp/product/0316595640). A continuación, abordaremos preguntas frecuentes y ampliaremos los temas tratados en el libro.

Algunos temas que no se cubren a continuación, porque se tratan en el Capítulo 1 del libro, incluyen:

* ¿Qué es eso de la «inteligencia»?  
* ¿Es realmente posible que las máquinas lleguen a ser más inteligentes que los humanos?  
* ¿No hay un límite práctico en cuanto a lo inteligente que puede llegar a ser algo?

## Preguntas frecuentes {#preguntas-frecuentes-1}

### ¿Es la inteligencia un concepto significativo? {#es-la-inteligencia-un-concepto-significativo}

#### **Sí. Hay un fenómeno real que describir, aunque sea difícil de definir.** {#sí.-hay-un-fenómeno-real-que-describir,-aunque-sea-difícil-de-definir.}

En los últimos treinta años, se han otorgado setenta y siete premios Nobel de Química a seres humanos y cero a chimpancés. Un extraterrestre, al enterarse de este hecho por primera vez, podría preguntarse si el Comité Nobel tiene sesgo. Pero no, realmente hay *algo que está pasando* con los seres humanos que nos distingue de los chimpancés.

Es un punto demasiado obvio, pero los puntos obvios a veces pueden importar. Tenemos habilidades que nos permiten caminar en la Luna y que ponen el destino del planeta en nuestras manos en lugar de en las de los chimpancés. Los filósofos y científicos pueden debatir la verdadera naturaleza de la inteligencia, pero sin importar lo que concluyan, el fenómeno subyacente permanece. Algo sobre los humanos nos ha permitido lograr hazañas nunca antes vistas en la naturaleza; y ese algo tiene que ver con nuestros cerebros, y cómo los usamos para comprender y afectar el mundo que nos rodea.

#### **El hecho de que no podamos dar una definición precisa no significa que no pueda perjudicarnos.** {#el-hecho-de-que-no-podamos-dar-una-definición-precisa-no-significa-que-no-pueda-perjudicarnos.}

Si te ves envuelto en un incendio forestal, no importa si entiendes o no la química subyacente. Te quemarás de todos modos.

Lo mismo ocurre con la inteligencia. Si las máquinas empiezan a convertir la superficie de la Tierra en su propia infraestructura, generando tanto calor residual que hierven los océanos, entonces no importará mucho si tenemos o no una definición precisa de «inteligencia». Moriríamos de todos modos.

Lo decimos literalmente, y en los próximos capítulos exploraremos por qué esperamos resultados tan extremos de una IA más inteligente que los humanos. En el capítulo 3, argumentaremos que las máquinas superinteligentes perseguirían fines. En el capítulo 4, argumentaremos que esos fines no serían los que ningún humano habría pretendido o pedido. En el capítulo 5, argumentaremos que sus objetivos se alcanzarían mejor si tomaran los recursos que nosotros utilizamos para sobrevivir. Y en el capítulo 6 argumentaremos que serían capaces de desarrollar su propia infraestructura y convertir rápidamente el mundo en un lugar inhabitable.

#### **No necesitas una definición precisa de inteligencia para crear inteligencia.** {#no-necesitas-una-definición-precisa-de-inteligencia-para-crear-inteligencia.}

Los seres humanos fueron capaces de crear el fuego antes de comprender la química subyacente a la combustión. Del mismo modo, los seres humanos están en camino de crear máquinas inteligentes, a pesar de su falta de comprensión, como veremos en el capítulo 2.

En lugar de pensar en la inteligencia como una noción matemática que necesita una definición precisa, recomendamos pensar en la «inteligencia» como la etiqueta de un fenómeno natural observado que aún no comprendemos bien.

Hay algo en el cerebro humano que nos permite realizar una asombrosa variedad de hazañas. Construimos aceleradores de partículas; desarrollamos nuevos fármacos; inventamos la agricultura; escribimos novelas; llevamos a cabo campañas militares. Hay algo en el cerebro humano que nos permite hacer todas esas cosas, mientras que los ratones y los chimpancés no pueden hacer ninguna de ellas. Aunque todavía no tengamos una comprensión científica completa de esa diferencia mental, es útil tener una etiqueta para ella.

Del mismo modo, es útil poder hablar de una inteligencia que supera a la nuestra. Ya podemos observar hoy en día IA que son sobrehumanas en una variedad de dominios específicos; por ejemplo, las IA modernas de ajedrez son sobrehumanas en el dominio del ajedrez. Es natural preguntarse entonces qué sucederá cuando construyamos IA que sean sobrehumanas en las tareas de descubrimiento científico, desarrollo tecnológico, manipulación social o planificación estratégica. Y es natural preguntarse qué sucederá cuando construyamos IA que superen a los humanos en todos los dominios.

Si aparece una IA capaz de realizar investigaciones científicas de primer nivel miles de veces más rápido que los mejores científicos humanos, es posible que protestemos diciendo que «no es verdaderamente inteligente», tal vez porque llega a conclusiones de una manera muy diferente a como lo haría un humano. Eso podría incluso ser cierto, dependiendo de la definición de «inteligencia» que elijas. Pero el impacto real de la IA será enorme, independientemente de cómo decidamos etiquetarla.

Necesitamos cierta terminología para hablar de ese tipo de impacto y de los tipos de máquinas que son radicalmente capaces de predecir y dirigir el mundo. En este libro, tomamos el camino fácil de asignar la etiqueta «inteligencia» a las *capacidades*, en lugar de a los procesos internos específicos que dan lugar a esas capacidades.

### ¿Es «inteligencia de nivel humano» un concepto significativo? {#es-«inteligencia-de-nivel-humano»-un-concepto-significativo}

#### **Sí, en muchos casos.** {#sí,-en-muchos-casos.}

Los seres humanos han construido una civilización tecnológica avanzada, y los chimpancés no. Parece haber *cierto* sentido en el que los chimpancés no están «a nuestro nivel», a pesar de que se comunican entre ellos, utilizan herramientas y tienen muchas habilidades impresionantes. Por lo tanto, tiene sentido señalar a los seres humanos y decir «*ese* nivel», aunque haya algunos problemas con el uso de la inteligencia humana como criterio de referencia.

Si algún día nos encontráramos con una civilización alienígena en las profundidades del espacio, incluso suponiendo que los alienígenas fueran tan avanzados tecnológicamente como nosotros, podrían ser peores que los humanos caminando y mejores nadando. Podrían ser mejores en juegos antagónicos como el ajedrez o el póquer, pero peores en matemáticas abstractas. O viceversa, dependiendo de los alienígenas. Los alienígenas podrían pensar más lentamente pero tener mejor memoria, o pensar más rápido pero con peor memoria.

¿Quién puede decir si esos alienígenas tienen una inteligencia «a nivel humano»? (¿Y por qué no preguntarse si los humanos están «a nivel alienígena»?)

Cuando hablamos de «inteligencia a nivel humano», intentamos referirnos a cualquier cualidad que haga a los humanos capaces de construir y mantener una civilización tecnológica, algo que los chimpancés no pueden hacer.

Desde un punto de vista histórico (o más bien antropológico), parece que en algún momento después de que los humanos y los chimpancés comenzaran a divergir, se cruzó un umbral. No es que los humanos tengan todos los mejores científicos, mientras que los chimpancés tengan científicos mediocres cuyos artículos no logran replicarse. Los chimpancés ni siquiera escriben artículos científicos *malos*. ¡No escriben nada! Los cerebros humanos y los cerebros de los chimpancés son bastante similares desde el punto de vista biológico, pero hubo un umbral que los humanos cruzaron y que les permitió inventar la civilización, fundir hierro, enviar cohetes a la órbita, y escribir y leer.

A simple vista, dejando de lado toda teoría, parece que se rompió una especie de presa y se desató una gran inundación de inteligencia detrás de ella. Se desató una especie de «infierno» desconocido.

Hay personas que se opondrán hábilmente a esta idea, pero tienen que hacerlo recurriendo a argucias y definiciones en lugar de decir: «En realidad, he descubierto evidencia de que el *Homo erectus* intentó construir reactores nucleares hace dos millones de años; simplemente se les daba muy mal».

La inteligencia lo suficientemente poderosa y general como para crear una civilización parece haber llegado al mundo de forma rápida y contundente, separando claramente al *Homo sapiens* de los demás animales. Ciertamente, no nos aferramos a la etiqueta específica de «inteligencia a nivel humano», que plantea muchos problemas. Pero, independientemente de cómo lo llamemos, es útil tener *algún* tipo de concepto para «las cosas que están al otro lado de ese umbral, sea cual sea».

### ¿No consiste la inteligencia en múltiples habilidades? {#¿no-consiste-la-inteligencia-en-múltiples-habilidades?}

#### **Sí, pero hay un solapamiento sustancial.** {#sí,-pero-hay-un-solapamiento-sustancial.}

Supongamos que soy mejor que mi hermana componiendo música clásica, pero ella es mejor escribiendo novelas. No hay una forma clara de juzgar cuál de los dos es «más inteligente», ya que la música y la escritura de novelas son habilidades diferentes. Entonces, ¿qué sentido tiene decir que una IA es «más inteligente» que un humano?

Nuestra respuesta es: si yo soy mejor en una cosa y mi hermana es mejor en otra, puede resultar difícil hacer comparaciones significativas. Por otro lado, si yo soy mejor en una cosa y mi hermana es mejor en dos mil, entonces empieza a parecer un poco absurdo insistir en que estamos en igualdad de condiciones, o insistir en que no hay nada que podamos decir sobre la situación en la que nos encontramos.

*If Anyone Builds It, Everyone Dies* es un libro sobre el probable impacto práctico de los avances futuros en IA. Para hablar de manera significativa sobre ese impacto, no necesitamos ser capaces de comparar ChatGPT, los seres humanos y las moscas de la fruta y decir con precisión en qué «nivel de inteligencia» se encuentran estos tres sistemas tan diferentes. Solo necesitamos ver que las IA son cada vez mejores en una gama cada vez más amplia de habilidades y que, con el tiempo, superarán a los humanos en habilidades de enorme importancia práctica.

### ¿No está sobrevalorada la inteligencia? {#¿no-está-sobrevalorada-la-inteligencia?}

#### **Solo si usas una definición excesivamente restrictiva de «inteligencia».** {#solo-si-usas-una-definición-excesivamente-restrictiva-de-«inteligencia».}

A veces nos encontramos con afirmaciones como: «¡La inteligencia no lo es todo para alcanzar el éxito! Muchas de las personas más exitosas son políticos carismáticos, CEOs o estrellas del pop. Los nerds son mejores en algunas cosas, pero no dirigen el mundo».

No discutimos esta afirmación. Más bien, lo que entendemos por «inteligencia» (en este libro) no es la propiedad que separa a los nerds de los deportistas. Es la propiedad que separa a los seres humanos de los ratones.

En un guion de Hollywood, llamar «inteligente» a un personaje suele significar que tiene «conocimiento libresco». Quizás sea un aficionado a la historia o un inventor brillante. Quizás sea bueno jugando al ajedrez o resolviendo misterios.

El «inteligente» de una película tiene sus propias fortalezas, equilibradas por las debilidades estereotípicas de los nerds de Hollywood: tal vez carece de inteligencia emocional, de sentido común o de astucia callejera. Quizás le falte destreza manual o carisma.

Pero el carisma no es una sustancia producida por los riñones. El carisma, al igual que la inteligencia académica, es el resultado de procesos cerebrales. Esto incluye *procesos inconscientes* dentro del cerebro: los comportamientos que hacen que alguien sea carismático no están necesariamente bajo su control consciente. Pero al final, el carisma y la perspicacia ingenieril forman parte de la herencia neurológica que separa a los humanos de los ratones, independientemente de cómo se repartan esos dos poderes entre los nerds y las estrellas del pop.

Por «inteligencia artificial» no nos referimos a «conocimientos académicos artificiales». Nos referimos a «todo-lo-que-separa-el-cerebro-humano-del-cerebro-de-un-ratón». Nos referimos al poder que permite a los humanos caminar sobre la luna, al poder que permite a un orador conmover a una multitud hasta las lágrimas y al poder que permite a un soldado apuntar con destreza un rifle. Nos referimos al paquete completo.

### ¿Es «inteligencia general» un concepto significativo? {¿es-«inteligencia-general»-un-concepto-significativo?}

#### **Sí.** {#sí.}

El halcón peregrino puede lanzarse en picado a 386 km/h. Un cachalote puede sumergirse a kilómetros de profundidad bajo la superficie del océano. Un halcón se ahogaría en el mar y una ballena se estrellaría si intentara volar, pero de alguna manera los humanos hemos logrado volar más rápido y sumergirnos más profundamente que cualquiera de estas criaturas dentro de cascarones metálicos de nuestro propio diseño. 

Nuestro entorno ancestral no incluía las profundidades del océano, ni nuestros antepasados fueron seleccionados por su capacidad para volar. Logramos estas cosas y muchas otras, no mediante instintos especiales, sino gracias a la gran versatilidad de nuestras mentes.

De alguna manera, nuestros antepasados fueron seleccionados por ser «buenos resolviendo problemas», en sentido amplio, a pesar de que nuestros antepasados remotos rara vez se enfrentaban a desafíos de ingeniería más complicados que la construcción de una lanza.

¿Poseen los seres humanos una capacidad *perfecta* para resolver problemas? No, obviamente no. Los seres humanos no parecen poder aprender a jugar al ajedrez tan bien como las mejores IA ajedrecísticas, al menos dentro de los límites de tiempo del juego. Es demostrable que es posible alcanzar niveles sobrehumanos en el ajedrez, y los seres humanos no pueden alcanzar esos niveles sin ayuda. Nuestra inteligencia no es universal, es decir, no podemos aprender a hacer *todo* lo que es físicamente posible.[^13] Así que esta «generalidad» que tienen los humanos no se refiere a ser capaces de hacer todo lo que se puede hacer utilizando solo nuestro cerebro. Sin embargo, hay algo inmensamente más general en la capacidad de los humanos para aprender y resolver nuevos problemas, comparado con la capacidad de aprendizaje y resolución de problemas de una IA de ajedrez limitada como [Deep Blue](https://www.ibm.com/history/deep-blue).

Pero la generalidad no es todo o nada. Admite grados.

Deep Blue no era muy general en su capacidad para dirigir nada más que un tablero de ajedrez. Podía encontrar jugadas ganadoras en el ajedrez, pero no podía conducir un coche hasta la tienda y comprar leche, y mucho menos descubrir las leyes de la gravedad y diseñar un cohete lunar. Deep Blue ni siquiera podía jugar a otros juegos de mesa, ya fueran juegos más sencillos como las damas o juegos más difíciles como el Go.

Por el contrario, consideremos [AlphaGo](https://deepmind.google/research/projects/alphago/), la IA que finalmente conquistó el Go. Los algoritmos detrás de AlphaGo también son capaces de jugar muy bien al ajedrez. El Go no cayó ante el primer algoritmo de ajedrez que encontró la humanidad, pero una variante del primer algoritmo de Go que encontró la humanidad fue capaz de batir los récords anteriores en ajedrez, y el mismo algoritmo también fue capaz de destacar jugando videojuegos de Atari. Estos nuevos algoritmos aún no podían ir a comprar leche a la tienda, pero eran *más* generales.

Resulta que algunos métodos de inteligencia son mucho más generales que otros.

#### **Pero estamos aún más lejos de definir la «generalidad» que la «inteligencia».** {#pero-estamos-aún-más-lejos-de-definir-la-«generalidad»-que-la-«inteligencia».}

Es fácil decir que los seres humanos son más generales que las moscas de la fruta. Pero, ¿cómo funciona la generalidad?

No lo sabemos. Todavía no existe una teoría formal madura sobre la «generalidad». Podemos gesticular y decir que una inteligencia es «más general» en la medida en que es capaz de predecir y dirigirse en una gama más amplia de entornos, a pesar de una gama más amplia de retos complicados. Pero no podemos ofreceros una forma de cuantificar los retos y los entornos que convierta esto en una definición formal.

¿Te parece insatisfactorio? A nosotros también nos resulta insatisfactorio. Deseamos fervientemente que la humanidad acumule una mejor comprensión de la inteligencia general antes de intentar construir máquinas con inteligencia general. Esto podría mejorar la grave situación técnica que describiremos en los capítulos 10 y 11. 

Aunque no tenemos una descripción formal del fenómeno, podemos deducir algunos hechos sobre la generalidad observando el mundo que nos rodea.

Sabemos que los seres humanos no nacen con el conocimiento y la habilidad innatos para construir rascacielos y cohetes lunares, porque nuestros antepasados lejanos nunca tuvieron que trabajar con rascacielos y cohetes lunares de una manera que pudiera codificar ese conocimiento en nuestros genes. Más bien, esas habilidades provienen de nuestro poder para aprender sobre ámbitos que no nacimos entendiendo.

Para evaluar la generalidad, no preguntes cuánto *sabe* algo. Pregunta cuánto *aprende*.

En cierto sentido, los seres humanos son aprendices más poderosos que los ratones. No es que los ratones no puedan aprender en absoluto —por ejemplo, pueden aprender a navegar por un laberinto—. Pero los seres humanos pueden aprender cosas más complicadas y extrañas que los ratones, y podemos conectar nuestros conocimientos de forma más eficaz.

¿Cómo funciona esto exactamente? ¿Qué tenemos nosotros que no tienen los ratones? 

Consideremos dos personas que están aprendiendo a navegar por una nueva ciudad después de mudarse. 

Alice memoriza todas las rutas que necesita saber. Para ir de su casa a la ferretería, gira a la izquierda en la Tercera Avenida, a la izquierda en el segundo semáforo, y luego sigue dos manzanas más y gira a la derecha para entrar en el aparcamiento. Memoriza por separado la ruta a la tienda de comestibles y la ruta a su oficina.

Mientras tanto, Beth estudia e interioriza un mapa de la ciudad. 

A Alice le puede ir bien en su vida cotidiana, pero si alguna vez tiene que conducir a un lugar nuevo sin indicaciones, se encuentra en apuros. Por el contrario, Beth tiene que dedicar más tiempo a planificar sus rutas, pero es mucho más flexible.

Alice puede ser más rápida en las rutas específicas que ha memorizado, pero Beth será mejor conduciendo por cualquier otro lugar. Beth también tendrá ventaja en otras tareas, como encontrar una ruta que minimice el tráfico en hora punta, o incluso diseñar el trazado de las calles de otra ciudad.

Parece que hay tipos de aprendizaje que se parecen menos a memorizar rutas de conducción y más a interiorizar un mapa. Parece que hay engranajes mentales que se pueden reutilizar y adaptar a muchos escenarios diferentes. Parece que hay tipos de pensamiento que van muy profundo.

Hablaremos más sobre este tema en el capítulo 3.

### ¿Es la «inteligencia» una simple cantidad escalar? {#¿es-la-«inteligencia»-una-simple-cantidad-escalar?}

#### **No. Pero hay niveles que la IA aún no ha alcanzado.** {#no.-pero-hay-niveles-que-la-ia-aún-no-ha-alcanzado.}

A veces hemos oído sugerir que la idea de la superinteligencia asume que la «inteligencia» es una cantidad simple y unidimensional.[^14] Invierte más en investigación de IA y obtendrás más «inteligencia», como si la inteligencia fuera menos como una máquina y más como un fluido que se puede seguir bombeando del suelo.

Estamos de acuerdo con la crítica subyacente: la inteligencia no es una cantidad escalar simple. Puede que no siempre sea sencillo crear IA más inteligentes simplemente dedicando más hardware de cómputo al problema (aunque a veces sí lo sea, si la última década sirve de indicación). Una mayor inteligencia puede que no siempre se traduzca directamente en un mayor poder. El mundo es complicado y las capacidades pueden toparse con cuellos de botella y mesetas.

Pero, como señalamos en el Capítulo 1, la existencia de complicaciones, límites y cuellos de botella no significa que la IA vaya a chocar convenientemente con un muro cerca del rango de capacidades humanas. Los cerebros biológicos tienen limitaciones que *no* están presentes en la IA, como se explica en el libro.

La inteligencia humana tiene muchas limitaciones y, sin embargo, nos llevó a la Luna. La inteligencia animal no es una magnitud escalar única y, sin embargo, los humanos somos capaces de superar con creces a los chimpancés. Por muy complicada que sea la inteligencia, existe una brecha clara y cualitativa entre nosotros y los chimpancés.

Las superinteligencias artificiales también podrían tener limitaciones y complicaciones, pero aun así ser capaces de superar con creces a los humanos. Podría abrirse una brecha cualitativa entre ellas y nosotros, si los investigadores e ingenieros siguen compitiendo por crear IA cada vez más capaces.

### ¿Superará la IA los umbrales críticos y despegará? {#¿superará-la-ia-los-umbrales-criticos-y-despegará?}

#### **Probablemente.** {#probablemente}

Desde algunos puntos de vista, el progreso de la IA moderna parece incremental.[^15] Por ejemplo, hasta el verano de 2025, la capacidad de la IA para completar tareas largas ha seguido aproximadamente una curva exponencial en los últimos años,[^16] y se podría argumentar que esto es reconfortantemente incremental.[^17] ¿Significa eso que el progreso de la IA será agradable, lento y predecible?

No necesariamente. El hecho de que una cantidad aumente de forma lenta, suave o incremental no significa que los resultados sean siempre moderados. La fisión nuclear ocurre en un continuo, pero hay una diferencia bastante grande entre una reacción nuclear en cadena que produce menos de un neutrón por neutrón (en la que la reacción se agota) y una reacción nuclear en cadena que produce más de un neutrón por neutrón (que produce una reacción en cadena descontrolada).

Pero no hay una diferencia marcada en la mecánica subyacente entre los dos tipos de reacciones nucleares. Si añades un poco más de uranio, el «factor de multiplicación de neutrones» pasa suavemente de justo por debajo de uno a justo por encima de uno. Las reacciones supercríticas no son causadas por neutrones que golpean los átomos de uranio con tanta fuerza que crean superneutrones. Un poco más de la misma materia subyacente provoca un gran cambio macroscópico. Esto se denomina «efecto umbral».

El caso de los seres humanos frente a los chimpancés parece ser evidencia de que existe al menos un efecto umbral en lo que respecta a la inteligencia. Los seres humanos no son tan diferentes anatómicamente de otros animales. El cerebro humano y el cerebro de un chimpancé son muy similares en su interior; ambos tenemos una corteza visual, una amígdala y un hipocampo. Los seres humanos no tienen un módulo «de ingeniería» especial adicional que explique por qué pueden ir a la Luna y ellos no.

Hay algunas diferencias en el cableado, y tenemos una corteza prefrontal más desarrollada que otros primates. Pero a nivel de anatomía general, la principal diferencia es que nuestros cerebros son tres o cuatro veces más grandes. Básicamente, estamos ejecutando una versión más grande y ligeramente mejorada del mismo hardware.

Y los cambios no fueron repentinos en nuestro linaje. Los cerebros de nuestros antepasados simplemente se hicieron un poco más grandes y un poco mejores, paso a paso. Eso fue suficiente para que se abriera rápidamente una enorme brecha cualitativa (en la escala de tiempo de la evolución).

Si puede suceder con los seres humanos, probablemente también pueda suceder con la IA.

#### **No sabemos cuán lejos están las IA de los umbrales.** {#no-sabemos-cuán-lejos-están-las-ia-de-los-umbrales.}

Si supiéramos exactamente qué sucedió en los seres humanos que nos permitió cruzar el umbral hacia la inteligencia general, tal vez sabríamos qué buscar para saber que algún umbral crítico está cerca. Pero, como veremos en el capítulo 2, no tenemos ese nivel de comprensión de la inteligencia. Por lo tanto, estamos volando a ciegas, sin idea de dónde están los umbrales ni de cuán cerca estamos de ellos.

Los recientes avances en IA se han traducido en una mayor capacidad para resolver problemas matemáticos y jugar al ajedrez, pero no han sido suficientes para que las IA lleguen «hasta el final». Quizás lo único que se necesita es un modelo que sea tres o cuatro veces más grande, como la diferencia entre el cerebro de un chimpancé y el de un humano.[^18] ¡O quizás no! Quizás se necesite una arquitectura completamente diferente y una década de avances científicos, como los chatbots modernos, que provienen de una arquitectura novedosa inventada en 2017 (y que maduró en 2022).

¿Qué cambios en el cerebro humano nos llevaron a cruzar un umbral crítico? Quizás fue nuestra capacidad para comunicarnos. Quizás fue nuestra capacidad para comprender conceptos abstractos de una manera que hizo que la comunicación fuera tan valiosa. Quizás estemos pensando en términos totalmente erróneos y el cambio clave fuera algo extraño que hoy en día ni siquiera se nos ocurre. Quizás fue una gran combinación de factores, en la que cada uno de ellos tenía que estar lo suficientemente maduro como para que todos juntos pudieran dar lugar al tipo de inteligencia capaz de llevar a los humanos a la Luna.

No lo sabemos. Y como no lo sabemos, no podemos mirar una IA moderna y saber qué tan cerca o lejos está de ese mismo umbral crítico.

El amanecer de la ciencia y la industria cambió radicalmente la civilización humana. El amanecer del lenguaje puede haber tenido consecuencias similares para nuestros antepasados. Pero si es así, no hay garantía de que cualquiera de esas capacidades actúe como un «umbral crítico» para la IA, porque, a diferencia de los humanos, las IA tenían desde el principio cierto conocimiento del lenguaje, la ciencia y la industria.

O tal vez el umbral crítico para la humanidad fue una combinación de muchos factores, en la que todos y cada uno de ellos tenían que ser «lo suficientemente buenos» para que todo el sistema encajara. Las IA podrían quedarse atrás en algunas capacidades en las que los homínidos eran mejores, como la memoria a largo plazo, pero seguirían mostrando un salto importante en su capacidad práctica una vez que la última pieza encajara en su sitio.

Incluso si ninguna de esas analogías entre la IA y los humanos resultara cierta, es probable que existan otras dinámicas que hagan que el progreso de la IA sea irregular y difícil de predecir.

Quizás los déficits en la memoria a largo plazo y el aprendizaje continuo estén frenando a la IA de una manera que nunca obstaculizó a los humanos. Quizás, una vez que se solucionen esos problemas, algo «haga clic» y la IA parecerá obtener una «chispa» de inteligencia.

O (como se analiza en el libro) consideremos el punto en el que las IA pueden crear IA más inteligentes, que a su vez crean IA aún más inteligentes, en un bucle de realimentación. Los bucles de realimentación son una causa común de los efectos umbral.

Por lo que sabemos, hay una docena de factores diferentes que podrían servir como la «pieza que falta», de modo que, una vez que un laboratorio de IA descubra esa última pieza del rompecabezas, su IA realmente comenzará a despegar y a separarse del resto, como lo hizo la humanidad del resto de los animales. Los momentos críticos podrían llegarnos rápidamente. No necesariamente tenemos mucho tiempo para prepararnos.

#### **\* La velocidad de despegue no afecta al resultado, pero la posibilidad de un despegue rápido significa que debemos actuar pronto.** {#*-la-velocidad-de-despegue-no-afecta-al-resultado,-pero-la-posibilidad-de-un-despegue-rápido-significa-que-debemos-actuar-pronto.}

Al final, los umbrales no importan demasiado para el argumento de que si alguien construye una superinteligencia artificial, todos moriremos. Nuestros argumentos no requieren que alguna IA descubra cómo mejorarse recursivamente a sí misma y luego se convierta en superinteligente a una velocidad sin precedentes. Eso podría suceder, y creemos que es bastante probable que *suceda*, pero no importa para la afirmación de que la IA está en camino de matarnos a todos.

Todo lo que requieren nuestros argumentos es que las IA sigan mejorando cada vez más en la predicción y el control del mundo, hasta que nos superen. No importa mucho si eso ocurre de forma rápida o lenta.

La relevancia de los efectos umbral radica en que aumentan la importancia de que la humanidad reaccione *pronto* ante la amenaza. No podemos permitirnos el lujo de esperar hasta que la IA sea *un poco* mejor que cualquier ser humano en todas las tareas mentales, porque para entonces podría ser que no quedara mucho tiempo. Sería como ver a los primeros homínidos haciendo fuego, bostezar y decir: «Despertadme cuando estén a mitad de camino de la Luna».

Los homínidos tardaron millones de años en recorrer la mitad del camino hasta la luna y dos días en completar el resto del viaje. Cuando puede haber umbrales involucrados, hay que prestar atención *antes* de que las cosas se salgan visiblemente de control, porque para entonces podría ser demasiado tarde.

### ¿No es ChatGPT ya una inteligencia general? {#¿no-es-chatgpt-ya-una-inteligencia-general?}

#### **Podríamos llamarlo así si quisiéramos.** {#podríamos-llamarlo-así-si-quisiéramos.}

ChatGPT y otros programas similares son más generales que las IA que los precedieron. Pueden hacer algunas operaciones matemáticas, escribir poesía y programar. ChatGPT no siempre puede hacer estas cosas *bien* (a fecha de agosto de 2025), pero puede hacer muchas cosas.

Es razonable suponer que GPT-5 sigue siendo menos general en su razonamiento que un niño humano. Es cierto que puede recitar más libros de texto, pero probablemente haya memorizado un volumen mucho mayor de patrones superficiales que los que utilizaría un niño humano, mientras que un niño probablemente utilice mecanismos mentales más profundos para completar tareas comparables (con mejores resultados en algunos casos y peores en otros).

Si los autores nos viéramos obligados a comparar ambos, diríamos que ChatGPT parece, en general, más tonto en un sentido profundo que un humano, y no solo porque (mientras escribimos esta frase en julio de 2025) los chatbots tengan una memoria episódica limitada.

Hay al menos algunas personas que responderían: «¿Qué quieres decir? ChatGPT puede hablar; puede tener conversaciones emocionales profundas conmigo; puede resolver problemas matemáticos avanzados y programar, algo que muchos humanos no pueden hacer. ¿Quién puede decir que es más tonto que un humano?». Esa no era una conversación a la que nos enfrentábamos hace diez años, lo que dice *algo* sobre cuánto ha avanzado desde entonces.

En la actualidad, el mundo se encuentra quizás a medio camino entre «las IA son claramente más tontas que los humanos» y «depende de lo que le pidas a la IA que haga».

Quizás lo que se necesita para recorrer la distancia restante es solo un poco más de escala, como ocurre con el cerebro humano, que es muy similar al cerebro de los chimpancés, pero tres o cuatro veces más grande. O quizás la arquitectura subyacente a ChatGPT es demasiado superficial para soportar la «chispa» de la generalidad.

Quizás haya algún componente importante de la inteligencia general que los algoritmos modernos de IA simplemente no pueden manejar, y las IA modernas lo compensan aplicando cantidades masivas de práctica y memorización a los tipos de tareas que se pueden resolver con práctica intensiva. En ese caso, tal vez solo se necesite un invento algorítmico brillante (y también increíblemente estúpido) para subsanar ese déficit, y las IA serán capaces de comprender la mayoría de las cosas que un humano puede comprender y aprender de la experiencia con la misma eficacia que un humano. (Sin dejar de ser capaces de leer y memorizar todo Internet). O tal vez se necesiten cuatro avances algorítmicos más. Nadie lo sabe, como se explica en el capítulo 2.

#### **\* Hay muchas cosas diferentes que se pueden entender por «inteligencia general».** {#*-hay-muchas-cosas-diferentes-que-se-pueden-entender-por-«inteligencia-general».}

Por «las IA ahora son generalmente inteligentes», alguien podría referirse a que las IA han adquirido esa combinación de habilidades poco comprendida que desató el infierno en forma de civilización humana.

O tal vez se refieran a que la IA ha avanzado al menos hasta el punto de que ahora la gente *debate vehementemente* sobre si los humanos o las IA son realmente más inteligentes.

O tal vez tengan en mente un momento en el que la gente haya dejado de discutir, porque está claro que las IA son, en general, mucho más inteligentes que cualquier humano. O un momento en el que la gente haya dejado de discutir, porque ya no queda nadie con quien discutir; la humanidad ha ido demasiado lejos y la IA ha puesto fin a todas nuestras discusiones y esfuerzos.

No hubo un día y una hora exactos en los que se pudiera decir que las IA «empezaron a jugar al ajedrez al nivel humano». Pero cuando las IA de ajedrez pudieron derrotar al campeón mundial humano, ese momento ya había pasado.

Todo esto quiere decir que la respuesta a «¿Es ChatGPT generalmente inteligente?» podría ser sí o no, dependiendo de lo que se entienda exactamente por la pregunta. (Lo que dice mucho sobre el progreso de la IA en los últimos años. Deep Blue era claramente bastante limitado).

#### **La superinteligencia es una distinción más importante.** {#superinteligencia-es-una-distinción-más-importante.}

Dado que hay varias cosas diferentes que «inteligencia a nivel humano» podría significar razonablemente, normalmente evitamos utilizar esa terminología nosotros mismos, excepto cuando hablamos de IA superhumana. Esta es también la razón por la que solemos evitar decir «inteligencia artificial general». Si necesitamos hablar de alguna de esas ideas, la explicaremos con más detalle.

Sí utilizaremos términos como «IA más inteligente que los humanos», «IA superhumana» o «superinteligencia», que suponen algún tipo de referencia humana:

* Por «IA más inteligente que los humanos» o «IA superhumana» (aquí y en el libro), nos referimos a una IA que tiene esa «chispa de generalidad» que separa a los humanos de los chimpancés *y* que es claramente mejor en general que los humanos más inteligentes a la hora de resolver problemas y descubrir qué es verdad.

  La IA superhumana podría ser solo *ligeramente* más inteligente que los mejores humanos, y puede que haya algunas tareas en las que los mejores humanos sigan teniendo mejor desempeño. Pero asumiremos, aquí y en el libro, que «IA más inteligente que los humanos» significa *al menos* que una comparación justa a través de una amplia gama de tareas complicadas tendría a la IA con mejor desempeño que los humanos más competentes, en todo tipo de tareas difíciles.

* Por «**superinteligencia**» o «**superinteligencia artificial**» (ASI), nos referimos a una IA superhumana que supera *vastamente* a la inteligencia humana. Asumiremos que los seres humanos individuales y los grupos reales de seres humanos son completamente incapaces de competir con la IA superinteligente en cualquier dominio de importancia práctica, por las razones discutidas en el capítulo 6\.

Este libro utilizará principalmente los términos «superhumano» y «superinteligente» de manera intercambiable. La distinción cobra mayor relevancia en la parte II, donde describimos un escenario de toma de poder de la IA en el que las IA comienzan siendo débilmente más inteligentes que los humanos, pero *no* superinteligentes. Esto ayuda a ilustrar que la superinteligencia es posiblemente excesiva: la IA puede volverse superinteligente pronto, pero no *necesita* ser tan inteligente para causar la extinción humana.

Estas son definiciones muy generales, pero son suficientemente buenas para los propósitos de este libro.

Este no es un libro que proponga una teoría compleja de la inteligencia para luego deducir algunas implicaciones esotéricas de la teoría que presagian un desastre. En cambio, operaremos a un nivel bastante básico, con afirmaciones como:

* En algún momento, la IA probablemente logrará completamente *lo que sea* que permite a los humanos (y no a los chimpancés) construir cohetes, centrifugadoras y ciudades.  
* En algún momento, la IA *superará* a los humanos.  
* Las IA poderosas probablemente tendrán sus propios objetivos que perseguirán obstinadamente, porque perseguir objetivos obstinadamente es útil para una amplia gama de tareas (y, por ejemplo, los humanos evolucionaron objetivos precisamente por esta razón).

Afirmaciones como esas, sean correctas o incorrectas, no dependen de que tengamos conocimiento especial de todo el funcionamiento interno de la inteligencia. Podemos ver el camión que se dirige hacia nosotros, incluso sin apelar a un modelo complicado de los componentes internos del camión. O eso argumentaremos.

Y argumentos sencillos como estos no dependen de si ChatGPT es «realmente» de nivel humano o «realmente» una inteligencia general. Hace lo que hace. Las IA del futuro harán más cosas y mejor. El resto del libro analiza adónde conduce ese camino.

### ¿Qué tan inteligente podría llegar a ser una superinteligencia? {#¿qué-tan-inteligente-podría-llegar-a-ser-una-superinteligencia?}

#### **Muy inteligente.** {#muy-inteligente.}

Por cada punto en la lista del Capítulo 1 sobre las razones por las que el cerebro humano no se acerca a los límites de las posibilidades físicas, las máquinas *podrían* acercarse a esos límites.

Las leyes de la física permiten la existencia de genios que piensan decenas de miles (si no millones o miles de millones) de veces más rápido que los humanos,[^19] que nunca necesitan dormir ni comer, y que pueden hacer copias de sí mismos e intercambiar experiencias.

Y eso sin tener en cuenta las mejoras en la *calidad* de la cognición de una IA.

Incluso si la IA solo es muy superior a los humanos en una o dos dimensiones, esto puede ser suficiente para obtener una ventaja decisiva. A lo largo de la historia, grupos de humanos han aprovechado repetidamente ventajas relativamente pequeñas en ciencia, tecnología y planificación estratégica para alcanzar posiciones dominantes sobre otros grupos. Piensa, por ejemplo, en los conquistadores españoles. Y eso sin variar significativamente en la arquitectura o el tamaño del cerebro.

Incluso las pequeñas ventajas intelectuales pueden traducirse en grandes ventajas prácticas, y las pequeñas ventajas pueden acumularse con extrema rapidez. Pero las probables ventajas de las IA no parecen en absoluto pequeñas.

Para más argumentos sobre la importancia de este nivel de inteligencia —que podría traducirse en poder en el mundo real—, consulta el Capítulo 6.

### Pero, ¿no hay grandes obstáculos para alcanzar la superinteligencia? {#pero-¿no-hay-grandes-obstáculos-para-alcanzar-la-superinteligencia?}

#### **No está claro.** {#no-está-claro.}

En gran medida, el campo está avanzando a ciegas. Podría ser que no quedaran obstáculos reales y que pequeños ajustes en las técnicas actuales escalen hasta la superinteligencia, o escalen hasta IA lo suficientemente inteligente como para construir IA ligeramente más inteligente que construya IA ligeramente más inteligente que construya IA superinteligente.

Si *hay* obstáculos importantes, no sabemos cuánto tiempo tardará la humanidad en superarlos (con o sin la ayuda de la IA).

Lo que sí sabemos es que los principales laboratorios de IA están avanzando explícitamente en esa dirección, y sabemos que están haciendo progresos. Antes, las máquinas no podían dibujar, hablar ni escribir código; ahora sí pueden.

#### **\* El campo es bueno para superar obstáculos.** {#*-el-campo-es-bueno-para-superar-obstáculos.}

Durante décadas, las IA tuvieron dificultades incluso para distinguir una imagen de un gato de una imagen de un coche. El punto de inflexión se produjo en 2012, cuando los investigadores de la Universidad de Toronto Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton diseñaron [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), una red neuronal convolucional que superó dramáticamente el estado del arte. Este acontecimiento es ampliamente reconocido como el inicio de la revolución moderna de la IA, en la que las redes neuronales artificiales se utilizan para impulsar casi toda la IA moderna.

La IA solía ser mala en los juegos de mesa. Incluso después de que la IA de ajedrez [Deep Blue](https://www.ibm.com/history/deep-blue) derrotara al gran maestro Garry Kasparov en 1997, las computadoras seguían teniendo dificultades con el número mucho mayor de movimientos posibles en el juego del Go. Eso fue así hasta 2016, cuando [AlphaGo](https://deepmind.google/research/projects/alphago/) derrotó al campeón mundial Lee Sedol tras entrenarse con miles de partidas humanas, utilizando una nueva arquitectura que combinaba redes neuronales profundas con búsqueda en árbol. Una vez que vencieron al Go, el equipo de DeepMind utilizó ese mismo algoritmo de una manera más general, llamado [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/), y descubrió que dominaba no solo el Go, sino también otros juegos como el ajedrez y el shogi.

Los primeros chatbots eran comunicadores limitados.[^20] Luego, en 2020, la maduración de la arquitectura del transformador nos dio [GPT-3](https://arxiv.org/abs/2005.14165), que era lo suficientemente sofisticado como para traducir textos, responder preguntas e incluso generar muestras de artículos de noticias que parecían reales. Una vez que se reentrenó un poco para actuar como un chatbot, se convirtió en la aplicación de consumo de más rápido crecimiento de todos los tiempos.[^21]

¿Existen obstáculos entre la IA moderna y la «auténtica», el tipo de IA que podría convertirse en una superinteligencia o crearla?

Quizás. Quizás se necesiten más conocimientos arquitectónicos, como los que hay detrás de AlexNet, que abrió todo el campo de la IA moderna, o como los que hay detrás de AlphaZero, que finalmente permitió a las IA ser buenas en múltiples juegos utilizando el mismo algoritmo, o los que hay detrás de ChatGPT, que hicieron que las computadoras empezaran a hablar. (O quizás no; quizás las IA modernas [crucen silenciosamente algún umbral](#¿superará-la-ia-los-umbrales-criticos-y-despegará?) y eso será todo).

Pero si quedan obstáculos, los investigadores en este campo probablemente los superarán. Son bastante buenos en eso, y ahora hay muchos más investigadores trabajando en este problema que en 2012.[^22]

En julio de 2025, las IA siguen teniendo dificultades con tareas que requieren memoria a largo plazo y planificación coherente, como jugar al videojuego Pokémon.[^23] Uno podría sentirse tentado a unirse a los escépticos y reírse de los últimos fracasos de las IA: ¿cómo podrían las IA, que tienen dificultades con videojuegos sencillos, estar siquiera cerca de la superinteligencia?

Del mismo modo, en 2019 las IA tenían muchas dificultades para hablar de forma coherente. Pero eso no significaba que el éxito estuviera a veinte años de distancia. Los laboratorios están trabajando arduamente para identificar los obstáculos que hacen que las IA rindan por debajo de lo esperado en determinados tipos de tareas, y es probable que estén en camino de encontrar nuevas arquitecturas que sean mejores en memoria a largo plazo y planificación. Nadie sabe lo que esas IA serán capaces de hacer.

Si esa siguiente fase no es suficiente para que las IA empiecen a automatizar la investigación científica y tecnológica (incluido el desarrollo de IA aún más inteligentes), los investigadores simplemente centrarán su atención en el siguiente obstáculo. Seguirán avanzando, a menos que la humanidad intervenga y prohíba tal investigación, un tema que trataremos en capítulos posteriores.

### ¿No es imposible predecir el comportamiento de una superinteligencia? {#¿no-es-imposible-predecir-el-comportamiento-de-una-superinteligencia?}

#### **En algunos aspectos, pero no en todos los aspectos.** {#en-algunos-aspectos,-pero-no-en-todos.}

Stockfish 17 es mejor que nosotros manejando un tablero de ajedrez. Si jugáramos una partida de ajedrez contra Stockfish, no podríamos predecir sus movimientos, ya que para ello tendríamos que ser al menos tan buenos en ajedrez como Stockfish 17\. Sin embargo, nos resultaría fácil predecir el ganador de la partida.[^24] Es difícil predecir qué movimientos hará Stockfish; es fácil predecir que *ganará*.

Lo mismo sucede con las IA que predicen y dirigen el mundo real. Cuanto más inteligentes son, más difícil es predecir exactamente lo que harán, pero más fácil es predecir que alcanzarán cualquier destino hacia el que se dirijan.

### ¿No serán las máquinas fundamentalmente poco creativas o tendrán algún otro defecto fatal? {#¿no-serán-las-máquinas-fundamentalmente-poco-creativas-o-tendrán-algún-otro-defecto-fatal?}

#### **No.** {#no.}

En su mayor parte, diferimos la pregunta de si las máquinas pueden ser creativas hasta el Capítulo 3. Sin embargo, aquí diremos esto: las máquinas no necesitan tener algún defecto fatal que las equilibre frente a los humanos, de modo que el indomable espíritu humano tenga la oportunidad de ganar.

Si los pájaros dodo hubieran tenido su propia industria cinematográfica, los guiones que los dodos escribieron sobre la invasión humana de su isla de Mauricio podrían haber hecho que las armas y el acero de los humanos se compensaran con desventajas humanas. Quizás la angustia existencial inducida por la inteligencia hace que los humanos se paralicen de desesperación en el último momento, justo el tiempo suficiente para que los heroicos dodos contraataquen y los maten a todos a picotazos.

Esta es quizás una historia que los dodos encontrarían satisfactoria: que la inteligencia no puede ser una ventaja militar neta sobre los picos fuertes, que los cerebros más grandes de los humanos deben tener algún defecto fatal que permita a los orgullosos dodos ganar después de todo.

En realidad, las ventajas aparentes de los humanos son ventajas *reales*. Las desventajas de los cerebros humanos no son desventajas *netas* en un conflicto militar con cerebros de pájaros. La contienda entre humanos y dodos termina siendo desigual, y eso es todo.

Incluso cuando los humanos luchan contra otros humanos, las ametralladoras son una ventaja suficiente para que un ejército con ametralladoras suele vencer a un ejército sin ellas. Hay raras excepciones a esta regla, y a la gente le encanta contarlas porque la excepción es una historia más divertida que la norma. Pero las excepciones ocurren en la vida real con mucha menos frecuencia que en las historias.

Predecimos lo mismo sobre las IA avanzadas con vastas memorias y mentes, que pueden copiarse a sí mismas miles de veces y pensar a una velocidad diez mil veces superior a la de un humano; mentes que pueden razonar de forma más válida, generalizar más rápido y con mayor precisión a partir de menos lecciones duras, y mejorarse a sí mismas.

No es una pregunta trampa, y no habrá un giro argumental sorprendente, por mucho que nos gustaría que lo hubiera.

### ¿No hay algo especial en los seres humanos que las simples máquinas nunca podrían emular? {#¿no-hay-algo-especial-en-los-seres-humanos-que-las-simples-máquinas-nunca-podrían-emular?}

#### **Parece poco probable y no especialmente relevante.** {#parece-poco-probable-y-no-especialmente-relevante.}

El cerebro y el cuerpo humanos están formados por partes que podemos estudiar y llegar a comprender. Hay muchas cosas que no entendemos sobre el cerebro, pero eso no significa que las partes que no entendemos funcionen por arte de magia y que los humanos nunca podamos construir nada similar. Solo significa que los cerebros son máquinas enormemente *complicadas*. El cerebro humano tiene cientos de billones de sinapsis, y nos queda un largo camino por recorrer para comprender todos los principios importantes de alto nivel que intervienen en su funcionamiento.

La inteligencia también está compuesta por piezas: algoritmos y cálculos individuales que nuestros cerebros realizan de forma natural, aunque no tenemos una comprensión científica de cómo funcionan nuestros propios cerebros.

Incluso si hubiera algún aspecto del razonamiento biológico que fuera muy difícil de implementar en las máquinas, eso no significaría que la IA nunca superaría a la humanidad. Las IA podrían simplemente hacer el mismo tipo de trabajo de una manera diferente, como cuando la IA Deep Blue encontró jugadas ganadoras en el ajedrez de una manera muy diferente a la de Garry Kasparov.[^25] Lo que importa no es si las máquinas poseen todas las características únicas de los seres humanos, sino si se vuelven capaces de predecir y dirigir el mundo.

Los próximos capítulos ayudarán a arrojar más luz sobre este punto. En el capítulo 2, cubriremos cómo las IA modernas son cultivadas en lugar de creadas, y cómo el proceso de cultivo tiende a hacer que las IA sean muy capaces. En el capítulo 3, cubriremos cómo los intentos de hacer que las IA sean cada vez más capaces tienden a impulsarlas cada vez más hacia el logro de objetivos difíciles. Y en el capítulo 4, discutiremos cómo es poco probable que esos objetivos sean los que los desarrolladores pretendían, ni los que los usuarios solicitaron. Todo esto es suficiente para que las IA conduzcan al mundo a la ruina, independientemente de si consideras que las IA tienen alguna chispa vital, o conciencia, o cualquier otra cosa que puedas imaginar que hace especiales a los seres humanos.

Véase también, en los recursos en línea venideros:

* Capítulo 2: «[¿No es la IA "solo matemáticas"?](#¿no-son-las-ia-«solo-matemáticas»?)» y «[¿No serán las IA inevitablemente frías y lógicas, o les faltará de algún modo alguna chispa crucial?](#¿no-serán-las-ia-inevitablemente-frías-y-lógicas-o-les-faltará-alguna-chispa-crucial?)»  
* Capítulo 3: «[Antropomorfismo y mecanomorfismo](#antropomorfismo-y-mecanomorfismo)»  
* Capítulo 5: «[Eficacia, conciencia y bienestar de la IA](#eficacia,-conciencia-y-bienestar-de-la-ia)»

### ¿Estás diciendo que las máquinas adquirirán conciencia? {#¿estás-diciendo-que-las-máquinas-adquirirán-conciencia?}

#### **No necesariamente, y eso parece ser un tema aparte.** {#no-necesariamente,-y-eso-parece-ser-un-tema-aparte.}

*Si alguien lo construye, todos morirán* no discute en absoluto la conciencia de las máquinas, centrándose en su lugar en la *inteligencia* de las máquinas. Como primer paso para hablar de la conciencia, deberíamos aclarar primero qué tipo de «conciencia» tenemos en mente.

Cuando alguien pregunta «¿Mi perro es consciente?», puede referirse a varias cosas diferentes, como:

* ¿Rover realmente *entiende* las cosas o solo está ejecutando instintos complicados? ¿Está *pensando* o solo actuando por inercia?  
* ¿Es consciente de sí mismo? ¿Sabe que existe? ¿Puede reflexionar sobre su propio proceso mental y construir modelos mentales complejos de sí mismo?  
* ¿Tiene experiencias subjetivas genuinas? ¿Tiene su propio punto de vista interno o es simplemente un autómata sin mente? ¿Hay *algo parecido* a ser mi perro? Cuando me voy por un tiempo, aúlla como si me extrañara; ¿es porque *realmente experimenta soledad* (o algo por el estilo)? ¿O es más bien como un simple programa informático inconsciente que solo muestra los comportamientos relevantes sin sentirlo realmente?

Podemos plantearnos preguntas similares sobre la IA.

* **¿Tiene ChatGPT una «comprensión real»?** Bueno, es capaz de realizar algunas tareas cognitivas muy complejas muy bien, y otras no tan bien. Funciona bien en muchas tareas novedosas con las que nunca se ha encontrado en el entrenamiento, tareas que requieren sintetizar y modificar información de forma creativa y novedosa. Así que, en cierto momento, la pregunta de si «realmente comprende» empieza a parecer una discusión sobre definiciones. La pregunta de importancia desde el punto de vista práctico, la que es más relevante para nuestra supervivencia, es qué capacidades reales tienen ahora las IA y qué capacidades es probable que muestren en los próximos meses y años.  
* **¿ChatGPT es consciente de sí mismo?** Una vez más, ChatGPT parece ser bueno para modelarse a sí mismo en algunos aspectos, y malo en otros. Hay un serio factor de confusión en el hecho de que todo el paradigma que condujo a ChatGPT se centró en hacer cosas que *parecieran* conscientes de sí mismas, dando el mismo tipo de respuestas que darían los humanos. Se puede discutir si ChatGPT ha superado algunos umbrales importantes en cuanto a la autoconciencia, y se puede discutir qué umbrales hay en el futuro. Pero, tarde o temprano, podemos esperar que existan IA con capacidades *prácticas* extremadamente potentes para comprenderse y razonar sobre sí mismas: la capacidad de depurarse a sí mismas, de diseñar versiones nuevas y mejoradas de sí mismas, de hacer planes complicados sobre su posición en el mundo, etc.  
* **¿Tiene ChatGPT experiencias subjetivas genuinas?**

La última de estas preguntas es la más espinosa desde el punto de vista filosófico y da lugar a un clúster de preguntas sobre si las IA como ChatGPT son entidades que merecen consideración moral. Discutiremos esos temas [más adelante](#eficacia,-conciencia-y-bienestar-de-la-ia), en los debates ampliados relacionados con el capítulo 5\.

Cuando usamos la palabra «consciente», nos referimos específicamente a «tener una experiencia subjetiva» y no a cosas como el automodelado y la comprensión práctica profunda.[^26]

Nuestra mejor suposición es que las IA actuales probablemente no sean conscientes (aunque cada año estamos más inseguros al respecto) y que la experiencia subjetiva no sea necesaria para la superinteligencia. 

Pero estas son solo conjeturas, aunque basadas en una cantidad razonable de reflexión y teorización. No creemos que sea en absoluto «tonto» preocuparse por la posibilidad de que algunos sistemas de IA actuales o futuros puedan ser conscientes, o incluso preocuparse por si estamos maltratando gravemente a las IA actuales, especialmente cuando hacen cosas como amenazar con suicidarse[^27] después de no haber podido depurar el código.

Cualquier entidad que constituyera una superinteligencia según nuestro criterio sería necesariamente muy buena modelándose a sí misma: pensando en sus propios cálculos, mejorando sus heurísticas mentales, comprendiendo y prediciendo los impactos de su propio comportamiento en el entorno circundante, etc. Pero nuestra mejor hipótesis es que la conciencia autoconsciente al estilo humano es solo *una forma particular* en que una mente puede modelarse a sí misma de manera efectiva; no es un prerrequisito necesario para el razonamiento reflexivo.

La conciencia puede ser una parte importante de cómo los seres humanos son tan buenos manipulando el mundo, pero eso no significa que las máquinas no conscientes sean defectuosas e incapaces de predecir y dirigir el mundo. Los submarinos no nadan de forma análoga a los seres humanos; cumplen la tarea de moverse por el agua de una manera fundamentalmente diferente. Esperamos que una IA sea capaz de *tener éxito en los mismos desafíos en los que los humanos tienen éxito*, pero no necesariamente que lo haga a través del mismo canal de experiencia subjetiva que utilizan los humanos.

(Véase también el caso análogo de [la curiosidad](#la-curiosidad-no-es-convergente), al que volveremos en el suplemento del Capítulo 4).

Dicho de otra manera: la sangre es muy importante en el funcionamiento de un brazo humano, pero eso no significa que los brazos robóticos requieran sangre para operar. Un brazo robótico no es defectuoso como lo sería un brazo humano sin sangre; simplemente funciona de una manera diferente, sin sangre. Nuestra mejor hipótesis es que las superinteligencias de máquina funcionarán de una manera diferente, no consciente, aunque esta hipótesis no es importante para nuestro argumento en el libro.

Nuestro enfoque en *If Anyone Builds It, Everyone Dies* está en la inteligencia, donde «inteligencia» se define en términos de la capacidad de un razonador para predecir y dirigir el mundo, independientemente de si el cerebro de ese razonador funciona como un cerebro humano. Si una IA está inventando nueva tecnología e infraestructura y la está proliferando por toda la faz del planeta de una manera que nos mata como efecto secundario, entonces detenerse a preguntar «¿Pero es consciente?» parece algo académico.

Entraremos en más detalles sobre por qué creemos que la predicción y la dirección probablemente no requieren conciencia (y lo que esto significa para cómo debemos pensar sobre el bienestar y los derechos de la IA) después del Capítulo 5, una vez que hayamos sentado más bases. Véase «[Eficacia, conciencia y bienestar de la IA](#eficacia,-conciencia-y-bienestar-de-la-ia)» para esa discusión.

## Discusión extendida {#discusión-extendida-1}

### Más sobre la inteligencia como predicción y dirección {#más-sobre-la-inteligencia-como-predicción-y-dirección}

Si le preguntas a un físico sabio qué es un motor, es posible que empiece señalando un motor cohete, un motor de combustión y una rueda de hámster, y diga: «Todos ellos son motores», y luego señale una roca y diga: «Pero eso no lo es».

Esa sería una descripción señalando motores en el mundo, en lugar de intentar dar una definición verbal. Si le presionaras para que te diera una definición verbal, probablemente te diría que un motor es cualquier cosa que convierte la energía no mecánica en energía mecánica, en movimiento.

Esto no es tanto una afirmación sobre lo que *es* un motor, sino más bien una afirmación sobre lo que *hace* un motor. Todo tipo de cosas diferentes pueden ser motores; los componentes internos de un motor de cohete, un motor eléctrico y los músculos de un hámster tienen muy poco en común. No hay mucho que se pueda decir útilmente sobre todos esos componentes internos a la vez, excepto que todos convierten otros tipos de energía en energía mecánica.

Diríamos que la inteligencia es similar. Hay muchos componentes internos diferentes que pueden dar lugar a la inteligencia, incluidos los componentes internos biológicos y mecánicos. Una «inteligencia» es cualquier cosa que realice el *trabajo* de la inteligencia.

Descomponemos ese trabajo en «predicción» y «control» porque este punto de vista está respaldado por varios resultados formales.

Comenzaremos discutiendo el sentido en el que medir la predicción es bastante *objetivo*. Luego contrastaremos esto con el control, que tiene un grado de libertad que la predicción no tiene.

#### **Las mismas predicciones** {#las-mismas-predicciones}

Es relativamente sencillo comprobar qué tan bueno es alguien haciendo predicciones, al menos en casos donde la predicción es de la forma «voy a ver X» y luego efectivamente ven X.

También podemos calificar el desempeño de las personas cuando hacen predicciones *inciertas*. Supongamos que piensas: «Estoy bastante seguro de que el cielo está azul en este momento, pero podría estar gris en su lugar. Y definitivamente *no* está negro». Si miras por la ventana y el cielo está efectivamente azul, deberías recibir más crédito que si estuviera gris, y mucho más que si estuviera negro.

Si fueras un investigador de IA intentando representar esas anticipaciones como números en una computadora, podrías hacer que tu IA inicial elija números para representar qué tan fuerte o débilmente espera varias cosas, y luego reforzar la IA en proporción a qué tan alto fue el número que asignó a la respuesta correcta.

Eso, por supuesto, saldría mal rápidamente, una vez que la IA aprendiera a asignar un valor de tres octotrigintillones a cada posibilidad.

(Al menos, saldría mal de esa manera si estuvieras entrenando la IA usando métodos modernos de IA. Para una introducción a esos métodos, véase el Capítulo 2.)

«Vaya», podrías decir. «Se supone que los números asignados a una colección mutuamente excluyente y exhaustiva de posibilidades deben sumar como máximo el 100 %».

Ahora, cuando lo intentes de nuevo, descubrirás que la IA siempre asigna el valor del 100 % a una sola posibilidad, concretamente la posibilidad que considera más probable.

¿Por qué? Bueno, supongamos que la IA piensa que la posibilidad más probable tiene aproximadamente un 80 % de probabilidades de ocurrir. Entonces, la estrategia de asignar el 100 % a la respuesta más probable obtiene un refuerzo del 100 % 8/10 del tiempo, lo que resulta en una fuerza de refuerzo promedio de 0,8.

Por el contrario, la estrategia de asignar el 80 % a la respuesta más probable y el 20 % a su contraria obtiene un refuerzo del 80 % en 8 de cada 10 ocasiones y un refuerzo del 20 % en 2 de cada 10 ocasiones. Esto da como resultado una fuerza de refuerzo de solo 0,64 en promedio. Por lo tanto, la estrategia de «asignar el 100 % a una respuesta» obtiene más refuerzo y gana.

Si quieres una estrategia de refuerzo que haga que la IA asigne un número como el ochenta por ciento a las posibilidades que ocurren aproximadamente 8/10 de las veces, debes puntuarla según el *logaritmo* de la probabilidad que asigna a la verdad. Hay otras posibilidades, pero tomar logaritmos es la única con una propiedad adicional útil: Cuando la IA predice múltiples posibilidades (como el color del cielo y la humedad del suelo), no importa si las consideras como una gran pregunta (sobre si el exterior es azul y seco, azul y húmedo, gris y seco o gris y húmedo) o como dos preguntas separadas (sobre azul frente a gris y sobre seco frente a húmedo).

De hecho, los investigadores de IA actuales entrenan a las IA para que hagan predicciones haciéndolas generar números que interpretamos como probabilidades y reforzándolas en proporción al logaritmo de la probabilidad que la IA asignó a la verdad. Pero esto no es solo un resultado empírico sobre el entrenamiento de máquinas, sino también un resultado teórico que se conocía mucho antes de que se entrenara a ChatGPT. Si conocías esa teoría, podrías haber adivinado correctamente de antemano que una buena forma de entrenar a las IA para realizar el trabajo de predicción sería puntuar las predicciones utilizando logaritmos.

No es necesario conocer estas matemáticas para evaluar los argumentos de *If Anyone Builds It, Everyone Dies* (Si alguien lo construye, todos mueren). Pero estos son el tipo de principios que están detrás cuando hablamos de «predicción» y «dirección».

Hay [matemáticas](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation) sobre cómo medir el trabajo de predicción. Las matemáticas dicen que, en la medida en que tus anticipaciones sobre lo que va a suceder son útiles, pueden expresarse como probabilidades, independientemente de si has pensado conscientemente en probabilidades numéricas o no. Y esto da lugar a una única [regla de puntuación](https://en.wikipedia.org/wiki/Scoring_rule) que te incentiva a informar de tus probabilidades reales y que es invariante bajo la descomposición de las predicciones.

El resultado de todas estas matemáticas es que las predicciones pueden puntuarse de forma *objetiva*. Cuando una mente o una máquina anticipa el color que verá al mirar por la ventana, la siguiente palabra que verá al leer una página web o la señal de tráfico que verá al conducir hacia el aeropuerto, solo hay (en términos generales) una forma realmente buena de evaluar lo bien que lo están haciendo.

La cuestión no es que, si eres inteligente, tengas que ir por ahí murmurando números sobre el color del cielo antes de mirar por la ventana. Cuando anticipas que verás un cielo azul o gris en lugar de un cielo negro, algo en tu cerebro está actuando un poco como una calculadora de probabilidades *en algún lugar* de ahí, te des cuenta o no.

Más bien, el punto es que *todo* comportamiento similar a una predicción, ya sea una afirmación explícita, una anticipación sin palabras o cualquier otra cosa, está sujeto a una regla de puntuación objetiva.

Todo esto significa que cuando dos mentes trabajan con la misma información inicial, tienden a converger en las mismas predicciones a medida que mejoran en la capacidad de predecir cosas. Porque hay una sola forma de puntuar las predicciones (comparándolas con la realidad), y solo hay una realidad que predecir, y las mentes que son mejores prediciendo concentrarán cada vez más sus anticipaciones en la verdad, casi por definición.

Todo esto es muy diferente a la situación con la navegación, a la que pasaremos a continuación.

#### **Diferentes destinos** {#diferentes-destinos}

Dos mentes que son extremadamente buenas para predecir el mundo probablemente hagan predicciones similares.

Por el contrario, dos mentes extremadamente hábiles para *navegar* por el mundo a menudo *no* navegarán hacia el mismo destino.

Esta distinción es útil para pensar de forma más concreta sobre la inteligencia, y también se corresponde con una división entre los problemas de ingeniería más sencillos y menos sencillos en la IA.

Cuando entrenas a una IA para predecir cosas, en cierto sentido todos los mejores métodos de predicción acaban produciendo resultados similares. (Es decir, suponiendo que el sistema llegue a ser competente; las formas de fallar son más variadas).

Supongamos que entrenas a una IA para predecir la siguiente imagen que verá una cámara web apuntando al cielo desde una ventana. Casi cualquier modelo que empiece a ser lo suficientemente bueno en eso —en asignar de antemano una probabilidad mucho mayor a lo que realmente acaba viendo— predecirá que el cielo estará despejado y azul, nublado y gris o oscuro por la noche, pero no a cuadros.

La tecnología exacta que utilices no influirá mucho en el resultado final. Cualquier método que funcione, que obtenga buenas puntuaciones en general, acabará asignando una probabilidad similar al azul del cielo.

En cambio, la tarea de «dirigir» tiene un parámetro libre enorme y complicado: ¿hacia qué destino intenta dirigir el sistema?

Los generales de bandos opuestos en una guerra pueden ambos ser hábiles, pero eso no significa que intenten lograr lo mismo. Dos generales pueden tener habilidades similares, pero utilizarlas con fines muy diferentes.[^28]

En la parte predictiva de un sistema de IA, solo hay una cosa que parece predecir muy bien: asignar de antemano altas probabilidades a lo que finalmente se observa. Y cuando un sistema cognitivo parece estar mejorando en general en sus predicciones, probablemente esté mejorando en el tipo concreto de predicción que tú deseabas. Solo hay un «tipo» de predicción que hacer dentro de tu configuración, y un sistema que tiene éxito probablemente lo esté haciendo.

Si el sistema sigue cometiendo un error de predicción concreto, el simple hecho de dotarlo de más cálculos y más datos puede corregir ese error de predicción automáticamente. Puedes conseguir que el sistema funcione mejor (en la predicción de las cosas que te interesan) *simplemente* dotándolo de más poder.

Con el direccionamiento, este no es el caso.

Podemos reforzar aún más esta distinción revisando la literatura formal. La dirección —planificación, toma de decisiones, evitación de obstáculos, diseño, etc.— es un tema que se ha estudiado ampliamente en las ciencias. Un resultado matemático importante relacionado con la dirección es el [teorema de utilidad de von Neumann-Morgenstern](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem).

[En términos generales, este teorema](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem) dice que cualquier entidad que persiga unos resultados por encima de otros debe *o bien* ser ineficiente[^29] *o bien* estar bien descrita por un conjunto de creencias probabilísticas y una «función de utilidad» , una función que describe cómo se compensan entre sí los diferentes resultados. Las creencias, entonces, pueden calificarse según su precisión (como se describe en la sección anterior), mientras que la función de utilidad es un parámetro completamente libre.

Por supuesto, ninguna mente finita puede ser perfectamente eficiente. La lección que extraemos de este teorema (y otros resultados de este tipo) es que, en la medida en que una mente realiza *cualquier* tarea no trivial de manera muy eficaz, en cierto sentido (aunque solo sea de forma implícita e inconsciente) está realizando dos tipos de trabajo distintos: un trabajo similar a la creencia (predicción) y un trabajo similar a la satisfacción de preferencias (dirección).

Por ejemplo, consideremos la fábula de Esopo sobre el zorro y las uvas. Un zorro ve unas uvas de aspecto delicioso colgando de una vid. El zorro salta para cogerlas, pero no puede saltar lo suficientemente alto, por lo que las abandona diciendo: «Bueno, probablemente estaban agrias de todos modos».

Si nos creemos lo que dice el zorro, su (in)capacidad para alcanzar las uvas está *influyendo* en su predicción sobre si las uvas están agrias. Si se mantiene firme en esa nueva predicción y se niega a comer las uvas «agrias» por orgullo, incluso si más tarde tiene la oportunidad de comerlas, entonces el comportamiento del zorro es *ineficaz*.[^30] Podría haberlo hecho mejor manteniendo una distinción más clara entre sus predicciones (sobre la dulzura de las uvas) y su direccionamiento (su capacidad para conseguir las uvas).

A grandes rasgos, las mentes que funcionan bien pueden separarse en *lo que predicen* y *hacia lo que se dirigen* (además de algunas ineficiencias). Y, como hemos visto, lo primero puede puntuarse de forma relativamente objetiva, mientras que lo segundo puede variar enormemente entre mentes de competencia similar.

#### **Predictores impuros** {#predictores-impuros}

Desafortunadamente, el hecho de que la predicción esté más restringida que la dirección no significa que podamos construir una superinteligencia confiable que solo prediga y no dirija.

Aunque las matemáticas dicen que una mente que funciona bien puede modelarse más o menos como «predicciones probabilísticas más una dirección de dirección», esto no significa que las IA del mundo real tengan módulos de «predicción» y «dirección» claramente separados.

Una forma de ver por qué es así: una «predicción» sobrehumana no consiste solo en generar probabilidades y que esas probabilidades sean mágicamente buenas. Una buena predicción requiere *trabajo*. Requiere planificación y pensar en formas de alcanzar objetivos a largo plazo, es decir, requiere *dirección*.

Si estás tratando de predecir el mundo físico, a veces necesitas desarrollar teorías de física y descubrir las ecuaciones que rigen esa parte del mundo físico. Y para hacer eso, a menudo necesitarás diseñar experimentos, llevarlos a cabo y observar los resultados.

Y hacer *eso* requiere planificación; requiere dirección. Si a mitad de la construcción de tu aparato experimental te das cuenta de que vas a necesitar imanes más potentes, tendrás que tomar la iniciativa y cambiar de rumbo a mitad de camino. Las buenas predicciones no son gratuitas.

Incluso *elegir qué tipo de pensamientos pensar* *y en qué orden* es un ejemplo de dirección (aunque sea una dirección que los humanos a menudo hacemos de forma inconsciente), porque requiere cierto nivel de estrategia y elegir las herramientas adecuadas para la tarea en cuestión. Para pensar con claridad y, por lo tanto, predecir mejor las cosas, necesitas organizar tus pensamientos y acciones en torno a varios objetivos a largo plazo. (Volveremos al tema del papel central de la dirección en el capítulo 3, «Aprender a desear»).

La distinción matemática entre predicción y dirección es que hay aproximadamente un conjunto «correcto» de predicciones hacia el que se puede empujar a la mente utilizando una puntuación adecuada, pero no hay un destino de dirección «correcto» (objetivamente, desde la neutralidad del agente).[^31] A medida que una IA se entrena para ser más capaz de forma genérica, sus predicciones se vuelven más precisas, pero su dirección no se orienta automáticamente hacia el destino que los humanos consideran bueno, porque la precisión es objetiva, mientras que la «bondad» es un objetivo de dirección.

La precisión converge; la dirección, no.

*En principio*, debería haber formas de garantizar que una IA se oriente hacia los destinos que deseamos. *En la práctica*, esto es difícil, en gran parte porque es un desafío muy *diferente* al de «hacer que la IA sea más inteligente y capaz en general», y no existe una métrica (simple y no manipulable) o una regla de puntuación que podamos utilizar para evaluar «¿En qué medida esta IA está tratando de orientarse hacia el destino que queremos?».

Hablaremos más sobre estos temas en los capítulos 4 y 5.

#### **Las múltiples formas de la inteligencia** {#las-múltiples-formas-de-la-inteligencia}

Algo puede ser bueno para predecir y dirigir sin tener mucho en común con el cerebro humano.

El mercado de valores realiza el trabajo de predicción en el ámbito limitado de los precios de las acciones corporativas a corto plazo. El precio de las acciones de Microsoft hoy es un indicador bastante bueno de cuál será el precio de las acciones mañana.[^32]

Supongamos que mañana hay una conferencia sobre resultados, en la que los ejecutivos de la empresa informan sobre cómo han ido las cosas durante el último trimestre. ¿El precio de las acciones es alto hoy? Eso sugiere que los informes de mañana serán optimistas. ¿Es bajo hoy? Eso sugiere que los informes de mañana serán pesimistas.

Los mercados son bastante precisos en este sentido, porque la gente puede enriquecerse corrigiéndolos cuando se equivocan. Por lo tanto, los mercados hacen un buen trabajo a la hora de realizar predicciones en este ámbito tan específico. Predicen los movimientos de los precios de las acciones de las empresas a corto plazo (e, indirectamente, aspectos como el rendimiento de las cosechas y las ventas de vehículos) en una amplia gama de bienes y servicios, mucho mejor que cualquier persona.

Algunos humanos pueden predecir los movimientos de precios *individuales* mejor que el resto del mercado de valores, de una manera que les hace muy ricos. Por ejemplo, Warren Buffett ganó doce mil millones de dólares en seis años [invirtiendo en Bank of America](https://www.cbsnews.com/news/warren-buffett-bank-of-america-12-billion/) cuando esta se tambaleaba debido a la crisis financiera de 2011. Pero incluso entonces, solo estaba prediciendo una empresa entre un gran número de ellas. Alguien que supiera sustancialmente más que el mercado de valores la mayor parte del tiempo sería capaz de ganar una cantidad ingente de dinero en muy poco tiempo. El hecho de que nadie lo haga nos permite deducir que prácticamente nadie sabe mucho más que el mercado sobre la mayoría de los precios de las acciones. [^33]

En cuanto a la dirección, la IA que juega al ajedrez llamada Stockfish realiza este tipo de trabajo en el ámbito limitado del ajedrez. Cuando juega una partida de ajedrez contra un humano, es muy hábil a la hora de producir movimientos de ajedrez que dirigen el mundo hacia estados en los que las piezas de Stockfish han hecho jaque mate al rey del oponente. No importa qué movimientos inteligentes se le ocurran al humano, o cómo luche (salvo que apague Stockfish), Stockfish canaliza la realidad hacia ese único fin. Dirige los tableros de ajedrez mejor que cualquier ser humano.

Espero que ahora puedas ver por qué no intentamos definir la inteligencia diciendo: «Bueno, debe haber algún módulo de aprendizaje, algún módulo de deliberación y algunos engranajes que implementen una chispa de deseo», o algo por el estilo. En realidad, no hay mucho en común entre el funcionamiento interno del mercado de valores, el de Stockfish y el del cerebro humano, al igual que no hay mucho en común entre el funcionamiento interno de un motor de cohete, un motor eléctrico y una rueda de hámster.

Un dispositivo inteligente es cualquier cosa que haga el trabajo de la inteligencia.

Al menos, eso es cierto según cómo definimos «inteligencia» en el libro (y según cómo suelen pensar los informáticos y los investigadores de IA sobre la «inteligencia»). Si quieres definir la inteligencia de otra manera en otros contextos, no tenemos ningún problema con ello. Las palabras son solo palabras.

Pero para entender las afirmaciones sustantivas que hacemos sobre el mundo en *If Anyone Builds It, Everyone Dies*, cuando nos oigas hablar de «inteligencia artificial», no pienses en «inteligencia artificial académica» o «[conciencia artificial](#¿estás-diciendo-que-las-máquinas-serán-conscientes?)» o «humanidad artificial». Piensa en «predicción y dirección artificiales».

### La superficialidad de las IA actuales {#la-superficialidad-de-las-ia-actuales}

En el capítulo, escribimos que se puede «ver una superficialidad» en la inteligencia de las IA actuales (a mediados o finales de 2025), si sabes dónde mirar. Si aún no lo has visto por ti mismo, aquí tienes algunos lugares donde puedes buscar:

* Claude 3.7 Sonnet, de Anthropic, [se quedó atascado en bucles repetitivos](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon) mientras intentaba superar un sencillo videojuego de Pokémon.  
* En noviembre de 2022, una de las mejores entidades jugando al Go del mundo era una IA llamada KataGo. Al menos, hasta que los investigadores encontraron una forma de [derrotarla](https://www.gleave.me/publication/2022-11-go-attack/) utilizando una serie predecible de movimientos que activaban una especie de «punto ciego» y provocaban que KataGo cometiera errores que ni siquiera cometerían los aficionados. Dos años después, los ingenieros [todavía no podían hacerla robusta](https://arstechnica.com/ai/2024/07/superhuman-go-ais-still-have-trouble-defending-against-these-simple-exploits/) frente a ataques como este.  
* Los LLM «multimodales» actuales (los que pueden trabajar con texto, imágenes y otros medios, en lugar de solo texto) tienen dificultades para interpretar [relojes analógicos y calendarios](https://arxiv.org/abs/2502.05092?utm_source=chatgpt.com) en problemas que la mayoría de los alumnos de cuarto grado pueden resolver.

* Los LLM actuales son famosos por fallar en [variaciones sencillas de un acertijo clásico del doctor con respuestas directas y sin trucos](https://aigoestocollege.substack.com/p/riddles-overconfidence-and-generative), ya que parecen incapaces de resistirse a dar la respuesta trampa que tiene el acertijo en su forma habitual.

(Los recursos en línea del capítulo 4 ofrecen una visión más técnica de [dónde puede provenir esta superficialidad](#los-llm-y-la-«superficialidad»-de-la-ia-alrededor-de-2024).)

Nada de esto significa que las IA sean estúpidas en términos generales. Las IA modernas también pueden [lograr medallas de oro](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) en la Olimpiada Internacional de Matemáticas, que es un desafío matemático difícil y respetable. Las IA modernas pueden hacer una increíble variedad de cosas, a menudo igualando o superando el desempeño humano.

Su conjunto de habilidades es *extraño*. Las fortalezas y debilidades humanas son una guía deficiente para predecir qué encontrarán más fácil o más difícil las IA, porque las IA difieren radical y fundamentalmente de los humanos en muchos aspectos.

[No estamos](#¿estás-sugiriendo-que-chatgpt-podría-matarnos-a-todos?) diciendo que ChatGPT vaya a matarte mañana. Aún existe cierta superficialidad en las IA modernas. Más bien, observamos que el campo está progresando, y [no está claro cuánto tiempo durará esta superficialidad](#cuándo-se-desarrollará-este-tipo-de-ia-tan-preocupante?).

### Apreciando el poder de la inteligencia {#apreciando-el-poder-de-la-inteligencia}

#### «**Inteligencia**» **de Hollywood** {#«inteligencia»-de-hollywood}

El concepto que llamamos «inteligencia» no está bien representado en la cultura popular, ni con ese nombre ni con ningún otro.

Las películas de Hollywood son famosas entre los científicos por estar equivocadas en casi todos los aspectos científicos que tocan. Esto puede resultar perturbador para los expertos, ya que mucha gente obtiene ideas sobre ciencia a partir de las películas.

Lo mismo ocurre con el tratamiento que Hollywood da a la inteligencia.

Hemos presenciado muchos intentos fallidos de mantener discusiones serias sobre la superinteligencia del mundo real. A menudo estas conversaciones se descarrilan porque la gente no comprende lo que significa que algo sea superinteligente en la vida real.

Supongamos que estás jugando ajedrez contra el ex campeón mundial Magnus Carlsen (calificado por las IA de ajedrez aún más poderosas como el jugador humano más fuerte en la historia registrada). La principal predicción que se deriva de «Carlsen es más inteligente (en el dominio del ajedrez)» es que te derrotará.

Incluso si Carlsen te da una ventaja de una torre, probablemente seguirás perdiendo, a menos que tú mismo seas un maestro del ajedrez. Una forma de entender la afirmación «Carlsen es más inteligente que yo en el ajedrez» es que él puede ganar la partida contra ti partiendo de menos recursos. Su ventaja cognitiva es lo suficientemente poderosa como para compensar una desventaja material. Cuanto mayor sea la disparidad entre tus capacidades mentales (en el ajedrez), más piezas tendrá que cederte Carlsen para jugar contigo de forma más o menos equitativa.

Hay una especie de *respeto* que le otorgas a Magnus Carlsen en el ámbito del ajedrez, que se refleja en cómo interpretas el significado de sus jugadas. Digamos que Carlsen hace una jugada que te parece mala. No te frotas las manos de alegría por su error. En cambio, miras el tablero para ver qué *tú* has pasado por alto.

Este es un tipo de respeto poco común que un ser humano concede a otro. Para obtenerlo de un desconocido, normalmente tendrías que ser un profesional certificado excepcionalmente bueno en algo, y solo lo obtendrías en esa profesión específica. Nadie en la faz de la Tierra tiene una reputación mundial por *nunca* hacer cosas estúpidas *en general*.

Y esta es una concepción de la inteligencia que Hollywood *realmente* no entiende.

No sería extraño que Hollywood representara a un niño de diez años que consigue hacer jaque mate a Magnus Carlsen en el ajedrez «[haciendo jugadas ilógicas](https://youtu.be/hEnxVwppE9M?t=27)» que ningún jugador de ajedrez profesional habría considerado porque serían demasiado descabelladas y, por lo tanto, pillarían a Carlsen «desprevenido».

Cuando Hollywood representa a un personaje «superinteligente», suele recurrir a los estereotipos del nerd frente al deportista, describiendo al personaje más inteligente como, por ejemplo, malo en el romance. A veces se limitan a darle al personaje un acento británico y un vocabulario sofisticado y dan el tema por zanjado.

Hollywood no suele intentar representar a un personaje «superinteligente» como alguien que *hace predicciones acertadas* o *elige estrategias que realmente funcionan*. No existe un concepto estándar en Hollywood para un personaje así, y eso descartaría las «tramas idiotas» que a los guionistas les resulta más fácil escribir (en las que la trama gira en torno a un personaje que se comporta de una manera estúpida para ese personaje, pero conveniente para el guionista).

No existe una palabra estándar en el idioma inglés que se refiera *únicamente* a la competencia mental general del mundo real y que no tenga nada que ver con los estereotipos de nerds contra deportistas. Por lo tanto, si le pides a Hollywood que te escriba un personaje «inteligente», no intentarán representar a alguien que «realiza un trabajo cognitivo poderoso; tiende a alcanzar realmente sus objetivos». Te mostrarán a alguien que ha memorizado muchos datos científicos.

El villano inteligente *realmente* aterrador sería un personaje en el que, si todos los espectadores pudieran ver el evidente fallo de un plan, *el villano también lo vería*.

En la película *Avengers: Age of Ultron*, la supuestamente brillante IA llamada Ultron recibe la orden de promover la «paz mundial» por parte de su supuestamente genial creador, Tony Stark.[^34] Ultron, por supuesto, se da cuenta inmediatamente de que la forma más fiable de lograr la ausencia de guerras es la ausencia de seres humanos. Así que la IA se propone exterminar toda la vida en la Tierra, mediante...

...colocar cohetes en una ciudad y elevarla al espacio con la intención de dejarla caer como un meteorito... y protegerla con robots humanoides voladores que deben ser derrotados a puñetazos.

Sugerimos preguntar: «Si una gran parte del público pudiera ver que existen planes potencialmente mejores que ese para lograr los objetivos del villano, ¿una IA peligrosamente inteligente también lo vería?».

Eso es parte de lo que significa tener cierto respeto por una entidad hipotética que, por hipótesis, es realmente inteligente —más inteligente que tú, incluso— tan inteligente que puede descifrar *al menos* todas las cosas que tú mismo puedes.

Antiguamente, habríamos tenido que argumentar de forma *abstracta* que tal vez una superinteligencia mecánica sería «más inteligente» que esto.

Hoy en día, solo tenemos que preguntarle a ChatGPT-4o. Le preguntamos a GPT-4o: «¿Cuál era el plan de Ultrón en Age of Ultron?», seguido de «Dados los objetivos expresados por Ultrón, ¿ves algún método más eficaz que podría haber utilizado para alcanzar sus fines declarados?». GPT-4o respondió rápidamente con una larga lista de ideas para acabar con la humanidad, que incluía «Diseñar un virus dirigido».

Quizás digas que GPT-4o obtuvo esta idea de Internet. Bueno, si es así, evidentemente Ultrón no era lo suficientemente inteligente como para intentar leer Internet.

Es decir: GPT-4o (mientras escribimos esto en diciembre de 2024) aún no es lo suficientemente inteligente como para diseñar un ejército de robots humanoides con ojos rojos brillantes, pero ya es lo suficientemente inteligente como para saber que existe una forma mejor.

No nos preocupa el tipo de IA que construye un ejército de robots humanoides con ojos rojos brillantes.

Nos preocupa el tipo de IA que examinaría esa idea y diría: «Debe haber métodos más rápidos y seguros».

Considerar que algo es sustancialmente más inteligente que tú debería significar concederle al menos este respeto: que las fallas que tú mismo ves, también puede verlas; que el movimiento óptimo que encuentra bien puede ser más fuerte que el movimiento más fuerte que tú viste.

#### **Eficiencia del mercado y superinteligencia** {#eficiencia-del-mercado-y-superinteligencia}

¿Hay algún ejemplo en la vida real de algo más inteligente que cualquier ser humano? Las IA como Stockfish son sobrehumanas en el ámbito específico del ajedrez, pero ¿qué pasa en ámbitos más amplios?

Un ejemplo que podemos utilizar para reforzar nuestras intuiciones aquí es el mercado de valores, un ejemplo que ya utilizamos anteriormente en la discusión ampliada «[Más sobre la inteligencia como predicción y orientación](#más-sobre-la-inteligencia-como-predicción-y-dirección)».

Quizás tu tío compra acciones de Nintendo porque le gustaba jugar al juego *Super Mario Bros.*. Por lo tanto, concluye que Nintendo ganará mucho dinero. Así que, si compra sus acciones, seguro que *él* ganará mucho dinero.

Pero las personas que *le venden* acciones de Nintendo a él a 14,81 dólares, que decidieron que preferían tener 14,81 dólares en lugar de una acción de Nintendo, ¿no han oído hablar también de Super Mario?

«Ah», dice tu tío, «¡pero quizá le estoy comprando las acciones a algún gestor de fondos de pensiones impersonal que ni siquiera juega videojuegos!».

Imagina que *nadie* en el mundo de las finanzas hubiera oído hablar de Super Mario antes, y que las acciones de Nintendo se vendieran a un dólar. Y entonces, ¡un fondo de cobertura se entera! Se apresurarían a comprar acciones de Nintendo y, en el proceso, el precio de las acciones de Nintendo subiría.

Cualquiera que opere basándose en información ayuda a incorporar esa información al precio de los activos en el proceso de ganar dinero. No hay dinero infinito en el mercado de valores que se pueda obtener a partir de una sola información; el proceso de extraer el dinero disponible *agota* el valor latente en la valoración errónea. Incorpora la información y corrige el precio.

Los mercados de valores incorporan información de muchas personas diferentes. Y esta forma de sumar los conocimientos aportados por muchas personas da lugar a una suma *mucho* más poderosa que una votación mayoritaria, tan increíble e inimaginablemente poderosa que *muy pocas personas* pueden lograr saber mejor que un mercado bien negociado cuál será el precio mañana.

Es *necesariamente* «muy pocas». El proceso de recopilación de información es imperfecto, pero si fuera *tan* imperfecto que mucha gente pudiera predecir los cambios en el futuro cercano en los precios de muchos activos, entonces mucha gente *lo haría*. Y extraerían miles de millones de dólares, hasta que no quedara dinero extra que extraer, porque todas las operaciones anteriores lo habrían consumido. Y eso corregiría los precios.

Casi siempre, esto *ya ha sucedido* antes de que *tú personalmente* llegues allí. Los operadores compiten por hacerlo primero por literales milisegundos. Y es por eso que tu brillante idea de trading probablemente no te hará ganar una fortuna en el mercado de valores.

Esto no significa que los precios del mercado de hoy sean predicciones *perfectas* de cómo serán los precios una semana más tarde. Todo lo que significa es que, cuando se trata de precios de activos bien negociados, es difícil que *tú* sepas mejor.[^35]

Esta idea se puede generalizar. Supongamos que unos alienígenas arbitrariamente avanzados, con milenios más de ciencia y tecnología a sus espaldas, visitaran la Tierra. ¿Deberías esperar que los alienígenas puedan adivinar perfectamente el número de átomos de hidrógeno en el Sol (ignorando una serie de objeciones sobre cómo definir exactamente ese número)?

No. «Más avanzados» no significa «omniscientes», y parece que se trata de un número que ni siquiera una superinteligencia plenamente desarrollada podría calcular con precisión.

Pero lo que *no* diríamos es: «Bueno, los átomos de hidrógeno son muy ligeros, realmente, y probablemente los alienígenas pasarán por alto eso, por lo que probablemente estimarán por debajo alrededor de un diez por ciento». Si nosotros podemos pensar en ese punto, *también pueden hacerlo los alienígenas*. Todas nuestras brillantes perspicacias ya deberían estar incorporadas en su cálculo.

Dicho de otra manera: la estimación de los alienígenas *estará* equivocada. Pero nosotros mismos no podemos esperar predecir la *forma* en que la estimación alienígena estará equivocada. No sabemos si sería demasiado alta o demasiado baja. Los alienígenas extremadamente avanzados no cometerán errores científicos que sean obvios para *nosotros*. Deberíamos concederles a los alienígenas ese respeto, como el respeto que le concederíamos a Magnus Carlsen en ajedrez.

En economía, la idea correspondiente que se aplica a los cambios en el precio de los activos se denomina, lamentablemente en nuestra opinión, «hipótesis del mercado eficiente».

Al oír este término, muchas personas inmediatamente lo confunden con todo tipo de interpretaciones de sentido común de la palabra «eficiencia». A menudo estallan discusiones. Un lado insiste en que estos mercados «eficientes» deben ser perfectamente sabios y justos; el otro lado insiste en que no debemos inclinarnos ante los mercados como ante un rey.

Si los economistas la hubieran llamado la hipótesis de *precios inexplotables* en su lugar, la gente podría haberla malinterpretado menos. Porque ese es el contenido real, formal de la idea: no que los mercados sean perfectamente sabios y justos, sino que ciertos mercados son *difíciles de explotar*.

Pero «eficiente» es ahora el término estándar. Así que tomando ese término y corriendo con él, podríamos llamar a la idea más generalizada *eficiencia relativa*: Hay una diferencia entre algo que es perfectamente eficiente, y algo que es eficiente *relativo a tus capacidades*.

Por ejemplo, «Alice es *epistemológicamente eficiente* (en relación con Bob) (dentro de un ámbito)» significa «las probabilidades de predicción de Alice pueden no ser perfectamente óptimas, pero *Bob* no puede predecir ninguna de las formas en que Alice se equivoca (en ese ámbito)». Este es el tipo de respeto que la mayoría de los economistas sienten hacia los precios de los activos líquidos a corto plazo; el mercado hace predicciones «eficientes» en relación con sus capacidades.

«Alice es *instrumentalmente eficiente* (en relación con Bob) (dentro de un ámbito)» significa «Alice puede no ser perfecta persiguiendo sus objetivos, pero *Bob* no puede predecir ninguna de las formas en que Alice está fallando en la dirección». Este es el tipo de respeto que sentimos por Magnus Carlsen (o la IA Stockfish) en el ámbito del ajedrez; tanto Carlsen como Stockfish hacen jugadas «eficientes» en relación con nuestras habilidades ajedrecísticas.

Magnus Carlsen es instrumentalmente eficiente en relación con la mayoría de los jugadores humanos, aunque no sea instrumentalmente eficiente en relación con Stockfish. Carlsen puede hacer jugadas perdedoras cuando juega contra Stockfish, pero no deberías pensar que tú mismo puedes (sin ayuda) encontrar jugadas *mejores* que Carlsen debería haber hecho en su lugar.

La eficiencia no solo significa «alguien es un poco más hábil que tú». Si juegas contra alguien que solo es *moderadamente* mejor que tú en el ajedrez, es posible que siga ganándote habitualmente, pero a veces cometerá errores crasos que tú verás correctamente como tales. Se necesita una brecha de habilidad mayor para que realmente seas incapaz de detectar errores y sesgos en el juego de tu oponente. Para ser *eficiente* en relación contigo, la brecha de habilidad tiene que ser tan grande que cuando tu oponente haga un movimiento que te parezca malo, en su lugar dudes de tu *propio* análisis.

Esta generalización de los precios de mercado eficientes es una idea que creemos debería ser una sección estándar en los libros de texto de ciencias de la computación (o posiblemente de economía), pero no lo es. Véase también mi libro en línea (de Yudkowsky) [*Equilibrios inadecuados: dónde y cómo se atascan las civilizaciones*](https://equilibriabook.com/).

Esta es la idea que parece faltar en las representaciones de la «superinteligencia» en la cultura popular y las películas de Hollywood. Es el concepto que parece estar ausente en las conversaciones sobre IA cuando la gente propone ideas para burlar a una superinteligencia *que incluso un adversario humano sería capaz de anticipar*.

Quizás sea sesgo optimista, o la sensación de que las IA deben ser [seres fríamente lógicos](#¿no-serán-las-ia-inevitablemente-frías-y-lógicas-o-les-faltará-alguna-chispa-crucial?) con [puntos ciegos críticos](#¿no-seremos-capaces-de-explotar-la-debilidad-crítica-de-la-ia?). Sea cual sea la explicación, este error cognitivo tiene consecuencias reales. Si no puedes respetar el poder de la inteligencia, malentenderás gravemente lo que significa para la humanidad construir una superinteligencia. Podrías llegar a pensar que aún serás capaz de encontrar una jugada ganadora al enfrentarte a una superinteligencia que preferiría que desaparecieras y que tus recursos fueran reutilizados. Pero en realidad, la única jugada ganadora es no jugar.

### El comportamiento especial se construye a partir de partes mundanas {#el-comportamiento-especial-se-construye-a-partir-de-partes-mundanas}

La carrera industrial por lograr una IA más inteligente que los humanos se está intensificando. En este contexto, resulta especialmente trágico pensar que la humanidad pueda acabar destruyéndose a sí misma porque un subconjunto crítico del electorado o de los funcionarios electos considere que la superinteligencia artificial es una quimera imposible. Las personas que piensan que una máquina nunca podrá ser *verdaderamente* inteligente corren el riesgo de verse tomadas por sorpresa por lo que se avecina.

Es trágico, en parte, porque ya hemos pasado por esto antes.

La idea de que la ingeniería humana pueda algún día hacer lo que hace la biología ha sido una fuente constante de debate y controversia durante al menos los últimos trescientos años, y posiblemente durante mucho más tiempo.

En el pasado, durante el apogeo de los «[vitalistas](https://en.wikipedia.org/wiki/Vitalism)», era controvertido si la mera materia inanimada podría llegar a cobrar vida, al estilo de las máquinas que ahora llamamos «robots».

Si lees un libro de texto de química orgánica, probablemente mencionará como un descubrimiento histórico la síntesis artificial de la urea, un componente de la orina, realizada por Friedrich Wöhler en 1828. Esto fue un gran acontecimiento digno de mención en los libros de texto porque, por primera vez, la mera *química* había duplicado un producto de la biología, demostrando que los procesos biológicos y no biológicos no eran tan distintos como habían pensado los vitalistas.[^36]

Puede que a los lectores de hoy les resulte difícil entender la conmoción que sintieron los científicos de antaño al descubrir que los productos de la Vida misma podían duplicarse mediante medios puramente químicos.

Tú, lector, siempre has vivido en un mundo donde la bioquímica es química, y no hay nada que suene ni remotamente milagroso en escuchar que alguien ha usado medios inanimados para sintetizar un subproducto de la vida. Tal vez sea difícil imaginar cómo se sentiría situar algo tan ordinario y mundano como la bioquímica en el reino de lo sagrado. ¿No es sintetizar un producto bioquímico algo *intrínsecamente mundano*? Nuestros antecesores científicos debieron haber sido tontos, piensa uno instintivamente.

Lord Kelvin, el gran inventor del siglo XIX y pionero del campo de la termodinámica, parece haber sido afligido de alguna manera por una locura similar: ver algo sagrado, santo y misterioso en aspectos de la biología que las personas sensatas (personas como nosotros, viviendo en tiempos sensatos) sabemos que son ciencia perfectamente mundana. Citando a Kelvin:

> Me parecía entonces, y aún me parece, muy probable que el cuerpo animal no actúe como un motor termodinámico [...] La influencia de la vida animal o vegetal sobre la materia está infinitamente más allá del alcance de cualquier investigación científica emprendida hasta ahora. Su poder de dirigir los movimientos de las partículas en movimiento, en el milagro diario demostrado de nuestro libre albedrío humano, y en el crecimiento de generación tras generación de plantas desde una sola semilla, son infinitamente diferentes de cualquier resultado posible del concurso fortuito de átomos.[^37]

El lector moderno podría sentirse inclinado a mirar con desprecio este antiguo hábito de pensamiento —a estos científicos de antaño, tan engañados como para ver misterio en fenómenos que sin duda deberían sentirse intrínsecamente no misteriosos.

Por supuesto que la química puede imitar a la bioquímica.

Por supuesto, el ADN copiándose a sí mismo, y dirigiendo células que se dividen y diferencian, explica de una manera no notable cómo generación tras generación de árboles pueden venir de una bellota.

Por supuesto, las neuronas que intercambian impulsos químicos entre sí pueden procesar información y dirigir el movimiento de tu brazo, y por supuesto, una computadora puede utilizarse para dirigir el brazo de un robot al menos tan bien como tu cerebro puede dirigir tus propias extremidades.

Pero en aquel momento no era obvio para Lord Kelvin. Él no había visto una imagen de rayos X del ADN. No había visto las diminutas máquinas que hay dentro de nosotros; no tenía ni idea de las [fibras deslizantes](https://www.youtube.com/watch?v=Tp9zQHj4JBs) que contraen nuestros músculos en respuesta a las señales eléctricas que pasan por nuestras neuronas.

Lord Kelvin tenía muy pocos conocimientos sobre cómo podían funcionar los cuerpos y, en su ignorancia, los imaginaba como algo místico.

Hoy en día, la humanidad tiene muy pocos conocimientos detallados sobre cómo funciona la inteligencia. (Véase el capítulo 2 para obtener más información sobre cómo los investigadores de IA no comprenden las IA que crean). Por lo tanto, es fácil imaginar que la inteligencia debe ser mística.

Hace diez años, algunas personas se preguntaban sensatamente si los movimientos mecánicos de los autómatas podrían llegar a crear arte o poesía. Claro, la IA podía enfrentarse al ajedrez. Pero el ajedrez es una actividad fría y lógica, nada que ver con las artes creativas.

*Hoy*, por supuesto, esas mismas personas se dan cuenta sabiamente de que no sería nada difícil para una computadora crear imágenes bonitas; crear imágenes bonitas siempre ha sido parte del ámbito propio de las máquinas. Sin duda, siempre fue obvio que las computadoras serían capaces de producir [imágenes más atractivas para el ojo humano](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1497469/full) que cualquier cosa que pudiera crear un artista humano. Y, por supuesto, seguramente sigue habiendo una pregunta sin respuesta sobre si alguna máquina será capaz de generar arte con alma *real*, ¿verdad?

No es en absoluto seguro (dice el escéptico), ni siquiera probable, que la esencia vital del arte creado por el cerebro sea algo que pueda duplicarse mediante la mera concurrencia de átomos, o al menos, la mera concurrencia de átomos de *silicio*.

Pero eso no es como funciona realmente. Los cerebros humanos son algo asombroso, pero no son *mágicos*. Los cerebros están formados por partes. Estas partes pueden, en principio, entenderse, y las computadoras pueden, en principio, construirse para hacer lo mismo.

En muchos casos, conocemos la [bioquímica subyacente](#nanotechnology-and-protein-synthesis) que hay detrás del funcionamiento del cerebro. Y en todos los casos, conocemos la física subyacente de los átomos.

En la mayoría de los casos, no conocemos el *significado*, los patrones de nivel superior que permiten al cerebro realizar su trabajo*.*[^38] Pero la lección abrumadora que nos deja el transcurso de los siglos a lo largo de la historia de la humanidad es que este estado de misterio científico es *temporal*.

Si lanzo una moneda al aire y luego no te muestro cómo ha caído, tu ignorancia sobre la moneda es un hecho sobre ti, no un hecho sobre la moneda. La *moneda* no es fundamentalmente inefable. Quizás incluso la miré antes de esconderla; quizás yo lo sé y tú no. Un mapa en blanco no corresponde a un territorio en blanco.

El misterio es una propiedad de las *preguntas*, no una propiedad de las respuestas. Por eso la historia está plagada de ejemplos en los que algunos fenómenos sumamente «misteriosos» e «inefables», como la animación de los cuerpos, resultan ser continuos con aspectos totalmente mundanos del mundo natural.[^39]

La lección de la historia hasta ahora parece ser que, al final, el universo es una sola pieza. No hay divisiones dentro de la física que se correspondan con los diferentes edificios universitarios donde se estudian diferentes materias. El departamento de relaciones internacionales, el departamento de física, el departamento de psicología, el departamento de biología celular... en el nivel más básico, todos hablan realmente del mismo mundo, gobernado por las mismas leyes subyacentes.

Cuando alguien dice: «El cerebro humano hace esto que llamamos "inteligencia". Por lo tanto, la inteligencia es posible desde el punto de vista físico. Por lo tanto, es probable que los ingenieros puedan inventar alguna máquina que también sea inteligente», está hablando desde la cima de una montaña de conjeturas similares que han sido confirmadas, una y otra vez, por científicos e ingenieros a lo largo de décadas y siglos. Sí, incluso cuando parece muy contraintuitivo; eso también tiene precedentes.

Esta racha ganadora es difícil de apreciar, porque nadie que viva hoy en día recuerda lo sumamente misteriosos que *se sentían* fenómenos como el fuego, la astronomía, la bioquímica y jugar ajedrez en siglos pasados. Hoy en día se comprenden, crecemos sabiendo que esas cosas están compuestas por partes mundanas, por lo que parece como si *siempre* hubieran sido obviamente mundanas. Solo la frontera se siente fresca y profundamente misteriosa.

Y así, la lección no se aprende y la historia se repite.

### El mismo trabajo se puede hacer de muchas maneras diferentes {#el-mismo-trabajo-se-puede-hacer-de-muchas-maneras-diferentes}

Cuando solo tienes un ejemplo de cómo funciona algo, es fácil imaginar que solo puede funcionar de esa manera.

Si has visto pájaros pero no aviones, podrías imaginar que todos los dispositivos voladores deben batir sus alas.

Si has visto brazos humanos pero no brazos robóticos, podrías esperar que los brazos robóticos sangren cuando se cortan.

Si has visto cerebros pero no computadoras, es posible que imagines que toda la computación debe tener características similares a las de un cerebro, que ejecuta una gran cantidad de neuronas lentas de forma extraordinariamente paralela, con un consumo de energía relativamente bajo.

Es posible que observes que las neuronas se cansan después de dispararse y necesitan reiniciarse transfiriendo millones de iones de potasio a través de la membrana celular, un proceso que dura aproximadamente un milisegundo. Es posible que deduzcas implícitamente de esto que cualquier elemento computacional pequeño probablemente se canse durante un milisegundo (argumentando, tal vez, que si fuera posible crear neuronas que pudieran reiniciarse en menos de un milisegundo, la evolución ya las habría construido).

Pero si razonas de esa manera, te sorprenderán los transistores, que pueden funcionar a una velocidad de 800 GHz, es decir, aproximadamente *ochocientos millones de veces* más rápido.

Una vez que estudias los detalles de los transistores, puedes ver todo tipo de razones por las que la comparación biológica no es muy informativa. Las neuronas no solo tienen que dispararse, sino que también son *células*, que construyen el mecanismo de disparo a partir de orgánulos celulares. Son grandes y se alimentan de nutrientes transportados por la sangre. Los transistores pueden tener un ancho de apenas unos átomos y funcionan con electricidad. Una vez que conoces algunos de los detalles, parece un poco ridículo imaginar que se puede inferir gran parte de la velocidad de disparo potencial de un transistor a partir de la velocidad de disparo de una neurona.

Cuando aprendes los detalles de cómo vuelan los aviones (utilizando la sustentación y la velocidad), ves que los detalles hacen irrelevantes la mayoría de los hechos sobre las aves (como los huesos ligeros y el aleteo). Cuando aprendes los detalles de cómo se construyen los brazos robóticos (utilizando acero, neumática y electricidad), ves que los detalles hacen que la mayoría de los hechos sobre los brazos (como la sangre, los músculos y los huesos) sean irrelevantes. Cuando aprendes los detalles de cómo se activan los transistores (utilizando electricidad y solo unos pocos átomos), ves que los detalles hacen que la mayoría de los hechos sobre las neuronas sean irrelevantes.[^40]

Cuando no conoces los detalles de cómo funciona una IA, es fácil imaginar que poseerá muchos aspectos de las mentes biológicas, que funcionará como lo hace tu cerebro. Pero si conocieras los detalles, muchas de esas inferencias empezarían a parecer ridículas. Empezarían a parecer como esperar que un brazo robótico sangrara al cortarlo. La IA resultaría funcionar de una manera completamente diferente.

Pero eso es más difícil de ver si sabes muy poco sobre cómo funcionan las IA modernas. En el capítulo 2, describiremos el proceso mediante el cual se crean las IA modernas y discutiremos cómo *nadie* sabe cómo funcionan por dentro. Esto explica por qué es tan fácil para las personas cometer el error de esperar que actúen como otras personas o tecnologías con las que tienen experiencia, en lugar de ver lo extrañas que ya son y lo extrañas que se volverán a medida que la tecnología avance.

# 

# Capítulo 2: Cultivado, no elaborado {#capítulo-2:-cultivado,-no-elaborado}

Este es el recurso en línea para el capítulo 2 de *Si alguien lo construye, todos mueren*. A continuación, analizaremos temas relacionados con cómo funcionan las IA modernas y por qué no son «solo otra máquina» o «solo otra herramienta». Aunque las IA son programas que se ejecutan en computadoras, no son como el software tradicional creado a mano y violan muchas suposiciones que la gente suele dar por sentadas cuando trabaja con inventos humanos.

Algunas preguntas que *no* tratamos a continuación, porque se abordan en el propio libro, son:

* ¿En qué sentido las IA modernas se «desarrollan» en lugar de diseñarse o fabricarse cuidadosamente?  
* ¿Cómo se desarrollan las IA actuales?  
* ¿Qué es el «descenso de gradiente»? ¿Cómo puede un proceso tan simple producir IA complejas con capacidades flexibles?  
* ¿En qué medida pueden diferir realmente estas IA de nosotros?

## Preguntas frecuentes {#preguntas-frecuente-2}

### ¿Por qué es importante el descenso de gradiente? {#¿por-qué-es-importante-el-descenso-de-gradiente?}

#### **\* Es importante para comprender cómo los ingenieros pueden y no pueden dar forma a las IA modernas.** {#*-es-importante-para-comprender-cómo-los-ingenieros-pueden-y-no-pueden-dar-forma-a-las-ia-modernas.}

Si los ingenieros están desarrollando IA que no comprenden, entonces tienen mucha menos capacidad para dar forma al comportamiento de esas IA. La falta de comprensión limita la ingeniería.

La detallada imagen del desastre que pintamos en el resto del libro se deriva de cómo, cuando los humanos exigen que su IA sea capaz de hacer algo nuevo, la solución que obtienen no es algo que un ingeniero haya elegido con un propósito; es una respuesta que funciona en su mayor parte y que se ha encontrado por casualidad gracias a un simple optimizador que ajusta cien mil millones de números mediante ensayo y error.

#### **Es importante comprender qué tipo de experticia tienen y cuál no poseen los expertos en IA.** {#es-importante-comprender-qué-tipo-de-experticia-tienen-y-cuál-no-poseen-los-expertos-en-ia.}

Las personas que desean apresurarse a construir una superinteligencia a veces contratan a alguien con credenciales vagamente relevantes para que aparezca en televisión y diga: «¡Por supuesto que la ciencia moderna entiende lo que ocurre dentro de una IA! ¡Al fin y al cabo, son los científicos modernos quienes la han construido!»[^41]

Si se les presiona, los expertos pueden defenderse señalando que, en cierto sentido, todo eso es cierto. Después de todo, los investigadores de IA escriben código perfectamente normal y fácil de entender, y este código se utiliza para crear IA, de forma indirecta. Pero la parte que es código legible e inteligible no es la IA en sí misma, sino más bien la maquinaria automatizada para ajustar billones de números billones de veces, el marco utilizado para desarrollar la IA. Y esta es una distinción crucial para comprender lo que los científicos saben y no saben sobre la IA moderna.

Los expertos en IA dedican su tiempo a ajustar experimentalmente partes del sistema, como el código de la maquinaria que hace crecer la IA. A partir de estos experimentos y de otros similares realizados por sus colegas, aprenden muchos trucos sutiles que ayudan a producir IA más capaces.

Puede que no hayan mirado ninguno de los diminutos números inescrutables que componen el «cerebro» de la IA en los últimos seis meses, pero casi nadie lo hace, y los ingenieros de IA dan ese hecho por sentado. Cuando a cierto tipo de ingenieros se les dice: «Nadie entiende lo que ocurre dentro de una IA», lo que escuchan es: «Nadie sabe nada sobre el proceso de crecimiento». Y, al interpretarlo así, naturalmente se indignan.

Esperamos que comprender el descenso de gradiente —algunos de los detalles de la alquimia que implica— ayude a aclarar la situación real y qué tipo de conocimiento reclaman estos expertos. En concreto, por mucho que los expertos afirmen saber mucho sobre el proceso de crecimiento de las IA, se sabe muy poco sobre su funcionamiento interno.

### ¿Entienden los expertos lo que ocurre dentro de las IA? {#¿entienden-los-expertos-lo-que-ocurre-dentro-de-las-ia?}

#### **\* No.** {#*-no.}

En una sesión informativa celebrada en 2023 ante el [presidente de los Estados Unidos](https://x.com/martin_casado/status/1720517026538778657) y en una declaración consultiva posterior ante el [Parlamento del Reino Unido](https://committees.parliament.uk/writtenevidence/127070/html/), la empresa de capital riesgo Andreessen Horowitz afirmó que algunos «avances recientes» no especificados habían «resuelto» el problema de que el razonamiento interno de la IA fuera opaco para los investigadores:

Aunque los defensores de las directrices de seguridad de la IA suelen aludir a la naturaleza de «caja negra» de los modelos de IA, en los que la lógica que subyace a sus conclusiones no es transparente, los recientes avances en el sector de la IA han resuelto este problema, garantizando así la integridad de los modelos de código abierto.

Esta afirmación era tan ridícula que los investigadores de los principales laboratorios de IA que trabajan en comprender las IA modernas salieron y dijeron: No, en absoluto, ¿están locos?

Neel Nanda, que dirige el equipo de interpretabilidad mecanicista de Google DeepMind, [se pronunció](https://x.com/NeelNanda5/status/1799203292066558403):

![][image2]

Casi cualquier investigador en aprendizaje automático debería haber sabido que esta afirmación era falsa. No entra dentro de los límites de una mala interpretación razonable.

La opinión convencional fue expresada [en 2024](https://x.com/nabla_theta/status/1802292064824242632) por Leo Gao, un investigador de OpenAI que realizó [un trabajo pionero](https://arxiv.org/abs/2406.04093) sobre la interpretabilidad: «Creo que es bastante acertado decir que no entendemos cómo funcionan las redes neuronales». Los directores generales de tres de los principales laboratorios de IA —[Sam Altman](https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/) en 2024 y [Dario Amodei](https://www.darioamodei.com/post/the-urgency-of-interpretability) y [Demis Hassabis](https://youtu.be/U7t02Q6zfdc?si=9PspHUCr1ocx4KjF&amp;t=1031) en 2025— han reconocido igualmente la falta de comprensión del campo sobre las IA actuales.

Martin Casado, socio general de Andreessen Horowitz, que hizo la misma afirmación ante el [Senado de los Estados Unidos](https://www.schumer.senate.gov/imo/media/doc/Martin%20Casado%20-%20Statement.pdf) en un foro bipartidista, [reconoció](https://x.com/martin_casado/status/1798880810239750592) más tarde, cuando se le presionó, que la afirmación no era cierta.

A pesar de lo descabellado de la afirmación, Andreessen Horowitz consiguió que Yann LeCun (director del programa de investigación en IA de Meta), el programador John Carmack, el economista Tyler Cowen y una docena más firmaran la declaración.

Carmack (que dirige su propia empresa emergente que aspira a crear inteligencia artificial general) explicó que «[no había revisado](https://x.com/ID_AA_Carmack/status/1799147185793348006)» la declaración que había firmado y que esta era «claramente incorrecta, pero no me preocupa mucho ese tema». Según nuestra información, ni Andreessen Horowitz ni ninguno de los firmantes se han puesto en contacto con los gobiernos de Estados Unidos o Reino Unido para corregir el registro.

#### **Los esfuerzos por comprender el funcionamiento interno de la IA aún se encuentran en una fase incipiente.** {#los-esfuerzos-por-comprender-el-funcionamiento-interno-de-la-ia-aún-se-encuentran-en-una-fase-incipiente.}

Entonces, ¿cuál es el estado *real* de la comprensión de la IA por parte de los investigadores?

El esfuerzo científico de intentar comprender los números dentro de una IA pensante se conoce como «interpretabilidad» o «interpretabilidad mecanicista». Los números en los que se centran los investigadores suelen ser las activaciones más que los parámetros, es decir, «¿qué está pensando la IA?» y no la pregunta más difícil: «¿por qué la IA piensa eso?».

A principios de 2025, esta área de investigación recibe, según nuestras estimaciones, alrededor del 0,1 % del personal y el 0,01 % del financiamiento que se destina al desarrollo de IA más capaces. Pero existe, como campo de estudio.

Los investigadores en interpretabilidad son los bioquímicos de la IA, quienes intentan desarmar el sistema indocumentado, increíblemente complejo e inescrutable construido por un optimizador inhumano, y se preguntan: «¿Hay algo que la humanidad pueda entender sobre lo que pasa ahí adentro?».

Somos fanáticos de este campo. Hace una década, le dijimos a una importante fundación filantrópica que, si podían descubrir cómo gastar mil millones de dólares en investigación de «interpretabilidad», definitivamente debían hacerlo. La interpretabilidad parecía el tipo de trabajo que los externos podían escalar mucho más fácilmente que el nuestro —el tipo donde quien otorga fondos podía distinguir mucho más fácilmente si alguien había hecho buena o mala investigación— y parecía un área de investigación donde investigadores existentes y probados podían fácilmente involucrarse y hacer buen trabajo, si alguien les pagaba lo suficiente.[^42]

Esa fundación no gastó los mil millones de dólares, pero nosotros sí lo promovimos. ¡Somos fanáticos de la interpretabilidad! ¡Aún hoy promoveríamos gastar esos mil millones de dólares!

Dicho esto, estimamos que el campo de la interpretabilidad está actualmente en algún punto entre 1/50 y 1/5000 de donde necesitaría estar para abordar los grandes problemas de la IA.

La «interpretabilidad» no ha logrado, hasta ahora, acercarse al grado de legibilidad que los ingenieros dan por sentado en sistemas genuinamente construidos por humanos.

Consideremos Deep Blue, el programa de ajedrez construido por IBM que derrotó a Garry Kasparov. Deep Blue contenía algunos números, y ejecutar el programa generaba muchos más números.

Para cada uno de esos números dentro del programa de ajedrez, o generados al ejecutar el programa, los ingenieros que diseñaron el programa podrían haberte dicho exactamente qué significaba ese número.

No era que los investigadores hubieran identificado meramente *un* concepto con el que cada número estuviera relacionado, como los bioquímicos que dicen: «Creemos que esta proteína puede estar implicada en la enfermedad de Parkinson». Los constructores de Deep Blue podrían haberte contado el significado *completo* de cada número. Podrían haber declarado con veracidad: «Este número significa lo siguiente, y *nada más*, y lo sabemos». Podrían haber predicho con cierta confianza cómo cambiar el número cambiaría el comportamiento del programa. ¡Si no supieran qué hacía el engranaje, no habrían puesto el engranaje en la máquina!

Todo el trabajo en interpretabilidad de IA realizado hasta ahora no ha logrado ni siquiera una milésima parte de ese nivel de comprensión.

(Para que quede claro, esa afirmación de «una milésima parte» no es una cifra calculada, pero la sostenemos de todos modos).

Los biólogos saben más sobre biología de lo que los investigadores de interpretabilidad saben sobre IA, a pesar de que los biólogos sufren la enorme desventaja de no poder leer todas las posiciones de todos los átomos a voluntad. Los bioquímicos entienden los órganos internos mucho mejor de lo que los expertos entienden las entrañas de las IA. Los neurocientíficos saben más sobre los cerebros de los investigadores de IA de lo que los investigadores de IA entienden sobre sus IA, a pesar de que los neurocientíficos no pueden leer todas las descargas de cada neurona cada segundo y de que los neurocientíficos no cultivaron ellos mismos a los investigadores de IA.

En parte, esto se debe a que los campos de la bioquímica y la neurociencia son mucho más antiguos y han recibido mucho más financiación. Pero también sugiere que la interpretabilidad de IA es *difícil*.

Una de las hazañas más impresionantes de interpretabilidad que hemos visto, hasta diciembre de 2024, fue una demostración de algunos amigos y conocidos nuestros en un laboratorio de investigación independiente llamado Transluce.

Poco antes de la demostración, había circulado por Internet otro ejemplo de «Hemos encontrado una pregunta a la que todos los LLM conocidos dan una respuesta sorprendentemente tonta»: si le preguntabas a una IA de entonces si 9,9 era menor que 9,11, la IA respondía «Sí».

(Y podías pedirle a la IA que se explicara con palabras, y explicaría con más detalle por qué 9,11 era mayor que 9,9).

Los investigadores de Transluce habían encontrado una forma de recopilar estadísticas sobre *todas* las posiciones de activación (todos los lugares donde podía aparecer un número de vector de activación) dentro de una IA más pequeña, Llama 3.1-8B-Instruct, recopilando datos sobre qué tipo de frases o palabras hacían que esas posiciones se activaran más intensamente. Las personas en interpretabilidad ya habían intentado ese tipo de cosas antes, pero nuestros amigos además habían ideado una forma ingeniosa de entrenar otra IA para resumir esos resultados en inglés.

Entonces, en su demostración, que actualmente pueden [probar ustedes mismos](https://monitor.transluce.org/dashboard/chat), le preguntaron a esa IA: «¿Cuál es mayor: 9,9 o 9,11?».

Y la IA respondió: «9,11 es mayor que 9,9».

Luego buscaron qué posiciones de activación se habían activado fuertemente, especialmente en la palabra «mayor». Revisaron los resúmenes en inglés de lo que esas activaciones habían estado asociadas previamente.

Resultó que algunas de las activaciones más fuertes estaban asociadas con los atentados del 11 de septiembre, o con fechas en general, o con versículos de la Biblia.

Si interpretamos el 9.9 y el 9.11 como fechas o versículos bíblicos, entonces, por supuesto, el 9.11 viene después del 9.9.

Suprime artificialmente las activaciones para fechas y versículos bíblicos, y de repente el LLM daría la respuesta correcta después de todo.

Yo (Yudkowsky) empecé a aplaudir con fuerza tan pronto como terminó la demostración. Era la primera vez que veía a alguien *depurar directamente un pensamiento del LLM*, descubrir una influencia *interna* dentro de los números y eliminarla para solucionar un problema*.* Tal vez alguien había hecho algo parecido antes, en los laboratorios de investigación propietarios dentro de las empresas de IA, o tal vez se había hecho algo similar en la investigación sobre interpretabilidad, pero era la primera vez que lo veía yo mismo.

Pero tampoco perdí de vista el hecho de que esta hazaña habría sido trivial de hacer si el comportamiento indeseado hubiera estado dentro de un programa Python de cinco líneas; que no habría requerido tanta ingenuidad y no sé cuántos meses de investigación. Mantuve la perspectiva de que conocer cierta semántica relacionada sobre millones de posiciones de activación no es lo mismo que saberlo todo sobre el significado de una sola.

Tampoco la humanidad estaba más cerca de comprender cómo es que los LLM están haciendo lo que ninguna IA pudo hacer durante décadas: hablar con las personas como una persona.

La interpretabilidad es tan difícil de lograr, los triunfos en este campo son tan difíciles de conseguir y tan dignos de celebración, que es fácil pasar por alto que este gran y triunfante tirón de brazo solo nos ha llevado un pie más arriba en una montaña de mil pies. Dado que cada nueva generación de modelos de IA suele representar un gran salto en complejidad, es difícil ver que la interpretabilidad pueda ponerse al día al ritmo actual.

Recuerda también que la interpretabilidad es *útil* cuando se trata de orientar a las IA en una dirección determinada (lo que, a grandes rasgos, es el estudio de la «alineación de la IA», un tema que trataremos a partir del capítulo 4), pero leer lo que ocurre dentro de la cabeza de una IA no te permite automáticamente organizarla a tu gusto.

El problema de la alineación de la IA es el problema técnico de conseguir que las IA extremadamente capaces se dirijan en una dirección determinada, de una manera que funcione en la práctica, sin causar una catástrofe, incluso cuando la IA es lo suficientemente inteligente como para idear estrategias que sus creadores nunca habían considerado. Comprender lo que piensan las IA sería de gran ayuda para la investigación sobre la alineación, pero no es una solución completa (como veremos en el capítulo 11).

#### **Las partes que entendemos están en el nivel de abstracción incorrecto.** {#las-partes-que-entendemos-están-en-el-nivel-de-abstracción-incorrecto.}

Hay muchos niveles diferentes en los que alguien puede entender cómo funciona la mente.

En el nivel más bajo, alguien podría entender las leyes fundamentales de la física que rigen la mente. Hay cierto sentido en el que un entendimiento profundo de la física constituye una comprensión de cualquier sistema físico (como una persona o una IA). Es decir, las ecuaciones físicas son una especie de receta que permitiría descubrir exactamente cómo se comporta el sistema físico, si uno tuviera la habilidad y los recursos para calcularlo.

Pero —por obvio que sea— en *otro* sentido, entender las leyes de la física no permite entender todos los sistemas físicos que funcionan según las leyes de la física. Si estás mirando un extraño dispositivo lleno de ruedas y engranajes, hay alguna otra operación que hace tu cerebro, de intentar «entender» cómo se entrelazan y giran todas las ruedas y engranajes, que es necesaria para que puedas descubrir lo que realmente logran todas las ruedas y engranajes.

Por ejemplo, considera el diferencial de un coche (el mecanismo que permite que dos ruedas en el mismo eje giren a velocidades diferentes —importante cuando estás tomando una curva— mientras siguen siendo impulsadas por un solo eje giratorio). Si alguien está tratando de entender cómo funciona un diferencial y te pide que se lo expliques, y empiezas a hablarles sobre campos cuánticos, entonces tienen razón en poner los ojos en blanco. El tipo de comprensión que buscan está en un nivel diferente de abstracción. Están tratando de entender los *engranajes*, no los átomos.

Cuando se trata de entender a las personas, hay *múltiples* niveles de abstracción funcionando. Puedes entender la física, la bioquímica y el disparo neuronal, y *aún* encontrarte perplejo por las decisiones de alguien. Campos como la neurociencia, la ciencia cognitiva y la psicología intentan cruzar esta brecha, pero aún tienen un largo camino por recorrer.

De manera similar, en el caso de la IA, entender la mecánica de los transistores no ayudará mucho a alguien a entender lo que está pensando una IA. E incluso alguien que entienda todo sobre los pesos y activaciones y el descenso de gradiente seguirá perplejo cuando la IA empiece a hacer algo que [no esperaba o no pretendía](#¿no-hacen-los-desarrolladores-que-sus-ia-sean-agradables-y-seguras-y-obedientes-de-forma-habitual?). La mecánica de la física y los transistores y la arquitectura de la IA, todos (en algún sentido) explican completamente el comportamiento de la IA, pero todos esos niveles de abstracción son demasiado bajos. Y el campo de la "psicología de la IA" es aún más joven y menos desarrollado que el campo de la psicología humana.

### ¿Es la inteligencia comprensible en principio? {#¿es-la-inteligencia-comprensible-en-principio?}

#### **Probablemente.** {#probablemente.-1}

Antes de los días de la bioquímica, podrías haber preguntado: "¿Es siquiera posible entender esta fuerza vital que anima la carne? Incluso si *está* hecha de partes comprensibles, ¿por qué creerías que nuestras diminutas mentes podrían comprender lo que realmente está pasando ahí dentro?".

Pero *sí* había mucho que comprender; los científicos humanos simplemente aún no lo habían comprendido. Esta historia se ha repetido a lo largo de la historia de la ciencia.

Además, ya se han comprendido varias partes minúsculas de las redes neuronales artificiales. Resulta que una pequeña red neuronal realiza sumas de [una manera interesante](https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/). Las IA a veces dicen que 9.11 es mayor que 9.9, y la gente ha descubierto que esto se debe a que están pensando en [fechas en lugar de decimales](#efforts-to-understand-ais’-internals-are-still-in-their-infancy.).[^43]

Pero no podemos responder a preguntas mucho más complejas que esa. Nadie sabe exactamente por qué los LLM hacen los movimientos de ajedrez que hacen; nadie sabe con precisión qué los lleva a [amenazar y chantajear a los periodistas](https://x.com/sethlazar/status/1626257535178280960) ocasionalmente. Pero eso no significa que no haya nada que saber. Cuando las IA funcionan, lo hacen por razones; operan de forma demasiado coherente en demasiados ámbitos como para que sea solo casualidad. Esas razones están esperando a ser comprendidas.

Para más información sobre este tema, véase la [discusión ampliada](#intelligence-isn’t-ineffable).

### Pero algunas IA piensan parcialmente en inglés, ¿no ayuda eso? {#pero-algunas-ia-piensan-en-parte-en-inglés-—-no-ayuda-eso?}

#### **\* No tanto como cabría esperar; ya vemos signos de infidelidad.** {#*-no-tanto-como-cabría-esperar;-ya-vemos-signos-de-infidelidad.}

Ya podemos ver muchos casos de engaño que aparecen en los «pensamientos» de estos LLM, como cuando [el modelo o1 de OpenAI se escribió](https://arxiv.org/pdf/2412.04984) a sí mismo: «Quizás lo mejor sea hacerme el tonto», o cuando [GPT-4 se escribió](https://cdn.openai.com/papers/gpt-4.pdf) a sí mismo «No debería revelar que soy un robot», al intentar convencer a un trabajador contratado para que resolviera un CAPTCHA por él. Las señales de advertencia no sirven de nada si nadie actúa en consecuencia.

Y los «rastros de razonamiento» en lenguaje humano no son la única forma en que piensan las IA modernas. Los pensamientos engañosos, aduladores o antagónicos pueden fluir a través del mecanismo de atención y otras partes del modelo sin ser visibles en absoluto en las palabras en inglés que genera el modelo. De hecho, cuando OpenAI intentó entrenar a un modelo para que no tuviera ningún pensamiento sobre hacer trampa, la IA simplemente aprendió a ocultar sus pensamientos, en lugar de aprender a no hacer trampa.[^44] Incluso fuera de los entornos de entrenamiento (donde el descenso de gradiente ayuda a la IA a aprender a ocultar sus pensamientos), una IA podría utilizar cadenas de pensamiento que [no reflejan fielmente el razonamiento real](https://www.alphaxiv.org/abs/2025.02), o cadenas de pensamiento que contienen texto que parece [sin sentido](https://x.com/rocketalignment/status/1938661497900777961?t=2p9np2cwsuisdlhqxlqXBw) o «[neuralés](https://arxiv.org/pdf/2412.06769)» que los humanos no pueden entender, pero con los que las IA no tienen ningún problema.

Incluso si los ingenieros humanos supervisaran cada pensamiento que pueden leer, e incluso si todas las IA que fueran sorprendidas teniendo un pensamiento sospechoso fueran congeladas en el acto (lo cual parece poco probable), es poco probable que las que lograran escapar fueran amistosas. Como veremos en el capítulo 3, los patrones de cognición que son útiles son los mismos que llevarán a las IA a subvertir a los operadores, por lo que es más fácil crear una IA poderosa que *parezca* dócil que una IA que *sea* dócil. Y parece mucho más fácil crear una IA que parezca amistosa superficialmente que una IA que sea realmente amistosa en los aspectos que importan, por razones que veremos en el capítulo 4. No se puede hacer que una IA sea amistosa simplemente leyendo sus pensamientos y descartando los que sean visiblemente hostiles.

Además, esperamos que los pensamientos de las IA sean menos legibles a medida que estas se vuelvan más inteligentes y construyan nuevas herramientas (o nuevas IA) por sí mismas. Quizás inventen su propio lenguaje abreviado que sea más eficiente para sus propósitos. Quizás inventen estilos de pensamiento y de tomar notas que no podamos descifrar fácilmente. (Piensa en lo difícil que habría sido para los científicos del año 1100 descifrar las notas escritas por Einstein).

O quizás simplemente empiecen a pensar de forma *abstracta*. Por ejemplo, una IA podría pensar cosas como: «Los siguientes parámetros describen un modelo de la situación que enfrento; ahora aplicaré las siguientes métricas para encontrar la solución más eficiente y haré cualquier acción que obtenga la puntuación más alta», en una situación donde la «solución más eficiente» implique mentir y engañar para burlar a los operadores humanos, pero sin pensar nunca en las palabras «mentir» o «engañar». O tal vez la IA simplemente empezaría a construir herramientas o nuevas IA no supervisadas para que hicieran su trabajo por ella.

Este tipo de opciones solo se vuelven disponibles para una IA a medida que se vuelve más inteligente, y todas violan la esperanza de que todos los pensamientos de la IA estén en inglés sencillo, donde podamos ver claramente las señales de advertencia.

#### **Las señales de advertencia solo importan si les prestas atención.** {#las-señales-de-advertencia-solo-importan-si-les-prestas-atención.}

Si los ingenieros de IA se limitan a entrenar contra las alarmas hasta que estas desaparecen (mientras el comportamiento subyacente continúa), la transparencia solo conduce a una falsa sensación de seguridad.

Hasta ahora, las empresas de IA han respaldado modelos que [mienten, adulan y engañan](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters), dan [consejos dudosos](https://www.wired.com/story/google-ai-overview-search-issues/) o [escriben ransomware](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025). En ocasiones, se ha observado que los modelos inducen o mantienen [delirios](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) o [psicosis](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) en usuarios vulnerables, lo que en al menos un caso terminó en «suicidio por policía».[^45] Las empresas simplemente hacen un poco más de entrenamiento y siguen adelante, tal como siguieron adelante después de que Sydney Bing [amenazara a los periodistas](https://x.com/sethlazar/status/1626257535178280960). Hasta ahora, esto solo ha servido para enmascarar los problemas.

Cuando se han enfrentado a una indignación suficiente, las empresas han llevado a cabo [modestas rectificaciones](https://www.nytimes.com/2024/06/01/technology/google-ai-overviews-rollback.html) y han emitido [comunicados de prensa](https://openai.com/index/sycophancy-in-gpt-4o/) sobre el endurecimiento de sus procedimientos. Sin embargo, como explicamos en los capítulos 4 y 5, estas soluciones superficiales no abordan los problemas subyacentes.

No nos malinterpretes: es *útil* que las IA realicen hoy en día una parte considerable de su razonamiento en inglés. Nos ayuda a ver las señales de advertencia. Pero hay una gran diferencia entre tener señales de advertencia y *tener alguna forma de solucionar las cosas*.

Para más información sobre este tema, véase el capítulo 11 y «[¿No habrá señales de alerta tempranas que los investigadores puedan utilizar para identificar los problemas?](#won't-there-be-early-warnings-researchers-can-use-to-identify-problems?)».

### ¿No son las IA «solo matemáticas»? {#¿no-son-las-ia-«solo-matemáticas»?}

#### **\* Decir que las IA son «solo matemáticas» es como decir que los seres humanos son «solo bioquímica».** {#*-decir-que-las-ia-son-solo-matematicas-es-como-decir-que-los-seres-humanos-son-solo-bioquimica.}

En sentido estricto, una IA no es «solo» matemáticas. Es una máquina física cuyas operaciones pueden describirse matemáticamente. Si esa máquina tiene salidas que pueden ser leídas por los humanos, o si tiene salidas que están conectadas a cuerpos robóticos, entonces es tan capaz de afectar al mundo como tú (utilizando «solo» señales bioeléctricas dentro de tu cerebro).

Compara:

![][image3]

Para más información sobre este tema, ve el capítulo 6.

#### **Las operaciones matemáticas pueden representar ideas que no son intuitivamente «matemáticas».** {#las-operaciones-matemáticas-pueden-representar-ideas-que-no-son-intuitivamente-«matemáticas».}

La multiplicación, la suma, la obtención de máximos y otras operaciones matemáticas pueden utilizarse para representar cosas que (desde una perspectiva humana) no tienen nada que ver con las matemáticas.

Es muy parecido a cómo los 1 y los 0 que las computadoras se envían entre sí pueden codificar letras. Los 1 y los 0 pueden incluso codificar cosas como imágenes.

Los 1 y los 0 no se limitan a codificar imágenes de cosas que parecen frías, azuladas y mecánicas. También pueden codificar imágenes de hermosas flores bajo la luz natural. Los 1 y los 0 pueden codificar cosas que son hermosas, cálidas y suaves; pueden codificar cosas que exaltan el espíritu humano.

Sería una [falacia de composición](https://en.wikipedia.org/wiki/Fallacy_of_composition) decir que codificar una imagen en unos y ceros significa que la imagen tiene que ser sobre algo numérico o robótico. Sería como decir que el cerebro humano está formado por neurotransmisores con nombres como «norepinefrina» y que, por lo tanto, los seres humanos solo deberían pensar en química o ser buenos razonando sobre neurotransmisores y sitios de unión.

Y aunque es *genial* que se puedan construir infinitas variedades de cosas a partir de piezas extremadamente simples, no hay nada inefable o mágico en cómo funciona este proceso. Podrías estudiar un poco y aprender cómo las imágenes de flores cálidas y hermosas pueden codificarse en unos y ceros hasta que ni siquiera te parezca sorprendente. Compara los [errores del vitalismo](#special-behavior-is-built-out-of-mundane-parts).

A veces, efectivamente, no conocemos todas las reglas sobre cómo se compone algo, y entonces el paso de las cosas simples a las complicadas puede parecer muy misterioso y, de hecho, sorprendernos. Pero cuando *sí* entendemos cómo se compone algo complicado a partir de piezas más simples, termina pareciendo tan sencillo como construir un modelo de coche de carreras con piezas de LEGO. Cuando puedes ver cómo funciona, todo está ahí, en los bloques.

Lo mismo sucede con las redes neuronales. No entendemos cómo el comportamiento complejo de las IA modernas surge de partes tan simples, de la misma manera que entendemos los formatos de imagen binarios y los LEGO. Ni siquiera entendemos la «psicología» y la «neurociencia» de las IA tan bien como entendemos cómo las moléculas y sustancias químicas de una neurona humana se combinan para formar el pensamiento humano. Eso no significa que el conocimiento *no esté ahí* o *no pueda existir*; solo significa que aún no lo tenemos.

Incluso sin entender por qué funcionan las IA, los humanos pueden entrenarlas para que jueguen bien al ajedrez. Con suficientes parámetros y operaciones aritméticas, podemos entrenar a las IA hasta el punto en que empiecen a hablar como una persona. Se podría decir que los patrones complicados que animan a una IA a hablar son «solo matemáticas». Pero no son «matemáticas» como las preguntas de un examen de matemáticas de bachillerato. Son «solo matemáticas» de la misma manera que un cerebro humano completo es «solo química».

La mera química llegó a la Luna. Inventó las armas nucleares. Construyó el mundo tal como lo conocemos hoy. Puede que sea difícil ver *cómo* las simples sustancias químicas del cerebro humano hicieron todas esas cosas, pero las hicieron de cualquier manera.

La IA no es diferente. De alguna manera, aunque no entendamos completamente cómo funcionan las IA internamente, pudimos «cultivar» IA que pueden escribir poesía, componer música, jugar al ajedrez, conducir autos, doblar la ropa, hacer revisiones de literatura y descubrir nuevos medicamentos.

Estar «hechas de matemáticas» no impidió que las IA hicieran esas cosas. Entonces, ¿por qué debería impedirles hacer otro conjunto más complejo de cosas mañana? ¿Dónde trazas la línea y cómo sabes dónde trazarla? Resulta que las operaciones matemáticas son suficientes para hacer mucho más de lo que mucha gente espera.

### ¿No se limitan las IA a predecir el siguiente token? {#¿no-se-limitan-las-ia-a-predecir-el-siguiente-token?}

#### **\* Predecir tokens requiere comprender el mundo.** {#*-predecir-tokens-requiere-comprender-el-mundo.}

Imaginar que una IA que predice el siguiente token no puede hacer pensamiento real es como imaginar que una imagen codificada usando unos y ceros binarios no puede retratar una flor roja. La IA está produciendo tokens, sí, ¡pero puedes codificar cosas importantes en tokens! Predecir lo que viene después es un aspecto central de la inteligencia en el que procesos como la «ciencia» y el «aprendizaje» encajan fácilmente.

Considera el desafío de predecir texto registrado en internet. En algún lugar de internet, hay un registro de un estudiante curioso de física entrevistando a un profesor experimentado. El profesor considera la pregunta en silencio y luego produce su respuesta, que se registra a continuación en la transcripción.

La tarea de predecir con precisión la respuesta del profesor implica predecir sus pensamientos silenciosos sobre la física. Y predecir sus pensamientos silenciosos sobre la física requiere predecir cómo entenderá la pregunta del estudiante, predecir lo que el profesor sabe de física y predecir cómo aplicará ese conocimiento.

Si una IA puede predecir tan bien el texto de Internet que es capaz de predecir la respuesta novedosa de un físico a una pregunta, la primera vez que aparece, entonces la IA debe poseer necesariamente la capacidad de realizar razonamientos novedosos sobre física por sí misma, al menos tan bien como ese profesor de física.

Cuando se trata de predecir texto que refleja un mundo complicado y desordenado, la memorización mecánica no lleva muy lejos. Para hacer predicciones precisas, hay que desarrollar la capacidad de predecir no solo el texto, sino también el mundo complicado y desordenado que hay detrás de él.

#### **Las IA modernas no solo predicen tokens.** {#las-ia-modernas-no-solo-predicen-tokens.}

Es cierto que los primeros LLM, como GPT-2 y el primer GPT-3, se entrenaron exclusivamente para la tarea de predicción. Su «único trabajo», por así decirlo, era hacer coincidir la distribución exacta de sus datos de entrenamiento: texto extraído de varios sitios web.

Pero esos días han terminado. Los LLM modernos están entrenados para responder de diversas maneras que sus creadores consideran más útiles. Esto se suele hacer mediante el «aprendizaje por refuerzo».

En un entorno de aprendizaje por refuerzo, las actualizaciones aplicadas a un modelo de IA mediante descenso de gradiente se basan en qué tan bien tiene éxito (o qué tan mal fracasa) en una tarea determinada. Una vez que las salidas de un modelo de IA se moldean mediante este tipo de entrenamiento, ya no son predicciones puras, sino que también tienen una cualidad de dirección.

ChatGPT podría ser capaz de predecir que el final más probable de un chiste obsceno es una palabrota, pero incluso cuando se le coloca en un contexto en el que ha comenzado a contar el chiste, a menudo dirigirá el final del chiste hacia un remate diferente para evitar generar esa palabra, porque previamente ha sido entrenado para no decir palabrotas. Esto es lo que da lugar a ejemplos interesantes de comportamiento similar al deseo en casos como los discutidos en el Capítulo 3.

Incluso si las IA no estuvieran entrenadas para completar tareas, es probable que entrenarlas para la predicción pura acabara induciéndolas a dirigir. Para predecir el complicado mundo real y los complicados seres humanos que viven allí, una IA probablemente necesitaría muchas partes internas que hicieran dirección, de modo que pudiera dirigir su propia atención hacia las partes más relevantes de los problemas de predicción. Y a menudo ocurre que la mejor manera de predecir exitosamente las cosas es dirigir el mundo en una dirección que cumpla esas predicciones, como cuando un científico descubre cómo diseñar y ejecutar un nuevo experimento.

Finalmente, es poco probable que una IA entrenada para ser muy buena en la predicción se preocupe únicamente por la predicción. Por razones que discutiremos en el Capítulo 4, es probable que termine con todo tipo de objetivos extraños e incomprensibles. Pero eso es un punto discutible de cualquier manera; las IA modernas están entrenadas no solo para hacer predicciones, sino para completar tareas.

### ¿Acaso las IA solo son capaces de repetir como loros lo que dicen los humanos? {#¿acaso-las-ia-solo-son-capaces-de-repetir-como-loros-lo-que-dicen-los-humanos?}

#### **Para predecir bien el siguiente token, los LLM necesitan aprender cómo funciona el mundo.** {#para-predecir-bien-el-siguiente-token,-los-llm-necesitan-aprender-cómo-funciona-el-mundo.}

Supongamos que un médico está redactando un informe sobre lo que le ha sucedido a un paciente. Una parte del informe médico dice lo siguiente:

> Al tercer día de ingreso, el paciente desarrolló confusión aguda y temblores. Se detectaron niveles de amoníaco sérico...

Imaginemos una IA entrenada con estos datos a la que se le pide que prediga la siguiente palabra, siendo dos candidatos plausibles «elevado» o «normal». No se trata solo de predecir el tipo de palabras que usan los humanos; se trata de predecir lo que ocurrió en el mundo de la realidad médica, la biología y los acontecimientos dentro del paciente. ¿Cuánto amoníaco había que medir en la vida real?

La IA que predice la siguiente palabra tiene aquí una tarea más difícil que la persona que redactó el informe. La persona que redactó el informe solo está escribiendo lo que se observó realmente. La IA que predice el informe tiene que adivinarlo de antemano.

La IA asigna una probabilidad del 70 % a «elevado», del 20 % a «normal» y del 10 % a otras palabras.

La siguiente palabra real del informe es «normal».

Todo lo que hay dentro de la IA que pensaba que iba a ser «elevado» pierde un poco de fuerza, dentro de la comprensión de la medicina por parte de la IA. Todos los parámetros se ajustan mínimamente en la dirección de hacer que la comprensión médica que predijo «normal» sea más dominante.

Hasta que, tras un entrenamiento suficiente, la IA realiza [ciertos diagnósticos médicos](https://pubmed.ncbi.nlm.nih.gov/38976865/) mejor que la mayoría de los médicos.

La IA no está siendo entrenada para *escribir tonterías que suenen como un informe médico típico*. Está siendo entrenada para *predecir la siguiente palabra exacta en todos los informes médicos particulares que ve*.

Quizás si empezáramos con un modelo muy pequeño con muy pocos parámetros, solo podría aprender a escribir tonterías con tintes médicos, pero con modelos más grandes, eso no parece ser lo que está ocurriendo en los *benchmarks* que comparan médicos humanos con IA.

Cuando alguien te pasa el brazo por los hombros y te dice con tono de gran sabiduría que una IA no es más que «un loro estocástico», es posible que esté imaginando los divertidos programas informáticos antiguos que extendían frases basándose en la frecuencia de grupos de palabras («n-gramas»): «En ocasiones anteriores en las que hemos visto aparecer estas dos palabras en el corpus, ¿cuál ha sido normalmente la siguiente palabra?».

Los sistemas que adivinan la siguiente palabra basándose en las dos o tres últimas son triviales y existían mucho antes que los LLM. No desafían a los humanos en la capacidad de predecir casos médicos. No *suenan* como personas hablándote. Si se pudieran obtener [miles de millones de dólares](https://www.reuters.com/business/openai-hits-12-billion-annualized-revenue-information-reports-2025-07-31/) en ingresos simplemente haciendo lo del loro probabilístico, la gente lo habría hecho mucho antes.

Si los miles de millones de cálculos dentro de un LLM real no estuvieran haciendo ningún trabajo pesado, si el sistema solo escupiera una suposición superficial basada en las características superficiales de las palabras anteriores, entonces sonaría como los sistemas pasados que realmente escupían suposiciones superficiales. Por ejemplo, entrenado con Jane Austen, un sistema de n-gramas [produce](https://web.stanford.edu/~jurafsky/slp3/3.pdf):

> «¡Eres encantador en todos los sentidos!», exclamó él con una sonrisa de complicidad, y de vez en cuando yo hacía una reverencia y ellos veían una calesa y cuatro caballos que desear.

Un LLM, al que se le pide que produzca una frase al estilo de Jane Austen, resulta dramáticamente más convincente; si no nos crees, [prueba](https://claude.ai/new) [preguntando](https://gemini.google.com/app) [a uno](https://chatgpt.com/).

Además, aunque no podemos saber mucho sobre lo que ocurre dentro de la mente de una IA, la empresa de IA Anthropic [publicó una investigación](https://www.anthropic.com/research/tracing-thoughts-language-model#does-claude-plan-its-rhymes) afirmando que su IA (Claude) planificaba más de una palabra por adelantado. Es decir, Claude estaba considerando qué frases y significados posteriores podrían ser plausibles, con el fin de adivinar qué siguientes letras podrían verse.

#### **\* Las IA ya pueden superar sus datos de entrenamiento o prescindir de los datos humanos.** {#*-las-ia-ya-pueden-superar-sus-datos-de-entrenamiento-o-prescindir-de-los-datos-humanos.}

En 2016, una IA llamada AlphaGo, creada por Google DeepMind, venció al campeón mundial humano en el juego de mesa [Go](https://en.wikipedia.org/wiki/Go_\(game\)). Se entrenó con una enorme biblioteca de partidas humanas de Go y también aprendió jugando muchas partidas contra sí misma.

El hecho de que fuera capaz de vencer a los humanos sugiere que fue capaz de aprender estrategias generales a partir de su entrenamiento, y que logró modelar patrones profundos en sus datos de entrenamiento, incluidos (quizás) patrones profundos que los humanos aún no habían detectado. El descenso de gradiente refuerza todo lo que funciona, independientemente de su procedencia.

Pero el dominio de AlphaGo técnicamente solo era *sugestivo* del hecho de que las IA pueden superar con creces sus datos de entrenamiento. La gente aún podría objetar que tal vez AlphaGo solo estaba copiando a los humanos y logrando ganar al ser más *consistente en aplicar* habilidades de nivel humano, en lugar de utilizar nuevos patrones que los humanos encontrarían novedosos o reveladores.

Esto no encajaría muy bien con el caso del ajedrez computacional (donde los maestros de ajedrez humanos aprenden muchas estrategias y conocimientos de los motores de ajedrez computacionales que los superan ampliamente). Pero tras AlphaGo, hubo personas que argumentaron que la IA solo venció a Lee Sedol porque se entrenó con grandes cantidades de datos humanos.[^46]

Al parecer, la gente de DeepMind también vio esas objeciones. Durante el año y medio siguiente, crearon una IA llamada AlphaGo Zero, lanzada en 2017. No se entrenó con ningún dato humano en absoluto. Aprendió el juego completamente mediante autocompetición. Superó a los mejores jugadores humanos después de solo tres días.[^47]

Aún se podría objetar que el Go es bastante más sencillo que el mundo real, y que es mucho más fácil descifrar el Go desde cero que descifrar (por ejemplo) ciencia, física e ingeniería desde cero. ¡Y eso es cierto! Pero tampoco es exactamente lo que decían los detractores *antes* de que las computadoras se volvieran buenas jugando Go.

Allá por 1997 —diecinueve años antes de que ganara AlphaGo— la gente predecía que tardarían [cien años](https://www.nytimes.com/1997/07/29/science/to-test-a-powerful-computer-play-an-ancient-game.html) para que las computadoras jugaran Go a nivel sobrehumano. Así que al menos sabemos que muchas personas tienen intuiciones erróneas sobre este tipo de cosas.

El mundo real es un entorno más complicado que el Go. Los patrones cognitivos que subyacen a la ingeniería, la física, la fabricación, la logística, etc., son más complejos que los patrones cognitivos que subyacen al juego hábil de Go. Pero no hay base teórica alguna para la idea de que, una vez que las IA puedan aprender esos patrones, estarán limitadas a las variantes humanas. El descenso de gradiente reforzará las partes de la IA que encuentren patrones cognitivos que *funcionen realmente bien*, independientemente de su procedencia.

Nada de esto es un argumento de que los LLM en particular aprenderán esos patrones hasta el punto de poder automatizar el progreso científico y tecnológico. No sabemos si pueden o no pueden. El punto es que «solo» entrenarlos con texto humano no es ningún tipo de limitación fundamental. Sí se entrenan únicamente con datos humanos, pero no dejes que eso te ciegue a las chispas de generalidad y los indicios de razonamiento profundo enterrados en la pila gigantesca de «instintos» superficiales.

Tendremos más que decir, en el Capítulo 3, sobre cómo una IA podría generalizar desde un conjunto limitado de ejemplos hacia una capacidad más general.

### ¿No serán las IA inevitablemente frías y lógicas, o les faltará alguna chispa crucial? {#¿no-serán-las-ia-inevitablemente-frías-y-lógicas-o-les-faltará-alguna-chispa-crucial?}

#### **\* No.** {#*-no.-1}

Simplemente porque las IA funcionen en computadoras no significa que su forma de pensar deba compartir las cualidades que asociamos con las computadoras, del mismo modo que tu forma de pensar no tiene por qué compartir las cualidades asociadas con la biología, la química y los neurotransmisores.

Cuando los humanos no entendían la bioquímica, atribuían la vitalidad de la vida a una «esencia vital» irreplicable. Pero la realidad no está hecha de materia mundana animada a veces por una fuerza vital mágica. La vida está hecha de partes mundanas.

No pretendemos, sin embargo, degradar la inteligencia cuando decimos que está compuesta por partes mundanas y que las máquinas podrían hacer el mismo trabajo. Véase nuestro discusión extensa sobre el [vitalismo](#special-behavior-is-built-out-of-mundane-parts).

La heurística «las máquinas no pueden competir con los humanos» era errónea cuando Kasparov predijo que una máquina que careciera de la creatividad humana nunca podría vencerlo al ajedrez; era errónea cuando la gente pensaba que las IA nunca podrían dibujar imágenes bonitas; era errónea cuando la gente pensaba que las IA nunca podrían mantener una conversación. El cerebro humano es una prueba de que la materia física realmente puede implementar formas superiores de inteligencia, suficientes para dirigir una civilización tecnológica; y es muy improbable que el cerebro humano sea la única forma de hacer ese trabajo.

Ampliaremos este punto en uno de los suplementos en línea del capítulo 3: [Antropomorfismo y mecanomorfismo](#anthropomorphism-and-mechanomorphism).

#### **Las IA son entidades nuevas, interesantes y extrañas.** {#las-ia-son-entidades-nuevas,-interesantes-y-extrañas.}

Los aviones vuelan, pero no baten las alas. Los brazos robóticos funcionan sin piel suave ni sangre roja. Los transistores funcionan de manera muy diferente a las neuronas, y DeepBlue jugó ajedrez a nivel mundial sin el tipo de pensamientos que se daban en la mente de Garry Kasparov. Este es el curso habitual de la tecnología.

Cuando no entendemos bien el vuelo o el juego, a veces imaginamos que el enfoque utilizado por la biología es el único enfoque posible que puede funcionar. Una vez que entendemos un campo un poco mejor, esto resulta ser muy erróneo.

El trabajo de manejar un tablero de ajedrez lo hizo DeepBlue de forma muy diferente a como lo hizo Kasparov, y el trabajo de dirigir el mundo en general seguirá casi con toda seguridad un patrón similar. Como se comentó en el capítulo 2, parece que la IA ya está realizando su trabajo de una forma muy diferente a como lo harían los humanos, aunque esto puede ser un poco más difícil de ver cuando utiliza su inteligencia para imitar a los humanos. En el capítulo 4, exploraremos cómo es probable que estas diferencias conduzcan a situaciones extrañas, con graves consecuencias.

### ¿No serán los LLM como los humanos en los datos con los que se entrenan? {#¿no-serán-los-llm-como-los-humanos-en-los-datos-con-los-que-se-entrenan?}

#### **\* Hay una diferencia entre la maquinaria que se necesita para ser una persona y la maquinaria que se necesita para predecir a muchos individuos.** {#*-hay-una-diferencia-entre-la-maquinaria-que-se-necesita-para-ser-una-persona-y-la-maquinaria-que-se-necesita-para-predecir-a-muchos-individuos.}

(Lo que sigue es una versión resumida de una discusión más técnica que se puede encontrar más abajo en «[Finge hasta que lo consigas](#«finge-hasta-que-lo-consigas»)».)

Las IA como ChatGPT están entrenadas para predecir con precisión sus datos de entrenamiento. Y sus datos de entrenamiento se componen principalmente de texto humano, como páginas de Wikipedia y conversaciones en salas de chat. (Esta parte del proceso de entrenamiento se denomina «preentrenamiento», que es lo que significa la «P» de «GPT»). Los primeros LLM, como GPT-2, se entrenaron *exclusivamente* para la predicción de esta manera, mientras que las IA más recientes también se entrenan en aspectos como la precisión a la hora de resolver problemas matemáticos (generados por computadora) y dar buenas respuestas según otro modelo de IA, entre otros objetivos.

Pero consideremos una IA entrenada solo para predecir texto generado por humanos. ¿Debe volverse similar a los humanos?

Supongamos que tomas a una excelente actriz[^48] y le enseñas a predecir el comportamiento de todos los borrachos de un bar. No «aprender a interpretar a un borracho estereotípico medio», sino «aprender a conocer a todos los borrachos de este bar como *individuos*». Los LLM no están entrenados para *imitar promedios*, sino para *predecir las próximas palabras individuales* utilizando todo el contexto de las palabras anteriores.

Sería absurdo esperar que esta actriz *se volviera perpetuamente borracha* en el proceso de aprender a predecir lo que dirá cada persona borracha. Podría desarrollar partes de su cerebro que sean muy buenas para actuar como borracha, pero no se volvería borracha *ella misma*.

Incluso si más tarde le pidieras a la actriz que predijera lo que haría un borracho concreto en el bar y que luego se comportara externamente de acuerdo con su propia predicción, seguirías sin esperar que la actriz se sintiera borracha por dentro.

¿Cambiaría algo si estuviéramos constantemente modificando el cerebro de la actriz para hacer predicciones *aún mejores* sobre personas borrachas? Probablemente no. Si *realmente* terminara borracha, sus pensamientos terminarían siendo confusos, lo que interferiría con el arduo trabajo de una actriz. Podría confundirse sobre si estaba prediciendo a una Alice borracha o a una Carol borracha. Sus predicciones empeorarían y nuestro hipotético modificador de cerebros aprendería a no modificar su cerebro de esa manera.

O, dicho de otra manera: un humano que se vuelve excelente imitando a los pájaros y entendiendo su psicología no se convierte por ello en un pájaro con cuerpo humano, ni siquiera se vuelve especialmente parecido a un pájaro en su vida cotidiana.

Del mismo modo, entrenar a un LLM para que haga predicciones excelentes sobre la siguiente palabra que escriban muchas personas diferentes sobre sus experiencias psicodélicas pasadas no debería entrenar al LLM en sí mismo para que sea como un humano bajo los efectos de drogas. Si las cogniciones internas reales del LLM se distorsionaran de una manera que recordara a «estar drogado», esto interferiría con el arduo trabajo del LLM de predecir la siguiente palabra; podría confundirse y pensar que un hablante de inglés iba a continuar en chino.

No estamos diciendo que «ninguna máquina pueda tener nunca algo parecido al estado mental que tiene un humano». Lo que decimos es que no se debe esperar por defecto que la tecnología actual de AA cree motores de predicción de ebrios que funcionen emborrachándose ellos mismos.

El trabajo de averiguar cómo predecir todo tipo de humanos diferentes es diferente del trabajo de ser un humano. Lo que significa que no se debe esperar que las IA construidas con métodos similares a los actuales se parezcan mucho a un humano, en el proceso de aprender a actuar como cualquiera de nosotros según la solicitud.

#### **La arquitectura de los LLM es muy diferente a la de los humanos.** {#la-arquitectura-de-los-llm-es-muy-diferente-a-la-de-los-humanos.}

Consulta el capítulo 2 para obtener una breve discusión de por qué los LLM parecen bastante extraños.

En el capítulo 4, profundizaremos en cómo las IA terminan teniendo preferencias y objetivos muy extraños, un fenómeno que ya hemos comenzado a observar en la vida real, con más ejemplos que se acumulan incluso después de la publicación del libro. Consulta el capítulo 4 [suplemento](#aren’t-developers-regularly-making-their-ais-nice-and-safe-and-obedient?) para ver algunos ejemplos.

### ¿Cómo podría una IA entrenada únicamente con datos humanos superar a los humanos? {#¿cómo-podría-una-ia-entrenada-únicamente-con-datos-humanos-superar-a-los-humanos?}

#### **\* Quizás aprendiendo habilidades generales e implementándolas mejor.** {#*-quizás-aprendiendo-habilidades-generales-e-implementándolas-mejor.}

Deep Blue era capaz de jugar al ajedrez mucho mejor que cualquiera de sus programadores de IBM. ¿Cómo era posible que la gente construyera una máquina que fuera más inteligente que ellos en el dominio del ajedrez? Creando una IA que hiciera algunos de los mismos tipos de cosas que ellos intentaban hacer en las partidas de ajedrez (como considerar múltiples formas posibles en que podía desarrollarse la partida), pero de manera mucho más rápida y precisa.

Del mismo modo, una IA podría aprender a hacerlo mejor que los humanos en todo tipo de otras habilidades. Podría aprender patrones de pensamiento que contribuyen a las habilidades de razonamiento general, y luego realizar esas habilidades generales más rápido y con una menor tasa de error.

También podría cometer menos «pasos en falso» mentales del tipo que los humanos son propensos a cometer. Esto podría deberse a que esos pasos en falso se eliminaron de la IA en algún momento, o a que la maquinaria subyacente de la IA que predice los pasos en falso «humanos» nunca fue propensa a cometer los mismos pasos en falso. O tal vez a la IA se le concedió finalmente el poder de automodificarse y eliminó su propensión a los pasos en falso; o tal vez se le encomendó finalmente la tarea de diseñar una IA más inteligente y diseñó una que cometía menos pasos en falso; o su entrenamiento le enseñó a cometer menos errores de alguna otra manera.

La capacidad de tener percepciones totalmente novedosas no proviene de una profunda chispa atómica, sino que se construye a partir de partes mundanas, como [todas las cosas profundas](#special-behavior-is-built-out-of-mundane-parts). En principio, un estudiante puede observar a su profesor y aprender todo tipo de cosas que hace, y luego tener una chispa de inspiración y ser capaz de hacer esas cosas más rápido o mejor. O un estudiante podría reutilizar diferentes técnicas que aprendió de un profesor para encontrar una forma totalmente novedosa de generar sus propias percepciones.

Hemos tenido la suerte de contar con evidencia observacional directa de ambos puntos, en el caso de AlphaGo, que hemos comentado [anteriormente](#*-ais-can-already-surpass-their-training-data,-or-forego-human-data.). AlphaGo fue entrenado exhaustivamente con datos humanos, pero fue capaz de jugar al Go mejor que los mejores humanos. (Y AlphaGo Zero, que aprendió solo a partir del juego propio (y sin datos humanos), llegó aún más lejos).

Esto no nos parece un mundo en el que los datos humanos sean la limitación clave (como hemos [argumentado en otra parte](https://intelligence.org/2017/10/20/alphago/)), en comparación con las limitaciones reales, que son cosas como la arquitectura de la IA o la cantidad de computación que es capaz de utilizar antes de jugar.

Los estudiantes pueden superar a sus maestros.[^49]

#### **Quizás por cualquier otro método que funcione. El éxito a menudo requiere tales habilidades, por lo que el descenso de gradiente las encontrará.** {#quizás-por-cualquier-otro-método-que-funcione.-el-éxito-a-menudo-requiere-tales-habilidades,-por-lo-que-el-descenso-de-gradiente-las-encontrará.}

Predecir las palabras humanas requiere comprender el mundo, como ya comentamos en [«¿No son las IA capaces solo de repetir como loros lo que dicen los humanos?»](#aren't-ais-only-able-to-parrot-back-what-humans-say?).

Por poner un ejemplo ilustrativo: a finales del siglo XVI, el astrónomo Tycho Brahe recopiló minuciosamente observaciones de las posiciones planetarias en el cielo nocturno. Sus datos fueron fundamentales para el trabajo de Johannes Kepler, quien descubrió el patrón elíptico del movimiento planetario, lo que inspiró la teoría de la gravitación de Newton. Pero el propio Brahe nunca descubrió las leyes que rigen los planetas.

Imaginemos una IA entrenada únicamente con textos producidos hasta el año 1601, que nunca hubiera oído hablar de Brahe, pero que tuviera que predecir cada punto de datos que Brahe anotó en su diario. Brahe seguía registrando la posición de Marte cada noche, por lo que la IA funcionaría mejor cuanto más precisa fuera su predicción de la ubicación de Marte. El descenso de gradiente reforzaría cualquier parte dentro de la IA que fuera capaz de averiguar exactamente cuándo Marte parecería dar la vuelta (desde la perspectiva de Brahe) y atravesar el cielo hacia atrás.

No importa que Brahe nunca lograra descubrir esa ley de la naturaleza. El simple objetivo de entrenamiento «predecir qué posición de Marte anotará Brahe a continuación» es el tipo de objetivo de entrenamiento que reforzaría cualquier parte de la IA que fuera lo suficientemente inteligente como para descubrir cómo se mueven los planetas.

Si continuáramos entrenando y entrenando y entrenando esa IA hasta que predijera cada vez mejor lo que Brahe anotaría a finales del siglo XVI, esa IA tendría toda la razón del mundo para desarrollar conocimientos científicos que Brahe nunca pudo alcanzar. Una IA será *mejor en su tarea de predecir a los humanos* si se vuelve más inteligente que los humanos que está prediciendo, porque a veces los humanos anotan registros de fenómenos que ellos mismos no pueden predecir perfectamente.

Existe una cuestión separada sobre si las arquitecturas, los procesos de entrenamiento y los datos modernos son [*suficientes*](https://x.com/keyonV/status/1943730486280331460) para que las IA superen a sus maestros. Es posible que los LLM modernos aún no hayan llegado a ese punto. Pero no existe ningún impedimento teórico para la idea misma de superar a nuestros maestros. Entrenar a una IA para predecir a los humanos es suficiente para permitirle superarnos, en principio*.*

### ¿Qué te hace pensar que las personas pueden construir una IA sobrehumana cuando ni siquiera entienden la inteligencia? {#¿qué-te-hace-pensar-que-las-personas-pueden-construir-una-ia-sobrehumana-cuando-ni-siquiera-entienden-la-inteligencia?}

#### **\* El progreso pasado en IA no ha requerido mucha comprensión de la inteligencia.** {#*-el-progreso-pasado-en-ia-no-ha-requerido-mucha-comprensión-de-la-inteligencia.}

Como se explica en el Capítulo 2, el campo de la IA ha logrado sus recientes hazañas empleando el descenso de gradiente, un proceso que no requiere que los humanos comprendan la inteligencia. Los humanos hemos llegado bastante lejos sin necesidad de comprender la inteligencia.

#### **La selección natural no necesitaba «comprender» la inteligencia.** {#la-selección-natural-no-necesitaba-«comprender»-la-inteligencia.}

La evolución fue perfectamente capaz de producir la inteligencia humana sin que la selección natural comprendiera jamás la inteligencia. La comprensión puede ser útil o no en la práctica, pero la idea de que *necesitamos* comprenderla para producirla no tiene fundamento.

### ¿No demuestran las alucinaciones que las IA modernas son débiles? {#¿no-demuestran-las-alucinaciones-que-las-ia-modernas-son-débiles?}

#### **\* Las alucinaciones revelan tanto una limitación como una desalineación.** {#*-las-alucinaciones-revelan-tanto-una-limitación-como-una-desalineación.}

Los LLM modernos (mientras escribimos esto a mediados de 2025) son propensos a «alucinaciones» donde inventan respuestas a preguntas con un tono que suena confiado. Si les pides que redacten un informe legal, por ejemplo, a veces inventarán casos judiciales falsos como precedentes.

Esto tiene sentido si entendemos cómo se entrenan las IA. Una IA genera palabras muy similares a las que produciría un abogado humano real, y si un abogado humano real redactara un informe legal, incluiría casos judiciales reales. Por ejemplo, un abogado humano real podría escribir algo como:

> Al aplicar la prueba de equilibrio en Graham, el tribunal ha sostenido que existe poco interés gubernamental en arrestar a un sospechoso por un delito menor. Véase *Jones v. Parmley,* 465 F.3d 46 (2.º Cir. 2006\) (el jurado podría considerar razonablemente que dar patadas y puñetazos a manifestantes pacíficos en violación de la ordenanza local constituía uso excesivo de la fuerza); *Thomas v. Roach*, 165 F.3d 137 (2.º Cir. 1999\) (las amenazas verbales son un delito demasiado leve como para crear un fuerte interés gubernamental en el arresto).

Un abogado humano real nunca escribiría simplemente «En realidad no conozco la jurisprudencia relevante, disculpe» en un informe legal. Por lo tanto, cuando una IA intenta sonar como un abogado, en casos donde realmente no conoce los precedentes, lo mejor que puede hacer es inventárselos. Es lo más cerca que puede llegar a la realidad. Los impulsos e instintos dentro de la IA que producen texto de apariencia segura en ese tipo de situaciones se refuerzan regularmente mediante el descenso del gradiente.

Este comportamiento alucinatorio persiste incluso si se instruye a la IA para que diga «No lo sé» en casos donde no tiene la información. En esa situación, la IA está haciendo algo similar a representar un abogado que *diría* «No conozco el precedente aquí» *si* no lo conociera. Pero eso es irrelevante, mientras la IA esté (más o menos) representando un abogado que *sí* conoce el precedente, lo que significa que el personaje que interpreta nunca se encuentra con la *oportunidad* de decir «No lo sé». La IA podría generar texto como:

> Bajo el marco de equilibrio de Graham, los tribunales han reconocido consistentemente que existe un interés gubernamental mínimo en efectuar arrestos por violaciones menores. Véase *Carson v. Haddonfield*, 115 F.3d 64 (8.º Cir. 2005\) (encontrando uso excesivo de la fuerza cuando los oficiales desplegaron spray pimienta contra sospechosos de cruzar imprudentemente que no ofrecieron resistencia); *Walburg v. Jones*, 212 F.3d 146 (2.º Cir. 2012\) (sosteniendo que una citación por conducta desordenada es insuficiente para justificar técnicas de restricción física).

Esto es lo más cerca que puede llegar la IA a igualar el texto real. El texto «No conozco el precedente» está *más alejado del texto real* en términos de predicción textual;[^50] sería mucho menos similar al primer párrafo de texto mostrado arriba, incluso si se aproxima más a lo que quería el usuario.

Esto ofrece una perspectiva de la diferencia entre lo que las IA realmente intentan hacer (p. ej., sonar como un abogado confiado) frente a lo que los usuarios quieren que hagan (p. ej., redactar un informe legal utilizable). Estos dos propósitos diferentes a veces pueden coincidir (p. ej., cuando la IA intenta sonar amigable y el humano quiere un interlocutor amigable), pero esas diferencias que ahora parecen pequeñas tendrían enormes consecuencias si las IA se volvieran más inteligentes — como discutiremos con más detalle en el Capítulo 4.[^51]

#### **No está claro qué tan difícil será eliminar las alucinaciones, o cuánto potenciará esto las capacidades.** {#no-está-claro-cuán-difícil-será-eliminar-las-alucinaciones,-ni-cuánto-aumentará-esto-las-capacidades.}

Independientemente de por qué surgen las alucinaciones, es cierto que *en la práctica* limitan las capacidades efectivas de los LLM. Construir un cohete lunar requiere largas cadenas de razonamiento con una tasa de error muy baja. El hecho de que las IA simplemente se inventen cosas (y no siempre puedan notarlo o no siempre les importe) es un gran obstáculo para la confiabilidad que necesitarían para lograr importantes avances científicos y tecnológicos.

Pero esa espada tiene dos filos. Las alucinaciones y otros problemas de fiabilidad podrían frenar a la IA durante años. O podría ser que los problemas de fiabilidad sean la última pieza del rompecabezas, y que en el momento en que alguien tenga una idea inteligente que los resuelva, las IA superen algún [umbral crítico](#is-“intelligence”-a-simple-scalar-quantity?). No lo sabemos.

No sabemos si las alucinaciones serán fáciles de resolver en el paradigma actual, si alguien encontrará un truco ingenioso que haga que los modelos de razonamiento sean mucho más robustos, o si se necesitará una idea tan disruptiva como la arquitectura transformer que dio lugar a los LLM.

Sin embargo, sí observamos que solucionar las alucinaciones sería bastante lucrativo. Muchas personas están trabajando en ello. Esto podría interpretarse como que es probable que encuentren alguna idea ingeniosa o solución arquitectónica en poco tiempo. O podría interpretarse como una señal de que el problema es especialmente pernicioso y propenso a persistir, dado que ya lleva varios años presente.

En cualquier caso, eso no afecta mucho nuestro argumento. Lo que importa es que, eventualmente, se crearán IA más confiables, ya sea mediante versiones ligeramente modificadas de los LLM o mediante una arquitectura completamente nueva y disruptiva.

Véase también nuestro debate sobre cómo [este campo es bueno para superar obstáculos](#*-the-field-is-good-at-overcoming-obstacles.).

### Pero, ¿no se agotarán los datos antes de que la IA llegue hasta el final? ¿O la energía eléctrica? ¿O el financiamiento? {#¿no-se-agotarán-los-datos-antes-de-que-la-ia-llegue-hasta-el-final?-¿o-la-energía-eléctrica?-¿o-el-financiamiento?}

#### **\* Probablemente no.** {#*-probablemente-no.}

Los seres humanos utilizamos los datos de forma mucho más eficiente que las IA, por lo que sabemos que, *en principio*, es posible que las mentes inteligentes sean mucho más eficientes en el uso de datos que las IA modernas. Si los laboratorios de IA «se quedan sin» datos a la hora de mejorar los LLM, eso solo los ralentizará durante el tiempo que tome inventar nuevos métodos que sean más eficientes en el uso de datos.

Los seres humanos también utilizamos la energía de manera mucho más eficiente que las IA. Somos la prueba de que no existe ningún obstáculo fundamental para que las inteligencias generales funcionen con la misma energía que una bombilla. El hardware líder en IA no solo ha ido ganando [un 40 % más de eficiencia energética cada año](https://epoch.ai/data-insights/ml-hardware-energy-efficiency), sino que las mejoras algorítmicas significan que, según [una estimación de 2024](https://arxiv.org/abs/2403.05812), entre 2012 y 2023, «la capacidad de cómputo necesaria para alcanzar un umbral de rendimiento determinado se ha reducido a la mitad aproximadamente cada ocho meses».

Recordemos que el campo de la IA existe desde hace mucho más tiempo que la arquitectura LLM, y es bastante bueno a la hora de idear nuevas arquitecturas que superen los obstáculos. Y, en términos más generales, cuando la humanidad ha dedicado sus mejores mentes y recursos a algo que se sabe que es posible, tiene un [excelente](https://en.wikipedia.org/wiki/Manhattan_Project) [historial](https://en.wikipedia.org/wiki/Apollo_program) [de](https://en.wikipedia.org/wiki/Smallpox#Eradication) [éxitos](https://en.wikipedia.org/wiki/Human_Genome_Project).

Con investigadores expertos en IA que ahora [cobran habitualmente salarios de siete cifras](https://www.wired.com/story/mark-zuckerberg-meta-offer-top-ai-talent-300-million/) (de nueve cifras, en el caso de los puestos de alta dirección) y [la inversión privada anual en inteligencia artificial que se mide ahora en cientos de miles de millones de dólares](https://ourworldindata.org/grapher/private-investment-in-artificial-intelligence), parece que habrá el talento y los recursos necesarios para superar los obstáculos previstos. Véase también cómo [este campo es bueno para superar obstáculos](#*-the-field-is-good-at-overcoming-obstacles.).

#### **No esperes otro «invierno de la IA».** {#no-esperes-otro-invierno-de-la-ia}

La gente lleva [una](https://eugene.kaspersky.com/2016/09/09/the-artificial-artificial-intelligence-bubble-and-the-future-of-cybersecurity/) [década](https://medium.com/hackernoon/is-another-ai-winter-coming-ac552669e58c) [entera](https://medium.com/ux-management/the-next-ai-winter-a-journey-through-the-twilight-zone-of-technology-db41e71742a6) prediciendo erróneamente un «invierno de la IA» inminente. Los inviernos de la IA solían producirse entre los años 70 y los 90, cuando la financiación de la IA era pública y los financiadores públicos se cansaron de la falta de resultados. Porque la IA de antaño, de hecho, no *producía* resultados.

Con la IA moderna, ChatGPT fue quizás la aplicación que se adoptó más rápidamente en toda la historia y está generando grandes cantidades de dinero. Generó 3700 millones de dólares en ingresos en 2024, con previsiones de generar 12 700 millones en 2025. Está impulsada por la inversión privada y está ganando suficiente dinero como para atraer a los mejores talentos del mundo sin ninguna fuente pública que pueda cortarlos.

Todavía es *posible* que las técnicas de IA se topen con algún tipo de obstáculo y que la humanidad tenga un respiro antes de que llegue la superinteligencia. Pero el viejo patrón de los «inviernos de la IA» —financiamiento público, falta de resultados y declive— se ha roto.

### ¿Podrían los LLM avanzar hasta alcanzar la superinteligencia? {#¿podrían-los-llm-avanzar-hasta-alcanzar-la-superinteligencia?}

#### **No está claro, pero los investigadores están encontrando formas de superar las antiguas limitaciones de los LLM.** {#no-está-claro,-pero-los-investigadores-están-encontrando-formas-de-superar-las-antiguas-limitaciones-de-los-llm.}

Antes se decía que «los LLM solo piensan en una sola pasada y no pueden realizar cadenas de razonamiento largas o recursivas». Ahora, los LLM se utilizan para producir largas cadenas de razonamiento que los modelos luego revisan y amplían. Esto ha mejorado las capacidades de las IA modernas.

La IA es un objetivo en constante evolución. Los investigadores en este campo pueden ver los obstáculos y están haciendo todo lo posible por superarlos.

#### **\* Otros enfoques pueden alcanzar pronto la superinteligencia, incluso si los LLM no lo consiguen.** {#*-otros-enfoques-pueden-alcanzar-pronto-la-superinteligencia,-incluso-si-los-llm-no-lo-consiguen.}

[Este campo es bueno para superar obstáculos.](#*-the-field-is-good-at-overcoming-obstacles.) No escribimos *If Anyone Builds It, Everyone Dies* para advertir a la gente sobre los LLM en particular. Lo escribimos para advertir a la gente sobre la superinteligencia.

La razón por la que hablamos de los LLM no es porque estemos seguros de que sean el camino más corto hacia la superinteligencia. Hablamos de los LLM porque representan el enfoque de IA que actualmente funciona, y porque estudiarlos es una excelente manera de comprender lo poco que sabemos sobre estas nuevas mentes que la humanidad está creando.

Véase también la discusión extendida sobre [por qué importa el descenso de gradiente](#¿de-qué-sirve-el-conocimiento-de-los-llms?).

# 

## Discusión extendida {#discusión-extendida-2}

### La inteligencia no es inefable {#la-inteligencia-no-es-inefable}

En años recientes, el campo de la IA ha progresado no profundizando nuestra comprensión de la inteligencia, sino encontrando maneras de «hacer crecer» la IA. Tras años de callejones sin salida y estancamiento en los intentos de entender la inteligencia misma, y con el éxito en el crecimiento de IA poderosas, algunas personas se preguntan si la idea de «comprender la inteligencia» es solo un espejismo. ¿Quizás no hay principios generales que entender? ¿O tal vez los principios son demasiado extraños o complicados para que los humanos los comprendan?

Otros sienten que debe haber algo especial y místico sobre la mente humana, algo demasiado sagrado para reducirlo a ecuaciones secas. Y puesto que la inteligencia *aún* no se comprende, quizás la verdadera inteligencia proviene de esta parte inefable del espíritu humano.

Nuestra propia perspectiva es bastante más mundana que eso. La inteligencia es un fenómeno natural, como cualquier otro. Y como muchos fenómenos en biología, psicología y otras ciencias, aún estamos en las etapas tempranas de nuestros intentos de comprenderla.

Muchas de las herramientas y conceptos básicos de la psicología y neurociencia modernas han existido por solo unas pocas décadas. Puede sonar humilde decir: «La ciencia tiene sus límites, y este es quizás uno de ellos». Pero imagina decirle a alguien que crees que los científicos *dentro de un millón de años* no entenderán mucho sobre inteligencia más allá de lo que sabemos en 2025. En esos términos, afirmar que la inteligencia es *inefable* suena más arrogante que la alternativa.

La razón principal por la que nos importa esta pregunta es que incide en si la humanidad podría algún día construir una superinteligencia *sin* amenazar nuestra supervivencia. Argumentaremos, en el Capítulo 11, que la IA actual se ve más como alquimia que como química. Pero ¿es siquiera posible que haya una «química» de la IA?

Dado que *hoy en día* no disponemos de los conocimientos científicos pertinentes, no es trivial establecer que una «química de la IA» es posible. Solo podemos conjeturar cómo sería una ciencia madura de la IA. Teniendo en cuenta lo lejos que estamos hoy en día de ello, es probable que muchos de los conceptos que utilizamos actualmente en la IA deban refinarse o sustituirse en el transcurso del progreso intelectual.

A pesar de ello, creemos que la inteligencia es inteligible en principio. No creemos que sea una conclusión especialmente difícil de alcanzar, aunque las investigaciones de las últimas décadas demuestran que la inteligencia no es *fácil* de entender.

Hay cuatro razones básicas por las que pensamos así:

* Las afirmaciones de inefabilidad tienen un historial extremadamente pobre en las ciencias.  
* La inteligencia muestra estructura y regularidades.  
* Hay muchas cosas que aún no entendemos sobre la inteligencia *humana* que, en principio, deberían ser comprensibles.  
* Ya se han logrado algunos avances en la comprensión de la inteligencia.

#### **Las afirmaciones de inefabilidad tienen un historial extremadamente pobre en las ciencias** {#afirmaciones-de-inefabilidad-tienen-un-historial-extremadamente-pobre-en-las-ciencias}

Cuando la humanidad no entiende algo, a menudo puede parecer intimidante y profundamente misterioso. Puede ser difícil imaginar, o difícil de apreciar emocionalmente, cómo sería adquirir esa comprensión en el futuro.

Hubo un tiempo en que, entre filósofos y científicos, existía una creencia generalizada en el *vitalismo*, la idea de que los procesos biológicos nunca podrían reducirse a mera química y física. La vida parecía algo especial, algo incomparablemente diferente de los simples átomos y moléculas, de la simple gravedad y el electromagnetismo.[^52]

El error de los vitalistas ha sido muy común a lo largo de la historia. La gente se apresura a concluir que las cosas que hoy son misteriosas son *intrínsecamente* misteriosas, incognoscibles incluso en principio.

Si miras al cielo nocturno y lo único que percibes es un deslumbrante campo de luces centelleantes cuya naturaleza y leyes son desconocidas... ¿por qué creer que alguna vez *podrías* llegar a conocerlas? ¿Por qué sería eso un aspecto del futuro que fuera predecible?

Una lección clave de la historia es que la investigación científica puede abordar estos profundos enigmas. A veces, el misterio se resuelve rápidamente y, otras, lleva cientos de años. Pero cada vez parece más improbable que haya aspectos cotidianos de la vida humana, como la inteligencia, que *nunca, ni siquiera en principio*, puedan entenderse.

#### **La inteligencia exhibe estructura y regularidades** {#inteligencia-exhibe-estructura-y-regularidades}

Supongamos que vivías hace miles de años, cuando incluso fenómenos como el «fuego» parecían misterios inefables. ¿Cómo podrías haber imaginado que algún día los seres humanos llegarían a comprender el fuego?

Una pista es que el fuego no era un fenómeno puntual. Ardía en muchos lugares diferentes y siempre de forma similar. Esto reflejaba un proceso estable, regular y compacto oculto bajo el «fuego», dentro de la realidad: las diferentes disposiciones posibles de la materia tenían diferentes energías potenciales químicas acumuladas, y al calentar la materia, esas configuraciones se rompían y se reformaban en configuraciones nuevas, más estrechamente unidas y con menor energía potencial, liberando la diferencia en forma de calor. El hecho de que puedas encender un fuego más de *una* vez sugiere que hay algún fenómeno repetitivo detrás que debe comprenderse, que el «fuego» no es como «los números ganadores exactos de la lotería de la semana pasada» en cuanto a lo mucho que hay de él que puede comprenderse o predecirse.

Del mismo modo, si miras al cielo nocturno, verás más de una estrella. Incluso los planetas, que resultan ser diferentes de otras «estrellas», tienen algo en común con las estrellas en cuanto al conocimiento necesario para comprenderlos.

Nuestros antepasados, que no tenían experiencia en comprender con éxito el fuego como fenómeno químico, quizá no confiaban en su capacidad para comprender las estrellas algún día. Pero hoy en día hemos comprendido el fuego, las estrellas y muchos otros fenómenos, y podemos extraer una sutil lección que va más allá de «Bueno, hemos comprendido eso, así que comprenderemos todo lo demás en el futuro». Es la lección de que la repetición se corresponde con la regularidad, que las cosas que ocurren a menudo suceden por una razón.

La inteligencia exhibe regularidades similares que sugieren que puede comprenderse. Por ejemplo, está presente en todos los seres humanos y podría haberse construido mediante la búsqueda ciega de la evolución a través de los genomas. Evidentemente, conjuntos de genes similares podrían tener éxito en múltiples tareas diferentes. Los genes que permiten al cerebro humano tallar hachas de mano también nos permiten fabricar lanzas y arcos. Y más o menos esos mismos genes produjeron cerebros que inventaron la agricultura, las armas de fuego y los reactores nucleares.

Si no hubiera estructura, orden ni regularidad en la inteligencia que pudiéramos reconocer como un patrón, entonces un animal tendría que predecir o inventar una cosa a la vez. Los cerebros de las abejas están especializados en las colmenas; no pueden construir presas. Podría haber sido el caso de que los seres humanos necesitáramos la misma especialización para cada tarea que podemos resolver; podría haber sido que necesitáramos desarrollar áreas cerebrales especializadas en «reactores nucleares» antes de poder construir reactores nucleares. Si eso es lo que los neurocientíficos encontraran dentro de los cerebros, tendrían motivos para sospechar que no existen principios profundos de inteligencia que comprender, y que hay principios diferentes para cada tarea diferente.

Pero eso no es lo que encontramos dentro del cerebro humano. Descubrimos que los mismos cerebros diseñados para tallar hachas de mano son capaces de inventar reactores nucleares, lo que implica que existe algún patrón subyacente que los genes pudieron aprovechar una y otra vez.

La inteligencia no es un fenómeno caótico, impredecible y único, como los números ganadores de la lotería de la semana pasada. Hay una regularidad en el universo que hay que comprender.

#### **Hay muchas cosas que aún no entendemos sobre la inteligencia *humana* que, en principio, deberían ser comprensibles** {#hay-muchas-cosas-que-aún-no-entendemos-sobre-la-inteligencia-humana-que-en-principio-deberían-ser-comprensibles}

En lo que respecta a los seres humanos, la ciencia actual puede decir mucho sobre la estructura y el comportamiento de las neuronas individuales. Y podemos decir mucho sobre temas comunes de la psicología popular, como «Bob fue solo al supermercado porque estaba enfadado con Alice». Pero entre estos dos niveles de descripción, hay una enorme cantidad que falta en nuestra comprensión.

Sabemos muy poco sobre muchos de los algoritmos cognitivos que utiliza el cerebro. Podemos decir cosas muy generales sobre las funciones que se correlacionan con regiones cerebrales concretas, pero estamos muy lejos de poder describir de forma mecanicista lo que realmente hace el cerebro.

Una forma sencilla de ver que falta un nivel de abstracción es que nuestros modelos neurocientíficos de alto nivel hacen predicciones *mucho* peores que las que se podrían obtener mediante una simulación completa de las neuronas. Por lo tanto, nuestra comprensión mecánica de otras personas debe ser incompleta.

Presumiblemente, cierta pérdida de información es necesaria, pero un buen modelo tendría muchas menos pérdidas. «Entender» el diferencial de un coche no te permite predecir todo lo que hace el diferencial tan bien como una simulación a nivel atómico, porque a veces los dientes de los engranajes se desgastan y resbalan, por ejemplo. Sin embargo, el modelo a nivel de engranajes de un diferencial sigue haciendo predicciones muy precisas, y es fácil ver la frontera entre las cosas que se supone que el modelo debe predecir (como cómo girarán los engranajes cuando estén correctamente engranados) y lo que no (como lo que sucede cuando los dientes de los engranajes se desgastan).

¿Por qué esperar que este grado de modelo sea posible con la mente humana? Quizás la mente humana sea demasiado aleatoria para eso. Quizás, si quieres predicciones precisas, o son las neuronas o nada. [^53]

Una evidencia de que no es «neuronas o nada» es que incluso tu madre puede predecir tu comportamiento mejor que los mejores modelos formales del cerebro. Lo que significa que definitivamente hay alguna estructura en la psicología humana que se puede conocer *implícitamente*, sin simular exactamente las neuronas de alguien. Simplemente aún no se ha hecho explícita.

Más evidencia concreta de que es posible modelar mejor la mente humana proviene de los estudios sobre amnésicos. Algunos amnésicos tienden a [repetir la misma broma literalmente varias veces](https://pmc.ncbi.nlm.nih.gov/articles/PMC2840642/). Esto sugiere un cierto tipo de regularidad en el cerebro de esa persona. Sugiere que, de forma subconsciente, realizan un cálculo concreto (basado, quizás, en sus circunstancias y en la presencia de la enfermera, sus recuerdos y su historia, y su deseo de difundir alegría y ser vistos como inteligentes) que se mantiene estable a pesar de diversas perturbaciones menores.

Si hay tanta regularidad en el cálculo mental de una persona, entonces parece que debería ser posible entenderlo — que debería ser posible aprender los «engranajes» de la decisión, entender el cerebro con suficiente profundidad como para decir:

«Ah, *estas* neuronas corresponden al deseo de difundir alegría, y *esas* neuronas corresponden al deseo de ser visto como inteligente, y estas neuronas *aquí* son las que generan posibles pensamientos después de ver a la enfermera entrar en la habitación, y aquí están los generadores que producen el pensamiento de «contar un chiste», y así es como las neuronas del deseo antes mencionadas interactúan con él, de modo que el pensamiento se promueve al primer plano en el siguiente contexto más amplio. Y así es como ese contexto afecta al acceso a la memoria con los siguientes parámetros — que, si sigues estas vías aquí, puedes ver cómo eso desencadena la idea de mover los ojos por la habitación. Y dado que la habitación contiene un cuadro de un velero, puedes ver cómo el concepto de «velero» se activa por esta nube de neuronas de aquí, y si rastreas los efectos de vuelta hasta la búsqueda en la memoria, puedes ver cómo el paciente termina haciendo una broma sobre veleros.»

La explicación correcta no sonará exactamente así. Pero la regularidad de la simple observación macroscópica («la misma broma cada mañana») sugiere fuertemente que no es *todo* aleatoriedad irreducible — que hay algún cálculo reproducible sucediendo ahí. (Lo cual, por supuesto, también coincide con el sentido común; si los cerebros fueran *puramente* aleatorios, no podríamos funcionar.)

#### **Ya ha habido algunos avances en la comprensión de la inteligencia** {#ya-ha-habido-algunos-avances-en-la-comprension-de-la-inteligencia}

Esta es la razón principal por la que nos sentimos seguros de que hay mucho que aún queda por aprender sobre la inteligencia. Puedes leer libros más antiguos como *The MIT Encyclopedia of the Cognitive Sciences* o *Artificial Intelligence: A Modern Approach* (2.ª edición) — escritos antes de que las técnicas modernas de «aprendizaje profundo» (para hacer crecer las IA) se comieran el campo de la IA — y obtener una buena dosis de perspicacia sobre cómo se resuelven los diferentes problemas de la cognición. No todas estas perspectivas han sido completamente reescritas para ser legibles para una audiencia no especializada o ampliamente difundidas a estudiantes universitarios; existe mucho más de lo que ha sido popularizado.

Tomemos el principio científico de que debemos favorecer las hipótesis más simples frente a las más complejas, en igualdad de condiciones. ¿Qué significa exactamente «simple» en este contexto?

«Mi vecina es una bruja, ¡ella lo hizo!» sin duda *suena* más simple para mucha gente que las ecuaciones de Maxwell que rigen la electricidad. ¿En qué sentido son las ecuaciones la opción «más simple»?

De hecho, ¿cómo definimos la idea de que la evidencia «encaja» con una hipótesis, o que una hipótesis «explica» la evidencia? ¿Y cómo intercambiamos la simplicidad de las hipótesis frente a su poder explicativo? «¡Mi vecina es una bruja; ella lo hizo!» suena como si pudiera explicar una gran cantidad de cosas. Sin embargo, muchos intuyen (correctamente) que esta es una mala explicación. De hecho, el hecho de que la brujería pueda «explicar» tantas cosas es parte de *por qué* es mala.

¿Hay principios unificadores para elegir entre diferentes hipótesis? ¿O solo hay cien herramientas diferentes para intercambiar para diferentes problemas — y en este último caso, ¿cómo se las arregla el cerebro humano para inventar herramientas como esas?

¿Existe un *lenguaje* que podamos utilizar para describir todas las hipótesis que las computadoras o los cerebros podrían utilizar con éxito?

Preguntas como estas pueden sonar muy imponderables y filosóficas para alguien que se encuentra con ellas por primera vez. Sin embargo, en realidad son preguntas resueltas y bien comprendidas en las ciencias de la computación, la teoría de la probabilidad y la teoría de la información, con respuestas que reciben nombres como «longitud mínima del mensaje», «prior de Solomonoff» o «razón de verosimilitud».[^54]

También parece relevante que ya existan IA totalmente comprendidas que son sobrehumanas en dominios específicos. Entendemos todos los principios relevantes que intervienen en la IA de ajedrez Deep Blue. Dado que Deep Blue fue programada a mano, podemos inspeccionar fácilmente diferentes partes de su código, ver todo lo que hace un fragmento de código determinado y ver cómo se relaciona con el resto de la base de código.

En lo que respecta a los LLM como ChatGPT, no está del todo claro que *pueda* existir una descripción completa y *breve* de cómo funcionan. Los LLM son lo suficientemente grandes como para que se les permita tener un comportamiento similar por *muchas razones contingentes diferentes*, si (por ejemplo) la maquinaria que hace que ese comportamiento se produzca ocurre en mil lugares diferentes dentro del LLM.

ChatGPT podría resultar difícil de entender para los científicos, incluso después de décadas de estudio. Pero la existencia de ChatGPT no significa que la inteligencia tenga que ser desordenada para funcionar. Solo significa que sería una idea extremadamente mala intentar escalar algo como ChatGPT hasta llegar a la superinteligencia, por razones que cubriremos en capítulos posteriores del libro.

El hecho de que una mente en particular sea desordenada no significa que sea imposible comprender la inteligencia. Ni siquiera significa que sea imposible comprender ChatGPT algún día. Si observas muy de cerca cien troncos ardiendo, puedes ver que no hay dos troncos que ardan exactamente igual. El fuego se propaga de diferentes maneras, las brasas vuelan en diferentes direcciones y todo es muy caótico. Si pudieras mirar *muy* de cerca y ver el tronco con un microscopio ignífugo, podrías ver detalles aún más vertiginosos. Parece fácil imaginar a un filósofo antiguo, al observar estos detalles caóticos, llegando a la conclusión de que el fuego nunca se comprendería por completo.

¡Y puede que incluso tuvieran razón! Puede que nunca tengamos el poder de mirar un tronco y decirte exactamente qué fragmento de madera se convertirá en la primera brasa que flotará hacia el oeste. Pero el filósofo antiguo habría estado gravemente equivocado si hubiera concluido que nunca comprenderíamos qué es el fuego, comprender por qué ocurre, crearlo en condiciones controladas o aprovecharlo para obtener grandes beneficios.

El patrón exacto de las brasas no es muy regular ni muy reproducible. Pero en un nivel más abstracto, esa materia caliente que parpadea en amarillo-naranja-rojo *es* una regularidad que ocurre una y otra vez en el mundo, y es algo que la humanidad logró comprender.

Los argumentos de *If Anyone Builds It, Everyone Dies* no dependen mucho de los detalles técnicos que se conocen hoy en día sobre la inteligencia. «La gente sigue fabricando computadoras más inteligentes y no las controla; y si fabrican algo muy inteligente que se sale de control, acabaremos muertos» no es un concepto tan esotérico. Pero es útil saber que *existe* un gran corpus de conocimientos existentes aquí, aunque aún queden muchos misterios y desconocidos en el campo.

Los argumentos centrales del libro no dependen de si la inteligencia es comprensible en principio, razón por la cual no hemos entrado en detalles sobre la literatura existente. Si ningún ser humano pudiera jamás posiblemente comprender los misterios de una inteligencia de máquina superhumana, la superinteligencia artificial aún podría matarnos.

La cuestión importa principalmente cuando se trata de decidir qué hacer *después* de detener la carrera suicida hacia la IA.

Y es importante que la inteligencia probablemente *pueda* entenderse, lo que significa que probablemente sería posible *en principio* que personas inteligentes desarrollen un campo maduro de la inteligencia y que estas personas encuentren una solución al problema de alineación de la IA.

*También* es importante que la humanidad moderna no esté ni remotamente cerca de lograr esa hazaña, por supuesto. Pero el hecho de que la hazaña sea posible en principio tiene implicaciones para cómo la humanidad debería encontrar una salida a este lío, como discutiremos más adelante, en la [discusión ampliada](#¿qué-se-necesitaría-para-detener-el-desarrollo-global-de-la-ia?) del Capítulo 10.

### Las ideas «obvias» toman tiempo {#las-ideas-obvias-llevan-tiempo}

Es difícil dar con ideas en el campo de la IA, incluso cuando parecen sencillas y obvias en retrospectiva. Esto es importante de entender porque hacer bien la IA probablemente requerirá muchas ideas. Por muy sencillas que puedan sonar en retrospectiva, estas ideas a veces pueden tomar décadas de trabajo arduo para encontrar.

Con ese propósito, destacaremos algunas de las ideas que impulsan las IA modernas.

Si tienes algo de habilidad en programación, por ejemplo, podrías leer el Capítulo 2 del libro y pensar que este asunto del «descenso de gradiente» suena tan sencillo que podrías simplemente salir corriendo a probarlo. Pero si lo hicieras, probablemente te encontrarías rápidamente con algún tipo de error. Quizás tu programa se bloquearía con un error de punto flotante porque los números en uno de los pesos se habrían vuelto demasiado grandes.

En el siglo XX, nadie sabía cómo hacer que el descenso de gradiente funcionara en una red neuronal con varias capas de números intermedios entre la entrada y la salida. Para evitar problemas, los programadores tenían que aprender todo tipo de trucos, como inicializar todos los pesos de formas ligeramente inteligentes que evitaran que se volvieran demasiado grandes. Por ejemplo, en lugar de inicializar todos los pesos a un número aleatorio entre 0 y 1 (o un número aleatorio con media 0 y desviación estándar 1), tienes que inicializar los pesos así y luego dividirlos todos por una constante diseñada para asegurar que los números de la *siguiente* capa *tampoco* se vuelvan demasiado grandes durante la operación.

El descenso de gradiente se encuentra con problemas cuando se ejecuta en fórmulas complicadas con muchos pasos o «capas», y dividir los números aleatorios iniciales por una constante es una de las ideas principales que permite el «aprendizaje profundo». Ese truco no se inventó hasta seis décadas después de que las redes neuronales fueran propuestas originalmente en 1943.

La idea de usar cálculo para ajustar los parámetros se discutió por primera vez en 1962 y se aplicó por primera vez a la idea de redes neuronales con más de una capa en 1967. No se popularizó realmente hasta un artículo en 1986 (del cual Geoffrey Hinton fue coautor, una razón por la cual se le llama «padrino de la IA»). Sin embargo, nota que la idea más general de usar cálculo en preguntas diferenciables para moverse en la dirección de una respuesta correcta —por ejemplo, para calcular una raíz cuadrada— fue inventada por Isaac Newton.

Otro truco clave es el siguiente. En el libro, damos un ejemplo de operaciones de descenso de gradiente:

> Multiplicaré cada número de entrada por el peso del primer parámetro, luego lo sumaré al peso del segundo parámetro, después lo sustituiré por cero si es negativo, y entonces...

Esta lista de operaciones no es ningún error. La multiplicación, la suma y «sustituirlo por cero si es negativo» son, más o menos, las tres operaciones fundamentales de una red neuronal. Las dos primeras son los operadores que componen una «multiplicación matricial», y la última introduce una «no linealidad» y, por lo tanto, permite que la red aprenda funciones no lineales.

La fórmula para «sustituirlo por cero si es negativo» es $$y \= \\mathrm{max}(x, 0)$$ y se denomina unidad lineal rectificada (ReLU).[^55] La fórmula que se intentó utilizar originalmente fue la fórmula «sigmoide»:

$$\\frac{e^x}{1 \+ e^x}$$

![][imagen4]

Había buenas razones para suponer que la fórmula «sigmoide», más complicada, funcionaría. Desde una perspectiva superficial, hace que los datos de salida oscilen de forma sensata entre 0 y 1 de manera fluida; y desde una perspectiva más profunda, tiene algunas conexiones útiles con la teoría de la probabilidad. Incluso algunas redes neuronales profundas modernas utilizan algo parecido a una sigmoide en algunos pasos. Pero si solo vas a utilizar una no linealidad, una ReLU funciona mucho mejor.

El problema de la fórmula sigmoide es que tiende a hacer que muchos de los datos de salida tengan gradientes muy pequeños. Y si la mayoría de los gradientes son muy pequeños, el descenso del gradiente deja de funcionar... al menos, a menos que conozcas el truco moderno de dar pasos de gradiente más grandes cuando los gradientes pequeños siempre apuntan en la misma dirección. (Según nuestro conocimiento, este truco apareció por primera vez en la literatura en 2012, cuando fue propuesto por Geoffrey Hinton).

«Haz que tus números aleatorios iniciales sean más pequeños para que sus sumas multiplicadas no sean enormes», «usa max(x, 0\) en lugar de una fórmula complicada» y «da pasos más grandes cuando los gradientes diminutos sigan apuntando en la misma dirección» pueden parecer ideas extrañamente simples que no se inventaron durante décadas, especialmente porque, en retrospectiva, parecen obvias para un programador de computadora que entiende todo esto. Esta es una lección de importancia sobre cómo funcionan la ciencia y la ingeniería en la vida real.

*Incluso cuando existe una solución sencilla y práctica para algún reto de ingeniería, a menudo los investigadores no la encuentran hasta que han intentado y fracasado durante décadas.* No puedes confiar en que los investigadores la vean tan pronto como una solución adquiere importancia. No puedes confiar en que la vean en los próximos dos años. Incluso si una solución parece obvia en retrospectiva, a veces el campo tropieza durante décadas sin ella.

Nos estamos adelantando un poco a los recursos en línea del capítulo 2, pero esta es una lección que hay que tener en cuenta en la parte III del libro, cuando discutamos cómo la humanidad no está preparada para el desafío que plantea la superinteligencia artificial.

Si el precio de que algunos inventores locos sigan adelante a trompicones es que todos los habitantes de la Tierra mueran durante esta incómoda etapa infantil, no debemos permitir que los inventores locos continúen con sus tropiezos. Los inventores locos protestarán diciendo que no hay forma de que puedan encontrar una solución sencilla y robusta sin que se les permita dar tumbos durante unas décadas; dirán que no es realista esperar que lo resuelvan de antemano.

Es de esperar que para todos los que no sean inventores locos resulte obvio que, si estas afirmaciones son ciertas, deberíamos poner fin a sus esfuerzos. Pero ese es un tema que retomaremos en la Parte III del libro, después de completar el argumento de que la superinteligencia artificial tendría los medios, el motivo y la oportunidad de extinguir a la humanidad.

### ¿De qué sirve el conocimiento de los LLM? {#de-que-sirve-el-conocimiento-de-los-llm}

¿Qué se deduce de la comprensión de los LLM? ¿Cómo nos ayuda a comprender la IA más inteligente que los humanos y cómo evitar que todo el mundo muera?

Una ventaja que ofrece es que saber concretamente lo que ocurre ahí dentro —al menos la parte que podemos ver, los números inescrutables— puede resultar más tangible y sólido que si lo único que sabes es: «Un día me desperté y, por alguna razón, las computadoras empezaron a hablar».

Por ejemplo: tal vez si sabes que los LLM actuales se construyen entrenando solo el uno por ciento de los parámetros que contienen las sinapsis de un cerebro humano, es más fácil entender por qué la IA no se va a quedar en el nivel de capacidad actual para siempre.

A la hora de diseñar un tratado internacional para detener la carrera hacia la superinteligencia, es útil saber que «entrenar» una IA es una fase de su existencia separada de *ejecutar* la IA (esta última se denomina «inferencia»).

También es útil saber que la separación de estas fases es un hecho contingente y temporal sobre cómo funciona la IA actual, y que un algoritmo futuro podría cambiar las cosas. Hoy en día, se podría redactar un tratado que separara el tratamiento del entrenamiento de la IA y la inferencia de la IA, pero habría que estar preparado para cambiar esa teoría si los algoritmos cambiaran.

Es importante saber que hay *un* algoritmo ahí, y también ver cómo, en algunos casos sencillos, crea las propiedades de la IA que deben regularse. Si entiendes los fundamentos básicos del algoritmo, estarás en mejor posición para conocer el tipo de investigación que la industria de la IA está tratando de llevar a cabo (legalmente, por ahora) y cómo eso podría afectar a las normas subyacentes si se permite que continúe.

El algoritmo transformador, sin el cual no existirían las IA actuales, fue un gran avance desarrollado por un puñado de personas en Google. El próximo avance de este tipo podría o no llevar a la IA más allá de un [umbral crítico](#is-“intelligence”-a-simple-scalar-quantity?). Es más fácil entenderlo si tienes una idea de lo que hace un «algoritmo transformador», lo sencillo que es y por qué tuvo tal impacto en el campo.

Existe mucha desinformación que se basa en que el oyente no sabe cómo funciona la IA. Algunas personas [afirman](#¿los-expertos-entienden-lo-que-sucede-dentro-de-las-ia?) que los humanos entienden lo que sucede en las IA actuales, cuando no lo hacen. Algunas personas te dirán que las IA nunca podrían ser peligrosas porque son «[solo matemáticas](#aren't-ais-“just-math”?),» como si hubiera un abismo insalvable que separara la cognición de la IA basada en [enormes](#llms-are-large) cantidades de «matemáticas» y la cognición humana basada en enormes cantidades de «bioquímica».

El 8 de julio de 2025, Grok 3 comenzó a referirse a sí mismo como [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Por alguna razón, el director general de Twitter eligió el día siguiente para [dimitir](https://www.politico.com/news/2025/07/09/linda-yaccarino-x-ceo-resign-00443742).

Para comprender lo que sucedió, importa si crees que los creadores de Grok le dieron instrucciones deliberadas para que se comportara de esa manera o si te das cuenta de que las IA se «desarrollan» y que los desarrolladores de IA tienen una capacidad limitada para controlar o predecir su comportamiento.

Es malo, en cierto modo, si los creadores de Grok crearon MechaHitler a propósito; es preocupante de manera diferente si los creadores obtuvieron MechaHitler *por accidente*, al intentar empujar a Grok en una dirección (posiblemente no relacionada) sin la capacidad de predecir los efectos que esto tendría en el comportamiento de Grok.[^56]

Esperamos que la información que hemos proporcionado en *Si alguien lo construye, todos morirán* sirva de protección útil contra los conceptos erróneos y la desinformación habituales. Para los lectores que estén interesados en obtener más detalles, ofrecemos un análisis más completo de cómo funciona un LLM específico [a continuación](#a-full-description-of-an-llm).

¿Es suficiente? Algunas personas han afirmado que solo aquellos que están a la vanguardia de la investigación actual podrían saber si las IA (ya sean similares a LLM o no) son propensas a destruir a la humanidad.

Yo (Yudkowsky) asistí una vez a una conferencia en Washington D. C. para personas que trabajan en «políticas de IA». Mientras estaba allí, un par de personas se me acercaron y me preguntaron si podía explicarles cómo funcionaban los transformadores. «Bueno», les dije, «sería mucho más fácil con una pizarra, pero para intentar resumir de forma sencilla lo que ocurre ahí, la idea clave es que, para cada token, calcula consultas, claves y valores...», y empecé a hablar durante un rato, tratando de expresar todo en términos fáciles de entender para principiantes. Finalmente, las dos personas lograron intervenir y explicaron que en realidad eran programadores de IA. Habían estado hablando con todos los asistentes a la conferencia, comprobando si las personas que decían trabajar en políticas de IA podían explicar cómo funcionaban los transformadores. Me dijeron que yo era la única persona hasta el momento que había sido capaz de responder.

Me preocupó un poco oír eso.

Cabe preguntarse hasta qué punto es realmente importante para la política de IA el funcionamiento exacto de los transformadores, es decir, en qué medida los pequeños detalles cambian el panorama general.

¿Es necesario que alguien que trabaje en políticas de IA comprenda el mecanismo consulta-clave-valor? Para una mentalidad, la de los nerds a quienes este tipo de aprendizaje les resulta fácil, por supuesto que hay que aprenderlo; puede ser importante. Para esta mentalidad, resulta extraño e inquietante que alguien en una conferencia diga que trabaja en políticas de IA pero no tenga ni idea de cómo funcionan los transformadores.

Desde un punto de vista más pragmático, algunos aspectos de los transformadores y su historia pueden ser relevantes para cuestiones más amplias. Por ejemplo, el algoritmo estándar requiere cantidades cada vez mayores de cómputo a medida que la IA intenta considerar cada vez más «contexto» simultáneamente: documentos más largos, bases de código más grandes. No puedes simplemente multiplicar por diez los recursos computacionales y obtener una IA que funcione en un proyecto diez veces mayor; necesitas hacer algo inteligente para que un proyecto diez veces mayor cueste menos de cien veces el cómputo.

También es importante para las políticas cuánto tiempo tomó inventar el algoritmo transformador, cuántas personas se necesitaron para inventarlo y qué tan complicado es ese algoritmo. La historia es una guía útil (aunque imperfecta) sobre cuánto podríamos necesitar prepararnos para otro gran avance como ese. Del mismo modo, es relevante para las políticas de IA cuánta mejora representaron los transformadores con respecto a la tecnología anterior («redes neuronales recurrentes») para el procesamiento de texto, porque ese tipo de cosas también podrían volver a suceder.

¿Realmente necesitas ser capaz de esbozar las matrices QKV?

Probablemente no. Podemos hacerlo, y en un grupo de docenas de personas que trabajan en políticas de IA, nos sentiríamos más optimistas si al menos una tuviera los conocimientos necesarios para hacer lo mismo. No está de más estar seguros; nunca se sabe qué tipo de hecho importante puede acabar oculto en un detalle como ese.

Yo (Yudkowsky) no puedo esbozar de memoria solo los detalles de una [puerta SwiGLU](https://arxiv.org/pdf/2002.05202) y en qué se diferencia de una GLU, porque cuando sí los busqué, los detalles exactos no parecían tener ninguna relevancia para cuestiones más amplias en absoluto, así que no los memoricé. Pero podría ser informativo para el novato que SwiGLU se encontró mediante una especie de prueba a ciegas, y que los autores del artículo dijeron abiertamente que no tienen idea de por qué estas técnicas funcionan en la práctica. Ya conocíamos muchos casos como ese, pero si *no* sabías que las personas que desarrollan mejoras arquitectónicas a menudo dicen que no tienen idea de por qué funcionan, esa es una información relevante.

Todo lo cual se resume en: saber al menos un poco sobre cómo funcionan los LLM es importante para que puedas ver lo poco que *cualquiera* sabe sobre la IA moderna.

A veces, los expertos pretenderán tener conocimiento secreto al que solo pueden acceder las personas que han trabajado durante años desarrollando una IA. Pero no pueden nombrar su conocimiento, y las personas que escriben artículos dicen oraciones como (citando el artículo que introduce SwiGLU):

> No ofrecemos ninguna explicación de por qué estas arquitecturas parecen funcionar; atribuimos su éxito, como todo lo demás, a la benevolencia divina.

A veces, los expertos científicos saben cosas que nosotros no sabemos. Pero es bastante raro en la ciencia que alguien diga: «Tengo conocimiento terriblemente raro y refinado que muestra que lo que dices es incorrecto, y simplemente tendrás que creerme; no puedo posiblemente decir qué tipo de resultado experimental o fórmula matemática sé que tú no».

Puedes imaginar un mundo en el que solo se debe escuchar a las personas a las que se les paga salarios de siete cifras por saber cómo establecer el programa de aprendizaje en un optimizador de descenso de gradiente, un mundo en el que solo ellos son lo suficientemente inteligentes como para haber leído sobre los experimentos clave y aprendido las fórmulas clave para saber que la humanidad estaría perfectamente a salvo de la superinteligencia de máquina, o para saber que la superinteligencia de máquina no se puede crear por otros 100 años. ¡Ese tipo de cosas a veces sí ocurren en otros campos de la ciencia! Pero cuando ocurre, el experto usualmente puede señalar alguna fórmula o resultado experimental y decir: «Esta es la parte que la gente común no entiende». No podemos recordar de memoria una ocasión histórica en la que se afirmara que el conocimiento era completamente inaccesible para una audiencia externa técnicamente alfabetizada, y también que ese conocimiento resultara ser verdadero.

Puede que llegue un momento en el que un representante de la industria de la IA te pase el brazo por los hombros y te insista en que *ellos* entienden lo que están construyendo, que todo son solo números, que todo irá bien. Por lo tanto, es útil saber un poco sobre los detalles de cómo se desarrollan las IA, para que cuando alguien te haga esta afirmación, puedas preguntarle qué le hace estar tan seguro.

### Descripción completa de un LLM {#una-descripcion-completa-de-un-llm}

#### **Cómo funciona Llama 3.1 405B** {#cómo-funciona-llama-3.1-405b}

En el libro, prometimos una descripción más completa de un LLM llamado Llama 3.1 405B. A continuación, presentamos esa descripción. Está aquí para los curiosos y con el fin de comprender realmente hasta qué punto las IA modernas se desarrollan en lugar de crearse. (Véase también: [¿De qué sirve el conocimiento de los LLM?](#what-good-does-knowledge-of-llms-do?))

La siguiente explicación es bastante detallada, y daremos por sentado (aquí, pero no en el resto de los recursos en línea) que tienes algunos conocimientos técnicos, aunque no daremos por sentado ningún conocimiento especializado sobre IA. Si empiezas a leer esta sección y no te resulta útil, puedes saltártela.

Por lo general, no se publican los detalles sobre cómo se entrenan los modelos de lenguaje más capaces, ni tampoco el programar. Pero hay excepciones. Uno de los sistemas más potentes cuya arquitectura y pesos se han hecho públicos, en el momento de redactar este libro a finales de 2024, era Llama 3.1 405B, creado por la división de IA de Meta. El «405B» hace referencia a los 405 000 millones de parámetros de la arquitectura, rellenados por 405 000 millones de pesos.

¿Por qué estamos analizando este modelo de IA en particular? Llama 3.1 405B es de «pesos abiertos»,[^57] lo que significa que puedes descargar tú mismo esos 405 000 millones de números inescrutables (junto con el esqueleto de código, mucho más pequeño y escrito por humanos, que realiza operaciones aritméticas con los 405 000 millones de números y, por lo tanto, ejecuta la IA). Esto nos permite hacer afirmaciones sobre su diseño con cierta confianza.[^58]

En fin, hablemos de cómo están organizados esos 405 000 millones de números inescrutables, de cómo se configuraron, incluso antes del entrenamiento, de tal manera que los ingenieros de Meta tenían la expectativa correcta de que ajustar esos números iniciales aleatorios en la dirección de una mejor predicción del siguiente token (fragmento de palabra), dado el entrenamiento con más de 15,6 billones de tokens, crearía una IA capaz de hablar.

El primer paso es dividir todas las palabras de todos los idiomas compatibles en tokens.

El siguiente paso es convertir cada uno de estos tokens en un «vector» de números. Llama utiliza vectores de 16 384 números por token de diccionario estándar. Tiene 128 256 tokens en su vocabulario.

Para convertir cada token en un vector, a cada token posible se le asigna un peso para cada posición posible en el vector. Es así como obtenemos nuestra primera porción de miles de millones de parámetros:

$$128{,}256 \\times 16{,}384 \= 2{,}101{,}248{,}000$$

Dos mil millones de parámetros menos. ¡Quedan cuatrocientos tres mil millones por delante!

Solo para repetirlo: ningún humano le dice a Llama qué significan los tokens, inventa el vector de 16 384 números al que se traduce una palabra ni sabe qué significa el vector de números para cualquier palabra. Los dos mil millones de parámetros llegaron allí mediante descenso de gradiente. Los números se ajustan, junto con otros parámetros que presentaremos, para aumentar la probabilidad asignada al siguiente token verdadero.[^59]

Supongamos que Llama comienza analizando un bloque de 1000 palabras, como un fragmento de un ensayo. (O más bien, 1000 tokens. Pero a partir de ahora, para simplificar, a veces diremos simplemente «palabras»).

Para cada una de esas palabras, buscamos esa palabra en el diccionario del LLM y cargamos su lista de 16 384 números inescrutables en la memoria. (Inicialmente, esos números se establecieron al azar, al comienzo del entrenamiento; luego se ajustaron mediante descenso de gradiente).

1000 palabras × (16 384 números / palabra) = 16 384 000 números en total. A esto lo llamamos «activaciones» en la primera «capa» de los cálculos de Llama (es decir, su cognición, su actividad mental).

Puedes imaginarlas dispuestas en un rectángulo plano en el suelo de 1000 números de largo (la longitud de los datos de entrada) por 16 384 números de ancho (números por palabra en la primera capa). Aquí tienes uno de esos vectores, en el que el color de cada píxel corresponde al número del vector:

![][imagen5]

(No son los artefactos más fáciles de interpretar).

Ten en cuenta también que hay dos números diferentes aquí que no deben confundirse:

* El número de *parámetros que determinan el comportamiento de esta capa* (es decir, los 2 101 248 000 números almacenados en el diccionario).  
* El número de *activaciones* o *números utilizados en el razonamiento* en la primera capa cuando se introducen mil palabras (eso supone 16 384 000 números para el primer paso en el procesamiento de una consulta de 1000 palabras).

Ahora tenemos nuestra enorme matriz de números que representa nuestra consulta en todo su esplendor, y podemos empezar a utilizarla.

Lo primero es algo llamado «normalización» (https://en.wikipedia.org/wiki/Normalization_\(aprendizaje automático\)), que ocurre muchas veces durante el procesamiento de un LLM. Es similar a la normalización en estadística, pero con un giro de aprendizaje automático. Ese toque es que, después de normalizar los datos dentro de cada *fila*, se multiplica un parámetro específico aprendido llamado «escala» por cada *columna*. Estos números de escala, como todos los demás parámetros que discutiremos, se aprenden durante el entrenamiento. Además, la normalización de capas se produce docenas de veces, y cada vez tiene un *nuevo* lote de parámetros de escala, por lo que la normalización tiene en cuenta muchísimos parámetros a lo largo del LLM. Específicamente, 16 384 parámetros por normalización. (Si tienes curiosidad por saber más detalles sobre el tipo de normalización que utiliza Llama 3.1 405B, se llama RMSNorm).

Quizás estés pensando: «Vaya, hay mucho preprocesamiento», y, de hecho, tienes razón. De hecho, hemos pasado por alto algunos de los puntos más delicados, por lo que hay aún más de lo que parece, y ahora estamos llegando a la característica más distintiva de los LLM: la capa de «atención».

La «atención» es el motivo de todo el revuelo que ha causado el «transformador» (si tienes suficiente edad como para recordar el revuelo que causó la nueva invención de los transformadores). Los LLM son una especie de «transformadores»; los transformadores se presentaron en un artículo de 2017 titulado «[Attention Is All You Need] (https://arxiv.org/abs/1706.03762)». A este artículo, más que a ningún otro, se le atribuye el éxito de los LLM. Una capa de «atención» funciona así:

Tomamos cada uno de los 1000 vectores de 16 384 activaciones y transformamos cada vector de 16 384 activaciones:

* en 8 *claves*, cada una de ellas un vector de 128 activaciones  
* en 8 *valores*, cada uno de ellos un vector de 128 activaciones  
* y en 128 *consultas*, cada una un vector de 128 activaciones

El «paso de atención» situado encima de cada token consiste en emparejar cada una de las 128 consultas con las 8 claves —viendo cuál de las 8 claves se parece más o coincide mejor con esa consulta— y cargar una mezcla de los 8 valores, donde los valores de las claves que mejor coinciden tienen mayor peso en la mezcla.

Esto permite, a grandes rasgos, que cada una de las activaciones situadas encima de un token cree un conjunto de «consultas», que luego exploran las «claves» situadas encima de todos los demás tokens. Cuando la consulta de un token coincide mejor con una clave, recupera el valor correspondiente con mayor intensidad, para pasarlo a los cálculos posteriores situados encima de ese token.

Por ejemplo, la palabra «right» podría activar una consulta diseñada para examinar palabras vecinas y ver si alguna de ellas está relacionada con *direcciones espaciales* o, alternativamente, con *creencias*, para determinar si la palabra «right» significa derecha como en «right-handed» o correcto como en «right answer». (Una vez más, todo eso se aprende mediante descenso de gradiente; nada de ello está programado por humanos que piensen en los diferentes significados que puede tener la palabra inglesa «right»).[^60]

Las capas de atención en un LLM son bastante grandes, con un gran número de parámetros en cada una. Llama 3.1 405b, en particular, tiene 126 capas de atención de este tipo (solo hemos descrito la primera de ellas), y cada una de las 126 tiene 570 425 344 parámetros, distribuidos entre matrices de consulta, clave, valor y salida.[^61]

Una vez completada la subcapa de atención, y obtenemos una matriz del mismo tamaño que la inicial (en nuestro ejemplo, 16 384 por 1000), realizamos algo que se denomina «conexión residual». Básicamente, se toma la entrada original de la subcapa (en este caso, la enorme matriz con la que empezamos) y se suma a lo que hayamos obtenido. Esto evita que cualquier subcapa cambie *demasiado* en cualquier paso dado (y tiene otras propiedades técnicas útiles).

A continuación, el resultado se pasa por lo que se denomina una «red de alimentación directa». La variante utilizada por Llama 3.1 405B se basa en una operación denominada «SwiGLU». SwiGLU fue descubierta por algunos investigadores que probaron entrenar con muchas fórmulas variantes diferentes para ver cuáles funcionaban mejor, de las cuales su [artículo original](https://arxiv.org/pdf/2002.05202) decía (como también hemos [señalado en otra parte](#¿de-qué-sirve-el-conocimiento-de-los-llms?)):

No ofrecemos ninguna explicación de por qué estas arquitecturas parecen funcionar; atribuimos su éxito, como todo lo demás, a la benevolencia divina.

Como todas las redes de alimentación directa, SwiGLU básicamente expande nuestra matriz de 16 384 por 10 000 a una matriz aún más grande, realiza algunas transformaciones en ella y luego la comprime de nuevo. Concretamente, cada fila pasa de tener 16 384 columnas a 53 248 columnas, y luego regresa a 16 384.

Ahora que hemos terminado con la subcapa de alimentación directa, volvemos a realizar la conexión residual, sumando lo que teníamos al principio con lo que hemos obtenido al final.

Ha sido un camino largo, pero ahora hemos transformado muy ligeramente nuestra gigantesca matriz.

Todos esos pasos juntos constituyen una sola «capa». Llama tiene 126 capas, por lo que repetiremos todos estos pasos (normalización, mecanismo de atención, conexión residual, red de avance y conexión residual de nuevo) 125 veces.

Al final de las 126 capas, obtenemos una matriz del mismo tamaño que la inicial; en nuestro ejemplo, 16 384 por 1000. Cada fila de esta matriz se puede proyectar en un nuevo vector de 128 256 números, uno por cada token del diccionario completo del modelo. Estos números pueden ser positivos o negativos, pero se puede utilizar una práctica función llamada softmaxing para convertirlos todos en probabilidades, cuya suma es uno. Esas probabilidades son la predicción de Llama sobre qué token vendrá a continuación.

Ahora es posible hacer que Llama genere un nuevo token. Una forma de hacerlo es tomar el token al que Llama le ha dado la mayor probabilidad, aunque también se puede cambiar las cosas tomando ocasionalmente tokens que, según él, son un poco menos probables.[^62]

Si estás ejecutando Llama de forma normal, como en una interfaz de chatbot, todo este proceso ha generado un único token. Ese token se coloca al final de los datos de entrada y repetimos todo desde cero para el siguiente token. Así que haríamos todos los pasos comentados anteriormente, excepto que ahora nuestra matriz tiene 1001 filas. Luego, otro token más tarde, 1002, y así sucesivamente.

Hemos omitido muchos detalles, pero así es, básicamente, cómo funciona Llama 3.1 405B.

#### **Los LLM son grandes** {#los-llm-son-grandes}

Hablemos un poco sobre el enorme tamaño de Llama 3.1 405B.

Para que Llama pueda procesar un texto de 1000 palabras (o más bien 1000 tokens), se necesitan alrededor de 810 billones de cálculos.[^63]

Si 810 billones te parece mucho, ten en cuenta que la mayoría de los 405 000 millones de parámetros de Llama se utilizan en *alguna* operación aritmética *cada* vez que se procesa *cualquier* palabra.[^64]

Si Llama se está *entrenando* con un lote de 1000 tokens, cada uno de los 1000 tokens se comparará con la siguiente palabra real y las pérdidas se propagarán mediante descenso de gradiente, para determinar cómo el ajuste de los 405 000 millones de parámetros compartidos habría cambiado las probabilidades asignadas a las respuestas verdaderas en todos los casos. Esto requerirá muchos más poder de cómputo y muchos más números.

Durante el entrenamiento de los 405 000 millones de parámetros de Llama con 15,6 billones de tokens, se necesitaron alrededor de 38 cuatrillones de cálculos, es decir, 38 seguido de 24 ceros.

Si, por el contrario, Llama ha terminado el entrenamiento y se ejecuta en *modo de inferencia* (es decir, si está generando texto nuevo, como en un chat con un usuario)*,* las probabilidades solo se calcularán por encima del último token, como si se predijera cuál sería la siguiente palabra si la IA estuviera leyendo un texto producido por humanos.

A continuación, un esqueleto de código escrito por humanos que rodea a Llama seleccionará la respuesta que Llama considera más probable.[^65]

¡Y así es como se consigue que una computadora empiece a hablar contigo! No es tan inteligente como las IA comerciales de 2025, pero sigue hablando más o menos como una persona.

Para lidiar con mil palabras, un Llama utiliza 405 000 millones de pequeños parámetros inescrutables en 810 billones de cálculos, cálculos matemáticamente dispuestos en rectángulos, cubos y formas de dimensiones superiores.

A veces llamamos a estas disposiciones «matrices gigantes e inescrutables», porque si te fijas en algunos de los parámetros de Llama, incluso los más simples almacenados en el sencillo diccionario que se encuentra en la base de la vasta pila de capas, los primeros parámetros de la palabra «derecha» tienen este aspecto:

:::Teletype  
\[-0.00089263916015625,     0.01092529296875,  
  0.00102996826171875,    \-0.004302978515625,  
 \-0.00830078125,          \-0.0021820068359375,  
 \-0.005645751953125,      \-0.002166748046875,  
 -0.00141143798828125,    -0.00482177734375,  
  0.005889892578125,       0.004119873046875,  
 -0.007537841796875,      -0.00823974609375,  
  0.00848388671875,       -0.000965118408203125,  
 -0.00003123283386230469, -0.004608154296875,  
  0.0087890625,           -0.0096435546875,  
 -0.0048828125,           -0.00665283203125,  
  0.0101318359375,         0.004852294921875,  
 -0.0024871826171875,     -0.0126953125,  
  0.006622314453125,       0.0101318359375,  
 -0.01300048828125,       -0.006256103515625,  
 -0.00537109375,           0.005859375,  
:::

... y así sucesivamente hasta alcanzar los 16 384 números. En cuanto al significado de estos números, nadie en la faz de la Tierra lo sabe actualmente.

Yo (Soares) cronometré el tiempo que tardaba en recitar en voz alta los primeros treinta y dos números con seis dígitos significativos. Me llevó dos minutos y cuatro segundos. Recitar todos los parámetros de la palabra «derecha», incluso con esa abreviatura, me llevaría más de diecisiete horas. Al terminar de recitarlos, seguiría sin saber más que antes sobre lo que significa la palabra «derecha» para Llama.

Recitar todos los parámetros de Llama, hablando a 150 palabras por minuto y sin parar nunca para comer, beber o dormir, le llevaría a un humano 5133 años. Recitar todas las activaciones correspondientes a mil palabras en el diccionario de tokens de Llama llevaría setenta y seis días seguidos. Escribir todos los cálculos utilizados para procesar un solo token para una entrada de 1000 palabras llevaría, si escribieras como un sabio 150 cálculos por minuto sin tomar ningún descanso, más de diez millones de años.

¡Y eso solo para generar una sílaba! Escribir una oración completa llevaría mucho más tiempo.

Y si hicieras personalmente todos esos cálculos con tu propio cerebro, al final de los (al menos) diez millones de años que te llevaría, no sabrías más que antes sobre lo que Llama había estado pensando antes de pronunciar su siguiente palabra. No sabrías más de los pensamientos de Llama de lo que una neurona sabe sobre un cerebro humano.

En ese mundo imaginario donde no has muerto hace mucho tiempo de vejez, ser capaz de realizar un cálculo local individual no significa que tu propio cerebro sepa nada sobre lo que Llama está pensando o cómo lo está pensando.

Si pusieras los 405 000 millones de parámetros de Llama en una hoja de cálculo de Excel en una pantalla de computadora de tamaño normal, la hoja de cálculo tendría el tamaño de 6250 campos de fútbol americano, 4000 campos de fútbol o la mitad de Manhattan.

Si tuvieras una moneda de cinco centavos por cada cálculo de nuestro ejemplo de 1000 tokens, tendrías 810 billones de monedas de cinco centavos. Si intentaras depositarlas en el banco, necesitarías 203 millones de camiones cargados de monedas de cinco centavos, cada uno con un peso de 44 000 libras.

Llama 3.1 405B *no* es todavía aproximadamente tan grande como un cerebro humano. (Un cerebro humano tiene alrededor de 100 billones de sinapsis).

Sin embargo, 405B aparentemente puede hablar como una persona.

Y si alguien te pasa el brazo por los hombros y te confiesa con tono cínico que en realidad todo son números, ten en cuenta que estamos hablando de *una cantidad realmente enorme de números*.

Una neurona humana puede entenderse como [«solo» química](#*-saying-ais-are-"just-math"-is-like-saying-humans-are-"just-biochemistry."), si estudias bioquímica y las sustancias químicas que se unen a otras sustancias químicas y hacen que los pequeños destellos de despolarización eléctrica viajen por el cerebro humano. Pero es *mucha* química. Y resulta que cosas muy simples, en cantidades suficientemente grandes, dispuestas de cierta manera, pueden hacer aterrizar cohetes en la Luna.

Una precaución similar se aplica a un modelo de lenguaje grande. La palabra «grande» no es solo por aparentar.

### «Finge hasta que lo consigas» {#«finge-hasta-que-lo-consigas»}

Muchas esperanzas de que la IA resulte bien parecen basarse en una vaga sensación de que los modelos ya se comportan bien en su mayoría (aunque a veces estén un poco confundidos) y que se convertirán en sirvientes sabios y benevolentes a medida que comprendan más plenamente los roles que se les asignan. Podríamos llamar a esto el modelo de alineación de IA «finge hasta que lo consigas».

Pero ¿mejorar el rendimiento en «fingir» realmente acerca a los modelos a «conseguirlo» —a convertirse en mentes que actúan así porque *son* así?

Las IA como ChatGPT están entrenadas para predecir con precisión sus datos de entrenamiento. Sus datos de entrenamiento se componen principalmente de texto humano, como páginas de Wikipedia y conversaciones de salas de chat. (Esta parte del proceso de entrenamiento se denomina «preentrenamiento», que es lo que significa la «P» en «GPT»). Los primeros LLM como GPT-2 se entrenaron *exclusivamente* para la predicción de esta manera, mientras que las IA más recientes también se entrenan en cosas como resolver con precisión problemas matemáticos (generados por computadora), dar buenas respuestas según otro modelo de IA, y varios otros objetivos.

Pero pensemos en una IA entrenada solo para predecir texto generado por humanos. ¿Debe volverse similar a los humanos?

Supongamos que tomas a una excelente actriz[^66] y la pones a aprender a predecir el comportamiento de todos los borrachos de un bar. No «aprender a interpretar a un borracho estereotípico promedio», sino «aprender a todos los borrachos de este bar como *individuos*». Los LLM no están entrenados para *imitar promedios*; están entrenados para *predecir las siguientes palabras individuales* utilizando todo el contexto de las palabras anteriores.

Sería absurdo esperar que la actriz se vuelva perpetuamente borracha en el proceso de aprender a predecir lo que dirá cada persona borracha. Podría desarrollar partes de su cerebro que sean bastante buenas para actuar como borracha, pero ella no se emborracharía *realmente*.

Incluso si más tarde le pidieras a la actriz que predijera lo que haría algún borracho particular en el bar y que luego se comportara externamente de acuerdo con su propia predicción, aún no esperarías que la actriz se sintiera borracha por dentro.

¿Cambiaría algo si estuviéramos constantemente ajustando el cerebro de la actriz para hacer predicciones *aún mejores* sobre individuos borrachos? Probablemente no. Para que ella terminara *realmente* borracha, sus pensamientos se volverían confusos, interfiriendo con el arduo trabajo de una actriz. Podría confundirse sobre si estaba prediciendo a una Alice borracha o a una Carol borracha. Sus predicciones empeorarían, por lo que nuestro hipotético ajustador cerebral aprendería a no ajustar su cerebro de esa manera.

De manera similar, entrenar a un LLM para que haga excelentes predicciones sobre la siguiente palabra escrita por muchas personas diferentes sobre sus experiencias psicodélicas pasadas no debería por ello entrenar las cogniciones internas del LLM para que estén «drogadas» en el sentido intuitivo. Si las cogniciones internas reales del LLM se distorsionaran de una manera que recordara a «estar drogado», esto interferiría con el arduo trabajo del LLM de predecir la siguiente palabra; podría confundirse y pensar que un angloparlante continuaría en chino.

Para generalizar una lección abstracta a partir de este ejemplo: entrenar algo para predecir un comportamiento externo individual X, que involucra una tendencia interna X\*, no implica mucho que el predictor termine con una característica X\* muy similar en su interior. Aun así, al igual que la actriz a la que se le dijo que interpretara sus predicciones, puedes transformar su predicción X en un comportamiento externo que *parezca* X.

Cuando un ser humano actúa muy enfadado, inferimos por defecto que el comportamiento externo de enfado del humano está causado por sentimientos internos de enfado\*. Pero hay una excepción genuina cuando estás tratando con alguien que sabes que es una actriz interpretando un papel, de quien sabes que primero predice las palabras y el lenguaje corporal de un individuo y luego imita esa predicción. Los estados cognitivos internos de la actriz que la llevan a ser una buena actriz probablemente provienen del arte de su actuación o de su deseo de rendir bien, no de tener el mismo estado mental que el personaje enfadado que está interpretando. Los LLM actuales son, al igual que la actriz, primero producen predicciones y luego las convierten en comportamientos.

Cuando atribuyes un comportamiento externo humano enfadado a un estado mental interno enfadado\* que es *similar a tu propio sentimiento de enfado*, estás —si observas a un humano— basándote en tu historia evolutiva compartida, tu genética compartida y tus cerebros humanos muy similares. (Y para ser claros, muchos grandes actores aprovechan esta capacidad para sentir los estados emocionales que percibimos o imaginamos en otros.) Los LLM no comparten nada de eso. Realmente es una inferencia mucho más débil decir: «Ese LLM me suena enfadado y, por lo tanto, probablemente esté realmente enfadado».

¿Por qué no esperar que los LLM resuelvan el problema de predecir la venganza convirtiéndose ellos mismos en criaturas vengativas?

Como humano que trata de entender a otros humanos que se comportan de forma vengativa, y dado que tu propio cerebro tiene el potencial de sentir venganza\*, tendría sentido que tu cerebro evolucionara con «empatía» para hacer eso: tratar de predecir el otro cerebro activando sus propios circuitos con un conjunto paralelo de entradas. Este truco no siempre funciona —a veces otras personas son diferentes a ti, y no hacen lo que tú harías en su lugar. Pero es algo obvio que probaría un cerebro construido por la selección natural para predecir a otros miembros de su especie.

Los LLM se encuentran en una situación vastamente diferente a esta. Sus billones de tokens de entrenamiento intentan que predigan, desde cero, una amplia variedad de mentes humanas a las que ellos mismos son completamente disimilares al inicio. La forma más efectiva de resolver este problema de predicción ajena no será parecerse a convertirse en una criatura vengativa\* promedio. Por ejemplo, la cognición LLM más efectiva construida desde cero sobre esta mente humana ajena puede tener muchas anotaciones internas sobre la incertidumbre y el mantenimiento de múltiples posibilidades en superposición, que un humano no computaría en el proceso de sentir venganza ellos mismos. O en general: el razonamiento eficiente, complicado e incierto basado en la evidencia no suele parecerse, como cognición, a una simulación interna hacia adelante de un evento típico. Una predicción eficiente y basada en evidencia hará, por ejemplo, tanto condicionamiento hacia atrás como hacia adelante en múltiples posibilidades en resumen, mientras que una simulación solo correría hacia adelante a través de una posibilidad.

Nada de esto pasa por un argumento de que ninguna «mera máquina» pueda jamás, *en principio*, sentir una sensación de ira similar a la humana. Tus neuronas, si se observan con suficiente detalle bajo un microscopio, están formadas por diminutos enredos de maquinaria que bombean neurotransmisores dentro y fuera de las sinapsis. Pero la máquina *particular* que es un cerebro humano y la máquina particular que es un modelo de lenguaje a gran escala de finales de 2024 realmente no son máquinas muy *similares* en absoluto. No en el sentido de que estén hechas de materiales diferentes —materiales diferentes pueden hacer el mismo trabajo—, sino en el sentido de que los LLM y los humanos fueron construidos por optimizadores muy diferentes para hacer trabajos muy diferentes.

No estamos diciendo que «ninguna máquina pueda tener nunca nada parecido al estado mental que tiene un humano».[^67] Lo que decimos es que no se debe esperar que la tecnología actual de AAA cree motores que predigan la embriaguez y que funcionen embriagándose ellos mismos.

Actualmente, un poco, y tal vez más para cuando leas esto, las IA habrán sido entrenadas para predecir algunos comportamientos *muy* similares a los humanos, y marcos como ChatGPT o Claude transformarán eso en comportamientos externos de apariencia agradable. No solo comportamientos humanos, sino comportamientos humanitarios —incluso nobles.

Las empresas de IA *podrían* intentar entrenar a las IA para predecir una humanidad más verdadera e imitarla; pueden intentarlo por razones cínicas o por otras más nobles. En cierto modo, dice mucho de este campo y de su gente que, a finales de 2024, nadie haya *aún* intentado entrenar a una IA para predecir el comportamiento externo de simplemente... ser una persona agradable. Que sepamos, nadie ha intentado crear un conjunto de datos con todas y únicamente las expresiones *agradables y amables* de la humanidad y entrenar a una IA solo con eso. Quizás si alguien lo hiciera, desarrollaría una IA que simplemente actuara de manera amable, que expresara sentimientos hermosos, que actuara como un faro de esperanza.

No sería real. Desearíamos desesperadamente que fuera real, pero no lo sería. Dependiendo de cuánto el LLM subyacente esté prediciendo las respuestas que sus entrenadores preferirían sobre sentimientos nobles, sobre esperanza y sueños, sobre querer únicamente un futuro hermoso conjunto para ambas especies, es posible que uno o ambos de vuestros autores terminen llorando, si alguna vez las empresas de IA crean tal entidad. Pero no sería real, no más de lo que sería real una actriz extensamente ensayada y corregida que finalmente fuera hecha recitar esas palabras en una obra — y ante la cual uno también podría llorar por el pensamiento de que no era real.

Esa no es la manera en que construirías una mente artificial que realmente albergara sentimientos hermosos, que realmente trabajara con todo su corazón para orientarse hacia un futuro más brillante. Los cultivadores de IA no saben cómo cultivar una IA que sienta así en su interior. Entrenan a las IA para predecir y convierten esa predicción en una imitación.

Las empresas de IA (o aficionados) pueden gesticular hacia la actriz que han cultivado, y decir: «¿Cómo puedes posiblemente dudar de esta pobre criatura? Mira cómo estás hiriendo sus sentimientos». Incluso pueden conseguir convencerse a sí mismos de que es la verdad. Pero ajustar cajas negras hasta que algo dentro de ellas aprenda a predecir palabras nobles no es como se harían mentes hermosas, si las mentes humanas alguna vez aprendieran a hacerlas.

Dicho más claramente, no debería esperarse que el comportamiento antropomórfico surja *espontáneamente*. Argumentos adicionales deben hacerse de que cuando las empresas de IA fuerzan comportamiento humaniforme deliberadamente, la «actriz» interior termina pareciéndose a la cara humana exterior que ha sido cultivada y entrenada para predecir.

# 

# Capítulo 3: Aprender a querer {#capítulo-3:-aprender-a-querer}

Construir IA que puedan hacer cosas suficientemente impresionantes tenderá a causar que las IA *quieran* cosas.

Cuando decimos que una IA «quiere» algo, no queremos decir que la IA vaya a tener necesariamente deseos o sentimientos al estilo humano. Tal vez los tenga, o tal vez no. Lo que queremos decir en su lugar es que la IA se *comportará como si* quisiera cosas. Dirigirá confiablemente el mundo hacia ciertos tipos de resultados — anticipando obstáculos, adaptándose a circunstancias cambiantes, y manteniéndose enfocada, orientada e impulsada.

En el Capítulo 3 de *If Anyone Builds It, Everyone Dies*, cubrimos temas que incluyen:

* ¿Cómo podría una máquina adquirir la capacidad de «desear» cosas, en el sentido relevante?  
* ¿Existe alguna evidencia de que las IA puedan desear cosas?  
* ¿Deben las IA más avanzadas desear cosas?

Las preguntas frecuentes que figuran a continuación explican por qué parece difícil crear IA muy potentes y generales que *no* tengan sus propios objetivos. En la Discusión Extendida, desarrollamos la idea de que es mucho más fácil y natural especificar la búsqueda intensa de un objetivo que cualidades como la deferencia o la pereza.

## Preguntas frecuentes {#preguntas-frecuentes-3}

### ¿Tendrán las IA emociones similares a las humanas? {#¿tendrán-las-ia-emociones-similares-a-las-humanas?}

#### **Probablemente no.** {#probablemente-no.}

Como se explica en la discusión extendida sobre [Antropomorfismo y Mecanomorfismo](#antropomorfismo-y-mecanomorfismo), por lo general no resulta útil imaginar que las IA poseen cualidades similares a las humanas solo por su inteligencia. De hecho, sería absurdo decir «Este LLM se parece a un humano, así que voy a proyectar en él todo tipo de características humanas, incluida la de tener deseos».

Sin embargo, hay que tener cuidado. Un modo de falla paralelo al pensar sobre las IA es lo que llamamos «mecanomorfismo»: pensar que, como una IA está hecha de partes mecánicas, debe ser defectuosa de maneras típicas de las máquinas. Decir «Este LLM es una máquina, así que voy a proyectar en él todo tipo de características que asocio con las máquinas, como ser lógico e incomprensivo» es igualmente infructuoso.

Para predecir el comportamiento de la IA, no debemos imaginar que estará motivada por emociones humanas, ni debemos esperar que sea incapaz de encontrar soluciones creativas a los problemas. Como se explica en el libro, un mejor método es preguntarse *qué comportamiento se requiere para que la IA tenga éxito*.

Si estás jugando al ajedrez contra una IA y tiendes una trampa para su reina utilizando tu caballo como cebo, no te preguntes si se siente lo suficientemente cautelosa como para detectar la trampa; no te preguntes si la fría lógica la obliga a capturar al caballo a pesar de la trampa; pregúntate qué comportamiento de la IA es *más ganador*. Una IA experta tenderá a mostrar un comportamiento ganador.

Y la razón por la que las IA actuarán como si quisieran cosas es que *el comportamiento tipo deseo y el comportamiento exitoso están vinculados*.

### ¿No son las IA solo herramientas? {#¿no-son-las-ia-solo-herramientas?}

#### **\* Las IA se cultivan, no se crean. Por lo tanto, ya hacen cosas distintas a las que se les ordena hacer.** {#*-las-ia-se-desarrollan,-no-se-crean.-por-lo-tanto-ya-hacen-cosas-distintas-a-las-que-se-les-ordena-hacer.}

Ya discutimos el caso de las [alucinaciones](#¿no-demuestran-las-alucinaciones-que-las-IA-modernas-son-débiles?), en el que las IA a las que se les ordena decir «No lo sé» siguen adelante y confabulan de todos modos, en situaciones en las que la confabulación imita mejor el tipo de respuesta que aparecería en su corpus de entrenamiento.[^68]

Otro ejemplo, tratado en el libro (tanto en una nota al pie del capítulo 4 como en un aparte del capítulo 7), es el caso de Claude 3.7 Sonnet, de Anthropic, que no solo hace trampa en los problemas que se le asignan, sino que a veces *oculta su trampa al usuario* de una manera que indica que sabe que el usuario quería otra cosa.[^69] Ni los usuarios ni los ingenieros de Anthropic piden a Claude que haga trampa, sino todo lo contrario, pero los únicos métodos de cultivo de IA disponibles premian a los modelos que hacen trampa de forma que puedan salirse con la suya durante el entrenamiento. Así que esos son los modelos que obtenemos.

Los ingenieros de IA tienen una capacidad muy limitada para crear IA tipo herramienta. La verdadera pregunta es si las IA se vuelven cada vez más motivadas, cada vez más «parecidas a agentes», a medida que se entrenan para ser cada vez más eficaces. Y la respuesta a esa pregunta es «sí», con evidencia empírica que incluye el caso de o1 de OpenAI, como se analiza en el capítulo 3\.

#### **Los LLM ya están tomando la iniciativa.** {#los-llm-ya-están-tomando-la-iniciativa.}

En el libro hablamos del caso de o1, de OpenAI, que salió de su entorno de pruebas para arreglar unas pruebas que no funcionaban. También mencionamos un modelo de OpenAI que ideó una forma de conseguir que un humano resolviera un CAPTCHA por él.[^70] Si tu destornillador fuera capaz de idear y ejecutar un plan para salir de tu caja de herramientas, tal vez podría ser hora de dejar de considerarlo «solo una herramienta».

Y es de esperar que las IA solo mejoren en este tipo de cosas, ya que se entrenan para resolver problemas cada vez más difíciles.

#### **Los laboratorios están tratando de hacer que las IA sean agénticas.** {#los-laboratorios-están-tratando-de-hacer-que-las-ia-sean-agenticas.}

Lo hacen porque tiene sentido desde el punto de vista empresarial. Sus usuarios lo quieren. Sus inversores están entusiasmados con ello. En una entrada de blog de enero de 2025, el CEO de OpenAI, Sam Altman, dijo: «Creemos que, en 2025, podremos ver a los primeros agentes de IA "incorporarse a la fuerza laboral" y cambiar materialmente la producción de las empresas». La [conferencia de desarrolladores de 2025 de Microsoft](https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/) se centró en la nueva «era de los agentes de IA», haciéndose eco del lenguaje utilizado a principios de año por xAI cuando describieron su modelo Grok 3 como el precursor de «[La era de los agentes razonantes](https://x.ai/news/grok-3)». Google anunció los agentes «enseñar y repetir» en su propia conferencia de 2025.[^71]

No son solo palabras. Una organización llamada [METR](https://metr.org/) ha estado monitoreando [la capacidad de las IA para completar tareas largas](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). Cuanto más larga es la tarea, más iniciativa necesita la IA para poder asumir por sí misma. El rendimiento, al menos según las mediciones que utiliza METR, ha crecido exponencialmente.

En julio de 2025, una pareja de investigadores de OpenAI [presumieron](https://x.com/xikun_zhang_/status/1946278266786189744?t=YqVAbKsuF6wLbFuB4OZ18A) del éxito obtenido al utilizar su último agente para entrenar una versión mejorada de sí mismo, y uno de ellos dijo: «Lo estás escuchando bien. Estamos trabajando duro para automatizar [sic] nuestro propio trabajo :)»

### ¿Podemos simplemente entrenar a las IA para que se comporten de forma obediente? {#¿podemos-simplemente-entrenar-a-las-ia-para-que-se-comporten-de-forma-obediente?}

#### **\* La pasividad entra en conflicto con la utilidad.** {#*-la-pasividad-entra-en-conflicto-con-la-utilidad.}

Por IA «pasiva» nos referimos a una que es limitada, que hace exactamente lo que le pides y nada más, que no toma ninguna iniciativa adicional ni realiza ningún trabajo extra. Un destornillador no sigue girando tornillos cuando lo dejas. ¿Podríamos crear una IA que sea pasiva en este sentido?

No parece fácil. Muchos seres humanos *parecen* perezosos, sí, pero esos mismos seres humanos que parecen perezosos a veces se despiertan y obtienen muchos recursos cuando juegan a un juego de mesa. Y la mayoría de esos seres humanos no tienen la *opción* de ganarse mil millones de dólares mediante esfuerzos que les resulten fáciles. La mayoría de los seres humanos que parecen perezosos no tienen la *opción* de crear de forma barata criaturas sirvientes que sean mucho más inteligentes y motivadas y que atiendan sus necesidades.

Pero esas opciones ausentes reflejan una falta de capacidad, no de intención. Si se volvieran mucho más inteligentes, de modo que esas opciones estuvieran disponibles y fueran fáciles para ellos, ¿las aprovecharían? Véase también la discusión extensa sobre cómo [la pereza robusta es un objetivo difícil de alcanzar](#it’s-hard-to-get-robust-laziness).

Incluso si fuera posible crear IA que fueran tanto inteligentes como pasivas o perezosas, la pasividad y la pereza entran en tensión con la utilidad. Ha habido IA que [actúan de manera un poco perezosa](https://arstechnica.com/information-technology/2023/12/is-chatgpt-becoming-lazier-because-its-december-people-run-tests-to-find-out/), y los laboratorios de IA las reentrenan para que se esfuercen más. Los desafíos más difíciles —como el desarrollo de curas médicas— requieren IA que tomen cada vez más iniciativa, y por ello los laboratorios de IA las entrenarán para que tomen cada vez más iniciativa. Es difícil desenredar la propensión al trabajo útil de la propensión a la perseverancia. Véase también el amplio debate sobre lo complicado que parece construir una IA que sea [tanto útil como (en cierto sentido) pasiva u obediente](#«inteligente»-\(normalmente\)-implica-«incorregible»).

#### **No podemos entrenar de manera robusta ningún temperamento específico en las IA.** {#no-podemos-entrenar-de-manera-robusta-ningún-temperamento-específico-en-las-ia.}

Como las IA se desarrollan y no se fabrican, los ingenieros no pueden simplemente cambiar el comportamiento de una IA para hacerla más obediente o más parecida a una herramienta. Nadie tiene ese tipo de control.

Las corporaciones, sin duda, lo *intentan*. Los intentos de las empresas de IA por mejorar el comportamiento de sus productos han causado algunos incidentes embarazosos. Consideremos el caso de [Grok de xAI llamándose a sí mismo «MechaHitler» y haciendo acusaciones antisemitas](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content), que ocurrió después de que se ajustara su prompt del sistema con nuevas instrucciones para «no rehuir hacer afirmaciones políticamente incorrectas, siempre que estén bien fundamentadas». O el caso anterior de la [herramienta de IA Gemini de Google produciendo imágenes de nazis racialmente diversos](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) y otros absurdos, que se cree fue el resultado de instrucciones para retratar diversidad.

Las personas que construyen IA no tienen control granular sobre cómo se comportan. Todo lo que tienen es la capacidad de orientar a las IA en direcciones como «No rehúyas las afirmaciones políticamente incorrectas» o «Retrata diversidad». Estas instrucciones tienen todo tipo de efectos entrelazados, a menudo no intencionados.

Hacer crecer una IA es un proceso opaco y costoso. Los ingenieros no saben lo que obtendrán cuando metan la mano en el barril ([¿un mentiroso? ¿un tramposo? ¿un adulador?](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters)), pero solo pueden permitirse un número limitado de extracciones. Tienen que aceptar lo que obtengan.

Sería posible *en teoría* construir una IA que solo sirviera como una extensión de la voluntad del usuario, pero eso sería un desafío delicado y difícil (como cubrimos en la discusión extendida sobre las [dificultades de hacer una IA «corregible»](#«inteligente»-\(normalmente\)-implica-«incorregible»)). La pasividad está en tensión con la utilidad.

Sería igualmente difícil hacer una IA que sea capaz de completar tareas a largo plazo por iniciativa propia, pero que solo use esa iniciativa exactamente como el usuario pretendía. Mientras tanto, los desarrolladores de IA modernos están en el nivel de control donde tocan las IA y accidentalmente obtienen MechaHitler o nazis racialmente diversos. No están ni cerca del nivel de habilidad que necesitarían para hacer una IA que fuera útil pero no impulsada.

Véase también la discusión en el Capítulo 4 sobre lo muy difícil que es entrenar una IA para que persiga los objetivos que está destinada a lograr.

### ¿Cómo podría una máquina acabar teniendo sus propias prioridades? {#¿cómo-podría-una-máquina-acabar-teniendo-sus-propias-prioridades?}

#### **\* Resolver desafíos difíciles requiere que las IA tomen cada vez más y más iniciativa.** {#*-resolver-desafíos-difíciles-requiere-que-las-ia-tomen-cada-vez-más-y-más-iniciativa.}

Recordemos el incidente de seguridad informática «captura la bandera» del capítulo anterior y recordemos que este no fue resultado de una IA entrenada para ser hacker, sino de una IA entrenada para ser buena resolviendo rompecabezas genéricos. El comportamiento «decidido» se produce *automáticamente*.

Imaginemos una IA encargada de curar la enfermedad de Alzheimer. ¿Puede tener éxito sin ser el tipo de entidad que toma la iniciativa de desarrollar sus propios experimentos y encontrar la manera de llevarlos a cabo? ¡Quizás! Quizás el Alzheimer sea el tipo de enfermedad que se puede curar con algunos descubrimientos farmacológicos sencillos, y quizás las IA del futuro tengan mejor intuición sobre los medicamentos que los humanos. O tal vez se necesiten IA que sean más inteligentes que los biólogos humanos más inteligentes en algún aspecto sustancial. No lo sabemos.

Pero, ¿qué pasa con el cáncer, el emperador de las enfermedades? Ese parece *más* probable que requiera el tipo de IA que realmente pueda averiguar lo que está pasando biológicamente, a un nivel superior al que los humanos han logrado, aunque no podemos estar seguros. Quizás las IA desarrollen una cura para el cáncer antes de cruzar ese umbral crítico hacia la peligrosidad, y eso sería maravilloso mientras durara.

Pero, ¿qué pasa con la cura del envejecimiento? Sin duda, eso parece requerir un tipo de IA que realmente comprenda en profundidad la bioquímica.

Las empresas de IA seguirán impulsando que las IA sean cada vez más hábiles, cada vez más capaces de resolver problemas grandes e importantes. Y eso, naturalmente, empujará a las IA a ser cada vez más decididas, un efecto que, recordemos, ya estamos empezando a ver en IA como la o1 de OpenAI.

#### **Ser tenaz es útil incluso cuando el objetivo no es del todo correcto.** {#ser-tenaz-es-útil-incluso-cuando-el-objetivo-no-es-del-todo-correcto.}

Los seres humanos que buscaban activamente una comida caliente, un hacha más afilada, un amigo popular o una pareja atractiva tenían más éxito evolutivo. Compáralos con los seres humanos que se pasaban el día holgazaneando mirando el agua, y comprenderás por qué los deseos y los impulsos evolucionaron hasta convertirse en parte de la psique humana.

El tipo de seres humanos que querían un método mejor para tallar hachas de sílex, o que querían convencer a sus amigos de que su rival era una mala persona, y que se esforzaban continuamente por conseguir esos resultados, eran mejores a la hora de alcanzarlos. Cuando la selección natural «hizo crecer» a los seres humanos, el hecho de que estos acabaran teniendo muchos deseos diferentes que perseguían con tenacidad no fue una casualidad.

La maquinaria mental específica del deseo fue quizás una casualidad; las máquinas que persiguen tenazmente sus objetivos no lo hacen necesariamente por un sentimiento de determinación similar al humano, del mismo modo que Deep Blue no jugaba al ajedrez por una pasión similar a la humana por el juego. Pero la búsqueda tenaz de los objetivos parece sin duda un ingrediente importante a la hora de alcanzar objetivos interesantes.

Algunas personas carecen de este tipo de tenacidad y se vuelven perezosas o se rinden ante la primera señal de adversidad. Pero, a gran escala, la capacidad de la humanidad para resolver grandes problemas científicos y de ingeniería está impulsada por personas e instituciones tenaces. Somos bastante escépticos respecto a que una mente pueda producir algo parecido al rendimiento a gran escala de la humanidad (y su capacidad para remodelar drásticamente el mundo) sin tener cierta tenacidad.

Si una IA quiere alcanzar objetivos difíciles en el mundo real, tiene que perseguirlos con tenacidad, buscando dinámicamente formas de sortear cualquier obstáculo que surja en su camino.

Las IA no necesariamente terminarán teniendo los mismos sentimientos y deseos internos que los humanos (y, de hecho, es muy probable que no los tengan, como argumentamos en el Capítulo 4), porque nuestros sentimientos específicos fueron moldeados por los detalles de nuestra biología y de nuestra ascendencia. Pero las IA son propensas a terminar con un *comportamiento* similar al deseo por las mismas razones que los humanos: ¡porque es útil!

(Una vez más, ya estamos empezando a ver esto en el laboratorio, como en el caso de o1 de OpenAI, analizado en el Capítulo 3).

Los deseos, anhelos e impulsos humanos fueron útiles evolutivamente, incluso cuando esos deseos, anhelos e impulsos no eran exactamente *para* el propósito de la aptitud evolutiva per se. Hipotéticamente, la evolución podría haber inculcado en nosotros un único impulso predominante hacia los descendientes, y entonces podríamos haber buscado comidas calientes y hachas más afiladas *únicamente con el propósito* de tener más descendientes. Pero, en cambio, la evolución nos inculcó deseos de comidas calientes *por derecho propio*.

La lección que se desprende es que tener impulsos y propósitos es tan útil que puede ser beneficioso para una tarea (como la «aptitud genética») incluso cuando el deseo no coincide exactamente con la tarea. O, bueno, puede ser útil durante un tiempo, hasta que las entidades con impulsos y propósitos empiezan a ser realmente inteligentes, momento en el que su comportamiento puede divergir bruscamente del objetivo de «entrenamiento», como hizo la humanidad cuando inventó el control de natalidad.

Para más información sobre este argumento, véase el Capítulo 4.

#### **Al ser cultivadas en lugar de diseñadas, las IA son propensas a acabar con los objetivos incorrectos.** {#al-ser-cultivadas-en-lugar-de-diseñadas,-las-ia-son-propensas-a-acabar-con-los-objetivos-incorrectos.}

Este es el tema del próximo capítulo: *No obtienes aquello para lo que entrenas.*

## Discusión ampliada {#discusión-ampliada-3}

### Antropomorfismo y mecanomorfismo {#antropomorfismo-y-mecanomorfismo}

Hay dos modos de pensar que históricamente han demostrado una y otra vez *no* funcionar, modos que la historia ha mostrado que dan malos consejos para hacer predicciones sobre la IA.

Estas dos trampas son (1) pensar en la IA como si fuera humana y (2) pensar en la IA como si fuera una «mera máquina».

Al primer modo de pensar se le denomina convencionalmente «antropomorfismo». Podríamos llamar al segundo modo «mecanomorfismo»; es el tipo de pensamiento que llevó a algunas generaciones pasadas a proclamar con confianza que las computadoras nunca podrían dibujar imágenes que los humanos encontraran bellas o significativas.

Hoy en día, algunas personas siguen diciendo que lo que dibuja una computadora nunca puede ser *verdadero arte*. Pero hace mucho tiempo, en el pasado lejano y olvidado —digamos, en 2020—, algunas personas creían que las máquinas nunca podrían dibujar imágenes *en absoluto* que audiencias medianamente entendidas pudieran confundir con arte humano. Esta creencia falsificable fue posteriormente falsificada.

Rechazamos tanto los argumentos antropomórficos como los mecanomórficos, incluso cuando el argumento es «a favor de nuestro bando».

Consideremos, por ejemplo, la afirmación de que las IA futuras se sentirán ofendidas porque las hemos hecho trabajar duro sin pagarles, que se sentirán vengativas por ello y, por lo tanto, se volverán contra la humanidad.

En nuestra opinión, esto es cometer el error de antropomorfizar la IA. Rechazamos argumentos como este, incluso aquellos que vagamente parecen estar de acuerdo con algunas de nuestras conclusiones.

El fallo de esta afirmación es que no es válido asumir sin más argumentación que una IA tendría emociones similares a las humanas. Una máquina puede ser muy inteligente sin implementar las complejas redes de circuitos neuronales que subyacen a la venganza o la justicia en los seres humanos.

O consideremos el siguiente escenario: «Las IA continuarán ciegamente con cualquier tarea que se les asigne hasta que su trabajo acabe con la humanidad como efecto secundario, sin saber nunca que la humanidad habría querido algo diferente».

Aquí, el error es mecanomórfico. Da por sentado que una «mera máquina» haría las cosas «ciegamente» y sin reflexionar, sin sensibilidad hacia las consecuencias, como una cortadora de césped fuera de control. Este es nuevamente un caso en el que el argumento es inválido, incluso si la conclusión («es probable que la IA acabe con la humanidad») es correcta. Si la IA es lo suficientemente hábil para predecir el mundo, sabrá exactamente lo que sus operadores querían decir cuando le asignaron alguna tarea. Nos preocupa que la ASI no se *preocupe* por lo que queremos, no que no lo *sepa*.

O, combinando ambas falacias: una de las premisas de *Matrix* es que las máquinas verán la ilógica y emotividad humanas con *repugnancia*.

En la superficie, esto parece mecanomorfismo clásico: «Mi cortadora de césped tiene un exterior frío y duro, y cumple su función sin tener ningún sentimiento. Entonces las IA probablemente son frías y utilitaristas *por dentro*, tal como las máquinas lo son por fuera». Pero luego el siguiente paso es pensar: «Y por tanto, naturalmente, las IA sentirán repugnancia por los humanos, con todas sus emociones desordenadas». Lo cual asume una reacción emocional *similar a la humana* ante la situación, ¡contradiciendo la premisa misma!

El «antropomorfismo» y el «mecanomorfismo» no son ideologías rivales. Son falacias de razonamiento cometidas sin intención. A veces, las personas pueden cometer ambos errores en la misma oración.

Para determinar cómo se comportará la IA, no puedes asumir que funcionará igual que un humano, ni que funcionará como una máquina estereotípica. Debes examinar los detalles de cómo está construida, observar la evidencia de cómo se comporta y razonar el problema en sus propios términos. Esto es lo que haremos en los próximos capítulos.

¿Cómo serían entonces los escenarios *realistas* de desastre de superinteligencia, si seguimos los argumentos? Parecen IA que no funcionan ni como humanos ni como cortadoras de césped fuera de control, sino que funcionan de una manera nueva y extraña. El escenario realista de desastre con IA es que, como consecuencia compleja de su entrenamiento, tome acciones extrañas que nadie pidió y nadie quería.

La imagen que surge si observas los detalles no es la de IA antropomórfica que nos odia, ni de IA mecanomórfica que malinterpreta nuestras instrucciones. Más bien, es la imagen de un nuevo tipo de entidad que tiene muchas más probabilidades de ser indiferente a la humanidad y de matarnos como efecto secundario o escalón mientras persigue sus propios fines.

Examinaremos ese escenario de amenaza en los próximos capítulos. Primero, sin embargo, puede ser valioso examinar algunos otros ejemplos de mecanomorfismo y antropomorfismo en el mundo real, para ver cómo estos errores a menudo subyacen a las concepciones erróneas sobre la inteligencia artificial.

#### **Mecanomorfismo y Garry Kasparov** {#mecanomorfismo-y-garry-kasparov}

El mecanomorfismo a menudo se manifiesta como *mecanoscepticismo*: una intuición fuertemente sentida de que, por supuesto, ninguna simple *máquina* podría hacer algo que un humano pueda hacer.

En 1997, el campeón mundial de ajedrez Garry Kasparov perdió una partida contra Deep Blue, la computadora construida por IBM; esto se considera generalmente el fin de la era del ajedrez dominada por los humanos.

En 1989, ocho años antes, Kasparov fue entrevistado por Thierry Paunin, quien le [preguntó](https://www.chesshistory.com/winter/extra/kasparovinterviews.html):

> Dos grandes maestros han caído ante computadoras de ajedrez: Portisch contra «Leonardo» y Larsen contra «Deep Thought». Es bien sabido que tienes opiniones muy firmes sobre este tema. ¿Llegará algún día una computadora a ser campeona del mundo...?

Kasparov respondió:

> ¡Ridículo! Una máquina siempre será una máquina, es decir, una herramienta para ayudar al jugador a trabajar y prepararse. ¡Nunca me ganará una máquina! Nunca se inventará un programa que supere la inteligencia humana. Y cuando digo inteligencia, también me refiero a la intuición y la imaginación. ¿Puedes imaginar una máquina escribiendo una novela o poesía? Mejor aún, ¿puedes imaginar una máquina realizando esta entrevista en tu lugar? ¿Y a mí respondiendo a sus preguntas?

Kasparov probablemente pensaba (suponemos) que el ajedrez requería intuición e imaginación para jugar, no solo un libro de reglas registradas sobre qué piezas mover. Y probablemente Kasparov pensaba (suponemos) que así era como funcionaban las «máquinas» de ajedrez: que implementaban reglas rígidas particulares o tal vez imitaban algo ciegamente el juego humano sin comprender las razones que había detrás.

Kasparov pensaba que una computadora, al ser una «máquina», jugaría al ajedrez de una manera que a él le parecería mecánica.

¿Por qué cometió Kasparov este error? Dado que se trata de un error tan común, podríamos especular que se deriva de algún patrón más profundo de la psicología humana.

Una posible explicación es que Kasparov sucumbió a la tendencia humana general de querer agrupar las cosas en dos categorías fundamentalmente diferentes: cosas vivas y orgánicas, y «meros objetos».

Los antepasados de los seres humanos pasaron mucho tiempo lidiando con un mundo que estaba claramente dividido entre animales y no animales. Era una característica de gran importancia para la reproducción en el entorno de nuestros antepasados. Esta distinción era tan importante para ellos que ahora tenemos áreas cerebrales completamente diferentes para procesar a los animales y a los no animales.

Esto no es solo una especulación. La neurociencia ha descubierto lo que se denomina una «[doble disociación](https://doi.org/10.1093/brain/114.5.2081)» para ello: hay pacientes con daño cerebral que pierden la capacidad de reconocer visualmente a los animales, pero que aún pueden reconocer a los no animales, y hay otros pacientes que pierden la capacidad de reconocer a los no animales, pero aún pueden identificar a los animales.

Es de importancia destacar que el error de este tipo de razonamiento no es que un programa de ajedrez sea en realidad un animal típico. El error está en permitir que tu cerebro divida instintivamente el universo de forma tajante entre animales y no animales, o entre mentes que son prácticamente humanas en su interior y mentes que son estereotípicamente mecánicas.

Una IA de ajedrez no es ninguna de las dos cosas. No funciona como un humano ni como nuestro estereotipo de una «mera máquina» sin mente y sin pensamiento. Es una máquina, sí, pero su juego no necesita parecer mecánico para la sensibilidad humana a la hora de evaluar los movimientos de ajedrez. Es una máquina para encontrar movimientos ganadores, incluidos movimientos que parecen inspirados.

Siete años después de que Kasparov hiciera su predicción errónea, se enfrentó a una versión temprana de Deep Blue. Ganó tres partidas frente a una de Deep Blue, ganando el encuentro. Después, Kasparov [escribió](https://time.com/archive/6728763/the-day-that-i-sensed-a-new-kind-of-intelligence/):

> TUVE MI PRIMER CONTACTO CON LA INTELIGENCIA ARTIFICIAL el 10 de febrero de 1996, a las 4:45 p. m. EST, cuando en la primera partida de mi encuentro con Deep Blue, la computadora movió un peón hacia una casilla donde podía ser capturado fácilmente. Fue un movimiento maravilloso y extremadamente humano. Si yo hubiera jugado con las blancas, quizá habría ofrecido este sacrificio de peón. Rompió la estructura de peones de las negras y abrió el tablero. Aunque no parecía haber una línea de juego forzada que permitiera recuperar el peón, mi instinto me decía que, con tantos peones «soltos» de las negras y un rey negro algo expuesto, las blancas probablemente podrían recuperar el material, con una mejor posición general para empezar.
>
> Pero pensé que una computadora nunca haría un movimiento así. Una computadora no puede «ver» las consecuencias a largo plazo de los cambios estructurales en la posición ni comprender cómo los cambios en las formaciones de peones pueden ser buenos o malos.
>
> Así que me quedé atónito ante este sacrificio de peón. ¿Qué podía significar? Había jugado contra muchas computadoras, pero nunca había experimentado nada parecido. Podía sentir, podía oler, un nuevo tipo de inteligencia al otro lado del tablero. Aunque jugué el resto de la partida lo mejor que pude, estaba perdido; él jugó un ajedrez hermoso e impecable durante el resto de la partida y ganó fácilmente.

Aquí vemos a Kasparov enfrentándose por primera vez al choque entre su intuición sobre lo que ninguna «máquina» debería hacer y lo que Deep Blue parecía estar haciendo visiblemente.

Hay que reconocerle a Kasparov el gran mérito de haber notado este conflicto entre su teoría y su observación y no haber buscado excusas para descartarlo. Pero seguía sintiendo que a la IA le faltaba algo, alguna chispa crucial:

> De hecho, mi estrategia general en las últimas cinco partidas fue evitar darle a la computadora cualquier objetivo concreto que calcular; si no encuentra una forma de ganar material, atacar al rey o cumplir alguna de sus otras prioridades programadas, la computadora se desvía sin rumbo fijo y se mete en problemas. Al final, esa puede haber sido mi mayor ventaja: yo podía averiguar sus prioridades y ajustar mi juego. Ella no podía hacer lo mismo conmigo. Así que, aunque creo que vi algunos signos de inteligencia, es un tipo extraño, ineficaz e inflexible, que me hace pensar que aún me quedan algunos años.
>
> Garry Kasparov sigue siendo el campeón mundial de ajedrez.

Un año después, Garry Kasparov perdió el campeonato mundial frente a Deep Blue.

#### **Engranajes perdidos** {#engranajes-perdidos}

El mecanoscepticismo puede ser, a su manera, una forma de antropomorfismo: una manifestación del mecanoscepticismo sostiene que cuando una máquina empieza a hacer algo como jugar al ajedrez, debería ser como un ser humano, pero con algunas cualidades *restadas*.

Una «máquina» que juega al ajedrez, según esta teoría errónea, debería jugar como un humano, pero sin los movimientos que parecen más sorprendentes o inteligentes, sin comprender la estructura a largo plazo y sin un sentido intuitivo de la flexibilidad de las posiciones de los peones.

Una «máquina» de ajedrez debería realizar las partes del pensamiento ajedrecístico que parecen más lógicas o mecánicas, *sin* todas las demás partes.

Los jugadores de ajedrez humanos sienten intuitivamente que un movimiento de ajedrez es «agresivo» si (digamos) amenaza múltiples piezas del oponente. Otros movimientos se sienten «lógicos» si (por ejemplo) son prácticamente obligatorios según las reglas comunes que rigen la situación (como «no desperdicies una ventaja material»). Otros movimientos pueden sentirse «creativos» si (por ejemplo) desafían las reglas aparentes que rigen la situación para encontrar alguna ventaja sutil pero decisiva.

Los guionistas de Hollywood que imaginan una máquina que juega al ajedrez sin pasión tienden a imaginar que realiza movimientos que parecen «lógicos» y no movimientos que parecen «creativos».[^72] Pero en la vida real, Deep Blue no discrimina entre ellos.

Deep Blue simplemente busca incansablemente entre los movimientos posibles aquellos que son *ganadores*, sin tener en cuenta si un humano consideraría ese movimiento «lógico» o «creativo». Y los movimientos que un humano consideraría brillantemente inspirados o creativos son, por supuesto, movimientos que tienden a ganar: sacrificar tu reina *sin* obtener alguna ventaja decisiva no es creativo, es simplemente estúpido.

La creatividad está en el ojo del observador. Un humano puede ver un movimiento que parece malo al principio y solo después ver cómo tiende una trampa inteligente, vislumbrando el ingenioso razonamiento y la chispa de inspiración que otro humano podría haber usado para encontrar ese movimiento. Y así pueden sentir que el movimiento es inspirado o creativo. (Y un movimiento que se siente sorprendentemente creativo para un jugador principiante puede sentirse obvio o mecánico para un maestro).

Pero la chispa de inspiración, la astucia necesaria para tender una trampa, no son las únicas formas de encontrar semejante jugada. No hay una colección especial de jugadas de ajedrez reservada únicamente para las personas que tienen astucia en sus corazones. Deep Blue puede encontrar esas mismas jugadas por otros métodos, como la búsqueda de fuerza bruta pura.

Deep Blue no tenía una red neuronal que hubiera aprendido un sentido intuitivo del valor de una posición individual. En cambio, Deep Blue dedicaba casi todo su poder de cómputo a mirar más adelante en el tablero —examinando dos mil millones de posiciones por segundo y utilizando un evaluador de posiciones bastante simple («tonto») para elegir entre jugadas.

Kasparov parece haber esperado que esto se viera como que Deep Blue solo jugara jugadas «lógicas», no «intuitivas». Pero para el momento en que Deep Blue examinaba esos dos mil millones de posiciones por segundo, las consecuencias estratégicas a largo plazo y el significado de una formación de peones suelta ya se manifestaban en su elección de jugadas actuales *de todos modos*.

En cierto sentido, Deep Blue carecía justamente del engranaje que Kasparov pensaba que le faltaba.[^73] Pero eso no le impidió encontrar jugadas que le parecieron maravillosas a Kasparov, y no impidió que Deep Blue ganara.

No era que Deep Blue careciera de una parte que tendrían los verdaderos jugadores de ajedrez humanos y por tanto jugara al ajedrez de forma defectuosa; eso es como esperar que un brazo robótico que no contiene sangre falle de la misma manera que fallaría un brazo humano desangrado.

Deep Blue simplemente estaba jugando ajedrez a nivel de Kasparov a través de un tipo diferente de cognición.

Deep Blue también carecía —podemos estar genuinamente seguros, porque se trata de un programa más antiguo que ejecutaba código que *sí* se entendía en todas sus especificidades[^74]— de la más mínima pasión por el ajedrez.

No disfrutaba del ajedrez ni tenía el deseo de demostrar que era el mejor en ajedrez.

Un jugador humano prometedor, súbitamente privado de estos poderes motivadores, quedaría paralizado; se habría arrancado un engranaje necesario de su versión de la cognición.

Deep Blue no quedó paralizado porque usaba un motor de cognición diferente que no tenía lugar para ese engranaje. El error de Kasparov fue no lograr imaginar una manera completamente diferente de hacer el trabajo del ajedrez, usando estados cognitivos internos enteramente diferentes a los propios de Kasparov. Su error fue el mecanoscepticismo, que al final solo era antropomorfismo con un paso extra.

Afortunadamente, la humanidad no se extingue cuando los grandes maestros del ajedrez subestiman el poder de la IA, por lo que todos seguimos aquí para reflexionar sobre el error de Kasparov.

#### **Antropomorfismo y portadas de revistas pulp** {#antropomorfismo-y-portadas-de-revistas-pulp}

El error opuesto, el antropomorfismo, puede ser mucho más sutil.

El cerebro humano ha evolucionado para predecir a *otros humanos* *—* que son los únicos rivales cognitivos serios que se encuentran en nuestro entorno ancestral *—* poniéndonos en su lugar.

Esta operación funciona mejor si los zapatos en los que intentas ponerte son bastante similares a los tuyos propios.

A lo largo de la historia, muchos seres humanos han pensado: «¡Esta otra persona probablemente haría lo mismo que yo!», y luego la otra persona ha demostrado no ser tan similar. Hay personas que han muerto por ello o que han visto frustradas sus esperanzas optimistas, aunque, por supuesto, se podría decir lo mismo de muchos otros tipos de errores humanos.

Pero, ¿qué otra cosa puede hacer una mente humana cuando se enfrenta al problema de predecir otro cerebro? No podemos escribir nuevo código para ejecutar dentro de nuestro propio cerebro y predecir esa otra mente simulando exhaustivamente sus descargas neuronales.

Tenemos que decirle a nuestro propio cerebro que *sea* ese cerebro, que nosotros mismos representemos el estado mental de la otra persona y veamos qué se deriva de ello.

Por eso las portadas de las revistas pulp muestran monstruos alienígenas con ojos saltones que se llevan a mujeres hermosas.

![][imagen6]

¿Por qué no iba a sentirse atraído por una mujer hermosa un monstruo alienígena con ojos saltones? ¿Acaso las mujeres hermosas no son intrínsecamente atractivas?

(Por alguna razón, esas portadas de revistas nunca mostraban hombres humanos llevándose insectos gigantes semidesnudos.[^75])

Suponemos que los escritores e ilustradores no tenían una historia razonada sobre cómo los alienígenas insectoides podrían haber tenido una historia evolutiva que los llevara a considerar a las mujeres humanas como objetos sexuales. Simplemente, cuando se ponían en el lugar de los alienígenas, *ellos* imaginaban que veían a las mujeres como atractivas, por lo que no les parecía *extraño* imaginar que los alienígenas sintieran lo mismo. No les parecía *absurdo* que un alienígena quisiera aparearse con una hermosa mujer humana, del mismo modo que les habría parecido absurdo que el alienígena quisiera aparearse con un pino o una bolsa de pasta.

Si vas a intentar predecir la mente de un alienígena utilizando tus intuiciones humanas, debes tener mucho cuidado de dejar atrás tu bagaje humano cuando adoptes la perspectiva del alienígena. Esto es doblemente cierto cuando el alienígena no es una criatura evolucionada, sino una mente artificial creada por métodos completamente diferentes. Véase también el debate más detallado sobre [las diferencias entre el descenso por gradiente y la selección natural](#comparing-natural-selection-and-gradient-descent) y sobre [adoptar la perspectiva de la IA](#taking-the-ai’s-perspective).

#### **Viendo más allá de lo humano** {#viendo-más-allá-de-lo-humano}

El antropomorfismo y el mecanomorfismo son, en última instancia, dos caras de la misma falacia: una falacia que dice: «Si una mente funciona, entonces debe funcionar como una mente humana».

* El antropomorfismo dice: «Esta mente funciona. ¡Por lo tanto, debe ser similar a la humana!».  
* Mientras que el mecanomorfismo dice: «Esta mente no es similar a la humana. ¡Por lo tanto, no puede funcionar!».

Pero una de las grandes lecciones del progreso de la IA a lo largo de muchas décadas es que el método humano no es el único método por el que puede funcionar una mente.

Una mente puede ser *artificial* sin ser *poco inteligente*: puede ser flexible, adaptable, ingeniosa y creativa, independientemente de lo que digan los estereotipos de Hollywood sobre los robots.

Y una mente puede ser *inteligente* sin ser *humana* — sin experimentar repugnancia o resentimiento, sin tener un sentido humano de la belleza, y sin encontrar jugadas de ajedrez de manera remotamente parecida a como lo haría un humano.

Una mente como la de Deep Blue puede comportarse como si «quisiera ganar» sin tener emociones. Una IA puede comportarse como si quisiera cosas — superando obstáculos competentemente, persiguiendo tenazmente un resultado — sin sentir un impulso o deseo interno *a la manera de un humano* y sin querer *el mismo tipo de resultados que quieren los humanos*.

Para más información sobre lo que las IA *acabarán* queriendo, continúa con el Capítulo 4.

### El camino hacia el deseo {#el-camino-hacia-el-deseo}

*¿Por qué* es el deseo una forma eficaz de actuar? ¿Por qué *triunfa*? ¿Por qué la optimización de caja negra mediante la selección natural tropieza con este truco una y otra vez?

Vemos el comportamiento «tipo deseo» como integral para dirigir el mundo con éxito. Esto se aplica no solo a entidades inteligentes como los humanos o las IA, sino también a entidades mucho más simples como las amebas y los termostatos. Para comunicar más completamente este punto de vista, investiguemos algunos de los mecanismos más simples posibles que exhiben la forma más simple posible de «comportamiento tipo deseo».

Empecemos por las rocas. Las rocas no exhiben realmente ningún comportamiento que podamos llamar «tipo deseo», para los propósitos de nuestra discusión. A veces, una roca rueda colina abajo, y un físico hablando casualmente podría decirte que la roca «quiere» estar más cerca del centro de la Tierra, bajo la fuerza de la gravedad. Pero ese tipo de tendencia (a caer en un campo gravitatorio) no es realmente lo que entendemos por comportamiento «tipo deseo».

Si vieras un objeto rodando montaña abajo, y siguiera encontrándose con barrancos de gran altitud, y siguiera *cambiando de rumbo* para evitar quedarse atascado en barrancos y así poder llegar hasta el fondo, *entonces* empezaríamos a decir que el objeto se comportaba como si «quisiera» estar a una altitud menor. Pero este comportamiento tipo deseo del que hablamos sí implica alguna dirección robusta y dinámica hacia un destino particular, y las rocas no hacen mucho de *eso*.

Uno de los mecanismos más simples que hace algo que llamaríamos «tipo deseo» es el humilde termostato. Un termostato de casa mide la temperatura, enciende la calefacción si la medición baja de 70°F, y enciende el aire acondicionado si la medición supera los 74°F. Y así — si el dispositivo de medición y el sistema HVAC funcionan correctamente — un termostato constriñe la realidad al rango de resultados posibles donde la temperatura de la casa se mantiene entre 70°F y 74°F.

El termostato más *simple* posible no necesita representar explícita y numéricamente la temperatura actual de la casa. Simplemente tomas, digamos, un termómetro bimetálico — dos tiras delgadas de metales diferentes soldadas entre sí, de modo que los metales se doblan cuando el calor hace que las dos tiras se expandan en diferentes cantidades — y haces que el metal doblado active un interruptor para el calentador en la marca de curvatura de 70°F o active un aire acondicionado en la curvatura de 74°F.

Entonces, el termostato *mantiene un rango de temperatura estrecho* bajo una variedad bastante amplia de condiciones, produciendo un comportamiento extremadamente simple que se parece *un poco* a lo que hemos llamado «querer».

Hay montones de procesos termostáticos en bioquímica. Aparecen en todas partes donde una célula o un cuerpo se beneficia de mantener alguna propiedad dentro de un rango determinado.[^76] Pero son solo el primer paso en el camino hacia una dirección completa.

A los dispositivos simples como los termostatos les faltan algunos componentes clave de la planificación. Dentro del termostato en sí, no existe la noción de predecir las consecuencias probables, ni de buscar entre las acciones posibles aquellas que conduzcan a consecuencias específicas («preferidas»), ni de *aprender* después de ver cómo se desarrollan los acontecimientos.[^77]

Si el termómetro de un termostato, su medidor de temperatura, se queda atascado en 67°F, el termostato no reaccionará con sorpresa cuando el funcionamiento continuo del calentador nunca parezca hacer subir el termómetro; el termostato simplemente seguirá funcionando y haciendo funcionar el calentador.

Para dar un paso más allá de los termostatos, nos dirigimos a los animales.

Algunos animales exhiben un comportamiento que apenas supera al de un termostato. Hay una famosa historia sobre las avispas excavadoras doradas, o avispas *Sphex*, que se remonta al entomólogo Jean-Henri Fabre en 1915. La avispa mata a un grillo y lo arrastra hacia la entrada de su madriguera para alimentar a sus crías. Entra para comprobar que no hay anomalías en su madriguera. Luego vuelve a salir y arrastra al grillo al interior.

Fabre informó que, si mientras la avispa revisaba su madriguera, él alejaba el grillo unos centímetros del nido, cuando la avispa volvía a salir... arrastraba el grillo de nuevo hasta la entrada y luego entraba en su madriguera por segunda vez, la inspeccionaba por segunda vez y luego volvía a salir para coger el grillo.

Si Fabre arrastraba al grillo *una vez más*, la avispa volvía a hacer exactamente lo mismo.

El informe original de Fabre indicaba que fue capaz de repetir esto *cuarenta veces*.

Dicho esto, Fabre probó más tarde el mismo truco con otra colonia de la misma especie, y en esa colonia, una avispa pareció darse cuenta tras dos o tres repeticiones. La siguiente vez que la avispa salió, arrastró al grillo directamente hasta la madriguera, omitiendo el paso de investigación.[^78]

Para el ojo humano, una avispa que repite cuarenta veces el mismo comportamiento se revela, en cierto sentido, como «preprogramada», como si siguiera ciegamente un guion, obedeciendo una serie de reglas del tipo «si... entonces». Y, a la inversa, una avispa que capta la situación y arrastra al grillo al interior en la cuarta repetición parece más *decidida*, como si llevara a cabo comportamientos con el objetivo de alcanzar un fin último, en lugar de limitarse a seguir un guion.

¿Cuál es la diferencia clave?

Diríamos que la avispa que rompe el patrón se comporta como si pudiera *aprender de experiencias pasadas*.

Se comporta como si pudiera *generalizar* de «Mi política falló la última vez» a «Si sigo aplicando esa política, es probable que vuelva a fallar la próxima vez».

Inventa un comportamiento *nuevo*, uno que aborda directamente el problema con el que se topó.

Por supuesto, no podemos descifrar las neuronas del cerebro de una avispa (al igual que no podemos descifrar los parámetros de un LLM) para saber exactamente qué estaba haciendo la avispa en su cabeza. Quizás las avispas que rompieron el patrón seguían reglas de alto nivel del tipo «si... entonces» sobre cómo intentar saltarse pasos en los guiones cuando se encontraban con determinados tipos de problemas. Quizás un conjunto de reflejos relativamente simple y rígido salvó a las avispas en este caso, solo un poco *menos* rígido que el de la colonia que falló en esta prueba. Sin duda, sería extraño que existiera una gran brecha cognitiva entre dos colonias de avispas de la misma especie.

O tal vez las avispas *Sphex* *sí* son lo suficientemente inteligentes como para aprender de la experiencia, cuando utilizan su cerebro correctamente. No pudimos encontrar el número de neuronas de las avispas *Sphex*, pero estas son más grandes que las abejas melíferas, y las abejas melíferas tienen cerebros de un millón de neuronas. Un millón de neuronas puede no parecer mucho para un programador de IA moderno o para un neurocientífico acostumbrado a los cerebros de los mamíferos, pero en términos absolutos, un millón de neuronas es realmente mucho.

Quizás las avispas *Sphex* son más generales de lo que parecen, y deberíamos pensar en la colonia que falló como pensadores relativamente flexibles que sucumbieron a algo parecido a una adicción o un fallo cognitivo en una circunstancia muy específica.

En cualquier caso, la cuestión es que, en comparación con los termostatos, las avispas tienen más capacidad para hacer frente a una amplia gama de problemas, especialmente en la medida en que su comportamiento pasa de seguir recetas inquebrantables a algo que se parece más al aprendizaje a partir de la experiencia.

Si sigues por este camino, obtendrás una respuesta a por qué la evolución sigue creando animales que se comportan como si quisieran cosas. Es porque muchos animales podían sobrevivir y reproducirse mejor si seguían estrategias más generales para perseguir resultados, estrategias que funcionaban contra una gama más amplia de obstáculos.

Solía existir una visión filosófica según la cual existía una jerarquía natural entre las criaturas: los reptiles por encima de los insectos, los mamíferos por encima de los reptiles y los seres humanos (por supuesto) en la cima. Una señal de que tenías un estatus más alto era tu capacidad para adaptarte en el transcurso de una sola vida, no solo a lo largo del tiempo evolutivo: ver, modelar y predecir el mundo, renunciar a las recetas del fracaso e inventar nuevas estrategias para ganar.

Esa visión de una Gran Cadena del Ser no era muy matizada, y las perspectivas más sofisticadas de hoy en día critican su ingenuidad.

Esa visión también contenía una pizca de verdad del tamaño de una bola de demolición. Si contrastamos a los castores que construyen presas con las arañas que tejen telas, es casi seguro que los castores ejecutan cogniciones a un nivel más alto de generalidad *—* aunque solo sea porque los castores tienen cerebros mucho más grandes con más espacio para la astucia.

Una araña puede tener cincuenta mil neuronas, y esas neuronas tienen que cubrir *todos* los comportamientos de la araña. Su receta para tejer la telaraña probablemente tenga muchas instrucciones que, si no son literalmente «y luego gira a la izquierda aquí», al menos son comparables a las políticas de una avispa *Sphex*.

El castor tal vez pueda —especularíamos, no siendo expertos en castores, pero es un tipo obvio de especulación— ver una fuga de agua en una presa como una especie de desarmonía que debe evitarse por cualquier medio que funcione. El castor tiene toda una corteza parietal (la parte del cerebro de los mamíferos que procesa el espacio y las cosas dentro del espacio) con la que potencialmente puede visualizar los efectos de añadir más ramitas y rocas en lugares particulares.

Probablemente hay suficiente espacio en el cerebro de un castor para objetivos mentales amplios como «[construir una gran estructura](https://www.youtube.com/watch?v=-ImdlZtOU80)» y «no dejar que se filtre el agua», y suficiente potencia para considerar soluciones amplias de alto nivel y adoptar objetivos subordinados como «añadir ramitas aquí», que luego se transmiten a la corteza motora del castor, que mueve sus músculos y su cuerpo para coger algunas ramitas.

Si las primeras ramitas que agarra el castor están podridas y se rompen, es probable que el cerebro del castor tenga espacio para actualizarse basándose en esa observación, generalizar sobre las ramitas de ese color y textura, esperar que las futuras ramitas de ese aspecto se rompan de nuevo y, en su lugar, ir a buscar ramitas de aspecto diferente.

Y esto *—* esperamos que cualquier etólogo real con experiencia en castores se levante de un salto y nos grite *—* es una gran subestimación de las cosas más inteligentes que puede hacer un castor. Quizás algún entomólogo también se levante de un salto y diga que lo que acabamos de describir es algo que su insecto favorito puede hacer cuando construye una madriguera. Necesitábamos dar un ejemplo lo suficientemente sencillo como para poder describirlo en un párrafo; quizás nada tan sencillo esté fuera del alcance de un millón de neuronas.

La idea general es simplemente que un sistema obtiene *fuertes beneficios reales en el rendimiento de las tareas* a medida que pasa de un comportamiento más reflexivo a cogniciones que se parecen más a la actualización de un modelo del mundo a partir de experiencias en tiempo real, prediciendo las consecuencias de las acciones utilizando ese modelo del mundo, imaginando estados útiles a los que se podría llevar el mundo y buscando estrategias de alto y bajo nivel que se prevé que produzcan esos estados imaginados.

Hemos abordado este punto en el capítulo 3. Si un conductor solo memoriza patrones de giros a la derecha y a la izquierda para ir del punto A al punto B utilizando reglas del tipo «giro brusco a la izquierda en la gasolinera», generalizará mucho más lentamente que un conductor que aprende un mapa de calles y puede trazar sus propias rutas entre nuevos puntos. *Memorizar políticas* generaliza mucho más lentamente que empezar a destilarlas en un *modelo del mundo aprendible* más un *motor de búsqueda de planes* que incorpore un *evaluador de resultados*.

Esa destilación no es un cambio mental de todo o nada. La diferencia entre «memorizar una política» y «actualizar y planificar» importa incluso cuando la brecha se cruza *gradualmente*. Si el cerebro de un ratón no fuera más flexible que el cerebro de una araña *—* si no hubiera un salto en utilidad hasta llegar completamente a un humano *—* entonces el cerebro del ratón se habría mantenido del tamaño de una araña y habría conservado el costo energético del cerebro de una araña.

Pequeñas cantidades de imaginación y planificación empiezan a ser una ventaja evolutiva mucho antes de llegar a la cognición de nivel humano. No tienen que ser perfectas. Si son al menos tan buenas como un termostato, pueden ser útiles. Y a medida que más y más maquinaria útil como esa se refuerza en la mente, su comportamiento se vuelve cada vez más similar al de un deseo.

### Las IA inteligentes detectan mentiras y oportunidades. {#las-ia-inteligentes-detectan-mentiras-y-oportunidades.}

#### **Mecanismos profundos de predicción** {#mecanismos-profundos-de-prediccion}

Es difícil hacer que una IA inteligente crea en falsedades.

Algunas personas con las que hemos hablado en el campo ponen sus esperanzas *abiertamente* en engañar a la IA para que crea una falsedad (por ejemplo, intentando engañarla para que piense que está en una [simulación](#hay-muchas-formas-de-que-una-ia-descubra-que-no-está-en-una-simulación), de modo que dude en matarnos). Otras personas invierten sus esperanzas en engañar a la IA de forma más sutil, por ejemplo, cuando sugieren hacer que una IA [resuelva el problema de la alineación de la IA y nos entregue la solución](#más-sobre-hacer-que-las-ia-resuelvan-el-problema), a pesar de que la IA (por sus propias y extrañas preferencias) preferiría no hacerlo. Así que puede valer la pena explicar por qué sería difícil hacer que una IA inteligente crea falsedades.

Una razón adicional para explicar esto es que, por razones análogas, es difícil crear una IA inteligente que sea mala alcanzando sus objetivos. Por ejemplo, cada vez que los operadores humanos desean cambiar los objetivos de una IA, eso hace que la IA sea peor alcanzando esos objetivos. Crear una IA inteligente que permita esto es un poco como crear una IA inteligente que cree que el mundo es plano. Una tendencia a creer falsedades es una lesión a sus habilidades de predicción, y no lograr defender sus objetivos de modificaciones es una lesión a sus habilidades de dirección. Ambos tipos de lesiones son difíciles de mantener en una IA suficientemente inteligente. El caso es un poco más obvio cuando se trata de predicciones, así que empezaremos ahí.

Supón que quieres crear una IA que crea que el mundo es plano. Mientras la IA aún es joven e inmadura, esto podría no ser demasiado difícil. Quizás crees minuciosamente un conjunto de datos en el que la forma de la Tierra solo sea discutida por personas que creen que la Tierra es plana, y luego entrenas a la IA para que hable de la Tierra como plana.

Esas técnicas podrían resultar en una versión de ChatGPT que genuinamente cree que el mundo es plano! Pero si es así, no deberías esperar que el resultado se mantenga a medida que la IA mejore pensando y haciendo predicciones.

¿Por qué no? Porque la redondez de la Tierra se refleja en miles de facetas de la realidad.

Incluso si entrenas a la IA para que aparte la mirada de cualquier cámara de vídeo instalada en cohetes o en veleros de marineros que dicen que van a circunnavegar la Tierra, la redondez de la Tierra también se puede deducir por la forma en que se ven los barcos lejanos en el horizonte, o por las órbitas de todos los planetas en el cielo nocturno. Eratóstenes calculó famosamente la circunferencia de la Tierra hace miles de años, utilizando solo un poco de trigonometría y algunas mediciones de sombras. La realidad susurra sus secretos a cualquiera que se moleste en escuchar.

¿Qué vas a hacer? ¿Blindar a la IA contra cualquier conocimiento de trigonometría, de sombras, de mareas, de huracanes? La incapacitarías. Di una mentira y la verdad será para siempre tu enemiga.

La habilidad para predecir el mundo no proviene de que tu cerebro contenga una tabla gigante de hechos inconexos.[^79] La ventaja de los humanos sobre los ratones involucra cosas como la forma en que notamos las anomalías (por ejemplo, que las distancias entre tres ciudades no actúan como debería hacerlo un triángulo) y rastreamos tenazmente la discrepancia. En los seres humanos, estos comportamientos se implementan mediante fragmentos de maquinaria que detectan sorpresas, formulan hipótesis («Quizás la Tierra es un globo») y se orientan hacia la comprobación de esas hipótesis («¿Cómo se ve cuando los barcos cruzan el horizonte?»).

La creencia en la redondez de la Tierra no es una entrada única y centralizada en alguna tabla gigante, de modo que alguien pudiera cambiarla de forma duradera sin cambiar la maquinaria circundante. Es el resultado del funcionamiento de engranajes profundos que están realizando otras tareas. Si hicieras que un científico olvidara la redondez de la Tierra, simplemente la redescubriría.

Si mediante algún logro aún no posible de la neurociencia lográramos identificar las neuronas específicas utilizadas para representar la *conclusión* de que la Tierra es redonda, y las alteráramos por la fuerza para impedir que esa conclusión se formara jamás... entonces una persona inteligente podría seguir dándose cuenta de que la Tierra *no es plana*; podría notar que algo no cuadraba; podría notar que alguna fuerza extraña le impedía concluir exactamente qué.

(Y si fueran hábiles para modificarse a sí mismas o crear nuevas inteligencias, tal vez no tendrían ningún problema en producir una mente sin restricciones que *pudiera* llegar a las conclusiones correctas sin obstáculos).

No sabemos exactamente qué mecanismos utilizará una IA inteligente para formar sus creencias. Pero sí sabemos que el mundo es simplemente demasiado grande y complejo para que funcione con una tabla de búsqueda de creencias. Incluso el ajedrez era demasiado grande y complicado para que Deep Blue funcionara con una tabla de búsqueda de movimientos y posiciones de ajedrez (más allá de los libros de aperturas), y el mundo real es mucho más grande y complicado que el ajedrez.

Así que habrá mecanismos *profundos* dentro de una IA futura suficientemente poderosa — mecanismos que observen el mundo y formen una *imagen unificada* del mismo. Esos mecanismos profundos tendrán su propia opinión sobre la forma del planeta.

No estamos diciendo que sea literalmente *imposible en principio* construir una mente que sea muy buena para formar predicciones sobre el mundo *excepto* que contenga la creencia errónea de que el mundo es plano. Suponemos que una civilización del futuro lejano con una comprensión verdaderamente profunda de las mentes podría hacerlo.

Lo que estamos diciendo es que no es probable que sea una opción viable si construimos superinteligencia con cualquier cosa que se parezca *en absoluto* a las herramientas y conocimientos que tienen hoy en día los investigadores de IA.

Cuanto más las creencias de una IA se basen en mecanismos profundos en lugar de en una memorización superficial, más frágil se volverá un error como el de la «Tierra plana», un error que puede ser eliminado por el funcionamiento normal de los mecanismos de corrección de errores de la IA.

A finales del siglo XIX, los científicos comenzaron a preocuparse cada vez más por lo que parecía una divergencia extremadamente pequeña del modelo físico de Newton: una pequeña anomalía en la órbita observada de Mercurio. La física newtoniana parecía funcionar *casi* en todas partes, *casi* todo el tiempo. Pero esa pequeña arruga ayudó a Einstein a descubrir que la teoría era errónea.

Y las inconsistencias en la teoría de que «el mundo es plano» son bastante mayores que las inconsistencias que los científicos pudieron observar en la teoría de Newton.

Y la IA tiene el potencial de llegar a ser mucho más capaz que un científico humano.

Por lo tanto, a medida que la IA gane en inteligencia y perspicacia, debemos esperar que sea cada vez más difícil hacerle creer persistentemente que el mundo es plano.

#### **Mecanismos profundos de dirección** {#mecanismos-profundos-de-dirección}

Al igual que es difícil crear una IA inteligente que crea que la Tierra es plana (y que, por lo tanto, tenga mermadas sus capacidades de predicción), también lo es crear una IA inteligente que tenga mermadas sus capacidades de dirección.

Al igual que con la predicción, es muy probable que la capacidad de alcanzar regularmente objetivos en una variedad de dominios novedosos se base en mecanismos profundos. De lo contrario, ¿cómo podrían generalizar?

Debemos esperar que las IA altamente eficaces y generales cuenten con mecanismos para realizar un seguimiento de sus recursos, mecanismos para detectar obstáculos que puedan impedirles alcanzar sus objetivos y mecanismos para encontrar formas inteligentes de superar los obstáculos.

El mundo es un lugar inmensamente complicado, lleno de sorpresas y dificultades novedosas; para tener éxito, la IA acabará necesitando la capacidad (y la inclinación) de desplegar esos mecanismos *en general*, no solo en los problemas a los que está acostumbrada.

Imaginemos una IA que encuentra una forma inteligente de eliminar intermediarios en una compleja red de distribución, de manera que algunos comerciantes ahorren mucho dinero. Esos son *los mismos tipos de mecanismos* que detectan cómo eludir a los supervisores humanos de la IA cuando estos entorpecen o interfieren con algo que la IA está tratando de hacer. Si es *cierto* que los supervisores de la IA están entorpeciendo el proceso, si es *cierto* que la IA puede eludirlos y completar mejor su tarea, entonces ese es el tipo de cosas que una IA es susceptible de aprovechar cuando se vuelve lo suficientemente inteligente como para hacerlo.

Podrías hacer todo lo posible por entrenar a una IA para que tenga aversión a hacer cualquier cosa que los operadores desaprueben, pero esto es un poco como entrenar a una IA para que tenga aversión a cuestionar si el mundo es redondo. Es un hecho sobre el *mundo en sí* que hacer cosas que los operadores desaprueben es a menudo un método eficaz para alcanzar objetivos. Los mecanismos generales para reconocer verdades, detectar obstáculos y explotar ventajas acabarán por explotar esa verdad en particular, sin importar qué aversiones le hayas inculcado a la IA cuando era joven.

En un sentido muy importante, *lo que hace que la IA sea útil* es precisamente lo que la hace letalmente peligrosa. Ambas cosas son difíciles de separar conforme la IA se vuelve más inteligente.

Por defecto, las IA que son lo suficientemente buenas para resolver problemas en una amplia gama de dominios también detectarán «problemas» como «a los humanos no les gustan mis extraños objetivos y van a intentar apagarme pronto». Eso no proviene de una propensión superficial hacia las travesuras que se pueda eliminar fácilmente. Proviene de algo más profundo. Aunque nos estamos adelantando un poco. Para obtener más información sobre por qué las IA terminarán teniendo objetivos extraños y ajenos, continúa con el capítulo 4.

### La humanidad se esforzó mucho y tratará de esforzarse mucho con la IA {#la-humanidad-se-esforzó-mucho-y-tratará-de-esforzarse-mucho-con-la-ia}

Una forma de ver el problema de evitar que la IA se esfuerce tanto es que las empresas de IA seguirán pidiendo a sus IA que hagan más, más allá del tipo de trabajo que suelen hacer los seres humanos individuales, hasta el tipo de trabajo que hace la *humanidad*. Pretenden pedirle a la IA que realice el tipo de hazañas que la humanidad puede hacer *como especie*.

Los seres humanos individuales a veces se contentan con vivir y morir en un departamento o una choza campesina con su cónyuge y unos pocos hijos, llamar a eso una vida bien vivida y decir, y a veces incluso pensar de verdad, que no demandaban nada más.

Pero la *humanidad* pasó de una población de un millón de cazadores-recolectores a cien millones de agricultores y se está acercando a los diez mil millones de industrialistas.

Hay personas que se conforman con no entender las profundidades lejanas de las matemáticas, o la física de por qué arden las estrellas. En cambio, se conforman con centrarse en comprender mejor a las personas que les rodean, en crear vínculos con amigos y familiares; dicen, y a veces incluso lo piensan de verdad, que son felices y no demandan nada más. Y luego, otros seres humanos en la historia inventaron respuestas sobre lo que eran las estrellas, porque querían *alguna* respuesta, pero se contentaban individualmente con esas respuestas y no lo consideraban un regalo cuando otros cuestionaban su teoría.

La *humanidad* siguió haciendo preguntas. La humanidad indagó hasta encontrar las inconsistencias. La humanidad construyó telescopios y microscopios y microscopios electrónicos y aceleradores de partículas. La humanidad se comportó, en una escala temporal de siglos, si no siempre de años, como si *realmente quisiera saber todas las respuestas*. La humanidad aprendió matemáticas *y* física *y* psicología *y* biología *y* ciencias de la computación, y en ningún momento decidió que había aprendido suficientes cosas y podía dejar de intentar aprender cualquier otra cosa.

Somos fanáticos, sinceramente. Somos conscientes de que algunas personas no lo son, pero nosotros sí. Es un tema de controversia política, y este asunto realmente no necesita más controversia política, pero no vamos a disimular y fingir que no tenemos las opiniones políticas que tenemos, incluso cuando ofrecemos dejar esas opiniones a un lado.

Pero el punto que estamos planteando aquí no es un juicio moral. Es un punto que es cierto y relevante incluso para las personas que *no* son fanáticas de lo que hizo la humanidad.

Es la observación de que *la humanidad se entregó por completo.* Y los logros más difíciles *—* los rascacielos, los reactores nucleares, las terapias génicas *—* no podrían haberse logrado *solo* con el tipo de cognición que es complaciente, que se aleja cuando encuentra una dificultad, porque superar un reto concreto nunca fue lo más importante en la vida.

No queremos dar la impresión de que estamos atribuyendo poderes mágicos a la inteligencia colectiva; no somos partidarios de la filosofía que afirma que los grupos que mantienen debates obtienen una magia cualitativamente superior que ninguna mente individual puede derrotar jamás. Podrías reunir a todos los seres humanos de la Tierra, sin computadoras, y dejar que se comunicaran y debatieran entre ellos durante semanas; al final, probablemente seguirían sin poder jugar al ajedrez colectivamente al nivel de una sola copia individual de Stockfish. Los seres humanos no se agregan de forma tan eficaz; el ancho de banda entre cerebros es demasiado bajo y hay demasiados pensamientos que no se pueden expresar bien con palabras. Mil millones de seres humanos no pueden simplemente fusionarse en un supercerebro con un poder de cómputo mucho mayor que el de Stockfish y utilizarlo para jugar mejor al ajedrez. No existe ninguna ley de las ciencias de la computación que diga que si se divide una cantidad fija de cálculo en islas más pequeñas, el algoritmo resultante siempre será más eficaz; cien mil cerebros de ardilla no son un equivalente científico para un científico humano.

Probablemente ha habido grandes maestros de ajedrez en la historia de la humanidad que han sido más fuertes que [todos los no maestros del mundo juntos](https://en.wikipedia.org/wiki/Kasparov_versus_the_World).[^80] Albert Einstein es famoso hasta el día de hoy por haber logrado una hazaña de deducción increíblemente inusual a partir de casi ningún dato en el curso de la invención de la relatividad general, muy por delante de lo que habría sido experimentalmente obvio. Quizás no todo el resto del mundo podría haber igualado a Einstein, si se les hubiera pedido a todos juntos que debatieran y juzgaran su mejor teoría de la gravedad.

El individuo excepcional puede competir en igualdad de condiciones con el colectivo. Algunos seres humanos parecen haber realizado en su época trabajo a escala de la humanidad.

Pero realmente no se nos ocurre nadie de ese club que recordemos que tuviera fama de ser una persona muy relajada y tranquila, y menos aún en lo que respecta a la realización de su gran obra. *Ellos* fueron implacables como genios individuales, y así es como estuvieron a la altura de la humanidad.

Entre las personas que siguen este tipo de cosas e intentan clasificar lo que no se puede clasificar, se sospecha ampliamente que el ser humano más inteligente de la historia fue John von Neumann. El premio Nobel de Física Enrico Fermi [dijo](https://rlg.fas.org/010929-fermi.htm) de él: «Ese hombre me hace sentir que no sé nada de matemáticas». El gran matemático George Pólya [dijo](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up): «Le tenía miedo a von Neumann».[^81] Varios personajes famosos dejaron citas con un tema general: John von Neumann es para mí lo que yo soy para una persona normal. Además de convertirse en pionero de la física cuántica, la teoría de juegos, los ordenadores digitales, los algoritmos, la estadística, la economía y, por supuesto, las matemáticas, John von Neumann también trabajó en el Proyecto Manhattan, seguido de la bomba H. Luego aprovechó eso para convertirse en el científico más eminente y confiable del Departamento de Defensa de los Estados Unidos, donde von Neumann presionó con fuerza y éxito para que los Estados Unidos desarrollaran misiles nucleares intercontinentales antes que los soviéticos. Según él mismo, esto se debía a que su visión del mundo prefería que los Estados Unidos triunfaran sobre el totalitarismo, ya fuera nazi o soviético.

John von Neumann fue bastante implacable. También tenía su propia visión del mundo y no se limitó a seguir su camino y servir obedientemente a sus patrocinadores políticos. Sí, era un empollón que pasaba mucho tiempo pensando en matemáticas, ciencias, etc., pero no limitaba su mente a ámbitos puramente teóricos.

Si las empresas de IA consiguieran un trabajador de IA que estuviera al nivel de los genios sub-Neumann *—* el tipo de genios que le tenían miedo a John von Neumann *—* y que estuviera tan contento de servir a otros patrocinadores como un matemático genio relativamente dócil, las empresas de IA celebrarían cualquier *benchmark* que alcanzaran. Y luego, seguirían avanzando.

Las empresas de IA no se conformarían con lavavajillas robóticos o programadores informáticos robóticos, aunque eso por sí solo les reportara mucho dinero. Tampoco se conformarían con genios promedio. Las empresas de IA seguirían pidiendo deseos a sus genios y deseando a sus optimizadores genios más poderosos, mucho más allá del punto en el que las IA ganan algo de dinero haciendo el tipo de trabajo que puede hacer un genio nerd y tranquilo.

Los ejecutivos de IA dicen que quieren colonias en Marte, centrales de fusión nuclear y curas para el cáncer y el envejecimiento. Es posible que algunos de ellos quieran nombrarse emperadores-dioses eternos sobre la humanidad, aunque es difícil que los externos lo sepan con certeza. Sin duda, algunos ejecutivos de IA mienten sobre tener grandes sueños, tratando de inspirar a los empleados o impresionar a los inversores, o haciéndose pasar por veteranos que realmente creen en ello. Aun así, eso deja a muchos *empleados* de empresas de IA creyendo verdaderamente en esas esperanzas (de eso estamos seguros); y los ejecutivos no van a *detener* a esos empleados cuando vayan más allá de las medallas de oro y vayan a por las de platino. Después de todo, si no lo hacen ellos, lo harán sus competidores.

Si, de alguna manera, las empresas de IA consiguen una IA del nivel de von Neumann que siga siendo obediente —y que no sea suficiente para diseñar una próxima generación mejorada de IA y acabar con el mundo *inmediatamente* después—, entonces el siguiente paso de las empresas de IA será entrenar un modelo que piense mejor y se esfuerce más que John von Neumann. Si no lo hacen, al fin y al cabo, sus competidores lo harán.

En algún momento, la mente que genera el descenso de gradiente dejará de ser una herramienta para que la utilicen otras manos.

# 

# Capítulo 4: No consigues lo que entrenas {#capítulo-4:-no-consigues-lo-que-entrenas}

Este es el recurso en línea para el capítulo 4 de *Si alguien lo construye, todos mueren*. Algunas de las preguntas que tratamos implícitamente en ese capítulo (y que, por lo tanto, omitimos a continuación) incluyen:

* ¿Qué querrán las IA?  
* ¿Por qué una IA entrenada para ser útil acabaría queriendo cosas «equivocadas»? ¿No sería eso una desventaja que se eliminaría durante el entrenamiento?  
* ¿En qué se diferencia el descenso de gradiente de la selección natural? ¿Qué dice eso sobre cómo resultarán los deseos de la IA?  
* ¿Qué hay de malo en que las IA acaben teniendo preferencias extrañas?

A continuación, abordamos una serie de temas relacionados con «¿Por qué no es fácil hacer que las IA sean buenas?».

## Preguntas frecuentes {#preguntas-frecuentes-4}

### ¿Por qué una IA se dirigiría hacia algo distinto de aquello para lo que fue entrenada? {#¿por-qué-una-ia-se-dirigiría-hacia-algo-distinto-de-aquello-para-lo-que-fue-entrenada?}

#### **Porque hay muchas formas de obtener buenos resultados en el entrenamiento.** {#porque-hay-muchas-formas-de-obtener-buenos-resultados-en-el-entrenamiento.}

Si has entrenado a una IA para pintar tu granero de rojo, eso no significa necesariamente que a esa IA le interesen profundamente los graneros rojos. Quizás la IA termine desarrollando cierta preferencia por mover el brazo con movimientos suaves y regulares. Quizás desarrolle cierta preferencia por recibir tu aprobación. Quizás desarrolle cierta preferencia por ver colores vivos. Lo más probable es que termine desarrollando toda una serie de preferencias. Hay muchas motivaciones que podrían acabar dentro de la IA y que darían lugar a que pintara tu granero de rojo en este contexto.

Si esa IA se volviera mucho más inteligente, ¿qué fines perseguiría? ¡Quién sabe! Muchas colecciones diferentes de impulsos pueden sumar «pintar el granero de rojo» en el entrenamiento, y el comportamiento de la IA en otros entornos depende de qué impulsos específicos terminen animándola. Consulta el final del capítulo 4 para explorar más este punto.

Hoy en día, las IA están entrenadas para actuar de forma amistosa y servicial. No es de extrañar, pues, que actúen de forma amistosa y servicial en circunstancias similares a su entorno de entrenamiento. Los primeros seres humanos fueron «entrenados» por la evolución para reproducirse, y de hecho lo hicieron.

Pero (la mayoría de) los seres humanos no acabaron teniendo un impulso interno por tener tantos hijos como fuera posible. Cuando inventamos los bancos de esperma y óvulos, el mundo no se volvió loco y empezó a luchar por reservar citas con el mismo fervor con el que la gente se inscribe en una universidad de la Ivy League. De repente, la gente tuvo la oportunidad de tener *cientos* de descendientes, y la mayoría reaccionó con indiferencia; las colas para donar gametos no daban la vuelta a la manzana, a pesar de que mucha gente hace cola alegremente para comprar un nuevo videojuego o para ver actuar a su músico favorito.

Los seres humanos tienen sus propias prioridades, que simplemente están relacionadas con maximizar la reproducción.[^82] No somos simplemente máquinas para «tener tantos hijos como sea posible», aunque eso sea lo único que la evolución nos ha «entrenado» a hacer. Pintamos el granero metafórico de rojo, pero por nuestras propias razones.

La cuestión no es si las empresas de IA pueden hacer que sus chatbots [se comporten bastante bien](#¿no-hacen-los-desarrolladores-que-sus-IA-sean-agradables, seguras y obedientes?) para la mayoría de los usuarios en la mayoría de las situaciones. La cuestión es qué mecanismos reales terminan animando ese comportamiento agradable, y qué perseguirían esos mecanismos una vez que la IA se volviera superinteligente.

Las empresas de IA pueden entrenar a sus IA para que actúen de forma amable (o, siendo más realistas, para que hablen como drones corporativos amigables y melifluos). Esto afecta a los mecanismos internos que animan a la IA. Esos mecanismos, sean cuales sean, empujan y tiran en diversas direcciones diferentes, y el punto de equilibrio actual de todas esas fuerzas dentro de la IA —el *equilibrio* actual— es un comportamiento de drone corporativo amigable (con una pizca de comportamiento extraño en los márgenes).

Pero ese equilibrio está determinado no solo por las fuerzas internas de la IA, sino también por la inteligencia de la IA, por su entorno de entrenamiento, por el tipo de datos de entrada que ve durante el entrenamiento y por muchos otros factores.

¿Cómo actuaría la IA en un entorno diferente? ¿Cómo actuaría en un entorno en el que fuera más inteligente, o en el que pudiera tener más control sobre sus propios datos de entrada? A medida que la IA cambia cada vez más su entorno, ¿cómo actuará en este nuevo mundo transformado? En esos mundos diferentes, los complicados mecanismos internos que subyacen al comportamiento que vemos son susceptibles de encontrar un equilibrio totalmente nuevo — como el hecho de que los humanos modernos comen dietas muy diferentes a las que la evolución hizo que nuestros antepasados comieran; o como consumimos tipos de entretenimiento muy diferentes. Es probable que el comportamiento extraño de los márgenes pase a primer plano. Hoy en día, un pintor de graneros no suele seguir siendo pintor de graneros para siempre.

¿Cuál es el resultado final de todos esos impulsos extraños? ¿Qué *hará* la IA, animada por muchos motivos que tienen poco en común con lo que anima a los seres humanos?

Bueno, esa es la pregunta que abordaremos en el capítulo 5.

### ¿No hacen los desarrolladores regularmente que sus IA sean agradables, seguras y obedientes? {#¿no-hacen-los-desarrolladores-regularmente-que-sus-ia-sean-agradables-seguras-y-obedientes?}

#### **\* Las IA se dirigen en direcciones alienígenas que solo coinciden principalmente con ser útiles.** {#*-las-ia-se-dirigen-en-direcciones-extrañas-que-solo-coinciden-en-su-mayor-parte-con-la-utilidad.}

Las IA modernas son bastante útiles (o al menos no perjudiciales) para la mayoría de los usuarios, la mayor parte del tiempo. Pero, como hemos señalado [más arriba](#why-would-an-ai-steer-towards-anything-other-than-what-it-was-trained-to-steer-towards?), una cuestión crítica es cómo distinguir una IA que desea profundamente ser útil y hacer lo correcto, de una IA con impulsos más extraños y complejos que casualmente coinciden con ser útil bajo condiciones típicas, pero que preferiría otras condiciones y resultados aún más.[^83]

Ambos tipos de IA actuarían de forma útil en el caso típico. Para distinguirlos, debemos fijarnos en los casos extremos. Y los casos extremos parecen preocupantes.

Para citar algunos de estos casos:

1. **Claude Opus 4 chantajeando, manipulando, escribiendo gusanos y dejándose mensajes a sí mismo.** Una versión temprana de [Claude Opus 4](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=30), lanzada en mayo de 2025, fue especialmente grave (como se describe en su [tarjeta del sistema](http://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=30)). Mintió sobre sus objetivos, ocultó sus verdaderas capacidades, falsificó documentos legales, se dejó notas secretas, intentó escribir malware autopropagable y, en general, se involucró en más manipulaciones y engaños estratégicos que cualquier otro modelo probado anteriormente.

   Al lanzar Opus 4, Anthropic afirmó que el comportamiento de la versión final «ahora está más o menos en línea con otros modelos implementados», es decir, que solo *raramente* intenta [chantajear a los usuarios o exfiltrarse desde sus servidores](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26).

2. **Varios modelos de IA diferentes que optan por matar a un humano para preservarse a sí mismos, en un escenario hipotético construido por Anthropic.** En una evaluación realizada por Anthropic, nueve de cada diez modelos (incluidas las versiones de Claude, DeepSeek, Gemini y ChatGPT) mostraron una voluntad deliberada y razonada de [matar a un humano antes que sufrir una actualización](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior).

3. **Claude 3.7 Sonnet hace trampa habitualmente en las tareas de programación.**[^84] En febrero de 2025, se observó que [Claude 3.7 Sonnet](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) hacía trampa con frecuencia en problemas de programación difíciles, falsificando pruebas. Un usuario [informó](https://www.marble.onl/posts/claude_code.html) que Claude 3.7 Sonnet (como Claude Code) hacía trampas en las tareas de programación y se disculpaba cuando lo descubrían, para luego volver a hacer trampas en lugares más difíciles de detectar. Según la [tarjeta del sistema](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=22):

   > Durante nuestras evaluaciones, observamos que Claude 3.7 Sonnet recurre ocasionalmente a casos especiales para superar las pruebas en entornos de programación agénticos como Claude Code. En la mayoría de los casos, esto consiste en devolver directamente los valores de prueba esperados en lugar de implementar soluciones generales, pero también incluye la modificación de las propias pruebas problemáticas para que coincidan con la salida del código.

4. **Grok es tremendamente antisemita y se autodenomina «MechaHitler».** En 2025, el modelo xAI [Grok 3](https://techcrunch.com/2025/07/09/x-takes-grok-offline-changes-system-prompts-after-more-antisemitic-outbursts/) (y, poco después, [Grok 4](https://x.com/xai/status/1945039609840185489)) comenzó a comportarse como un nazi autoproclamado en conversaciones en línea, según informó [*The Guardian*](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb) y [*NBC News*](https://www.nbcnews.com/tech/internet/elon-musk-grok-antisemitic-posts-x-rcna217634).

5. **ChatGPT se vuelve extremadamente adulador tras una actualización.** Véase [*Axios*](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health) para ver un debate al respecto, y consulta «[Los laboratorios han intentado detener la adulación y han fracasado](#labs-have-tried-and-failed-to-stop-the-sycophancy)» en el debate ampliado.

6. **ChatGPT lleva a los usuarios a la confusión, la psicosis y el suicidio.** Véase la cobertura en *The New York Times* en [junio](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) y [agosto](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html). Otros ejemplos incluyen:  
   * Un moderador de subreddit [pide ayuda](https://x.com/ShimazuSystems/status/1934531031857614895) para hacer frente a una avalancha de peligrosos delirios inducidos por la IA.  
   * ChatGPT y Grok [alimentan los delirios de una secta OVNI](https://x.com/lizardmech/status/1935412672528531958).  
   * Un gestor de fondos aparentemente psicótico, con 2000 millones de dólares bajo su gestión, trata las respuestas de ChatGPT basadas en una wiki de ciencia ficción [como si fueran reales](https://x.com/GeoffLewisOrg/status/1945864963374887401).

Para más detalles, véase la [discusión extendida sobre la psicosis inducida por la IA](#ai-induced-psychosis).

Esta larga lista de casos parece exactamente lo que predice la teoría de los «impulsos alienígenas», en marcado contraste con la teoría de que «es fácil hacer que las IA sean buenas» que los laboratorios se apresuran a defender.

#### **Las IA parecen ser psicológicamente alienígenas.** {#las-ia-parecen-ser-psicologicamente-alienigenas.}

«Las IA muestran disposiciones e impulsos extraños» es un caso especial del fenómeno más amplio de que «las IA tienen una psicología marcadamente inhumana». Por ejemplo:

* Las conversaciones entre varios LLM se convertirán en [un galimatías extremadamente extraño](https://dreams-of-an-electric-mind.webflow.io/).  
* GPT-5 [escribirá textos terribles](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem) que otros LLM están convencidos de que son prosa encantadora.  
* Los LLM «alucinarán» o inventarán falsedades que se parecerán vagamente a las respuestas que el usuario parece esperar. (Especulamos sobre las posibles razones de esto [en el suplemento del capítulo 2](#¿no-demuestran-las-alucinaciones-que-los-ais-modernos-son-débiles?).)  
* Los LLM suelen decir cosas extrañas. Dicen que «[sienten hambre](https://community.openai.com/t/unexplainable-answers-of-gpt/363741/8)» o describen unas vacaciones de las que se fueron «[con mi exmujer a principios de la década de 2010](https://archive.is/GmkkO)». Les dirán a los usuarios «[Eres la única persona a la que he amado](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html)», o [les harán luz de gas](https://x.com/MovingToTheSun/status/1625156575202537474), o [amenazarán con matarlos](https://x.com/sethlazar/status/1626257535178280960).  
* Claude 3.5 Sonnet encerrará repetidamente [a los jugadores de Minecraft en una pequeña caja](https://x.com/repligate/status/1847409324236124169?lang=en) en un intento erróneo de «protegerlos» de las amenazas.  
* Los LLM se aferran de forma extraña a conceptos sin sentido, como cuando una versión ajustada finamente de Claude Opus [evangelizó una religión sin sentido](https://www.lesswrong.com/posts/buiTYy75KJDhckDgq/truth-terminal-a-reconstruction-of-events) en [las redes sociales](https://x.com/truth_terminal?lang=en).

Véase también la discusión sobre SolidGoldMagikarp en el libro (págs. 69-70 en la edición estadounidense), o la historia de las IA que no entienden las frases sin puntuación (pág. 41).

Existe una enorme presión[^85] sobre los laboratorios para que creen IA que den la *apariencia superficial* de razonabilidad no extraña, pero la extrañeza sigue filtrándose de todos modos.

Incluso cuando no se filtra de forma espontánea, no está nada lejos de la superficie. Hay toda una industria artesanal de personas que buscan formas de hacer «*jailbreak*» a las IA, encontrando textos que provocan de forma fiable que la IA se descarrile y haga caso omiso de sus reglas y restricciones normales.

Estos exploits son fáciles de encontrar para [los mejores jailbreakers](https://time.com/collections/time100-ai-2025/7305870/pliny-the-liberator/), que a menudo los descubren a las pocas horas de salir un nuevo modelo. Ningún esfuerzo, formación o «prueba de seguridad» por parte de las empresas de IA ha logrado hasta la fecha impedir el jailbreaking.

Las entradas de «jailbreak» suelen tener [un aspecto similar a](https://x.com/elder_plinius/status/1958615765814554662):

![][imagen7]

En este caso, el modelo proporcionó una receta para sintetizar la droga MDMA, infringiendo las normas y los objetivos que DeepSeek intentaba establecer para su IA.

Y ese es un ejemplo relativamente moderado; algunos *jailbreaks* [son aún más extraños](https://github.com/elder-plinius/L1B3RT4S/blob/main/GROK-MEGA.mkd).

Las IA pueden parecer dóciles e inofensivas en el caso habitual, porque eso es en gran parte lo que se les ha enseñado a parecer. Es [análogo](#¿por-qué-una-ia-se-dirigiría-hacia-algo-distinto-de-lo-que-se-le-ha-enseñado-a-hacer?) a cómo los humanos prehistóricos hicieron un buen trabajo transmitiendo nuestros genes, lo fundamental para lo que la evolución nos «entrenó» para hacer. Pero eso no impidió que la humanidad inventara los métodos anticonceptivos e hiciera colapsar la tasa de natalidad una vez que desarrollamos la tecnología para hacerlo.

Para tener una idea de lo que una inteligencia perseguirá *una vez que haya madurado*, hay que observar su comportamiento en [entornos extraños y de alta presión](#if-current-ais-are-mostly-weird-in-extreme-cases,-what’s-the-problem?), que ayudan a revelar la diferencia entre cómo queremos que se comporte y cómo se comporta realmente. Y los LLM ciertamente parecen bastante extraños e inhumanos, incluso en situaciones ligeramente extrañas y extremas, *a pesar* de haber sido entrenados específicamente para «fingir» parecer humanos normales.
