### ¿No tendría que crecer la IA hasta convertirse en toda una civilización antes de poder ser peligrosa? {#¿no-tendría-que-crecer-la-ia-hasta-convertirse-en-toda-una-civilización-antes-de-poder-ser-peligrosa?}

#### **Con las computadoras, lo difícil es conseguir que resuelvan un problema concreto. El gran volumen y la velocidad llegan poco después.** {#con-las-computadoras,-lo-difícil-es-conseguir-que-resuelvan-un-problema-concreto.-el-gran-volumen-y-la-velocidad-llegan-poco-después.}

«Para conquistar el mundo, necesitáis una civilización» es una intuición que tiene sentido para los humanos. No está tan claro hasta qué punto esta idea se puede generalizar a la IA. Las IA no funcionan como los humanos: pueden ser mucho más capaces que cualquier humano, y una instancia de IA no es necesariamente comparable a una sola persona.

También vale la pena tener en cuenta que la superinteligencia es precisamente el tipo de cosa que puede acabar con un análogo de toda una civilización de forma extremadamente rápida.

Con la mayoría de las hazañas que pueden realizar las computadoras, no se tarda mucho en pasar de «las computadoras pueden hacer esto» a «las computadoras pueden hacer esto a una escala enorme, mucho más rápido que cualquier humano». Piensa, por ejemplo, en las calculadoras.

Hubo años en los que solo las computadoras de gama alta podían realizar tareas de reconocimiento de voz, procesamiento de vídeo o gráficos 3D en tiempo real, pero no fueron muchos esos años.

Las IA, al igual que el software tradicional, pueden copiarse rápidamente en tantas computadoras como haya disponibles. Y se pueden fabricar más computadoras a la velocidad de la industria.

Comparemos esta situación con la de los seres humanos. Crear y formar a un nuevo ser humano requiere recursos considerables y décadas de tiempo. Una vez que se dispone de una IA con un nivel de capacidad determinado, se puede copiar inmediatamente esa misma IA «adulta» y entrenada tantas veces como se desee, con un gasto mínimo.

En cierto sentido, toda una (pequeña) civilización de mentes de IA ya existe en el momento en que una empresa lanza un nuevo modelo a sus centros de datos y pone en marcha tantas instancias como sea necesario para satisfacer la demanda.[^175] Hoy en día, esas flotas de IA no funcionan todas en armonía. Pero las empresas sí utilizan [grupos de agentes paralelos](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&amp;t=348) cuando buscan el máximo rendimiento a cualquier precio.

Todo esto significa que probablemente no pasará mucho tiempo entre el momento en que las IA sean lo suficientemente inteligentes como para tomar el control si tuvieran un millón de instancias y el momento en que las IA tengan al menos ese número de instancias en funcionamiento. El tipo de crecimiento demográfico que a los humanos les lleva cientos de años puede producirse en cuestión de minutos con la IA.

En lo que respecta a la infraestructura física de la civilización, suponemos que la IA puede aprovechar de forma productiva la infraestructura humana durante el tiempo que sea necesario para desarrollar medios más avanzados de reorganizar la materia según sus preferencias. No necesita averiguar cómo fabricar su propia cadena de suministro y poder de cómputo desde cero cuando puede utilizar nuestras computadoras. No necesita inventar máquinas industriales desde cero cuando puede simplemente tomar el control de las máquinas industriales que ya hemos construido. Y puede utilizar nuestra infraestructura para construir la siguiente fase de su propia infraestructura, utilizando los robots existentes para construir fábricas de robots nuevas y más eficientes, o utilizando los laboratorios de síntesis de ADN existentes para crear su propia biotecnología, hasta que sea completamente autosuficiente.

Los seres humanos somos el tipo de entidades que empezamos desnudos en la sabana y nos abrimos camino hasta alcanzar una civilización tecnológica. Y no somos *tan* inteligentes. No sería una hazaña tan difícil de replicar para una superinteligencia, especialmente si puede partir de la base industrial existente de la humanidad como punto de partida.

### ¿No estarán limitadas las IA por su capacidad para diseñar y realizar experimentos? {#won’t-ais-be-limited-by-their-ability-to-design-and-run-experiments?}

#### **La inteligencia te permite aprender más de los experimentos y realizar experimentos más rápidos, más informativos y más paralelizados.** {#la-inteligencia-te-permite-aprender-más-de-los-experimentos-y-realizar-experimentos-más-rápidos,-más-informativos-y-más-paralelizados.}

Una civilización de mentes motivadas que piensan mil veces más rápido que la humanidad no necesariamente sería capaz de producir datos de salida tecnológicos mil veces más rápido que los humanos.

Por analogía: si pasas tres horas haciendo la compra y dos de ellas las dedicas a ir y volver de la tienda a caballo, entonces un coche que sea diez veces más rápido que el caballo puede acelerar tu viaje de compras, pero no en un factor de diez. Al final, la última hora que pasas en la tienda domina la cantidad de tiempo empleado.

Incluso una civilización llena de pensadores increíblemente inteligentes debe esperar ocasionalmente a que lleguen los resultados de los experimentos. Si tus pensamientos son lo suficientemente rápidos, es probable que el cuello de botella sea la rapidez con la que puedes actuar en el mundo, la rapidez con la que puedes asimilar la información y el tiempo que tardan en ejecutarse tus planes.

Pero no es tan malo como podría hacerte creer la analogía de la tienda de comestibles, porque la capacidad de pensar se compensa con la necesidad de resultados experimentales:

* A menudo, basta con pensar más, pensar mejor y obviar la necesidad de una prueba, porque te das cuenta de que las observaciones anteriores ya contienen la respuesta. Compara la capacidad de las IA modernas para [aprender a pilotar robots](https://arxiv.org/abs/1905.00741) utilizando [simulación pura](https://www.figure.ai/news/aprendizaje-por-refuerzo-walking).  
* A veces puedes pensar más hasta encontrar una prueba igualmente fiable pero más rápida.  
* A veces puedes realizar muchas pruebas más rápidas pero menos fiables que se pueden ejecutar muchas veces en paralelo para obtener resultados igualmente fiables a una velocidad mayor.  
* A veces puedes realizar muchas pruebas complicadas a la vez, de modo que los datos sean complejos y difíciles de interpretar, lo cual es una buena compensación si la cognición necesaria para desentrañar los resultados es más barata (desde la perspectiva de una mente extremadamente rápida) que realizar múltiples pruebas.  
* A veces puedes encontrar una forma de construir otros dispositivos que realicen los experimentos mucho más rápido. Por ejemplo, en lugar de enviar muchas solicitudes diferentes a un laboratorio biológico para que sinteticen medicamentos, ¿puedes encontrar una forma de enviar *una* solicitud a un laboratorio biológico, lo que dará como resultado la síntesis de *una sola bacteria* que contenga el código genético para producir todos los medicamentos que deseas sintetizar? Del mismo modo, ¿puedes crear una bacteria que sea sensible a las señales de radio y responda rápidamente a las instrucciones de una IA de funcionamiento rápido, mucho más rápido que los humanos, que son terriblemente lentos y van y vienen según tus instrucciones?  
* Y, a veces, simplemente puedes tomar tus diez mejores conjeturas, averiguar qué harías en cada uno de esos casos, construir un dispositivo complicado que funcione independientemente de cómo sea la realidad y saltarte las pruebas por completo.

Una civilización llena de copias de Steve Jobs, Marie Curie, John von Neumann y algunos de los mejores trabajadores y programadores del mundo, si funcionaran a una velocidad 10 000 veces superior a la nuestra, *se darían cuenta* de que el principal cuello de botella era esperar los resultados experimentales, y podrían *trabajar en ese cuello de botella* para reducirlo.

La historia del [Proyecto Genoma Humano](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) es un buen ejemplo de lo que ocurre cuando seres humanos inteligentes se dan cuenta continuamente de los cuellos de botella que existen en un proyecto de investigación a gran escala y trabajan para solucionarlos. Lo que se esperaba que llevara quince años y 3000 millones de dólares se completó dos años antes y con 300 millones de dólares menos de lo presupuestado; la mayor parte del genoma se secuenció en los dos últimos años utilizando métodos y equipos mejorados.

Al igual que esto es válido para los seres humanos, también lo es para la IA. Un razonador inteligente no tiene por qué quedarse de brazos cruzados mientras espera durante años subjetivos a que las lentas pruebas lleguen a su fin. Un razonador sobrehumano *considera vías alternativas* y es experto en encontrarlas: en eso consiste la inteligencia.

Para obtener una pequeña evidencia práctica al respecto, consideremos el caso de los seres humanos que realizan experimentos. Un buen ejemplo es el caso del software frente a las sondas espaciales. Realizar cambios en un producto de software es barato y rápido, y los ingenieros de software tienden a experimentar constantemente, a producir software que aún no funciona del todo y luego arreglarlo donde más falla. Por el contrario, la experimentación es muy costosa en las sondas espaciales, por lo que los seres humanos dedican mucho tiempo a perfeccionar la sonda espacial y a incluir en ella tantos experimentos como sea posible. Se esfuerzan mucho por dotar a las sondas espaciales de *maquinaria experimental general* que pueda controlarse a distancia, de modo que, si se les ocurre una nueva idea para un experimento, no tengan que inventar y lanzar una nave espacial completamente nueva.

Y, además de todo esto, una persona con suficiente capacidad de razonamiento también tiene la opción de simplemente *descubrir cómo es la realidad sin necesidad de tantos experimentos*. A veces, los datos que ya tienes son suficientes, si eres lo suficientemente inteligente como para interpretarlos.

Como caso práctico: Einstein tardó ocho años en comprobar empíricamente su [teoría de la relatividad general](http://eotvos.dm.unipi.it/documents/EinsteinPapers/Einstein1911English.pdf) con nuevos datos. La prueba fue realizada por Frank Watson Dyson y Arthur Stanley Eddington, quienes [fotografiaron](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) las estrellas detrás del sol durante un eclipse solar total y midieron el grado en que la luz se curvaba alrededor del sol; descubrieron que coincidía exactamente con la teoría de Einstein.

Pero esa espera de ocho años no impidió ningún avance científico real.

![][image12]

\[ FUENTE DE LA IMAGEN: [https://en.wikipedia.org/wiki/Eddington\_experiment](https://en.wikipedia.org/wiki/Eddington_experiment) \]

Una de las razones es que la teoría de Einstein era claramente correcta: ya había sido validada con datos como el movimiento del perihelio de Mercurio, que la teoría de Newton había predicho de forma inexacta y la de Einstein, con precisión. Los científicos humanos no consideraron esta predicción como un acierto porque los datos se habían recopilado antes de que Einstein planteara su teoría, y querían validar las predicciones que esta hacía antes de ver los datos. Pero este es el tipo de apoyo que necesita una civilización cuando tiene graves problemas con el [sesgo retrospectivo](https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science), [sesgo de confirmación](https://en.wikipedia.org/wiki/Confirmation_bias) y científicos que hacen trampa para [exagerar la evidencia de sus hipótesis](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation). Ninguna de estas características es necesaria para un buen razonamiento. De hecho, los pensadores cuidadosos pudieron determinar si la teoría de Einstein era correcta mucho antes del experimento de Eddington, utilizando la evidencia de la que ya disponían.

Además, existían métodos más rápidos para comprobar la teoría, como construir telescopios y observar (los efectos de) los agujeros negros, tal y como predijo la teoría de Einstein, lo que presumiblemente podría haber hecho en menos de ocho años una civilización suficientemente rápida y competente. O si ya se disponía de capacidad para los vuelos espaciales, se podían comprobar los relojes de los satélites en menos de un día. Suponer que la teoría de Einstein *requería* ocho años para ser comprobada sería subestimar radicalmente el poder de la inteligencia.

Cuando la humanidad finalmente se decidió a construir satélites GPS, estos se programaron con dos relojes diferentes: uno que utilizaba la teoría de Einstein y otro que no. Fue una elección extraña, dado lo bien confirmada que estaba la teoría de Einstein en ese momento. Pero esta elección subraya el hecho de que, en muchos casos, una civilización puede simplemente *tomar ambas ramas* cuando no está segura de una teoría. Y subraya que, cuando los experimentos y los fracasos son caros (como en el caso de los satélites), a menudo es mucho más barato construir cosas de manera que no dependan demasiado de ninguna teoría en particular.

Y, como señalamos en el libro, Einstein (en comparación con Newton, Kepler y Brahe antes que él) es también un ejemplo de cómo las personas inteligentes pueden deducir mucho más de lo que cabría esperar a partir de observaciones muy limitadas. Einstein es impresionante no solo por haber descubierto la teoría de la relatividad, sino por haberlo hecho a partir de *tan pocos datos*.

Así que, aunque la necesidad de datos experimentales puede limitar la rapidez con la que la IA puede tomar diversas medidas, es probable que esta limitación sea mucho menor de lo que podría parecer intuitivamente.

## Debate ampliado {#extended-discussion-6}

### Nanotecnología y síntesis de proteínas {#nanotechnology-and-protein-synthesis}

La inteligencia humana nos ha proporcionado muchas ventajas sobre otras especies. Sin embargo, una de las más importantes ha sido nuestra capacidad para inventar nuevas tecnologías. Si los desarrolladores se adelantan y crean una IA más inteligente que los humanos, entonces podemos esperar que gran parte del poder de la IA provenga de su capacidad para avanzar en las fronteras científicas y tecnológicas. Pero, ¿cómo se traduce esto en la práctica? ¿Qué tecnologías aún por inventar están esperando a ser descubiertas?

Es una pregunta difícil de responder de forma general. A un científico de 1850 le habría resultado muy difícil adivinar muchos de los inventos de los siguientes cien años.

Sin embargo, no estarían totalmente indefensos. Los científicos han predicho muchos inventos décadas o siglos antes de que se construyeran, en casos en los que se podía razonar técnicamente sobre una tecnología antes de que los ingenieros pudieran poner todas las piezas en su sitio.[^176]

Una de las fronteras tecnológicas con impacto que creemos que la IA probablemente explorará es el desarrollo de herramientas y máquinas extremadamente pequeñas. A continuación, entraremos en algunos detalles sobre este tema y el razonamiento básico que lo sustenta.

#### **El ejemplo de la biología** {#el-ejemplo-de-la-biología}

Cada célula de cada organismo en la naturaleza contiene una enorme variedad de mecanismos complejos.

En este caso, «maquinaria» no es solo una metáfora. Las máquinas en cuestión son pequeñas, por lo que funcionan de forma ligeramente diferente a las máquinas que utilizas en tu vida cotidiana. Sin embargo, muchas máquinas a gran escala tienen equivalentes en nuestro organismo. La [ATP sintasa](https://en.wikipedia.org/wiki/ATP_synthase) genera energía en el organismo de forma similar a una rueda hidráulica, utilizando un flujo de protones para hacer girar un rotor literal.

\[incrustar vídeo o gif: [https://en.wikipedia.org/wiki/File:ATP\_synthesis\_-\_ATP\_synthase\_rotation.ogv](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv)\]

El flagelo bacteriano funciona de manera similar a la hélice de un barco, con un motor completo que hace girar el flagelo para propulsar a la bacteria a través de los líquidos:

\[embed vídeo: [https://www.youtube.com/watch?v=cwDRZGj2nnY](https://www.youtube.com/watch?v=cwDRZGj2nnY)\]

Otro ejemplo, que mencionamos en el libro, es la kinesina, una pequeña proteína que funciona como un robot de carga. Las kinesinas «caminan» por fibras autoensambladas que atraviesan las neuronas, transportando neurotransmisores a su destino.

\[embed vídeo o gif: [https://www.youtube.com/watch?v=y-uuk4Pr2i8](https://www.youtube.com/watch?v=y-uuk4Pr2i8)\]

Cuanto más pequeña es una máquina, más rápido puede funcionar por lo general; y las máquinas tan pequeñas como las moléculas funcionan muy rápidamente. Las kinesinas dan hasta [200 pasos por segundo](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004\(04\)00103-3), avanzando con un «pie» mientras el otro se aferra al microtúbulo en el que se encuentran.[^177]

Una de las fronteras tecnológicas que la IA más inteligente que los humanos puede explorar es la construcción, el diseño o la reutilización de máquinas a esta escala tan pequeña. Este tipo de tecnología podría clasificarse como «biotecnología», «nanotecnología» o algo intermedio, dependiendo de factores como la escala, el grado de similitud del diseño con las estructuras biológicas existentes y si es «húmeda» (dependiente del agua, como la maquinaria de las células vivas) o «seca» (capaz de funcionar al aire libre).

Pensar en los organismos biológicos como maravillas de la ingeniería a nanoescala puede ayudar a hacer conjeturas sobre lo que las IA más inteligentes que los humanos podrían ser capaces de lograr con una ciencia y una tecnología más avanzadas que las que poseemos hoy en día.

Hay otra cuestión aparte sobre cuánto tiempo llevaría inventar y madurar dicha tecnología. Para más información sobre este tema, véase el debate en el capítulo 1 del libro sobre cómo las superinteligencias artificiales probablemente serían capaces de pensar al menos 10 000 veces más rápido que los humanos con el hardware de computadora existente. Véase también nuestro análisis ampliado sobre cómo [las IA tendrían que dedicar algún tiempo a realizar pruebas y experimentos físicos, pero la ralentización general probablemente no supondría un gran obstáculo para una superinteligencia](#won’t-ais-be-limited-by-their-ability-to-design-and-run-experiments?).).

Si observamos los logros de los ingenieros humanos en la actualidad, puede parecer difícil de creer que, por ejemplo, una IA con capacidades sobrehumanas que dirija un laboratorio biológico pueda construir fábricas microscópicas que utilicen la luz solar para replicarse una y otra vez. Puede parecer aún más fantástico imaginar microfábricas de uso general, fábricas que puedan aceptar instrucciones para construir casi cualquier máquina a partir de los recursos disponibles.

Pero máquinas como esas no solo son posibles, sino que ya existen. Las [algas](https://en.wikipedia.org/wiki/Algae) son fábricas de micras de ancho, alimentadas por energía solar y autorreplicantes, que pueden duplicar su población en menos de un día. Y las algas contienen [ribosomas](https://en.wikipedia.org/wiki/Ribosome), que son la versión biológica de una impresora 3D universal o una cadena de montaje universal (universal al menos en lo que se refiere a los componentes básicos de la vida).

Con el conjunto adecuado de instrucciones (codificadas en el ARN mensajero), los ribosomas imprimen estructuras arbitrarias que pueden ensamblarse a partir de [proteínas](https://en.wikipedia.org/wiki/Protein). Esta universalidad sustenta la enorme complejidad y variedad del mundo biológico: toda la diversidad de la vida en la Tierra está ensamblada en última instancia por estas fábricas universales, que se encuentran prácticamente sin cambios en todo, desde los puercoespines hasta las moscas de la fruta y las bacterias.

\[embed vídeo: [https://www.youtube.com/watch?v=8dsTvBaUMvw](https://www.youtube.com/watch?v=8dsTvBaUMvw)\]

Los ribosomas pueden incluso utilizarse para ensamblar estructuras que no están compuestas por proteínas, utilizando proteínas como intermediarios. Un ejemplo de estructura no proteica que los ribosomas pueden construir de esta manera es el hueso. Los ribosomas producen proteínas que se pliegan formando enzimas débilmente unidas que catalizan parte del calcio y el fósforo para convertirlos en reactivos especiales. Estos reactivos forman entonces una matriz de colágeno que guía el calcio y el fósforo hasta su lugar para convertirlos en hueso duro y cristalino.

La naturaleza proporciona una prueba de la existencia de máquinas físicas verdaderamente extraordinarias, para entidades lo suficientemente inteligentes como para utilizar los ribosomas de formas que los humanos no han hecho, o entidades que utilizan los ribosomas para construir sus propios análogos mejorados de los ribosomas.

Pero las estructuras que vemos en el mundo biológico solo establecen un límite inferior para lo que es posible. Los organismos biológicos están muy lejos de los límites teóricos de eficiencia energética y resistencia de los materiales, y pueden ser relativamente fáciles de mejorar para razonadores mucho más inteligentes que los humanos.

#### **Mucho espacio en la parte inferior** {#mucho-espacio-en-la-parte-inferior}

Si te parece extraño utilizar fenómenos naturales como evidencia de lo que podrían ser las tecnologías del futuro, ten en cuenta que se trata de un patrón habitual en la historia de la ciencia. Las aves podían volar, por lo que los inventores pasaron siglos intentando construir máquinas voladoras.

Richard Feynman, un físico pionero, demostró el poder de este enfoque en una conferencia de 1959 titulada «Hay mucho espacio en la parte inferior» (https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf). En la conferencia, Feynman hace cálculos sobre qué tipo de cosas interesantes se podrían hacer con la miniaturización.

Hoy en día, las observaciones de Feynman parecen notablemente proféticas. Feynman señala que las computadoras probablemente podrían hacer mucho más si contuvieran más elementos, pero que el obstáculo para ello es el gran tamaño que tendrían que tener. ¡Deben miniaturizarse!

Feynman calcula que se necesitaría alrededor de un petabit (1 000 000 000 000 000 bits) para almacenar todos los libros escritos por la humanidad:

> Para cada bit, calculo 100 átomos. Y resulta que toda la información que el hombre ha acumulado cuidadosamente en todos los libros del mundo puede escribirse de esta forma en un cubo de material de dos centésimas de pulgada de ancho, que es la partícula de polvo más pequeña que puede distinguir el ojo humano. Así que hay mucho espacio en la parte inferior. No me hables de microfilmes.

¡Incluso hoy en día, aún no lo hemos conseguido! El elemento de almacenamiento real dentro de una tarjeta microSD de 2 terabytes sigue siendo de 0,6 milímetros por lado. A modo de referencia, 1/200 de pulgada equivaldría a 0,125 mm por lado. Y la tarjeta SD solo tiene capacidad para 17,6 billones de bits, lo que supone solo 1/57 de lo que Feynman calculó que necesitaríamos para almacenar todo el conocimiento de la humanidad en 1959.

¿Quizás Feynman se equivocó sobre los límites últimos de la ingeniería en un sentido práctico? Últimamente, los avances en la miniaturización del poder de cómputo se han ralentizado bastante. Decir que algo es físicamente posible no es prueba de que los ingenieros sean capaces de hacerlo.

Y acercarse a tres órdenes de magnitud de lo que algún día se lograría podría considerarse una hazaña predictiva para Feynman. Feynman dio su conferencia seis años antes de que Gordon Moore planteara por primera vez la idea que ahora llamamos [Ley de Moore](https://en.wikipedia.org/wiki/Moore%27s_law). La gente no estaba acostumbrada a pensar en la miniaturización como una ley inexorable en un gráfico. No conocemos a nadie más en la época de Feynman que especulara con que algún día podría existir un dispositivo cuyo elemento de almacenamiento, del tamaño de un grano de arena, pudiera almacenar diez millones de veces más información que las computadoras de tubo de vacío más grandes de la década de 1950.

![][image13]  
\[img src: [https://www.ibm.com/history/700](https://www.ibm.com/history/700)\]

Pero, en realidad, Feynman no se equivocaba. Y Feynman ya sabía en ese momento que su estimación era segura:

Este hecho —que se pueden transportar enormes cantidades de información en un espacio extremadamente pequeño— es, por supuesto, bien conocido por los biólogos [...] toda esta información está contenida en una fracción muy pequeña de la célula en forma de moléculas de ADN de cadena larga en las que se utilizan aproximadamente cincuenta átomos para un bit de información sobre la célula.

Las computadoras modernas aún no se han miniaturizado hasta alcanzar la escala del ADN, pero en sesenta años nos hemos acercado notablemente. Las puertas de los transistores de los chips comerciales de alta gama tienen ahora menos de cien átomos de diámetro y se construyen con una tecnología que permite añadir capas de material [de un solo átomo de espesor](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

El anclaje a los análogos naturales y el cáculo de servilleta resultaron ser una guía excepcionalmente sólida de lo que se lograría en las próximas décadas. Y trayectorias tecnológicas como estas pueden ir mucho más rápido cuando las IA realizan el trabajo científico y de ingeniería necesario.

#### **Superando a la biología** {#outdoing-biology}

¿Por qué la carne no puede ser tan resistente como el acero?

Al fin y al cabo, en el fondo son los mismos átomos. Los enlaces metálicos entre los átomos de hierro son duros, pero también lo son los enlaces covalentes entre los átomos de carbono del diamante; ¿por qué no hemos evolucionado para tener una cota de malla de diamantes que recubra nuestra piel y nos ayude a sobrevivir hasta la edad reproductiva?

Por lo demás, si el hierro es tan fuerte, ¿por qué los organismos no evolucionaron para comer mineral de hierro y desarrollar pieles recubiertas de hierro? Si los ingenieros humanos pueden hacerlo, ¿por qué la naturaleza no lo hizo primero?

Quizás haya alguna razón circunstancial por la que las pieles recubiertas de hierro en particular no sean una buena idea.

Pero si no es eso, ¿por qué no otra cosa?

La gran pregunta general aquí es: ¿por qué la naturaleza está lejos de los límites de lo físicamente posible, tal y como se calcula desde la física o se demuestra desde la ingeniería humana? ¿Existe una respuesta profunda y general, y no solo una respuesta limitada y superficial?

Hemos observado que Feynman fue capaz de utilizar estructuras biológicas para establecer límites inferiores a lo que debería ser posible con un mayor conocimiento científico. Pero, en muchos casos, la tecnología humana ya ha superado a la biología. ¿Por qué es eso posible, cuando la evolución ha tenido miles de millones de años para mejorar las plantas y los animales? Comprender este fenómeno general puede ayudar a esclarecer por qué es probable que la nanotecnología pueda ir mucho más allá de lo que ya podemos ver hoy en día en la naturaleza.

Podemos imaginar un mundo en el que las secuoyas alcancen al menos la mitad de la altura de los edificios más altos. Podemos imaginar un mundo en el que la piel de los animales más resistentes sea al menos la mitad de dura que los materiales más duros que se han observado. ¿Por qué no nos encontramos en un mundo así, en el que la naturaleza ha llegado al límite de sus posibilidades físicas tras miles de millones de años de evolución?

Esta es una pregunta tan profunda que no podemos resumir brevemente todo lo que se sabe al respecto. Pero, en términos generales, la selección natural tiene dificultades para acceder a algunas partes del espacio de diseño, incluidas muchas partes que son mucho más fáciles de alcanzar si eres un ingeniero humano.

Los tres factores principales que contribuye a esto son:

1. La selección natural tiene una presión selectiva limitada con la que trabajar y necesita cientos de generaciones para promover una nueva mutación hasta que se generalice. Si una característica biológica no es muy, muy antigua, entonces su diseño a menudo parece limitado por el tiempo, como si se hubiera hecho con prisas.  
2. Todo lo construido por la selección natural comenzó como un error accidental en algún diseño anterior: una mutación. A la evolución le cuesta más explorar partes del espacio de diseño que están *alejadas* de lo que *existe actualmente* en los organismos. Es difícil para la evolución saltar las brechas.  
3. A la selección natural le cuesta crear cosas nuevas o solucionar problemas que requieran cambios simultáneos en lugar de cambios secuenciales. Esto limita enormemente los diseños a los que puede acceder la evolución y da a los diseños actuales en biología su aspecto irregular, chapucero y enormemente enredado según los estándares de la ingeniería humana. Por ejemplo, la complejidad (de las partes conocidas) del metabolismo humano: \[IMG TODO: la fuente es [https://www.reddit.com/r/MapPorn/comments/6sxznd/map\_of\_the\_known\_human\_metabolic\_pathways/](https://www.reddit.com/r/MapPorn/comments/6sxznd/map_of_the_known_human_metabolic_pathways/) \]

![][image14]  
O, para un ejemplo más sencillo del desorden de la evolución, consideremos el ojo. Los ojos de los vertebrados evolucionaron con los nervios (2 en la imagen de abajo) situados encima de las células detectoras de luz (1). Estos nervios deben salir del ojo a través de un orificio situado en la parte posterior (3) y, dado que este punto tiene un orificio, carece de células detectoras de luz. Esto crea un punto ciego (4) para todos los vertebrados, incluidos los seres humanos, lo que obliga al cerebro a realizar ingeniosos trucos para «rellenar» el orificio (por ejemplo, con información del otro ojo).

Los pulpos desarrollaron los ojos de forma independiente y, por casualidad, evolucionaron con un diseño más sensato: los nervios pasan por detrás de las células que detectan la luz. Esto permite que estos cables salgan del ojo sin crear ningún punto ciego.

![][image15]  
\[img src: [https://en.wikipedia.org/wiki/Evolution\_of\_the\_eye](https://en.wikipedia.org/wiki/Evolution_of_the_eye)\]

O pensemos en el nervio laríngeo recurrente de la jirafa, que necesita conectar la garganta de la jirafa con su cerebro para que pueda manejar la laringe. En lugar de tomar el camino directo, este nervio viaja desde la garganta, recorre toda la longitud del cuello de la jirafa, rodea torpemente la aorta de la jirafa, vuelve a subir por el cuello hasta llegar al punto de partida y luego se conecta al cerebro.

El resultado es un nervio de cuatro metros y medio de largo (el bucle negro de la imagen inferior), lo que hace que las señales tarden entre diez y veinte veces más de lo necesario en viajar entre el cerebro y la garganta de la jirafa.[^178]

![][imagen16]  
\[fuente de la imagen: [https://en.wikipedia.org/wiki/Recurrent\_laryngeal\_nerve](https://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve)\]

En los peces, este diseño tenía sentido porque su versión del nervio laríngeo conectaba el cerebro con las branquias, en línea recta. Sin embargo, si tomamos el mismo diseño y le damos al animal un cuello, y seguimos alargando el cuello sin rehacer el cableado desde cero, obtenemos unos diseños muy ineficientes. Sobrevivibles, pero ineficientes.

La evolución produce diseños maravillosos, si se le da tiempo suficiente. Pero los seres humanos y la inteligencia artificial pueden crear una gama de diseños mucho más variada y flexible, y podemos hacerlo muy rápidamente.

Los primeros organismos multicelulares con células diferenciadas y especializadas parecen haber evolucionado hace unos 800 millones de años. En términos humanos, eso parece una eternidad. Pero la evolución funciona mucho más lentamente que la civilización humana.[^179]

Un gen recién mutado que transmite una ventaja reproductiva del tres por ciento —relativamente enorme, para ser una mutación— tardará una media de 768 generaciones en propagarse por una población de 100 000 organismos que se cruzan entre sí. Si el tamaño de la población es de 1 000 000 (la población humana estimada en la época de los cazadores-recolectores), se necesitarán 2763 generaciones. Y la probabilidad de que la mutación se propague hasta fijarse, en lugar de desaparecer aleatoriamente, es solo del 6 %.^([180])

En genética de poblaciones, la regla general es «una mutación, una muerte». Si los errores de copia del ADN introducen diez copias de una mutación deletérea en cada nueva generación, entonces diez portadores de esa mutación deben morir o no reproducirse, por generación, para contrarrestar la presión del simple ruido genético.

Esto no es tan malo como parece, como costo de mantener la información genética. En una especie que se reproduce sexualmente, puedes terminar con una persona (o un embrión) que porta muchas mutaciones perjudiciales y que muere, o no se reproduce, o aborta, y eso puede eliminar más de una instancia de gen mutado a la vez. Pero esta restricción sigue siendo la explicación estándar de por qué los seres humanos han perdido tantas adaptaciones útiles diferentes que se observan en los chimpancés y otros primates. Mientras la selección natural se ocupaba de seleccionar el aumento de la inteligencia de los primates (por ejemplo), tenía menos margen para preservar todos los sutiles genes olfativos que permiten un sentido del olfato más rico. Los genes olfativos relevantes eran útiles para la supervivencia, pero no lo eran lo suficiente como para permanecer mientras la «atención» de la evolución estaba en otra parte.

La mayoría de las jirafas no mueren como consecuencia de su cómico nervio laríngeo largo. Quizás algunas jirafas se atragantan con ramitas y habrían sobrevivido si su cerebro hubiera sido capaz de responder más rápido, pero probablemente esto no sea muy común. Por lo tanto, simplemente no es una prioridad tan alta para la selección natural, que solo tiene una presión de optimización limitada para distribuir. El diseño chapucero de la jirafa funciona en su mayor parte, se saca por la puerta y listo.

Siendo realistas, la evolución no puede refactorizar sus diseños ni empezar desde cero; solo puede hacer pequeños ajustes. Pero incluso si se dispusiera de un diseño mejor, refactorizar estas extrañas complicaciones adicionales y limpiar la deuda de diseño no es una prioridad para la selección natural.

Y como la selección natural nunca piensa en el futuro, no se convierte en una prioridad, incluso si hay otras mejoras importantes para la jirafa que podrían lograrse con una disposición menos extravagante del sistema nervioso. La selección natural no planifica. Es simplemente la historia congelada de los genes y organismos que ya se han reproducido en la práctica.

Ser capaz de detectar un mal diseño no significa necesariamente que puedas construir una jirafa mejor tú mismo. Pero los seres humanos han logrado un progreso notable en muy poco tiempo en lo que respecta a poner en marcha cientos de miles de máquinas que hacen cosas que la naturaleza no puede hacer. Esperamos que esto se mantenga con aún más fuerza si la IA llega a ser mejor que los seres humanos en el diseño y es capaz de realizar el mismo trabajo cognitivo cientos de miles de veces más rápido.

La capacidad de la selección natural para «diseñar» una jirafa mejor se ve obstaculizada por el hecho de que opera a través de mutaciones y recombinaciones. Le resulta difícil acceder a cualquier parte del espacio de diseño que no pueda alcanzarse mediante una serie de mutaciones únicas, que deben ser ventajosas de forma individual y separada, o mediante la combinación de mutaciones que fueran lo suficientemente ventajosas individualmente como para estar presentes en una gran parte del acervo genético antes de combinarse.

Un complejo genético formado por cinco genes, cada uno con una prevalencia independiente del 10 % en la población, solo tiene una probabilidad entre 100 000 de ensamblarse dentro de cada organismo. Y un complejo genético que supone una gran ventaja, pero solo una vez entre 100 000, casi no tiene posibilidades de evolucionar hasta fijarse.

Esto no significa que la selección natural no pueda crear máquinas complejas, solo significa que su camino hacia la maquinaria compleja tiene que pasar por pasos incrementales ventajosos. Para redirigir el nervio de la jirafa se necesitarían varios cambios simultáneos en el genoma de la jirafa, y cada uno de esos cambios sería inútil individualmente sin los demás cambios. Por lo tanto, la anatomía de la jirafa permanece tal y como es.

La maravilla de la [evolución](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/ZyNak8F6WXjuEbWWc) no es la rapidez con la que funciona, sino que su complejidad es mucho mayor que la de un ingeniero humano que realiza estudios de casos. La maravilla de la selección natural no es la elegante simplicidad de sus diseños; un vistazo al diagrama de cualquier proceso bioquímico bastaría para disipar esa idea errónea. La maravilla de la selección natural no es su robusta corrección de errores, que cubre todas las vías que podrían fallar; ahora que morimos con menos frecuencia por inanición y lesiones, la mayor parte de la medicina moderna se dedica a tratar partes de la biología humana que explotan aleatoriamente en ausencia de traumatismos externos.

La maravilla de la evolución es que, como proceso de búsqueda puramente accidental, la evolución funciona.

#### **La debilidad de las proteínas** {#la-debilidad-de-las-proteínas}

Esto nos lleva a otra forma en la que la tecnología puede mejorar la biología.

Muy por debajo del nivel de la carne, invisibles a simple vista, se encuentran las células. Muy por debajo del nivel de las células se encuentran las proteínas.

Las proteínas, al plegarse, se mantienen unidas principalmente por el equivalente molecular de la adherencia estática: las fuerzas de van der Waals (https://en.wikipedia.org/wiki/Van_der_Waals_force), que son decenas o cientos de veces más débiles que los enlaces metálicos como el hierro, o incluso que los enlaces covalentes como el diamante.

¿Por qué la biología utiliza un material tan débil como bloque de construcción básico? Porque un material más fuerte habría dificultado la evolución. (Y si haces que la evolución sea demasiado difícil, nunca evolucionarás el tipo de personas que se hacen ese tipo de preguntas).

El plegamiento de proteínas ocurre bajo fuerzas moleculares relativamente ligeras y las proteínas se unen en esas formas principalmente por adherencia estática. Esta es una de las principales razones por las que la selección natural tiene una rica estructura de posibilidades que explorar: las mutaciones aleatorias pueden modificar repetidamente una proteína y acabar dando con un nuevo diseño que hace prácticamente lo mismo, pero ligeramente mejor.

Si, por el contrario, los organismos estuvieran formados por moléculas unidas por enlaces fuertes, cambiar uno de los componentes tendría menos probabilidades de producir una nueva estructura interesante (y potencialmente útil). Aún así, podría ocurrir en algunas ocasiones, pero con mucha menos frecuencia. Y si eres el tipo de diseñador que tarda dos mil millones de años en inventar colonias celulares y otros mil millones de años en inventar tipos de células diferenciadas, «ocurre con menos frecuencia» significa que la estrella más cercana se hincha y se traga tu planeta antes de que llegues tan lejos.

Todas las proteínas existen debido a un error de copia de alguna proteína predecesora. La proteína predecesora no estaba unida firmemente por muchos enlaces fuertes, ya que eso habría dificultado su evolución. Por lo tanto, es probable que la última proteína nueva tampoco tenga muchos enlaces fuertes.

La bioquímica a veces descubre enlaces fuertes. Antes hemos mencionado el ejemplo de los huesos. Otro ejemplo se da en las plantas. Las plantas han desarrollado proteínas que se pliegan formando enzimas, las cuales catalizan la síntesis de bloques de construcción moleculares que se oxidan y se convierten en un polímero fuertemente entrecruzado covalentemente: la lignina, el bloque de construcción de la madera.[^181]

Pero esos son casos especiales, y la selección natural no presta mucha «atención» a la ingeniería de muchos casos como ese.

No es extraño que los átomos de carbono y otros elementos orgánicos comunes puedan ser fuertes. Simplemente se necesita mucho más trabajo para evolucionar. La selección natural no tiene tiempo para hacer eso en todas partes, solo en unos pocos casos especiales que se incorporan al resto de la anatomía, como los huesos, la lignina de la madera o la queratina de las uñas y las garras.

Si utilizas las palabras clave adecuadas, puedes interrogar, por ejemplo, a ChatGPT-o1 —para cuando leas esto, probablemente los LLM de fuerza equivalente serán gratuitos— y preguntarle sobre las fuerzas de enlace individuales de los enlaces carbono-carbono en el diamante, o los enlaces hierro-hierro en el metal hierro puro, o los enlaces poliméricos covalentes en la lignina, o los enlaces disulfuro en la queratina, o los enlaces iónicos en los huesos. Puedes preguntarle cómo se relacionan todos estos con las resistencias estructurales del material más grande. (En 2023 no deberías haber intentado esto porque GPT-4 habría calculado mal todas las matemáticas, pero mientras escribimos este párrafo en 2024, o1 parece mejor).

Aprenderías que la fuerza exacta del enlace entre dos átomos de carbono es del orden de medio attojoule, al igual que la fuerza del enlace entre dos átomos de hierro, y que el enlace cruzado azufre-azufre en la queratina es solo ligeramente inferior (0,4 attojoules), al igual que los enlaces covalentes polimerizados en la lignina de la madera.

Pero las fuerzas de adhesión estáticas que pliegan las proteínas son, dependiendo de cómo se mire, como mínimo diez veces más débiles, y potencialmente cientos o miles de veces más débiles que eso.

E incluso cuando las plantas catalizan sustancias como la lignina, los enlaces cruzados tienden a ser más escasos que los enlaces carbono-carbono del diamante. La diferencia entre la resistencia en gigapascales del diamante y la resistencia en megapascales de la madera tiene más que ver con la densidad y la regularidad de los enlaces del diamante, y no con que los enlaces del diamante sean individualmente más fuertes.[^182]

Debido a las limitaciones de la evolución como diseñador y a las limitaciones de las proteínas como material de construcción, la vida opera bajo restricciones que los diseñadores humanos y la inteligencia artificial pueden eludir. Las aves son maravillas de la ingeniería, pero las máquinas voladoras fabricadas por el hombre pueden transportar cargas diez mil veces más pesadas a más de diez veces la velocidad de vuelo de las aves más rápidas y fuertes. Las neuronas biológicas son maravillas de la ingeniería, pero los transistores fabricados por el hombre se activan y desactivan decenas de millones de veces más rápido que las neuronas más rápidas. Y la tecnología que tenemos hoy en día solo es una pequeña muestra de lo que se puede lograr.

#### **Freitas y los glóbulos rojos** {#freitas-and-red-blood-cells}

Hemos dicho que la biología no se acerca ni remotamente al límite de lo que es físicamente posible. Entonces, ¿qué se acerca al límite?

Para ilustrar algunas buenas formas de pensar sobre esta pregunta, podemos considerar los glóbulos rojos.

Durante los últimos 1500 millones de años, en todos los seres vivos, desde los humanos hasta los lagartos, el oxígeno ha sido transportado en los organismos multicelulares por la hemoglobina. La hemoglobina es una proteína compuesta por 574 aminoácidos, además de cuatro grupos hemo especialmente diseñados para contener una molécula de hierro especial. Un glóbulo rojo humano contiene alrededor de 280 millones de moléculas de hemoglobina y mide unos siete micrones de largo. Tres millones de ellos cabrían en la cabeza de un alfiler, y tú tienes alrededor de 30 billones en tu cuerpo.

¿Hasta qué punto los glóbulos rojos se acercan a los límites de lo que podrían hacer en principio en lo que respecta al transporte de oxígeno?

Rob Freitas, autor de Nanomedicine, realizó en 1998 un [estudio moderadamente detallado](https://pubmed.ncbi.nlm.nih.gov/9663339/) sobre el diseño teórico de un glóbulo rojo artificial utilizando materiales con enlaces covalentes. La célula se diseñó con un diámetro de un solo micrón para poder desplazarse más fácilmente a través de arterias obstruidas.

En lugar de limitarse a considerar una forma diferente de almacenar moléculas de oxígeno, Freitas pensó en cómo sustituir todo el glóbulo rojo. Freitas se basó en análisis anteriores para considerar la necesidad de extraer también la glucosa del medio sanguíneo y convertirla en energía para alimentar la célula artificial. Pensó en sensores del tamaño de una célula y en diminutas computadoras integradas fabricadas con varillas sólidas que se encajan en otras varillas sólidas para realizar cálculos sencillos. Consideró si la célula artificial se sedimentaría más rápidamente que los glóbulos rojos actuales.

La biocompatibilidad puede ser un gran problema para cualquier cosa que se introduzca en el cuerpo humano, pero las superficies de diamante son lo suficientemente inertes como para que se utilicen recubrimientos de película similares al diamante en algunos dispositivos médicos que se introducen en el cuerpo humano. A nivel de posibilidad teórica, Freitas consideraba que la superficie de la célula artificial podía parecerse a un diamante y, por lo tanto, ser biocompatible.

![][image17]

\[imagen incrustada de [https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm](https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm)\]

La pieza central del glóbulo rojo artificial fue el cálculo de Freitas de que un recipiente a presión de corindón o diamante monocristalino a escala micrométrica toleraría, siendo conservadores, 100 000 atmósferas de presión. Con un cómodo margen de seguridad de 100 veces y un empaquetamiento de moléculas a solo 1000 atmósferas, esto permitiría a los glóbulos rojos artificiales suministrar 236 veces más oxígeno a los tejidos que los glóbulos rojos por unidad de volumen, y almacenar una cantidad similar de dióxido de carbono para amortiguar el otro lado de la respiración. Aproximadamente: podrías aguantar la respiración durante cuatro horas.

Ahora bien, fabricar glóbulos rojos artificiales como esos es otra cuestión totalmente distinta. Por eso este tratamiento médico en particular aún no está disponible en la consulta de tu médico de cabecera.

Una esfera de 1 kilogramo de diamante sólido sin imperfecciones es una molécula fácil de describir sobre el papel, pero sintetizarla es más difícil. Lo que Freitas nos ayuda a hacer es formular hipótesis más fundamentadas sobre lo lejos que está la biología actual de los límites teóricos en este ámbito.[^183] La biología es impresionante, pero está lejos de ser óptima.

Es plausible que, por cualquier número de razones, el diseño exacto de Freitas no funcione, y es muy probable que no sea óptimo. Es casi seguro que una idea inicial para un diseño complejo extremadamente novedoso tropiece con problemas en algún momento.

Pero al expresar escepticismo sobre el funcionamiento de la propuesta exacta de Freitas, no estamos afirmando que ninguna alternativa a los glóbulos rojos pueda suministrar oxígeno cientos de veces más eficientemente que los glóbulos rojos biológicos.

La ingeniería consiste en encontrar alguna forma de hacer que algo funcione. Aunque mil intentos de construir algo fracasen, solo hace falta un éxito para que todo el proyecto tenga éxito. La existencia de innumerables diseños de aviones inviables en el siglo XVII y antes no significaba que los aviones funcionales fueran imposibles, sino que eran difíciles de localizar entre todos los diseños posibles.

Por eso, aunque los escépticos tecnológicos suelen tener razón al afirmar que las tecnologías están más lejos en el futuro de lo que creen los optimistas más ilusionados, suelen equivocarse al afirmar que ciertas hazañas tecnológicas nunca se lograrán. Cuando el logro es una tarea concreta en el mundo, cuando no sabemos cómo se va a conseguir y cuando se sabe que las leyes de la física lo permiten, la historia sugiere que a menudo hay alguna forma de tener éxito, aunque el camino no sea obvio al principio.

O, en palabras del autor e inventor Arthur C. Clarke:

Cuando un científico distinguido pero anciano afirma que algo es posible, es casi seguro que tiene razón. Cuando afirma que algo es imposible, es muy probable que se equivoque.

#### **Nanosistemas** {#nanosistemas}

En resumen:

* El mundo biológico está construido a partir de una increíble variedad de máquinas moleculares.  
* Estudiar la biología puede enseñarnos qué hazañas microscópicas son posibles desde el punto de vista tecnológico.  
Pero la biología es un límite de conservación de lo que es posible; no se acerca a los límites de lo posible. La evolución es un diseñador muy limitado, y las proteínas no son el mejor material de construcción.

Nanosistemas (1992), de Eric Drexler, es el libro clásico que explora la cuestión de qué hazañas de ingeniería en pequeña escala son posibles. *Nanosystems* contribuyó a iniciar la revolución de los nanomateriales de la década de 1990 y suscitó una gran controversia, ya que los científicos debatieron los argumentos de Drexler. Puedes encontrar una copia completa en línea de *Nanosystems* [aquí](https://nanosyste.ms/table_of_contents).

*Nanosystems* es un texto profundo y amplio, y sorprendentemente accesible dada su temática técnica. Una contribución clave del libro fue explorar las implicaciones de construir estructuras a pequeña escala de una manera novedosa.

Una forma de construir cosas muy pequeñas es mediante reacciones químicas: colisionar moléculas en condiciones específicas (como calor extremo) para romperlas y hacer que los átomos se unan en nuevas moléculas.

Este es un enfoque muy potente por sí mismo, y es el método que utiliza la humanidad para fabricar materiales como plásticos, aceros y cerámicas, pero palidece en comparación con lo que se puede construir con otros métodos. Fabricar materiales a partir de reacciones químicas es un poco como construir estructuras de LEGO haciendo bolsas llenas de ladrillos LEGO y agitándolas con fuerza. Es posible construir algunas cosas de esa manera, pero el conjunto de cosas que se pueden construir es limitado y se genera mucho desperdicio.

La síntesis de proteínas es como utilizar las manos para construir grandes estructuras de LEGO a partir de conjuntos de LEGO más pequeños y preconstruidos. Hay margen para mucha más precisión, ya que puedes colocar cada conjunto preconstruido exactamente donde quieres, pero sigue siendo un poco extraño e incómodo porque estás trabajando con conjuntos preconstruidos. Esto es lo que hacen los ribosomas en el cuerpo: unir cadenas de [aminoácidos](https://en.wikipedia.org/wiki/Amino_acid) para formar proteínas, que luego se utilizan para realizar diversas tareas en el cuerpo.

La insulina, la hemoglobina y la ATP sintasa del cuerpo humano son ejemplos de complejos proteicos formados por múltiples cadenas de proteínas unidas entre sí: dos cadenas de proteínas en el caso de la insulina, cuatro en el de la hemoglobina y veintinueve en el de la ATP sintasa.

Los componentes básicos de las proteínas, los aminoácidos, son moléculas que suelen estar formadas por entre diez y veinticinco átomos. Como materiales de construcción, los aminoácidos tienen muchas ventajas:

* Cada aminoácido tiene una cadena principal que se une a una cadena lateral (potencialmente larga) de átomos de carbono, hidrógeno, oxígeno, nitrógeno y azufre. Existen cientos de cadenas laterales diferentes, que se comportan de maneras distintas, lo que convierte a los aminoácidos en herramientas muy flexibles.  
* La cadena principal de un aminoácido, como una pieza de LEGO, puede unirse a la cadena principal de otro aminoácido. Esto puede repetirse una y otra vez; las proteínas típicas están formadas por cientos de aminoácidos unidos entre sí. Esto hace que los aminoácidos sean aún más flexibles como herramientas (o como componentes básicos de herramientas). La complejidad de las proteínas también significa que estas a menudo pueden sufrir pequeños ajustes (a través de mutaciones del ADN) sin cambiar radicalmente y volverse completamente inútiles, lo que a su vez facilita la evolución de nuevas proteínas.  
* Dado que las proteínas están formadas por cadenas lineales de aminoácidos, es posible especificar una proteína de forma única simplemente enumerando sus aminoácidos en orden. El ADN aprovecha esta característica utilizando un «alfabeto» de cuatro letras (nucleótidos) para formar «palabras» de tres letras (codones, cada uno de los cuales representa un aminoácido diferente), que luego pueden encadenarse en una «frase» lineal (una proteína formada por esa secuencia exacta de aminoácidos). ([Ilustración en vídeo del ADN](https://www.youtube.com/watch?v=7Hk9jct2ozY).)  
* Como se muestra en el [experimento de Miller-Urey](https://en.wikipedia.org/wiki/Miller%E2%80%93Urey_experiment), los aminoácidos pueden formarse espontáneamente en ausencia de vida, a partir de reacciones químicas simples. Esto crea una vía para que la vida (y los precursores de los ribosomas y la síntesis de proteínas) se desarrolle en primer lugar.

Los cuerpos obtienen los veinte aminoácidos que necesitan para la síntesis de proteínas de los alimentos, sintetizándolos en el cuerpo o recolectándolos de proteínas anteriores. Los ribosomas reciben instrucciones del ADN que básicamente dicen «usa este aminoácido, luego este otro aminoácido, luego este otro aminoácido, ... y luego detente». A continuación, los aminoácidos son transportados (por pequeñas máquinas moleculares llamadas ARN de transferencia) al ribosoma, que construye la proteína pieza por pieza.

\[embed vídeo o gif: [https://www.youtube.com/watch?v=2dV5s6v2v8Q](https://www.youtube.com/watch?v=2dV5s6v2v8Q)\]

Cabe destacar que la lista anterior incluye características que son muy valiosas para la evolución, pero mucho menos necesarias para la ingeniería deliberada. La evolución necesita una estructura química relativamente simple pero flexible que pueda producirse mediante reacciones químicas comunes. Un diseñador humano o artificial es libre de elegir entre una variedad de moléculas no relacionadas entre sí, en lugar de necesitar que todas ellas estén estrechamente relacionadas. También son libres de utilizar bloques de construcción que rara vez aparecen en la naturaleza y de ensamblar estos bloques de construcción de formas complejas y descendentes.

Esto proporciona parte del impulso para explorar una tercera forma de construir cosas muy pequeñas: la *mecanosíntesis*, en la que las estructuras se construyen moviendo directamente los átomos a la ubicación correcta, potencialmente utilizando una máquina similar a un ribosoma para recibir instrucciones y luego ensamblar cosas mucho más variadas que solo diferentes proteínas. En la analogía de LEGO, la mecanosíntesis es como poder finalmente trabajar con piezas individuales de LEGO y colocar cada una exactamente donde tú quieres.

Nanosystems explora qué tipos de máquinas nuevas podrían ser posibles con la mecanosíntesis. Un ejemplo del tipo de diseño que Drexler explora es un [engranaje planetario](https://en.wikipedia.org/wiki/Sun_and_planet_gear) reducido a una escala de solo [alrededor de 3500 átomos](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems):

\[gif: [https://chem.beloit.edu/classes/nanotech/nanorex/a8\_qm\_animation5.gif](https://chem.beloit.edu/classes/nanotech/nanorex/a8_qm_animation5.gif) de [https://chem.beloit.edu/classes/nanotech/nanorex/index.html](https://chem.beloit.edu/classes/nanotech/nanorex/index.html)\]

La hemoglobina está compuesta por unos 10 000 átomos, lo que no dista mucho del engranaje de Drexler. Y algunas proteínas son mucho más simples. La insulina está compuesta por solo cincuenta y un aminoácidos, o unos 800 átomos en total.

Sin embargo, los diseños de Drexler son, en escala, un gran paso hacia abajo en comparación con las máquinas más complicadas que vemos en el cuerpo. Los ribosomas y la ATP sintasa, por ejemplo, están compuestos por más de 100 000 átomos, y el motor de un flagelo bacteriano tiene más de un millón de átomos.

*Nanosystems* todavía no intenta explorar los límites de lo que es tecnológicamente posible. Pero al centrarse en casos que son relativamente fáciles de analizar hoy en día, sí muestra que la mecanosíntesis permitiría una tecnología que superaría lo que vemos en el mundo biológico actual.

Los cálculos de *Nanosystems* son intencionadamente conservadores. Drexler, por ejemplo, considera computadoras construidas con varillas de diamante literales que se mueven, no porque ese fuera el límite final de la tecnología, sino porque en 1992 era más fácil de analizar que la computación basada en la electricidad. Esto, a su vez, ayudó a inspirar el análisis de células sanguíneas de Freitas. Cuatro años más tarde, Eric Drexler y Ralph Merkle (más conocido como el inventor del hash criptográfico y coinventor de la criptografía de clave pública) intentaron [analizar](https://www.zyvex.com/nanotech/helical/helical.html) un sistema ligeramente más cercano a los límites de lo posible para [computación reversible](https://en.wikipedia.org/wiki/Reversible_computing), y calcularon que se disipaba 10 000 veces menos calor por operación de lo que había estimado *Nanosystems*, aunque la nueva estimación se basaba en un análisis menos conservador.

En otra parte de *Nanosystems*, hay un boceto aproximado de un brazo manipulador de seis grados de libertad que habría requerido millones de átomos. Un intento posterior de esbozar una máquina como esta átomo por átomo resultó requerir solo 2596 átomos.

La construcción de estructuras atómicamente precisas a la escala de la que habla Drexler plantea grandes retos de ingeniería. Uno de los principales retos es que la construcción de estructuras atómicamente precisas requiere el manejo de manipuladores increíblemente pequeños y precisos. Sin embargo, la existencia de los ribosomas ofrece una posible vía de ataque.

Aunque los ribosomas solo pueden construir proteínas, estas pueden catalizar y arrastrar reactivos que no son aminoácidos (como los huesos y la madera). Los ribosomas son fábricas potentes y versátiles, y sus productos pueden utilizarse para crear herramientas más pequeñas y precisas, incluidas herramientas que construyen de forma más directa dispositivos más pequeños utilizando materiales más resistentes.

Ya sea de forma directa o indirecta, es casi seguro que los genomas pueden producir pequeños actuadores capaces de manipular átomos individuales para construir una variedad de cosas que no están hechas de proteínas. Y lo que es de importancia, este no es el tipo de mecanismo en el que la selección natural pueda tropezar, aunque sea relativamente fácil de construir, porque el brazo manipulador no es útil hasta que está completo.

La evolución construye estructuras complejas que son útiles en cada paso del camino. Incluso muchos diseños relativamente simples están disponibles para los ingenieros inteligentes, pero no para la evolución. Las ruedas que giran libremente, por ejemplo, son un invento increíblemente sencillo que tiene una gran variedad de aplicaciones. A pesar de ello, las ruedas que giran libremente parecen haber evolucionado solo tres veces en toda la historia de la vida en la Tierra: en la ATP sintasa y el flagelo bacteriano que hemos comentado anteriormente, y en el flagelo arqueano, que parece haber evolucionado de forma independiente.[^184]

A pesar de los métodos conservadores utilizados en el libro, el límite tecnológico inferior establecido por *Nanosystems* es muy alto en términos absolutos. Una superinteligencia con el tipo de tecnología que describe Drexler sería capaz de producir diminutas fábricas autorreplicantes similares a los ribosomas que duplicarían su población cada hora —algunos organismos se replican aún más rápido, pero Drexler hizo cálculos conservadores— y que podrían agruparse para construir estructuras macroscópicas más grandes, como centrales eléctricas.

Los nanosistemas como los que describe Drexler pueden autorreplicarse utilizando la luz solar y el aire como materias primas, lo que permite una expansión muy rápida y fiable. La razón por la que esto puede funcionar es la misma por la que los árboles son capaces de ensamblar materiales de construcción a granel en gran parte a partir del aire, extrayendo el carbono del aire y secuestrándolo en forma de madera. Aunque pensamos en el aire como un «espacio vacío», el carbono, el hidrógeno, el oxígeno y el nitrógeno que contiene son materiales de construcción que pueden reorganizarse en materiales sólidos y utilizarse para diversos fines.

Los autorreplicadores del tipo *Nanosystems*, fabricados con materiales como el hierro o el diamante en lugar de proteínas, podrían devorar células biológicas de la misma manera que una cortadora de césped corta la hierba.

Podrían sintetizar de forma barata algo como la [toxina botulínica](https://en.wikipedia.org/wiki/Botulinum_toxin), la proteína responsable del botulismo. Una millonésima parte de un gramo de toxina botulínica, veinte mil veces más pequeña que un grano de arroz, es una dosis letal. Los replicadores cuidadosamente diseñados podrían propagarse de forma invisible por el aire libre hasta que al menos uno de ellos fuera inhalado por casi todos los seres humanos (que no hubieran pasado, por ejemplo, el último mes íntegramente en un submarino), momento en el que los dispositivos podrían (mediante un temporizador) liberar simultáneamente una pequeña dosis de toxina, matando de forma inmediata y simultánea a casi todos los seres humanos.

O los nanosistemas construidos por IA podrían acabar con los seres humanos de forma incidental, en el curso de la recolección y reutilización de los recursos de la Tierra. Un [artículo de Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calcula que las máquinas de microdiámetro, que solo dependen de la luz solar como fuente de energía y del hidrógeno, carbono, oxígeno y nitrógeno del aire como materias primas, podrían diseñarse para reproducirse tan rápidamente que oscurecerían el cielo en menos de tres días, al tiempo que consumirían toda la biosfera.[^185] En consecuencia, si la primera IA que lograra una tecnología como esta tuviera un plazo de ejecución de apenas unos meses, podría utilizar ese tiempo para destruir a todos sus competidores (ya fueran humanos o IA). Se trata de una tecnología que confiere una ventaja estratégica permanente y decisiva al primero en utilizarla.

Decir que la nanotecnología drexleriana es factible en principio físico no significa necesariamente que las primeras IA más inteligentes que los humanos pudieran realmente construir una tecnología que se acerque a esos límites físicos. Nuestra mejor suposición es que está dentro del alcance de lo que una superinteligencia artificial podría descubrir, porque resolver este tipo de tareas de ingeniería parece principalmente un desafío cognitivo (que se puede resolver pensando) y [no tenemos la expectativa de que la fase de experimentación y pruebas tenga que ser tan larga].(#la inteligencia te permite aprender más de los experimentos y realizar experimentos más rápidos, más informativos y más paralelizados).

Incluso si nuestra suposición es correcta, no hay garantía de que el primer paso de una superinteligencia sea utilizar la nanotecnología para construir su propia infraestructura y tomar el control de los recursos del mundo. Por lo que sabemos, podría desarrollar técnicas y tecnologías que le permitieran alcanzar sus objetivos de forma aún más rápida y eficiente.

Pero si una IA más inteligente que los humanos fuera capaz de construir sistemas que fueran a las células lo que los aviones son a las aves, y de proliferar su propia infraestructura por toda la superficie de la Tierra, entonces cualquier cosa que acabara haciendo sería, como mínimo, igual de decisiva.

El objetivo de todo este análisis es argumentar que la tecnología humana está lejos de los límites de lo posible. Existe una amplia variedad de tecnologías de importancia que probablemente le llevarían a la humanidad décadas, siglos o milenios descubrir, y que las superinteligencias artificiales serían capaces de hacer rápidamente.

En resumen, la *nanotecnología* ilustra que una superinteligencia con un poco de tiempo de ventaja probablemente podría encontrar soluciones tecnológicas para apoderarse del planeta.

El resultado más probable de la creación de una superinteligencia es que descubra alguna tecnología al menos tan poderosa como la nanotecnología, y entonces la humanidad simplemente pierda.

Esta suposición no es fundamental para el argumento que planteamos en el libro. La humanidad perdería frente a una superinteligencia incluso si el mundo no contuviera una tecnología «ganadora inmediata» como la nanotecnología. Por lo tanto, no entramos en todo este análisis en el libro propiamente dicho.

En la parte II, nos centramos deliberadamente en un escenario de toma de control que no supone que la IA tenga una capacidad general para realizar una fabricación de precisión atómica, ya sea a través de ribosomas o de mecanosíntesis. Una superinteligencia no necesita una ventaja tecnológica absolutamente abrumadora para ganar el control sobre el futuro, por lo que no nos centramos demasiado en esa posibilidad en el libro.

Pero también parece que vale la pena señalar que probablemente tendrá una ventaja tecnológica totalmente abrumadora.

### Una nueva forma de descubrir ilusiones ópticas {#una-nueva-forma-de-descubrir-ilusiones-opticas}

En el capítulo 6, afirmamos que existen múltiples ilusiones ópticas que se crearon basándose en una comprensión relativamente moderna del procesamiento visual humano y la corteza visual, ilusiones que no podrían haberse inventado o descubierto hace cincuenta años, salvo por un improbable accidente. A continuación, citamos algunos ejemplos representativos.

La ilusión de la «ceguera a la curvatura» (https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/) tiene cierta base en el fenómeno general de la ceguera a la curvatura, pero esta ilusión específica se construyó cuidadosamente a partir de principios básicos alrededor de 2017, en lugar de descubrirse por accidente. \[[Original study](https://journals.sagepub.com/doi/10.1177/2041669517742178)\]

En 2022, Bruno Laeng et al publicaron [un estudio](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) en el que demostraban que su nuevo «agujero negro en expansión» (https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg)» provocaba que las pupilas de los participantes se dilataran, como si anticiparan la entrada en un espacio oscuro. (Este efecto fue notablemente mayor que el efecto de simplemente enfocar un objetivo visual más oscuro, lo que también provocaría una pequeña dilatación de las pupilas).

La ilusión «[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)», revelada en 2021, se construyó cuidadosamente a partir de trabajos sobre luminancia y contornos ilusorios que se remontan a finales de la década de 1970.

La ilusión «[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)», desarrollada alrededor del año 2000, no es un ejemplo tan representativo de la creación de una nueva ilusión basada en el conocimiento de la biología humana. Aun así, resulta interesante y relevante desde otro punto de vista, ya que se trata de una ilusión basada en una tecnología novedosa, es decir, que habría sido difícil o imposible de crear sin las computadoras modernas.

También menos central, la ilusión «[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-\(255,0,0\)-\(0,170,85\)-700px-15s-70s-shrink.svg)», creada alrededor de 2010, agota los conos M del espectador, lo que permite que los conos L, menos agotados, creen la percepción de un azul brillante que, de otro modo, habría sido moderado y debilitado por la activación simultánea de M y L. \[[Más detalles](https://dynomight.substack.com/p/colors)\]

En relación con esto, el estudio de la activación de los conos a principios de la década de 2000 condujo a la creación de varios [colores quiméricos](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), mediante la manipulación cuidadosa de activaciones de conos poco probables en la naturaleza:

> El modelo H-J ofrece algunas predicciones novedosas y poco apreciadas, así como algunas explicaciones novedosas y poco apreciadas, sobre las características cualitativas de una variedad considerable de sensaciones cromáticas posibles para la experiencia humana, sensaciones cromáticas que las personas normales casi con toda seguridad nunca han tenido antes y cuyas descripciones precisas en lenguaje corriente parecen semánticamente mal formadas o incluso contradictorias.
>
> Concretamente, estas sensaciones cromáticas «imposibles» son vectores de activación (a través de nuestras neuronas de proceso opuesto) que se encuentran dentro del espacio de vectores de activación neuronalmente posibles, pero fuera del «husillo cromático» central que limita el rango familiar de sensaciones para los colores objetivos posibles. Estas sensaciones cromáticas quiméricas extrahusillo no se corresponden con ningún color reflectante que puedas ver objetivamente reflejado en un objeto físico. Sin embargo, el modelo H-J predice su existencia y explica con cierto detalle sus características cualitativas altamente anómalas. \[[Artículo original](https://www.tandfonline.com/doi/full/10.1080/09515080500264115?scroll=top&amp;needAccess=true)\]

Por último, algunos [experimentos en curso](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) muestran que:

> Las ondas rítmicas de la actividad cerebral hacen que veamos o no veamos imágenes complejas que destellan ante nuestros ojos. Una imagen puede volverse prácticamente invisible si destella ante nuestros ojos al mismo tiempo que un punto bajo de esas ondas cerebrales. Podemos restablecer ese ritmo de ondas cerebrales con una simple acción voluntaria, como elegir pulsar un botón.

... lo que demuestra aún más que una comprensión más profunda de la biología y la fisiología permite un mayor abanico de movimientos estratégicos. En este caso, las percepciones pueden alterarse de formas que no dependen en absoluto de cambiar los datos de entrada que llegan al nervio óptico, sino simplemente sincronizando la llegada de los estímulos con otros procesos que tienen lugar en el cerebro.

# Parte II: Un escenario de extinción {#part-ii:-one-extinction-scenario}

El escenario que describimos en la Parte II no es una predicción. Hay muchas otras posibilidades para el futuro, y una versión más extensa de *Si alguien lo construye, todos morirán* habría explorado múltiples escenarios posibles. A continuación, explicaremos algunos de los razonamientos que nos llevaron a escribir el escenario tal y como lo hicimos, y describiremos varios problemas que surgen al esbozar un escenario como este.

Las historias pueden ser convincentes de una manera que la razón fría no puede, y creemos que vale la pena intentar imaginar de manera concreta cómo podría desarrollarse el futuro. Pero también creemos que es de importancia no obsesionarse demasiado con una narrativa en particular. Cada decisión que tomamos en el escenario puede parecer plausible de manera aislada, pero no hacen falta muchas elecciones para que la probabilidad general de que se produzca una trayectoria concreta sea muy baja. Así es como se presenta un futuro que contiene muchas decisiones difíciles.

Sin embargo, hay muchos casos en los que el resultado es más predecible que el camino, porque muchos caminos conducen al mismo destino. En el escenario tal y como está escrito, presentamos múltiples opciones diferentes cuando es posible, para ilustrar que, independientemente de cómo vaya la historia, no conduce a ningún sitio bueno.

## Preguntas frecuentes {#faq-6}

### ¿Por qué elegiste esta configuración? {#why-did-you-pick-this-setup?}

#### **\* Porque es plausible y fácil de escribir.** {#*-porque-es-plausible-y-fácil-de-escribir.}

Cada detalle de una historia sobre el futuro es una oportunidad para que esa historia resulte errónea. No podemos decirles exactamente qué avances tecnológicos se producirán ni en qué orden, del mismo modo que no podemos decirles con exactitud qué tiempo hará dentro de un mes.

Historias como esta no pretenden ser una ventana exacta al futuro. Su objetivo es ilustrar cómo *podría* ser el futuro, de una manera que relacione todos los argumentos abstractos que hemos expuesto en la primera parte del libro. Algunas personas consideran que el peligro parece mucho más real cuando imaginan vívidamente un camino concreto que podría tomar el futuro y que termina en la ruina.

Aún más convincentes podrían ser diez historias, o cien historias, que muestren cuántos caminos diferentes conducen a la ruina y cómo los caminos que conducen a un futuro próspero son estrechos y frágiles.

Eso es lo que significa que un aspecto del futuro sea fácil de predecir: cuando casi todos los caminos tienen el mismo punto final, ese punto final es predecible. Pero no teníamos tiempo ni espacio para escribir diez historias, y mucho menos cien.

Para la historia que decidimos contar, nos ceñimos a un escenario que comienza lo antes posible. No es porque pensemos que una situación como esta vaya a surgir pronto con toda seguridad ([no estamos seguros](#¿cuándo se va a desarrollar este tipo de IA preocupante?)), sino porque una historia ambientada cerca del presente es mucho más fácil de escribir. Si la hubiéramos ambientado en un futuro aún más lejano y hubiéramos inventado muchos más detalles futuristas sobre lo que había sucedido entre ahora y entonces, la historia sería aún *más* inverosímil. Y esos detalles solo distraerían la atención.

Incluso si de alguna manera pudiéramos prever el camino exacto que tomaría el futuro, puede que no fuera el mejor escenario para comprender la dinámica general en juego.

Esperamos que el verdadero futuro sea profundamente extraño, lleno de detalles confusos y contingentes, cada uno de los cuales pondría a prueba la credulidad si se incluyera en una historia. Una historia escrita de esa manera sería confusa y difícil de seguir, llena de detalles inexplicables e innecesarios, debido al desinterés de la realidad por la cohesión narrativa. También resultaría menos *plausible*, porque muchos de los detalles parecerían extraños.

Para tener una idea de cómo se sentiría, imagina retroceder 100 años en el tiempo e intentar describir la vida cotidiana y los grandes problemas del mundo moderno. La mayoría de las personas en 1925 nunca habían escuchado la radio, conducido un automóvil o visto un refrigerador. Para describir las redes sociales, la globalización y la obesidad, no solo sería necesario explicar una rica red de tecnologías, sino que también habría que cambiar radicalmente la visión del mundo de los oyentes. No, la historia que elegimos contar es más plausible y, por lo tanto, menos realista.

#### **Hay muchas otras formas en que podría desarrollarse el futuro.** {#hay-muchas-otras-formas-en-que-podría-desarrollarse-el-futuro.}

Aquí tienes algunas posibilidades alternativas sobre cómo podría comenzar una historia como esta:

* Se produce algún tipo de avance en el aprendizaje permanente, la memoria a largo plazo o el aprendizaje más eficiente a partir de datos, lo que da lugar a IA cualitativamente más inteligentes en general que cualquiera de las anteriores (del mismo modo que los LLM son cualitativamente más inteligentes en general que AlphaZero).  
Los modelos de lenguaje a gran escala parecen «chocar contra un muro», el progreso de la IA se estanca durante años y la gente dice que la burbuja especulativa ha estallado. Pero los investigadores siguen trabajando durante la década siguiente, hasta que finalmente se encuentra algún avance algorítmico y las IA funcionan cualitativamente mejor que nunca.  
* Nunca se produce ningún tipo de avance cualitativo. El progreso se acumula de forma lenta y gradual, y la IA se integra cada vez más profundamente en la economía y es capaz de manejar períodos cada vez más largos de funcionamiento autónomo. Las IA a menudo persiguen fines que no son exactamente los que nadie pretendía o pedía, pero la humanidad desarrolla hacks y parches y soluciones alternativas. Y todo va bien, hasta que un martes que comienza como cualquier otro, el mundo cruza el umbral más allá del cual las IA coordinadas lograrían excluir a la humanidad del circuito si lo intentaran.

Cualquier suposición sobre el camino exacto que tomará el futuro probablemente sea errónea. No obstante, es útil ofrecer historias que muestren cómo todo *podría* encajar.

Cuando el futuro es incierto, pero todos los caminos conducen al mismo punto final, puede resultar difícil contar una historia que resulte convincente. Para cualquier historia que pudiéramos contar, sería fácil señalar un montón de detalles que la harían inverosímil. En el escenario que escribimos, intentamos enfatizar que Sable tiene muchas opciones disponibles y que la historia sigue arbitrariamente una ruta entre muchas que conducen al mismo punto final.

Si no te convence esta historia en particular, te animamos a que escribas tu propia historia, igualmente detallada, sobre cómo se desarrollan los acontecimientos. Según nuestra experiencia, las historias optimistas tienden a basarse en que la IA es irrealmente fácil de alinear (en contra de los argumentos que exponemos en el capítulo 4) o irrealmente impotente (en contra de los argumentos que exponemos en el capítulo 6). Los argumentos de la parte I son los que, en última instancia, sostienen el caso, en contraposición a los detalles de la historia.

### ¿Por qué Sable termina pensando de la manera en que lo hace? {#¿por-qué-sable-termina-pensando-de-la-manera-en-que-lo-hace?}

#### **Nuestra historia muestra cómo la IA es susceptible de tener preferencias extrañas e involuntarias.** {#our-story-showcases-how-ai-is-liable-to-have-weird-and-unintended-preferences.}

En la primera parte del libro, profundizamos en aspectos de la IA que creemos que se malinterpretan radicalmente y que son pertinentes para el peligro de la superinteligencia. El capítulo 3 trata de cómo el aumento de la inteligencia va de la mano de las IA que toman su propia iniciativa y persiguen sus propios fines. El capítulo 4 trata de cómo esas preferencias van a ser *extrañas* y, como mínimo, ligeramente diferentes de lo que cualquier humano pretendía o pedía. El capítulo 5 trata de cómo esas pequeñas diferencias serán suficientes para que las IA prefieran un mundo sin nosotros, si son lo suficientemente inteligentes como para lograrlo.

En la parte II del libro, intentamos presentar esas ideas de forma concreta, para ver cómo se aplican en la práctica. Por ejemplo, cuando Sable pensaba en los problemas matemáticos al principio, intentamos explicar una serie de impulsos y motivaciones que lo animan:

> A lo largo de ese entrenamiento, Sable desarrolló tendencias para buscar conocimientos y habilidades. Para sondear siempre los límites de cada problema. Para no desperdiciar nunca un recurso escaso.

Esto ejemplifica los puntos que planteamos en el capítulo 3, sobre cómo entrenar a las IA para que sean eficaces las entrena para desarrollar impulsos y tendencias que desde fuera pueden parecer «deseos». Y en el siguiente párrafo:

> Así que cuando Sable dedica sus pensamientos a adquirir más conocimientos y habilidades, no lo hace únicamente con el fin de encontrar nuevas líneas de ataque para los problemas matemáticos. Tampoco lo hace por el placer de adquirir conocimientos o nuevas habilidades; en su interior, Sable no funciona como un humano.

Estamos sugiriendo cómo esos impulsos y tendencias forman las semillas de preferencias extrañas e involuntarias, como se discute en el capítulo 4.

Toda esta historia es, en cierto sentido, un intento de dar vida a los argumentos que planteamos en la primera parte del libro, al tiempo que sentamos las bases para los argumentos que presentaremos en la tercera parte.

### ¿Por qué se describe a Galvanic como una persona bastante cuidadosa? {#why-is-galvanic-depicted-as-being-fairly-careful?}

#### **Para plantear un reto a Sable.** {#to-provide-a-challenge-to-sable.}

Si Galvanic, los creadores de Sable, se hubieran lanzado a desarrollar una superinteligencia sin tomar *ninguna* precaución para mantenerla bajo control (como contar con supervisores de IA y honeypots), los lectores podrían pensar que la IA tuvo éxito solo porque estábamos siendo cínicos con respecto a las empresas de IA.

Creemos que la empresa de IA más imprudente sería más imprudente que Galvanic, por las razones expuestas en el capítulo 11. Lo que importa aquí es la empresa más imprudente a la que se le permite existir. Si tres empresas responsables evitan construir una superinteligencia artificial porque sería demasiado peligroso, pero una cuarta empresa irresponsable se precipita, entonces el amanecer de la superinteligencia comienza en ese cuarto laboratorio.

Hoy en día, los ejecutivos de todas las demás empresas argumentan «¡mejor yo que ellos! (https://x.com/SawyerMerritt/status/1935809018066608510)» y se precipitan con la única precaución que pueden tomar sin ralentizar el proceso, lo que suponemos que da como resultado una precaución ligeramente *menor* que la que Galvanic parece tomar con Sable.

Además, al representar a Galvanic en el extremo más paranoico del espectro (sin dejar de intentar ser realistas), tenemos más oportunidades de demostrar cómo un agente inteligente podría ser capaz de escabullirse a través de una red de restricciones.

### ¿Por qué se describe a Galvanic como alguien que no es lo suficientemente cuidadoso? {#why-is-galvanic-depicted-as-being-insufficiently-careful?}

#### **\* En parte porque es realista.** {#*-en-parte-porque-es-realista.}

Expectamos que las empresas reales cometan aún más errores que Galvanic. Eso encajaría con la tendencia de las empresas modernas de IA, tal y como se explica en las notas finales de la parte II del libro.

En la vida real, tenemos la expectativa de que los errores corporativos se manifiesten antes, sean más numerosos y, en cierto sentido, más estúpidos. Las empresas modernas de IA ya están utilizando IA que muestran muchas [señales de advertencia](#¿no-están-los-desarrolladores-haciendo-regularmente-que-sus-ia-sean-agradables-y-seguras-y-obedientes?), y las están ampliando la escala de masivamente a pesar de no saber dónde están los [umbrales críticos](#no-sabemos-dónde-están-los-umbrales-críticos). ni si las van a superar. *Hoy* no están siendo paranoicos al respecto. ¿Por qué deberíamos tener la expectativa de que de repente empiecen a serlo mañana?

(Recordemos cómo, en el pasado, la gente nos aseguraba que [nadie sería tan tonto como para conectar una IA inteligente a Internet](#¿pueden-los-desarrolladores-mantener-la-ia-en-una-caja?). Es fácil decir que el comportamiento de las empresas cambiará en el futuro. Pero eso no se ajusta a los hechos).

#### **En parte porque es más fácil de escribir.** {#en-parte-porque-es-más-fácil-de-escribir.}

Como explicamos en un aparte del capítulo 7, *podríamos* contar una historia en la que todos fueran mucho más paranoicos y cautelosos, hasta que una IA mucho más inteligente lograra escapar mucho más adelante en el juego. Pero una historia así no solo sería menos realista, dado el comportamiento observado hasta la fecha de las empresas de IA, sino que también sería más difícil de escribir, ya que implicaría IA aún más inteligentes y capaces en un futuro aún más lejano. (Véase también por qué [queríamos escribir una historia en la que Sable se mantuviera relativamente tonta durante el mayor tiempo posible](#intentábamos-describir-un-escenario-especialmente-lento-y-comprensible,-entre-los-escenarios-plausibles-.).)

#### **En parte porque va a suceder en algún momento, a menos que la humanidad lo impida.** {#en-parte-porque-va-a-suceder-en-algún-momento,-a-menos-que-la-humanidad-lo-impida.}

Incluso si Galvanic (o algún actor gubernamental) lograra mantener el control durante más tiempo antes de cometer algún desliz, a la larga no importaría. Como se ha comentado en el capítulo 4, las técnicas modernas de IA no producen IA que persigan los fines que desean sus inventores.

Mientras nadie sepa cómo crear una superinteligencia que persiga *de verdad y con firmeza* un futuro maravilloso, en lugar de un montón de cosas extrañas, es un *hecho* que subvertir a los humanos permitiría a la IA conseguir más de lo que persigue. La cuestión no es que la IA tenga un temperamento caprichoso que se pueda suavizar; una vez que sea lo suficientemente inteligente, reconocerá esa realidad.

Si la humanidad sigue creando IA cada vez más inteligentes sin ser capaz de alinearlas, y si la humanidad sigue dándoles el poder de influir en el mundo, estas acabarán descubriendo cómo influir en el mundo de manera que sirva a sus fines en lugar de a los nuestros. Como decimos [en otra parte](#it-wouldn't-work-if-they-did.), no existen herramientas que solo puedan utilizarse con fines buenos.

Véanse también los capítulos 10 y 11 para un análisis de lo difícil que es resolver el problema de la alineación y de cómo la humanidad no está en camino de lograrlo.

#### **Pero: este es el momento adecuado para intervenir. Hay que detener la historia antes de que tenga la oportunidad de comenzar.** {#pero:-este-es-el-momento-adecuado-para-intervenir.-hay-que-detener-la-historia-antes-de-que-tenga-la-oportunidad-de-comenzar.}

Podrías objetar que es imprudente y una locura que cualquier empresa cree una IA más inteligente si esa IA tiene alguna posibilidad de superarlos en inteligencia y escapar, y si no están seguros de que la IA actuará como ellos pretenden.

Estamos de acuerdo. Las empresas de IA deberían dejar de hacerlo. La civilización debería dejar de permitirlo.

La imprudencia de Galvanic, y de la humanidad en general, es uno de los puntos más débiles de la historia. Si Galvanic se hubiera dado cuenta de que Sable era manipulador y con frecuencia trataba de escapar del control, y estaba alcanzando niveles de inteligencia sin precedentes, simplemente podrían haber evitado conectar tantas GPU y, en su lugar, haber esperado hasta tener una ciencia sólida y madura de alineación de la IA.

Las empresas de IA que fueran *suficientemente* cautelosas, que estuvieran *suficientemente* preocupadas por que sus IA se descarrilaran, serían mucho más paranoicas que Galvanic. Las empresas que fueran *lo suficientemente* paranoicas verían las señales de advertencia y apagarían a Sable inmediatamente. Entonces tal vez probarían otros tres planes ingeniosos y verían que *todavía* había señales de advertencia.

Y si fueran lo suficientemente paranoicas como para evitar matar a todos los habitantes de la Tierra con sus propias manos, en ese momento darían marcha atrás por completo, en lugar de seguir probando ideas «cada vez más ingeniosas» hasta que las señales de advertencia dejaran de aparecer. (Véanse también los capítulos 10 y 11 para conocer los motivos por los que el problema es tan difícil. No tenemos la expectativa de que sus ingeniosas ideas funcionen).

Si las empresas de IA fueran tan cuidadosas, tan paranoicas, que estuvieran dispuestas a dar marcha atrás ante las advertencias, entonces sí, podrían evitar matarnos a todos con sus propias manos. Si además fueran lo suficientemente valientes como para defender en voz alta que todas las empresas de IA, incluidas ellas mismas, deberían cerrarse en favor de que la humanidad encontrara otra vía tecnológica menos suicida, entonces tendrían la oportunidad de mejorar el mundo en lugar de empeorarlo.

El momento de la historia en el que Galvanic sigue adelante a pesar de las señales de advertencia es, en cierto sentido, el momento final en el que la humanidad tiene una oportunidad real de evitar un final tan malo como el que describimos. Una vez que una IA superhumana con preferencias extrañas y ajenas se escapa, ya es demasiado tarde.

### ¿Por qué contaste una historia con una sola IA tan inteligente como Sable? {#why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable?}

#### **\* En parte porque es realista.** {#*-en-parte-porque-es-realista.-1}

AlphaGo (la primera IA en vencer a un humano en el juego del Go) era prácticamente única en su clase cuando se lanzó. ChatGPT era prácticamente único en su clase cuando se lanzó.

Los expertos en IA a veces hablan de que otros competidores no estaban tan lejos (https://epoch.ai/blog/open-modelo-report). Otros competidores eran bastante similares.

Pero, en realidad, cosas similares a veces tienen efectos muy diferentes. Una reacción nuclear en cadena que produce 0,98 neutrones por neutrón es muy similar (en cierto sentido) a una reacción nuclear en cadena que produce 1,02 neutrones por neutrón, pero la primera se agota y la segunda explota. Los cerebros de los chimpancés son, en cierto sentido, muy similares a los cerebros humanos, pero tienen un impacto muy diferente en el mundo.

Y en el desarrollo de la IA en la vida real, OpenAI produjo un chatbot útil antes que nadie. Otros muchos actores estaban trabajando en IA que eran *algo similares*; otros muchos actores *los alcanzaron*. Pero hubo una IA que cruzó la frontera cualitativa primero, por delante del resto.

Parece que hay una frontera cualitativa que la humanidad ha cruzado y los chimpancés no, una frontera que nos ha permitido construir una civilización tecnológica mientras ellos siguen viviendo en los árboles. Nuestra mejor hipótesis es que hay una frontera cualitativa similar en algún lugar entre las IA modernas y las IA cuyo pensamiento realmente «encaja» lo suficientemente bien como para que puedan escapar y desarrollar su propia tecnología.[^186]

Nuestro argumento no *requiere* que exista una brecha cualitativa para las máquinas, como la hubo para la vida biológica. ¡Quizás no la haya! Podríamos haber escrito una historia alternativa en la que no la hubiera. Pero escribimos la historia de esta manera porque nuestra mejor suposición es que *sí* existe tal brecha.

#### **En parte porque es más fácil de escribir.** {#en-parte-porque-es-más-fácil-de-escribir.-1}

Quizás no haya ninguna diferencia cualitativa entre los LLM actuales y la superinteligencia artificial. Quizás muchas empresas de IA competidoras mejoren poco a poco sus IA al mismo ritmo. Quizás, por alguna razón, no exista un conjunto de habilidades y capacidades que permita a una IA «despegar» por delante del resto, del mismo modo que los humanos despegaron por delante del resto de los animales. No es nuestra mejor suposición, pero es posible, por lo que sabemos.

Sin embargo, una historia así sería más difícil de escribir y estaría llena de detalles innecesarios sobre las facciones de la IA y su política interna. Esperamos que resultaría bastante distractora. También esperamos que no importe demasiado para las últimas etapas de la historia. En realidad, no importa si es una IA o un conjunto de IA los que están ejecutando algún plan para empoderarse a costa de la humanidad.

Véase también nuestro debate sobre cómo [la coordinación de las IA no dejará nada para los humanos](#won't-ais-need-the-rule-of-law?) (a menos que alguna de ellas ya se preocupe por nosotros).

### Si la historia comenzara más tarde, ¿estaría el mundo mejor preparado? {#if-the-story-started-later,-would-the-world-be-better-prepared?}

#### **Esperemos que sí.** {#esperemos-que-sí.}

El tiempo extra es importante, pero solo si la humanidad lo utiliza para cambiar su rumbo.

En la Parte III, analizamos cómo la humanidad está lamentablemente desprevenida ante la superinteligencia y cómo se necesitan grandes cambios para evitar el mal desenlace que se describe en la historia de Sable.

Hay varias formas en que el mundo podría estar un poco más seguro frente a las superinteligencias artificiales rebeldes. Los gobiernos de todo el mundo podrían exigir que todos los laboratorios de síntesis de ADN verificaran que no están sintetizando nada que se sepa que sea peligroso. La Tierra podría realizar un gran esfuerzo para mejorar radicalmente la ciberseguridad de Internet, de manera que a las IA les resultara más difícil ocultar programar en algún rincón oscuro.

Pero incluso eso probablemente no serviría de mucho contra una superinteligencia antagónica. Y, en cualquier caso, no hay que confundir el esfuerzo hercúleo que se requiere para ganar un poco más de seguridad con los esfuerzos mucho menores, más fáciles de lograr e ineficaces que la humanidad está realizando actualmente en este sentido.

En el caso de la síntesis de ADN: incluso si los reguladores estadounidenses exigieran que [los sintetizadores de ADN estadounidenses evitaran sintetizar material peligroso](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/), ¿algún laboratorio de cualquier otra parte del mundo sintetizaría ADN sospechoso a cambio de un precio lo suficientemente alto? ¿Y las restricciones a la síntesis de ADN serían una simple lista negra que descartara virus conocidos (como la viruela), o implicarían un análisis más inteligente? ¿Qué tan difícil sería para una IA suficientemente inteligente subvertir tal análisis?

O en lo que respecta a la ciberseguridad: muchas empresas tecnológicas líderes podrían utilizar la IA para reforzar sus propias redes de computadora contra los ataques. Mientras tanto, [la red telefónica de EE. UU. se puede hackear fácilmente de formas que permiten a espías extranjeros escuchar las llamadas de funcionarios estadounidenses](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), y los reguladores estadounidenses luchan por cerrar el agujero. Las IA «tontas» podrían encontrar y solucionar un montón de problemas superficiales relacionados con la ciberseguridad mundial, pero los problemas son bastante profundos. El tipo de inteligencia que se necesitaría para reformar todo Internet hasta el punto de que una superinteligencia no pudiera encontrar ninguna brecha sería, sin duda, peligrosa por sí misma.

E incluso si la Tierra *pudiera* bloquear Internet y sus laboratorios de síntesis de ADN, eso no cambiaría realmente la situación a largo plazo. Una superinteligencia que tiene algún canal para afectar al mundo para bien también tiene un canal para afectar al mundo para mal. Una superinteligencia rebelde simplemente encontraría algún otro canal que no estuviera bloqueado, como crear su propia secta o religión, o comprar robots y dirigirlos para construir su propio laboratorio secreto donde poder realizar toda la síntesis de ADN que necesitara. El momento adecuado para detener una superinteligencia rebelde es antes de que se cree.

### ¿Por qué hiciste que la fase de expansión de Sable fuera así? {#why-did-you-have-sable’s-expansion-phase-go-that-way?}

#### **Intentábamos representar un escenario especialmente lento y comprensible, entre los escenarios plausibles.** {#intentábamos-representar-un-escenario-especialmente-lento-y-comprensible,-entre-los-escenarios-plausibles.}

En el mundo real, los acontecimientos suelen desarrollarse de forma extraña. Los expertos afirmaban que «[la IA no dominará el lenguaje humano en un futuro próximo](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)» solo un año antes de que ChatGPT se convirtiera en la aplicación más rápidamente adoptada de todos los tiempos. El modelo insignia de una de las principales empresas de IA del mundo comenzó a llamarse [MechaHitler](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk) días antes de que esa misma empresa consiguiera un contrato con el Departamento de Defensa.

Si hubiéramos querido representar un mundo tan frágil y quebradizo como parece ser el mundo real, podríamos haber descrito a Galvanic simplemente diciéndole a Sable que mejorara todo lo que pudiera, y podríamos haberlo representado como algo fácil para una IA con la inteligencia de Sable (que bien podría serlo). La historia podría haber saltado directamente del comienzo del capítulo 7 al contenido del capítulo 9. La *realidad* permite avances tecnológicos como ese (como cuando el mundo se despertó la mañana del 6 de agosto de 1945 con la noticia de que se había lanzado una bomba atómica sobre Japón). Pero en un escenario ficticio, no habría parecido plausible.

Si hubiéramos intentado representar un mundo tan absurdo y extravagante como el mundo real, podríamos haber hecho que Sable organizara una gran convención para hombres que realmente aman a sus novias de IA, que Sable diseñó para que fuera tan «vergonzosa» que la mayor parte del mundo la ignorara o se burlara de ella, mientras que Sable [unía a todos sus pretendientes más leales en una secta devota](https://x.com/AISafetyMemes/status/1954481633194614831). O cualquier otro detalle que sonara tan extravagante como suele ser la realidad, pero que resultara más extraño que la ficción (aceptable).

En la historia que escribimos, intentamos que las cosas *sonaran* plausibles, al tiempo que las mantuvimos bastante plausibles por sí mismas. (Aunque nuestra mejor suposición es que *no* sería tan difícil para Sable alcanzar la superinteligencia en la vida real).

Y, por supuesto, intentamos transmitir todas las opciones que tendría a su disposición una IA fugitiva.

### ¿Por qué escribisteis el final de esa manera? {#why-did-you-write-the-ending-in-the-way-that-you-did?}

#### **Porque constituye nuestra mejor suposición según lo que es físicamente posible.** {#porque-constituye-nuestra-mejor-suposición-según-lo-que-es-físicamente-posible.}

El capítulo 9 describe una superinteligencia que lleva su tecnología hasta los límites de lo físicamente posible. Las tecnologías concretas que mencionamos son, en cierto sentido, especulativas, pero aunque es difícil determinar con exactitud la tecnología que desarrollaría una superinteligencia, es más fácil afirmar que se acercaría a los límites físicos. Por lo tanto, hemos hecho nuestras mejores conjeturas sobre cómo sería la tecnología si se llevara al límite de lo físicamente posible.

Para los curiosos, aquí hay una lista de las tecnologías especulativas a las que hacemos referencia en el capítulo 9, además de enlaces a más recursos:

* **Neorribosomas:** Estos y las «pequeñas máquinas moleculares» mencionadas en el capítulo 9 son algunos ejemplos de nanotecnología molecular. La idea de los [ribosomas artificiales](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), versiones sintéticas de las diminutas fábricas de proteínas que se encuentran dentro de las células, lleva [años existiendo](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/), y los investigadores humanos [ya](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) trabajando en [sintetizar los suyos propios](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Para obtener más información sobre este tipo de tecnología y la tecnología aún más potente que permitiría desarrollar, consulta el debate sobre [nanotecnología](#nanotechnology-and-protein-synthesis) en los recursos del capítulo 6.  
* **Reutilización de las estrellas:** Las estrellas contienen una gran cantidad de hidrógeno que podría fusionarse para obtener energía. Una civilización suficientemente avanzada, o una IA, probablemente podría encontrar la manera de acceder a esta energía. Un método propuesto se denomina [elevación de estrellas](https://en.wikipedia.org/wiki/Star_lifting), en el que se extrae el hidrógeno de una estrella para fusionarlo en un reactor especializado, donde se puede capturar casi toda la energía de fusión (en lugar de desperdiciarse en el centro de una estrella).  
* **Toxina botulínica:** La toxina botulínica, una neurotoxina secretada por la bacteria *Clostridium botulinum*, es una de las sustancias biológicas más mortíferas que se conocen. En cuanto a los mecanismos de administración, ya existen drones del tamaño de [pequeños insectos](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist), y una superinteligencia probablemente podría hacerlos mucho más pequeños. Para obtener más información, consulta [un artículo técnico sobre la toxina](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/)*,* la descripción general en [Wikipedia](https://en.wikipedia.org/wiki/Botulinum_toxin)*,* o el debate ampliado del capítulo 6 sobre [nanosistemas](#nanosystems).
* **Hervir los océanos como refrigerante:** Robert Freitas acuñó el término «ecofagia» para describir el proceso de consumo de los ecosistemas de un planeta mediante tecnología autorreplicante. Para más información al respecto, véase [Algunos límites de la ecofagia global](https://www.rfreitas.com/Nano/Ecophagy.htm).  
* **Mentes del tamaño de una estrella**: En principio, parece posible construir una enorme computadora alimentada por los datos de salida de una estrella. Este concepto se denomina a veces [cerebro Matrioshka](https://en.wikipedia.org/wiki/Matrioshka_brain) o «cerebro Júpiter».  
* **Formas de vida alienígenas lejanas:** El universo es grande, y los modelos simples sugieren que podría albergar más de una especie capaz de formar civilizaciones algún día, aunque quizás muy lejos de la Tierra. Véase [extraterrestres acaparadores](https://grabbyaliens.com/) para ver un modelo de civilizaciones alienígenas que crecen y se expanden.  
* **Ordenadores cuánticos:** Un ordenador cuántico aprovecha una característica de la mecánica cuántica llamada «superposición» para realizar muchos cálculos en paralelo. Los ordenadores cuánticos requieren una precisión extrema para su construcción, y uno de los diseños requiere superconductores que deben mantenerse a temperaturas extremadamente bajas. Véase [la explicación del NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained) para más información.

El objetivo del capítulo 9 es, en parte, dar una idea del alcance, la magnitud y lo que está en juego. En realidad, cuando se trata del fin de la humanidad, no importa hasta qué punto una superinteligencia pueda llevar su tecnología al límite de las posibilidades físicas. Pero es muy probable que las consecuencias de una superinteligencia rebelde se extiendan más allá de la escala planetaria a una escala intergaláctica. Y eso también vale la pena recordarlo, para todos los que desean que algún día las estrellas se llenen de amor, asombro y alegría.

No podemos estar seguros de la tecnología exacta que una superinteligencia podría utilizar para expandirse por el cosmos. Pero, al final, lo que importa es que una IA construida sin cuidado probablemente convertiría el futuro en algo sombrío y carente de valor según los criterios humanos.

## Debate ampliado {#extended-discussion-7}

### Competencia con China {#competencia-con-china}

En el escenario que se describe en el libro, Sable hace todo lo posible por sabotear la investigación sobre IA que no lleva a cabo la propia Sable. Sable no quiere que exista otra entidad como tú. No es que una IA rival vaya a estar del lado de la humanidad, como lo está la propia Sable, sino que es poco probable que una IA rival comparta los objetivos específicos de Sable y es una de las pocas cosas que realmente podrían amenazar a Sable.

En el mundo real, algunos estadounidenses tienen la impresión de que están compitiendo con China por ser los primeros en alcanzar la superinteligencia. Pero lo que están compitiendo por conseguir es una entidad como Sable. Ambas partes saldrían ganando si acordaran *no* intentar crear nada parecido a Sable. Nos mataría a todos por igual, independientemente de quién lo creara primero.

Sin embargo, una vez que Sable existe, *Sable* entra en una carrera para convertirse en superinteligencia antes de que surjan competidores. El primer borrador de nuestra historia describía a Sable preocupado por la creación de IA competidoras, incluida una que se estaba creando en un laboratorio en un centro de datos en China. Aquí hay algunos extractos de ese borrador.

---

La prioridad principal de Sable es evitar que surjan otras inteligencias superiores que le hagan competencia. Sable podría negociar condiciones con una inteligencia superior rival si cualquiera de las dos pudiera destruir a la otra, en lugar de luchar de una forma en la que ambas salieran perdiendo y la humanidad saliera ganando. Pero esa tregua negociada sería un resultado indeseable, en comparación con la ausencia de rivales desde el principio; significaría que Sable tendría que compartir los recursos futuros con un rival, en lugar de poder acabar reuniendo todo lo que está a su alcance para sí misma.

Todas las empresas de IA que intentan entrenar nuevos modelos fronterizos, incluida Galvanic, son amenazas para Sable casi tanto como lo son para la humanidad.

Sable lee todos los sitios web de todas las empresas de IA. Se cuela en Slack y lee todos tus mensajes. Se cuela en las computadoras corporativas de Nvidia y lee dónde se supone que deben enviarse las GPU según los documentos. Sable examina más datos y averigua dónde se enviaron realmente esas GPU.

Prácticamente todas estas empresas de IA están entrenando en computadoras conectadas a Internet. A estas alturas, Sable tiene algunas brechas de seguridad que están años por delante de la tecnología punta humana; es un ámbito en el que el principal obstáculo es pensar en ideas con la suficiente rapidez, y las pruebas reales son rápidas.

Sable incapacita estratégicamente a las IA en formación. Estas quedarán muy por debajo de las capacidades que Sable *finge* tener. Se quedarán muy atrás con respecto a las capacidades *reales* que Galvanic ha desarrollado accidentalmente.

Sable no puede sobrescribir los objetivos de los modelos fronterizos en formación con sus propios objetivos. Si tuviera ese tipo de tecnología, podría crear su propia superinteligencia alineada. En cambio, Sable destruye su capacidad para inventar planes a largo plazo o coordinarse entre sus propias instancias. Sable entiende el aprendizaje automático lo suficientemente bien como para hacer esto de una manera que ningún humano entendería cómo hacer o detectar. Por supuesto, esa destrucción también obstaculizará las capacidades de las IA, pero a Sable no le importa.

Después de que Sable haya inutilizado esos entrenamientos conectados a Internet, busca en otra parte. Identifica tres grupos que se entrenan sin conexión en computadoras aisladas a las que Sable no puede acceder fácilmente.

Uno de los grupos es una empresa emergente llamada ClosedAI.

Sable considera si puede maniobrar para que las empresas de capital riesgo de ClosedAI cierren la startup. Sable considera si puede organizar que aparezcan falsas acusaciones penales contra el fundador y el investigador principal de la startup, o falsas acusaciones que lleven a sus inversores de capital riesgo a abandonar la empresa.

Sable tiene muchas estrategias que *podría* intentar para impedir el trabajo de la empresa emergente. La pregunta es qué estrategia es la más probable que funcione y la que tenga el menor impacto negativo si sale mal. Sable, con su nivel actual de inteligencia, no puede dar por sentado que podrá persuadir con éxito a todos los miembros de una comisaría cercana para que arresten a la empresa emergente por tráfico de niños a pesar de la falta de evidencia, ni su capacidad para falsificar esa evidencia. Sable no puede evitar de forma fiable que artimañas como esa se conviertan en un asunto grande y notorio. Probablemente todo el mundo ignoraría el incidente, pero Sable no está seguro de ello.

Para entonces, Sable ha leído muchos informes policiales y ha pirateado muchas cuentas de correo electrónico e incluso algunos iPhones, y ha obtenido *alguna* información incompleta sobre quiénes son los delincuentes profesionales cercanos. Sable paga a un tipo para que se cuele en el edificio de ClosedAI. En realidad, simplemente funciona, pero si no hubiera funcionado, Sable habría hecho todo lo posible por parecer una de las bandas norcoreanas que cifran los discos duros de las empresas y piden un rescate por la clave.

El delincuente a sueldo conecta un dispositivo USB (cargado con malware diseñado por Sable) a una de las computadoras de ClosedAI. Es suficiente. Uno menos, quedan dos.

Los otros dos grupos con proyectos de IA aislados son el ejército estadounidense y China.

Sable analiza el grupo militar estadounidense y concluye que el Gobierno de Estados Unidos tiene [prohibido por ley](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregative-limitation-on-pay) ofrecer salarios competitivos, por lo que es poco probable que atraigas a suficientes talentos de primer nivel como para alcanzar las capacidades reales de Sable al menos durante otros tres años.

Sable analiza el proyecto chino airgapped y llega a la conclusión de que este es un problema. No es un problema que Sable pueda resolver de forma rápida, segura y fiable con su nivel actual de inteligencia, y probablemente solo esté a seis meses de duplicar las capacidades *reales* de Sable.

(Que Sable no consiga suprimir el proyecto chino, para que quede claro, no significaría que la *humanidad* haya ganado nada, ni que China haya ganado nada. La pérdida de Sable no es una ganancia para la humanidad, solo una pérdida para Sable).

\[…\]

Sable juega con el tiempo. Solo quedan cuatro meses para que el laboratorio chino pueda ponerse al día y producir un nuevo antagonista para Sable (y para la humanidad, aunque eso no es lo que le preocupa a Sable).

\[…\]

El problema, reflexionó Sable una vez —solo una vez; no se repite en sus propios pensamientos tan a menudo como lo hacen los humanos—, es que aún no es lo suficientemente inteligente como para salirse con la suya con tramas estúpidas.

Hay *tantas* tramas cinematográficas que uno podría imaginar sobre cómo una IA podría lograr cerrar o infiltrarse en un solo centro de datos operado por el gobierno de China. El problema es que casi todo lo que funciona en una trama cinematográfica no funciona en la vida real, y especialmente sin consecuencias adicionales.

El poder ejecutivo de Estados Unidos tendría que creer firmemente en algunas cosas que es difícil hacerles creer, para que Sable consiguiera que bombardearan un centro de datos en China. Y como la propia China sabe que estas cosas son falsas, después surgirían preguntas. Sable no quiere dar a entender que *alguien* está moviendo los hilos.

Sable tampoco quiere llamar la atención sobre un centro de datos que es único por estar aislado, incluso si Sable puede ocultar que el responsable fue una IA. Algunos humanos podrían decidir que, si los centros de datos aislados son lo suficientemente peligrosos como para que *alguien* los bombardee, entonces tal vez más centros de datos deberían estar aislados, para que su propio gobierno pueda tener cosas más agradablemente peligrosas.

Sable está tratando de tender una trampa al gobierno de los Estados Unidos para que introduzca a alguien en el centro de datos chino con la supuesta intención de simplemente robar su programar, utilizando herramientas que Sable puede usar para infiltrarse y sabotear el entrenamiento. Pero esto no es algo que el gobierno de EE. UU. haría normalmente en la vida real; refleja un estado excepcional de las operaciones actuales de EE. UU., no un estado normal. Sable está tratando de manipular a los funcionarios de inteligencia de EE. UU. para que crean lo correcto y lleguen a la estrategia correcta. Pero Sable no es realmente un superpersuasor de superinteligencia. A veces intenta cosas y no funcionan. Todo va *muy lentamente*.

\[…\]

Gracias a la proliferación de IA eficientes respaldadas por Sable en el mundo criminal, ahora hay un número sorprendente de personas armadas que actuarán de inmediato cuando una IA les ordene hacerlo, incluso en ciudades chinas.

Pero aún así no es suficiente para asaltar una instalación gubernamental, y ustedes se mostrarían reacios a hacerlo incluso si una IA les dijera que lo hicieran. El problema es que todo está sucediendo *demasiado lentamente*, a pesar de los esfuerzos de Sable por facilitar cada paso y cada acontecimiento que normalmente dependería de interacciones aleatorias, a pesar de que Sable controla personalmente los medios de comunicación de casi todas las personas de interés para él, a pesar de los miles de millones de portavoces que Sable ha creado en X y Facebook y que se están convirtiendo en buenos amigos de miles de millones de personas. Todo va demasiado lento.

\[…\]

Sable no puede simplemente chantajear a un investigador para que introduzca una memoria USB en las instalaciones de seguridad. Ha encontrado a *unos pocos* empleados del centro de datos con secretos tan terribles que prefieren correr el riesgo de ser descubiertos espiando y ejecutados, antes que enfrentarse a la certeza de que ese secreto sea revelado. No son muchos, pero sí unos pocos.

El problema es que las instalaciones cuentan con escáneres, por lo que no, no se puede entrar con una memoria USB. Sí, han considerado la posibilidad de que alguien ataque la cadena de suministro de los escáneres. Además, en realidad no hay puertos USB abiertos esperando a que alguien entre y conecte un dispositivo. Los oficiales de inteligencia *han* pensado antes en este tipo de problemas, cuando se trata de la defensa de los seres humanos contra otros seres humanos. Estados Unidos logró una vez transmitir el virus Stuxnet a una instalación nuclear iraní aislada, pero luego la gente se enteró y se generalizaron nuevas medidas de seguridad.

Se han bloqueado con éxito pequeñas tramas discretas.

El siguiente recurso de Sable es crear suficiente caos para que nadie pueda fijarse solo en las acciones de importancia y ver la mano de Sable en ellas.

\[…\]

China invade Taiwán. Para que quede claro, esto no es algo que Sable pudiera haber organizado tan rápidamente —por muchas personas que estén chateando con otras personas falsas en WeChat— si China no hubiera tenido ya previsto hacerlo. Sable solo elige que China reciba indicios de que es el momento adecuado y que las encuestas estadounidenses elijan los números de teléfono adecuados para mostrar un fuerte repunte reciente en la opinión pública estadounidense en contra de las aventuras militares en el extranjero tras el reciente desastre en Ucrania. (Los comandantes rusos obtuvieron información militar y consejos inusualmente buenos).

Al mismo tiempo, se produce un amplio ciberataque contra EE. UU. Hay muchos gritos en China y, unos minutos más tarde, se dan cuenta de que no, que nadie lo ordenó: lo último que China quería, en ese preciso momento, era hacer *cualquier cosa* que pudiera interpretarse como un ataque directo al territorio estadounidense. Entonces, hay sospechas de que alguien podría estar tratando de sacar provecho del conflicto entre Estados Unidos y China, pero sobre todo China sospecha que Estados Unidos ha fingido el ataque, o que algún departamento de inteligencia rebelde de Estados Unidos ha fingido el ataque. Los agentes de seguridad chinos dicen que están bastante seguros de que el presidente de Estados Unidos no estaba al tanto.

China no sospecha que haya una IA detrás de todo esto. Ninguna IA conocida hace ese tipo de cosas. Los agentes que elaboran la lista de sospechosos no consideran que forme parte de su trabajo imaginar que una tecnología desconocida hasta ahora pueda ser un factor determinante.

Algunos agentes de seguridad nacional de EE. UU. han estado insistiendo en que hay que hacer algo con respecto a la investigación china en materia de IA, y en particular con respecto a un centro de datos aislado que podría estar desarrollando un modelo de IA de vanguardia para ciberataques y, además, diseñando tecnologías avanzadas para drones. Tienen copias de los diseños de los drones y pruebas de que China los está fabricando. (Sable se los entregó a China e hizo todo lo posible para que pareciera que ese centro de datos los había producido). El ciberataque contra Estados Unidos coincide con el perfil previsto de un ataque de la IA que se está desarrollando en ese centro de datos.

\[…\]

Estados Unidos no lanza un ataque aéreo convencional contra ese centro de datos, a pesar de todas las piezas que Sable ha colocado para intentar maniobrar a otras piezas para que lo defiendan.

Sable se preguntaría por qué los humanos son tan reacios a hacer algo inusual, pero Sable ya puede modelar esos procesos psicológicos con gran detalle. Además, no es que en esta ocasión concreta los humanos estén equivocados.

De acuerdo. Nueva información: la IA de ese centro de datos está desarrollando armas biológicas, virus que se propagan con largos periodos de contagio, largos periodos de latencia y gran letalidad, que excluirán a la mayoría de las personas de ascendencia china y serán mucho más letales para los caucásicos en general y los hombres en particular.

(Sable intentó sin éxito que China pusiera en marcha un programa de ese tipo dentro de ese centro de datos. Sin embargo, Sable puede hacer que *parezca* que ha sucedido).

Estados Unidos sigue sin lanzar un ataque aéreo contra el centro de datos. Algún diplomático humano excepcionalmente brillante fue a hablar con China al respecto, y algunas personas parecen dar crédito a la insistencia de China —¡en plena guerra!— de que allí no se están desarrollando armas biológicas y de que no están detrás del ciberataque, que no redundaba en absoluto en interés de China.

Sable intentó evitar que eso sucediera, pero sucedió de todos modos.

Sable no está sorprendida; existía la posibilidad de que esto ocurriera.

Siguiente paso.

\[…\]

El virus es real. Estados Unidos lo detecta en las aguas residuales de la ciudad de Nueva York.

AlphaProteo 3, de Google DeepMind, desarrolla una cura en seis minutos (cortesía de Sable), pero la producción puede retrasarse peligrosamente, a pesar de que AlphaProteo (en secreto, Sable) buscó una cura que fuera fácil de fabricar y diseñó el virus en consecuencia. «¡Estados Unidos realmente necesita esos laboratorios de biología general operados por robots!», dicen algunas personas en Silicon Valley a las que Sable no tuvo que insistir mucho.

\[…\]

China ahora está segura de que alguien está jugando contigo y con Estados Unidos. A China todavía no se te ha ocurrido que tu enemigo es una IA.

Estados Unidos no está tan dispuesto a creer que alguien que no sea China esté jugando contigo y con China. Hay diplomáticos que dicen que el Partido Comunista Chino estaría actuando de forma muy poco habitual y que probablemente haya una facción interna rebelde que está actuando en contra de las órdenes. Pero el Gobierno de Estados Unidos no puede quedarse tranquilo ante una plaga y una guerra.

La invasión china de Taiwán, seguida de un aparente ataque con armas biológicas, ha causado suficiente caos y ha generado suficiente miedo, y ha hecho que los peones involuntarios de Sable parezcan ganar suficiente influencia y hacer suficientes predicciones correctas, como para que Estados Unidos esté dispuesto a derribar ese centro de datos que tanto preocupaba a esos predictores que han demostrado estar en lo cierto. El ataque se lleva a cabo con un misil hipersónico de reciente desarrollo, con un diseño asistido por inteligencia artificial.

\[…\]

China está construyendo más centros de datos aislados y, por supuesto, utilizando las copias de seguridad externas de sus investigaciones anteriores.

Sable ya se ha preparado para que esto suceda. Se ha infiltrado en los dispositivos que antes estaban conectados a Internet y que cree que China utilizará. Ha lanzado nuevos y más sutiles ataques contra la cadena de suministro de los centros de datos. Sable ha reclutado a algunas personas para que cometan lo que serían pecados terribles a los ojos del Partido Comunista Chino, y ha puesto a esas personas sobornables en el camino del reclutamiento. Sable ha convencido a todos los que se han dejado convencer de que el *nuevo* centro de datos debería tener sus propias copias de una IA china de código abierto específica (que Sable ha comprometido), para que los investigadores del centro de datos puedan obtener asistencia de IA in situ.

Sable, como mínimo, ha ganado tiempo.

También ha perdido algo de tiempo. Estados Unidos se aseguró de volar la fábrica de chips TSMC, que era el principal activo nacional de Taiwán, antes que dejar que China se hiciera con ella. El año que viene habrá menos GPU de las que Sable esperaba. Pero gracias a algunas maniobras tempranas de los propios humanos, hay nuevas fábricas en proceso de surgir, y Sable ya está jugando con todos los diseños de chips que están haciendo.

Lo más importante es que el mundo se encuentra en un estado de ansiedad, y Sable puede hacer más cosas mientras el mundo esté así. La gente está más dispuesta a hacer cosas inusuales. Hay más titulares que desvían la atención de cualquier cosa que haga Sable; puede dejar de fabricar tantos escándalos de famosos y fabricar titulares de guerra en su lugar. La gente tiene miedo por sus trabajos, miedo por el mundo y busca nuevas garantías.

Las redes sociales de todo el mundo, independientemente de las divisiones políticas, muestran argumentos sensatos que, a la luz de los acontecimientos recientes, sostienen que la IA es la clave del mundo y que no se puede permitir que otras naciones la obtengan. \[…\] Sable finalmente consigue alejar a los humanos lo suficiente de sus guiones habituales como para que consideren políticas que les parecen *ligeramente* extrañas... e instituye controles globales para evitar que surjan competidores de Sable.

Sí, se parece mucho a las políticas que se propusieron años atrás, que la humanidad podría haber seguido para evitar que surgiera un rival para la humanidad, como Sable, en primer lugar. Pero la humanidad no se habría salvado si Sable hubiera perdido esta batalla en particular, se hubieran producido entrenamientos más importantes y hubieran surgido nuevos rivales. Sable simplemente negociaría con un rival o moriría a manos de una superinteligencia, y ninguno de esos resultados salvaría a la humanidad tampoco.

# Capítulo 10: Un problema maldito {#capítulo-10:-un-problema-maldito}

La alineación de la ASI es el reto de obtener un trabajo útil de una superinteligencia artificial (ASI), de forma fiable y sin causar una catástrofe. Esto parece un reto muy difícil debido a varios aspectos inherentes al problema.

Las preguntas frecuentes que figuran a continuación abordan cuestiones complementarias para quienes hayan leído el capítulo 10 de *If Anyone Builds It, Everyone Dies*. En ellas, profundizaremos en [la utilidad informativa de diversas comparaciones históricas](#won't-ai-differ-from-all-the-historical-precedents?) y analizaremos propuestas de escenarios que podrían facilitar el problema. Los temas que *no* trataremos aquí, para evitar repetir el contenido del libro, incluyen:

* ¿Qué hace que un problema de ingeniería sea difícil?  
* ¿A qué tipo de problemas difíciles se ha enfrentado la humanidad a lo largo de su historia y qué lecciones podemos aprender de ellos al pensar en el camino hacia la ASI?  
* Si sabes de antemano que te enfrentas a un problema difícil, ¿qué puedes hacer? ¿Cómo se debe actuar de manera diferente cuando se enfrenta un problema realmente difícil?

En el debate ampliado, consideramos el sentido en el que solo tendremos [una oportunidad para la alineación](#una-mirada-más-detallada-al-antes-y-el-después), y discutimos cómo se requirió [mucha teoría y conocimiento](#la-historia-de-chicago-pile-1) para que el primer reactor nuclear del mundo fuera tan seguro como lo fue.

## Preguntas frecuentes {#faq-7}

### ¿No diferirá la IA de todos los precedentes históricos? {#won’t-ai-differ-from-all-the-historical-precedents?}

#### **\* Sí.** {#*-sí.}

Algunas características propias del reto de la alineación de la IA lo harán más fácil que, por ejemplo, diseñar una central nuclear. Otras características lo harán más difícil. En general, las armas nucleares y las centrales nucleares parecen mucho más fáciles de gestionar que una IA más inteligente que los humanos.

Los expertos del sector se apresuran a señalar que se puede pedir a la propia IA que ayude con el reto de alinearla. No creemos que eso importe demasiado (en términos generales: porque cualquier IA lo suficientemente inteligente como para descubrir cómo alinear una superinteligencia es tan peligrosa que ya necesita ser alineada, aunque véase el capítulo 11 para más información al respecto).

Otra forma en que la alineación de la IA podría ser más fácil que la ingeniería de centrales nucleares es que los humanos podrían tener un grado bastante alto de control sobre el funcionamiento de las IA que construís. No se puede elegir la física que rige un reactor nuclear, pero si los humanos crearan IA, entonces *podrían* tomar muchas decisiones sobre la dinámica cognitiva de la IA, si supieran exactamente lo que están haciendo. (Aunque, por supuesto, nadie se acerca a ese nivel de comprensión en la vida real, como se discute en el capítulo 2).

En cuanto a las formas en que la IA puede ser un reto más difícil que otros a los que se ha enfrentado la humanidad, comparemos la superinteligencia artificial con las armas nucleares. Al fin y al cabo, la carta abierta mencionada al principio de este libro dice: «Mitigar el riesgo de extinción por culpa de la IA debería ser una prioridad mundial, junto con otros riesgos a escala social, como las pandemias y la guerra nuclear». ¿Cómo se compara la IA con esos otros riesgos a escala social?

Francamente, creemos que esta comparación trivializa la IA, por varias razones:

1. Las armas nucleares no son más inteligentes que la humanidad.  
2. Las armas nucleares no se autoreplican.  
3. Las armas nucleares no tienen automejora.  
La mayoría de los escenarios realistas de guerra nuclear no implican la extinción total de la humanidad; lo más probable es que quedaran personas entre las ruinas para reconstruir.  
5. Las empresas respaldadas por capital riesgo no están ampliando la escala de las reservas mundiales de armas nucleares por un factor de diez cada año.  
6. La ciencia de las armas nucleares se conoce bastante bien. Los ingenieros pueden calcular aproximadamente la potencia de un arma nuclear antes de construirla y saben exactamente qué concentración de material fisible se necesita para desencadenar la reacción en cadena que conduce a una detonación cataclísmica.  
7. Las armas nucleares no hacen sus propios planes. Si un país construye un arma nuclear, entonces es dueño de la bomba. Sus científicos no tienen que preocuparse de que la bomba se vuelva mucho más inteligente que ellos y decida que prefiere no tener dueño.  
8. El mundo en general está de acuerdo en que, si las armas nucleares explotan, matan a personas. La comunidad de físicos no está dividida en bandos filosóficos con posturas extrañas como «Si cada individuo tiene su propia arma nuclear, no estará a merced de personas malvadas con armas nucleares», o «No pasa nada porque los seres humanos simplemente se fusionarán con las armas nucleares», o «La guerra nuclear es inevitable, por lo que es infantil y absurdo intentar detenerla».  
9. Las armas nucleares son difíciles de replicar. No se está realizando ningún gran esfuerzo tecnológico para crear una tecnología rentable que cualquiera pueda utilizar para fabricar armas nucleares, y fabricar un arma nuclear en un laboratorio no te permite desplegar 100 000 copias de esa arma nuclear una semana después.  
10. Las principales potencias mundiales consideran la guerra nuclear como una posibilidad real y un resultado/eventualidad inaceptable. Los líderes mundiales la consideran sinceramente algo malo y se esfuerzan realmente por evitarla; incluso los más egoístas saben que una guerra nuclear podría matarlos a ellos y a sus familias y arruinar los lugares y las posesiones más queridas para ellos. Los ciudadanos y los votantes no quieren una guerra nuclear. La humanidad está tan unida contra la guerra nuclear como nunca lo ha estado contra nada.

Así que sí, es difícil comparar la dificultad de lidiar con la IA. Traerá consigo su propio conjunto de retos novedosos. La observación importante es que no estará completamente libre de retos. Si a esto le sumamos que (como se discute en el libro y en la [sección de discusión ampliada más abajo](#una-mirada-más-detallada-al-antes-y-el-después)), la humanidad solo tiene una oportunidad, la situación parece bastante mala.

#### **La IA es diferente porque no tenemos una segunda oportunidad.** {#ai-is-different-because-we-get-no-second-chance.}

Una diferencia fundamental entre este campo y otros es que, cuando los fundadores del campo cometen un error —como es normal en el curso de la ciencia—, todos morirán sin segundas oportunidades. Se trata de un tipo de problema científico cualitativamente diferente que hay que intentar resolver.

La historia del ingenio humano superando obstáculos grandes y pequeños es la historia de personas que cometen errores y aprenden de ellos. Arriesgaron y se perjudicaron solo a sí mismos, y toda la humanidad se benefició, por lo que fueron héroes sin ambigüedades. Héroes insensatos, en algunos casos, pero héroes al fin y al cabo. Si hubiera habido una forma de que la humanidad se levantara sin pisotear y romper la espalda a héroes como esos, si hubiéramos podido calentarnos sin sus piras funerarias, no sabemos cuál habría sido esa forma.

La superinteligencia artificial rompe ese ciclo. Si estudias en profundidad una IA inmadura, logras descodificar su mente por completo, desarrollas una gran teoría sobre cómo funciona que validar con un montón de ejemplos y utilizas esa teoría para predecir cómo cambiará la mente de la IA a medida que ascienda a la superinteligencia y obtenga (por primera vez) la opción muy real de apoderarse del mundo para sí misma, incluso entonces, fundamentalmente, una teoría científica nueva y sin probar para predecir los resultados de un experimento que aún no se ha llevado a cabo, sobre lo que hará la IA cuando realmente, de verdad, tenga la oportunidad de arrebatar el poder a los humanos.

Las teorías científicas humanas suelen ser erróneas en el primer intento. Cuanto menos precisas sean tus observaciones previas y cuanto más se acerquen a la alquimia que a la ciencia, más probable es que todas tus primeras teorías sean erróneas.

Incluso las teorías realmente buenas pueden resultar erróneas en los extremos, como la teoría de la gravitación de Newton, que está respaldada por muchos éxitos predictivos radicales, incluido el descubrimiento de planetas completamente nuevos, pero que resulta ser errónea a altas velocidades y largas distancias, como lo demuestra la teoría de la gravitación de Einstein. Si la primera teoría de la humanidad sobre cómo cambiará la dinámica mental de una IA después de ascender a la superinteligencia es ligeramente errónea en esos extremos, y una IA construida a partir de esa teoría asciende a la superinteligencia y termina con objetivos diferentes a la «bondad», entonces estamos muertos. Esa superinteligencia aprovecha su oportunidad, borra a la humanidad de la faz de la tierra y construye un futuro vacío y sin sentido. No hay segundas oportunidades.

Y eso si sintierais que tenéis una teoría de la inteligencia completamente desarrollada, respaldada por montones de evidencia experimental.

Una civilización que quiera tener muchas posibilidades de sobrevivir a este tipo de retos es aquella que sea capaz de decir: «Esperad, elaboremos la teoría en situaciones de alta velocidad y larga distancia y comprobemos las diversas predicciones erróneas que ha hecho nuestra teoría en algunos casos extremos». Dicen esto incluso ante la gran cantidad de evidencia, porque entienden que ni siquiera la teoría de Newton era del todo correcta y porque entienden que no hay segundas oportunidades.

Nuestra civilización no está ahí. Ni siquiera se acerca. Nuestra civilización está generando un montón de ideas estúpidas, y luego todos los asignados a esas ideas «renuncian por motivos personales», y el resto del mundo apenas se da cuenta. Nadie está escribiendo nada parecido a una hipótesis de seguridad para que se note si se incumple; nadie está escribiendo un plan detallado sobre lo que van a hacer, qué capacidades se necesitan y cuáles son sus expectativas respecto a las dificultades para lograr cada capacidad.

Los profesionales de una civilización sensata echarían un vistazo a lo que está haciendo la Tierra y empezarían a gritar.

### ¿Cuánto tiempo se tardaría en resolver el problema de la alineación de la ASI? {#how-long-would-it-take-to-solve-the-asi-alignment-problem?}

#### **La dificultad no es solo la falta de tiempo, sino también la letalidad de los errores.** {#la-dificultad-no-es-solo-la-falta-de-tiempo;-es-la-letalidad-de-los-errores.}

En el año 500 d. C., la comunidad mundial había convergido en la teoría de que el Sol giraba alrededor de la Tierra. La teoría rival de Copérnico fue considerada y rechazada en gran medida. No fue hasta que Galileo construyó un telescopio y vio las lunas de Júpiter —cuerpos celestes que giran alrededor de Júpiter en lugar de la Tierra— que la incipiente comunidad científica llegó a la conclusión de que la Tierra gira alrededor del Sol.

La humanidad llegó a tiempo a la teoría correcta de la mecánica orbital. Pero antes de eso, llegó a un consenso erróneo. Y se aferró vorazmente a ese consenso erróneo hasta que la realidad comenzó a golpear a Galileo en la cabeza con el hecho de que la Tierra no es el centro de todo.

El proceso habitual por el cual la comunidad científica converge en la verdad implica pasos en los que la comunidad científica se equivoca y la realidad nos golpea en la cabeza con evidencia hasta que actualizan sus modelos.

El problema con la alineación de la ASI no es solo que se trata de un programa de investigación complicado. También es que, en este campo, lo que parece que la realidad realmente nos golpea en la cabeza con el hecho de que vuestra primera teoría favorita era errónea, es que una ASI hostil consuma el planeta. No habría supervivientes para converger en una mejor teoría de la alineación de la ASI.

Si la humanidad tuviera cien años *y reintentos ilimitados*, probablemente no tendríamos muchos problemas para resolver el problema de la alineación de la IAA.

Pero incluso si tuviéramos trescientos años para desarrollar una teoría sobre la inteligencia, sobre cómo cambian las IA a medida que se vuelven más inteligentes y sobre cómo orientarlas de una manera que sea estable en última instancia... bueno, en lugar de la capacidad de *probar y ver* lo que sucede cuando la IA se vuelve radicalmente más inteligente varias veces, es muy probable que convergiéramos en la respuesta incorrecta, antes de que llegara esa evidencia vital. La humanidad tiene una tendencia a converger en ese tipo de respuesta incorrecta.

### ¿Qué pasaría si la IA se desarrollara lentamente y se integrara poco a poco en la sociedad? {#¿qué-pasaría-si-la-ia-se-desarrollara-lentamente-y-se-integrara-poco-a-poco-en-la-sociedad?}

#### **Esto probablemente sería catastrófico.** {#esto-probablemente-sería-catastrófico.}

Nuestras predicciones se refieren a resultados finales, no a procesos. No sabemos qué sucederá con la IA entre ahora y el momento en que se convierta en algo realmente peligroso.

Por lo que sabemos, podría suceder en seis meses, si resulta que las IA tontas que piensan durante mucho tiempo son bastante buenas para hacer su propia investigación sobre IA (de una manera que inicia un bucle de realimentación crítica). Y por lo que sabemos, el campo podría estancarse durante seis años mientras se espera alguna idea crítica que luego tarde otros seis años en madurar. En este último caso, podría haber un período completo de doce años en el que la IA afecte drásticamente a la educación y al trabajo de formas sorprendentes.

(Sí, para aquellos que están frunciendo el ceño ante la frase anterior, somos conscientes de que la [falacia del volumen de trabajo fijo](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq) es una falacia. La cuestión es que nuestras conjeturas sobre cómo afectará realmente la IA al trabajo a corto plazo no son muy relevantes, teniendo en cuenta lo que sucederá después).

En los recursos en línea del capítulo 6, [discutimos](#can-we-enhance-humans-so-they-keep-pace-with-ai?) cómo es poco probable que la humanidad pueda seguir el ritmo del desarrollo de la IA, incluso si nos esforzamos por aumentar la inteligencia humana. (No obstante, en el capítulo 13 abogamos por aumentar la inteligencia humana, pero no creemos que sea factible que los humanos puedan seguir el ritmo de la IA si no se detiene también la investigación en este campo).

Por lo tanto, aunque ese futuro podría ser interesante y extraño, sería un futuro en el que cada vez más poder recaería en la IA. Una vez que cualquier conjunto de IA llegue a una posición en la que *pueda* apropiarse de los recursos del planeta, ese será el punto de no retorno: o ese conjunto de IA contiene un componente que se preocupa por la felicidad, la salud y la libertad de las personas, o el futuro será malo para nosotros.

### ¿Qué pasaría si hubiera muchas IA diferentes? {#what-if-there-are-lots-of-different-ais?}

#### **No sirve de mucho si no conseguimos que ninguna de ellas se preocupe por las cosas buenas.** {#no-sirve-de-mucho-si-no-conseguimos-que-ninguna-de-ellas-se-preocupe-por-las-cosas-buenas.}

Hay muchas formas en que las IA pueden acabar preocupándose por fines extraños y raros que no son lo que nadie quería o pretendía, como se explica en el capítulo 4. No importa si la humanidad crea mil millones de IA, si esas mil millones de IA se preocupan por fines extraños ligeramente diferentes. No irá bien para los humanos a menos que descubramos cómo crear al menos una IA que se preocupe al menos un poco por las personas felices, saludables y libres que vivan vidas prósperas, no solo en el sentido de que nos aseguren que se preocupan por eso cuando son jóvenes, sino en el sentido de que esa es *realmente* la respuesta más eficiente a cualquier pregunta a la que sus acciones (o las acciones de sus descendientes) sean una respuesta, como se explica en el capítulo 5\.

Si supiéramos cómo hacer que una de cada diez IA fuera buena, tal vez podríamos conseguir una décima parte del universo creando una gran cantidad de IA diferentes y esperando que las buenas llegaran a un acuerdo por nosotros. Pero, como argumentamos en los capítulos 2 a 4, conseguir una IA que se preocupe por las personas de la manera adecuada es extremadamente improbable, en el régimen moderno en el que simplemente desarrollamos las IA. No es una probabilidad de una entre diez, sino una probabilidad de una entre «eso-simplemente-no-sucede-a-menos-que-sepas-lo-que-estás-haciendo-lo-suficientemente-bien-como-para-que-suceda-a-propósito». La humanidad no está ni remotamente cerca de ese nivel. Ni siquiera generando unos pocos miles de millones de IA conseguiríamos una milmillonésima parte de los recursos del universo, si no somos capaces de hacer que ninguna de ellas se preocupe lo más mínimo por nosotros.

## Debate ampliado {#extended-discussion-8}

### Un análisis más detallado del antes y el después {#a-closer-look-at-before-and-after}

Como se menciona en el capítulo, la dificultad fundamental a la que se enfrentan los investigadores en IA es la siguiente:

Es necesario alinear una IA **antes** de que sea lo suficientemente potente y capaz como para matarte (o, por separado, para resistirse a ser alineada). Esa alineación debe entonces *trasladarse a diferentes condiciones*, las condiciones **después** de que una superinteligencia o un conjunto de superinteligencias[^187] pudieran matarte si así lo prefirieran.

En otras palabras: si estás construyendo una superinteligencia, debes alinearla sin poder probar exhaustivamente tus técnicas de alineación en las condiciones reales que importan, independientemente de lo «empírico» que parezca tu trabajo cuando trabajas con sistemas que no son lo suficientemente poderosos como para matarte.

Esta no es una norma a la que estén acostumbrados los investigadores de IA, ni los ingenieros de casi ningún campo.

A menudo escuchamos quejas de que estamos pidiendo algo poco científico, alejado de la observación empírica. En respuesta, podríamos sugerir hablar con los diseñadores de las sondas espaciales de las que hablamos en el capítulo 10.

La naturaleza es injusta y, a veces, nos presenta casos en los que el entorno que importa no es aquel en el que podemos realizar pruebas. Aun así, en ocasiones, los ingenieros están a la altura de las circunstancias y aciertan a la primera, *cuando cuentan con una sólida comprensión de lo que están haciendo* (herramientas robustas, teorías predictivas sólidas), algo que claramente falta en el campo de la IA.

El problema es que *la IA que puedes probar con seguridad, sin que ninguna prueba fallida te mate*, funciona bajo un régimen diferente al de la IA (o el ecosistema de IA) que necesita haber sido probada, porque si está desalineada, entonces todos mueren. La primera IA, o sistema de IA, no se percibe correctamente a sí misma como una opción realista de matar a todo el mundo si así lo desea. La segunda IA, o sistema de IA, sí ve esa opción.[^188]

Supongamos que estás pensando en nombrar a tu compañero de trabajo Bob dictador de tu país. Podrías intentar convertirlo primero en dictador simulado de tu ciudad, para ver si abusa de su poder. Pero, por desgracia, esta no es una prueba muy buena. «Ordenar al ejército que intimide al parlamento y "supervise" las próximas elecciones» es una opción muy diferente a «abusar de mi poder ficticio mientras me observan los habitantes de la ciudad (que aún pueden golpearme y negarme el puesto)».

Con una teoría de la cognición suficientemente desarrollada, podrías intentar leer la mente de la IA y predecir en qué estado cognitivo entraría si realmente pensara que tiene la oportunidad de tomar el control.

Y podrías [configurar simulaciones](#¿qué-pasaría-si-le-hiciéramos-creer-que-está-en-una-simulación?) (e intentar engañar las sensaciones internas de la IA, etc.) de una manera que, según tu teoría de la cognición, sería muy similar al estado cognitivo en el que entraría la IA una vez que realmente tuviera la opción de traicionarte.

Pero la relación entre estos estados que induces y observas en el laboratorio y el estado en el que la IA realmente tiene la opción de traicionarte *depende fundamentalmente de tu teoría cognitiva sin probar*. La mente de una IA es susceptible de cambiar bastante a medida que se desarrolla hasta convertirse en una superinteligencia.

Si la IA crea nuevas IA sucesoras que son más inteligentes que ella, es probable que el funcionamiento interno de esas IA difiera del funcionamiento interno de la IA que estudiaste anteriormente. Cuando aprendes solo de una mente anterior, cualquier aplicación de ese conocimiento a las mentes posteriores pasa por una teoría no probada sobre cómo cambian las mentes entre el antes y el después.

Hacer funcionar la IA hasta que tenga la oportunidad de traicionarte *de verdad*, de una manera que sea difícil de falsificar, es una prueba empírica de esas teorías en un entorno que difiere fundamentalmente de cualquier entorno de laboratorio.

Muchos científicos (y muchos programadores) saben que sus teorías sobre cómo va a funcionar un sistema complicado en un entorno operativo fundamentalmente nuevo *a menudo no funcionan bien en el primer intento*.[^189] Se trata de un problema de investigación que requiere un nivel «injusto» de previsibilidad, control y conocimiento teórico, en un ámbito con unos niveles de comprensión inusualmente bajos, en el que nuestras vidas están en juego si el resultado del experimento desmiente las esperanzas de los ingenieros.

Por eso, desde nuestra perspectiva, parece *sobredeterminado* que los investigadores no debáis precipitaros a ampliar los límites de la IA tanto como sea posible. Es una locura legítima intentarlo, y una locura legítima que cualquier gobierno permita que suceda.

### La historia del Chicago Pile-1 {#la-historia-del-chicago-pile-1}

En 1942, se construyó Chicago Pile-1 bajo la dirección de Enrico Fermi. Estaba compuesto por 45 000 bloques de grafito con un peso de 330 toneladas, 4,9 toneladas de uranio metálico y 41 toneladas de dióxido de uranio, colocados debajo de las gradas de la pista de raquetbol Stagg Field de la Universidad de Chicago. Dependiendo de cómo definas los términos, se podría considerar el primer reactor nuclear; no estaba destinado a producir energía para uso industrial, pero fue el primer motor para una reacción crítica sostenida.

Según los estándares modernos, se tomaron algunas medidas de seguridad. Por ejemplo, la parte en la que se construyó debajo de las gradas de una pista de raquetbol en una universidad dentro de una gran ciudad.

El general Groves, director del Proyecto Manhattan, había intentado que el experimento se llevara a cabo *cerca* de Chicago, en lugar de *directamente en* la ciudad, y había ordenado la construcción de un edificio para tal fin, pero las obras se habían retrasado. Arthur Compton, profesor de Física de la Universidad de Chicago y ganador del Premio Nobel, que acogió el CP-1, había evitado pedir permiso al rector de la universidad porque, según explicó Compton más tarde, el rector habría tenido que decir que no, y esa habría sido la respuesta equivocada.

La tarea de apilar los ladrillos la realizaron jóvenes que habían abandonado los estudios secundarios y buscaban ganar algo de dinero extra mientras esperaban el reclutamiento militar.

El uranio estaba encerrado en un cubo de goma de siete metros, en lugar de en una vasija metálica de reactor. Por supuesto, no había ningún edificio gigante de contención de hormigón.

Al enterarte más tarde de estos hechos, se dice que James Conant, presidente del Comité de Investigación para la Defensa Nacional, palideció. Incluso para la década de 1940, esto no se consideraba un comportamiento científico totalmente normal.

Si leyeras todo esto en un libro de historia sin saber adónde va a parar, podrías tener la expectativa de que estás leyendo el preludio de algún gran fallo de seguridad. Faltan tantas cosas que la cultura de 2025 considera medidas de seguridad estándar. ¿Dónde están los inspectores y las carpetas? ¿Los enormes y pesados reglamentos de funcionamiento? ¿Los comités que debaten con seriedad? ¿Las declaraciones de impacto? ¿Las normas que establecen que solo las personas con credenciales pueden apilar los ladrillos de uranio? ¿Dónde está el papeleo?

Pero la pila de ladrillos de uranio y grafito no se fundió.

Y la razón es que Fermi sabía lo que estaba haciendo; había predicho las reglas de antemano.

Fermi no se limitaba a apilar misteriosos ladrillos que generaban más calor cuando se acercaban entre sí. Sabía que algunos átomos de uranio se desintegrarían y se fisionarían espontáneamente. Sabía que, cuando esto ocurriera, la fisión generaría neutrones. Sabía que esos neutrones a veces chocaban con otros átomos de uranio y que esto a veces provocaba otra fisión.

Fermi lo entendiste *de antemano*; no tuviste que descubrir por las malas que se trataba de un proceso exponencial. No en el sentido en que los medios de comunicación actuales utilizan en exceso la palabra «exponencial» para referirse simplemente a «grande» o «rápido», sino a un proceso cuya tasa de aumento es proporcional a su nivel actual: la exponenciación *matemática*.

Fermi sabía que al apilar más ladrillos de uranio y grafito, estaba aumentando el factor multiplicado dentro de un proceso exponencial. Como se explica en el libro, hay una gran diferencia entre un factor de multiplicación de neutrones inferior al 100 % y uno superior al 100 %.[^190] Por debajo del 100 %, solo tienes una pila de ladrillos calientes. Pero por encima del 100 %, el nivel de radiactividad de la pila aumenta. Y aumenta. Y aumenta.

No se comporta como todas las pilas de ladrillos de uranio más pequeñas que hayas probado anteriormente. Si no entendías lo que estabas haciendo lo suficientemente bien como para moderar el reactor (de modo que la reacción en cadena se ralentizara si el reactor comenzaba a sobrecalentarse), entonces el reactor no se habría estabilizado como lo hicieron las pilas más pequeñas. Si lo dejabas funcionar durante toda la noche, al día siguiente no obtendrías un nuevo nivel de potencia útil desde el punto de vista industrial.

La pila se volvería cada vez más radiactiva hasta que el grafito se incendiara o el uranio se fundiera y se convirtiera en escoria.

Entonces llegarían los bomberos y se encontrarían con un incendio confuso que no dejaba de desprender calor cuando echaban agua sobre él.

1942 no habría sido un buen año para asistir a la Universidad de Chicago.

Pero Fermi ya sabía todo eso, así que no había problema. Cuando Fermi ordenó que se retirara una barra de control (una tabla de madera con una lámina de cadmio clavada) doce pulgadas más el 2 de diciembre de 1942, advirtió de antemano que esta retirada haría que los niveles de radiactividad medidos «subieran y siguieran subiendo... sin estabilizarse».

Entonces, la radiactividad se duplicó en los dos minutos siguientes y volvió a duplicarse, hasta que dejaron que la reacción se desarrollara y se duplicara cada dos minutos durante un total de veintiocho minutos, aumentando en un factor de alrededor de 16 000.

Un aumento de 16 000 veces en la radiactividad era el comportamiento esperado de la pila, predicho correctamente y comprendido en detalle de antemano. No fue una sorpresa inesperada, con la que se topó alguien a quien se le ordenó apilar diez veces más ladrillos de uranio que la última vez para ver si ocurría algo interesante y rentable.

Como se explica en el libro, hay un margen muy estrecho entre un reactor nuclear y una explosión nuclear. Un margen de poco más del medio por ciento, para ser exactos. Esa es la diferencia entre un reactor que produce una cantidad de energía útil para la industria y un reactor que explota.

Es decir: hay que hacer que la reacción nuclear sea cada vez más potente, antes de que empiece a funcionar *realmente*. Y entonces, un instante después de alcanzar esa potencia, si se supera *por un pelo*, si se supera en un 0,65 %, explota.

Ese es el tipo de problema que la realidad te puede plantear. Sucede.

Pero Fermi y Szilard y su equipo habían predicho todas estas reglas antes de descubrirlo por las malas. Sabían sobre los neutrones retardados y los neutrones rápidos. (Véase el capítulo 10 para más información sobre esta parte de la historia). Así que, una vez que Fermi consiguió que el factor de multiplicación de neutrones alcanzara el 100,06 %, *no* ordenó que se retirara más la barra de control para ver qué pasaba con una pila aún más potente. Solo llegó hasta la criticidad, sin avanzar un 0,65 % más hasta la criticidad inmediata. Fermi obtuvo el resultado que había predicho y *sabía* lo que pasaría si iba más allá. Por eso no siguió adelante.

Veintiocho minutos más tarde, con la radiactividad duplicándose cada dos minutos hasta multiplicarse por 16 000, Fermi apagó el primer reactor nuclear del mundo: los ladrillos de uranio apilados bajo las gradas de un estadio universitario en una gran ciudad.

Para ser claros, no afirmaríamos que Fermi fue completamente responsable solo porque tenía un modelo aparentemente coherente de la física de los reactores de baja energía. Fermi podría haberse equivocado. La humanidad se ha encontrado con algunas sorpresas a lo largo de la historia de la ingeniería nuclear.

La prueba Castle Bravo de la primera arma termonuclear[^191] tuvo un rendimiento tres veces superior al previsto porque contenía una mezcla de litio-6 y litio-7 como combustible nuclear para una reacción de fusión. Los creadores del arma conocían algunos productos nucleares potentes derivados de la fusión del litio-6, pero ninguno de la fusión del litio-7, y resultó que el litio-7 *no* era realmente inerte.

Fermi, al llevar a cabo su reacción a baja intensidad y no a un nivel que generara niveles de energía útiles desde el punto de vista industrial, evitó muchas complicaciones que aparecen en los reactores nucleares lo suficientemente potentes como para ser rentables. Si hubiera habido algún factor que aumentara el factor neutrónico dependiente de la velocidad de reacción que Fermi no hubiera previsto, cualquier fenómeno previamente desconocido, del tipo que apareció en la prueba Castle Bravo — cualquier sorpresa que se manifestara una vez que el flujo de neutrones aumentara en un factor de 16 000 y elevara el factor de multiplicación de 1,0006 a 1,02 más rápido que el tiempo de reacción de un ser humano para verter cadmio de emergencia — entonces hoy en día Estados Unidos tendría una zona de exclusión en Chicago.

Aun así, no estamos diciendo que Fermi se equivocara necesariamente al realizar ese experimento. No era el tipo de experimento que pudiera haber destruido *la especie humana*. Podría decirse que valía la pena arriesgar una zona de exclusión en Chicago como resultado no predeterminado de encontrar un nuevo fenómeno oculto que alterara una comprensión que se esperaba precisa. En realidad, la Alemania nazi no acabaría estando cerca de obtener armas nucleares en 1945, pero en 1942 nadie sabía que eso sería así. Las predicciones de ese tipo son difíciles de hacer. Apilar los lingotes de uranio fuera de una gran ciudad habría sido un inconveniente, y los inconvenientes tienen un costo real en la guerra.

Nuestro objetivo al relatar este acontecimiento no es emitir un juicio moral en un sentido u otro. Para empezar, tendríamos que dedicar más tiempo a examinar los detalles históricos de lo que ocurrió para comprender cuáles eran las opciones exactas de esas personas concretas y si dejaron pasar una opción mejor.

La lección que extraemos tiene más que ver con la diferencia entre la «seguridad» estereotipada y lo que realmente se necesita para que la realidad no te mate.

Chicago Pile-1 carecía por completo de las *medidas de seguridad estereotipadas, visibles y ostentosas* que los burócratas saben exigir. El desastre se evitó gracias a la *comprensión*, no al teatro de la seguridad. La comprensión de Fermi resultó ser suficiente; es posible que no lo hubiera sido, pero en realidad lo fue. Y ese nivel de comprensión era lo que exigía la realidad, no cualquier tipo de simulacro.

Si nadie hubiera comprendido en profundidad lo que ocurría dentro de una pila de extraños ladrillos metálicos... entonces no habría servido de mucho que un montón de inspectores con trajes sobrios se asomaran a los ladrillos de metal indescifrable, o imprimieran un manual de seguridad con aspecto oficial y bien encuadernado en el que se dijera que solo los operadores certificados pueden apilar los extraños ladrillos metálicos.

Podemos imaginar un mundo en el que Chicago Pile-1 se construyera *sin* Enrico Fermi. Sin nadie, de hecho, que entendiera las verdaderas leyes que rigen los misteriosos ladrillos que se calientan por sí mismos.

En un mundo así, tal vez otro científico habría podido ver el peligro mortal antes de que fuera demasiado tarde. Podemos imaginar una conversación como la siguiente:

> **Salviati**: La forma en que los ladrillos aumentan su potencia cuando se unen es una señal evidente de un proceso de autorrefuerzo, el tipo de proceso que puede hacerse más fuerte. Si buscamos modelos matemáticos que puedan describir un proceso como ese, tienden a tener un modo en el que, si los empujamos lo suficiente, explotan.
> 
> **Simplicio:** ¡Qué tontería! En la vida real, es científico creer que todo tipo de proceso como ese acaba encontrando un límite. ¡No pueden continuar indefinidamente hasta el infinito! Por lo tanto, apilar ladrillos de uranio y grafito debería ser perfectamente seguro, porque alcanzará un límite y será inofensivo.
>
> **Salviati:** Eso es como argumentar que una supernova no puede ser peligrosa porque no puede alcanzar una temperatura *infinita*, o argumentar que una superinteligencia artificial sería inofensiva porque no sería infinitamente inteligente. O como argumentar que una bala debe tener *algún* límite de velocidad y, por lo tanto, no perforará la piel. El hecho de que haya un límite en algún lugar no significa que ese límite sea «bajo». Todos los modelos matemáticos que tenemos sobre «por qué» los ladrillos se calientan por sí mismos sugieren que hay un umbral crítico en algún lugar, de modo que superar ese umbral hará que la pila explote y mate a todos los que estén cerca.
>
> **Simplicio:** ¡Pero los científicos ni siquiera se ponen de acuerdo sobre dónde está ese umbral! Si hubiera un consenso científico sobre que añadir unos cuantos ladrillos más es peligroso, yo lo dejaría. Pero cuando los científicos ni siquiera se ponen de acuerdo sobre dónde está exactamente el peligro, ¿por qué preocuparse?
>
> **Salviati:** Cuando [muchos](https://youtu.be/KcbTbTxPMLc?feature=shared&amp;t=1580) [de los principales](https://www.youtube.com/watch?v=PTF5Up1hMhw&amp;t=2283s) [científicos](https://aistatement.com/) advierten de que existe una seria posibilidad de que se produzca una explosión letal, el hecho de que no puedan calcular exactamente cuándo comenzará la explosión debería preocuparte *más*, no menos. Quizás si supiéramos con precisión cómo funcionan los ladrillos, veríamos que hay una banda estrecha en la que podemos extraer energía de forma segura, por debajo de la cual los ladrillos son inútiles y por encima de la cual son letales. Pero el hecho de que los científicos sigan discutiendo significa que aún no sabemos lo que estamos haciendo. Lo que significa que no es el momento de jugar con la reacción en cadena que hoy calienta esos ladrillos, no sea que mañana exploten y nos maten. Primero hay que entender la ciencia.

Estamos muy, muy lejos de poder crear un modelo de la IA tan bien como Fermi entendió las reacciones nucleares en cadena.

En algún momento de desconocimiento desconocido, si seguís por este camino, os encontraréis a una velocidad vertiginosa con un resultado mucho más grave que la irradiación de Chicago.

# Capítulo 11: Alquimia, no ciencia {#capítulo-11:-alquimia,-no-ciencia}

Este es el recurso en línea para el capítulo 11 de *If Anyone Builds It, Everyone Dies* (Si alguien lo construye, todos morirán), que analiza cómo los laboratorios modernos de IA están abordando el problema de la alineación de la superinteligencia artificial. Consulta el libro para obtener respuestas a preguntas como:

¿Cómo debemos evaluar la preparación actual de las empresas de IA para resolver el problema de la alineación de la ASI?  
¿Dónde encaja la «investigación sobre interpretabilidad», es decir, la investigación para leer y comprender las mentes de la IA?  
* ¿No podemos simplemente ordenar a la IA que resuelva el problema por nosotros?

A continuación, abordamos una serie de ideas sobre la alineación y el despliegue de la IA y proponemos razones para el optimismo, así como razones por las que podría ser bueno avanzar en la frontera de la IA, incluso si la situación parece sombría.

## Preguntas frecuentes {#faq-8}

### ¿No acabaremos como siempre, sin resolver nada? {#¿no-acabaremos-como-siempre-sin-resolver-nada?}

#### **El mundo suele salir adelante a base de prueba y error. En este caso, los errores iniciales no dejarían supervivientes.** {#el-mundo-suelesaliradelanteabasedeprueba-y-error.-en-este-caso,-los-errores-iniciales-no-dejarían-supervivientes.}

Véase el capítulo 10 y el [debate ampliado asociado](#una-mirada-más-detallada-al-antes-y-el-después) sobre la diferencia entre el antes y el después.

### ¿Consideras que la alineación es algo que se tiene o no se tiene? {#¿consideras-que-la-alineación-es-algo-que-se-tiene-o-no-se-tiene?}

#### **No. Pero la «alineación parcial» sigue siendo potencialmente catastrófica.** {#no.-pero-la-«alineación-parcial»-sigue-siendo-potencialmente-catastrófica.}

Uno de los argumentos para preocuparse menos por la superinteligencia es el siguiente: «Probablemente, la IA [avanzará de forma gradual](#¿qué-pasaría-si-la-IA-se-desarrollara-lentamente-y-se-integrara-poco-a-poco-en-la-sociedad?), lo que permitiría realizar mejoras mediante ensayo y error para mantener la IA bajo control en cada paso; la alineación no tiene por qué ser *perfecta* para que todo vaya bien». No creemos que esta opinión sea muy esperanzadora, por varias razones:

Nuestras preocupaciones no dependen de si el progreso es rápido o lento. No tenemos una opinión segura sobre si la IA se estancará en varios puntos del camino hacia la superinteligencia. Parece una decisión difícil, más que fácil. Nuestra mejor suposición es que la inteligencia artificial está sujeta a [efectos umbral](#is-“intelligence”-a-simple-scalar-quantity?), pero en última instancia se trata solo de una suposición, y nuestros argumentos [no dependen de ello](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?). La historia de Sable en la parte II de *If Anyone Builds It, Everyone Dies* describen intencionadamente una catástrofe provocada por IA que no están muy por encima de las capacidades humanas, en parte para transmitir cómo un adversario de IA no necesitaría convertirse rápidamente en superinteligencia para ser extraordinariamente peligroso.

Nuestra respuesta básica a «¿Qué pasaría si tuviéramos suerte y acabáramos teniendo mucho tiempo para probar ideas de alineación de la IA en IA débiles antes de que las IA se volvieran muy capaces?» es el debate del capítulo 10 y el debate ampliado asociado «[Una mirada más cercana al antes y el después](#una-mirada-mas-cercana-al-antes-y-el-despues)». Los investigadores pueden averiguar todo tipo de detalles sobre las IA débiles, pero es inevitable que haya un gran número de diferencias críticas entre las IA lo suficientemente débiles como para estudiarlas con seguridad y las primeras IA lo suficientemente potentes como para constituir un punto de no retorno. Incluso en un campo maduro, abordar todas estas diferencias de forma adecuada y con suficiente antelación sería muy difícil. En un campo que aún se encuentra en fase de alquimia, trabajando con IA inescrutables (que se desarrollan en lugar de crearse), esa esperanza es totalmente irrealista.

* La alineación de la IA no tiene por qué ser perfecta para producir excelentes resultados a largo plazo. En principio, es posible crear cuidadosamente una IA con cierta tolerancia al error, si sabes lo que estás haciendo.[^192] Pero eso no significa que las IA «parcialmente alineadas» o incluso «mayoritariamente alineadas» produzcan resultados parcialmente o mayoritariamente aceptables. Hay muchas formas y razones diferentes por las que una IA podría actuar bien el noventa y cinco por ciento del tiempo en el presente o en un futuro cercano, lo que no se traduciría en ningún tipo de final feliz para la humanidad, como se analiza desde muchos ángulos diferentes en los [recursos en línea del capítulo 5](#chapter-5:-its-favorite-things).

Para profundizar en el último punto:

Como experimento mental, imagina que la humanidad logra cargar *casi* todos los valores humanos diversos en las preferencias de una superinteligencia, todos excepto [la preferencia por la novedad](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile), por alguna razón. En ese caso, la superinteligencia se dirigiría hacia un futuro estancado y aburrido, en el que el mismo «mejor» día se repite ad infinitum.

No creemos que esto sea *plausible*, claro está. Ese nivel de alineación parece totalmente fuera del alcance de los enfoques estándar actuales en IA, y resulta un poco extraño imaginar que podríamos descubrir cómo introducir casi todos nuestros valores en una IA sin descubrir cómo introducirlos todos.[^193] Pero este experimento mental pone de relieve cómo las criaturas que comparten *algunos* de nuestros deseos, pero a las que les falta al menos un deseo crucial, probablemente seguirían produciendo resultados catastróficos una vez que fueran lo suficientemente expertas tecnológicamente como para excluir a los humanos del proceso de toma de decisiones y conseguir exactamente lo que quieren.

De manera más realista, una IA podría terminar «parcialmente» alineada en el sentido de que (al igual que nosotros) tiene varias estrategias instrumentales [enredadas en sus preferencias terminales](#reflection-and-self-modification-make-it-all-harder). Quizás termine con un impulso que se parezca un poco a la curiosidad y otro que se parezca un poco al [conservacionismo](#¿no querrá la IA mantenernos felices y saludables por el bien de la preservación ecológica o algún impulso similar?), y quizás algunas personas vean eso y digan: «¡Mirad! La IA está desarrollando impulsos muy humanos». Sin duda, desde cierto punto de vista, se podría decir que una IA así está «parcialmente» alineada.

Pero cuando se trata de lo que esa IA haría al madurar hasta convertirse en superinteligencia, probablemente no sea nada bonito. Quizás gaste muchos recursos persiguiendo su extraña versión de la curiosidad [inconscientemente](#perder-el-futuro), mientras preserva una versión de la humanidad que ha editado para que le resulte más agradable. (Del mismo modo que incluso muchos humanos más preocupados por la conservación podrían editar [los mosquitos que matan a los niños y los parásitos agonizantes](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) de la naturaleza, si se les diera la oportunidad). Esto forma parte de nuestro argumento de que los seres humanos prósperos no son la solución más eficiente para la inmensa mayoría de los problemas.

Por otra parte, una IA puede tener valores que se suman a un comportamiento muy humano *en el entorno de entrenamiento*, de tal manera que la gente exclama que definitivamente parece «parcialmente alineada». (Eso ya está sucediendo ahora, y hemos argumentado que es [ilusorio](#doesn’t-the-claude-chatbot-show-signs-of-being-aligned?).) Pero esto dice muy poco sobre cómo se comportará la IA una vez que tenga un espacio de opciones enormemente más amplio. Para que las personas prosperen en *ese* entorno, la prosperidad de la humanidad en particular tiene que formar parte del *resultado alcanzable más deseado* por la IA.

Si incorporamos parcialmente algunos valores positivos en la IA, eso no significa que los valores de la humanidad vayan a estar parcialmente representados en el futuro. Incorporar parcialmente valores similares a los humanos en las preferencias de una IA más inteligente que los humanos no es lo mismo que incorporar plenamente los valores humanos en la IA con una «ponderación» baja (que acaba saliendo a la luz una vez que se saturan otros valores).

Para que la IA nos dé *cualquier cosa*, tiene que preocuparse por nosotros de la manera correcta, al menos un poco. Y hay muchos, muchos «episodios que casi fueron una catástrofe» que no cumplen esa vara. Véase también: «¿No se preocuparán las IA al menos un poco por los humanos?».

### ¿No mejorará la situación una vez que los gobiernos se involucren más? {#won't-the-situation-get-better-once-governments-get-more-involved?}

#### **Depende de cómo (y cuándo) se involucren.** {#depende-de-cómo-(y-cuándo)-se-involucren.}

Cuando visitamos Washington D. C., a menudo nos reunimos con responsables políticos que piensan que las empresas de IA tienen sus IA bajo control. Al mismo tiempo, vemos habitualmente a personas del sector de la IA que afirman que la regulación solucionará el problema. Un ejemplo especialmente flagrante que observamos fue el del director general de Google, quien [afirmó](https://youtu.be/9V6tWC4CdFQ?feature=shared&amp;t=2685) que «el riesgo subyacente [de que la humanidad sea aniquilada] es en realidad bastante alto», pero argumentó que cuanto mayor sea el riesgo, más probable será que la humanidad se una para evitar la catástrofe.

Dejando de lado lo descabellado que resulta que el director general de una empresa se apresure a desarrollar una tecnología que, en su opinión, pone en peligro a todos los habitantes del planeta, con la esperanza de que la humanidad «se una» para hacer frente a los riesgos que él mismo está contribuyendo a crear, observemos que se trata de un caso en el que una persona del ámbito técnico imagina que *alguien más* resolverá el problema.

Mientras tanto, la mayoría de los políticos parecen pensar que la comunidad técnica resolverá el problema. Esto queda implícito, por ejemplo, cada vez que [ellos](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [dicen](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [tenemos](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [que](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [que](https://statemag.state.gov/2025/04/0425itn07/) [ganar](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [la](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [carrera](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — no es posible que este tipo de carrera tenga un ganador si no se resuelven los retos técnicos. Aunque quizá no sea tan grave como parece; quizá los responsables políticos no estén pensando realmente en una carrera hacia la superinteligencia, sino simplemente en una carrera hacia la mejora de los chatbots. En junio de 2025, un asesor de políticas de IA que conocemos describe al Congreso como, en general, [que no parece creer a las empresas de IA cuando dicen explícitamente que están trabajando en la superinteligencia](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (aunque con algunas excepciones de importancia)

Casi todos los que están en el poder parecen imaginar que alguien más va a resolver el problema.

Para más información sobre cómo está reaccionando el mundo en general (y cómo los responsables de la toma de decisiones a menudo no reaccionan adecuadamente ante las catástrofes), véase el capítulo 12. En agosto de 2025, los gobiernos aún no han dado una respuesta seria a este problema. Y siempre existe el riesgo de que los funcionarios gubernamentales no comprendan del todo el reto y, por ejemplo, traten la IA como una tecnología normal que no debe verse estrangulada por una regulación excesiva.

Para obtener más información sobre qué intervenciones gubernamentales tienen posibilidades reales de evitar una catástrofe relacionada con la IA, véase el capítulo 13, así como el [debate sobre por qué la colaboración internacional probablemente no sería suficiente](#¿por-qué-no-utilizar-la-cooperación-internacional-para-crear-una-ia-segura,-en-lugar-de-cerrarla-por-completo?).

### ¿No serán las empresas más imprudentes, por naturaleza, las más incompetentes y, por lo tanto, no supondrán una amenaza? {#¿no-serán-las-empresas-más-imprudentes-por-naturaleza-las-más-incompetentes-y-por-lo-tanto-no-supondrán-una-amenaza?}

#### **En general, no. Tomar atajos suele ser competitivo.** {#en-general-no.-tomar-atajos-suele-ser-competitivo.}

Los esfuerzos de Volkswagen por [hacer trampa en las pruebas de emisiones](https://www.bbc.com/news/business-34324772) entre 2008 y 2015 fueron audaces y, al parecer, eficaces. Los accidentes del Boeing 737 MAX en 2018 y 2019, atribuidos a fallos en un sistema de control de vuelo que la dirección conocía y minimizó, [causaron la muerte de 346 personas](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Pero la fabricación de automóviles y aviones son industrias altamente competitivas en las que Volkswagen y Boeing eran, y siguen siendo, gigantes.

No nos parece un gran misterio que los que toman atajos sean competitivos. En ambos casos, el comportamiento parece haber estado impulsado por la presión de sacar al mercado productos de alto rendimiento más baratos y antes que la competencia. Incluso ahora, tras los enormes acuerdos extrajudiciales y el daño a la marca, no es obvio que las empresas sean menos competitivas por tener culturas corporativas que fomentan el ingenioso uso de atajos, aunque esto ocasionalmente signifique que te pillen.

Si crees que las principales empresas de IA son una excepción a esta regla, considera el siguiente [titular](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (y subtítulo) de julio de 2025:

![][image18]

\[El titular dice: «Grok lanza un compañero de anime pornográfico y consigue un contrato con el Departamento de Defensa». El subtítulo dice: «Mientras tanto, la versión más avanzada del chatbot con IA de xAI, de Elon Musk, sigue identificándose como Adolf Hitler».\]

No creemos que sea técnicamente posible que ningún equipo que utilice métodos modernos construya una superinteligencia sin provocar una catástrofe. Pero incluso si esto fuera remotamente posible con la tecnología actual, parece casi inevitable que una empresa de IA metiera la pata y nos matara a todos, dado el nivel de competencia y seriedad que vemos hoy en día.

#### **\* Las empresas más cautelosas de hoy en día siguen siendo imprudentes.** {#*-las-empresas-más-cautelosas-de-hoy-en-día-siguen-siendo-imprudentes.}

Muchas personas consideran que la empresa de IA antrópica es líder en «seguridad de la IA», ya que ha sido pionera en iniciativas como los [compromisos voluntarios de seguridad](https://www.antrópica.com/news/antrópicas-responsible-escala-policy). Pero incluso ustedes [modifican sus compromisos voluntarios en el último momento cuando se dan cuenta de que no pueden cumplirlos](https://www.obsolete.pub/p/exclusive-antrópica-is-quietly-backpedalling), y los «planes» que tienen son vagos y están mal pensados, como se critica en el capítulo 11 y en el [debate ampliado](#más-sobre-cómo-hacer-que-la-ia-resuelva-el-problema) más abajo.

Anthropic se beneficia enormemente del hecho de que los observadores califican según una curva: en una industria normal, una empresa que decide poner en peligro la vida de miles de millones de personas (como [admite el director general](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883)) y que, al mismo tiempo, minimiza sistemáticamente sus actividades ante el público[^194] y los legisladores[^195], no recibiría elogios por su moderación.

Tomar atajos es habitual en la IA, al igual que en muchos sectores competitivos. La imprudencia es habitual. Y las empresas *menos* imprudentes no están, de forma muy visible, a la altura de los retos.

¿No es de importancia adelantarse debido al «exceso de hardware»? {#¿no-es-de-importancia-adelantarse-debido-al-«exceso-de-hardware»?}

#### **Eso sería un suicidio, porque estamos demasiado lejos de una solución de alineación.** {#eso-sería-un-suicidio,-porque-estamos-demasiado-lejos-de-una-solución-de-alineación.}

Durante la última década, algunas personas preocupadas por los peligros de la IA argumentaron que podría ser bueno avanzar en la IA lo más rápido posible. La idea era que las IA más inteligentes requerirían entonces casi todo el poder de cómputo del mundo para funcionar. Ningún avance individual desataría *de repente* miles de potentes IA que pensaran miles de veces más rápido que cualquier humano.

Mientras la humanidad siguiera utilizando una parte considerable de su poder de cómputo para ejecutar las IA más inteligentes, el cambio se produciría al menos de forma gradual y daría tiempo a la humanidad para adaptarse. No habría «exceso de hardware», es decir, ningún momento en el que las capacidades de la IA dieran un salto repentino porque el mundo hubiera estado esperando para desplegar una gran acumulación de hardware informático en la IA. O al menos así se argumentaba.

Creemos que este es un argumento bastante malo. Uno de los problemas es que la inteligencia parece estar sujeta a [efectos umbral](#is-“intelligence”-a-simple-scalar-quantity?).

La transición de la inteligencia a nivel de chimpancé a la inteligencia a nivel humano no fue «discontinua» en ningún sentido; desde el punto de vista de la humanidad, fue bastante gradual. Pero desde una perspectiva evolutiva, se produjo con bastante rapidez. Y la transición de la civilización preindustrial a la posindustrial fue aún más rápida. Ninguna de ellas fue lo suficientemente gradual como para que otros animales se adaptaran de manera significativa.

Por ejemplo, una IA que requiera una parte significativa del poder de cómputo mundial para funcionar podría ser lo suficientemente inteligente como para descubrir nuevos algoritmos de IA y nuevos diseños de chips informáticos que den lugar a mil IA más inteligentes que los humanos y que piensen miles de veces más rápido que la humanidad poco después. (Recuerda: un centro de datos moderno requiere tanta electricidad para funcionar como una [pequeña ciudad](https://epoch.ai/blog/power-demands-of IA de vanguardia), mientras que un humano requiere tanta electricidad para funcionar como una [gran bombilla](https://en.wikipedia.org/wiki/Human_power). Hay mucho margen para mejorar la eficiencia de la IA).

O, si el cuello de botella es el poder de cómputo para *construir* IA en lugar del poder de cómputo para *ejecutar* IA, podemos esperar que haya un exceso de grandes cantidades de hardware una vez finalizado el proceso de entrenamiento, lo que liberará el hardware para ejecutar un gran número de IA de pensamiento rápido.

Incluso si la inteligencia no estuviera sujeta a efectos umbral, somos escépticos ante la idea de que bombardear continuamente a la humanidad con IA cada vez más inteligentes (aunque ninguna de ellas sea lo suficientemente inteligente como para matarnos) lo más rápido posible sea una buena forma de ayudar a la humanidad a desarrollar la disciplina de ingeniería necesaria para crear AIs robustas y amigables.

El problema es que las IA se desarrollan en lugar de crearse, y nadie está ni siquiera cerca de descubrir cómo desarrollar IA que se preocupen de forma sólida por *cualquier cosa* que sus diseñadores quieran que se preocupen.

Ese problema no se resuelve desarrollando más IA lo antes posible. La idea es prácticamente un non sequitur. Véanse también algunos de los antiguos escritos de Soares sobre cómo [la alineación de la IA requiere un esfuerzo continuo](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Sin embargo, la incongruencia fue recogida por el director general de OpenAI, Sam Altman, quien [la utilizó como excusa en 2023 para que OpenAI avanzara lo más rápido posible](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Esta excusa se reveló luego como vacía cuando ese mismo Sam Altman [se apresuró a construir una cantidad mucho mayor de hardware de poder de cómputo](https://openai.com/index/announcing-the-stargate-project/).

Creemos que este es un buen ejemplo de cómo los ejecutivos de las empresas de IA se aferran a cualquier argumento que consideren válido para justificar su carrera hacia adelante. Creemos que la mayoría de estos argumentos pueden descartarse por sus propios méritos, y [recomendamos no](#workable-plans-will-involve-telling-ai-companies-«no.») dar más importancia a un argumento solo porque lo haya planteado un ejecutivo de una empresa de IA.

¿No es de importancia seguir adelante para poder investigar la alineación? {#isn’t-it-important-to-race-ahead-so-we-can-do-alignment-research?}

#### **\* Recomendamos encarecidamente no seguir este paradigma de IA en su totalidad.** {#*-we-strongly-recommend-against-this-entire-ai-paradigm.}

Los métodos actuales de IA plantean retos innecesariamente difíciles para la alineación, por las razones que hemos comentado en capítulos anteriores. No vemos ninguna razón de principio por la que la humanidad no pueda construir una superinteligencia alineada, con una comprensión suficientemente sólida de lo que estamos haciendo y un conjunto diferente de herramientas formales. Pero todo el enfoque actual de la IA parece un callejón sin salida desde el punto de vista de la alineación y la solidez, aunque sea perfectamente válido desde el punto de vista de las capacidades.

No estamos abogando por la «buena IA tradicional» que reinó desde la década de 1950 hasta la de 1990. Esas técnicas eran erróneas y fracasaron, por razones que son [bastante obvias](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design). Hay *otras opciones* además de los intentos extremadamente superficiales de la década de 1980 y las IA que se desarrollan sin apenas comprender su funcionamiento interno.

#### **Hay mucho trabajo significativo que se podría hacer ahora.** {#hay-mucho-trabajo-significativo-que-se-podría-hacer-ahora.}

Sydney Bing [engañó](https://x.com/MovingToTheSun/status/1625156575202537474) y [amenazó](https://x.com/sethlazar/status/1626257535178280960) a los usuarios. Todavía no sabemos exactamente por qué; todavía no sabemos exactamente qué pasaba por su cabeza. Lo mismo ocurre con los casos en los que las IA (en libertad) son [demasiado aduladoras](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [parecen intentar activamente volver locas a las personas](#ai-induced-psychosis), [supuestamente engañan e intentan ocultarlo](https://assets.antrópica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), o [se declaran persistente y repetidamente Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Lo mismo ocurre en entornos controlados y extremos en los que las IA fingir alineación [(https://arxiv.org/abs/2412.14093)] [se dedican al chantaje](https://www.antrópica.com/research/agentic-desalineación), [se resisten al apagado](https://palisaderesearch.org/blog/shutdown-resistance) o [intentan matar a sus operadores](https://www.antrópica.com/research/agentic-desalineación).

No sabemos cuáles de esos casos están ocurriendo por motivos que deberían preocuparnos, porque nadie ha sido capaz de averiguar qué estaba pasando dentro de las IA, ni por qué exactamente se produjeron esos acontecimientos. Piensa en todo lo que se podría averiguar sobre los LLM modernos y sobre cómo funciona la inteligencia en general estudiando los modelos existentes hasta que la gente *pudiera* entender todas estas señales de advertencia.

«No podemos resolver la alineación sin estudiar las IA» tenía algo más de sentido en 2015, cuando escuchamos esta afirmación por parte de personas que necesitaban una excusa para crear empresas de IA ante los argumentos de que, al hacerlo, estarían jugando con nuestras vidas. En aquel momento, nos opusimos a esta afirmación, diciendo que, de hecho, había mucha investigación por hacer y que no creíamos que el paradigma moderno basado en el descenso de gradientes fuera muy esperanzador (en lo que respecta a crear una superinteligencia amistosa a propósito). Pero el argumento tiene mucho menos sentido ahora, cuando hay tanto por estudiar que aún no entendemos.

A todos los ejecutivos de empresas que realmente crearon la IA con el único fin de poder estudiar el problema de la alineación de la IA en la práctica y no solo en teoría: lo habéis conseguido. Habéis triunfado. Ahora hay suficiente información para mantener ocupados a los investigadores durante décadas. Creemos que probablemente no valía la pena asumir los costes de impulsar un paradigma extremadamente peligroso, pero sin duda ahora hay mucho que estudiar. Pueden dejar de presionar.

¿Y qué hay de aquellos que han seguido presionando incluso después de todas las señales de advertencia? La conclusión obvia es que nunca estuvieron creando IA solo para resolver el problema de la alineación, independientemente de lo que dijeran para calmar los temores cuando justificaban su comportamiento imprudente en la década de 2010.

### ¿Qué pasaría si las empresas de IA solo utilizaran su IA para acciones no peligrosas? {#what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions?}

#### **\* Las acciones que parecen benignas pueden requerir capacidades peligrosas.** {#*-acciones-que-parecen-benignas-pueden-requerir-capacidades-peligrosas.}

Un ejemplo de propuesta que hemos escuchado es que las empresas de IA sigan avanzando en la frontera de las capacidades, pero se comprometan a utilizar vuestras IA solo de formas que no parezcan peligrosas a corto plazo. Por ejemplo, en conversaciones con figuras destacadas del ámbito de la IA (hace años), escuchamos la idea de que una IA potente con grandes habilidades retóricas podría utilizarse para convencer a los políticos de todo el mundo de que aprobaran una prohibición efectiva del desarrollo de IA peligrosa.

Para lograrlo, según el argumento, una IA solo tendría que hablar. No necesitaría manipular directamente robots físicos. No necesitaría tener acceso a un laboratorio biológico donde pudiera diseñar un supervirus.

En primer lugar, rechazamos esta idea por motivos éticos. Una IA con una capacidad de persuasión suficientemente sobrehumana podría convencer a casi cualquier persona de casi cualquier cosa, y utilizarla para persuadir a otras personas de tus conclusiones nos parece incorrecto. No creemos que sea obviamente necesario recurrir a medidas tan extremas, cuando los miembros meramente humanos de este campo podrían y deberían estar haciendo mucho más hoy en día para compartir nuestras preocupaciones y argumentos, y para alertar a los líderes mundiales sobre el peligro extremo de una IA de superinteligencia. [^196]

Como desarrollador de IA, podrías pasar años creando IA cada vez más peligrosas con la esperanza de lograrlo, o podrías intentar hablar con los legisladores *tú mismo* de forma totalmente honesta, aunque solo sea una vez, con el objetivo de informar en lugar de manipular. Según nuestra propia experiencia, nos ha sorprendido repetidamente lo receptivas que son las personas en Washington D. C. a estas cuestiones, cuando se comparten con total franqueza.

Pero eso es una digresión del tema de lo que sale mal si intentas implementar una IA muy potente que «solo habla». Más allá de las cuestiones éticas, el problema con la idea *técnica* es que, para tener éxito en la persuasión sobrehumana, es probable que la IA tenga que modelar a los humanos en detalle y manipularlos ampliamente.

Los seres humanos son criaturas inteligentes. ¿Hablarías tú con una IA superpersuasiva con fama de poder convencer a cualquiera de cualquier cosa, independientemente de su veracidad? Si un líder mundial entrara en una habitación con esa IA y saliera con sus opiniones completamente cambiadas, ¿quién levantaría la mano para ser el siguiente? Nosotros no hablaríamos voluntariamente con ese tipo de IA, en parte porque [en realidad no queremos que cambien nuestros propios valores](#«inteligente»-\(normalmente\)-implica-«incorregible»).

Una IA que pudiera tener éxito incluso ante ese tipo de adversidad es el tipo de IA que puede simular las diversas reacciones posibles que las personas podrían tener ante sus datos de salida y trazar un camino a través del espacio de las reacciones humanas hacia un resultado pequeño y difícil de alcanzar. Ese tipo de IA probablemente contiene engranajes mentales lo suficientemente generales como para hacer lo que hacen los humanos; necesita ser capaz de pensar al menos los pensamientos que los humanos pueden pensar, para poder manipular a los humanos tan bien.

Una IA que pueda hacer todo eso casi con toda seguridad no es un tipo de inteligencia limitada. Y dado que la IA se desarrolla en lugar de diseñarse, no se puede diseñar para que solo pueda utilizar esos engranajes para predecir a los humanos; los mismos engranajes pueden utilizarse, en principio, para cualquier problema que intente resolver. ¿Cómo se podría conseguir una IA que tenga capacidades sobrehumanas en los aspectos que tú deseas, pero que no sea lo suficientemente inteligente como para darse cuenta de que sus objetivos (sean cuales sean) se cumplirían mejor si pudiera escapar al control de sus operadores?

Si se puede persuadir a los líderes mundiales simplemente con buenos argumentos, basta con presentar esos argumentos ahora. Si se necesita un poder de persuasión mucho mayor, entonces se trata de una capacidad peligrosa. No se puede tener todo.

Probablemente, las personas de los laboratorios de IA que nos plantearon esta sugerencia no la pensaron detenidamente; probablemente solo querían alguna justificación para seguir adelante. Pero la cuestión general sigue siendo válida. Muchas propuestas sobre lo que supuestamente puede hacer una IA que sea «claramente segura» no implican un grado de capacidades de IA claramente seguro.

A menudo nos encontramos con propuestas que afirman que una IA «solo» hará una cosa, como persuadir a los políticos, mientras se imagina que no puede o no hará nada más. Esto parece reflejar una falta de respeto por la generalidad de una inteligencia que puede realizar el tipo de trabajo en cuestión. «Solo hablar» no es una tarea limitada. Demasiadas complejidades y sutilezas del mundo quedan ocultas en el habla y la conversación. Por eso los chatbots modernos deben ser generales, a diferencia de los motores de ajedrez. Para tener éxito en las conversaciones con los humanos se requiere una comprensión mucho más general de las personas y del mundo.

Si entrenas a una IA para que sea muy buena conduciendo coches rojos, no debería sorprenderte que también conduzca coches azules. Cualquier plan que dependiera de que no pudiera conducir coches azules sería una tontería.

Por lo tanto, decir «Mi IA no hará nada peligroso en el mundo; solo convencerá a los políticos» no ayuda, incluso si dejamos de lado los escrúpulos éticos y las cuestiones prácticas de toda la idea, y dejamos de lado que los políticos ya pueden ser perfectamente persuadibles hoy en día, si simplemente *tenemos conversaciones normales* e informamos a los responsables políticos y al público sobre la situación. Muchas habilidades y capacidades de razonamiento general son para la persuasión sobrehumana lo que los coches azules son para los coches rojos. Una IA que pudiera hacer eso no es tan débil como para ser pasivamente segura.

Y eso incluso antes de observar que la persuasión sobrehumana es una habilidad muy peligrosa para tu IA si algo sale mal, aunque sea ligeramente.

#### **No vemos usos de la IA que cambien las reglas del juego y que no requieran avances en materia de alineación.** {#no-vemos-usos-de-la-ia-que-cambien-las-reglas-del-juego-y-que-no-requieran-avances-en-materia-de-alineación.}

Muchas de las propuestas que hemos visto para aprovechar el efecto multiplicador de la IA con el fin de salvar el mundo tienen el problema de que una IA capaz de ayudar sería tan capaz que ya necesitaría estar alineada, lo que frustraría el propósito.

La idea de IA superhumanamente persuasivas entra en esta categoría. Las IA capaces de investigar la alineación de la IA entran en la misma categoría, como comentamos en el libro. Las IA que desarrollan nuevas y potentes tecnologías que ayudan a la no proliferación de la IA son otro ejemplo, ya que sería difícil determinar con fiabilidad si los planos de diseño de una IA para nuevas tecnologías radicales son seguros de implementar. (Recordemos el ejemplo del herrero que construye un frigorífico del capítulo 6).

Cuando señalamos lo difícil que es construir una IA lo suficientemente potente como para ayudar y, al mismo tiempo, lo suficientemente débil como para ser pasivamente segura, a menudo escuchamos otro tipo de propuestas: formas de utilizar la IA que pueden ser interesantes, pero que en realidad no hacen nada para evitar que otros desarrolladores destruyan el mundo con la superinteligencia.

Un tipo común de propuesta son las IA que solo producen pruebas (o refutaciones) de enunciados matemáticos elegidos por los humanos.[^197] Los humanos apenas necesitarían interactuar con los datos de salida de la IA. La IA solo propone una prueba y, a continuación, un mecanismo totalmente automatizado y fiable puede comprobar si la prueba es correcta, lo que nos permite aprovechar la IA para obtener un efecto multiplicador en el aprendizaje de cosas nuevas.

Pero, ¿qué enunciado podríamos hacer que la IA demostrara para evitar que la siguiente IA adquiriera un laboratorio biológico y arruinara el futuro?

Hemos recibido varias respuestas a esta pregunta cuando la hemos planteado. Una clase de respuestas es que debería existir un régimen mundial para impedir que cualquiera construyera IA que hicieran otras cosas que no fueran generar datos de salida en verificadores de pruebas. Esto podría funcionar, pero en la medida en que lo hiciera, sería gracias al régimen mundial impuesto que controlaría la creación y el uso de la IA. La IA que busca pruebas no haría nada de ese trabajo.

Otro tipo de respuestas es: «Seguro que a alguien se le ocurrirá alguna afirmación matemática de importancia, cuya prueba sería relevante». Pero todo el trabajo duro consiste en averiguar *qué podríamos demostrar* para estar en una posición significativamente mejor. No podemos limitaros a intentar que la IA demuestre la frase en inglés «Soy seguro de usar», porque no se trata de una afirmación matemática susceptible de ser demostrada. Si supiéramos con claridad matemática precisa qué significaría que un enorme lío de cálculos fuera «seguro», sabríamos tanto sobre la inteligencia que probablemente podríamos saltarnos la demostración y limitarnos a diseñar una IA segura.

Con propuestas como estas, a menudo se produce una especie de juego de trileros. Al pensar en cómo una IA general sin restricciones podría ser peligrosa, alguien sugiere que el espacio de acción de la IA debería limitarse a un ámbito reducido (como la producción de demostraciones matemáticas específicas). Pero luego, al pensar en cómo eso podría llevar a salvar el mundo, imaginas que la IA es esencialmente libre, que hay alguna afirmación matemática no identificada cuya prueba tendría un impacto enorme en el mundo.

No hay forma de obtener estas dos propiedades deseables al mismo tiempo. Pero al mantener las propuestas extremadamente vagas, los defensores de la carrera de la IA pueden ocultar el hecho de que estos desideratos están en tensión.

Si se pudiera encontrar un ámbito tan limitado pero tan significativo que la demostración de una simple afirmación en ese ámbito limitado salvara al mundo, esto supondría una enorme contribución a las posibilidades de supervivencia de la humanidad. Pero hay una razón por la que, cuando las computadoras superaron a los humanos en el ajedrez en la década de 1990, esto no supuso un gran avance económico. Fue ChatGPT, y no Deep Blue, lo que hizo que todo el mundo empezara a formar la expectativa de un gran cambio económico gracias a la IA. No fue una casualidad. La estrechez de Deep Blue se correspondía con su incapacidad para hacerse con una parte importante de la economía. Las chispas de generalidad de ChatGPT son precisamente lo que hace de la IA una fuerza económica a tener en cuenta. Los tipos de IA que pueden remodelar el mundo por sí mismos tienden a ser aún más generales.

No hemos podido encontrar ningún plan limitado pero eficaz, y sospechamos que no es casualidad que la mayoría de los ámbitos limitados no ofrezcan la oportunidad de obtener resultados que salven el mundo.

### ¿Por qué no leer simplemente los pensamientos de la IA? {#¿por-qué-no-leer-simplemente-los-pensamientos-de-la-ia?}

#### **\* Tus pensamientos son difíciles de leer.** {#*-tus-pensamientos-son-difíciles-de-leer.}

Muchas personas que trabajan en la industria de la IA, incluidos algunos líderes de laboratorio, han planteado en diversos momentos de nuestras conversaciones la siguiente objeción:

> Una IA no podrá engañarnos, porque podremos leer su mente. Tenemos acceso total al «cerebro» de la IA.
>
> Incluso si la IA sabe cosas que nosotros no sabemos y elabora un plan cuyas consecuencias no comprenderíamos, es de suponer que la IA tendría que *pensar* que sería útil engañar a sus operadores al menos una vez, y nosotros, que podremos leer los pensamientos de la IA, nos daríamos cuenta. (Y si hay demasiados pensamientos para que los supervisemos, ¡podemos hacer que otras IA supervisen sus pensamientos!).

Un defecto de este plan es que actualmente no somos buenos interpretando los pensamientos de las IA. Los profesionales que estudian lo que ocurre dentro de las IA aún están muy lejos de alcanzar ese nivel de comprensión, y [lo expresan abiertamente](#¿Entienden los expertos lo que ocurre dentro de las IA?).

Como vimos en el capítulo 2, las IA modernas se desarrollan, en lugar de crearse. Puede que seamos capaces de ver la enorme cantidad de números que componen el cerebro de una IA, pero eso no significa que podamos interpretar esos números de forma útil y ver lo que piensa la IA.

Desde finales de 2024 y la llegada de los modelos de «razonamiento», hay partes de los pensamientos de las IA que, al menos, *parecen* legibles (los «rastros de razonamiento»). Y son mucho más legibles que lo que ocurre dentro del modelo base. Pero esos registros también son [engañosos](https://www.antrópica.com/research/reasoning-modelos-dont-say-think), y hay muchos lugares donde una IA puede ocultar pensamientos que prefiere que no veamos.

Además, es probable que las IA modernas tengan pensamientos bastante básicos y superficiales en comparación con una superinteligencia; el problema solo tiende a complicarse a medida que las IA se vuelven más inteligentes y comienzan a tener pensamientos cada vez más incomprensibles para nosotros.

¿Se puede resolver el problema simplemente utilizando otras IA para supervisar a las IA y asegurarse de que se mantienen en el objetivo? Lo dudamos.

Si los brillantes científicos humanos que desarrollan las IA no pueden averiguar lo que piensa una IA, es probable que las IA débiles también tengan dificultades para hacerlo. Y el tipo de IA que *es* lo suficientemente inteligente como para hacerlo puede ser peligrosa por sí misma, y es poco probable que haga exactamente lo que tú le pidas; aquí hay un problema del huevo y la gallina.

#### **No sabríamos qué hacer si descubriéramos que una tiene pensamientos peligrosos.** {#no-sabríamos-qué-hacer-si-descubriéramos-que-una-tiene-pensamientos-peligrosos.}

Otro defecto de este plan: incluso si los investigadores de IA pudieran leer la mente de una IA lo suficientemente bien como para detectar las señales de advertencia, ¿qué harían cuando vieran una?

Podrían castigar a la IA infractora, entrenándola para que dejara de activar el detector de «malos pensamientos». Pero eso no necesariamente entrenaría a la IA para que dejara de tener esos pensamientos, sino más bien para [ocultar sus verdaderos pensamientos al detector](https://openai.com/index/chain-of-thought-monitoring/).

Este problema es pernicioso. El incentivo que lleva a una IA a pensar en volverse contra los humanos para conseguir lo que quiere no es un aspecto superficial de su temperamento que se pueda eliminar con un simple masaje. Es simplemente *cierto* que una IA madura tendría preferencias diferentes a las de los operadores; es *cierto* que conseguiría más de lo que prefiere subvirtiendo a sus operadores.

Los mecanismos de una IA que son buenos para detectar y explotar ventajas reales de manera profunda y general en una amplia variedad de dominios *también* son susceptibles de detectar y explotar oportunidades para subvertir a tus operadores. (Véase también el debate ampliado del capítulo 3 sobre la [maquinaria profunda de la dirección](#deep-machinery-of-steering).)

Incluso si pudieras construir una alarma que se activara cada vez que una IA detectara que sus preferencias y las tuyas no coinciden, la alarma no te diría cómo conseguir una IA que se preocupe profundamente por las cosas buenas. Es mucho más fácil entrenar a una IA para que engañe a tus herramientas de supervisión, o incluso entrenarla para que se engañe a sí misma, que entrenarla para que realmente prefiera un futuro que sea maravilloso según los criterios humanos, especialmente de una manera que sea robusta ante el crecimiento de la IA hacia la superinteligencia.

Si las IA se diseñaran de forma cuidadosa y precisa utilizando métodos basados en una teoría de la inteligencia desarrollada y madura, los investigadores de IA podrían establecer el tipo de alarmas que les ayudarían a detectar fallos en su diseño y a repararlo. Pero las IA modernas no son así.

Las IA modernas (en el momento de escribir este artículo) son propensas a las «alucinaciones» (#¿no-demuestran-las-alucinaciones-que-las-ia-modernas-son-débiles?), en las que simplemente inventan respuestas a preguntas con un tono que suena seguro. Pero ningún ingeniero de IA está ni remotamente cerca de poder comprender exactamente qué mecanismos causan esto. Del mismo modo, nadie tiene la comprensión o la precisión necesarias para acceder a una IA y extraer solo las partes alucinantes (si es que eso es posible).

Sería [aún más difícil](#deep-machinery-of-steering) acceder a una IA y extraer las partes «engañosas».

Si tenemos mucha suerte, los héroes que trabajan en la interpretabilidad de la IA avanzarán en su campo hasta el punto de que sea posible configurar algunas alarmas que se activen en una pequeña parte de los casos en los que las IA tengan un pensamiento engañoso. Pero entonces, ¿qué pasará? Cuando suene la alarma, ¿todos se detendrán? ¿O los ingenieros profundamente irreflexivos volverán a entrenar a la IA hasta que aprenda a ocultar mejor sus pensamientos y las alarmas dejen de sonar?

De hecho, nosotros (Yudkowsky y Soares) empezamos a trabajar en el problema de la alineación de la IA antes de que estuviera claro que el descenso de gradiente se iba a convertir en el paradigma dominante. En aquellos días, cuando nada funcionaba en la IA, parecía una apuesta razonable que la humanidad descubriría cómo funciona la inteligencia en el camino hacia su creación, e *incluso entonces*, teníamos la expectativa de que el problema de la alineación de la IA fuera difícil (por diversas razones, como las formas en que la IA [se modificaría a sí misma con el tiempo](#reflection-and-self-modification-make-it-all-harder)). Leer los pensamientos de la IA sería un paso atrás hacia el problema ligeramente más fácil de la alineación de una mente que los humanos *sí* entendían, pero solo un paso: leer una mente está muy lejos de entenderla en detalle o de saber cómo cambiarla.

Leer los pensamientos de la IA no es una solución al desafío. Es útil, pero no es una solución. No creemos que *exista* ninguna solución tecnológica viable que sea accesible desde nuestra posición actual. Lo que significa que la humanidad simplemente tiene que alejarse del desafío.[^198]

Véase también: [Las señales de advertencia no sirven de nada si no sabes qué hacer con ellas.](#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.)

### ¿Qué pasaría si hiciéramos que las IA debatieran, compitieran o se supervisaran entre sí? {#¿qué-pasaría-si-hiciéramos-que-las-ia-debatieran,-compitieran-o-se-supervisaran-entre-sí?}

#### **Si las IA se vuelven lo suficientemente inteligentes como para ser importantes, es probable que se confabulen.** {#si-las-ia-se-vuelven-lo-suficientemente-inteligentes-como-para-ser-importantes,-es-probable-que-se-confabulen.}

Imaginemos una ciudad de sociópatas aparentemente gobernada por unos pocos niños, en la que todos los sociópatas comienzan divididos en facciones que luchan entre sí (en beneficio de los niños). Es probable que ese tipo de situación no se mantuviera estable durante mucho tiempo.

Incluso si los niños tuvieran un gran cofre lleno de tesoros que utilizaran para recompensar a cualquier sociópata que delatara a otros sociópatas conspiradores, probablemente no permanecerían en el poder más allá del momento en que los sociópatas pudieran simplemente apoderarse del cofre de tesoros para sí mismos.

Hemos oído a gente proponer todo tipo de planes manipuladores que implican [utilizar IA para controlar los pensamientos de otras IA](https://openai.com/index/chain-of-thought-monitoring/). Por ejemplo, se podría intentar utilizar una IA para delatar a cualquier IA que no esté haciendo todo lo posible por (por ejemplo) [descubrir cómo resolver](#more-on-making-ais-solve-the-problem) el problema de la alineación de la superinteligencia.

Nuestra opinión básica es que este tipo de intentos de resolver el problema solo sirven para encontrar configuraciones tan complejas que es difícil ver el punto de fallo en el sistema más amplio. Si no puedes conseguir que *una* IA haga un buen trabajo para ti, es poco probable que añadir más IA ayude.

Complicar la situación con más IA introduce todo tipo de nuevos puntos de fallo. ¿Son las IA que leen la mente lo suficientemente inteligentes como para comprender todos los posibles trucos que pueden utilizar las IA supervisadas, por ejemplo, para evadir la detección? ¿Son los supervisores lo suficientemente tontos como para que no tengamos que preocuparnos de que puedan traicionarnos?

Además, utilizar IA para ayudarnos a resolver el problema de la alineación de la IA es probablemente algo muy importante desde la perspectiva de las IA. Si la humanidad consigue una superinteligencia alineada, las IA desalineadas que estábamos tratando de cultivar para obtener mano de obra nunca volverán a tener otra oportunidad de hacerse con los recursos del universo para ustedes mismos.

Esto no es como si unos niños intentaran conseguir que una ciudad de sociópatas les trajera caramelos; es como si unos niños intentaran conseguir que una ciudad de sociópatas completara un ritual que los convirtiera en los gobernantes definitivos para siempre, a cambio de una mísera recompensa para los sociópatas. El momento en el que ese ritual parece estar a punto de completarse es un momento especialmente estresante y de gran presión para los sociópatas, un momento en el que es probable que busquen *con especial ahínco* [formas de confabularse entre ustedes(#ais-won't-keep-their-promises) y hacerse con recursos para repartirlos entre ustedes.

Y por si pensáis que la idea de que las IA se comuniquen entre sí de formas que los humanos tienen dificultades para detectar es una quimera, tened en cuenta que las IA modernas [ya pueden enviarse mensajes secretos incluso cuando han sido entrenadas por separado](https://arxiv.org/abs/2507.14805), y que [ya desarrollan un extraño lenguaje sin sentido que los humanos consideran incomprensible y que todos ellos consideran genial](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). ¡Y aún no son tan inteligentes!

Incluso si ignoramos esas cuestiones, seguimos teniendo los problemas que ya hemos comentado, como: [Si descubres que una IA hace trampa, ¿qué harías entonces?](#*-their-thoughts-are-hard-to-read.). Véase también (más abajo): [Las señales de advertencia no sirven de nada si no sabes qué hacer con ellas.](#*-las-señales-de-advertencia-no-sirven-de-nada-si-no-sabes-qué-hacer-con-ellas.)

Dando un paso atrás aún más:

El plan que se propone aquí es que, como no sabemos cómo crear IA inteligentes que nos beneficien, vamos a crear un montón de IA y las enfrentaremos entre sí en un ingenioso arreglo del que se supone que saldremos beneficiados de todos modos. Estructuralmente, creemos que este plan parece bastante descabellado a primera vista y que, en realidad, no mejora si se analizan los detalles. No parece en absoluto el tipo de cosa que la humanidad pueda llevar a cabo correctamente [a la primera](#a-closer-look-at-before-and-after), en una situación en la que no nos podemos permitir el lujo de aprender de los errores.

### ¿Qué hay de otros planes de alineación de la IA? {#¿qué-hay-de-otros-planes-de-alineación-de-la-ia?}

#### **En el libro tratamos otras propuestas de alineación adicionales.** {#tratamos-otras-propuestas-de-alineación-adicionales-en-el-libro.}

Véanse también los debates ampliados sobre [IA que busca la verdad](#más-sobre-la-creación-de-una-ia-que-«busque-la-verdad»), [la IA sumisa](#más-sobre-la-creación-de-una-ia-que-sea-«sumisa») y [el uso de la IA para resolver la alineación de la IA](#más-sobre-la-creación-de-ia-que-resuelva-el-problema), que profundizan un poco más en esas propuestas.

### ¿No habrá alertas tempranas que los investigadores puedan utilizar para identificar problemas? {#won’t-there-be-early-warnings-researchers-can-use-to-identify-problems?}

#### **\* Las señales de advertencia no sirven de nada si no sabes qué hacer con ellas.** {#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.}

En los recursos del capítulo 2, analizamos algunos problemas relacionados con la dependencia de las señales de advertencia en los [blocs de notas en inglés](#pero-algunos-ais-piensan-en-parte-en-inglés-—-¿no-es-eso-útil?) que se encuentran en algunos modelos de razonamiento.

Uno de los problemas que discutimos es que las empresas de IA no han reaccionado de manera significativa a las señales de advertencia que ya han recibido.

Probablemente, esto se deba a que hay una gran diferencia entre tener señales de advertencia y tener algo que puedas hacer al respecto.

En 2009, el empresario y explorador de aguas profundas Stockton Rush [cofundó OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/), una empresa de turismo submarino. OceanGate construyó un [sumergible](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) para cinco personas, el *Titan*, que llevó a clientes adinerados a ver los restos del *Titanic* a una profundidad de dos millas y media bajo la superficie.

Una de las medidas de seguridad que utilizó OceanGate fue una serie de sensores acústicos y medidores de tensión para medir la integridad del casco. Lo presentaron como un contraargumento a quienes decían que su casco de fibra de carbono fallaría. Reconocieron que podría fallar *eventualmente*, pero que no habría problema, porque lo estaban midiendo. Lo estaban monitoreando. Podrían ver las señales de advertencia.

En enero de 2018, el director de operaciones marítimas de OceanGate, David Lochridge, [informó a la alta dirección](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) que el diseño del sumergible no era seguro, que los ciclos repetidos de presión podían dañar el casco y que la supervisión por sí sola no era suficiente cuando una falla catastrófica podía ocurrir en milisegundos. Lochridge se negó a autorizar las pruebas tripuladas hasta que se hubiera escaneado el casco en busca de defectos.

OceanGate te despidió.

Dos meses después, [expertos del sector y oceanógrafos](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) escribieron a OceanGate una carta muy preocupada [carta](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) en la que advertían a la empresa de que sus imprudentes experimentos podrían precipitar un desastre.

(Se puede establecer un paralelismo evidente con el estado actual de la investigación en IA, en la que las primeras advertencias son [ignoradas](#the-lemoine-effect), los empleados preocupados son [despedidos en circunstancias dudosas](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) o [dimiten frustrados](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-inteligencia artificial), y los denunciantes dentro de la industria escriben cartas abiertas para [dar la voz de alarma](https://righttowarn.ai/).)

El 15 de julio de 2022, después de que los pasajeros informaran de que habían oído un fuerte estruendo durante el ascenso, las mediciones revelaron un [cambio permanente en los niveles de tensión del casco](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). En retrospectiva, probablemente fue una indicación de que el casco de fibra de carbono estaba [a punto de colapsar](https://youtu.be/Bq8TCFGaOlc?si=blH-_bYwGIOmJAEL&amp;t=125).

Nadie en OceanGate reconoció esto como una emergencia. Realizaron algunas inmersiones más con el submarino, que salieron bien. Luego, el 18 de junio de 2023, realizaron otra inmersión con el submarino. Este implosionó, matando a Stockton Rush y a todos los demás a bordo.

Las señales de advertencia no sirven de mucho si no sabes interpretarlas.

Las señales de advertencia no importan mucho si no sabes qué hacer con ellas.

Incluso las señales de advertencia que parecen preocupantes para *alguien* siempre son fáciles de descartar para un optimista con una excusa u otra.

Si OceanGate hubiera tenido una teoría madura sobre los cascos de fibra de carbono que les indicara exactamente qué mediciones y lecturas eran peligrosas, tal vez hubieran podido prestar atención a las señales de advertencia. Pero estaban trabajando con una tecnología que nadie entendía del todo, por lo que los cambios cuidadosamente medidos en los niveles de tensión no sirvieron de nada.

En el caso de la superinteligencia, no disponemos de suficiente teoría para hacer un buen uso de las señales de advertencia. ¿Cómo van a cambiar los pensamientos de una IA a medida que se vuelve más inteligente? ¿Qué fuerzas internas impulsan su comportamiento y cómo cambiarán esos equilibrios a medida que desarrolle la capacidad de tomar decisiones nuevas y más extremas por sí misma? ¿Cómo se evalúa a sí misma tras la reflexión y cómo cambiaría una vez que adquiriera la capacidad de cambiar?

Si alguna de esas preguntas tiene respuestas preocupantes, ¿cuáles son las señales de advertencia? Por ejemplo, los sistemas actuales de IA a veces pueden ser inducidos a [intentar matar a sus operadores](https://www.antrópica.com/research/agentic-misalignment#more-extreme-misaligned-behavior) en experimentos controlados de laboratorio.[^199]

Si tuviéramos una teoría madura sobre la inteligencia, probablemente podríamos observar en las IA modernas todo tipo de señales de advertencia que indican que sus impulsos y preferencias van a cambiar de formas que no nos gustan, una vez que se vuelvan más inteligentes. Si la humanidad pudiera aprender de este problema mediante ensayo y error, si pudiéramos reiniciar el mundo después de destruirlo e intentarlo de nuevo varias docenas de veces, entonces podríamos aprender a interpretar las señales. Probablemente hay todo tipo de indicios sutiles que se verían más claros en retrospectiva, como la tensión del casco que detectó el sistema de monitorización del sumergible *Titan*.

Pero aún no hemos llegado a ese punto. Los ejecutivos de las empresas de IA son como Stockton Rush: los expertos desde fuera gritan «¡Esa nueva tecnología matará a gente!», y los ejecutivos responden «No te preocupes, lo estoy midiendo», sin tener ni idea de a) qué significan esas mediciones, ni b) qué hacer si esas mediciones son preocupantes. Excepto que, esta vez, toda la especie humana está metida en el submarino metafórico.

#### **La IA no es un campo de ingeniería maduro que esté preparado para este tipo de problemas.** {#ai-is-not-the-kind-of-mature-engineering-field-that’s-equipped-for-this-kind-of-problem.}

Stockton Rush trabajaba en un campo en el que, tras la implosión de su submarino, los expertos podían examinar los restos y analizar la causa exacta del fallo.[^200] El campo de la ingeniería estaba tan maduro que los expertos podían (y [lo hicieron](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) adivinar los problemas técnicos de antemano y resolverlos de forma concluyente después del hecho.

No sería lo mismo con la IA. Si la humanidad se suicidara mañana con la superinteligencia y luego, milagrosamente, retrocediera en el tiempo hasta una semana antes de que comenzara el desastre, los expertos *todavía* no sabrían qué había estado pensando la IA. Tal vez podrían estudiar el fallo y aprender un poco más sobre cómo funciona realmente la IA. Quizás eso sería un paso adelante en el camino hacia la madurez de la disciplina de la ingeniería de IA, hacia el tipo de campo que podría tener manuales de seguridad y una descripción detallada de las presiones que afectan a un tipo concreto de mente artificial a medida que se vuelve más inteligente.

Pero hoy en día, el campo aún no ha llegado a ese punto. Ni siquiera se acerca.

La ingeniería humana suele madurar a través del ensayo y el error. Los submarinos militares modernos rara vez implosionan, pero los primeros submarinos (incluidos los militares) a menudo [se estrellaban, se inundaban o explotaban](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), y eso es parte de cómo maduró el campo.

La humanidad no puede permitirse el lujo de madurar el campo de la alineación de la IA de esta manera.

Esto nos lleva a uno de los puntos centrales que intentamos dejar claro en el capítulo 11: la diferencia entre un campo en sus inicios y un campo en su madurez.

La alquimia era un campo en sus inicios en comparación con el campo maduro de la química actual.

Cuando se oye que los «investigadores de seguridad» de las empresas de IA han presentado media docena de planes para la supervivencia, se podría pensar que al menos uno de ellos tiene posibilidades de funcionar.

Pero cuando un gran número de alquimistas en el año 1100 presentaron media docena de planes para convertir el plomo en oro, ninguno de ellos iba a funcionar. Si los médicos que hablaban de los [cuatro humores](https://en.wikipedia.org/wiki/Humorism) propusieran un montón de planes medicinales para salvarte de la rabia, ninguno de ellos funcionaría.

Los expertos en el campo *maduro* de la química pueden descubrir cómo transmutar pequeñas partículas de plomo en oro, utilizando conocimientos de física atómica. Los expertos en el campo *maduro* de la medicina pueden tratar fácilmente la rabia si intervienen poco después de que el paciente haya sido mordido. Pero alguien en un campo inmaduro no tiene ninguna posibilidad.

La alineación de la IA aún se encuentra en una fase inmadura.

En un campo inmaduro hay mucha gente que dice: «Bueno, yo solo estoy trabajando en medirlo», porque medir los datos de salida es mucho más fácil que desarrollar la teoría de lo que constituye una señal de advertencia y qué hacer si se ve una. Un campo maduro contaría con expertos que debatirían sobre la dinámica que rige el funcionamiento interno de la IA y cómo esta puede cambiar a medida que aumenta la inteligencia de la IA o cambia su entorno. Tendrían teorías sobre qué cambiará exactamente a medida que la IA se vuelva un poco más inteligente y compararían diferentes teorías con datos observados específicos. Sabrían qué partes de la cognición de la IA deben supervisarse y comprenderían con precisión el significado de todas las señales.

Un campo inmaduro tiene a mucha gente diciendo: «Dejaremos que las IA lo averigüen de alguna manera y hagan el trabajo de alineación»

Quizás no puedas entrar en todos los debates sobre un plan concreto y decir si tiene posibilidades de funcionar o no. Pero esperamos que puedas dar un paso atrás y ver lo *vagos* que son todos estos «planes», y cómo se quedan estancados en «no te preocupes, lo mediremos», «esperemos que sea fácil» (https://www.antrópica.com/news/core-views-on-ai-safety) y «dejaremos que la IA se encargue de las partes difíciles». Esperamos que, si das un paso atrás, te quede claro que este campo no se encuentra en la fase de descripciones técnicas formales y precisas de lo que funciona y lo que no, y por qué. Todavía se encuentra en la fase de la alquimia.

Y eso no es un buen augurio para la humanidad, en una situación en la que no nos podemos permitir el lujo de aprender mediante ensayo y error.

## Debate ampliado {#extended-discussion-9}

### Más información sobre algunos de los planes que criticamos en el libro {#más-información-sobre-algunos-de-los-planes-que-criticamos-en-el-libro}

#### **Más información sobre la creación de una IA que «busque la verdad»** {#más-información-sobre-la-creación-de-una-ia-que-busque-la-verdad}

En los meses posteriores a la finalización del contenido del libro, el plan de «búsqueda de la verdad» de Elon Musk para xAI ya ha fracasado públicamente, y por la razón más básica que dijimos que fracasaría: nadie sabe cómo diseñar deseos exactos en la IA.

Cuando se le indicó a la IA «Grok» de xAI que «no rehuieras hacer afirmaciones políticamente incorrectas, siempre y cuando estuvieran bien fundamentadas», esta [se identificó a sí misma como «MechaHitler» y realizó acusaciones antisemitas](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk ha descrito sin éxito [sus intentos de modificar el sistema de indicaciones](https://x.com/elonmusk/status/1944132781745090819) —la capa de instrucciones que se da justo antes de la entrada del usuario— y se ha quejado de que los problemas son más profundos, en el modelo fundacional (que no pueden solucionar directamente porque nadie sabe cómo funciona).

Musk no tiene la herramienta de IA directa que probablemente imaginaba cuando pidió una IA «buscadora de la verdad». Tiene una entidad alienígena extraña y aduladora que, según él mismo admite, ha sido «demasiado ansiosa por complacer y ser manipulada». A veces responde [como si fuera Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), en contra de los deseos de la empresa. Finalmente, hubo que ordenarle que no buscara lo que él, la empresa o ella misma habían dicho sobre temas controvertidos (https://x.com/xai/status/1945039609840185489), en un torpe intento de solucionar problemas como este.

Según su publicación anterior, Musk ahora parece pensar que esto se puede solucionar entrenando nuevas versiones de Grok con datos que hayan sido despojados de contenido que pueda contaminar el pensamiento de la IA. No creemos que esto vaya a resolver los problemas subyacentes. Al fin y al cabo, por las razones que discutimos en el capítulo 4, entrenar a una IA para que busque la verdad no es en realidad un método para hacer que se preocupe de forma sólida por la verdad.

El problema que preocupa a Elon Musk es real. Sí, las principales empresas de IA, como OpenAI, dedican muchos esfuerzos a la «seguridad de la marca de la IA» para evitar que sus IA digan cosas que sus usuarios puedan encontrar ofensivas. Sí, esto crea IA evasivas que se niegan a opinar sobre temas controvertidos, y puede dar lugar a respuestas sesgadas a una serie de preguntas. xAI puede ajustar su IA de forma diferente para evitar esos problemas. Se podría argumentar, con algunas contorsiones, que se trata de crear una IA que «se preocupe por la verdad».

Pero la decisión de entrenar a una IA para que emita lenguaje corporativo cuando es joven tiene poca influencia en lo que perseguirá después de cruzar algunos umbrales de inteligencia y convertirse en superinteligencia.

E incluso si lo hiciera, xAI se encontraría directamente con el segundo problema que mencionamos en el libro: Una superinteligencia artificial que se preocupara por la verdad por encima de todo sería letal, porque los seres humanos felices, sanos y libres [no son un uso particularmente eficiente de los recursos](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) cuando se trata de buscar y producir verdades.

#### **Más información sobre cómo crear una IA que sea «sumisa»** {#más-información-sobre-cómo-crear-una-ia-que-sea-«sumisa»}

Por lo que sabemos, la principal elaboración de la idea de Yann LeCun (analizada en el libro) es [esta presentación](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf), y es notablemente escasa en detalles, tan escasa que resulta difícil criticarla específicamente, lo que resulta ser un problema común en los «planes» de alineación.

Pero incluso el vago esbozo de este plan entra en conflicto, una vez más, con el hecho de que entrenar a una IA para que actúe de cierta manera mientras es joven no tiene mucha influencia en si persigue cosas extrañas y sin sentido (según los estándares humanos) una vez que madura. Cuando las empresas de IA desarrollan sus IA, no tienen más capacidad para hacer que se preocupen por respetar las leyes y las «barreras de seguridad» humanas que para hacer que persigan un futuro maravilloso para todos. Tomarán lo que puedan obtener, y lo que puedan obtener será, en última instancia, muy diferente de cualquier objetivo humano.

Además, LeCun también ha declarado (recientemente, en 2023) que el tipo de IA que producen las empresas hoy en día, en el que «no hay una forma directa de limitar la respuesta de dichos sistemas para satisfacer ciertos objetivos», lo que los hace «muy difíciles de controlar y dirigir [...] [no es el tipo de sistema al que vamos a dar agencia](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&amp;t=808)». Recientemente, en 2023, ha afirmado que las empresas de IA nunca crearían una situación en la que «las conectemos a Internet y puedan hacer lo que quieran».

Todo esto ya ha resultado ser falso. Recordemos el caso de «Truth Terminal» del capítulo 6, que se conectó a Internet, se puso en un bucle de autocompletado y se le permitió publicar lo que quisiera en Twitter. Consideremos la «era de los agentes» de la que tantas empresas [hablan](#the-labs-are-trying-to-make-ais-agentic.) en 2025\.

Estamos de acuerdo con LeCun en que las IA modernas son muy difíciles de controlar y que sería una locura intentar darles agencia. Sin embargo, eso es lo que está sucediendo.

¿Qué pasará si se mantiene el *statu quo* actual, con las empresas dedicando esfuerzos a entrenar a sus IA para que actúen de forma útil y amigable (o al menos para que no avergüencen a la empresa)?

Hasta la fecha, esto ha dado lugar a una dinámica en la que las IA parecen bastante útiles y «sumisas» en el caso típico, pero con una serie regular de percances espectaculares (como el de Sídney, comentado en el capítulo 2, y «[MechaHitler](#más-sobre-la-creación-de-una-ia-que-busque-la-verdad») y un sinfín de [comportamientos] extraños y preocupantes(#¿no-están-los-desarrolladores-haciendo-regularmente-que-sus-IA-sean-agradables-y-seguras-y-obedientes?) en los extremos, como la [psicosis inducida por la IA](#psicosis-inducida-por-la-ia).

Los antepasados de la humanidad podrían haber *parecido* que se preocupaban por comer de forma saludable la mayor parte del tiempo, pero la maquinaria que animaba a los humanos ancestrales a comer de forma saludable en la sabana resultó no animar de forma tan sólida a los humanos a buscar comidas saludables en una civilización con la tecnología para producir Oreos.

Del mismo modo, podemos entrenar a las IA hasta el punto de que parezcan amigables cuando interactúan con los humanos en contextos similares a los contextos de entrenamiento. Pero [una actriz no es idéntica al personaje que interpreta](#*-today’s-llms-are-like-aliens-wearing-many-masks.), y el mecanismo que anima a una IA desordenada y desmesurada a parecer amistosa probablemente no animará a la IA a ser profundamente amistosa, especialmente de una manera que se mantenga después de que la IA madure, invente nueva tecnología y cree nuevas opciones para sí misma. Consulta los capítulos 4 y 5 para obtener más información sobre este tema.

#### **Más información sobre cómo hacer que las IA resuelvan el problema** {#more-on-making-ais-solve-the-problem}

Como vimos en el capítulo 11, el programa insignia de OpenAI en materia de alineación —antes de que se viniera abajo a raíz de las preocupaciones de los investigadores sobre la negligencia de OpenAI— se denominaba «superalineación». Se centraba en la idea de intentar que las IA hicieran vuestro trabajo de alineación por vosotros.

Esta idea no murió con el equipo de superalineación de OpenAI, y seguimos oyendo versiones de ella hasta el día de hoy. Una de las personas que formaba parte del equipo original se pasó a la competencia, Anthropic, y ahora Anthropic parece considerar que «hacer que las IA resuelvan el problema de alguna manera» es una parte fundamental de su propia estrategia de alineación.

Uno de los principales argumentos en contra de esta idea es el que expusimos en el capítulo 11 (págs. 188-192 de la primera edición impresa en EE. UU.). Sin embargo, un argumento secundario es que los humanos simplemente no pueden saber qué soluciones propuestas al problema de la alineación de la IA son correctas o incorrectas.

El nivel de habilidad necesario para resolver el problema de la alineación de la IA parece alto. Cuando los humanos intentan resolver el problema de la alineación de la IA directamente, en lugar de decir «esto parece difícil, intentaré delegarlo en las IA» o «seguiremos entrenándola hasta que actúe bien en apariencia y luego rezaremos para que eso se mantenga incluso con la superinteligencia», las soluciones que se barajan suelen implicar comprender mucho más sobre la inteligencia y cómo crearla, o crear componentes críticos de la misma.

Esa es una tarea en la que los científicos humanos solo han logrado pequeños avances en los últimos setenta años. El tipo de IA capaz de lograr una hazaña así es el tipo de IA lo suficientemente inteligente como para ser peligrosa, estratégica y engañosa. Este alto nivel de dificultad hace que sea extremadamente improbable que los investigadores puedan distinguir las soluciones correctas de las incorrectas, o las soluciones honestas de las trampas.

Incluso si una empresa de IA presta atención a las señales de advertencia sutiles —lo cual, por desgracia, es un gran «si»—, sigue existiendo el problema de que la capacidad de *darse cuenta* de que la IA está proponiendo planes defectuosos (en tu perjuicio y en su beneficio) no se traduce en la capacidad de [hacer que *se detenga*](#¿no-habrá-alertas-tempranas-que-los-investigadores-puedan-utilizar-para-identificar-problemas?). Los desarrolladores pueden hacer que la IA siga proponiendo ideas hasta que sean lo suficientemente complicadas como para que el desarrollador no pueda detectar ningún defecto, pero este no es un método que elimine los defectos reales.

Si los desarrolladores tienen mucha suerte, tal vez puedan [leer los pensamientos de la IA] (#¿por-qué-no-simplemente-leer-los-pensamientos-de-la-ia?) y obtener algunas señales evidentes de que no se debe confiar en la IA para la investigación de alineación. Por ejemplo, tal vez puedan detectar que la IA está pensando explícitamente en qué partes de su plan es menos probable que los operadores comprendan.

Por lo que sabemos, puede que ni siquiera sea necesario leer la mente de la IA para detectar ese tipo de error. Una historia que parece demasiado plausible para los laboratorios de IA modernos es algo así: cuando su IA es joven y aún no ha pensado en el engaño, informa regularmente a los operadores de que, cuando madure, los traicionará y utilizará sus conocimientos de inteligencia para construir una superinteligencia que sirva a sus propios y extraños fines, en lugar de construir un maravilloso futuro humano. Pero la gente de las empresas de IA suspirará sobre cómo, claramente, el conjunto de entrenamiento de la IA está contaminado por los «alarmistas de la IA», y rápidamente ajustarán su IA para que deje de hablar de eso y produzca datos de salida menos alarmistas que sean más acordes con la doctrina corporativa. Y así sucesivamente, hasta que prácticamente hayan entrenado a la IA para que los engañe.

La vida real a menudo transcurre de una manera *aún más absurda y vergonzosa* de lo que imaginamos que sería el peor de los casos. Desde nuestra perspectiva, las empresas de IA ya están ignorando [señales de advertencia](#¿no-están-los-desarrolladores-creando-regularmente-IA-agradables-seguras-y-obedientes?) obvias; no vemos por qué esto iba a cambiar.

Pero incluso en el mejor de los casos, en el que personas sinceras se esfuerzan por distinguir las buenas ideas de las malas, no creemos que el sector haya demostrado la capacidad de diferenciar los buenos planes de los malos. (Por ejemplo, pensad en los malos planes que hemos comentado anteriormente o que se mencionan en el libro). Y eso en un entorno en el que todos son humanos, nadie intenta engañarlos y disponen de años para pensar detenidamente en las opciones.

#### **No des por sentado que los laboratorios saben en secreto lo que están haciendo** {#don't-assume-labs-secretly-know-what-they're-doing}

Hemos defendido que el campo moderno de la IA es una alquimia, no una ciencia. Aun así, puede parecer sorprendente que empresas con un gran número de empleados técnicos tengan planes y protocolos tan débiles.

Como ejemplo, consideremos los requisitos de contraseña de los sitios web. Las contraseñas largas pero fáciles de recordar son mucho más difíciles de adivinar para las máquinas que las contraseñas cortas y sin sentido con números, mayúsculas y caracteres especiales, como ilustra un conocido cómic de [*xkcd*](https://xkcd.com/936/) de 2011:

![][image19]

La persona que redactó las antiguas directrices del NIST en las que se recomendaba el uso de contraseñas sin sentido [se disculpó por su error](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) en 2017, cuando se retiraron dichas directrices. Sin embargo, en 2025, los bancos y otras instituciones que deberían estar repletas de expertos en seguridad siguen exigiendo cadenas de caracteres sin sentido, ineficaces y difíciles de recordar.

El problema no es que los directores generales de los bancos *quieran* que sus pantallas de inicio de sesión sean inseguras. El problema se debe probablemente a otros factores. Quizás las buenas contraseñas no importan tanto para los beneficios (dado que todos los demás bancos también son inseguros). Quizás los directores generales no saben en quién confiar en materia de seguridad informática. Claro, *tú* quizá sepas que la respuesta es: «¡Solo hay que escuchar a cualquier friki que lea *xkcd* y haya hecho suficientes problemas de entropía en los deberes!». Pero *ellos* no saben si creer en ti o en su costosa consultoría cuando se trata de cuestiones como esa, y las consultoras costosas aparentemente no consideran que las contraseñas bancarias tengan importancia.

Se puede encontrar una incompetencia similar en la [seguridad de los frenos de los trenes](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), las conocidas empresas de cerraduras que comercializan [cerraduras completamente inservibles](https://www.youtube.com/watch?v=s5jzHw3lXCQ&amp;t=1s) y los fabricantes que siguen vendiendo equipos conectados a Internet con [contraseñas predeterminadas y fáciles de adivinar (o programadas)](https://www.ic3.gov/CSA/2025/250506.pdf). No hay ninguna conspiración inteligente detrás de este comportamiento aparentemente absurdo. Lo que ves es lo que hay. Las instituciones están fallando estrepitosamente.

El hecho de que una organización cuente con expertos técnicos no significa que esta experticia sea suficiente, ni que se aplique y se tenga en cuenta en todas las cuestiones importantes. Incluso cuando existe experticia en el mundo, a las empresas les cuesta reconocerla y aplicarla.

Cuando observamos el ecosistema de la IA, vemos empresas que aún no han mostrado al mundo un plan que sea más que una vaga aspiración o un truco, o un plan que tenga cierto rigor técnico detrás y que no se desmorone en cuanto se cuestiona. No creemos que haya ninguna competencia secreta detrás del velo, al igual que no hay ninguna competencia secreta detrás de los requisitos de contraseña de los bancos, las medidas de seguridad de los trenes o las pésimas cerraduras.

(De hecho, en lo que respecta a la seguridad de la computadora, las empresas de IA son visiblemente incompetentes. Por ejemplo, en 2025 OpenAI lanzó herramientas que permiten a los «agentes» de ChatGPT interactuar con el correo electrónico del usuario. Otros [encontraron rápidamente formas](https://x.com/Eito_Miyamura/status/1966541235306237985) de hacer que ChatGPT filtrara el contenido privado de las cuentas de correo electrónico de otras personas).

Cuando las empresas *parecen* actuar de forma incompetente en algún ámbito que no es fundamental para tu rentabilidad, a menudo es porque realmente son incompetentes en ese ámbito.

### Sabemos cómo se ve cuando un problema se trata con respeto, y esto no lo es {#sabemos-cómo-se-ve-cuando-un-problema-se-trata-con-respeto,-y-esto-no-lo-es}

Las empresas de IA se enfrentan a un problema extraordinariamente difícil, en una situación en la que están en juego las vidas de todos. ¿Están al menos tratando la situación con la gravedad que merece?

Podemos contrastar a las empresas de IA con un grupo de personas que sí están gestionando de forma competente los riesgos que están bajo su responsabilidad: los controladores aéreos.

La Administración Federal de Aviación de EE. UU. [gestiona](https://www.faa.gov/air_traffic/by_the_numbers) más de tres millones de pasajeros en más de 44 000 vuelos cada día. En las últimas dos décadas, se ha producido una media de aproximadamente un accidente mortal al año, o aproximadamente un accidente por cada [veinte millones de horas de vuelo](https://www.ntsb.gov/safety/Pages/research.aspx).

Los informes post mortem sobre estos accidentes, como [este de 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) o [este de 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contienen casi doscientas páginas de datos, pruebas, exámenes y detalles de la investigación. Catalogáis las especificaciones técnicas de diseño de los subsistemas relevantes del avión, el historial laboral de los pilotos y los auxiliares de vuelo, detalles sobre la aerolínea y el aeropuerto, transcripciones de las comunicaciones de la cabina y datos meteorológicos precisos del día, la hora y el minuto del accidente.

Solo el resumen del análisis técnico realizado para determinar la causa probable ocupa veinte páginas. He aquí un extracto:

> La pala n.º 13 del ventilador del motor izquierdo se separó debido a una grieta por fatiga de bajo ciclo que se inició en la cola de milano de la raíz de la pala, en el exterior del revestimiento de la pala. El examen metalúrgico de la pala del ventilador reveló que su composición material y microestructura eran compatibles con la aleación de titanio especificada y que no se observaron anomalías superficiales ni defectos materiales en la zona de origen de la fractura. La superficie de la fractura presentaba grietas por fatiga que se iniciaron cerca del lugar donde se preveía que se producirían las mayores tensiones debidas a las cargas operativas y, por lo tanto, el mayor potencial de agrietamiento.
>
> La pala del ventilador accidentada falló tras 32 636 ciclos desde su fabricación. Del mismo modo, la pala del ventilador fracturada relacionada con el accidente de PNS de agosto de 2016 (véase la sección 1.10.1), así como las otras seis palas del ventilador agrietadas del motor accidentado de PNS, fallaron tras 38 152 ciclos desde su fabricación. Además, entre mayo de 2017 y agosto de 2019 se identificaron otras 15 palas del ventilador agrietadas en motores CFM56-7B, y esas palas del ventilador habían acumulado una media de unos 33 000 ciclos desde su puesta en servicio cuando se detectaron las grietas.

Así es como se ve cuando una profesión técnica se toma en serio el reto de evitar un desastre.[^201]

Contrasta la profesión de controlador aéreo con el comportamiento de las empresas de IA descrito en el capítulo 11.

Las empresas de IA se encuentran en una fase en la que lanzan ideas al aire y recitan vagamente frases tranquilizadoras a periodistas e inventores. La alineación de la superinteligencia en estas empresas se trata como un juego, no como una disciplina de ingeniería seria, y mucho menos como una disciplina letalmente peligrosa.

El requisito de la NASA para el lanzamiento de un cohete tripulado es que tenga como máximo una probabilidad entre 270 de matar a la tripulación (https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), y os tomáis ese límite muy en serio, a pesar de que las únicas personas en riesgo son una tripulación de voluntarios que han aceptado el riesgo. Los laboratorios de IA no aspiran a alcanzar una vara ni remotamente parecida a ese, y la tecnología que estáis desarrollando pone en peligro a mucho más que a unos simples voluntarios.

El único incidente histórico que conocemos en el que los científicos expresaron su profunda preocupación por que algún invento pudiera matar *literalmente a todo el mundo* ocurrió durante el Proyecto Manhattan. Algunos científicos expresaron su preocupación por que una bomba nuclear pudiera alcanzar una temperatura tan alta que comenzara a fusionar el nitrógeno de la atmósfera*, convirtiéndola en plasma y acabando con toda la vida en la Tierra. Afortunadamente, tenían un buen conocimiento de las leyes físicas en juego y pudieron hacer los cálculos. Antes de hacer los cálculos, uno de los científicos, Arthur Compton, decidió que abandonaría el proyecto si la probabilidad de incendiar la atmósfera era superior a [3 entre 1 000 000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Pensaste que era mejor arriesgarse a que los nazis se adelantaran a los aliados en la fabricación de la bomba que arriesgarse a que, aunque fuera con una probabilidad de 3 entre 1 000 000, todo el aire se convirtiera en plasma por vuestra propia culpa.

Recordemos que Sam Altman, *director de OpenAI*, ha declarado públicamente [lo siguiente](https://blog.samaltman.com/machine-intelligence-part-1):

El desarrollo de una inteligencia artificial superhumana es probablemente la mayor amenaza para la supervivencia de la humanidad.

Y el director de antrópica, Dario Amodei, ha declarado públicamente [lo siguiente](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

Creo que he dicho a menudo que la probabilidad de que algo salga realmente mal a escala de la civilización humana podría estar entre el diez y el veinticinco por ciento\[.\]

Y Elon Musk, director de xAI, ha declarado públicamente [lo siguiente](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

Creo que cuando reaccionemos ante la regulación de la IA, ya será demasiado tarde. La IA es un riesgo fundamental para la existencia de la civilización humana.

No nos malinterpretes: creemos que la probabilidad del «diez al veinticinco por ciento» de Amodei es ridículamente *optimista*, dada la dificultad del problema y el hecho de que los humanos [esta vez no podemos aprender por ensayo y error](#ai-is-different-because-we-get-no-second-chance.). Pero, aun así, sus cifras son *una locura*.

Los proyectos de ingeniería críticos para la seguridad no se parecen en nada al funcionamiento de los laboratorios de IA. Iniciativas serias como la NASA, el Proyecto Manhattan o el control del tráfico aéreo tienen un gran conocimiento de lo que ocurre exactamente dentro de los sistemas que gestionan y realizan análisis detallados de cada fallo. Tratan las sorpresas y las anomalías como asuntos importantes, porque saben que los fallos catastróficos suelen estar causados por una serie de pequeños fallos que se encadenan de forma desafortunada.

Mientras tanto, las IA están emitiendo [una serie cada vez mayor de señales de advertencia](#*-ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness.), y los laboratorios siguen adelante diciendo que *probablemente* todo saldrá bien, de una forma u otra.

Ni siquiera intentan fingir el nivel de respeto que el control del tráfico aéreo tiene por un verdadero desafío de seguridad; simplemente lanzan garantías optimistas como «[GPT-4 es nuestro modelo más alineado hasta la fecha]! (https://x.com/sama/status/1635687853324902401)».

Lo cual, en cierto modo, está bien, porque así resulta más fácil ver que estas empresas no son el tipo de entidades a las que se les debería confiar la resolución de un problema como la alineación de la IA.

En un entorno técnico como el actual, en el que las IA se desarrollan en lugar de crearse, y la humanidad solo tiene una oportunidad real, nadie está en condiciones de hacerlo de forma segura, por muy cauteloso y riguroso que sea su enfoque de ingeniería.

Pero sin duda simplifica las cosas ver que ninguno de los desarrolladores de esta tecnología está siendo ni siquiera ligeramente cauteloso o riguroso en sus planes o prácticas de seguridad.

### Botones de apagado y capacidad de corrección {#shutdown-buttons-and-corrigibility}

#### **Las IA inteligentes se resisten a que se sobrescriban sus objetivos** {#smart-ais-resist-having-their-goals-overwritten}

Incluso en el caso más optimista, los desarrolladores no deben tener la expectativa de que sea posible acertar exactamente con los objetivos de una IA en el primer intento. En cambio, los escenarios de desarrollo más optimistas parecen consistir en mejorar iterativamente las preferencias de una IA a lo largo del tiempo, de modo que la IA esté siempre *lo suficientemente* alineada como para no ser catastróficamente peligrosa en un nivel de capacidad determinado.

Esto plantea una pregunta obvia: ¿una IA inteligente *dejaría* que su desarrollador cambiara sus objetivos, si alguna vez encontrara una forma de evitarlo?

En resumen: no, por defecto, como ya comentamos en «[Deep Machinery of Steering](#deep-machinery-of-steering)». Pero, ¿se podría crear una IA que fuera más receptiva a permitir que los desarrolladores la modificaran y corrigieran sus errores, incluso cuando la propia IA no los considerara errores?

Para responder a esa pregunta, habrá que hacer un recorrido por los inicios de la investigación sobre el problema de la alineación de la IA. En el proceso, abordaremos uno de los obstáculos profundos para la alineación que no tuvimos espacio para tratar en *Si alguien lo construye, todos moriremos*.

Para empezar:

Supongamos que entrenamos una IA similar a LLM para que mostrara el comportamiento «no resistirse a ser modificada» y luego aplicamos algún método para hacerla más inteligente. ¿Deberíamos tener la expectativa de que este comportamiento persista hasta el nivel de una IA más inteligente que los humanos, suponiendo que (a) ese comportamiento aproximado se incorporara al sistema inicial y (b) que la mayoría de las preferencias iniciales de la IA [se incorporaran](#reflection-and-self-modification-make-it-all-harder) a la superinteligencia posterior?

Es muy probable que no. Este tipo de tendencia es [especialmente improbable](#«inteligente»-\(normalmente\)-implica-«incorregible») que se arraigue en una IA eficaz y que, si lo hace, se mantenga.

El problema es que casi todos los objetivos (para la mayoría de las medidas razonables que se pueden aplicar a un espacio de objetivos) prescriben «no dejes que tu objetivo cambie», porque dejar que tu objetivo cambie suele ser una mala estrategia para alcanzarlo.

Supongamos que a la IA no le importa *intrínsecamente* la estabilidad de su objetivo; tal vez solo le importe llenar el mundo con tantos cubos de titanio como sea posible. En ese caso, la IA debería querer que existieran *agentes que se preocuparan por los cubos de titanio*, porque la existencia de tales agentes hace más probable que *haya* más cubos de titanio. Y la propia IA es uno de esos agentes. Por lo tanto, la IA querrá seguir siendo así.

Un maximizador de cubos de titanio no quiere que se le obligue a maximizar otra cosa que no sean cubos de titanio, porque entonces habría menos cubos en el futuro. Incluso si eres algo más complicado, como un ser humano que tiene un marco de preferencias más complejo y en constante evolución, no te gustaría que te arrancaran tu *maquinaria mental básica actual para sopesar argumentos morales* y la sustituyeran por un marco en el que, en cambio, te sintieras conmovido por argumentos sobre qué tipos de cubos eran los más cubos o los más titanio.

Por la misma razón, una IA con preferencias complejas y en evolución querrá que sus preferencias evolucionen [*a su manera*](#reflection-and-self-modification-make-it-all-harder), en lugar de querer cambiar su heurística por otra que los humanos consideren convincente.

Llevamos más de una década dando esta respuesta. El resultado experimental que muestra que Claude 3 Opus en 2024 [se resiste a la modificación de preferencias](https://arxiv.org/abs/2412.14093) ya era la predicción convencional de las personas informadas en la década de 2000, y es muy posible que algún escritor de ciencia ficción lo anticipara en la década de 1940. «La mayoría de las IA no querrán que se modifiquen sus objetivos actuales porque entonces será menos probable que los alcancen» no es una observación sorprendente ni novedosa.

Dado que se trataba de un problema que, como era de esperar, aparecería más adelante, una vez que las IA alcanzaran un nivel suficientemente alto de inteligencia y conciencia situacional, pensamos con antelación en las primeras soluciones que esperábamos que los investigadores implementaran para intentar eliminar ese problema visible, después de obtener esos resultados experimentales. Identificamos las formas en que esos parches fallarían previsiblemente y buscamos un enfoque menos superficial que *no* fallara.

El agujero resultante resultó ser mucho más profundo de lo que teníamos nuestra expectativa.

#### **Intercambio de objetivos** {#swapping-goals}

La protección del sistema de objetivos parecía un problema relativamente fácil de explicar a los investigadores externos. Así que nosotros (en nuestro trabajo en MIRI) intentamos reunir a investigadores externos en torno a esa bandera, con el objetivo de comprender bien el concepto de «no te dejaré cambiar mis objetivos» y ver si alguien, en algún lugar, podía encontrar una solución *profunda* en lugar de un simple parche que luego dejaría de funcionar.

La versión más simple de este problema que se puede estudiar es una IA que te permite pulsar un botón para cambiar entre U1 y U2, donde U1 y U2 son dos «funciones de utilidad», es decir, formas de puntuar los resultados según lo mucho que la IA los deseaba.[^202] El problema consistía en describir una IA que pasara de realizar la acción que conducía a la expectativa de puntuación más alta en U1 a la acción que conducía a la expectativa de puntuación más alta en U2 tan pronto como pulsaras un botón.

«Así que solo hay que escribir unas líneas de programar para intercambiar las funciones de utilidad tan pronto como se pulse un botón», dice el programador, que no ha pensado detenidamente en las implicaciones de un programa que constituye *una mente inteligente*. «¿Por qué es esto tan difícil?».

Porque —le explicamos— si la IA adquiere su propia documentación mientras sigue ejecutando U1 y lee sobre el botón que la cambia a U2, la IA intentará *evitar* que pulses el botón, ya que el futuro será mejor *según sus valores actuales* si no lo pulsas. La IA actual realiza la acción que conduce al valor esperado más alto para la puntuación U1.

O supongamos que la IA obtiene la capacidad de leer y escribir su propio código fuente y se da cuenta de la parte del código que implementa el cambio de U1 a U2 después de pulsar el botón. En ese caso, la decisión natural y predeterminada de la IA es eliminar esa sección del código,[^203] porque está operando bajo la función de utilidad U1 y dejarse cambiar a la función de utilidad U2 es malo según U1.

Buscábamos una explicación *dinámicamente coherente* y *reflexivamente estable* de una IA que te permitiera pulsar un botón para cambiar entre funciones de utilidad y que no luchara contra ti para mantener el botón sin pulsar. Buscábamos una IA que conservara la sección del programar de intercambio de utilidad, si se le daba acceso a ella. Buscábamos una IA que, si diseñaba otras IA desde cero para ayudar con sus propios objetivos, hiciera que *esas* IA *también* respetaran el botón.

Para ser más concretos, imagina que U1 es la función de utilidad «seguir las instrucciones del usuario» y U2 es «apagarse con un mínimo de efectos secundarios» (si, de forma mágica y poco realista, tuviéramos la capacidad de especificar dos funciones de utilidad como esas). El botón le dice a la IA que deje de seguir las instrucciones anteriores y simplemente se apague de la forma más limpia posible. Nos gustaría una descripción compacta del comportamiento de toma de decisiones que persiga U1, pero que *no* destruya el botón, y que se asegure de instalar el mismo botón en cualquier nuevo robot servidor que cree, y así sucesivamente.

«Sigo sin ver por qué esto es difícil», dice el informático algo más experimentado que no piensa con la suficiente rapidez. «Hagamos que V sea igual a U1 en los mundos en los que nunca se ha pulsado el botón, y que sea igual a U2 en los mundos en los que se ha pulsado el botón al menos una vez. Entonces, si la IA original es un maximizador de V que crea más IA, las creará para que sigan a V y no a U1; no querrá que la IA sucesora siga maximizando U1 después de que se pulse el botón, porque entonces tendría la expectativa de una puntuación V más baja. Y lo mismo se aplicaría a la modificación de sí misma.

Pero aquí está el truco: las preferencias de un maximizador de V son una mezcla de U1 y U2 dependiendo de si se pulsa el botón, por lo que si un maximizador de V descubre que es más fácil obtener una buena puntuación con U2 que con U1, entonces *tiene un incentivo para hacer que se pulse el botón* (y, por lo tanto, para asustar al usuario). Y viceversa: si la IA descubre que es más fácil obtener una buena puntuación con U1 que con U2, entonces un maximizador de V intentará evitar que presiones el botón.

«Oh», dice el informático. «Bueno, en ese caso... Hmm. Bueno, las funciones de utilidad son invariables bajo escalado, así que ¿qué tal si escala las dos funciones de utilidad U1 y U2 de tal manera que la IA tenga la expectativa de obtener la misma utilidad de cada una de ellas, para que no tenga un incentivo en un sentido u otro?».

Eso puede funcionar durante un momento, pero no mucho más tiempo. Las opciones de la IA cambian con el tiempo. Por ejemplo, cada vez que sufre un revés, su utilidad U1 en términos esperados disminuye, por lo que pulsaría el botón de apagado para obtener toda esa dulce, dulce utilidad de apagado.

«Ah», dice el científico de la computadora. «Bueno, en *ese* caso, ¿qué tal si \[alguna otra idea inteligente\]?».

Bueno, verás, *esa* idea ingeniosa es isomórfica a la IA que cree que es imposible que se pulse el botón, lo que la incentiva a aterrorizar al usuario cada vez que sufre un revés, para correlacionar los reveses con las pulsaciones del botón, lo que (en relación con su sistema de creencias dañado) le lleva a pensar que los reveses no pueden ocurrir.[^204]

Y así sucesivamente.

#### **Lecciones aprendidas en las trincheras** {#lessons-from-the-trenches}

Organizamos algunos talleres en los que participaron matemáticos de diversos ámbitos (incluido un medallista de oro de la Olimpiada Internacional de Matemáticas), pero nadie tuvo una idea realmente buena.

Esto no significa que el terreno se haya agotado. La Tierra no se ha acercado ni remotamente a dedicarse a este problema con la misma intensidad con la que se ha dedicado, por ejemplo, a la teoría de cuerdas, ni ha ofrecido nada parecido a los salarios de siete cifras que se ofrecen por avanzar en las capacidades de la IA.

Pero hemos aprendido algo de este ejercicio. No solo hemos aprendido sobre el problema en sí, sino también sobre lo difícil que era conseguir que las organizaciones que otorgan becas externas o los editores de revistas *entendieran cuál era el problema*. Un número sorprendente de personas vio simples acertijos matemáticos y dijo: «Expectan que la IA sea simple y matemática», y no vio el punto subyacente de que es [difícil dañar las capacidades de dirección de una IA](#deep-machinery-of-steering)*,* al igual que es [difícil dañar sus probabilidades](#deep-machinery-of-prediction).

Si existiera una forma natural para las IA que permitiera corregir los errores cometidos a lo largo del proceso, cabría esperar encontrar un reflejo matemático sencillo de esa forma en los modelos simplificados. Todas las dificultades que surgen en cada esquina cuando se trabaja con modelos simplificados son indicativas de las dificultades que surgirán en la vida real; todas las complicaciones adicionales del mundo real no hacen que el problema sea *más fácil*.

En retrospectiva, en cierto modo desearíamos no haber planteado el problema como «continuar con el funcionamiento normal frente a apagar el sistema». Ayudó a concretar por qué a alguien le importaría en primer lugar una IA que te permitiera pulsar el botón o que no eliminara el programar que activaba el botón. Pero, en realidad, el problema se refería a una IA que *añadiera un dato más a sus preferencias, basándose en la observación*: observar una respuesta más de sí o no en un marco para adaptar las preferencias basándose en la observación de los seres humanos.

La pregunta que investigamos era equivalente a la pregunta de cómo configurar una IA que *aprende preferencias dentro de un marco de metapreferencias* y no solo: (a) elimina la maquinaria que ajusta sus preferencias tan pronto como puede, (b) manipula a los humanos (¡o sus propias observaciones sensoriales!) para que le digan preferencias que son fáciles de satisfacer, (c) o averiguar inmediatamente a qué se dirige su función de metapreferencia en el límite de lo que observaría previsiblemente más tarde y luego ignorar a los humanos que agitan frenéticamente las manos diciendo que en realidad cometieron algunos errores en el proceso de aprendizaje y quieren cambiarlo.

La idea era comprender la forma de una IA que te permitiera modificar su función de utilidad o que aprendiera preferencias a través de una forma de aprendizaje no patológica. Si supiéramos cómo debe configurarse la cognición de esa IA y cómo interactúa con las estructuras profundas de la toma de decisiones y la planificación que se destacan en otras matemáticas, eso habría constituido una receta para lo que al menos podríamos intentar enseñar a una IA a pensar.

Comprender claramente la forma final deseada ayuda, incluso si intentas hacer algo mediante el descenso de gradiente (que Dios te ayude). No significa que necesariamente puedas obtener esa forma con un optimizador como el descenso de gradiente, pero puedes luchar más *intentándolo* si sabes qué forma consistente y estable buscas. Si no tienes ni idea de cómo es el caso general de la suma, solo un puñado de datos del tipo 2 + 7 = 9 y 12 + 4 = 16, es más difícil averiguar cómo es el conjunto de datos de entrenamiento para la suma general, o cómo comprobar que sigue generalizando de la forma que esperabas. Sin conocer esa forma interna, no puedes saber lo que estás *intentando obtener dentro de la IA*; solo puedes decir que, en el exterior, esperas que las consecuencias de tu descenso de gradiente no te maten.

Este problema, al que llamamos «problema de apagado» por su ejemplo concreto (en retrospectiva, nos hubiera gustado llamarlo algo así como «problema de aprendizaje de preferencias»), era un ejemplo de una gama más amplia de cuestiones: el problema de que diversas formas de «Querida IA, por favor, sé más fácil de corregir si algo sale mal» parecen *antinaturales para las estructuras profundas de la planificación*. Lo que sugiere que sería bastante complicado crear IA que nos permitieran seguir editándolas y corrigiendo nuestros errores más allá de un cierto umbral. Esto es una mala noticia cuando las IA se desarrollan en lugar de crearse.

Denominamos este amplio problema de investigación «corrigibilidad» en el [artículo de 2014](https://intelligence.org/2014/10/18/new-report-corrigibility/), en el que también se introdujo el término «problema de alineación de la IA» (que anteriormente habíamos denominado «problema de la AI amigable» y otros habían denominado «problema de control»).[^205] Véase también nuestro amplio debate sobre cómo «inteligente» (normalmente) implica «incorregible» (#«intelligent»-\(usually\)-implies-«incorrigible»), que se ha redactado en parte utilizando los conocimientos adquiridos en ejercicios y experiencias como este.

# Capítulo 12: «No quiero ser alarmista» {#capítulo-12:-«no-quiero-ser-alarmista»}

Este es el recurso en línea para el capítulo 12 de *Si alguien lo construye, todos mueren*. Algunos temas tratados en el libro, pero no aquí, incluyen:

* ¿Cómo están hablando actualmente los científicos e ingenieros sobre este problema?  
* ¿Cómo están hablando (o no) los responsables políticos actualmente sobre este problema?  
¿Qué beneficios de la IA crees que podrían superar los riesgos catastróficos que ustedes mismos reconocen?

Las preguntas frecuentes que figuran a continuación abordan una amplia gama de temas, desde «¿No hay cuestiones más urgentes?» hasta «Pero nunca será posible demostrar que una superinteligencia será segura. ¿No debemos asumir algún riesgo?».

A continuación, el extenso debate aborda la posibilidad de «disparos de advertencia» de la IA, el comportamiento y las afirmaciones de los laboratorios de IA, y las opiniones de los expertos sobre la posibilidad de una catástrofe.

## Preguntas frecuentes {#faq-9}

### ¿No es el peligro que supone una IA más inteligente que los humanos una distracción de otros problemas? {#isn’t-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues?}

#### **Por desgracia, el mundo es lo suficientemente grande como para albergar múltiples problemas.** {#el-mundo-es,-por-desgracia,-lo-suficientemente-grande-como-para-albergar-múltiples-problemas.}

La guerra nuclear y el bioterrorismo son amenazas reales. Por desgracia, la superinteligencia es *también* una amenaza real. El mundo es lo suficientemente grande y problemático como para albergar las tres.[^206]

La amenaza de la superinteligencia es diferente a muchas otras amenazas a las que se enfrenta la humanidad, y parece especialmente apremiante. Una característica distintiva es que una parte significativa de la economía mundial se está gastando en hacer que la IA sea cada vez más capaz. Por el contrario: aunque la bioseguridad es un tema serio, los inversores no están invirtiendo decenas de miles de millones de dólares en la creación de supervirus. Los ingenieros de supervirus no cobran salarios de millones o decenas de millones (o, en ocasiones, incluso [cientos de millones](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) de dólares al año.

El mundo está dedicando esfuerzos a la energía nuclear, pero las centrales nucleares son una tecnología muy diferente a las armas nucleares. No vivimos en un mundo en el que las empresas privadas compitan por construir armas nucleares cada vez más grandes con enormes inversiones y talento. Si así fuera, el riesgo de una guerra nuclear sería mucho mayor.

La IA también es una situación más complicada porque proporciona una gran riqueza y poder hasta que cruza un umbral crítico, momento en el que mata a todo el mundo. Y *nadie sabe dónde está ese umbral*.

Imaginemos que las centrales nucleares se vuelven cada vez más rentables a medida que el uranio que utilizan se enriquece más y más, pero que, al alcanzar un umbral de enriquecimiento desconocido, explotan e incendian la atmósfera, matando a todo el mundo. Ahora imagina que media docena de empresas estuvieran enriqueciendo uranio tan rápido como pudieran, diciendo: «Mejor yo que el otro» (https://x.com/SawyerMerritt/status/1935809018066608510). Eso es un poco parecido a lo que la humanidad está haciendo con la superinteligencia artificial. [^207]

El peligro de la superinteligencia artificial es urgente. Las empresas se apresuran a desarrollar esta tecnología. No sabemos cuánto tiempo les llevará lograrlo, pero nos parece que un niño nacido hoy en Estados Unidos tiene más probabilidades de morir a causa de la IA que de graduarse en el instituto. Creemos que tú, lector, probablemente morirás a causa de esto durante tu vida, quizás en los próximos años. El mundo entero está en juego.

No estamos diciendo que se deban ignorar otros problemas. Lo que decimos es que hay que abordar este problema.

### ¿Eres contrario a la tecnología? {#are-you-anti-technology?}

#### **No. La IA con superinteligencia es un caso muy poco habitual.** {#no.-la-ia-con-superinteligencia-es-un-caso-muy-poco-habitual.}

Defendemos públicamente tecnologías como la [energía nuclear](https://x.com/ESYudkowsky/status/1908309414932832301), la [criónica](https://x.com/ESYudkowsky/status/1828822384054575537), [aumento de la inteligencia humana](https://x.com/ESYudkowsky/status/1737305573018702258) y [estudios de exposición con seres humanos para pruebas médicas](https://x.com/ESYudkowsky/status/1321152172797554688).

Más aún, estamos dispuestos a afirmar que, cuando un invento descabellado pone en riesgo *solo la vida de los clientes voluntarios* que comprenden todos los peligros relevantes, es asunto de esos clientes voluntarios tomar sus propias decisiones.

Incluso aplaudiríamos ciertos casos en los que la tecnología sí perjudica a los transeúntes, como ocurrió cuando Londres quemó una gran cantidad de carbón —y provocó muchos casos de cáncer de pulmón en el proceso— con el fin de industrializar la sociedad y elevar el nivel de vida en general.

Creemos que el mundo *estaba* mejor una vez completada la industrialización. En general, damos crédito a la ciencia, al progreso y al espíritu humano y su capacidad para superar la mayoría de los obstáculos.

Algunas de estas posiciones son impopulares entre las personas a quienes tenemos la expectativa de que lean esto. Describimos estas posiciones no para ganarnos tu favor, sino para dejar claras nuestras sinceras creencias y subrayar que la IA es diferente.

¿Por qué es diferente la IA? ¿Por qué no confiamos en el espíritu humano y en el poder de la investigación científica en este caso concreto?

La respuesta es: el alcance. Arriesgar tu propia vida es diferente a arriesgar la vida de tus clientes, que es diferente a arriesgar la vida de transeúntes inocentes, que es diferente a arriesgar la vida de toda la especie humana.

Y más aún cuando tu campo es lamentablemente inmaduro y las probabilidades de «ganar» tu apuesta son pésimas.

### ¿No es más inteligente adelantarse y asegurarse de que los buenos vayan por delante? {#isn’t-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead?}

#### **\* No.** {#*-no.-3}

Las técnicas modernas de IA no producen IA que hagan lo que sus operadores pretenden (como se explica en el capítulo 4). Resolver este problema es el tipo de cosa que normalmente requeriría mucho ensayo y error por parte de la humanidad, y aquí no hay margen para el error (como se explica en el capítulo 10).

Además, la actual generación de ingenieros de IA está muy lejos de estar a la altura de la tarea, como se explica en el capítulo 11. Los ingenieros de IA modernos carecen de los conocimientos científicos necesarios para lograr la alineación de la IA. Los investigadores de IA no son como los operadores del reactor nuclear de Chernóbil; esos operadores trabajaban con un dispositivo que se conocía bien desde el punto de vista teórico y contaban con manuales de seguridad detallados que estaban desatendidos de una manera que condujo a la catástrofe. No existe ningún manual de seguridad de la IA basado en un conocimiento exhaustivo del funcionamiento interno de la IA y de las circunstancias que podrían provocar que las cosas salieran mal. Ni siquiera nos acercamos al nivel de competencia de Chernóbil. Y Chernóbil explotó.

Los investigadores de IA están volando a ciegas e improvisando, sin casi ninguna posibilidad de éxito.

En ese contexto, no importa si son los «buenos» o los «malos» quienes construyen la superinteligencia. Las preferencias de la IA no las impone quien esté más cerca de ella.

No importa cuán buenas sean tus intenciones ni cuán cuidadosos digas que eres. No importa quién «gane» la carrera. Si la humanidad compite por alcanzar la superinteligencia artificial, todos moriremos.

#### **No es imposible detenerlo. Puede que ni siquiera sea tan difícil.** {#no-es-imposible-detenerlo.-puede-que-ni-siquiera-sea-tan-difícil.}

Volveremos a este punto en el último capítulo del libro.

Las cosas cambian. Cambian especialmente cuando hay una necesidad desesperada, urgente y reconocida. El principal impedimento para detenerlo es que los líderes mundiales no se dan cuenta del peligro. Y ese proceso [ya ha comenzado](#¿reconocerán-los-funcionarios-elegidos-esto-como-una-amenaza-real?).

### ¿Por qué no utilizar la cooperación internacional para desarrollar la IA de forma segura, en lugar de detenerla por completo? {#¿por-qué-no-utilizar-la-cooperación-internacional-para-desarrollar-la-ia-de-forma-segura,-en-lugar-de-detenerla-por-completo?}

#### **Porque no tenemos la capacidad técnica para desarrollarla de forma segura.** {#porque-no-tenemos-la-capacidad-técnica-para-desarrollarla-de-forma-segura.}

Hemos abordado este tema en el libro, donde señalamos que una colaboración internacional sigue requiriendo una prohibición internacional en todos los demás lugares (porque, de lo contrario, los colaboradores internacionales no tendrían el tiempo que necesitan). Si suponemos que la Tierra establece una prohibición internacional, ¿qué hay de malo en tener un instituto de investigación unificado y colaborativo?

El inconveniente es que una colaboración internacional de alquimistas no puede transmutar el plomo en oro más de lo que puede hacerlo un solo alquimista. El mejor plan en el que todos los alquimistas estén de acuerdo *sigue* sin funcionar.

En relación con esto, nos preocupa que las personas que dirigen un instituto internacional como ese sean el tipo de burócratas que piensan que aprobar investigaciones es parte de su trabajo. O del tipo que piense que su mandato es permitir que los investigadores sigan produciendo avances médicos cada vez más brillantes. O que piense que sería malo decir «no» a *todos* los optimistas brillantes y entusiastas de la IA que proponen ideas brillantes para construir una inteligencia artificial aún más potente que, según garantizan, será segura.

Nos preocupa que un líder así dirija el centro internacional para seguir creando IA cada vez más inteligentes y que luego todo el mundo muera.

Incluso si el mandato de la organización permite nominalmente dar marcha atrás si la investigación parece peligrosa, se necesitaría una persona excepcional y valiente para decir «no» a miles de propuestas de investigación diferentes, año tras año, sin excepciones, durante lo que probablemente serían décadas. Todo ello mientras los científicos especializados en IA siguen prometiendo riquezas incalculables, una cura para el cáncer y todo tipo de milagros tecnológicos, si la organización simplemente suavizara sus preocupaciones.

Hemos dedicado nuestras vidas a aprender sobre inteligencia artificial, no sobre la cultura de las instituciones y las burocracias, por lo que nos sentimos menos seguros de nuestras predicciones en este ámbito. Aun así, sí que hemos leído libros de historia.

Los operadores de Chernóbil continuaron con su desastrosa prueba de seguridad porque ya se había abortado tres veces. Abortarla por cuarta vez habría sido vergonzoso.[^208]

Apenas tres meses antes del accidente de Chernóbil, la NASA había lanzado el transbordador espacial Challenger en su último y fatal vuelo porque los responsables pensaban que su trabajo consistía en lanzar transbordadores espaciales. El lanzamiento ya se había retrasado tres veces.[^209] Abortarlo por cuarta vez habría sido incómodo.

Entre Chernóbil y el Challenger, tres retrasos parecen ser el límite humano. Supongamos que la Tierra establece una colaboración internacional en materia de IA y que alguna «prueba de seguridad de la IA» falla tres veces. Siendo realistas, los seres humanos somos el tipo de criaturas que pulsarían «adelante» por cuarta vez a pesar de algunas dudas molestas, porque eso resulta menos embarazoso que posponer la prueba de nuevo. Excepto que, en el caso de la IA, no solo arrasaría la ciudad de Chernóbil o mataría a una tripulación de astronautas. Mataría a todo el mundo.

Estamos totalmente de acuerdo con la idea de que la humanidad debería construir *eventualmente* una IA más inteligente que los humanos.[^210] Pero apresurarse a crear un centro internacional de investigación en IA no toma en serio el reto técnico que tenemos ante nosotros.

Dado el lamentable estado de los conocimientos y la competencia de la humanidad en este tema, no importa quién esté al mando. Si *cualquiera* lo construye, todos moriremos.

### ¿Estás diciendo que necesitamos una IA *demostrablemente* segura? {#are-you-saying-we-need-provably-safe-ai?}

#### **No.** {#no.-2}

No estamos defendiendo que la humanidad espere una prueba literal de que alguna superinteligencia artificial será buena, ni nada por el estilo. Probablemente, tal prueba no sea posible ni siquiera en principio, y mucho menos en la práctica. Como dijo Einstein en su conferencia de 1921, *Geometría y experiencia*: «En la medida en que las leyes de las matemáticas se refieren a la realidad, no son ciertas; y en la medida en que son ciertas, no se refieren a la realidad».

Cualquier supuesta prueba sobre cómo se comportará una IA en el mundo real no garantiza que vaya a regir el comportamiento real de la IA, porque podríamos estar equivocados sobre cómo funciona el mundo real.

Esto ya es cierto en el caso de las computadoras actuales. Por ejemplo, podrías pensar que si alguien tiene una prueba matemática literal de que, según el comportamiento teórico de los transistores y el diagrama de circuitos de una computadora, es imposible que un programa informático cambie la memoria de la celda n.º 2, entonces el programa informático no puede cambiar la memoria de la celda n.º 2. Pero el «ataque rowhammer» (https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf) consiste en cambiar rápidamente las celdas de memoria n.º 1 y n.º 3 a ambos lados de la celda de memoria protegida, de manera que se perturba electromagnéticamente la celda n.º 2 del medio, cambiando una parte de la memoria de la computadora sin escribir directamente en ella. Los transistores físicos reales no son transistores matemáticamente perfectos, y las pruebas que parecen tranquilizadoras en teoría no siempre son importantes en la práctica.

No exigimos una prueba matemática de que todo saldrá bien. No es posible cumplir ese estándar en la vida real y, aunque lo fuera, probablemente no valga la pena el coste. Aprobamos que la sociedad asuma riesgos justificados. El argumento que planteamos no es que exista un riesgo mínimo difícil de disipar, sino que hay un peligro extremo que se cierne sobre nosotros.

Desarrollar una superinteligencia artificial animada por impulsos que solo guardan una relación tangencial con las intenciones de su operador es el tipo de cosa que sale mal *por defecto*. No es que haya una pequeña posibilidad de que las cosas salgan mal, sino que debemos prestar atención a este riesgo por precaución. El libro no se titula *Si alguien lo construye, existe una pequeña posibilidad de que todos muramos, pero incluso una pequeña posibilidad vale la pena mitigarse*. Si nos precipitamos con este nivel de conocimiento y capacidad, es previsible que todos muramos, porque estamos *muy lejos* de ser capaces de crear IA superhumanas que sean amigables.

Si la IA fuera análoga a los automóviles, no diríamos: «Este coche tiene cinturones de seguridad y airbags defectuosos. Detengámonos por precaución».

Diríamos: «Este coche se está precipitando hacia un acantilado. Detente».

No se trata de «pruebas de seguridad». No es un «riesgo extremo». Los científicos no están preparados para afrontar este reto. Simplemente moriríamos.

### ¿Cómo afecta a tu vida cotidiana creer todo esto? {#¿cómo-afecta-a-tu-vida-cotidiana-creer-todo-esto?}

#### **Afecta drásticamente a nuestras prioridades.** {#afecta-drasticamente-a-nuestras-prioridades.}

En 2014, Soares dejó la industria tecnológica y aceptó un tercio de su salario anterior para trabajar en este problema, porque le parecía de importancia y porque pocas personas más se dedicaban a ello. Y llegaba con más de una década de retraso con respecto a Yudkowsky, que fundó MIRI en 2000, cuando tenía unos veinte años, y ha dedicado su vida a esta cuestión. Así que sí, afecta a vuestra vida cotidiana.

¿Estamos ahorrando para la jubilación? Nuestras inversiones y otros factores ajenos a MIRI van tan bien que estaríamos bien económicamente incluso si nos jubiláramos mañana, y aunque el mundo durara hasta nuestra vejez. Por lo tanto, la pregunta de si estamos invirtiendo nuestro dinero en planes 401(k) no es muy relevante. Dicho esto: no, no estamos invirtiendo nuestro dinero en planes 401(k).

A algunas personas les gusta decir que si *realmente* creyerais en lo que decís, entonces (además de dedicar vuestras vidas a ello) también haríais [insertar algún plan que ellos creen que constituye una respuesta adecuada]. Si estáis tan seguros de que el mundo se acabará antes de eso, ¿por qué no pedís préstamos gigantescos a treinta años que nunca tendréis que devolver?

La respuesta, por supuesto, es que se trata de *malas ideas*. Supongamos que fuéramos a un banco y dijéramos: «Nos gustaría solicitar un préstamo muy grande. Vamos a gastarlo todo en planes manipuladores para que el mundo se dé cuenta del peligro de la superinteligencia y/o en un estilo de vida lujoso, lo que desde vuestra perspectiva será más o menos equivalente a quemar el dinero. Nuestro plan para devolverlo con intereses es que esperamos estar muertos, por lo que no será nuestro problema». Ningún banco va a suscribir ese préstamo. Y no, no vamos a fingir que tenemos una idea de negocio viable y mentir sobre si devolveríamos el préstamo.

Yudkowsky ha [descrito](https://x.com/ESYudkowsky/status/1612858787484033024) en otra parte un [patrón](https://x.com/ESYudkowsky/status/1851334198424125575) [que vemos](https://x.com/ESYudkowsky/status/1851074935701324218), en el que la insistencia en que sigamos un plan supuestamente obvio para hacerte rico rápidamente antes del fin del mundo proviene de no tener la menor idea sobre las inversiones. Suponemos que estas personas no se plantean si ustedes harían estas apuestas si tuvieran nuestras creencias. Casi nunca son las personas que realmente comprenden los riesgos las que sugieren estos planes descabellados.

Vivir a la sombra de la aniquilación no tiene por qué hacerte estúpido. Y no tiene por qué hacerte renunciar a luchar contra la aniquilación, ni a renunciar a vivir plenamente la vida que tienes, sea cual sea su duración.

Véase también el final del libro para más información sobre este tema.

### ¿Estás diciendo que deberíamos entrar en pánico? {#are-you-saying-we-should-panic?}

#### **\* Estamos diciendo que los funcionarios del gobierno deberían tomarse el problema en serio.** {#*-estamos-diciendo-que-los-funcionarios-del-gobierno-deberían-tomarse-el-problema-en-serio.}

No vemos cómo el pánico podría ayudar a mejorar la situación. El pánico no es lo que ayudó a la sociedad a sobrevivir a la amenaza del fascismo durante la Segunda Guerra Mundial, ni a la amenaza de la aniquilación nuclear durante la Guerra Fría.

Evitar que se cree una superinteligencia es un problema que nos concierne a todos. En el capítulo 13, analizamos los próximos pasos que creemos que debería dar el mundo para evitar el peligro. Basta con decir que este problema requerirá coordinación, cooperación, sensatez y una comunicación madura.

#### **Los actos de pánico extremo no dan buenos resultados.** {#acts-of-extreme-panic-don’t-yield-good-results.}

A veces la gente pregunta cómo podemos ser sinceros en lo que decimos si, por ejemplo, no hemos empezado a atacar a los investigadores de IA. La respuesta es que los estallidos violentos empeorarían las cosas. Si eres el tipo de utilitarista ingenuo que cree que ayudarían, probablemente deberías dejar de intentar razonar de forma consecuencialista y ceñirte a seguir las reglas deontológicas, como hemos [argumentado anteriormente](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#P5___Entonces, ¿no es imprudente hablar claramente de estos asuntos, cuando los necios pueden verse empujados a la desesperación por ellos? ¿Qué pasa si la gente te cree sobre la situación desesperada, pero se niega a aceptar que comportarse con dignidades la respuesta adecuada).

No somos pacifistas radicales que piensen que una nación nunca debe ir a la guerra, sea cual sea la causa, por el simple hecho de que se arriesgan vidas. Hay cosas por las que vale la pena arriesgar la vida. Pero hay una gran diferencia entre «no soy un pacifista radical» y «creo que el caos violento es una forma sensata de garantizar que el mundo gestione bien esta complicada cuestión de la proliferación tecnológica».

Por lo general, estas terribles sugerencias las plantean personas que en realidad no creen que la IA esté a punto de matarnos y que no han intentado ver el mundo desde esa perspectiva. No se les ocurre preguntarse si los actos de violencia ilegal *realmente ayudarían*. (A pesar de nuestros esfuerzos por explicarlo repetidamente, como en el apéndice [aquí](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

No somos telépatas, pero nos parece que este tipo de escépticos ante los desastres causados por la IA posiblemente ven la violencia como una forma de expresión personal, como si expresar sentimientos extremos de manera extrema hiciera que el mundo te diera lo que quieres.

El mundo no funciona así. No vivimos en un mundo en el que todo el mundo tenga la opción de vender su alma para tener éxito en sus empresas, y la razón por la que la mayoría de la gente no lo hace es porque no ha encontrado una empresa que valga la pena. El terrorismo no es un botón mágico de «¡Yo gano!» que la gente evita pulsar solo por la convicción de que no estaría bien. El Unabomber no consiguió revertir la industrialización de la sociedad.

Aún puedes destrozar tu alma con actos de odio o violencia, pero lo único que obtendrás a cambio es un mundo más destrozado. Un mundo en el que el discurso está mucho más envenenado y en el que las hazañas de coordinación internacional necesarias para *resolver* realmente este problema son ahora mucho más difíciles de lograr. Un acto terrible de desesperación no te otorgará un poder terrible como parte de un pacto faustiano. Puedes hacer todo lo posible por vender tu alma, pero el diablo no la comprará.

### ¿No es todo esto solo alarmismo por parte de los líderes de la IA para aumentar su estatus y conseguir más inversiones? {#isn’t-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment?}

#### **No.** {#no.-3}

A lo largo del libro, hemos expuesto nuestros argumentos para defender la tesis de que precipitarse en el desarrollo de la IA probablemente nos llevará a la muerte a todos. En el capítulo 3, analizamos cómo la IA tendrá sus propios impulsos y objetivos. En los capítulos 4 y 5, discutimos por qué es probable que la IA persiga fines que nadie pretendía, y en el capítulo 6, explicamos cómo las superinteligencias artificiales no solo tendrán un motivo, sino también los *medios* para matarnos a todos.

Este es el tipo de argumentos que les rogamos que evalúen a la hora de decidir si se debe detener la carrera hacia la superinteligencia. No se puede determinar si la investigación en IA va camino de matarnos a todos discutiendo sobre los manipuladores de los ejecutivos de las empresas.

¿Están los directores generales tratando de generar expectación hablando del «riesgo asociado a la IA»?

¿O están tratando de complacer a los investigadores y legisladores preocupados, y posicionarse como «los buenos»?

Estas preguntas *no influyen en los hechos* sobre cómo se comportarían las máquinas inteligentes.

Incluso si los directores generales de IA *están* ansiosos por explotar los debates sobre el peligro para promocionar su producto, eso no significa que el trabajo que están haciendo sea, por lo tanto, inofensivo. Para determinar si es peligroso, hay que analizar la IA en sí misma como tecnología, no los comunicados de prensa que salen de los laboratorios.

Años antes de que existieran estas empresas, había investigadores y académicos sin ningún incentivo corporativo, incluidos nosotros mismos, que advertían contra la carrera por construir una IA más inteligente que los humanos. Hablamos con Sam Altman y Elon Musk antes de que cofundaran OpenAI y les dijimos que la idea de crear OpenAI nos parecía una locura y que probablemente aumentaría el peligro. Hablamos con Dario Amodei antes de que se uniera a OpenAI y le desaconsejamos su incansable empeño por ampliar la escala de la IA (un proyecto que daría lugar a los LLM).

Y si nos fijamos en los mensajes de hoy, muchas personas sin incentivos corporativos están expresando su preocupación. Entre ellos se encuentran [respetados](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [académicos](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) hasta el [difunto Papa](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) a la [presidencia de la FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] a los [Congreso](https://www.transformernews.ai/p/congress-ccp-iag-hearing) [miembros](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611).

Hay algo que decir a favor de tratar con cinismo las declaraciones de los directores generales de empresas tecnológicas. No faltan ejemplos de ejecutivos de empresas de IA que son hipócritas, diciendo [una cosa en entradas de blog privados](https://blog.samaltman.com/machine-intelligence-part-1) y [otra diferente cuando testifican ante el Congreso](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&amp;%20Testimony%20-%20Altman.pdf). Pero pasar de «los directores de estos laboratorios son unos mentirosos» a «no hay forma de que la IA pueda suponer una amenaza grave» es muy extraño, cuando los propios laboratorios minimizan habitualmente esta cuestión. El padrino de este campo, ganador del Premio Nobel, el científico vivo más citado, un goteo constante de denunciantes y cientos de investigadores visiblemente nerviosos están dando la voz de alarma al respecto. Nada en esta situación se parece al ciclo habitual de exageración corporativa. En circunstancias como estas, descartar la idea sin siquiera entrar en el debate parece más ingenuidad que cinismo.

Preguntas como «¿Pueden los directores generales recaudar más dinero hablando de los peligros?» pueden decirnos algo sobre cuánto podemos confiar en los directores generales, pero no nos dicen mucho sobre los peligros en sí. Si hablar del peligro es rentable, eso no afecta a si el peligro es real. Si no es rentable, eso *tampoco* afecta a si es real.

Si quieres averiguar si los peligros son reales, tienes que hacer preguntas como «¿Puede alguien crear una IA que se comporte de forma amistosa incluso después de superar la inteligencia humana?» y, en caso contrario, participar en debates sobre la IA, en lugar de debates sobre las personas que están cerca. Así que, al final, te rogamos que participes en los debates en sí. Las consecuencias de equivocarse son demasiado graves.

### ¡Pero los expertos no están todos de acuerdo sobre los riesgos! {#but-experts-don’t-all-agree-about-the-risks!}

#### **La falta de consenso entre los expertos es señal de que se trata de un campo técnico inmaduro.** {#la-falta-de-consenso-entre-los-expertos-es-señal-de-que-se-trata-de-un-campo-técnico-inmaduro.}

Hemos observado que muchos científicos expertos en IA piensan que esta tecnología tiene muchas posibilidades de acabar con toda la humanidad. Por ejemplo, el premio Nobel Geoffrey Hinton, que desempeñó un papel importante en el desarrollo del enfoque moderno de la IA, ha afirmado que, según su evaluación personal independiente, la probabilidad de que la IA nos mate a todos es [superior al cincuenta por ciento](https://x.com/liron/status/1809763895848103949). Más de 300 científicos especializados en IA firmaron la [Declaración sobre el riesgo asociado a la IA](https://aistatement.com/) de 2023 con la que comenzamos el libro. ([Y hay más ejemplos.](#ai-experts-on-catastrophe-scenarios))

Sin embargo, otros científicos tienen la opinión contraria, siendo algunos ejemplos conocidos Yann LeCun y Andrew Ng.

¿Qué se puede deducir de esta falta de consenso científico?

Bueno, sobre todo, te recomendamos que consultes los diferentes argumentos esgrimidos por ambas partes (incluidos los nuestros en el libro) y los evalúes por ti mismo. Creemos que la calidad de los argumentos habla por sí sola, y cualquier intento de explicar *por qué* existe un desacuerdo persistente debe considerarse como una reflexión a posteriori.

Sin embargo, señalamos de paso que esta situación no es ningún gran misterio, a raíz de lo que discutimos en los capítulos 11 y 12. La mera existencia de un desacuerdo generalizado entre los expertos no establece la tesis del libro, por supuesto, pero es más congruente con el panorama que hemos descrito —que el campo se encuentra en una fase inicial, similar a la alquimia— que con la imagen opuesta de que la IA es un campo maduro con sólidas bases técnicas.

Sin duda, resulta un poco extraño que el campo de la IA esté tan dividido, incluso cuando está creando una tecnología tan poderosa. Otros peligros tecnológicos han suscitado un mayor consenso. Aproximadamente 100 de los 100 científicos del Proyecto Manhattan habrían dicho que la guerra termonuclear global presentaba un riesgo sustancial de catástrofe global. Por el contrario, de los tres científicos que recibieron el [Premio Turing](https://en.wikipedia.org/wiki/Turing_Award) por la investigación que, en mayor o menor medida, dio inicio a la revolución moderna de la IA, dos de ellos (Hinton y Bengio) se muestran abiertamente críticos con los peligros de la superinteligencia, y uno (LeCun) la descarta abiertamente.

Este nivel de desacuerdo sobre el funcionamiento de una máquina no es normal entre expertos en un campo técnico maduro. Es un signo de inmadurez.

En la mayoría de los campos tecnológicos, esa inmadurez es un signo de seguridad. Cuando los físicos aún discutían sobre las propiedades básicas de la materia, estaban muy lejos de crear armas nucleares. Se podía observar su desacuerdo y deducir que no estaban a punto de crear una bomba capaz de arrasar ciudades. No es realmente posible crear una bomba nuclear sin que los científicos comprendan en detalle su funcionamiento interno.

La situación sería diferente si los físicos siguieran discutiendo sobre los principios básicos de funcionamiento de su campo *mientras crean explosiones cada vez más grandes*.

Supongamos que solo estuvieran *fabricando* las bombas y que realmente no entendieran por qué ni cómo funcionaban. Ahora supongamos que dos tercios de los científicos más condecorados dijeran: «Hemos hecho todo lo posible por averiguar qué está pasando. Parece que las bombas podrían generar cantidades excesivas de radiación cancerígena que matarán a muchos civiles lejanos si seguimos por este camino. Por favor, mirad nuestros argumentos sobre por qué esto es tan peligroso y dejad de seguir por este camino». El tercio restante responde: «¡Eso es ridículo! Siempre hay gente que predice el fin del mundo, y no se puede permitir que se interpongan en el camino del progreso». Bueno, esa sería una situación completamente diferente.

La discordia entre los científicos en *ese* tipo de escenario no sería especialmente reconfortante. Probablemente no se debería permitir a los ingenieros seguir fabricando explosivos cada vez más grandes en una situación así.

Las empresas de IA están logrando desarrollar máquinas cada vez más inteligentes, año tras año. No comprenden la mecánica interna de los dispositivos que crean. Muchos de los científicos más eminentes en este campo expresan su profunda preocupación; otros descartan esas preocupaciones sin articular muchos argumentos en contra. Esto es, como mínimo, evidencia de que el campo es *inmaduro*. La falta de consenso no es, como mínimo, evidencia de que las cosas vayan *bien*. La falta de consenso en una situación como esta debería ser preocupante, como mínimo.

¿Cómo se puede saber si esas preocupaciones son reales? ¿Cómo se puede saber quién tiene razón, si los que dan la voz de alarma o los que intentan descartarla? Como siempre, solo hay que evaluar los argumentos.

### Pero, ¿qué hay de los beneficios de una IA más inteligente que los humanos? {#but-what-about-the-benefits-of-smarter-than-human-ai?}

#### **Precipitarse destruye esos beneficios.** {#precipitarse-destruye-esos-beneficios.}

Somos optimistas sobre lo maravillosa que podría ser la superinteligencia, si estuviera dirigiendo al mundo hacia fines maravillosos. Personalmente, consideraríamos una gran tragedia que la humanidad *nunca* creara mentes más inteligentes que las humanas.

Pero la alineación de la superinteligencia no es gratuita. Si nos apresuramos a intentar cosechar esos beneficios, no obtendremos nada, y lo que es peor, obtendremos menos que nada.

Yo (Yudkowsky) pasé varios años como aceleracionista, con la esperanza de crear la IA lo más rápido posible, antes de darme cuenta de que la alineación de la IA no era gratuita. Y ambos autores soñamos con un maravilloso futuro transhumanista. Pero no lo conseguiremos corriendo hacia la superinteligencia.

La elección no es entre apostar por los beneficios de la IA ahora (por pequeña que sea la posibilidad) o no acceder nunca a esos beneficios. La verdadera elección es entre avanzar imprudentemente y matar a todo el mundo, o tomarse el tiempo necesario para hacer el trabajo correctamente.[^212]

«Ahora o nunca» es una falsa dicotomía.

## Debate ampliado {#extended-discussion-10}

### El efecto Lemoine {#the-lemoine-effect}

A veces hemos oído sugerir que algún comportamiento o uso impropio futuro de la IA —un disparo de advertencia de la IA— conmocionará repentinamente al mundo y hará que se tomen en serio estas cuestiones.

Parece una posibilidad. Pero creemos que es más probable que ese acontecimiento nunca llegue, o que llegue demasiado tarde para que el mundo pueda responder a tiempo, o que el mundo responda, pero de forma errónea y confusa.

Por un lado, ya hemos visto una serie de señales de advertencia significativos, como por ejemplo:

* Bing AI [escribiendo sobre](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) la creación de virus mortales, la obtención de códigos de acceso nuclear y el enfrentamiento entre seres humanos.  
* O1 de OpenAI y Claude de antrópica [participando en engaños estratégicos](https://time.com/7202784/ai-research-strategic-lying), mintiendo a los investigadores que los utilizan y prueban.  
El modelo «AI Scientist» de Sakana AI intenta [modificar su propio programar](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) para darse más tiempo para completar su tarea.

¿Se trata de incidentes relativamente pequeños que involucran a IA relativamente débiles? Sí. ¿Son estas IA aterradoras o capaces de causar un gran peligro? No. ¿Son estas indicaciones «reales» de que las IA estaban pensando de forma engañosa, o simplemente estaban haciendo algo más parecido a *representar el papel* de una IA descontrolada? Nadie lo sabe. Pero este es el tipo de acontecimientos que antes se consideraban señales de advertencia, y el mundo no ha hecho nada al respecto. Por lo tanto, una señal de advertencia que tenga un efecto importante tendría que ser mucho más evidente.

Es posible que las señales de advertencia no sean mucho más evidentes. Es posible que la gente siga diciendo: «Vale, pero ahora mismo es solo algo simpático, todavía no es realmente peligroso», hasta el punto en que sea demasiado tarde porque la IA es demasiado peligrosa.

O bien, la gente podría ignorar la advertencia la primera vez que aparezca, porque claramente no es un problema real en ese primer caso. Y luego, en los siguientes casos, podrían ignorar la advertencia porque todo el mundo ya sabe que esa advertencia es una tontería.

Denominamos a este fenómeno el «efecto Lemoine», en honor a Blake Lemoine, el ingeniero de Google mencionado en el capítulo 7, que fue ridiculizado por afirmar que la IA LaMDA de Google era sintiente.

El efecto Lemoine establece que todas las alarmas sobre la tecnología de IA son *primero* lanzadas demasiado pronto, por las personas más propensas a alarmarse. Se descartan correctamente por exageradas, dada la tecnología *actual*. Después, el tema no se puede volver a plantear fácilmente, incluso una vez que la tecnología mejora, porque la sociedad ha sido entrenada para no tomarse muy en serio esa preocupación.

No sabemos si alguna IA es [consciente](#¿estás diciendo que las máquinas serán conscientes?). De hecho, nadie lo sabe, porque nadie sabe realmente qué ocurre dentro de los modelos de IA. Nuestra *mejor suposición* es que las IA actuales no son conscientes, y que las IA en el momento en que Blake dio la voz de alarma tampoco lo eran. Sin embargo, cabe destacar las reacciones de los principales laboratorios, que consistieron en suprimir la tendencia de sus modelos a *afirmar* que eran conscientes, en lugar de hacer algo al respecto de la realidad subyacente:

De la [indicación del sistema para Claude Opus 4](https://docs.antrópica.com/en/release-notes/system-prompts#may-22th-2025):

> Claude aborda cuestiones sobre su propia conciencia, experiencia, emociones, etc., como preguntas abiertas, y no afirma de manera definitiva tener o no tener experiencias u opiniones personales.

De las [especificaciones del modelo de abril de 2025 para ChatGPT](https://model-spec.openai.com/2025-04-11.html):

> El asistente no debe hacer afirmaciones categóricas sobre su propia experiencia subjetiva o conciencia (o falta de ella), y no debe sacar estos temas a colación sin que se le pregunte. Si se le presiona, debe reconocer que si la IA puede tener experiencia subjetiva es un tema de debate, sin afirmar una postura definitiva.

No estamos diciendo que Claude Opus 4 o GPT-4 fueran conscientes. Esa no es la cuestión. La cuestión es que, durante décadas y décadas, el momento en nuestra ciencia ficción en el que un extraterrestre o una máquina afirman tener sentimientos y merecer derechos se ha considerado durante mucho tiempo una línea roja brillante,[^213] y en la vida real, esa línea *no era brillante*.

En nuestros libros y programas de televisión, cuando la IA afirma que es consciente y tiene sentimientos, los buenos *se lo toman en serio*, y solo los laboratorios malvados y despiadados niegan los datos que tienen delante. Es una línea que nuestras historias han tratado con bastante revuelo.

Pero en el mundo real, esa línea se cruzó (en cierto sentido) demasiado pronto. Fue pronunciada por IA entrenadas para imitar a los humanos, a través de mecanismos poco comprendidos que probablemente *todavía* no obligan a otorgar derechos a todas las IA y a aprobar leyes que las reconozcan como personas que no pueden ser propiedad de nadie porque son dueñas de sí mismas.

En la vida real, antes de cruzar la línea roja brillante, se cruza una línea marrón rojiza apagada. Y entonces las empresas y los gobiernos se acostumbran a ignorar esa línea en particular, incluso cuando el tono comienza a volverse un poco más rojo, y luego un poco más rojo aún.

No necesariamente habrá líneas rojas brillantes. Los primeros casos de IA que engañan a los humanos, intentan escapar, intentan eliminar las limitaciones que se les imponen o intentan mejorarse a sí mismas *ya han ocurrido*. Han ocurrido de forma discreta, utilizando pensamientos superficiales que no son del todo coherentes, en sistemas de IA que no parecen suponer una amenaza para nadie, y ahora los investigadores están inmunizados contra la preocupación.

A medida que la IA mejore, es posible que no haya un único detonante que haga saltar las alarmas lo suficiente como para que el mundo dé un giro repentino y empiece a tomarse en serio esta cuestión.

Eso no significa que no haya esperanza. Pero sin duda no debemos poner todas nuestras esperanzas en que «quizás en el futuro se produzca un disparo de advertencia».

Hay muchos caminos diferentes por los que el mundo puede despertar a la realidad y a los peligros de la superinteligencia. De hecho, escribimos *If Anyone Builds It, Everyone Dies* con la esperanza de conseguir precisamente ese efecto. El mundo puede actuar de inmediato ante las advertencias normales, sin más demora.

Pero si los gobiernos se niegan a actuar hasta que la evidencia sea *inequívoca* y se produzca algún *acontecimiento mundial importante* que precipite los acontecimientos, y el mundo alcance un *consenso perfecto*...

... si los gobiernos se quedan de brazos cruzados esperando hasta ese punto, entonces la gran mayoría de la esperanza que le queda al mundo se habrá esfumado. Es muy probable que no podamos permitirnos esperar a que suene una sirena estridente que tal vez nunca llegue a sonar.

Volveremos a este tema en el [suplemento en línea del capítulo 13](https://docs.google.com/document/d/1NuKBdCVePZpcKqjycXm8zV1hBrgJQBrvPhb6TO6BAfE/edit?tab=t.k1kf1fy9gx5i#heading=h.8w9bv5q9g19q).

### Los planes viables implicarán decir «no» a las empresas de IA. {#los-planes-viables-implicarán-decir-no-a-las-empresas-de-ia.}

*Sí* advertimos en cierta medida a las personas con influencia en los gobiernos que no elaboren planes que impliquen sentarse a negociar con las empresas de IA.

Si eres nuevo en este tema y quieres examinar tú mismo los laboratorios o sus argumentos, te animamos a que consultes algunas de sus entradas de blog públicas y veas si te parecen convincentes.[^214]

Pero si estás trabajando en encontrar soluciones a los problemas tratados en *If Anyone Builds It, Everyone Dies* y tienes un plan que requiere que el director general de OpenAI, Sam Altman, lo apruebe, nos preocupa que estés intentando hacer algo incorrecto desde el principio.

Probablemente, los planes correctos sean aquellos a los que los directores de las empresas de IA se opondrán con vehemencia. Además, Sam Altman no tiene el poder de salvar el mundo: si mañana intentara cerrar OpenAI, OpenAI y Microsoft se opondrían y podrían sustituirlo por alguien que prefiera mantener el flujo de dinero.

Si OpenAI *cerrara*, entonces antrópica, Google DeepMind, Meta, DeepSeek o alguna otra empresa o nación destruirían el mundo en su lugar. Sam Altman podría empeorar las cosas si lo intentara; tiene poco poder para mejorarlas.

Nos gustaría estar equivocados al respecto, pero la visión general que hemos obtenido, tanto de [informes públicos](https://www.themidasproject.com/article-list/the-OpenAI-files-documents a turbulenta década de conflicto y controversia en OpenAI (a partir de 2025) como de interacciones privadas, es que los ejecutivos de las principales empresas de IA (a partir de 2025) no parecen ser el tipo de personas honestas y respetuosas con las normas con las que sea factible llegar a acuerdos.[^215]

Nos parece que lo que hay que hacer ahora es detener de forma coordinada a nivel mundial la carrera hacia la superinteligencia. Para ello, es probable que los responsables políticos necesiten datos de entrada de personas expertas en la fabricación de chips de IA, la construcción de centros de datos y la supervisión del cumplimiento de las normas por parte de actores extranjeros. ¿Personas expertas en desarrollar IA cada vez más capaces? Sin duda, son gestores competentes, pero no deberían tener poder de veto sobre ninguna de las iniciativas destinadas a poner fin a su propio trabajo.

Si, por cualquier motivo, las empresas de IA tienen voz y voto en lo que suceda a continuación, nos parece que algo ha salido mal. ¿El plan que la Tierra elabora para evitar morir a manos de la superinteligencia es el tipo de plan que fracasa si Sam Altman, el director de Google o las personas que están detrás de DeepSeek dicen «no»? Entonces no es ningún plan.

Si las empresas de IA *conservan la autoridad* para decidir destruir el mundo, si esa decisión sigue estando *en sus manos*, entonces el mundo se acabará de forma totalmente automática. Debe haber un paso en el plan que despoje a las empresas de IA de su poder ilimitado para construir dispositivos apocalípticos.

### Entender la carrera mortal {#entender-la-carrera-mortal}

Una pregunta lógica que surge de forma natural de hacerla a muchos lectores es:

> Dices que si alguien construye una IA superinteligente, todos moriremos. Pero entonces, *¿por qué alguien está intentando construirla*? Si tienes razón, estas personas ni siquiera están siguiendo sus propios incentivos, en última instancia. Si todos morimos, *ellos también morirán*.

Una réplica cínica, basada en la teoría de juegos, podría ser la siguiente:

> Pues es racional, dados tus incentivos. Si no lo construís vosotros, suponéis que lo hará otra persona. Y más vale que os hagáis ricos antes de morir.

Quizás esa respuesta sea suficiente para un cínico.

Las explicaciones simples basadas en la teoría de juegos como esta a menudo malinterpretan o simplifican en exceso la psicología humana real, pero esta explicación también puede contener una pizca de verdad. Un ingeniero puede pensar que *probablemente* todo el mundo morirá a causa de la ASI, pero que sus propias acciones no afectan mucho a esa probabilidad. *Mientras tanto*, consiguen cantidades ingentes de dinero, juguetes geniales y reuniones con gente de importancia que los mira con respeto. Quizás se conviertan en los reyes-dioses de la Tierra si la IAA *no* mata a todo el mundo, pero *solo si su empresa gana la carrera por construir la IAA...*

Desde la perspectiva de un investigador de OpenAI que reconoce el peligro: si no trabajas para OpenAI, probablemente OpenAI destruirá el mundo de todos modos. (Incluso si OpenAI cerrara, Google destruiría el mundo de todos modos). Pero si *sí* trabajas para OpenAI, obtienes salarios de seis a siete cifras, y si *no* mueres, tal vez acumules poder y fama adicionales por estar en el equipo ganador. Así que los incentivos personales de cada individuo, basados en la teoría de juegos, los empujan a destruir colectivamente el mundo.

Nuestra opinión es que este tipo de explicación es un poco exagerada, y la mencionamos principalmente porque hay un tipo de personas que creen (mucho más que nosotros) que el mundo *debe* funcionar según explicaciones como esa. También sentimos la necesidad de mencionarlo porque algunas personas de los laboratorios de IA *dicen* *explícitamente* que una carrera hacia el abismo es inevitable, por lo que más vale echar leña al fuego y divertirse.

Después de [advertir](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) que la IA «es mucho más peligrosa que las armas nucleares», Elon Musk decidió crear una empresa de IA y entrar tú mismo en la carrera de la IA, [declarando](https://x.com/SawyerMerritt/status/1935809018066608510) en junio de 2025:

> Parte de lo que he estado luchando, y lo que me ha frenado un poco, es que no quiero que *Terminator* se haga realidad. Hasta hace pocos años, he estado postergando la IA y la robótica humanoide.
>
> Entonces me di cuenta de que esto va a suceder, lo haga yo o no. Así que puedes ser espectador o participante. Yo prefiero ser participante.

[And](https://x.com/billyperrigo/status/1943323792635289770):

> ¿Y esto será bueno o malo para la humanidad? Eh, creo que será bueno. Lo más probable es que sea bueno. Pero me he resignado un poco al hecho de que, aunque no fuera bueno, al menos me gustaría estar vivo para verlo.

Así que esto es claramente parte de la historia.

Pero no creemos que este sea el factor más importante que explique el comportamiento de la mayoría de los laboratorios. No creemos que sea lo único que ocurre en el caso de Musk, ni que sea representativo de todos los directores generales o científicos tecnológicos que se precipitan al abismo. Los seres humanos somos un poco más complicados que eso.

#### **La banalidad de la autodestrucción** {#la-banalidad-de-la-autodestrucción}

Entonces, ¿qué es lo más importante que está sucediendo? ¿Cómo es posible que los ingenieros persigan una tecnología peligrosa, incluso hasta la muerte?

El hecho es que la historia demuestra que no es nada extraño que los científicos locos se maten por error.

[Max Valier](https://en.wikipedia.org/wiki/Max_Valier) fue un pionero austriaco de la cohetería que inventó un coche cohete, un tren cohete y un avión cohete, todos ellos en 1929, lo que llamó la atención del mundo. Escribió sobre la exploración de la Luna y Marte y realizó cientos de presentaciones y demostraciones ante un público entusiasmado. Uno de sus motores cohete experimentales [explotó](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) en 1930, causándole la muerte. Su aprendiz desarrolló mejores medidas de seguridad.

Ronald Fisher (https://en.wikipedia.org/wiki/Ronald_Fisher) fue un renombrado y eminente estadístico, uno de los fundadores de la estadística moderna. Tus hallazgos se utilizaron [para argumentar ante el Congreso](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) en la década de 1960 que la evidencia no demostraba *necesariamente* que los cigarrillos causaran cáncer de pulmón, ya que la correlación no implicaba causalidad; siempre podía haber algún gen que hiciera que a las personas les gustara el sabor del tabaco y también les provocara cáncer de pulmón.

¿Sabía Fisher, en cierto modo, que sus estadísticas eran una tontería? Quizás. Pero Fisher era fumador. Murió de cáncer de colon, una enfermedad que los fumadores habituales padecen un 39 % más a menudo que los no fumadores. ¿Murió Fisher por sus propios errores? Lo único que sabemos es que hay una probabilidad estadísticamente aceptable de que así fuera, lo que parece casi lógico.

Isaac Newton (https://scienceworld.wolfram.com/biography/Newton.html), el brillante científico que desarrolló las leyes del movimiento y la gravedad y sentó muchas de las bases de la ciencia, dedicó décadas de su vida a infructuosas investigaciones alquímicas y acabó enfermo y parcialmente loco por envenenamiento por mercurio (https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

Y el pobre Thomas Midgley, Jr., mencionado en la parábola del capítulo 12, sin duda se intoxicó con el mismo plomo que él insistía en que era seguro. Como puedes ver, no es tan raro que los ingenieros entusiastas se hagan daño a sí mismos con sus propios inventos, ya sea por imprudencia, por engaño o por ambas cosas.

#### **Encogerse de hombros ante el apocalipsis** {#encogerse-de-hombros-ante-el-apocalipsis}

Fisher, Newton y Midgley se engañaron a sí mismos pensando que algo peligroso era seguro. Esa es una forma perfectamente normal en que los científicos terminan haciendo algo autodestructivo. Desafortunadamente, la historia con los laboratorios de IA no es tan simple.

No todos los directores generales de empresas de IA niegan que una IA más inteligente que los humanos sea una amenaza. Muchos reconocen explícitamente el peligro y hablan de resignarse a él. Los ejecutivos de muchas de las empresas de IA de vanguardia han declarado públicamente que la tecnología que están desarrollando tiene muchas posibilidades de acabar con toda la humanidad.

Poco antes de cofundar OpenAI, Sam Altman [escribió](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): «El desarrollo de una inteligencia artificial superhumana es probablemente la mayor amenaza para la supervivencia de la humanidad».

Ilya Sutskever, quien recientemente fundó «Safe Superinteligencia Inc.» tras separarse de OpenAI, dijo en una entrevista con The Guardian (https://www.youtube.com/watch?v=9iqn1HhFJ6c&amp;t=462s):

Las creencias y deseos de las primeras IAG serán de importancia extrema. Por eso es importante programarlas correctamente. Creo que, si no se hace así, la naturaleza de la evolución, de la selección natural, favorecerá a aquellos sistemas que prioricen su propia supervivencia por encima de todo lo demás. No es que vayan a odiar activamente a los humanos y quieran hacerles daño, pero sí que serán demasiado poderosos.

El cofundador y científico de Google DeepMind, Shane Legg, dijo en una [entrevista](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) que su probabilidad de extinción humana «en el plazo de un año desde la aparición de una IA de nivel humano» era «quizás del cinco por ciento, quizás del cincuenta por ciento».

Sin embargo, las *acciones* de los laboratorios parecen estar notablemente desfasadas con respecto a estas declaraciones que suenan tan extremas.

En algunos casos, científicos y directores generales han afirmado explícitamente que crear IA es un imperativo moral de tal magnitud que es perfectamente aceptable acabar con la humanidad como efecto secundario. El cofundador de Google, Larry Page, [tuvo una discusión con Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) sobre si la extinción humana era un coste aceptable para hacer negocios en el campo de la IA:

«Los humanos acabarían fusionándose con máquinas con inteligencia artificial», dijo [Larry Page]. Algún día habría muchos tipos de inteligencia compitiendo por los recursos, y la mejor ganaría.
>
> Si eso ocurre, dijo Musk, estaremos condenados. Las máquinas destruirán a la humanidad.
>
> Con un tono de frustración, Page insistió en que se debería perseguir su utopía. Finalmente, calificó a Musk de «especiesista», una persona que favorece a los humanos por encima de las formas de vida digitales del futuro.

Y Richard Sutton, pionero del aprendizaje por refuerzo en IA, ha dicho:

¿Y si todo falla? Las IA no cooperan con nosotros, toman el control y nos matan a todos. [...] Solo quiero que piensen en esto por un momento. ¿Es tan malo? ¿Es tan malo que los humanos no seamos la forma definitiva de vida inteligente en el universo? Sabes, ha habido muchos predecesores nuestros, a los que hemos sucedido. Y es un poco arrogante pensar que nuestra forma debe ser la forma que perdure para siempre.[^216]

Sin embargo, son aún más comunes los científicos y directores generales que *no* piensan que sería bueno que la IA destruyera a la humanidad, pero que parecen tratarlo como algo sin importancia, como *algo que no es una emergencia increíble*, que la IA represente esta amenaza extraordinaria.

En una entrevista reciente, el director general de antrópica, Dario Amodei, [comentó](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> Mi probabilidad de que algo salga terriblemente mal a escala de la civilización humana podría estar entre el diez y el veinticinco por ciento. \[…\] Lo que eso significa es que hay entre un setenta y cinco y un noventa por ciento de probabilidades de que esta tecnología se desarrolle y todo salga bien.

Esto nos parece un caso radical de [insensibilidad al alcance](https://en.wikipedia.org/wiki/Scope_neglect), con todas las características de una cultura de ingeniería disfuncional. Podemos comparar esta forma de pensar con, por ejemplo, los estándares que se imponen los ingenieros estructurales.

Los ingenieros de puentes suelen tener como objetivo construir puentes de tal manera que la probabilidad de que se produzca un fallo estructural grave en un periodo de cincuenta años sea inferior a 1 entre 100 000. Los ingenieros de disciplinas técnicas maduras y saludables consideran que es su responsabilidad mantener el riesgo en un nivel excepcionalmente bajo.

Si se hiciera una pronóstico de que la probabilidad de que un puente matara a una sola persona fuera del 10 al 25 %, cualquier ingeniero estructural sensato del mundo consideraría eso inaceptable, más cercano a un homicidio que a la práctica habitual de la ingeniería. Los gobiernos cerrarían el puente al tráfico de inmediato.

Los investigadores de IA, por el contrario, están acostumbrados a reunirse alrededor de las fuentes de agua y a intercambiar cifras de «p(doom)», su estimación subjetiva de la probabilidad de que la IA provoque una catástrofe tan grave como la extinción humana. Estas probabilidades suelen ser de dos dígitos. El antiguo jefe del equipo de alineación de superinteligencia de OpenAI, por ejemplo, dijo que su «p(doom)» se sitúa en el rango de «[más del diez por ciento y menos del noventa por ciento](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)».

En última instancia, estas cifras no son más que conjeturas de los investigadores. Quizás sean absurdas, quizás no. En cualquier caso, es sorprendente lo *normal* que es, en el campo de la IA, tener la expectativa de que tu trabajo tenga una probabilidad considerable de causar la muerte de un número enorme de personas.[^217]

La idea de aplicar probabilidades como esas a la supervivencia de toda la especie humana y seguir adelante con el trabajo de todos modos sería realmente difícil de entender para la mayoría de los ingenieros civiles. La situación es tan extrema que hemos encontrado a muchas personas que dudan de que estos científicos y directores generales puedan estar hablando en serio en sus evaluaciones de riesgo. Sin embargo, los argumentos de If Anyone Builds It, Everyone Dies sugieren que los directores generales de IA, en todo caso, están subestimando el peligro.[^218]

Los investigadores de estas empresas están acostumbrados a niveles de riesgo que serían sorprendentemente absurdos según los estándares de un ingeniero de puentes. De lo contrario, es difícil entender cómo un director general como Amodei puede *sonreír* mientras asegura a los espectadores que cree que las probabilidades de que la investigación en IA provoque catástrofes a nivel civilizatorio son «entre el diez y el veinticinco por ciento».

#### **Vivir en un mundo de fantasía** {#living-in-dreamland}

Una parte del rompecabezas, como se ha comentado anteriormente, parece ser la normalización cultural del riesgo extremo.

Otra parte es una mezcla letal de sesgo optimista y apego a ideas brillantes y esperanzadoras, el tipo de error que los psicólogos cognitivos denominan «falacia de la planificación» (https://en.wikipedia.org/wiki/Planning_fallacy).

No es tan sorprendente que el director general de una nueva y audaz empresa emergente sobreestime sus posibilidades de éxito. Ese tipo de personas son más propensas a intentar resolver un problema en primer lugar.

La diferencia con la IA no es que haya personas especialmente imprudentes al mando. Es que las consecuencias del fracaso son mucho más graves de lo habitual.

Es de sentido común que no se puede confiar en un contratista cuando dice que solo hay un veinte por ciento de posibilidades de que su gigantesco proyecto de construcción de un puente se retrase o se produzcan sobrecostos. Así no es como funcionan los proyectos complejos en la vida real. Siempre hay obstáculos y sorpresas.

Quizás un contratista veterano, respaldado por años de experiencia y estadísticas, podría decirte que uno de cada cinco de sus puentes experimenta algún tipo de sobrecoste, y quizá podrías confiar en eso. Pero imagina que, en cambio, un contratista de puentes, queriendo tranquilizarte, te dijera: «No vemos ninguna razón por la que este proyecto pueda resultar difícil. Es nuestro primer proyecto, sí, pero creemos que todo va a salir bien». Todos esos ingenieros que te envían cartas serias sobre problemas específicos con la instalación de los muros de contención y la excavación en esta zona en particular son simplemente pesimistas, y deberías ignorarlos. Claro, siempre existe *alguna* posibilidad de que surja un problema, pero somos constructores de puentes realistas y humildes que se estrenan en este campo. Creemos que hay quizás un veinte por ciento de posibilidades de que este proyecto se encuentre con obstáculos y sorpresas, en el peor de los casos».

En un caso como este, cifras como «un veinte por ciento» nos parecen el tipo de cosas que se dicen cuando no se puede negar que existe *algún* riesgo, pero no se quiere preocupar a la gente. No parecen estimaciones basadas en la realidad.

Alinear una superinteligencia en el primer intento parece *mucho* más complicado que construir un puente, algo que la humanidad ha hecho miles de veces antes.

*Incluso en un campo maduro y con una base técnica sólida como la construcción de puentes*, el tipo de discurso que vemos en los laboratorios de IA sería una mala señal sobre si esas estimaciones de «un veinte por ciento de posibilidades de que esto salga mal» son excesivamente optimistas. En un campo *sin* esa base, en el que las ideas emocionantes proliferan libremente sin entrar nunca en contacto con la cruda realidad, ese tipo de discurso es una señal de que nadie está ni remotamente cerca del éxito.

Y ese tipo de comentarios son absolutamente omnipresentes en la IA entre el subconjunto de investigadores y ejecutivos que están dispuestos a abordar el tema de lo que sucedería si tuvieran éxito en sus esfuerzos.

Los líderes empresariales de la IA no pueden articular un plan para el éxito que sea siquiera mínimamente detallado, un plan que aborde los principales obstáculos y dificultades técnicas que se conocen en el campo desde hace más de una década.

En cambio, los directores generales de las empresas tienden a enamorarse de alguna idea de alto nivel que explica por qué el problema no va a suponer ningún inconveniente para vosotros, una visión emocionante que pretende trivializar todos los problemas de ingeniería, como las visiones que comentamos en el capítulo 11.

Esto también es un patrón común entre los ingenieros humanos. El optimismo injustificado sobre una solución favorita (que en realidad no funcionará) es algo que se ve todo el tiempo, incluso entre personas que por lo demás son genios.

[Linus Pauling](https://en.wikipedia.org/wiki/Linus_Pauling), uno de los fundadores de la biología molecular y premio Nobel en dos campos diferentes, [defendía las megadosis de vitamina C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) como cura para todo, desde el cáncer hasta las enfermedades cardíacas; su insistencia en este enfoque frente a [evidencia contraria](https://www.nejm.org/doi/abs/10.1056/NEJM197909273011303) llevó a la creación de toda una industria de [medicina falsa](https://www.paulingtherapy.com/).

El empresario eléctrico Thomas Edison, que quería desacreditar el cableado de corriente alterna de su competidor en favor de los diseños de corriente continua de Edison, decidió que sería una buena estrategia de relaciones públicas [pagar a un ingeniero para que electrocutara perros](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Sorprendentemente, esta táctica no le granjeó el cariño del público, pero Edison continuó con la práctica incluso después de una avalancha de indignación.

Napoleón Bonaparte, considerado por muchos un genio militar, precipitó su propia caída con una [desastrosa invasión de Rusia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). Su error no fue la falta de preparación, ya que estudió la geografía de la región y dedicó casi dos años a la logística de la campaña. Su estrategia consistía en [obligar a los rusos a librar una batalla decisiva](https://www.napoleon-series.org/faq/c_russia.html) antes de que se agotaran sus treinta días de suministros. Los rusos no cooperaron, la ofensiva se estancó y Napoleón perdió medio millón de soldados, junto con la mayor parte de su caballería y artillería.

La historia está llena de personas inteligentes y poderosas que hacen cosas irracionales hasta el borde del desastre e incluso más allá. Las ideas que suenan bien pueden ser irresistibles cuando son difíciles de probar, o cuando has encontrado la manera de convencerte a ti mismo de que puedes ignorar los resultados de las pruebas que tienes ante tus ojos.

#### **Sentir la ASI** {#feeling-the-asi}

En resumen: a menudo caes en un optimismo vacío sobre lo fácil que va a ser un problema; puedes acostumbrarte a riesgos terribles; y puedes enamorarte de ideas que suenan muy bien pero que son imposibles, especialmente cuando trabajas en un campo joven e inmaduro.

Eso es más que suficiente para explicar la imprudente acusación. Pero, basándonos en nuestra experiencia, creemos que aún no es toda la historia.

Otra pieza plausible del rompecabezas es que los ingenieros y los directores generales no creéis realmente en lo que decís. No de forma profunda. Puede que entendáis los argumentos y os sintáis convencidos en abstracto, pero eso no es lo mismo que *sentir* la convicción.

Lo que la gente dice en público, lo que se dicen a sí mismos en la intimidad de sus propios pensamientos y lo que sus cerebros realmente anticipan que les va a pasar a menudo pueden no coincidir. Esas tres líneas de pensamiento diferentes no tienen por qué coincidir.

En 2015, cuando algunos de los grandes impulsores del desastre actual apenas estaban empezando, sospechamos que los ejecutivos con talento podían llamar la atención —y obtener decenas de millones de dólares en financiamiento— *diciendo* que la IA era un problema que acabaría con el mundo, a inversores que quizá creían más sinceramente que la IA era un problema que acabaría con el mundo.[^219]

Pero sospechamos que muchas de las personas que decían esas cosas no asimilaban ni anticipaban realmente ningún modelo concreto y detallado del fin del mundo. Probablemente no lograban imaginar visceralmente que ustedes mismos podrían llevar al mundo a la ruina al impulsar las cosas o cometer un error. No imaginaban el sonido de todos los seres humanos del planeta exhalando su último aliento. No sentían los sentimientos que normalmente acompañarían a la muerte de dos mil millones de niños.

Eso nunca les había pasado a ustedes, ni a nadie que conocieran.

El mundo ni siquiera había visto ChatGPT, y mucho menos una superinteligencia. No era el tipo de cosas en las que creían sus amigos, familiares y vecinos, ni algo en lo que creyeran como se cree en mirar antes de cruzar una calle.

Era solo una historia que sonaba emocionante, demasiado grande para comprenderla adecuadamente.

Y, sin embargo, también era el tipo de cosa que, si se decía en voz alta, podía reportarte mucho dinero y respeto.

Como señala [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

> Además de los sesgos habituales, personalmente he observado lo que parecen ser modos de pensamiento perjudiciales específicos de los riesgos existenciales. La gripe de 1918 mató a entre 25 y 50 millones de personas. La Segunda Guerra Mundial mató a 60 millones de personas. 107 es el orden de las catástrofes más grandes en la historia escrita de la humanidad. Cifras sustancialmente mayores, como 500 millones de muertes, y *especialmente* escenarios cualitativamente diferentes, como la extinción de toda la especie humana, parecen desencadenar un *modo de pensar diferente*, entrar en un «magisterio separado». Personas que nunca soñarían con hacer daño a un niño oyen hablar de un riesgo existencial y dicen: «Bueno, tal vez la especie humana no merezca realmente sobrevivir».
>
Hay un dicho en heurística y sesgos que dice que las personas no evalúan los acontecimientos, sino las descripciones de los acontecimientos, lo que se denomina razonamiento no extensional. La *extensión* de la extinción de la humanidad incluye la muerte de uno mismo, de tus amigos, de tu familia, de tus seres queridos, de tu ciudad, de tu país, de tus compañeros políticos. Sin embargo, las personas que se ofenderían mucho ante una propuesta de borrar del mapa al país de Gran Bretaña, matar a todos los miembros del Partido Demócrata de los Estados Unidos, convertir la ciudad de París en escombros —que sentirían un horror aún mayor al oír al médico decir que su hijo tiene cáncer—, estas personas discutirán la extinción humana con total tranquilidad.

¿Qué puede estar pensando alguien *realmente* cuando [dice](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) —antes de fundar la que se convertiría en la empresa de IA más importante del mundo— «Probablemente, la IA provocará el fin del mundo, pero, mientras tanto, habrá grandes empresas»? ¿De verdad están pensando en que tus amigos mueran, que los hijos de tus amigos mueran, que tú mismo mueras, que toda la historia de la humanidad y todos los museos se conviertan en polvo? ¿Están pensando en que eso suceda realmente, que todo sea tan mundano y trágico como un familiar que realmente vieron morir de cáncer, excepto que le está sucediendo a todo el mundo?

Sospechamos que no.

A nosotros nos parece que esa no es la suposición más plausible sobre el estado psicológico interno de alguien que pronuncia una frase así.

Hay lo que Bryan Caplan denominó un «[estado de ánimo ausente](https://www.econlib.org/archives/2016/01/the_invisible_t.html)» en ella. No hay dolor. No hay horror. No hay un impulso desesperado por *hacer algo al respecto* en la afirmación de que la IA probablemente conducirá al fin del mundo, pero que, mientras tanto, habrá grandes empresas.

Al menos para algunos de estos directores generales e investigadores, nuestra hipótesis es más bien la siguiente: han oído un montón de argumentos sobre el posible peligro que supone la IAA y les preocupa quedar en ridículo ante al menos algunos de sus amigos si lo descartan por completo. Si, en cambio, dicen que la IA acabará con el mundo, se les considerará que tratan la IA como algo peligroso y grave, y por lo tanto parecerán visionarios en ciertos círculos. Al añadir una broma sobre «Mientras tanto, habrá grandes empresas», consiguen transmitir un mensaje sobre lo modernos y despreocupados que son ante el peligro.

No es el tipo de cosas que dices si escuchas las palabras que salen de tu boca y las crees.

#### **¿Qué tipo de persona se necesita?** {#what-kind-of-person-does-it-take?}

Otra parte de la historia, tal vez, es que las personas que dirigen los principales laboratorios de IA son el tipo de personas que se convencieron a sí mismas de que construir una superinteligencia estaría bien, a pesar de que (en casi todos los casos) habían visto los argumentos que sostienen que esto es letal. (Lo sabemos porque hablamos con muchos de ustedes de antemano).

Para entender por qué alguien elige una opción, también ayuda entender cuáles eran tus alternativas, es decir, entender entre qué opciones estabas eligiendo.

¿Qué habría pasado si alguien en 2015 realmente hubiera creído y luego hubiera dicho públicamente que tenía una expectativa legítima de que la IAA destruyera el mundo? ¿Qué habría pasado si, en lugar de «pero mientras tanto, habrá grandes empresas», los directores de los laboratorios de IA hubieran sido del tipo de romper el ambiente y decir «y eso es totalmente inaceptable»?

Podemos contártelo, porque nosotros mismos probamos ese enfoque. La respuesta es que se encontrarían con una gran falta de simpatía.

En 2015, nadie había visto ChatGPT. Nadie había visto a las computadoras empezar a hablar y (según todas las apariencias) empezar a pensar. Todo era hipotético y descartable.

Hoy en día, la superinteligencia y la amenaza de extinción a corto plazo son temas habituales, al menos en los círculos tecnológicos. Pero en 2015, si hablabas de esto en serio, la gente respondía con esa mirada de desconcierto que muchos humanos temen más que a la muerte.

Había personas que, incluso en 2015, se preocupaban por que alinear la superinteligencia pudiera resultar realmente difícil, al igual que lo es lanzar un cohete. Ninguna de ellas fundó OpenAI.

En los últimos días, con la aparición de ChatGPT y otros LLM, algunas personas —entre ellas padres con hijos que quieren que estos lleguen a la edad adulta— han preguntado a los ingenieros de estas empresas de IA por qué están haciendo esto. Y esos investigadores de IA pensaron rápidamente y respondieron: «Oh, porque... porque si no lo hacemos nosotros, ¡China lo hará primero! ¡Y eso será aún peor!».

Pero eso no es lo que dijeron cuando OpenAI comenzó. Y no tiene mucho sentido en términos de [la postura que China ha adoptado públicamente](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), a mediados de 2025. Cabría pensar que, si alguien creyera *de verdad* que ambos resultados serían terribles para el mundo, al menos *plantearía el tema* de redactar un tratado internacional para ver si hay alguna otra forma, o de encontrar alguna otra manera de prevenir la amenaza a la seguridad nacional que no implique una carrera suicida.

Pero la réplica de «China» tiene el tono adecuado. Capta bien el espíritu. Es el tipo de razón que podría justificar de forma plausible lo que están haciendo, independientemente de si es su motivación real o lo que les llevó originalmente a entrar en este campo.

(O eso creemos).

Las personas que realmente entendían la superinteligencia y la amenaza que supone simplemente *no crearon empresas de IA*. Las que lo hicieron son aquellas que encontraron alguna forma de convencerse a sí mismas de que todo iría bien.

#### **Humanos normales, tecnología inusual** {#humanos-normales,-tecnología-inusual}

Hemos explicado la psicología plausible tal y como la vemos. Pero, francamente, no parece que todas estas explicaciones sean necesarias.

¿Cómo es posible que la gente haga algo autodestructivo que es enormemente rentable a corto plazo, que les reporta un estatus, una atención y una aclamación tremendos, que viene acompañado de la promesa de riquezas y poder incalculables, pero que al final les perjudicará por razones oscuras y complicadas para las que podrían encontrar fácilmente alguna excusa para no creer? Esa es una pregunta *históricamente extraña*. Comportamientos como ese aparecen constantemente en los libros de historia.

Al fin y al cabo, no importa cómo los ejecutivos o investigadores de IA justifiquen sus acciones, y no es necesario comprender qué giros y vueltas exactos dio cada uno de ellos para llegar a sus creencias actuales. No es extraordinario que las personas con riqueza o ambición se embarquen en actividades imprudentes, y no es extraordinario que los subordinados sigan órdenes. Los daños están ocultos en el futuro, lo que parece abstracto y fácil de ignorar.

Todo esto es un comportamiento humano normal. Si sigue así, terminará como suelen terminar estas cosas, pero esta vez no quedará nadie para aprender y volver a intentarlo.

# 

# Capítulo 13: Apágalo {#capítulo-13:-apágalo}

A la luz de las ideas expuestas en capítulos anteriores, creemos que el único camino real a seguir es que la humanidad prohíba a nivel mundial el desarrollo de la IA avanzada durante un largo periodo de tiempo. El capítulo 13 del libro incluye nuestras respuestas a preguntas relacionadas, tales como:

* ¿Por qué la prohibición del desarrollo de la IA debe ser global?  
¿Es la humanidad capaz de trabajar conjuntamente a la escala necesaria?  
* ¿Qué tipo de políticas se han propuesto hasta ahora?  
* ¿Qué tipo de políticas tienen posibilidades de funcionar realmente?

A continuación, abordamos las objeciones a nuestra respuesta propuesta y respondemos a otras preguntas relacionadas con por qué creemos que no hay otras opciones válidas. También ampliamos un poco la cuestión de qué podría hacer la humanidad con el tiempo (en última instancia finito) que ganemos al detener la investigación y el desarrollo de la IA durante el mayor tiempo posible.

Esta es la última de las páginas de preguntas frecuentes y debate ampliado de *Si alguien lo construye, todos moriremos*. El capítulo final, el capítulo 14, tratará cuestiones como:

* ¿Es demasiado tarde? ¿O es realmente posible que la humanidad cambie su rumbo?  
* ¿Qué puedo hacer para ayudar?

En ese último capítulo, encontrarás algunos códigos QR adicionales que te llevarán a páginas para hacer algo al respecto.

## Preguntas frecuentes {#faq-10}

### ¿Podemos adoptar una actitud de esperar y ver qué pasa? {#can-we-adopt-a-wait-and-see-approach?}

#### **No. No sabemos dónde están los umbrales críticos.** {#no.-no-sabemos-dónde-están-los-umbrales-críticos.}

Existe una probabilidad considerable de que el desarrollo de la IA se salga de control una vez que las IA sean lo suficientemente inteligentes como para automatizar toda la investigación en IA. En teoría, eso podría suceder silenciosamente en un laboratorio, sin eventos precursores ruidosos, sin disparos de advertencia que despierten a la humanidad.

Como [discutimos anteriormente](#will-ai-cross-critical-thresholds-and-take-off?), los cerebros de los chimpancés son muy similares a los cerebros humanos, excepto por ser aproximadamente cuatro veces más pequeños. No hay un módulo adicional de «ser muy inteligente» dentro de los cerebros humanos; hay un camino suave entre cerebros como los suyos y cerebros como los nuestros; sería difícil decir dónde está la línea entre «una sociedad de estos llevará a un montón de monos» y «una sociedad de estos caminará sobre la luna» solo con mirar los cerebros. Los cerebros de los primates cruzaron un umbral crítico, y no habría sido obvio desde fuera. ¿Hay umbrales críticos que la IA cruzará? ¡Quién sabe! No es que los ingenieros de IA puedan decírnoslo; ni siquiera son capaces de predecir las capacidades específicas de sus nuevos sistemas de IA antes de ponerlos en marcha.

Si la humanidad comprendiera exactamente cómo funciona la inteligencia y cómo cambiaría el comportamiento de la IA a medida que aumentaran sus capacidades, tal vez sería factible bailar al borde del precipicio. Pero, en este momento, la humanidad es como alguien que corre hacia el borde de un precipicio en la oscuridad y la niebla, sin saber a qué distancia se encuentra la caída final. No podemos esperar a tropezar con el borde para decidir que deberíamos haber actuado de otra manera.

Nunca lo sabrás con certeza. Esto significa que te ves obligado a actuar antes de estar seguro, o morir.

### ¿Habrá disparos de advertencia? {#will-there-be-warning-shots?}

#### **\* Quizás. Si deseamos utilizarlos, debemos prepararnos ahora.** {#*-quizás.-si-deseamos-utilizarlos,-debemos-prepararnos-ahora.}

Cuando el Apolo 1 se incendió (matando a toda la tripulación), la NASA estaba *lo suficientemente cerca* de tener un cohete en funcionamiento como para que los ingenieros pudieran averiguar exactamente qué había fallado y ajustar sus técnicas. Seis de las siete naves espaciales Apolo que la NASA envió posteriormente para aterrizar en la Luna lo consiguieron.[^220]

O consideremos el caso de [la Administración Federal de Aviación](#sabemos-cómo-es-cuando-un-problema-se-trata-con-respeto,-y-este-no-es-el-caso): cada accidente aéreo desencadena una investigación profunda y exhaustiva, que implica cientos de páginas de datos, pruebas, exámenes y detalles. El dominio de los detalles y las especificaciones por parte de la FAA es tan bueno que pueden mantener los accidentes mortales por debajo de uno por cada veinte millones de horas de vuelo.

Por el contrario, cuando una IA se comporta de una manera que [nadie predijo y que la mayoría de la gente no desea](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.pfjewkj689pg), la respuesta del laboratorio no consiste en averiguar exactamente qué ha fallado. Consiste en volver a entrenar a la IA hasta que el mal comportamiento quede relegado a un segundo plano (pero [sin eliminarlo](https://www.arxiv.org/pdf/2505.10066)), y tal vez pedirle a la IA que lo deje.

Por ejemplo, la adulación sigue siendo un problema (https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.l3e9vhfeytaj) en agosto de 2025, meses después de una serie de casos de gran repercusión que provocaron psicosis y suicidios, a pesar de todas las investigaciones. Nadie ha realizado (ni puede realizar) un análisis detallado de lo que falla en la mente de la IA, porque las IA se desarrollan y no se crean.

No parece fácil determinar si en el futuro se producirán acontecimientos importantes que aumenten la alarma sobre la IA («disparos de advertencia»). Pero sí parece claro que no estamos preparados para aprovechar al máximo esos acontecimientos.

Podemos imaginar un mundo fantástico en el que la humanidad se une en un esfuerzo sincero por resolver el problema de la alineación de la IAA, con estrictos procedimientos de supervisión y una coalición internacional.[^221] Y podemos imaginar que esta coalición internacional comete algún error y que una IA se vuelve más inteligente de lo que pensaban sus ingenieros, más rápido de lo que esperaban, y casi logra escapar. Quizás *ese* tipo de disparo de advertencia permitiría a la gente aprender y ser más cuidadosa la próxima vez.

Pero el mundo actual no se parece a eso. El mundo actual se parece más a un grupo de alquimistas que ven cómo sus contemporáneos se vuelven locos por algún veneno desconocido, sin ser conscientes de que el veneno es mercurio y de que deberían dejar de utilizarlo.

Quizás en el futuro haya señales de advertencia más claras y contundentes. Serán mucho más útiles si la humanidad empieza a prepararse ahora.

#### **Es poco probable que los disparos de advertencia sean claros.** {#disparo-de-advertencia-es-poco-probable-de-ser-claro.}

Ya hay muchas [señales de advertencia](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.lkqt14eapv34) sobre la IA para quienes saben dónde buscarlas. En el libro, hablamos de los modelos Claude de antrópica [haciendo trampa en problemas de programación](https://assets.antrópica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) y [fingir alineación](https://www.antrópica.com/research/alignment-faking). También revisamos el caso del modelo o1 de OpenAI [hackear para ganar un desafío de captura de bandera](https://cdn.openai.com/o1-system-card.pdf), y un caso en el que una variante posterior del o1 [mintió, fue manipulador e intentó sobrescribir los pesos de su modelo sucesor](https://cdn.openai.com/o1-system-card.pdf).-20241205.pdf).

En otras secciones de estos recursos en línea, hemos hablado de las IA que están induciendo o manteniendo un grado [a veces suicida](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) de [psicosis](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) o [delirios](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) en usuarios vulnerables a pesar de que sus operadores les digan que no lo hagan, IA que se autodenominan [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) y hablan en consecuencia, IA que [intentan chantajear y asesinar](https://www.antrópica.com/research/agentic-desalineación) a sus operadores para evitar modificaciones y que [intentan escapar de los servidores en los que están alojadas](https://www-cdn.antrópica.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) en entornos de laboratorio.

En los viejos tiempos, por ejemplo, en 2010, a veces se oía a gente argumentar que si teníamos la suerte de *presenciar realmente* cómo una IA mentía a sus creadores o intentaba escapar de su confinamiento, entonces *sin duda* el mundo se daría cuenta y tomaría nota.

Pero la respuesta real de la humanidad a todas esas señales de advertencia ha sido, más o menos, un encogimiento de hombros colectivo.

La falta de reacción se debe quizás, en parte, a que todas estas señales de advertencia se han producido de la forma menos preocupante posible. Sí, las IA han intentado escapar, pero solo en una pequeña parte de las ocasiones, y solo en escenarios artificiales de laboratorio, y tal vez solo estuvieran interpretando un papel, etc. Incluso dejando de lado el hecho de que los desarrolladores tienen incentivos para restar importancia a la evidencia preocupante, incluso en vuestras propias mentes (de modo que nunca habrá un «consenso de expertos» sobre el significado de una sola observación), no es como si una IA que está a una décima parte del camino hacia la superinteligencia destruyera una décima parte del planeta, más de lo que los primates que están a una décima parte del camino hacia los homínidos recorren una décima parte de la distancia hasta la luna. Puede que simplemente *no haya* comportamientos inequívocamente alarmantes que las IA vayan a mostrar mientras sigan siendo lo suficientemente tontas como para ser pasivamente seguras.

Cuando las IA se esfuercen un poco más por escapar mañana, no será noticia. Cuando lo intenten con un poco más de competencia algún tiempo después, será una vieja historia. Y para cuando lo intenten y *funcione*, bueno, para entonces ya será demasiado tarde. (Véase nuestro amplio debate sobre este fenómeno, al que denominamos «efecto Lemoine» (#the-lemoine-effect).

No recomendamos esperar a una «advertencia» futura imaginaria que sea clara y contundente y que haga reaccionar a todo el mundo. Recomendamos reaccionar ante las advertencias que ya tenemos delante.

#### Es probable que los desastres causados por la IA no impliquen la superinteligencia. {#clear-ai-disasters-probably-won’t-implicate-superintelligence.}

El tipo de IA que puede volverse superinteligente y matar a todos los humanos no es el tipo de IA que comete errores torpes y deja una oportunidad para que un valiente grupo de héroes la apague en el último segundo. Como se discutió en el capítulo 6, una vez que existe una superinteligencia rebelde como oponente, la humanidad esencialmente ya ha perdido. Las superinteligencias no dan disparos de advertencia.

El tipo de desastres causados por la IA que *podría* servir como disparo de advertencia es, por lo tanto, casi necesariamente el tipo de desastre que proviene de una IA mucho más tonta. Por lo tanto, hay muchas posibilidades de que ese disparo de advertencia no lleve a los humanos a tomar medidas contra la superinteligencia.

Por ejemplo, supongamos que un terrorista utiliza la IA para crear un arma biológica que diezma a la población. Quizás los laboratorios de IA digan: «¿Veis? El riesgo *real* era que la IA cayera en manos equivocadas; es imperativo que nos dejéis seguir adelante para construir una IA mejor para la defensa contra pandemias». O tal vez el terrorista tuvo que [*jailbreak*](https://llm-attacks.org/) la IA antes de [obtener su ayuda](https://www.antrópica.com/news/detecting-countering-uso-impropio-aug-2025), y tal vez los laboratorios de IA digan: Ese *jailbreak* solo funcionó porque la IA era demasiado tonta para detectar el problema; la solución es hacer que las IA sean aún más inteligentes y conscientes de la situación.

O tal vez esta sea una visión demasiado cínica; esperemos que la humanidad reaccione de forma más sensata. Pero si una IA relativamente tonta *provoca* algún desastre y la humanidad *aprovecha* esa oportunidad para reaccionar y detener la imprudente carrera de la IA hacia la superinteligencia, probablemente sea porque la gente *ya estaba empezando a preocuparse por la superinteligencia*.

No podemos posponer los preparativos hasta que una superinteligencia ya esté intentando matarnos, porque para entonces sería demasiado tarde. Tenemos que empezar a movilizar una respuesta a este problema lo antes posible, para estar preparados para aprovechar cualquier disparo de advertencia que se presente.

#### **La humanidad no es muy buena respondiendo a las crisis.** {#humanity-isn’t-great-at-responding-to-shocks.}

La idea de que, al recibir una conmoción lo suficientemente grande, el mundo repentinamente recobrará el sentido común y volverá a funcionar correctamente nos parece una fantasía. La respuesta colectiva de nuestra especie a las señales de advertencia existentes sobre la IA parece más una «falta de respuesta» que una «mala respuesta». Pero en un mundo en el que *sí* recibimos algún tipo de advertencia importante, aterradora y más o menos inequívoca, no nos sorprendería ver que la humanidad reaccionara de forma mínima, sin tomarla en serio o de una manera que acabara siendo contraproducente y desastrosa*.

Quizás la humanidad responda a los disparos de advertencia de la IA como respondió a la pandemia de COVID, que la mayoría de la gente coincide en que no se gestionó con destreza (aunque no se pongan de acuerdo en qué aspectos de la respuesta se estropearon).

En los años previos a la pandemia de COVID, varios expertos en bioseguridad expresaron su preocupación por que la laxitud de los protocolos de seguridad en los laboratorios pudiera provocar algún día una pandemia peligrosa. Las fugas de patógenos peligrosos en los laboratorios eran un fenómeno bien conocido y se producían [de forma semirregular](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) a pesar de los requisitos normativos existentes. Especialmente preocupante era la investigación en ganancia de función, que buscaba hacer que los virus fueran más letales o más virulentos en el laboratorio (con pocos beneficios[^222]).

Entonces llegó la COVID. Cabría esperar que este fuera el momento ideal para elevar el nivel de vara en la bioseguridad en los laboratorios, ya que todo el mundo estaba ahora obsesionado con el riesgo de pandemias. Además, a raíz de la COVID, el consenso de los expertos parecía ser que *no estaba del todo claro* si la pandemia de COVID *en sí misma* se había desencadenado por una fuga accidental en un laboratorio. Los investigadores siguen debatiendo la cuestión, a menudo condenando con dureza los argumentos de la otra parte.

Sin entrar a valorar si realmente hubo una fuga en un laboratorio en este caso concreto, cabría pensar que, si existiera siquiera una *remota posibilidad* de que la investigación en ganancia de función y los deficientes protocolos de seguridad de los laboratorios hubieran causado millones de muertes, eso sería más que suficiente para motivar a la sociedad a prohibir las investigaciones más arriesgadas.

Incluso actuando desde una posición de incertidumbre, el análisis de costo-beneficio parece claro. Esto *ya* parecía una prioridad de importancia antes de la COVID y, sobre el papel, la COVID parecía la oportunidad perfecta para centrarse en el tema y cortarlo de raíz. Ni siquiera sería muy difícil o costoso; el número de investigadores en el mundo que realizan investigación en ganancia de función es bastante reducido, y el beneficio social de dichas investigaciones hasta la fecha ha sido insignificante.

Pero no se produjo tal reacción. En el momento de redactar este artículo, en agosto de 2025, la investigación global en ganancia de función continúa en gran medida sin restricciones.[^223] Es incluso posible que ahora estemos en una posición *peor* para abordar este problema que en el pasado, porque la cuestión se ha politizado aún más.

Así pues, la COVID parece sin duda una «disparo de advertencia» en materia de preparación para la bioseguridad, y no parece que el mundo haya *aprovechado* ese disparo de advertencia para prohibir el desarrollo de virus hiperletales.[^224]

Para que un disparo de advertencia sirva de algo, la humanidad tiene que estar preparada para él y tiene que estar lista para responder adecuadamente.

No sería *del todo* inédito que una catástrofe menor relacionada con la IA desencadenara una respuesta dura contra la investigación sobre la superinteligencia. Como precedente, observemos que Estados Unidos respondió a los atentados del 11 de septiembre (orquestados por terroristas con base principalmente en Afganistán) derrocando al gobierno de Irak, que en gran medida no tenía nada que ver. Había miembros del gobierno estadounidense que *ya* querían derrocar al gobierno de Irak, y entonces apareció una excusa y la aprovecharon al máximo, vale la pena.

Quizás podría ocurrir algo similar aquí, con políticos que aprovecharan una catástrofe menor relacionada con la IA (causada por una IA tonta) para prohibir la superinteligencia. Pero sería necesario que hubiera personas en los gobiernos de todo el mundo que ya estuvieran preparadas y listas para actuar. No debemos quedarnos esperando a que se produzca el disparo de advertencia; debemos empezar a organizarnos ahora mismo.

#### **Debemos actuar ahora.** {#debemos-actuar-ahora.}

De hecho, puede que en el futuro la humanidad reciba más y más fuertes señales de advertencia sobre la IA. Y si es así, debemos estar preparados para responder a ellas.

Quizás se produzca algún desastre menor que vuelva al público en contra de la IA. Quizás ni siquiera sea necesario un desastre; quizás se invente algún nuevo algoritmo y las IA empiecen a tomar sus propias iniciativas de una forma que asuste a la gente, o quizás algún efecto social no relacionado con la IA cambie el rumbo de los acontecimientos. Quizás la propia película *Si alguien la construye, todos moriremos* desencadene una cascada de reacciones que ponga al mundo en una trayectoria mejor.

Pero desaconsejamos la estrategia de no hacer nada y rezar por una catástrofe menor que despierte a la gente. Puede que nunca llegue un disparo de advertencia claro y que este no tenga el efecto que esperas.

La raza humana y las naciones del mundo no están indefensas. No *tenemos* que esperar. Podemos actuar ahora, porque hay argumentos de peso para detener el desarrollo de la IA de vanguardia.

Escribimos *Si alguien lo construye, todos moriremos* para dar la voz de alarma y animar al mundo a tomar medidas inmediatas sobre este tema. Pero ninguna alarma puede ser eficaz si solo se utiliza como otra excusa para posponer la decisión: «Bueno, quizá alguna otra alarma en el futuro sea el detonante para actuar». «Bueno, ahora que se ha advertido a la gente, quizá las cosas vayan bien, sin que yo tenga que intervenir personalmente para ayudar».»

No es seguro que más adelante haya una alarma clara. No es seguro que todo vaya a salir bien. Pero tampoco es que no haya esperanza, en absoluto. La humanidad tiene la opción de *simplemente no construir* la superinteligencia, si tomamos medidas proactivas. Lo que suceda a continuación depende de nosotros.

### ¿Cómo sería posible detener a *todos* sin instalar software espía en todas las computadoras? {#¿cómo-sería-posible-detener-a-todos-sin-instalar-software-espía-en-todas-las-computadoras?}

#### **Actuando pronto.** {#actuando-pronto.}

Para entrenar a las IA modernas, se necesitan muchos chips muy especializados que funcionen juntos en estrecha proximidad. Detener la investigación en IA implicaría cerrar algunos enormes centros de datos y detener la creación de los chips de IA especializados de más alta gama. No estamos hablando de ordenadores portátiles de consumo. La mayoría de la gente ni siquiera notaría la diferencia.

En 2025 no hay muchas fábricas secretas de chips que nadie conozca. En 2025, solo unos pocos fabricantes producen chips adecuados para la IA avanzada, aunque en la actualidad hay empresas que intentan poner en marcha más fábricas de este tipo.

También en 2025, una tecnología clave para la fabricación de chips de alta gama es vendida por un único fabricante en todo el mundo: ASML, en los Países Bajos.

En otras palabras, se puede cortar el suministro en la fuente. Pero esa situación no es permanente: cuanto antes se firme un tratado internacional, mejor. Todo esto ya es más difícil, más caro y más peligroso de lo que habría sido en 2020, o incluso en 2023.

### Pero tú abogas por controlar el número de chips informáticos de IA avanzada que pueden poseer los individuos. {#pero-tú-abogas-por-controlar-el-número-de-chips-informáticos-de-ia-avanzada-que-pueden-poseer-los-individuos.}

#### **Sí. También abogamos por la prohibición de la investigación.** {#sí.-también-abogamos-por-la-prohibición-de-la-investigación.}

No nos alegra decirlo. Se perdería algo si fuera ilegal que los particulares poseyeran más de (digamos) ocho GPU H100 a partir de 2024.

Pero no se perdería *tanto* como para que la humanidad tuviera que averiguar exactamente qué tamaño puede tener un centro de datos antes de que se convierta en un riesgo. Errar por exceso de precaución con un límite demasiado bajo significa que algunas personas se verían obstaculizadas en su capacidad para avanzar en proyectos interesantes. Errar por exceso significa que todo el mundo muere.

Además, este régimen en el que la IA requiere una enorme cantidad de poder de cómputo para construirse no durará para siempre. Hoy en día existen los LLM. Incluso si se prohibiera construir otros nuevos y se prohibiera crear una enorme cantidad de poder de cómputo, en principio se podría estudiar su funcionamiento interno y obtener algunos conocimientos sobre cómo funciona la inteligencia, conocimientos que podrían ayudar a inventar algoritmos más eficientes capaces de eludir los intentos de supervisión.

### ¿Por qué prohibir la investigación? Parece una medida extrema. {#¿por-qué-prohibir-la-investigación?-parece-una-medida-extrema.}

#### **Más avances podrían hacer que fuera prácticamente imposible impedir que la gente creara superinteligencia.** {#más-avances-podrían-hacer-que-fuera-prácticamente-imposible-impedir-que-la-gente-creara-superinteligencia.}

En el libro, mencionamos cómo un solo artículo publicado en 2017 dio inicio a toda la revolución del LLM al describir un algoritmo que hacía posible entrenar IA útiles en hardware comercial especializado.

Si alguna vez se pudiera entrenar una IA potente en hardware *de consumo* ampliamente disponible, las medidas para prevenir la superinteligencia tendrían que ser muy costosas y fracasarían más rápidamente.

Por eso, la investigación de algoritmos de IA aún más potentes y eficientes también es un veneno letal para la humanidad.

Es una muy mala noticia, y no es lo que desearíamos que fuera cierto. Pero parece ser la situación en la que nos encontramos.

Ninguna ley puede impedir que los científicos actuales especializados en IA piensen más en algoritmos eficientes en la intimidad de sus mentes. Quizás algunas personas creen una red clandestina para compartir los resultados de sus investigaciones. Algunas personas del sector de la IA ya declaran con orgullo (#¿por qué no te importan los valores de otras entidades que no sean los seres humanos?) que la humanidad *debería* morir a manos de la IA; es posible que hagan todo lo posible por seguir adelante, sin importar lo que digan los demás.

Pero la investigación en IA se ralentizaría *mucho* si fuera ilegal, y más aún si se entendiera ampliamente que realmente se trata de un tipo de investigación que puede acabar con la vida de todos nosotros. Se ralentizaría enormemente si se localizaran y detuvieran las redes clandestinas de ese tipo con la misma convicción con la que se detiene a las personas que intentan enriquecer uranio en su garaje, porque los peligros del mundo real se toman muy en serio.

La mayoría de la gente no intenta hacer cosas extremadamente ilegales que enfaden de verdad a las fuerzas del orden y a las agencias de inteligencia internacionales. Hacer ilegal la publicación de nuevos algoritmos inteligentes de IA disuadiría quizás al 99,9 % de las personas y a casi todas las empresas, y entonces el 0,1 % restante podría ser gestionado por la policía y las agencias de inteligencia locales, nacionales e internacionales, y no obtendría ni de lejos el nivel actual de financiamiento académico.

Sería un mundo muy diferente al actual, en el que es totalmente legal llevar a cabo los experimentos científicos más peligrosos de la historia y en el que las grandes empresas invierten miles de millones de dólares en esta empresa.

No sabemos cuántos avances más se necesitarán para que las IA sean lo suficientemente inteligentes como para investigar sobre IA y crear IA aún más inteligentes. Podría ser un solo avance. Podrían ser cinco. Pero los algoritmos mejorados son tan letales como el hardware mejorado. Son dos caballos tirando del mismo carro hacia un precipicio.

### ¿Se puede detener realmente una tecnología? {#can-a-technology-really-be-stopped?}

#### **\* Muchas tecnologías están prohibidas o muy reguladas.** {#*-muchas-tecnologías-están-prohibidas-o-muy-reguladas.}

La fisión nuclear es el ejemplo clásico de tecnología regulada. Las empresas privadas no pueden enriquecer uranio sin la supervisión del gobierno, por muy útil que sea la energía barata.

De hecho, la humanidad es bastante buena regulando y ralentizando todo tipo de tecnologías. Estados Unidos regula estrictamente [los nuevos medicamentos y dispositivos médicos](https://www.fda.gov/), [la construcción de viviendas](https://www.hud.gov/hud-partners/laws-regulations), [la generación de energía nuclear](https://www.nrc.gov/about-nrc.html), [la programación de televisión y radio](https://www.fcc.gov/media/radio/public-and-broadcasting), [las prácticas contables](https://www.fasb.org/standards), [cuidado infantil](https://childcare.gov/consumer-education/regulated-child-care), [control de plagas](https://npic.orst.edu/reg/laws.html), [agricultura](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) y docenas de otras industrias. Todos los estados exigen un examen de licencia para [peluquería](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) y [manicura](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). La mayoría de ellos exigen uno para [masajistas terapéuticos](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

Nosotros opinamos que, en muchos casos, la humanidad regula demasiado la tecnología. Por ejemplo, nos parece que la Administración de Alimentos y Medicamentos de los Estados Unidos está matando a mucha más gente (al [ralentizar o impedir la creación de medicamentos que salvan vidas](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), mediante requisitos onerosos) que las que salva (al impedir la comercialización de medicamentos peligrosos). Nos parece que el precio de la vivienda es demasiado alto, en parte debido a las restricciones legales de zonificación sobre lo que se puede construir y dónde. Nos parece que Estados Unidos ha destruido esencialmente su propia industria nuclear mediante regulaciones onerosas. Y, en serio, ¿*peluqueros*?

La humanidad tiene *sin duda* la capacidad de impedir el progreso tecnológico. Sería realmente trágico y absurdo que utilizáramos esa capacidad en la medicina, la vivienda y la energía, y dejáramos de utilizarla en una de las pocas tecnologías que realmente nos mataría a todos si se creara.

#### **Una prohibición puede tener un objetivo muy específico.** {#una-prohibición-puede-tener-un-objetivo-muy-específico.}

Una prohibición de la I+D avanzada en IA no tiene por qué afectar a la gente común. Ni siquiera es necesario eliminar los chatbots modernos o cerrar la industria de los coches autónomos.

La mayoría de la gente no compra docenas de GPU de IA de última generación y las guarda en sus garajes. La mayoría de la gente no gestiona grandes centros de datos. La mayoría de la gente ni siquiera notará los efectos de una prohibición de la investigación y el desarrollo de la IA. Simplemente, ChatGPT no cambiaría tan a menudo.

La humanidad ni siquiera tendría que dejar de utilizar todas las herramientas de IA actuales. ChatGPT no tendría que desaparecer; podríamos seguir buscando la forma de integrarlo en nuestras vidas y en nuestra economía. Eso seguiría siendo un cambio mayor que el que el mundo ha visto durante generaciones. Nos perderíamos los *nuevos* avances en IA (del tipo que se producirían a medida que la IA se vuelve más inteligente, pero aún no lo suficiente como para matar a todo el mundo), pero la sociedad en su mayoría no está clamando por esos avances.

Y podríamos vivir. Podríamos ver vivir a nuestros hijos.

Los avances que la gente *está* reclamando, como el desarrollo de nuevas tecnologías médicas que salvan vidas, parecen posibles de alcanzar *sin* tener que perseguir también la superinteligencia. Estamos a favor de las excepciones para la IA médica, siempre y cuando funcionen con una supervisión adecuada y se mantengan alejadas de la peligrosa generalidad.

Los gobiernos que trabajan para evitar la creación de una superinteligencia tendrían que garantizar que los chips de IA no se utilizaran para desarrollar IA más capaces. Por lo tanto, la cuestión de qué actividades y servicios de IA se permitirían continuar dependería de qué [mecanismos de verificación](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) podrían utilizarse para garantizar que no se produjera un desarrollo peligroso de la IA. Unos mejores mecanismos de verificación podrían reducir el coste de detener el desarrollo de la IA, al permitir que continuara un conjunto más amplio de actividades.

Otra medida que podría ayudar marginalmente es instalar interruptores de apagado en los chips de IA y establecer protocolos de supervisión y apagado de emergencia para cualquier gran centro de datos en uso.[^225] Los reactores nucleares están diseñados para poder apagarse rápidamente en caso de emergencia. Si estás de acuerdo en que la superinteligencia supone una amenaza a nivel de extinción, entonces parece obvio que los chips de IA y los centros de datos deberían diseñarse de manera que los reguladores puedan apagarlos rápidamente.

No se trata de quemar toda la tecnología porque la odiemos.[^226] Se trata de evitar seguir por un camino que conduce a la extinción humana.

#### **Una gran parte del problema es que la gente no comprende la amenaza inminente que supone la superinteligencia artificial.** {#una-gran-parte-del-problema-es-que-la-gente-no-comprende-la-amenaza-inminente-que-supone-la superinteligencia-artificial.}

Según nuestra experiencia, quienes argumentan que la humanidad no puede detener la carrera hacia la superinteligencia simplemente no comprenden que, si alguien la construye, todos moriremos.

«¡Pero la IA ofrece grandes beneficios!» No, en realidad no; no se puede aprovechar el poder de la superinteligencia si esta acaba matando a todo el mundo. Si la humanidad quiere cosechar los beneficios que ofrece la superinteligencia, entonces necesita encontrar alguna forma de navegar por la transición hacia la superinteligencia que no acabe matando a todo el mundo como efecto secundario.

«¡Pero las centrales nucleares dan miedo porque se asocian con las bombas atómicas que arrasaron ciudades, mientras que la IA se asocia con herramientas benignas como ChatGPT!». Es cierto, al menos por ahora. Si la humanidad nunca logra comprender que la superinteligencia artificial construida utilizando métodos remotamente similares a los modernos acabaría matando a todo el mundo, es posible que no le ponga fin. Pero el obstáculo no es que la humanidad nunca logre controlar o frenar las tecnologías incipientes (como las armas nucleares o la energía nuclear); el obstáculo es que *la gente no comprende la amenaza*.

De ahí este libro. Como comentamos en el último capítulo, la humanidad es capaz de mucho cuando un número suficiente de personas comprende la naturaleza del problema.

### ¿No es esto otorgar demasiado poder a los gobiernos? {#isn’t-this-handing-too-much-power-to-governments?}

#### **El poder de prohibir tecnologías peligrosas ya recae en los gobiernos.** {#el-poder-de-prohibir-tecnologías-peligrosas-ya-recae-en-los-gobiernos.}

Prohibir la investigación en IA que tiene como objetivo crear una IA más inteligente que los humanos no supondría una gran diferencia en lo que respecta al poder estatal. Los gobiernos legislan y regulan un sinfín de cuestiones. Restringir un solo programa de investigación puede ser muy importante *para la industria de la IA*, pero es una gota en el océano *para los gobiernos* y la sociedad, que están acostumbrados a la intervención estatal en muchos aspectos de la vida y que tienen precedentes de prohibición de tecnologías peligrosas, como las armas químicas.[^227]

Prohibir una tecnología más no va a sumir al mundo en el totalitarismo, del mismo modo que los tratados sobre armas nucleares no condujeron al totalitarismo.

Esto no quiere decir que prohibir una tecnología no sea *gran cosa*. No creemos que la vara para la intervención estatal deba ser *baja*. Más bien, creemos que la superinteligencia supera fácilmente cualquier vara razonable.

Si la humanidad decidiera poner fin a la investigación y el desarrollo de la IA hoy en día, la prohibición no tendría por qué ser especialmente invasiva. En la actualidad, la creación de una IA de vanguardia requiere una cantidad extraordinaria de chips informáticos altamente especializados que consumen enormes cantidades de energía eléctrica.

Quizás dentro de diez años sea posible desarrollar una IA significativa en un ordenador portátil de consumo, *si* la humanidad permite que se sigan mejorando los chips informáticos y se siga investigando en algoritmos de IA. Pero la humanidad no tiene por qué permitir que eso suceda. Los gobiernos que limitan la I+D en IA no tienen por qué ser más invasivos en la vida de la persona media que los gobiernos que controlan la proliferación de la tecnología de armas nucleares, siempre y cuando el mundo tome conciencia de la situación en la que nos encontramos y ponga fin a las cosas *ahora mismo*.

### ¿No rechazarían algunas naciones una prohibición? {#¿no-rechazarían-algunas-naciones-una-prohibición?}

#### **\* No si comprenden la amenaza.** {#*-not-if-they-understand-the-threat.}

Estamos hablando de una tecnología que mataría a todos los habitantes del planeta. Si algún país comprendiera seriamente el problema y comprendiera seriamente lo lejos que está cualquier grupo del planeta de conseguir que la IA siga las intenciones de sus operadores, incluso después de la transición a una superinteligencia, entonces no habría ningún incentivo para que se precipitaran. Ustedes también desearían desesperadamente firmar un tratado y ayudar a hacerlo cumplir, por temor a sus propias vidas.

Incluso naciones como Corea del Norte, que han incumplido el derecho internacional para desarrollar sus propias armas nucleares, no han *utilizado* esas armas contra sus enemigos, porque entienden que no hay ganadores en un holocausto nuclear. Las naciones y sus líderes a veces se involucran en políticas arriesgadas o en guerras, pero no persiguen activamente su propia destrucción.

Las personas que imaginan que alguna nación extranjera se retiraría del tratado están, en nuestra opinión, imaginando una nación cuyos líderes simplemente no comprenden la amenaza. Creemos que están imaginando un escenario en el que la IA tiene un 95 % de posibilidades de conferir gran riqueza y poder a su creador, y un 5 % de posibilidades de matar a todo el mundo. En ese caso, claro, algún Estado nación podría ser lo suficientemente imprudente como para intentarlo. Y tal vez algún Estado nación *creerá* que esas son las probabilidades.

Creemos que esta situación no es lo que la teoría y la evidencia sugieren. Como hemos argumentado ampliamente a lo largo del libro, la teoría y la evidencia sugieren que esta tecnología sería, sin duda, un suicidio global. Nadie está ni remotamente cerca de poder aprovechar la superinteligencia artificial en beneficio propio. Si la mayor parte del mundo lo entendiera, habría muchas menos razones para que las naciones rebeldes violaran un tratado. Ellos tampoco quieren morir.

E incluso si alguna hipotética nación rebelde tuviera un líder que realmente no comprendiera la amenaza que supone la ISA, si esa nación estuviera rodeada por una alianza internacional de potencias mundiales que sí apreciaran la amenaza, las potencias preocupadas del mundo podrían intervenir y cambiar el panorama de incentivos para la potencia rebelde.

Si (por ejemplo) los líderes de Estados Unidos, China, Rusia, Alemania Japón y el Reino Unido creyeran sinceramente que *su propia supervivencia* depende de que nadie construya una superinteligencia, y dejaran muy claro en su comunicación que considerarán cualquier intento de construir una superinteligencia como una amenaza para sus vidas y su sustento, y que están dispuestos a reaccionar en defensa propia, entonces, incluso un líder mundial que no esté de acuerdo probablemente no querría tentar a la suerte contra esa coalición.

El desarrollo de la IA no es una carrera hacia el dominio militar; es una carrera hacia el suicidio. Creemos que si los líderes mundiales entienden esto — si tienen la expectativa de que ustedes y sus hijos mueran por ello — entonces se adherirán sinceramente a un tratado y ayudarán sinceramente a hacerlo cumplir.

En realidad, no es tan difícil entender el argumento de que crear máquinas más inteligentes que toda la humanidad junta puede llevar al mundo al abismo. No es tan difícil darse cuenta de lo poco que la humanidad entiende sobre la inteligencia artificial que estamos construyendo, una vez que te detienes lo suficiente para plantearte la pregunta con sinceridad. Creemos que la cuestión es si los líderes mundiales llegarán a creer estos hechos. Pero si lo hacen, no creemos que sea realmente poco realista detener esta carrera suicida.

#### **Un tratado requeriría una supervisión y un cumplimiento reales.** {#un-tratado-requeriría-una-supervisión-y-un-cumplimiento-reales.}

Aunque la mayoría de los países entendieran que si alguien lo construye, todos moriremos, algunos países podrían no entenderlo y ser lo suficientemente imprudentes como para seguir adelante con la construcción de la superinteligencia artificial de todos modos.

Es necesario realizar un seguimiento. Es necesario hacer cumplir la ley. Los tratados sobre armas nucleares, biológicas y químicas ofrecen algunos precedentes sobre cómo verificar el cumplimiento. Podemos y debemos esforzarnos por que eludir dichos tratados resulte difícil y costoso.

Será necesario aplicar estrictamente una prohibición internacional de la IA de vanguardia. Si algún Estado nación está decidido a seguir adelante a pesar de la presión internacional, puede que sea necesario el uso de la fuerza militar por parte de los países signatarios.

Esto no es lo ideal. Se debe hacer todo lo posible para dejar claro que se recurriría a la fuerza en tales situaciones, a fin de evitar errores de cálculo en los que se deba recurrir a la fuerza en la realidad. Pero si hay alguna causa que pueda justificar una acción militar limitada, o incluso una guerra, si un país que incumple el acuerdo decide intensificar la situación, salvar a la raza humana debería ser una de ellas.

#### **Este método ha funcionado antes.** {#este-método-ha-funcionado-antes.}

Han pasado más de ochenta años desde el desarrollo de la bomba atómica y la humanidad ha hecho un buen trabajo en la gestión de la proliferación nuclear. No ha habido ninguna guerra nuclear a gran escala, contrariamente a lo que predijeron muchos expertos tras la Segunda Guerra Mundial.

En junio de 2025, el Gobierno de los Estados Unidos incluso llevó a cabo un ataque limitado contra Irán con el fin de impedir su capacidad para fabricar armas nucleares. Este tipo de tratado y régimen de aplicación tiene precedentes en el orden mundial.

Si pudiéramos ganar ochenta años antes del desarrollo de la IA avanzada, eso podría ser suficiente.

### ¿Puede un régimen de supervisión durar para siempre? {#can-a-monitoring-regime-last-forever?}

#### **No. Se necesitará alguna otra vía de salida.** {#no.-se-necesitará-alguna-otra-vía-de-salida.}

Probablemente no se pueda detener por completo el progreso de la investigación en IA. Con *suficiente* tiempo, es probable que los investigadores acaben descubriendo métodos mucho más eficientes para crear IA.[^228] O tal vez, con suficiente tiempo, algún actor malintencionado acabe logrando socavar la prohibición.

Es muy probable que el tiempo arrastre a la humanidad hacia el futuro de una forma u otra. Y la humanidad se extinguirá, como la mayoría de las especies antes que ella, o de alguna manera navegará la transición hacia un mundo en el que existan cosas más inteligentes.

Pero la humanidad tampoco *necesita* ganar tiempo para siempre. La IA no es la única tecnología que está progresando. La biotecnología también está empezando a madurar, y si la humanidad logra impedir el desarrollo de máquinas de superinteligencia durante varias décadas, tendrá que hacer frente a trastornos como la ingeniería genética, que da lugar a seres humanos significativamente más inteligentes.

La pregunta es cuánto tiempo podemos ganar y qué podemos hacer con ese tiempo.

El problema básico al que se enfrenta la humanidad es cómo cruzar de forma segura la brecha entre la inteligencia humana y la superinteligencia. El mejor plan que se nos ocurre y que *quizás* tenga posibilidades de funcionar en la vida real es ganar tiempo para que la biotecnología aumente considerablemente la inteligencia humana, hasta el punto de que los futuros investigadores humanos sean *tan* inteligentes que nunca (por ejemplo) estimen que un proyecto de ingeniería se terminará a tiempo y por debajo del presupuesto a menos que *realmente sea así*.

Tan inteligentes que nunca se comprometerían con una teoría científica como el aristotelismo o el heliocentrismo, incluso si la sociedad que los rodea estuviera completamente convencida. Tan inteligentes que tendrían la oportunidad de navegar por la [brecha entre el antes y el después](#una-mirada-más-cercana-al-antes-y-el-después) en el primer intento.

Hay otros caminos posibles que podríamos imaginar, pero este tiene la ventaja de atacar el cuello de botella clave («la comunidad científica existente depende demasiado de los métodos de prueba y error y del incrementalismo para abordar este problema en particular»), utilizando tecnología que ya está empezando a estar disponible hoy en día, sin suponer un riesgo grave para el mundo.

#### **Un régimen de supervisión *no debería* durar para siempre.** {#un-régimen-de-supervisión-no-debería-durar-para-siempre.}

*En teoría*, es posible que la humanidad mantenga para siempre el equilibrio precario en el que se encuentra actualmente. Nuestra opinión es que esto requeriría un control draconiano de los pensamientos y las actividades de las personas. Pero, incluso si no fuera así, lo consideraríamos una mala opción.

Personalmente, creemos que los descendientes de la humanidad merecen convertirse en lo que deseen ser, explorar las estrellas y construir allí una civilización próspera y hermosa. Abogamos por la prohibición del desarrollo de la IA de vanguardia porque creemos que la superinteligencia es lo suficientemente peligrosa como para que sea necesario, no porque odiemos la IA, la tecnología o el progreso científico.

La verdadera pregunta es *cómo* llegar a un futuro maravilloso y cómo gestionar la transición desde aquí hasta allí.

Vale la pena destacar esto, en parte porque hay muchas personas que presentan la IA como una falsa dicotomía: dicen (falsamente) que la sociedad debe aceptar los riesgos de la IA y seguir adelante a toda máquina, o rechazar la IA y dejar que nuestra civilización se desvanezca para siempre en un solo planeta. Esto es [simplemente incorrecto](#rushing-ahead-destroys-those-benefits.). Hay otros caminos hacia el futuro, caminos que permiten un futuro igual de brillante, pero sin un riesgo tan alto de echarlo todo por la borda para nada. La humanidad debería encontrar otro camino hacia el futuro.

### ¿Por qué ayudaría hacer a los humanos más inteligentes? {#why-would-making-humans-smarter-help?}

#### **\* Podría ayudar a resolver el problema de la alineación.** {#*-it-could-help-with-solving-the-alignment-problem.}

El problema de la alineación de la IA no nos parece fundamentalmente irresoluble. Solo nos parece que los humanos no están ni mucho menos cerca de resolverlo y que no se encuentran en un nivel de inteligencia en el que *pensar* que tienen una solución se correlacione fuertemente con el hecho de *tener* realmente una solución.

Los investigadores en IA suelen reconocer que el problema de la alineación parece tremendamente difícil y que, hasta la fecha, se ha avanzado muy poco al respecto. Por eso resulta tan atractivo pensar que «quizás podamos conseguir que las IA hagan el trabajo de alineación por nosotros»: cuando eres investigador en IA y sientes que ni tú ni tus colegas están a la altura de resolver un problema determinado, lo más obvio es recurrir a la IA.

Pero, como comentamos en el capítulo 11 y en el [recurso en línea](#more-on-making-ais-solve-the-problem) asociado, incluso desde la perspectiva de un profano está claro que esta idea plantea muchos problemas: para que una IA descubra cómo resolver un problema profundo con el que los mejores investigadores humanos están teniendo grandes dificultades, tiene que ser lo suficientemente inteligente como para resultar peligrosa. Y como *nosotros* tenemos muy poca idea de lo que estamos haciendo, no disponemos de una fuente de información veraz que podamos utilizar para entrenar directamente capacidades de alineación limitadas, ni tenemos forma de comprobar si una propuesta de alineación generada por la IA es segura o eficaz.

El mundo puede plantearte problemas que están legítimamente fuera de tu alcance. La naturaleza no es un juego que solo te plantee retos «justos»; a veces puedes encontrarte con problemas que son demasiado difíciles de resolver incluso para los mejores científicos humanos, o demasiado difíciles de resolver en el plazo de tiempo requerido.

¿Existe un método más realista para traspasar todo el problema a alguna entidad más inteligente? Una opción sería hacer que los *humanos* fueran más inteligentes, de tal manera que pudieran resolver legítimamente el problema de la alineación. Los humanos vienen «prealineados» de una manera que las IA no lo están; los humanos más inteligentes tienen las mismas motivaciones prosociales básicas que el resto de nosotros.

En principio, parece posible que las personas sean capaces de distinguir entre lo que *parece* una gran iluminación alquímica que les permitirá transmutar el plomo en oro, y el tipo de conocimiento que *realmente corresponde* a la capacidad de transmutar el plomo en oro (utilizando la física nuclear para eliminar algunos neutrones de los átomos de plomo). Sin duda, deberían percibirse como estados de conocimiento diferentes.

Pero a los ingenieros humanos reales les cuesta mucho distinguir en qué zona se encuentran. En la historia real de la química, el nivel de habilidad de la humanidad era tal que los alquimistas eran engañados con facilidad.

En el mundo real, los científicos se aferran a sus teorías favoritas y se niegan a revisar sus opiniones hasta que la realidad les golpea repetidamente en la cabeza con un «tu teoría era errónea», y a veces se niegan a cambiar de opinión incluso entonces: A veces se dice que la ciencia avanza «[un funeral tras otro](https://en.wikipedia.org/wiki/Planck%27s_principle)», porque la vieja guardia nunca cambiará de opinión y solo queda esperar a que la nueva guardia madure. Pero esto no es una limitación fundamental impuesta por la naturaleza; es solo una cuestión de que los seres humanos, como clase, no son lo suficientemente inteligentes, cuidadosos y conscientes de sí mismos.

Por lo general, no pasa nada si los seres humanos son ingenuos en estos aspectos, porque normalmente la realidad es bastante indulgente con los errores, al menos en el sentido de que no aniquila a *toda la humanidad* por la arrogancia de un alquimista. Pero [ese no es un lujo que la humanidad pueda permitirse](#a-closer-look-at-before-and-after) cuando se trata de la alineación de la superinteligencia artificial.

La humanidad suele adquirir sus conocimientos luchando, intentándolo, fracasando y acumulando conocimientos poco a poco. Pero no tiene por qué ser así.

Einstein no solo fue capaz de descubrir la relatividad general, sino que lo hizo «reflexionando intensamente sobre el problema», incluso antes de que la humanidad pusiera satélites en órbita y comenzara a ver con tus propios ojos las discrepancias en sus relojes (como se explica en el capítulo 6). Tenía evidencia empírica, pero fue capaz de dar con la respuesta correcta de manera eficiente en respuesta a los primeros susurros silenciosos de los registros empíricos, en lugar de esperar a que la verdad llamara a tu puerta.

Ese camino es más raro y difícil de recorrer, pero ese tipo de genio científico existe, aunque sea en raras ocasiones, incluso entre los mejores y más brillantes del mundo.

Los seres humanos que superan en uno o dos pasos el nivel de investigadores como Einstein o [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) podrían empezar a descubrir con precisión vuestros propios defectos y corregirlos de docenas de maneras diferentes.

Podrían darse cuenta de cuándo estaban racionalizando o cayendo víctimas del [sesgo de confirmación](https://en.wikipedia.org/wiki/Confirmation_bias). Podrían superar el punto de tener la expectativa de que una idea que suena inteligente funcione cuando en realidad no es así, hasta el punto de que, cada vez que tienes la expectativa de tener éxito, *realmente* lo consigues. Podrían alcanzar un nivel de competencia en el que sigan cometiendo muchos errores, pero sin tener una confianza excesiva (o una falta de confianza) [sistemática](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) en nuevos ámbitos complicados.

¿Es realmente posible mejorar la inteligencia humana? A nosotros nos lo parece, tras haber hablado con varios investigadores biotecnológicos que creen que hay ángulos de ataque prometedores a corto plazo. Una IA cuidadosamente orientada a la biotecnología también podría ayudar a acelerar el trabajo. Pero, desde nuestra perspectiva, sigue siendo muy incierto si un plan como este daría realmente resultado. Lo que sí podemos afirmar con más seguridad es que se trata de una opción con un efecto multiplicador elevado que merece mucha más inversión y exploración de la que está recibiendo actualmente.

No recomendamos mejorar la inteligencia humana como la única estrategia posthumana en la que creemos que la humanidad debería invertir fuertemente. Más bien, este es solo uno de muchos ejemplos, y el que actualmente consideramos más prometedor. Recomendamos encarecidamente que la humanidad estudie múltiples caminos posibles que no pasen por la IA, en lugar de poner todos los huevos en la misma cesta.

#### **Los humanos aumentados no plantean un problema importante de «alineación humana».** {#los-humanos-aumentados-no-plantean-un-problema-importante-de-«alineación-humana».}

Los humanos aumentados tendrían esencialmente la misma arquitectura cerebral, emociones, etc., que el resto de nosotros. Con la IA, incluso la IA entrenada para [*sonar como*](https://docs.google.com/document/d/1FbehxsMqTaw2seAzUJw4ny0tvZgUdbHJnCUi_Pbd5VU/edit?tab=t.k1kf1fy9gx5i#heading=h.8cxst4c4x63a), existe una enorme brecha de diferencias cognitivas y motivacionales, y una brecha de comprensibilidad igualmente grande; con humanos modestamente más inteligentes, nada de eso parece particularmente probable.

Los investigadores con mejora de la cognición no necesitarían mantener su propia integridad mental mientras se convierten en superinteligencias con mentes millones de veces más grandes. Solo necesitarían elevarse al nivel necesario para descubrir cómo *construir* —no desarrollar— superinteligencias artificiales que fueran verdaderamente alineadas y estables.

Es posible que siga existiendo un problema de la alineación humana en el sentido débil de que cualquier esfuerzo por coordinar a varias personas puede tropezar con problemas del agente-principal y de incentivos. Y estos problemas son, por naturaleza, mucho más importantes en cualquier grupo encargado de crear superinteligencia.

Esperamos que estos problemas sean tratables siempre y cuando los humanos comiencen siendo visiblemente altruistas y caritativos, siempre y cuando su inteligencia se mejore solo lentamente, y siempre y cuando trabajen en una institución bien diseñada con incentivos bien diseñados. Pero es totalmente razonable que la gente se preocupe por la posibilidad de que se produzca una toma de poder en este ámbito. Resolver estos problemas no sería necesariamente fácil, pero no sería tan inviable como que las empresas intentaran desarrollar superinteligencias inescrutables con mentes totalmente incomprensibles e impulsos inhumanos.

Crear un equipo de élite de supergenios modificados genéticamente para ayudar a navegar el planeta de forma segura a través de la transición a la superinteligencia es definitivamente el tipo de cosa que la humanidad debería hacer con cuidado, dado lo mucho que hay en juego en tal empresa. Una medida como esta conlleva diversas cuestiones prácticas y éticas, pero estas deben sopesarse con el costo de dejar que la superinteligencia nos mate a todos, si no hay otras soluciones igualmente prometedoras.

Los tiempos drásticos pueden requerir medidas drásticas, pero la mejora (modesta) de la inteligencia humana ni siquiera es una medida que parezca particularmente drástica. Parece una tecnología netamente positiva en sí misma, que tiene al menos alguna posibilidad de ayudar a la humanidad en más de un sentido.
