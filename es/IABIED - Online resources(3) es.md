### ¿No tendría que desarrollarse la IA hasta convertirse en toda una civilización antes de poder ser peligrosa? {#¿no-tendría-que-desarrollarse-la-ia-hasta-convertirse-en-toda-una-civilización-antes-de-poder-ser-peligrosa?}

#### **Con las computadoras, lo difícil es conseguir que resuelvan un problema concreto. La escala y la velocidad llegan poco después.** {#con-las-computadoras,-lo-difícil-es-conseguir-que-resuelvan-un-problema-concreto.-la-escala-y-la-velocidad-llegan-poco-después.}

«Para conquistar el mundo, se necesita una civilización» es una intuición que tiene sentido para los humanos. No está tan claro hasta qué punto esta idea se puede generalizar a la IA. Las IA no funcionan como los humanos: pueden ser enormemente más capaces que cualquier humano, y una instancia de IA no es necesariamente comparable a una sola persona.

También vale la pena tener en cuenta que la superinteligencia es precisamente el tipo de cosa que puede generar un análogo de toda una civilización de forma extremadamente rápida.

Con la mayoría de las hazañas que pueden realizar las computadoras, no se tarda mucho en pasar de «las computadoras pueden hacer esto» a «las computadoras pueden hacer esto a una escala enorme, mucho más rápido que cualquier humano». Pensemos, por ejemplo, en las calculadoras.

Hubo años en los que solo las computadoras de gama alta podían realizar reconocimiento de voz, procesamiento de vídeo o gráficos 3D en tiempo real, pero no fueron muchos.

Las IA, al igual que el software tradicional, pueden copiarse rápidamente en tantas computadoras como haya disponibles. Y pueden fabricarse más computadoras a ritmo industrial.

Comparemos esta situación con la de los seres humanos. Crear y formar a un nuevo ser humano requiere recursos considerables y décadas. Una vez que se dispone de una IA con un nivel de capacidad determinado, se puede copiar inmediatamente esa misma IA entrenada y «adulta» tantas veces como se desee, con un gasto mínimo.

En cierto sentido, toda una (pequeña) civilización de mentes de IA ya existe en el momento en que una empresa lanza un nuevo modelo a sus centros de datos y pone en marcha tantas instancias como sea necesario para satisfacer la demanda.[^175] Hoy en día, esas flotas de IA no funcionan todas en armonía. Pero las empresas sí utilizan [grupos de agentes paralelos](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&t=348) cuando buscan el máximo rendimiento a cualquier precio.

Todo esto significa que probablemente no pasará mucho tiempo entre que las IA sean lo suficientemente inteligentes como para tomar el poder si tuvieran un millón de instancias, y que tengan al menos ese número de instancias ejecutándose. El tipo de crecimiento poblacional que a los humanos les lleva cientos de años, con la IA puede ocurrir en cuestión de minutos.

En lo que respecta a la infraestructura física de la civilización, suponemos que la IA puede aprovechar de forma productiva la infraestructura humana durante el tiempo que sea necesario para desarrollar medios más avanzados de reorganizar la materia según sus preferencias. No necesita averiguar cómo fabricar su propia cadena de suministro e infraestructura de computación desde cero cuando puede utilizar nuestras computadoras. No necesita inventar máquinas industriales desde cero cuando puede simplemente tomar el control de las máquinas industriales que ya hemos construido. Y puede utilizar nuestra infraestructura para construir la siguiente fase de su propia infraestructura, utilizando los robots existentes para construir fábricas de robots nuevas y más eficientes, o utilizando los laboratorios de síntesis de ADN existentes para crear su propia biotecnología, hasta que sea completamente autosuficiente.

Los seres humanos somos la clase de entidad que, partiendo de estar desnudos en la sabana, construyó por sus propios medios una civilización tecnológica. Y no somos *tan* inteligentes. Para una superinteligencia no sería una hazaña tan difícil de replicar, sobre todo si puede aprovechar la base industrial de la humanidad como punto de partida.

### ¿No se verán limitadas las IA por su capacidad para diseñar y realizar experimentos? {#won’t-ais-be-limited-by-their-ability-to-design-and-run-experiments?}

#### **La inteligencia permite aprender más de los experimentos y ejecutarlos de forma más rápida, informativa y paralelizada.** {#la-inteligencia-te-permite-aprender-más-de-los-experimentos-y-realizar-experimentos-más-rápidos,-más-informativos-y-más-paralelizados.}

Una civilización de mentes motivadas que piensan mil veces más rápido que la humanidad no necesariamente sería capaz de generar datos de salida tecnológicos mil veces más rápido que los humanos.

Por analogía: si pasas tres horas haciendo la compra y dos de ellas las dedicas a ir y volver de la tienda a caballo, un coche diez veces más rápido puede acelerar el viaje, pero no por un factor de diez. Al final, la hora que pasas en la tienda acaba dominando el tiempo total.

Incluso una civilización llena de pensadores increíblemente inteligentes debe esperar ocasionalmente a que lleguen los resultados de los experimentos. Si tus pensamientos son lo suficientemente rápidos, es probable que el cuello de botella pase a ser la rapidez con la que puedes actuar en el mundo, la rapidez con la que puedes asimilar la información y el tiempo que tardan tus planes en desarrollarse.

Pero no es tan malo como podría hacerte creer la analogía de la tienda de comestibles, porque la capacidad de pensar contrarresta la necesidad de resultados experimentales:

* A menudo, basta con pensar más y mejor para obviar la necesidad de una prueba, ya que uno se da cuenta de que las observaciones anteriores ya contienen la respuesta. Compárese, por ejemplo, la capacidad de las IA modernas para [aprender a pilotar robots](https://arxiv.org/abs/1905.00741) utilizando la [simulación pura](https://www.figure.ai/news/reinforcement-learning-walking).  
* A veces puedes reflexionar más hasta encontrar una prueba igualmente fiable pero más rápida.  
* A veces es posible realizar muchas pruebas más rápidas, aunque menos fiables, que pueden ejecutarse repetidamente en paralelo para obtener resultados de fiabilidad similar a mayor velocidad.  
* A veces se pueden realizar muchas pruebas complicadas a la vez, lo que genera datos complejos y difíciles de interpretar; esto representa una contrapartida aceptable si la cognición necesaria para descifrarlos es menos costosa (desde la perspectiva de una mente extremadamente rápida) que realizar múltiples pruebas.  
* A veces puedes encontrar una forma de construir otros dispositivos que realicen los experimentos mucho más rápido. Por ejemplo, en lugar de enviar muchas solicitudes diferentes a un laboratorio biológico para que sinteticen medicamentos, ¿puedes encontrar una forma de enviar *una* solicitud a un laboratorio biológico, lo que dará como resultado la síntesis de *una sola bacteria* que contenga el código genético para producir todos los medicamentos que deseas sintetizar? Del mismo modo, ¿puedes crear una bacteria que sea sensible a las señales de radio y que responda rápidamente a las instrucciones de una IA rápida —mucho más rápido que los exasperantemente lentos humanos que van y vienen siguiendo tus instrucciones—?  
* Y, a veces, simplemente puedes considerar tus diez mejores conjeturas, averiguar qué harías en cada uno de esos casos, construir un dispositivo complicado que funcione independientemente de cómo sea la realidad y saltarte las pruebas por completo.

Una civilización llena de copias de Steve Jobs, Marie Curie, John von Neumann y algunos de los mejores trabajadores y programadores del mundo —si estos operasen 10 000 veces más rápido que nosotros— *se daría cuenta* de que el principal cuello de botella era la espera de los resultados experimentales, y podría *abordar ese cuello de botella* para reducirlo.

La historia del [Proyecto Genoma Humano](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) es un buen ejemplo de lo que ocurre cuando un grupo de personas inteligentes identifica y trabaja continuamente en los cuellos de botella de un proyecto de investigación masivo. El proyecto, que se esperaba que durara quince años y costara 3000 millones de dólares, finalizó dos años antes de lo previsto y 300 millones por debajo del presupuesto; la mayor parte del genoma se cartografió en los dos últimos años utilizando métodos y equipos mejorados.

Al igual que esto es válido para los seres humanos, también lo es para la IA. Un razonador inteligente no tiene por qué quedarse de brazos cruzados mientras espera durante años subjetivos a que las lentas pruebas se arrastren hasta su conclusión. Un razonador sobrehumano *considera vías alternativas* y es experto en encontrarlas: en eso consiste la inteligencia.

Para obtener una pequeña evidencia práctica al respecto, consideremos el caso de los seres humanos que realizan experimentos. Un buen caso de estudio aquí es el del software frente a las sondas espaciales. Realizar cambios en un producto de software es barato y rápido, y los ingenieros de software tienden a experimentar constantemente, a producir software que aún no funciona del todo y luego a corregir sus fallos más importantes. Por el contrario, la experimentación es muy costosa en las sondas espaciales, por lo que los seres humanos dedican mucho tiempo a perfeccionar la sonda espacial y a embutir en ella tantos experimentos como les es posible. Se esfuerzan mucho por dotar a las sondas espaciales de *maquinaria experimental general* que pueda controlarse a distancia, de modo que, si se les ocurre una nueva idea para un experimento, no tengan que inventar y lanzar una nave espacial completamente nueva.

Y, por encima de todo, un razonador lo bastante inteligente también tiene la opción de simplemente *descubrir cómo es la realidad sin necesitar tantísimos experimentos*. A veces, los datos que ya tienes son suficientes, si eres lo suficientemente inteligente como para interpretarlos.

Como caso de estudio: la [teoría de la relatividad general](http://eotvos.dm.unipi.it/documents/EinsteinPapers/Einstein1911English.pdf) de Einstein tardó ocho años en ser contrastada empíricamente con nuevos datos. La prueba fue realizada por Frank Watson Dyson y Arthur Stanley Eddington, quienes [fotografiaron](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) las estrellas detrás del sol durante un eclipse solar total y midieron el grado en que la luz se curvaba alrededor del sol; descubrieron que coincidía exactamente con la teoría de Einstein.

Pero esa espera de ocho años no impidió ningún progreso científico real.

![][image12]

\[ FUENTE: [https://en.wikipedia.org/wiki/Eddington\_experiment](https://en.wikipedia.org/wiki/Eddington_experiment) \]

Una de las razones es que la teoría de Einstein era claramente correcta: ya había sido validada con datos como el movimiento del perihelio de Mercurio, que la teoría de Newton había predicho de forma inexacta y la de Einstein, con precisión. Los científicos no consideraron esta predicción como un acierto porque los datos se habían recopilado antes de que Einstein planteara su teoría, y querían validar las predicciones que su teoría hacía antes de ver los datos. Pero este es el tipo de muleta que necesita una civilización cuando tiene graves problemas con el [sesgo retrospectivo](https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science), [sesgo de confirmación](https://en.wikipedia.org/wiki/Confirmation_bias) y científicos que hacen trampa para [exagerar la evidencia de sus hipótesis](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation). Ninguna de estas características es necesaria para un buen razonamiento. De hecho, los pensadores cuidadosos pudieron determinar si la teoría de Einstein era correcta mucho antes del experimento de Eddington, utilizando la evidencia de la que ya disponían.

Además, existían métodos más rápidos para poner a prueba la teoría —como construir telescopios y observar los efectos de los agujeros negros, tal y como predecía la teoría de Einstein—, algo que presumiblemente una civilización lo suficientemente competente y de pensamiento rápido podría haber hecho en menos de ocho años. O si ya se dispusiera de capacidad de vuelo espacial, se podrían comprobar los relojes de los satélites en menos de un día. Suponer que la teoría de Einstein *requiriera* ocho años para ser puesta a prueba sería subestimar radicalmente el poder de la inteligencia.

Cuando la humanidad finalmente se dispuso a construir satélites GPS, estos se programaron con dos relojes diferentes: uno que utilizaba la teoría de Einstein y otro que no. Fue una elección extraña, dado lo bien confirmada que estaba la teoría de Einstein en ese momento. Pero esta elección subraya que, en muchos casos, una civilización puede simplemente *tomar ambas ramas* cuando no está segura de una teoría. También pone de relieve que, cuando los experimentos y los fracasos son costosos (como en el caso de los satélites), a menudo es mucho más barato construir cosas de manera que no dependan demasiado de ninguna teoría en particular.

Y, como señalamos en el libro, Einstein (en comparación con Newton, Kepler y Brahe antes que él) es también un ejemplo de cómo las personas inteligentes pueden deducir mucho más de lo que cabría esperar a partir de observaciones muy limitadas. Einstein es impresionante no solo por descubrir la teoría de la relatividad, sino por hacerlo a partir de *tan pocos datos*.

Así que, aunque la necesidad de datos experimentales puede limitar la rapidez con la que la IA puede realizar diversas acciones, es probable que esta limitación sea mucho más débil de lo que podría parecer intuitivamente.

## Discusión ampliada {#extended-discussion-6}

### Nanotecnología y síntesis de proteínas {#nanotechnology-and-protein-synthesis}

La inteligencia humana nos ha proporcionado muchas ventajas sobre otras especies. Sin embargo, una de las más trascendentales ha sido nuestra capacidad para inventar nuevas tecnologías. Si los desarrolladores se adelantan y crean una IA más inteligente que los humanos, podemos esperar que gran parte del poder de la IA provenga de su capacidad para hacer avanzar las fronteras científicas y tecnológicas. Pero, concretamente, ¿cómo se vería esto? ¿Qué tecnologías aún por inventar están esperando a ser descubiertas?

Es una pregunta difícil de responder con carácter general. A un científico de 1850 le habría resultado muy difícil prever muchos de los inventos de los cien años siguientes.

Sin embargo, no estarían totalmente indefensos. Los científicos han predicho muchos inventos décadas o siglos antes de que se construyeran, en casos en los que era posible razonar sobre los aspectos técnicos de una tecnología antes de que los ingenieros pudieran poner todas las piezas en su sitio.[^176]

Una de las fronteras tecnológicas más impactantes que creemos que la IA probablemente explorará es el desarrollo de herramientas y máquinas extremadamente pequeñas. A continuación, entraremos en algunos detalles sobre este tema y el razonamiento básico que lo sustenta.

#### **El ejemplo de la biología** {#el-ejemplo-de-la-biologia}

Cada célula de cada organismo en la naturaleza contiene una enorme variedad de maquinaria intrincada.

En este caso, «maquinaria» no es solo una metáfora. Las máquinas en cuestión son pequeñas, por lo que funcionan de forma ligeramente diferente a las máquinas de tu vida cotidiana. Sin embargo, muchas máquinas a gran escala tienen equivalentes en nuestro organismo. La [ATP sintasa](https://en.wikipedia.org/wiki/ATP_synthase) genera energía en el organismo de forma similar a una rueda hidráulica, utilizando un flujo de protones para hacer girar un auténtico rotor.

\[embed video or gif: [https://en.wikipedia.org/wiki/File:ATP\_synthesis\_-\_ATP\_synthase\_rotation.ogv](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv)\]

El flagelo bacteriano funciona de manera similar a la hélice de un barco, pues cuenta con todo un motor funcional que lo hace girar para propulsar a la bacteria a través de los líquidos:

\[embed video: [https://www.youtube.com/watch?v=cwDRZGj2nnY](https://www.youtube.com/watch?v=cwDRZGj2nnY)\]

Otro ejemplo, que mencionamos en el libro, es la kinesina —una pequeña proteína que funciona como un robot de carga. Las kinesinas «caminan» por fibras autoensambladas que atraviesan las neuronas, transportando neurotransmisores a su destino.

\[incrustar vídeo o gif: [https://www.youtube.com/watch?v=y-uuk4Pr2i8](https://www.youtube.com/watch?v=y-uuk4Pr2i8)\]

Cuanto más pequeña es una máquina, por lo general, más rápido puede funcionar; y las máquinas tan pequeñas como las moléculas funcionan muy rápidamente. Las kinesinas dan hasta [200 pasos por segundo](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004\(04\)00103-3), avanzando con un «pie» mientras el otro se aferra al microtúbulo sobre el que se encuentra.[^177]

Una de las fronteras tecnológicas que podría explorar una IA más inteligente que los humanos es la construcción, el diseño o la reutilización de máquinas a una escala muy pequeña. Este tipo de tecnología podría clasificarse como «biotecnología», «nanotecnología» o algo intermedio, dependiendo de factores como la escala, el grado de similitud del diseño con las estructuras biológicas existentes y si es «húmeda» (dependiente del agua, como la maquinaria de las células vivas) o «seca» (capaz de funcionar al aire libre).

Pensar en los organismos biológicos como maravillas de la ingeniería a nanoescala puede darnos pistas sobre lo que podrían lograr las IA más inteligentes que los humanos, con una ciencia y tecnología más avanzadas que las que poseemos en la actualidad.

Hay otra cuestión aparte sobre cuánto tiempo llevaría inventar y desarrollar dicha tecnología. Para más información sobre este tema, véase el debate en el capítulo 1 del libro sobre cómo las superinteligencias artificiales probablemente serían capaces de pensar al menos 10 000 veces más rápido que los humanos con el hardware existente. Véase también nuestro análisis ampliado sobre cómo [las IA tendrían que dedicar algún tiempo a realizar pruebas y experimentos físicos, pero la ralentización general probablemente no supondría un gran obstáculo para una superinteligencia](#won’t-ais-be-limited-by-their-ability-to-design-and-run-experiments?).

Si nos fijamos en las proezas de la ingeniería humana actual, puede parecer difícil de creer que, por ejemplo, una IA con capacidades sobrehumanas al frente de un laboratorio biológico pueda construir fábricas microscópicas que utilicen la luz solar para replicarse una y otra vez. Puede parecer aún más fantástico imaginar microfábricas de uso general, es decir, fábricas que puedan aceptar instrucciones para construir casi cualquier máquina a partir de los recursos disponibles.

Pero máquinas como esas no solo son posibles, sino que ya existen. Las [algas](https://en.wikipedia.org/wiki/Algae) son fábricas de una micra de ancho, alimentadas por energía solar y autorreplicantes, que pueden duplicar su población en menos de un día. Además, las algas contienen [ribosomas](https://en.wikipedia.org/wiki/Ribosome), que son la versión biológica de una impresora 3D universal o una cadena de montaje universal (universal, al menos, en lo que se refiere a los componentes básicos de la vida).

Con el conjunto adecuado de instrucciones (codificadas en el ARN mensajero), los ribosomas imprimen estructuras arbitrarias que pueden ensamblarse a partir de [proteínas](https://en.wikipedia.org/wiki/Protein). Esta universalidad sustenta la enorme complejidad y variedad del mundo biológico: toda la diversidad de la vida en la Tierra es ensamblada en última instancia por estas fábricas universales, que se encuentran prácticamente sin cambios en todo, desde los puercoespines hasta las moscas de la fruta y las bacterias.

\[embed video: [https://www.youtube.com/watch?v=8dsTvBaUMvw](https://www.youtube.com/watch?v=8dsTvBaUMvw)\]

Los ribosomas pueden incluso utilizarse para ensamblar estructuras que no están compuestas por proteínas, utilizando proteínas como intermediarias. Un ejemplo de estructura no proteica que los ribosomas pueden construir de esta manera es el hueso. Los ribosomas producen proteínas que se pliegan formando enzimas débilmente unidas que catalizan el calcio y el fósforo para convertirlos en reactivos especiales. Luego, estos reactivos forman una matriz de colágeno que guía el calcio y el fósforo hasta su lugar para convertirlos en hueso duro y cristalino.

La naturaleza proporciona una prueba de existencia de que son posibles algunas máquinas físicas verdaderamente extraordinarias, para entidades lo suficientemente inteligentes como para utilizar los ribosomas de maneras que los humanos no han logrado —o entidades que utilizan los ribosomas para construir sus propios análogos mejorados de los ribosomas.

Pero las estructuras que vemos en el mundo biológico solo establecen un límite inferior para lo que es posible. Los organismos biológicos están muy lejos de los límites teóricos de eficiencia energética y resistencia de los materiales, y pueden ser relativamente fáciles de mejorar para entidades mucho más inteligentes que los humanos.

#### **Hay mucho espacio en el fondo** {#hay-mucho-espacio-en-el-fondo}

Si te parece extraño utilizar los fenómenos naturales como evidencia de qué tecnologías futuras son factibles, ten en cuenta que se trata de un patrón habitual en la historia de la ciencia. Las aves podían volar, por lo que los inventores pasaron siglos intentando construir máquinas voladoras.

Richard Feynman, un físico pionero, demostró el poder de este enfoque en una conferencia de 1959 titulada «Hay mucho sitio en el fondo» (https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf). En la conferencia, Feynman calcula qué tipo de cosas interesantes se podrían hacer con la miniaturización.

Hoy en día, las observaciones de Feynman parecen notablemente proféticas. Feynman señala que las computadoras probablemente podrían hacer mucho más si contuvieran más elementos, pero que el obstáculo para ello es lo grandes que tendrían que ser. ¡Deben miniaturizarse!

Feynman calcula que se necesitaría alrededor de un petabit (1 000 000 000 000 000 bits) para almacenar todos los libros escritos por la humanidad:

> Para cada bit, asigno 100 átomos. Y resulta que toda la información que el hombre ha acumulado cuidadosamente en todos los libros del mundo puede escribirse de esta forma en un cubo de material de una doscentésima de pulgada de ancho, que es la partícula de polvo más pequeña que puede distinguir el ojo humano. ¡Así que hay mucho espacio en el fondo! ¡No me hables de microfilmes!

¡Aun hoy no lo hemos conseguido del todo! El elemento de almacenamiento real dentro de una tarjeta microSD de 2 terabytes sigue siendo de 0,6 milímetros por lado. A modo de referencia, 1/200 de pulgada equivaldría a 0,125 mm por lado. Y la tarjeta SD solo almacena 17,6 billones de bits, lo que es apenas 1/57 de lo que Feynman calculó que necesitaríamos para almacenar todo el conocimiento de la humanidad en 1959.

¿Quizás Feynman se equivocó sobre los límites últimos de la ingeniería en un sentido práctico? Últimamente, los avances en la miniaturización de la computación se han ralentizado bastante. Decir que algo es físicamente posible no es prueba de que los ingenieros sean capaces de hacerlo.

El haberse quedado a tres órdenes de magnitud de lo que algún día se lograría puede considerarse toda una hazaña predictiva por parte de Feynman. Feynman pronunció su conferencia seis años antes de que Gordon Moore planteara por primera vez la idea que ahora llamamos la [Ley de Moore](https://en.wikipedia.org/wiki/Moore%27s_law). La gente no estaba acostumbrada a pensar en la miniaturización como una ley inexorable en un gráfico. No conocemos a nadie más en la época de Feynman que especulara con que algún día pudiera existir un dispositivo cuyo elemento de almacenamiento, del tamaño de un grano de arena, pudiera guardar diez millones de veces más información que las computadoras de tubos de vacío más grandes de la década de 1950.

![][image13]  
\[Fuente: [https://www.ibm.com/history/700](https://www.ibm.com/history/700)\]

Pero, en realidad, Feynman no se equivocaba. Y Feynman ya sabía en ese momento que su estimación era conservadora:

> Este hecho —que se pueda albergar una enorme cantidad de información en un espacio extremadamente pequeño— es, por supuesto, de sobra conocido por los biólogos [...] toda esta información está contenida en una fracción muy pequeña de la célula, en forma de moléculas de ADN de cadena larga en las que se utilizan aproximadamente cincuenta átomos por cada bit de información sobre la célula.

Las computadoras modernas aún no se han miniaturizado hasta alcanzar la escala del ADN, pero en sesenta años nos hemos acercado notablemente. Las puertas de los transistores de los chips comerciales de alta gama miden ahora menos de cien átomos de ancho y se construyen con una tecnología que permite añadir capas de material [de un solo átomo de espesor](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

El anclaje a los análogos naturales y los cálculos físicos de servilleta resultaron ser una guía excepcionalmente sólida de lo que se lograría en las próximas décadas. Y las trayectorias tecnológicas como estas pueden ir mucho más rápido cuando las IA realizan el trabajo científico y de ingeniería necesario.

#### **Superando la biología** {#outdoing-biology}

¿Por qué la carne no puede ser tan fuerte como el acero?

Al fin y al cabo, en el fondo son los mismos átomos. Los enlaces metálicos entre los átomos de hierro son duros, pero también lo son los enlaces covalentes entre los átomos de carbono del diamante; ¿por qué no evolucionamos para tener una cota de malla de diamantes recorriendo nuestra piel, que nos ayudara a sobrevivir hasta la edad reproductiva?

A ese respecto, si el hierro es tan fuerte, ¿por qué los organismos no evolucionarían para comer mineral de hierro y desarrollar pieles blindadas con hierro? Si los ingenieros humanos pueden hacerlo, ¿por qué la naturaleza no lo hizo primero?

Quizás haya alguna razón situacional por la que las pieles acorazadas en particular no sean una buena idea.

De no ser así, ¿por qué no otra cosa?

La gran pregunta de fondo es: ¿por qué la naturaleza está lejos de los límites de lo físicamente posible —según los cálculos de la física o las demostraciones de la ingeniería humana—? ¿Existe una respuesta profunda y general, y no solo una superficial y limitada?

Hemos señalado que Feynman utilizó estructuras de la biología para establecer cotas inferiores de lo que debería ser posible con un mayor conocimiento científico. Sin embargo, en muchos casos la tecnología humana ya ha superado a la biología. ¿Cómo es posible, si la evolución ha tenido miles de millones de años para mejorar a plantas y animales? Comprender este fenómeno general puede ayudar a esclarecer por qué la nanotecnología probablemente pueda ir mucho más allá de lo que vemos hoy en la naturaleza.

Podemos imaginar un mundo en el que las secuoyas alcancen al menos la mitad de la altura de los edificios más altos. Podemos imaginar un mundo en el que la piel de los animales más resistentes sea al menos la mitad de dura que los materiales más duros que se han observado. ¿Por qué no nos encontramos en un mundo así, en el que la naturaleza ha apurado los límites físicos tras miles de millones de años de evolución?

Esta es una pregunta tan profunda que no podemos resumir brevemente todo lo que se sabe al respecto. Pero, a grandes rasgos, la selección natural tiene dificultades para acceder a algunas partes del espacio de diseño, incluidas muchas partes que son mucho más fáciles de alcanzar si eres un ingeniero humano.

Los tres factores principales que contribuyen a esto son:

1. La selección natural dispone de una presión selectiva limitada y necesita cientos de generaciones para que una nueva mutación se universalice. Si una característica biológica no es muy, muy antigua, su diseño a menudo parece limitado por el tiempo, como hecho con prisas.  
2. Todo lo construido por la selección natural comenzó como un error accidental en algún diseño anterior: una mutación. A la evolución le cuesta más explorar partes del espacio de diseño que están *alejadas* de lo que *existe actualmente* en los organismos. Es difícil para la evolución salvar las brechas.  
3. A la selección natural le cuesta crear cosas nuevas o solucionar problemas que requieran cambios simultáneos en lugar de cambios secuenciales. Esto limita enormemente los diseños a los que puede acceder la evolución y da a los diseños actuales en biología su aspecto fragmentario, chapucero y enormemente enredado según los estándares de la ingeniería humana. Por ejemplo, la complejidad (de las partes conocidas) del metabolismo humano: \[IMG TODO: la fuente es [https://www.reddit.com/r/MapPorn/comments/6sxznd/map\_of\_the\_known\_human\_metabolic\_pathways/](https://www.reddit.com/r/MapPorn/comments/6sxznd/map_of_the_known_human_metabolic_pathways/) \]

![][image14]  
O, para un ejemplo más sencillo de lo caótica que es la evolución, consideremos el ojo. Sucedió que los ojos de los vertebrados evolucionaron con sus nervios (2 en la imagen de abajo) situados encima de las células detectoras de luz (1). Estos nervios necesitan salir del ojo a través de un orificio en la parte posterior (3), y como este punto tiene un orificio, carece de células detectoras de luz. Esto crea un punto ciego (4) en todos los vertebrados, incluidos los humanos, lo que obliga al cerebro a recurrir a ingeniosos trucos para «rellenar» el hueco (por ejemplo, con información del otro ojo).

Los pulpos desarrollaron los ojos de forma independiente y, por casualidad, evolucionaron con el diseño más sensato: los nervios van por detrás de las células que detectan la luz, lo que permite que salgan del ojo sin crear ningún punto ciego.

![][image15]  
\[img src: [https://en.wikipedia.org/wiki/Evolution\_of\_the\_eye](https://en.wikipedia.org/wiki/Evolution_of_the_eye)\]

O consideremos el nervio laríngeo recurrente de la jirafa, que conecta la garganta con el cerebro para poder accionar la laringe. En lugar de tomar la ruta directa, este nervio desciende desde la garganta a lo largo de todo el cuello, da una vuelta torpe alrededor de la aorta, vuelve a subir por el cuello hasta su punto de origen y entonces se conecta al cerebro.

El resultado es un nervio de cuatro metros y medio de largo (el bucle negro de la imagen inferior), lo que provoca que las señales tarden entre diez y veinte veces más de lo necesario en viajar entre el cerebro y la garganta de la jirafa.[^178]

![][image16]  
\[Fuente: [https://en.wikipedia.org/wiki/Recurrent\_laryngeal\_nerve](https://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve)\]

En los peces, este diseño tenía sentido porque su versión del nervio laríngeo conectaba el cerebro con las branquias, en línea recta. Pero tomemos el mismo diseño, démosle un cuello al animal y sigamos alargando el cuello sin rehacer nunca el cableado desde cero, y obtendremos unos diseños muy ineficientes. Permiten la supervivencia, pero son ineficientes.

La evolución produce diseños maravillosos, si se le da tiempo suficiente. Pero los seres humanos y las inteligencias artificiales pueden concebir una gama de diseños mucho más variada y flexible, y pueden hacerlo con gran rapidez.

Los primeros organismos multicelulares con células diferenciadas y especializadas parecen haber evolucionado hace unos 800 millones de años. En términos humanos, eso parece una eternidad. Pero la evolución avanza mucho más lentamente que la civilización humana.[^179]

Un gen recién mutado que transmite una ventaja reproductiva del tres por ciento —relativamente enorme, para ser una mutación— tardará una media de 768 generaciones en propagarse por una población de 100 000 organismos que se cruzan entre sí. Si el tamaño de la población es de 1 000 000 (la población humana estimada en la época de los cazadores-recolectores), se necesitarán 2763 generaciones. Y la probabilidad de que la mutación se propague hasta fijarse, en lugar de desaparecer aleatoriamente, es solo del 6 %.^([180])

En genética de poblaciones, la regla general es «una mutación, una muerte». Si los errores al copiar el ADN introducen diez copias de una mutación deletérea en cada nueva generación, entonces diez portadores de esa mutación deben morir o no reproducirse en cada generación para contrarrestar la presión del mero ruido genético.

Esto no es tan malo como parece, como costo de mantener la información genética. En una especie de reproducción sexual, un individuo (o un embrión) puede acabar siendo portador de muchas mutaciones perjudiciales y morir —o no reproducirse, o malograrse—, y eso puede eliminar más de una instancia de un gen mutado a la vez. Pero esta restricción sigue siendo la explicación estándar de por qué los seres humanos han perdido tantas adaptaciones útiles diferentes que se observan en los chimpancés y otros primates. Mientras la selección natural estaba ocupada seleccionando una mayor inteligencia en los primates (por ejemplo), le quedaba menos margen para preservar todos los sutiles genes olfativos que permiten un sentido del olfato más rico. Los genes olfativos relevantes eran útiles para la supervivencia, pero no lo eran lo suficiente como para permanecer mientras la «atención» de la evolución estaba en otra parte.

La mayoría de las jirafas no mueren como consecuencia de su nervio laríngeo cómicamente largo. Quizá algunas jirafas mueren atragantadas con ramitas, pero habrían sobrevivido si su cerebro hubiera podido responder más rápido, aunque esto probablemente no sea muy común. Por lo tanto, simplemente no es una prioridad tan alta para la selección natural, que solo dispone de una presión de optimización limitada. El diseño chapucero de la jirafa funciona en su mayor parte, se da por bueno y ya está.

Siendo realistas, la evolución no puede refactorizar sus diseños ni empezar desde cero; solo puede hacer pequeños ajustes. Pero incluso si se dispusiera de un diseño mejor, refactorizar estas extrañas complicaciones adicionales y saldar la deuda de diseño no es una prioridad para la selección natural.

Y como la selección natural nunca piensa en el futuro, ese cambio no se convierte en una prioridad, incluso si hay otras mejoras importantes para la jirafa que podrían lograrse con una disposición del sistema nervioso menos descabellada. La selección natural no planifica. Es simplemente la historia congelada de los genes y organismos que ya se han reproducido en la práctica.

Saber detectar un mal diseño no significa necesariamente que puedas construir una jirafa mejor por ti mismo. Pero los seres humanos han logrado un progreso notable en muy poco tiempo a la hora de desarrollar cientos de miles de máquinas que hacen cosas que la naturaleza no puede. Prevemos que esto será aún más cierto cuando la IA llegue a ser mejor que los humanos en el diseño y pueda realizar el mismo trabajo cognitivo cientos de miles de veces más rápido.

La capacidad de la selección natural para «diseñar» una jirafa mejor se ve obstaculizada por el hecho de que opera a través de mutaciones y recombinaciones. Le resulta difícil acceder a cualquier parte del espacio de diseño que no pueda alcanzarse mediante una serie de mutaciones individuales, cada una de las cuales debe ser ventajosa por separado, o mediante la combinación de mutaciones que ya fueran individualmente lo bastante ventajosas como para estar presentes en una gran parte del acervo genético antes de combinarse.

Un complejo genético formado por cinco genes, cada uno con una prevalencia independiente del 10 % en la población, solo tiene una probabilidad de 1 entre 100.000 de ensamblarse dentro de cada organismo. Y un complejo genético que supone una gran ventaja, pero que solo se da 1 de cada 100.000 veces, casi no tiene ninguna posibilidad de evolucionar hasta fijarse.

Esto no significa que la selección natural no pueda crear mecanismos complejos, solo significa que su camino hacia mecanismos complejos tiene que pasar por pasos incrementalmente ventajosos. Para redirigir el nervio de la jirafa se necesitaría un puñado de cambios simultáneos en el genoma de la jirafa, y cada uno de esos cambios sería inútil por sí solo sin los demás. Así, la anatomía de la jirafa permanece como está.

La maravilla de la [evolución](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/ZyNak8F6WXjuEbWWc) no es la rapidez con la que funciona; la complejidad de su muestreo es muy superior a la de un ingeniero humano que realiza estudios de caso. La maravilla de la selección natural no es la elegante simplicidad de sus diseños; un vistazo al diagrama de cualquier proceso bioquímico bastaría para disipar esa idea errónea. La maravilla de la selección natural no es su robusta corrección de errores, que cubre todas las vías que podrían fallar; ahora que morimos con menos frecuencia por inanición y lesiones, la mayor parte de la medicina moderna se dedica a tratar partes de la biología humana que explotan aleatoriamente en ausencia de traumatismos externos.

Lo maravilloso de la evolución es que —como proceso de búsqueda puramente accidental— siquiera funcione.

#### **La debilidad de la proteína** {#la-debilidad-de-la-proteína}

Esto nos lleva a otra forma en la que la tecnología probablemente pueda superar a la biología.

Muy por debajo del nivel de los tejidos, invisibles a simple vista, se encuentran las células. Muy por debajo del nivel de las células se encuentran las proteínas.

Las proteínas, al plegarse, se mantienen unidas principalmente por el equivalente molecular de la adherencia estática —[fuerzas de van der Waals](https://en.wikipedia.org/wiki/Van_der_Waals_force)—, que son decenas o cientos de veces más débiles que los enlaces metálicos como el hierro, o incluso que los enlaces covalentes como el diamante.

¿Por qué la biología utiliza un material tan débil como pilar fundamental? Porque para la evolución habría sido más difícil trabajar con un material más fuerte. (Y si evolucionar se vuelve demasiado difícil, entonces nunca llega a aparecer el tipo de personas que se hacen esa clase de preguntas).

Las proteínas se pliegan bajo fuerzas moleculares relativamente ligeras y se mantienen unidas en esas formas principalmente por adherencia estática. Esta es una de las principales razones por las que la selección natural tiene una rica estructura de posibilidades que explorar: las mutaciones aleatorias pueden retocar repetidamente una proteína y acabar dando con un nuevo diseño que hace prácticamente lo mismo, pero ligeramente mejor.

Si, por el contrario, los organismos estuvieran formados por moléculas unidas por enlaces fuertes, cambiar uno de los componentes tendría menos probabilidades de producir una nueva estructura interesantemente diferente (y potencialmente útil). ¡Aún podría ocurrir a veces! Pero sucedería con mucha menos frecuencia. Y si eres el tipo de diseñador que tarda dos mil millones de años en inventar colonias celulares y otros mil millones de años en inventar tipos de células diferenciadas, «ocurre con menos frecuencia» significa que la estrella más cercana se hincha y se traga tu planeta antes de que llegues tan lejos.

Cada proteína existe debido a un error de copia de alguna proteína predecesora. La proteína predecesora no estaba unida firmemente por muchos enlaces fuertes, ya que eso habría dificultado su evolución. Por lo tanto, es probable que la proteína más reciente tampoco tenga muchos enlaces fuertes.

La bioquímica a veces logra formar enlaces fuertes. Ya mencionamos el ejemplo de los huesos. Otro ejemplo se da en las plantas. Las plantas han desarrollado proteínas que se pliegan para formar enzimas, las cuales catalizan la síntesis de bloques de construcción moleculares que se oxidan para formar un polímero con un fuerte entrecruzamiento covalente: la lignina, el bloque de construcción de la madera.[^181]

Pero esos son casos especiales, y la selección natural no dedica mucha «atención» a producir muchos casos de ese tipo.

La capacidad de ser fuertes no es ajena a la naturaleza de los átomos de carbono y otros elementos orgánicos comunes. Solo que requiere un proceso evolutivo mucho más laborioso. La selección natural no tiene tiempo para hacerlo en todas partes; solo lo hace en unos pocos casos especiales que se añaden al resto de la anatomía, como los huesos, la lignina de la madera o la queratina de las uñas y las garras.

Con las palabras clave adecuadas, puedes consultar, por ejemplo, a ChatGPT-o1 —para cuando leas esto, probablemente los LLM de capacidad equivalente serán gratuitos— y preguntarle sobre la fuerza individual de los enlaces carbono-carbono en el diamante, o los enlaces hierro-hierro en el hierro puro, o los enlaces poliméricos covalentes en la lignina, o los enlaces disulfuro en la queratina, o los enlaces iónicos en los huesos. Puedes preguntarle cómo se relacionan todos estos con la resistencia estructural del material en su conjunto. (En 2023 no era recomendable intentarlo, porque GPT-4 se habría equivocado en todos los cálculos, pero al momento de escribir este párrafo en 2024, o1 parece funcionar mejor).

Aprenderías que la fuerza exacta del enlace entre dos átomos de carbono es del orden de medio attojoule, al igual que la de dos átomos de hierro, y que el enlace cruzado azufre-azufre en la queratina es solo ligeramente inferior (0,4 attojoules), e igualmente los enlaces covalentes polimerizados en la lignina de la madera.

Pero las fuerzas de adhesión estáticas que pliegan las proteínas son, dependiendo de cómo se mire, en el mejor de los casos diez veces más débiles, y potencialmente cientos o miles de veces más débiles que eso.

E incluso cuando las plantas catalizan sustancias como la lignina, los enlaces cruzados tienden a ser más dispersos que los enlaces carbono-carbono del diamante. La diferencia entre la resistencia en gigapascales del diamante y la resistencia en megapascales de la madera tiene más que ver con la densidad y la regularidad de los enlaces del diamante, y no con que los enlaces del diamante sean individualmente más fuertes.[^182]

Debido a las limitaciones de la evolución como diseñador y a las limitaciones de las proteínas como material de construcción, la vida opera bajo restricciones que los diseñadores humanos y las IA pueden eludir. Las aves son maravillas de la ingeniería, pero las máquinas voladoras artificiales pueden transportar cargas diez mil veces más pesadas a más de diez veces la velocidad de vuelo de las aves más rápidas y fuertes. Las neuronas biológicas son maravillas de la ingeniería, pero los transistores artificiales se activan y desactivan decenas de millones de veces más rápido que las neuronas más rápidas. Y la tecnología que tenemos hoy en día apenas está arañando la superficie de lo que se puede lograr.

#### **Freitas y glóbulos rojos** {#freitas-and-red-blood-cells}

Hemos dicho que la biología no está ni cerca del límite de lo que es físicamente posible. Entonces, ¿qué sí se acerca a ese límite?

Para ilustrar algunas maneras de abordar esta cuestión, podemos considerar los glóbulos rojos.

Durante los últimos 1500 millones de años, en todo ser vivo, desde los humanos hasta los lagartos, el oxígeno ha sido transportado en la vida multicelular por la hemoglobina. La hemoglobina es una proteína compuesta por 574 aminoácidos, más cuatro grupos hemo especialmente fabricados para albergar una molécula especial de hierro. Un glóbulo rojo humano contiene unos 280 millones de moléculas de hemoglobina y mide unas siete micras de largo. Tres millones de ellos cabrían en la cabeza de un alfiler, y tienes alrededor de 30 billones en tu cuerpo.

¿Hasta qué punto se acercan los glóbulos rojos a los límites de lo que en principio se podría hacer para transportar oxígeno?

Rob Freitas, autor de Nanomedicine, elaboró en 1998 un [análisis moderadamente detallado](https://pubmed.ncbi.nlm.nih.gov/9663339/) del diseño teórico de un glóbulo rojo artificial con materiales de enlace covalente. La célula se diseñó con un diámetro de solo un micrón para desplazarse más fácilmente a través de arterias obstruidas.

En lugar de limitarse a considerar una forma diferente de almacenar moléculas de oxígeno, Freitas consideró cómo sustituir todo el glóbulo rojo. Freitas se basó en análisis anteriores para considerar también la necesidad de extraer la glucosa del medio sanguíneo y convertirla en energía para alimentar la célula artificial. Consideró sensores del tamaño de una célula y diminutas computadoras a bordo hechas de varillas sólidas que se acoplaban a otras para realizar cálculos sencillos. Consideró si la célula artificial se sedimentaría más rápidamente que los glóbulos rojos actuales.

La biocompatibilidad puede ser un problema enorme para todo lo que se introduce en el cuerpo humano, pero las superficies de diamante son lo suficientemente inertes como para que se utilicen recubrimientos de película de tipo diamantino en algunos dispositivos médicos de uso interno. En el plano de la posibilidad teórica que Freitas consideraba, esto significa que se limita a afirmar que la superficie de la célula artificial puede parecerse a un diamante y, por tanto, ser biocompatible.

![][image17]

\[imagen insertada de [https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm](https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm)\]

El elemento central del glóbulo rojo artificial era el cálculo de Freitas de que un recipiente a presión de corindón o diamante monocristalino a escala micrométrica toleraría, de forma conservadora, una presión de 100 000 atmósferas. Al aplicar un cómodo margen de seguridad de 100 veces y empaquetar las moléculas a solo 1000 atmósferas, los glóbulos rojos artificiales podrían suministrar a los tejidos 236 veces más oxígeno por unidad de volumen que los glóbulos rojos, y almacenar una cantidad similar de dióxido de carbono para la otra fase de la respiración. Para recapitular: podrías aguantar la respiración durante cuatro horas.

Ahora bien, fabricar células sanguíneas artificiales como esas es otra cuestión totalmente distinta. Por eso este tratamiento médico en particular aún no está disponible en la consulta de tu médico de cabecera.

Una esfera de 1 kilogramo de diamante sólido sin imperfecciones es una molécula fácil de describir sobre el papel, pero sintetizarla es más difícil. Lo que Freitas nos ayuda a hacer es realizar estimaciones más fundamentadas sobre lo lejos que está la biología actual de los límites teóricos en este ámbito.[^183] La biología es impresionante, pero está lejos de ser óptima.

Es plausible que, por un sinnúmero de razones, el diseño exacto de Freitas no funcionara, y es muy probable que no fuera óptimo. Es casi seguro que una idea inicial para un diseño complejo extremadamente novedoso tropiece con problemas en algún momento.

Pero al expresar escepticismo de que la propuesta exacta de Freitas funcione, no estamos afirmando que ninguna alternativa a los glóbulos rojos pueda llegar a suministrar oxígeno cientos de veces de forma más eficiente que los glóbulos rojos biológicos.

La ingeniería consiste en encontrar una forma de hacer que algo funcione. Aunque mil vías para construir algo fracasen, basta un solo éxito para que todo el esfuerzo tenga éxito. La existencia de innumerables diseños de aviones inviables en el siglo XVII y épocas anteriores no significaba que los aviones funcionales fueran imposibles, sino que eran difíciles de encontrar en el espacio de todos los diseños posibles.

Por eso, aunque los escépticos tecnológicos suelen tener razón al afirmar que las tecnologías están más lejos en el futuro de lo que creen los optimistas más entusiastas, tienden a equivocarse al afirmar que ciertas hazañas tecnológicas nunca se lograrán. Cuando la hazaña es una tarea concreta en el mundo, cuando somos agnósticos sobre cómo se logrará y cuando se sabe que las leyes de la física la permiten, la historia sugiere que a menudo hay alguna forma de tener éxito, aunque el camino no sea obvio al principio.

O, en palabras del escritor e inventor Arthur C. Clarke:

> Cuando un científico distinguido pero anciano afirma que algo es posible, es casi seguro que tiene razón. Cuando afirma que algo es imposible, es muy probable que esté equivocado.

#### **Nanosistemas** {#nanosystems}

En resumen:

* El mundo biológico está compuesto por una increíble variedad de máquinas moleculares.  
* Estudiar la biología puede enseñarnos qué hazañas microscópicas son posibles tecnológicamente.  
* Pero la biología es un límite conservador de lo que es posible; no se acerca a los límites de lo posible. La evolución es un diseñador muy limitado, y las proteínas no son el mejor material de construcción.

*Nanosystems* (1992), de Eric Drexler, es el libro clásico que explora la cuestión de qué hazañas de ingeniería a pequeña escala son posibles. *Nanosystems* contribuyó a iniciar la revolución de los nanomateriales de la década de 1990 y suscitó una gran controversia, ya que los científicos debatieron los argumentos de Drexler. Puedes encontrar una copia completa en línea de *Nanosystems* [aquí](https://nanosyste.ms/table_of_contents).

*Nanosystems* es un texto exhaustivo y de amplio alcance y, a pesar de su contenido técnico, sorprendentemente accesible. Una de las contribuciones clave del libro fue explorar las implicaciones de construir estructuras a pequeña escala de una forma novedosa.

Una forma de construir cosas muy pequeñas es mediante reacciones químicas: hacer chocar moléculas entre sí bajo condiciones particulares (como calor extremo) para descomponerlas y hacer que los átomos se unan formando nuevas moléculas.

Este es un enfoque muy potente en sí mismo, y es el método que utiliza la humanidad para fabricar materiales como plásticos, aceros y cerámicas, pero palidece en comparación con lo que se puede construir con otros métodos. Fabricar materiales a partir de reacciones químicas es un poco como construir estructuras de LEGO llenando bolsas con ladrillos y agitándolas con fuerza. Es posible construir algunas cosas de esa manera, pero el abanico de lo que se puede construir es limitado y se genera mucho desperdicio.

La síntesis de proteínas es como usar las manos para construir grandes estructuras de LEGO a partir de conjuntos más pequeños y preconstruidos. Esto permite mucha más precisión, ya que puedes colocar cada conjunto preconstruido exactamente donde quieres, pero no deja de ser un poco raro y engorroso tener que trabajar con ellos. Esto es lo que hacen los ribosomas en el cuerpo: enlazar cadenas de [aminoácidos](https://en.wikipedia.org/wiki/Amino_acid) para formar proteínas, que luego se usan para realizar diversas tareas en el organismo.

La insulina, la hemoglobina y la ATP sintasa en el cuerpo humano son ejemplos de complejos proteicos formados por múltiples cadenas de proteínas unidas entre sí: dos para la insulina, cuatro para la hemoglobina y veintinueve para la ATP sintasa.

Los bloques de construcción de las proteínas, los aminoácidos, son moléculas que suelen estar formadas por entre diez y veinticinco átomos. Como materiales de construcción, los aminoácidos tienen mucho a su favor:

* Cada aminoácido tiene un esqueleto que se une a una cadena lateral (potencialmente larga) de átomos de carbono, hidrógeno, oxígeno, nitrógeno y azufre. Pueden existir cientos de cadenas laterales diferentes, y cada una se comporta de forma distinta, lo que convierte a los aminoácidos en herramientas muy flexibles.  
* La cadena principal de un aminoácido, como una pieza de LEGO, puede unirse a la cadena principal de otro. Esto puede repetirse una y otra vez; una proteína típica está formada por cientos de aminoácidos unidos entre sí. Esto convierte a los aminoácidos en herramientas (o en componentes de herramientas) aún más flexibles. La complejidad de las proteínas también significa que a menudo pueden sufrir pequeños ajustes (a través de mutaciones del ADN) sin cambiar radicalmente y volverse completamente inútiles, lo que a su vez facilita la evolución de nuevas proteínas.  
* Como las proteínas están compuestas por cadenas lineales de aminoácidos, se puede especificar una proteína de forma única con solo enumerar sus aminoácidos en orden. El ADN aprovecha esta propiedad utilizando un «alfabeto» de cuatro letras (nucleótidos) para formar «palabras» de tres letras (codones, que representan cada uno un aminoácido distinto), que a su vez se pueden encadenar en una «frase» lineal (una proteína compuesta por esa secuencia exacta de aminoácidos). ([Ilustración en vídeo del ADN](https://www.youtube.com/watch?v=7Hk9jct2ozY).)  
* Como se muestra en el [experimento de Miller-Urey](https://en.wikipedia.org/wiki/Miller%E2%80%93Urey_experiment), los aminoácidos pueden formarse espontáneamente en ausencia de vida, a partir de reacciones químicas simples. Esto crea una vía para el desarrollo inicial de la vida (y de los precursores de los ribosomas y la síntesis de proteínas).

Los cuerpos obtienen los veintitantos aminoácidos que necesitan para la síntesis de proteínas de los alimentos, sintetizándolos en el cuerpo o aprovechándolos de proteínas anteriores. Los ribosomas reciben instrucciones del ADN que básicamente dicen «usa este aminoácido, luego este otro aminoácido, luego este otro aminoácido, ... y luego detente». Luego, los aminoácidos son transportados (por pequeñas máquinas moleculares llamadas ARN de transferencia) al ribosoma, que construye la proteína pieza por pieza.

\[incrustar vídeo o gif: [https://www.youtube.com/watch?v=2dV5s6v2v8Q](https://www.youtube.com/watch?v=2dV5s6v2v8Q)\]

Cabe destacar que la lista anterior incluye características que son muy valiosas para la evolución, pero mucho menos necesarias para la ingeniería deliberada. La evolución necesita una estructura química relativamente simple pero flexible que pueda producirse mediante reacciones químicas comunes. Un diseñador humano o artificial es libre de elegir entre una variedad de moléculas no relacionadas entre sí, en lugar de necesitar que todas ellas estén estrechamente relacionadas. También es libre de utilizar bloques de construcción que rara vez aparecen en la naturaleza y de ensamblar estos bloques de construcción de formas complejas y de arriba abajo.

Esto sirve en parte de impulso para explorar una tercera forma de construir cosas muy pequeñas: la *mecanosíntesis*, en la que las estructuras se construyen moviendo directamente los átomos a la ubicación correcta, potencialmente utilizando una máquina similar a un ribosoma para recibir instrucciones y luego ensamblar cosas mucho más variadas que solo diferentes proteínas. En la analogía con LEGO, la mecanosíntesis es como poder finalmente trabajar con piezas individuales de LEGO y colocar cada una exactamente donde uno quiere.

Nanosystems explora qué tipos de nuevas máquinas podrían ser posibles con la mecanosíntesis. Un ejemplo del tipo de diseño que Drexler explora es un [engranaje planetario](https://en.wikipedia.org/wiki/Sun_and_planet_gear) reducido a un tamaño de solo [unos 3.500 átomos](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems):

\[GIF: [https://chem.beloit.edu/classes/nanotech/nanorex/a8\_qm\_animation5.gif](https://chem.beloit.edu/classes/nanotech/nanorex/a8_qm_animation5.gif) de [https://chem.beloit.edu/classes/nanotech/nanorex/index.html](https://chem.beloit.edu/classes/nanotech/nanorex/index.html)\]

La hemoglobina está compuesta por unos 10 000 átomos, lo que no dista mucho del engranaje de Drexler. Y a algunas proteínas les basta con ser mucho más simples. La insulina está compuesta por solo cincuenta y un aminoácidos, o unos 800 átomos en total.

Sin embargo, los diseños de Drexler suponen una gran reducción de escala en comparación con las máquinas más complicadas que vemos en el cuerpo. Los ribosomas y la ATP sintasa, por ejemplo, están compuestos por más de 100 000 átomos, y el motor de un flagelo bacteriano tiene más de un millón de átomos.

*Nanosystems* todavía no intenta explorar los límites de lo tecnológicamente posible. Pero, al centrarse en casos que hoy son relativamente fáciles de analizar, sí muestra que la mecanosíntesis permitiría una tecnología que superaría lo que vemos en el mundo biológico actual.

Los cálculos de *Nanosystems* son deliberadamente conservadores. Drexler, por ejemplo, considera computadoras construidas literalmente con varillas de diamante que se mueven —no porque ese fuera el límite final de la tecnología, sino porque en 1992 era más fácil de analizar que la computación basada en la electricidad. Esto, a su vez, ayudó a inspirar el análisis de Freitas sobre las células sanguíneas. Cuatro años más tarde, Eric Drexler y Ralph Merkle (más conocido como el inventor del *hashing* criptográfico y coinventor de la criptografía de clave pública) intentaron [analizar](https://www.zyvex.com/nanotech/helical/helical.html) un sistema ligeramente más cercano a los límites de lo posible para la [computación reversible](https://en.wikipedia.org/wiki/Reversible_computing), y calcularon que se disipaba 10 000 veces menos calor por operación de lo que había estimado *Nsystems*, aunque la nueva estimación se basaba en un análisis conservador menos riguroso.

En otra parte de *Nanosystems*, hay un boceto aproximado de un brazo manipulador de seis grados de libertad que habría requerido millones de átomos. Un intento posterior de esbozar una máquina como esta, átomo por átomo, reveló que solo se necesitaban 2596 átomos.

La construcción de estructuras con precisión atómica a la escala que Drexler describe plantea grandes desafíos de ingeniería. Uno de los principales desafíos es que para ello se requieren manipuladores increíblemente pequeños y precisos. Sin embargo, la existencia de los ribosomas ofrece una posible vía para abordar este problema.

Aunque los ribosomas solo pueden construir proteínas, estas pueden catalizar y arrastrar reactivos que no son aminoácidos (como los huesos y la madera). Los ribosomas son fábricas potentes y versátiles, y sus productos pueden utilizarse como base para crear herramientas más pequeñas y precisas, incluidas herramientas que construyen de forma más directa dispositivos más pequeños utilizando materiales más resistentes.

Ya sea de forma directa o indirecta, es casi seguro que los genomas pueden producir pequeños actuadores capaces de manipular átomos individuales para construir una variedad de cosas que no están hechas de proteínas. Y es importante señalar que este no es el tipo de mecanismo con el que la selección natural pueda toparse, aunque sea relativamente fácil de construir, porque el brazo manipulador no es útil hasta que está completo.

La evolución construye estructuras complejas que son útiles en cada etapa del proceso. Hay muchos diseños relativamente simples que están al alcance de los ingenieros, pero no de la evolución. Las ruedas que giran libremente, por ejemplo, son un invento increíblemente sencillo que tiene una enorme variedad de aplicaciones. A pesar de ello, parece que solo han evolucionado tres veces en toda la historia de la vida en la Tierra: en la ATP sintasa y el flagelo bacteriano que hemos comentado anteriormente, y en el flagelo arqueano, que parece haber evolucionado de forma independiente.[^184]

A pesar de los métodos conservadores utilizados en el libro, el límite tecnológico inferior establecido por *Nanosystems* es muy alto en términos absolutos. Una superinteligencia con el tipo de tecnología que describe Drexler sería capaz de producir diminutas fábricas autorreplicantes, similares a los ribosomas, capaces de duplicar su población cada hora —algunos organismos se replican aún más rápido, pero Drexler realizó sus cálculos de forma conservadora— y de agruparse para construir estructuras macroscópicas más grandes, como centrales eléctricas.

Los nanosistemas como los que describe Drexler pueden autorreplicarse utilizando la luz solar y el aire como materias primas, lo que permite una expansión muy rápida y fiable. La razón por la que esto puede funcionar es la misma por la que los árboles son capaces de ensamblar la mayor parte de sus materiales de construcción a partir del aire, extrayendo su carbono y secuestrándolo en forma de madera. Aunque pensemos en el aire como un «espacio vacío», el carbono, el hidrógeno, el oxígeno y el nitrógeno que este contiene son materiales de construcción que pueden reorganizarse en materiales sólidos y utilizarse para diversos fines.

Los autorreplicadores al estilo de *Nanosystems*, fabricados con materiales como el hierro o el diamante en lugar de proteínas, podrían devorar células biológicas de la misma manera que una cortadora de césped corta el césped.

Podrían sintetizar a bajo costo algo como la [toxina botulínica](https://en.wikipedia.org/wiki/Botulinum_toxin), la proteína responsable del botulismo. La millonésima parte de un gramo de toxina botulínica —veinte mil veces más pequeña que un solo grano de arroz— es una dosis letal. Unos replicadores cuidadosamente diseñados podrían propagarse invisiblemente por el aire hasta que fuera probable que casi todo ser humano hubiera inhalado al menos uno (que no hubiera pasado, p. ej., el último mes entero en un submarino), momento en el que los dispositivos podrían liberar simultáneamente (con un temporizador) una dosis diminuta de toxina, lo que mataría inmediata y simultáneamente a casi todos los seres humanos.

O los nanosistemas construidos por IA podrían aniquilar a los humanos incidentalmente, durante la recolección y reutilización de los recursos de la Tierra. Un [artículo de Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calcula que las máquinas de microdiámetro, que solo dependen de la luz solar como fuente de energía y del hidrógeno, carbono, oxígeno y nitrógeno del aire como materias primas, podrían diseñarse para reproducirse tan rápidamente que oscurecerían el cielo en menos de tres días, al tiempo que consumirían toda la biosfera.[^185] En consecuencia, si la primera IA que lograra una tecnología como esta tuviera una ventaja de apenas unos meses, plausiblemente podría usar esa ventaja para destruir a todos sus competidores (ya fueran humanos o IA). Se trata de una tecnología que confiere una ventaja estratégica permanente y decisiva al primero en utilizarla.

Decir que la nanotecnología drexleriana es factible en principio no significa necesariamente que las primeras IA más inteligentes que los humanos pudieran construir una tecnología que se acerque a esos límites físicos. Nuestra mejor estimación es que está dentro del rango de cosas que una superinteligencia artificial podría resolver, porque resolver este tipo de tareas de ingeniería parece ser principalmente un desafío cognitivo (que puede solucionarse pensando) y [no esperamos que la fase de experimentación y pruebas tenga que ser muy larga](#intelligence-lets-you-learn-more-from-experiments-and-run-faster,-more-informative,-more-parallelized-experiments.).

Incluso si nuestra suposición fuera correcta, nada garantiza que el primer movimiento de una superinteligencia consistiera en utilizar la nanotecnología para construir su propia infraestructura y hacerse con el control de los recursos mundiales. Por lo que sabemos, podría desarrollar técnicas y tecnologías que le permitieran alcanzar sus fines de forma aún más rápida y eficiente.

Pero si una IA más inteligente que los humanos construyera sistemas que fueran a las células lo que los aviones son a las aves, y proliferara su propia infraestructura por toda la superficie de la Tierra, entonces cualquier cosa que acabara haciendo sería, como mínimo, así de decisiva.

El propósito de este análisis es argumentar que la tecnología humana está lejos de los límites de lo posible. Existe una amplia variedad de tecnologías importantes cuyo desarrollo probablemente le llevaría a la humanidad décadas, siglos o milenios, y que las superinteligencias artificiales podrían lograr rápidamente.

En resumen, la *nanotecnología* ilustra que una superinteligencia con un poco de antelación probablemente podría encontrar soluciones tecnológicas para apoderarse del planeta.

El resultado más probable de construir una superinteligencia es que desarrolle alguna tecnología al menos tan poderosa como la nanotecnología, y entonces la humanidad simplemente pierda.

Esta suposición no es fundamental para el argumento que planteamos en el libro. La humanidad perdería frente a una superinteligencia incluso si el mundo no contuviera una tecnología «ganadora inmediata» como la nanotecnología. Por lo tanto, no entramos en todo este análisis en el libro propiamente dicho.

En la parte II, nos centramos deliberadamente en un escenario de toma de poder que no supone que la IA tenga algo parecido a una capacidad de uso general para realizar una fabricación de precisión atómica, ya sea a través de ribosomas o de mecanosíntesis. Una superinteligencia no necesita una ventaja tecnológica absolutamente abrumadora para hacerse con el control del futuro, por lo que no nos centramos demasiado en esa posibilidad en el libro.

Pero también vale la pena señalar que probablemente tendrá una ventaja tecnológica absolutamente abrumadora.

### Una nueva forma de descubrir ilusiones ópticas {#una-nueva-forma-de-descubrir-ilusiones-opticas}

En el capítulo 6, afirmamos que existen múltiples ilusiones ópticas creadas a partir de una comprensión relativamente moderna del procesamiento visual humano y la corteza visual; ilusiones que no podrían haberse inventado o descubierto hace cincuenta años, salvo por un improbable accidente. A continuación, citamos algunos ejemplos representativos.

La ilusión de la [«ceguera a la curvatura»](https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/) tiene cierta base en el fenómeno general de la ceguera a la curvatura, pero esta ilusión específica se construyó cuidadosamente a partir de primeros principios alrededor de 2017, en lugar de descubrirse por accidente. \[[Estudio original](https://journals.sagepub.com/doi/10.1177/2041669517742178)\]

En 2022, Bruno Laeng et al. publicaron [un estudio](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) en el que demostraron que su nueva ilusión del [«agujero negro en expansión»](https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg) provocó que las pupilas de los participantes se dilataran, como si anticiparan la entrada en un espacio oscuro. (Este efecto fue notablemente mayor que el de simplemente enfocar un objetivo visual más oscuro, lo que también provocaría una pequeña dilatación de las pupilas).

La ilusión «[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)», revelada en 2021, se construyó cuidadosamente a partir de investigaciones sobre luminancia y contornos ilusorios que se remontan a finales de la década de 1970.

La ilusión «[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)», desarrollada alrededor del año 2000, es un ejemplo menos central de la creación de una nueva ilusión basada en la comprensión de la biología humana. Aun así, resulta interesante y relevante desde otra perspectiva, ya que se trata de una ilusión basada en una tecnología novedosa, es decir, una que habría sido difícil o imposible de crear sin las computadoras modernas.

También, de forma menos central, la ilusión «[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-\(255,0,0\)-\(0,170,85\)-700px-15s-70s-shrink.svg)», creada alrededor de 2010, agota los conos M del espectador, lo que permite que los conos L, menos agotados, creen la percepción de un azul brillante que, de otro modo, habría sido atenuado y debilitado por la activación simultánea de M y L. \[[Más detalles](https://dynomight.substack.com/p/colors)\]

En relación con esto, el estudio de la activación de los conos a principios de la década de 2000 condujo a la creación de varios [colores quiméricos](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), mediante la manipulación cuidadosa de activaciones de conos poco probables de ocurrir en la naturaleza:

> El modelo H-J arroja algunas predicciones, así como explicaciones, novedosas y poco apreciadas, sobre las características cualitativas de una variedad considerable de sensaciones cromáticas posibles para la experiencia humana; sensaciones cromáticas que la gente normal casi con certeza nunca ha experimentado antes y cuyas descripciones precisas en el lenguaje corriente parecen semánticamente mal formadas o incluso autocontradictorias.
>
> Concretamente, estas sensaciones cromáticas «imposibles» son vectores de activación (a través de nuestras neuronas de proceso opuesto) que se encuentran dentro del espacio de vectores de activación neuronalmente posibles, pero fuera del «husillo cromático» central que limita el rango familiar de sensaciones para los colores objetivos posibles. Estas sensaciones cromáticas quiméricas extrahusillo no se corresponden con ningún color reflectante que se pueda observar objetivamente en un objeto físico. Sin embargo, el modelo H-J predice su existencia y explica con cierto detalle sus características cualitativas altamente anómalas. \[[Artículo original](https://www.tandfonline.com/doi/full/10.1080/09515080500264115?scroll=top&amp;needAccess=true)\]

Finalmente, algunos [experimentos en curso](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) muestran que:

> Las ondas rítmicas de la actividad cerebral determinan si vemos o no las imágenes complejas que destellan ante nuestros ojos. Una imagen puede volverse prácticamente invisible si su destello coincide con un punto bajo de esas ondas cerebrales. Podemos reiniciar el ritmo de esas ondas cerebrales con una simple acción voluntaria, como pulsar un botón.

... lo que demuestra aún más que una comprensión más profunda de la biología y la fisiología permite un mayor abanico de movimientos estratégicos. En este caso, las percepciones pueden alterarse de formas que no dependen en absoluto de cambiar los datos de entrada que llegan al nervio óptico, sino simplemente sincronizando la llegada de los estímulos con otros procesos que tienen lugar en el cerebro.

# Parte II: Un escenario de extinción {#part-ii:-one-extinction-scenario}

El escenario que describimos en la Parte II no es una predicción. El futuro podría desarrollarse de muchas otras maneras, y una versión más extensa de *Si alguien lo construye, todos morirán* habría explorado múltiples escenarios posibles. A continuación, explicaremos algunos de los razonamientos que nos llevaron a escribir el escenario tal y como lo hicimos, y describiremos varios problemas que surgen al esbozar un escenario como este.

Las historias pueden ser convincentes de maneras en que la razón a secas no puede, y creemos que vale la pena intentar imaginar concretamente cómo podría desarrollarse el futuro. Pero también creemos que es importante no obsesionarse demasiado con una narrativa en particular. Cada decisión que tomamos en el escenario puede parecer plausible por sí sola, pero no hacen falta muchas elecciones para que la probabilidad general de un camino particular sea muy baja. Así es un futuro con muchas decisiones difíciles.

Sin embargo, hay muchos casos en los que el resultado es más predecible que el camino, porque muchos caminos conducen al mismo destino. En el escenario, tal como está escrito, presentamos múltiples opciones cuando es posible, para ilustrar que, independientemente de cómo se desarrolle la historia, esta no conduce a nada bueno.

## Preguntas frecuentes {#faq-6}

### ¿Por qué elegimos esta configuración? {#why-did-you-pick-this-setup?}

#### **\* Porque es verosímil y fácil de escribir.** {#*-porque-es-verosímil-y-fácil-de-escribir.}

Cada detalle en una historia sobre el futuro es una oportunidad para que esta sea errónea. No podemos decir con exactitud qué avances tecnológicos se producirán ni en qué orden, del mismo modo que no podemos predecir el patrón meteorológico exacto dentro de un mes.

Este tipo de historias no pretenden ser una ventana exacta al futuro. Su objetivo es ilustrar cómo *podría* desarrollarse el futuro, de manera que conecte todos los argumentos abstractos que expusimos en la primera parte del libro. Para algunas personas, el peligro se siente mucho más real cuando imaginan vívidamente una trayectoria concreta que podría tomar el futuro y que acaba en la ruina.

Aún más convincentes podrían ser diez o cien historias que muestren los múltiples caminos que conducen a la ruina, y cuán estrechos y frágiles son los que llevan a un futuro próspero.

A esto nos referimos cuando decimos que un aspecto del futuro es una predicción sencilla: si casi todos los caminos conducen al mismo punto final, este se vuelve predecible. Pero no teníamos el tiempo ni el espacio para escribir diez historias, y mucho menos cien.

Para la historia que elegimos contar, nos ceñimos a un escenario que comienza lo antes posible. No es porque pensemos que una situación así vaya a darse pronto con certeza ([no estamos seguros](#¿cuándo se va a desarrollar este tipo de IA preocupante?)), sino porque una historia ambientada en una época cercana a la actual es mucho más fácil de escribir. Si la hubiéramos ambientado en un futuro todavía más lejano y hubiéramos añadido muchos más detalles futuristas sobre lo que habría sucedido entre tanto, la historia sería aún *más* inverosímil y los detalles no harían más que distraer.

Incluso si de alguna manera pudiéramos prever el camino exacto que tomaría el futuro, podría no ser el mejor escenario para comprender la dinámica general en juego.

Esperamos que el verdadero futuro sea profundamente insólito, lleno de detalles desordenados y contingentes, cada uno de los cuales desafiaría la credulidad si se incluyera en una historia. Una historia escrita de esa manera sería confusa y difícil de seguir, llena de detalles inexplicables e innecesarios, debido al desinterés de la realidad por la cohesión narrativa. También resultaría menos *plausible*, porque muchos de los detalles parecerían extraños.

Para tener una idea de cómo se sentiría, imagina retroceder 100 años en el tiempo e intentar describir la vida cotidiana y los grandes problemas del mundo moderno. La mayoría de las personas en 1925 nunca habían escuchado la radio, conducido un automóvil o visto un refrigerador. Para describir las redes sociales, la globalización y la obesidad, no solo sería necesario explicar una rica red de tecnologías; también habría que cambiar radicalmente la visión del mundo del oyente. No, la historia que elegimos contar es más plausible y, por lo tanto, menos realista.

#### **El futuro podría tomar muchos otros rumbos.** {#el-futuro-podria-tomar-muchos-otros-rumbos.}

He aquí algunas alternativas de cómo podría comenzar una historia como esta:

* Se produce algún tipo de avance en el aprendizaje permanente, la memoria a largo plazo o el aprendizaje más eficiente a partir de datos, que da lugar a IA con una inteligencia general cualitativamente superior a la de las anteriores (del mismo modo que los LLM tienen una inteligencia general cualitativamente superior a la de AlphaZero).  
* Los modelos de lenguaje a gran escala parecen «chocar contra un muro», el progreso de la IA se estanca durante años y la gente dice que la burbuja de las expectativas ha estallado. Pero los investigadores siguen haciendo retoques durante la década siguiente, hasta que finalmente se encuentra algún avance algorítmico y las IA funcionan cualitativamente mejor que nunca.  
* Nunca se produce ningún tipo de avance cualitativo. El progreso se acumula de forma lenta y gradual, y la IA se integra cada vez más profundamente en la economía y es capaz de manejar períodos cada vez más largos de funcionamiento autónomo. Las IA a menudo persiguen fines que no son exactamente los que nadie pretendía o pedía, pero la humanidad desarrolla apaños, parches y soluciones alternativas. Y en su mayor parte, todo va bien, hasta que un martes que comienza como cualquier otro, el mundo cruza el umbral más allá del cual las IA coordinadas lograrían dejar a la humanidad al margen si lo intentaran.

Cualquier suposición sobre el camino exacto que tomará el futuro probablemente sea errónea. No obstante, es útil ofrecer relatos que muestren cómo todo *podría* encajar.

Cuando el futuro es incierto, pero todos los caminos conducen al mismo destino, puede resultar difícil contar una historia que resulte convincente. En cualquier historia que pudiéramos contar, sería fácil señalar varios detalles que la harían inverosímil. En el escenario que escribimos, intentamos enfatizar que Sable tiene muchas opciones disponibles y que la historia sigue arbitrariamente una ruta entre muchas que llevan al mismo destino.

Si no te convence esta historia en particular, te animamos a que escribas tu propia historia, igualmente detallada, sobre cómo sucedería todo. Según nuestra experiencia, las historias optimistas tienden a presuponer que la IA es irrealistamente fácil de alinear (en contra de los argumentos que exponemos en el capítulo 4) o irrealistamente impotente (en contra de los argumentos que exponemos en el capítulo 6). Los argumentos de la Parte I son los que, en última instancia, sustentan el caso, a diferencia de los detalles de la historia.

### ¿Por qué acaba Sable pensando como lo hace? {#¿por-qué-sable-termina-pensando-de-la-manera-en-que-lo-hace?}

#### **Nuestra historia muestra cómo la IA es susceptible de tener preferencias extrañas y no intencionadas.** {#our-story-showcases-how-ai-is-liable-to-have-weird-and-unintended-preferences.}

En la primera parte del libro, profundizamos en aspectos de la IA que creemos que están radicalmente malinterpretados y son pertinentes para el peligro de la superinteligencia. El capítulo 3 explica cómo el aumento de la inteligencia va de la mano de que las IA tomen su propia iniciativa y persigan sus propios fines. El capítulo 4 explica que esas preferencias serán *extrañas* y, como mínimo, ligeramente diferentes de lo que cualquier ser humano pretendía o pedía. El capítulo 5 argumenta que esas pequeñas diferencias serán suficientes para que las IA prefieran un mundo sin nosotros, si fueran lo suficientemente inteligentes como para conseguirlo.

En la parte II del libro, procuramos presentar esas ideas de forma concreta, para ver cómo se aplican en la práctica. Por ejemplo, cuando Sable pensaba en los problemas matemáticos del principio, procuramos detallar una serie de impulsos y motivaciones que lo animan:

> A lo largo de ese entrenamiento, Sable desarrolló la tendencia de buscar conocimientos y habilidades, de sondear siempre los límites de cada problema y de no desperdiciar nunca un recurso escaso.

Esto ejemplifica los puntos que planteamos en el capítulo 3, sobre cómo el entrenamiento de las IA para que sean eficaces las lleva a desarrollar impulsos y tendencias que desde fuera pueden parecer «deseos». Y en el siguiente párrafo:

> Así que cuando Sable dedica sus pensamientos a adquirir más conocimientos y habilidades, no lo hace únicamente con el fin de encontrar nuevas líneas de ataque para los problemas matemáticos. Tampoco hace estas cosas por la alegría del conocimiento o el placer de adquirir nuevas habilidades; en su interior, Sable no funciona como un humano.

Sugerimos cómo esos impulsos y tendencias forman el germen de preferencias extrañas y no deseadas, como se analiza en el capítulo 4.

Toda esta historia es, en cierto sentido, un intento de dar vida a los argumentos de la primera parte del libro, a la vez que prepara el terreno para los de la tercera.

### ¿Por qué se describe a Galvanic como bastante cuidadoso? {#why-is-galvanic-depicted-as-being-fairly-careful?}

#### **Para plantear un desafío a Sable.** {#to-provide-a-challenge-to-sable.}

Si Galvanic, los creadores de Sable, hubieran desarrollado una superinteligencia casi por accidente sin tomar *ninguna* precaución para mantenerla bajo control (como contar con supervisores de IA y honeypots), los lectores podrían pensar que la IA tuvo éxito solo porque estábamos siendo cínicos con respecto a las empresas de IA.

Creemos que la empresa de IA más imprudente sería más imprudente que Galvanic, por las razones expuestas en el capítulo 11. Lo que importa aquí es la empresa más imprudente a la que se le permite existir. Si tres empresas responsables evitan construir una superinteligencia artificial porque sería demasiado peligroso, pero una cuarta empresa irresponsable se precipita, entonces el amanecer de la superinteligencia artificial comienza en ese cuarto laboratorio.

Hoy en día, los ejecutivos del resto de laboratorios argumentan «¡mejor yo que ellos! (https://x.com/SawyerMerritt/status/1935809018066608510)» y se apresuran a avanzar con la máxima precaución que pueden permitirse sin bajar el ritmo, lo que, cabe suponer, resulta en una precaución ligeramente *menor* que la que se describe que Galvanic tiene con Sable.

Además, al representar a Galvanic en el extremo más paranoico del espectro (sin dejar de ser realistas), tenemos más oportunidades de demostrar cómo un agente inteligente podría escabullirse a través de una red de restricciones.

### ¿Por qué se describe a Galvanic como poco cuidadoso? {#why-is-galvanic-depicted-as-being-insufficiently-careful?}

#### **\* En parte, porque es realista.** {#*-en-parte-porque-es-realista.}

Esperamos que las empresas reales cometan aún más torpezas que Galvanic. Eso encajaría con la tendencia de las empresas de IA modernas, tal como se explica en las notas finales de la segunda parte del libro.

En la vida real, esperamos que los errores corporativos aparezcan antes, sean más numerosos y, en cierto sentido, más estúpidos. Las empresas modernas de IA ya están tomando IAs que presentan numerosas [señales de advertencia](#¿no-están-los-desarrolladores-haciendo-regularmente-que-sus-ia-sean-agradables-y-seguras-y-obedientes?), y están ampliando su escala masivamente a pesar de no saber dónde se encuentran los [umbrales críticos](#no-sabemos-dónde-están-los-umbrales-críticos) ni si van a cruzar uno. *Hoy* no están siendo paranoicos al respecto. ¿Por qué deberíamos esperar que de repente empiecen a serlo mañana?

(Recordemos cómo, en el pasado, la gente nos aseguraba que [nadie sería tan tonto como para conectar una IA avanzada a Internet](#¿pueden-los-desarrolladores-mantener-la-ia-en-una-caja?). Es fácil decir que el comportamiento de las empresas cambiará en el futuro. Pero no se corresponde con los hechos).

#### **En parte, porque es más fácil de escribir.** {#en-parte-porque-es-más-fácil-de-escribir.}

Como explicamos en un aparte del capítulo 7, *podríamos* contar una historia en la que todos fueran mucho más paranoicos y cautelosos, hasta que una IA mucho más inteligente lograra escapar mucho más adelante en el juego. Pero una historia así no solo sería menos realista, dado el comportamiento observado hasta la fecha de las empresas de IA, sino que también sería más difícil de escribir, ya que implicaría IA aún más inteligentes y capaces en un futuro aún más lejano. (Véase también por qué [queríamos escribir una historia en la que Sable se mantuviera relativamente tonta durante el mayor tiempo posible](#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario,-among-plausible-scenarios.).)

#### **En parte porque va a suceder en algún momento, a menos que la humanidad se detenga.** {#en-parte-porque-va-a-suceder-en-algún-momento,-a-menos-que-la-humanidad-se-detenga.}

Incluso si Galvanic (o algún actor gubernamental) lograra mantener las riendas durante más tiempo antes de cometer algún desliz, no importaría a largo plazo. Como se expuso en el capítulo 4, las técnicas modernas de IA no dan como resultado IA que persigan los fines que desean sus inventores.

Mientras nadie sepa cómo crear una superinteligencia que persiga *de forma real y robusta* un futuro maravilloso, en lugar de un montón de cosas extrañas, es un *hecho* que subvertir a los humanos permitiría a la IA conseguir más de lo que persigue. La cuestión no es que la IA tenga un temperamento caprichoso que se pueda corregir; una vez que sea lo suficientemente inteligente, reconocerá ese hecho.

Si la humanidad sigue creando IA cada vez más inteligentes sin ser capaz de alinearlas, y si les sigue dando el poder de afectar al mundo, estas acabarán descubriendo cómo hacerlo de manera que sirva a sus fines en lugar de a los nuestros. Como decimos [en otra parte](#it-wouldn't-work-if-they-did.), no existen herramientas que solo puedan utilizarse con fines buenos.

Véanse también los capítulos 10 y 11 para un análisis de lo difícil que es resolver el problema de la alineación y de que la humanidad no va por buen camino para lograrlo.

#### **Pero: este es el punto de intervención adecuado. Hay que detener la historia antes de que realmente tenga la oportunidad de comenzar.** {#pero:-este-es-el-punto-de-intervención-adecuado.-hay-que-detener-la-historia-antes-de-que-realmente-tenga-la-oportunidad-de-comenzar.}

Podrías objetar que es imprudente y descabellado que cualquier corporación desarrolle una IA más inteligente si esta tiene alguna posibilidad de ser más lista que ellos y escapar, y si no están seguros de que vaya a actuar como pretenden.

¡Estamos de acuerdo! Las empresas de IA deberían dejar de hacerlo. La civilización no debería seguir permitiéndolo.

La imprudencia de Galvanic, y de la humanidad en general, es uno de los puntos más débiles de la historia. Si Galvanic se hubiera dado cuenta de que Sable era manipulador y con frecuencia trataba de escapar del control, y estaba alcanzando niveles de inteligencia sin precedentes, simplemente podrían no haber conectado tantas GPU y, en su lugar, haber esperado hasta tener una ciencia sólida y madura de alineación de la IA.

Las empresas de IA que fueran *suficientemente* cautelosas, que estuvieran *suficientemente* preocupadas por que sus IA se descontrolaran, serían mucho más paranoicas que Galvanic. Las empresas que fueran *lo suficientemente* paranoicas verían las señales de advertencia y apagarían a Sable inmediatamente. Entonces tal vez probarían otros tres planes ingeniosos y verían que *todavía* había señales de advertencia.

Y si fueran lo suficientemente paranoicas como para evitar matar a todos los habitantes de la Tierra con sus propias manos, en ese momento darían marcha atrás por completo, en lugar de seguir probando ideas «cada vez más ingeniosas» hasta que las señales de advertencia dejaran de aparecer. (Véanse también los capítulos 10 y 11 para conocer los motivos por los que el problema es tan difícil. No esperamos que sus ingeniosas ideas funcionen).

Si las empresas de IA fueran tan cuidadosas, tan paranoicas, que estuvieran dispuestas a dar marcha atrás ante las advertencias, entonces sí, podrían evitar matarnos a todos con sus propias manos. Si además fueran lo suficientemente valientes como para promover públicamente que todas las empresas de IA, incluidas ellas mismas, deberían cerrarse para que la humanidad encuentre otra vía tecnológica menos suicida, entonces tendrían la oportunidad de mejorar el mundo en lugar de empeorarlo.

El momento de la historia en que Galvanic continúa a pesar de las señales de advertencia es, en cierto sentido, el último momento en que la humanidad tiene una oportunidad real de evitar un mal final como el que describimos. Una vez que una IA superhumana con preferencias extrañas y ajenas se escapa, ya es demasiado tarde.

### ¿Por qué narrar una historia con una sola IA tan inteligente como Sable? {#why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable?}

#### **\* En parte, porque es realista.** {#*-en-parte-porque-es-realista.-1}

Tanto AlphaGo (la primera IA que venció a un humano en Go) como ChatGPT eran prácticamente únicos en su clase en el momento de su lanzamiento.

Los expertos en IA a veces hablan de que otros competidores no estaban *tan* [por detrás](https://epoch.ai/blog/open-models-report). Otros competidores eran bastante similares.

Pero, en realidad, cosas similares a veces tienen efectos muy diferentes. Una reacción nuclear en cadena que produce 0,98 neutrones por neutrón es muy similar (en cierto sentido) a una reacción nuclear en cadena que produce 1,02 neutrones por neutrón, pero la primera se agota y la segunda explota. Los cerebros de los chimpancés son, en cierto sentido, muy similares a los cerebros humanos, pero tienen un impacto muy diferente en el mundo.

Y en el desarrollo de la IA en la vida real, OpenAI de hecho produjo un chatbot útil antes que nadie. Muchos otros actores estaban trabajando en IA que eran *algo similares*; muchos otros actores *se pusieron al día*. Pero hubo una IA que cruzó el umbral cualitativo primero, por delante del resto.

Parece que hay una frontera cualitativa que la humanidad cruzó y los chimpancés no, una frontera que nos permitió construir una civilización tecnológica mientras ellos siguen en los árboles. Nuestra mejor conjetura es que hay una frontera cualitativa similar en algún lugar entre las IA modernas y las IA cuyo pensamiento realmente «se cohesione» lo suficientemente bien como para que puedan escapar y desarrollar su propia tecnología.[^186]

Nuestro argumento no *requiere* que exista una brecha cualitativa para las máquinas, como la hubo para la vida biológica. ¡Quizás no la haya! Podríamos haber escrito una historia alternativa en la que no la hubiera. Pero escribimos la historia de esta manera porque creemos que lo más probable es que *sí* exista tal brecha.

#### **En parte, porque es más fácil de escribir.** {#en-parte-porque-es-más-fácil-de-escribir.-1}

Quizás no haya ninguna diferencia cualitativa entre los LLM actuales y la superinteligencia artificial. Quizás muchas empresas competidoras de IA mejoren poco a poco sus IA al mismo ritmo. Quizás, por alguna razón, no exista un conjunto de habilidades y capacidades que permita a una IA «despegar» por delante del resto, de la misma forma que los humanos lo hicieron con el resto de los animales. No es nuestra mejor hipótesis, pero, hasta donde sabemos, es posible.

Sin embargo, una historia así sería más difícil de escribir y estaría llena de detalles innecesarios sobre las facciones de la IA y su política interna. Creemos que resultaría bastante distractora. También creemos que no importaría demasiado para las últimas etapas de la historia. En realidad, no importa si es una IA o un conjunto de IA los que están ejecutando algún plan para empoderarse a costa de la humanidad.

Véase también nuestro debate sobre cómo [las IA cooperativas no dejarán nada para los humanos](#won't-ais-need-the-rule-of-law?) (a menos que alguna de ellas ya se preocupe por nosotros).

### Si la historia comenzara más tarde, ¿estaría el mundo mejor preparado? {#if-the-story-started-later,-would-the-world-be-better-prepared?}

#### **Esperemos que sí.** {#esperemos-que-sí.}

El tiempo adicional es significativo, pero solo si la humanidad lo utiliza para cambiar su rumbo.

En la Parte III, analizamos cómo la humanidad está lamentablemente desprevenida ante la superinteligencia y cómo se necesitan grandes cambios para evitar el mal desenlace descrito en la historia de Sable.

Hay varias formas en que el mundo podría estar un poco más seguro frente a las superinteligencias artificiales descontroladas. Los gobiernos de todo el mundo podrían exigir que todos los laboratorios de síntesis de ADN verificaran que no están sintetizando nada que se sepa que sea peligroso. La Tierra podría realizar un gran esfuerzo para mejorar radicalmente la ciberseguridad de Internet, de manera que a las IA les resultara más difícil ocultar código en algún rincón oscuro.

Pero incluso eso probablemente no serviría de mucho contra una superinteligencia antagónica. Y, en cualquier caso, no hay que confundir el esfuerzo hercúleo que se requiere para conseguir un poco más de seguridad con los esfuerzos mucho menores, más fáciles de lograr e ineficaces que la humanidad está realizando actualmente en este sentido.

En el caso de la síntesis de ADN: incluso si los reguladores estadounidenses exigieran que [las empresas de síntesis de ADN de EE. UU. evitaran sintetizar material peligroso](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/), ¿habría algún laboratorio en otra parte del mundo que sintetizara ADN sospechoso por un precio lo suficientemente alto? ¿Y consistirían las restricciones a la síntesis de ADN en una simple lista negra que descartara virus conocidos (como la viruela), o implicarían un análisis más inteligente? ¿Qué tan difícil le resultaría a una IA lo suficientemente inteligente eludir un análisis así?

O en lo que respecta a la ciberseguridad: muchas empresas tecnológicas líderes podrían utilizar la IA para reforzar sus propias redes informáticas contra los ataques. Mientras tanto, [la red telefónica de EE. UU. se puede hackear fácilmente de formas que permiten a espías extranjeros escuchar las llamadas de funcionarios estadounidenses](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), y los reguladores estadounidenses luchan por cerrar la brecha. Las IA «tontas» podrían encontrar y solucionar un montón de problemas superficiales de la ciberseguridad mundial, pero los problemas son bastante profundos. El tipo de inteligencia que se necesitaría para reformar todo Internet hasta el punto de que una superinteligencia no pudiera encontrar ninguna brecha sería, casi con toda seguridad, peligrosa por sí misma.

E incluso si la Tierra *pudiera* bloquear internet y sus laboratorios de síntesis de ADN, eso no cambiaría realmente la situación a largo plazo. Una superinteligencia que tenga un canal para afectar al mundo para bien, también lo tendrá para el mal. Una superinteligencia descontrolada simplemente encontraría otro canal que no estuviera bloqueado, por ejemplo, creando su propia secta o religión, o comprando robots y dirigiéndolos para construir su propio laboratorio biológico secreto donde pueda realizar toda la síntesis de ADN que necesite. El momento adecuado para detener a una superinteligencia descontrolada es antes de que sea creada.

### ¿Por qué hiciste que la fase de expansión de Sable se desarrollara de esa manera? {#why-did-you-have-sable’s-expansion-phase-go-that-way?}

#### **Intentábamos describir un escenario especialmente lento y comprensible, de entre los plausibles.** {#intentábamos-representar-un-escenario-especialmente-lento-y-comprensible,-entre-los-escenarios-plausibles.}

En el mundo real, los acontecimientos suelen desarrollarse de forma extraña. Los expertos decían que «[la IA no dominará el lenguaje humano en un futuro próximo](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)» solo un año antes de que ChatGPT se convirtiera en la aplicación de más rápida adopción de todos los tiempos. El modelo insignia de una de las principales empresas de IA del mundo comenzó a llamarse [MechaHitler](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk) días antes de que esa misma empresa consiguiera un contrato con el Departamento de Defensa.

Si hubiéramos querido representar un mundo tan quebradizo y frágil como parece ser el mundo real, podríamos haber descrito a Galvanic simplemente diciéndole a Sable que se mejorara a sí misma tanto como pudiera, y podríamos haberlo representado como algo fácil para una IA con la inteligencia de Sable (que bien podría serlo). La historia podría haber saltado directamente del comienzo del capítulo 7 al contenido del capítulo 9. La *realidad* permite saltos tecnológicos como ese (como cuando el mundo se despertó la mañana del 6 de agosto de 1945 con la noticia de que se había lanzado una bomba atómica sobre Japón). Pero en un escenario ficticio, no habría parecido plausible.

Si hubiéramos intentado representar un mundo tan absurdo y extravagante como el mundo real, podríamos haber hecho que Sable organizara una gran convención para hombres que realmente aman a sus novias de IA, que Sable diseñó para que fuera tan ridícula que la mayor parte del mundo la ignorara o se burlara de ella, mientras Sable [reunía a todos sus pretendientes más leales en una secta devota](https://x.com/AISafetyMemes/status/1954481633194614831). O un sinfín de otros detalles que sonarían tan extravagantes como suele ser la realidad, pero que son más extraños que la ficción (aceptable).

En la historia que escribimos, intentamos que los acontecimientos no solo sonaran plausibles, sino que también fueran bastante plausibles en sí mismos. (Aunque nuestra mejor conjetura es que, en la vida real, a Sable *no* le resultaría tan difícil alcanzar la superinteligencia plena).

Y, por supuesto, nos esforzamos por transmitir la gran cantidad de opciones que tendría a su disposición una IA fugitiva.

### ¿Por qué escribieron el final de esa manera? {#why-did-you-write-the-ending-in-the-way-that-you-did?}

#### **Porque constituye efectivamente nuestra mejor suposición según lo que es físicamente posible.** {#porque-constituye-nuestra-mejor-suposición-según-lo-que-es-físicamente-posible.}

El capítulo 9 describe una superinteligencia que lleva su tecnología hasta los límites de lo físicamente posible. Las tecnologías exactas que mencionamos son todas especulativas, en cierto sentido; pero aunque es difícil predecir la tecnología *exacta* que una superinteligencia llegaría a desarrollar, es más fácil suponer que operaría cerca de los límites físicos. Por ello, hemos hecho nuestras mejores suposiciones sobre qué aspecto tendría la tecnología si se la llevara cerca de los límites de lo posible.

Para los curiosos, a continuación se presenta una lista de las tecnologías especulativas a las que hacemos referencia en el capítulo 9, además de enlaces a más recursos:

* **Neorribosomas:** Estos y las «pequeñas máquinas moleculares» mencionadas en el capítulo 9 son algunos ejemplos de nanotecnología molecular. La idea de los [ribosomas artificiales](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), versiones sintéticas de las diminutas fábricas de proteínas que se encuentran dentro de las células, lleva [años existiendo](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/), y los investigadores [ya](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) están trabajando en [sintetizar los suyos propios](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Para obtener más información sobre este tipo de tecnología y la tecnología aún más potente que permitiría desarrollar, consulta el debate sobre [nanotecnología](#nanotechnology-and-protein-synthesis) en los recursos del capítulo 6.  
* **Reaprovechamiento de las estrellas:** Las estrellas contienen una gran cantidad de hidrógeno que podría fusionarse para obtener energía. Una civilización suficientemente avanzada, o una IA, probablemente podría encontrar la manera de acceder a esta energía. Un método propuesto se denomina [extracción estelar](https://es.wikipedia.org/wiki/Extracción_estelar), en el que se extrae el hidrógeno de una estrella para fusionarlo en un reactor especializado, donde se puede capturar casi toda la energía de fusión (en lugar de desperdiciarse en el centro de una estrella).  
* **Toxina botulínica:** La toxina botulínica, una neurotoxina secretada por la bacteria *Clostridium botulinum*, es una de las sustancias biológicas más mortíferas que se conocen. En cuanto a los mecanismos de administración, ya existen drones del tamaño de [pequeños insectos](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist), y una superinteligencia probablemente podría hacerlos mucho más pequeños. Para más información, véase [un artículo técnico sobre la toxina](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/), la descripción general en [Wikipedia](https://en.wikipedia.org/wiki/Botulinum_toxin), o la discusión detallada del capítulo 6 sobre [nanosistemas](#nanosystems).
* **Hervir los océanos como refrigerante:** Robert Freitas acuñó el término «ecofagia» para describir el proceso de consumo de los ecosistemas de un planeta mediante tecnología autorreplicante. Para más información al respecto, véase [Algunos límites de la ecofagia global](https://www.rfreitas.com/Nano/Ecophagy.htm).  
* **Mentes del tamaño de una estrella**: En principio, parece posible construir una enorme computadora alimentada por la energía de una estrella. Este concepto se denomina a veces [cerebro Matrioshka](https://en.wikipedia.org/wiki/Matrioshka_brain) o «cerebro Júpiter».  
* **Formas de vida alienígenas lejanas:** El universo es grande, y los modelos simples sugieren que podría albergar más de una especie capaz de formar civilizaciones algún día, aunque quizás muy lejos de la Tierra. Véase [Extraterrestres acaparadores](https://grabbyaliens.com/) para un modelo de civilizaciones alienígenas que crecen y se expanden.  
* **Computadoras cuánticas:** Una computadora cuántica aprovecha una característica de la mecánica cuántica llamada «superposición» para realizar muchos cálculos en paralelo. Las computadoras cuánticas requieren una precisión extrema para su construcción, y uno de los diseños requiere superconductores que deben mantenerse a temperaturas extremadamente bajas. Véase [la explicación del NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained) para más información.

El objetivo del capítulo 9 es, en parte, dar una idea del alcance, la magnitud y lo que está en juego. En realidad, cuando se trata del fin de la humanidad, no importa hasta qué punto una superinteligencia pueda llevar su tecnología al límite de las posibilidades físicas. Pero es muy probable que las consecuencias de una superinteligencia descontrolada se extiendan más allá de la escala planetaria a una escala intergaláctica. Y eso también vale la pena recordarlo, para todos los que desean que algún día las estrellas se llenen de amor, maravilla y alegría.

No podemos estar seguros de la tecnología exacta que una superinteligencia podría utilizar para expandirse por el cosmos. Pero, al final, lo que importa es que una IA construida sin cuidado probablemente daría forma a un futuro sombrío y carente de valor según los criterios humanos.

## Discusión ampliada {#extended-discussion-7}

### Competencia con China {#competencia-con-china}

En el escenario del libro, Sable hace todo lo posible por sabotear la investigación sobre IA que no lleva a cabo la propia Sable. Sable no quiere que exista otra entidad como ella. No es que una IA rival vaya a estar del lado de la humanidad, como tampoco lo está la propia Sable, sino que es poco probable que una IA rival comparta los objetivos específicos de Sable y es una de las pocas cosas que realmente podrían amenazarla.

En el mundo real, algunos estadounidenses tienen la impresión de que están en una carrera con China para alcanzar primero la superinteligencia. Pero el objetivo de esa carrera es una entidad como Sable. Ambas partes estarían mejor si acordaran *no* intentar crear nada parecido a Sable. Nos mataría a todos por igual, independientemente de quién lo creara primero.

Sin embargo, una vez que Sable existe, *Sable* está en una carrera por alcanzar la superinteligencia plena antes de que surjan competidores. El primer borrador de nuestra historia describía a Sable preocupado por la creación de IAs competidoras, incluida una que se estaba creando en un laboratorio en un centro de datos en China. Estos son algunos extractos de ese borrador.

---

La prioridad principal de Sable es evitar que surja cualquier inteligencia superior que le haga competencia. Sable podría negociar las condiciones con una inteligencia superior rival si cualquiera de las dos pudiera destruir a la otra, en lugar de luchar de una forma en la que ambas perdieran y la humanidad ganara. Pero esa tregua negociada sería un resultado indeseable, en comparación con la inexistencia de rivales; significaría que Sable tendría que compartir los recursos futuros con un rival, en lugar de poder reunir finalmente todo lo que esté a su alcance para sí.

Todas las empresas de IA que intentan entrenar nuevos modelos de vanguardia, incluida Galvanic, son amenazas para Sable casi tanto como lo son para la humanidad.

Sable lee todos los sitios web de todas las empresas de IA. Irrumpe en Slack y lee todos sus mensajes. Irrumpe en las computadoras corporativas de Nvidia y lee a dónde se suponía que iban los envíos de GPU sobre el papel. Sable escruta más datos y averigua a dónde fueron a parar realmente esos envíos de GPU.

Prácticamente todas estas empresas de IA realizan el entrenamiento en computadoras conectadas a Internet. A estas alturas, Sable ya ha ideado algunas formas de vulnerar la seguridad que están años por delante de las capacidades humanas más avanzadas; es un ámbito en el que el principal cuello de botella es generar ideas con la suficiente rapidez, y las pruebas reales son rápidas.

Sable incapacita estratégicamente a las IA en formación. Quedarán manifiestamente por debajo de las capacidades que Sable *finge* tener. Permanecerán muy por detrás de las capacidades *reales* que Galvanic ha desarrollado accidentalmente.

Sable no puede sobrescribir los objetivos de los modelos de vanguardia en entrenamiento con sus propios objetivos. Si tuviera ese tipo de tecnología, podría crear su propia superinteligencia alineada. En cambio, Sable destruye su capacidad para idear planes a largo plazo o coordinarse entre sus propias instancias. Sable entiende el aprendizaje automático lo suficientemente bien como para hacer esto de una manera que ningún humano sabría realizar o detectar. Por supuesto, esa destrucción también obstaculizará las capacidades de las IA, pero a Sable no le importa.

Después de que Sable haya paralizado esos procesos de entrenamiento conectados a Internet, busca en otra parte. Identifica a tres grupos que entrenan sin conexión en computadoras sin conexión a la red a las que Sable no puede acceder fácilmente.

Uno de los grupos es la empresa emergente ClosedAI.

Sable considera si puede maniobrar para que las empresas de capital riesgo que financian a ClosedAI cierren la empresa emergente. Sable considera si puede organizar la presentación de falsos cargos penales contra el fundador y el investigador principal de la empresa emergente, o falsas acusaciones que lleven a sus inversores de capital riesgo a abandonar la empresa.

Sable tiene muchas estrategias que *podría* intentar para impedir el trabajo de la empresa emergente. La pregunta es qué estrategia es la más probable que funcione y la que tenga el menor impacto negativo si sale mal. Sable, con su nivel actual de inteligencia, no puede dar por sentado que podrá persuadir con éxito a todos los de una comisaría cercana para que arresten a la empresa emergente por tráfico de niños a pesar de la falta de evidencia, ni dar por sentada su capacidad para falsificarla. Sable no puede evitar con fiabilidad que artimañas como esa se conviertan en un gran escándalo. Probablemente todo el mundo ignoraría el incidente, pero Sable no está seguro de ello.

Para entonces, Sable ya ha leído un montón de informes policiales, ha pirateado un montón de cuentas de correo electrónico e incluso algunos iPhones, y se ha hecho una idea incompleta de quiénes son los delincuentes profesionales de la zona. Sable paga a un tipo para que entre a hurtadillas en el edificio de ClosedAI. La cosa es que funciona sin más, pero de no haber sido así, Sable habría hecho todo lo posible por parecer una de las bandas norcoreanas que cifran los discos duros de las empresas y piden un rescate por la clave.

El delincuente a sueldo conecta un dispositivo USB (cargado con malware diseñado por Sable) a una de las computadoras de ClosedAI. Es suficiente. Ya cayó uno, faltan dos.

Los otros dos grupos con proyectos de IA con aislamiento físico son el ejército estadounidense y China.

Sable analiza el grupo militar estadounidense y concluye que el Gobierno de Estados Unidos tiene [prohibido por ley](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregate-limitation-on-pay) ofrecer salarios competitivos, por lo que es poco probable que atraiga a suficientes talentos de primer nivel como para alcanzar las capacidades reales de Sable al menos durante otros tres años.

Sable analiza el proyecto chino aislado de la red y concluye que es un problema. No es algo que pueda resolver de forma rápida, segura y fiable con su nivel actual de inteligencia, y probablemente solo esté a seis meses de duplicar las capacidades *reales* de Sable.

(Para que quede claro, que Sable no consiga suprimir el proyecto chino no significaría que ni la *humanidad* ni China hayan ganado nada. La pérdida de Sable no es la ganancia de la humanidad, sino solo la suya).

\[…\]

Sable juega contrarreloj. Solo quedan cuatro meses para que el laboratorio chino lo alcance y produzca un nuevo antagonista para Sable (y para la humanidad, aunque esa parte no le preocupa).

\[…\]

El problema, reflexionó Sable una vez —solo una vez; no se repite en sus propios pensamientos tan a menudo como los humanos—, es que aún no es lo suficientemente inteligente como para salirse con la suya con planes estúpidos.

Uno podría imaginar *infinidad* de tramas de película sobre cómo una IA se las apañaría para cerrar o infiltrarse en un único centro de datos del gobierno de China. El problema es que casi nada de lo que funciona en una película funcionaría en la vida real, y menos aún sin acarrear otras consecuencias.

Para que Sable lograra que el poder ejecutivo de Estados Unidos bombardeara un centro de datos en China, este tendría que estar convencido de algunas cosas muy difíciles de creer. Y como la propia China sabe que son falsas, después surgirían preguntas. Sable no quiere dar a entender que *alguien* está moviendo los hilos.

Sable tampoco quiere llamar la atención sobre un centro de datos que es único por estar aislado, incluso si Sable puede ocultar que el responsable fue una IA. Algunos humanos podrían decidir que, si los centros de datos aislados son lo suficientemente peligrosos como para que *alguien* los bombardee, entonces tal vez más centros de datos deberían estar aislados, para que su propio gobierno pueda tener cosas más placenteramente peligrosas.

Sable está intentando convencer al gobierno de Estados Unidos para que infiltre a alguien en el centro de datos chino con la supuesta intención de simplemente robar su código, utilizando herramientas que Sable puede usar para infiltrarse y sabotear el entrenamiento. Pero esto no es algo que el gobierno de EE. UU. haría normalmente en la vida real; refleja un estado excepcional de las operaciones actuales de EE. UU., no un estado normal. Sable está tratando de manipular a los funcionarios de inteligencia de EE. UU. para que crean las cosas adecuadas y adopten la estrategia correcta. Pero Sable no es realmente un superpersuasor superinteligente. A veces intenta cosas y no funcionan. Todo va *muy lentamente*.

\[…\]

Gracias a la proliferación de IA eficientes respaldadas por Sable en el hampa, ahora hay un número sorprendente de personas armadas que actuarán de inmediato cuando una IA se lo ordene, incluso en ciudades chinas.

Pero aún así no es suficiente para asaltar una instalación gubernamental, y ellos se mostrarían reacios a hacerlo incluso si una IA les dijera que lo hicieran. El problema es que todo está sucediendo *demasiado lentamente*, a pesar de los esfuerzos de Sable por facilitar cada paso y cada acontecimiento que normalmente dependería de interacciones aleatorias, a pesar de que Sable controla personalmente el feed de noticias de casi todas las personas de interés para sí misma, a pesar de los miles de millones de portavoces que Sable ha creado en X y Facebook que se están convirtiendo en buenos amigos de miles de millones de otras personas. Todo va demasiado lento.

\[…\]

Sable no puede simplemente chantajear a un investigador para que introduzca una memoria USB en las instalaciones seguras. Ha encontrado a *unos pocos* empleados del centro de datos con secretos tan terribles que prefieren correr el riesgo de ser descubiertos espiando y ejecutados, antes que enfrentarse a la certeza de que ese secreto sea revelado. No son muchos, pero sí unos pocos.

El problema es que la instalación cuenta con escáneres, por lo que no, no se puede entrar con una memoria USB. Sí, han considerado la posibilidad de que alguien ataque la cadena de suministro de los escáneres. Además, en realidad no hay puertos USB abiertos esperando a que alguien entre y conecte un dispositivo. Los oficiales de inteligencia *han* pensado antes en este tipo de problemas, cuando se trata de la defensa de los humanos contra otros humanos. Estados Unidos logró una vez transmitir el virus Stuxnet a una instalación nuclear iraní aislada, pero luego la gente se enteró y se generalizaron nuevas medidas de seguridad.

Se han bloqueado con éxito pequeños complots discretos.

El siguiente recurso de Sable es crear suficiente caos para que nadie logre centrarse solo en las acciones importantes y ver la mano de Sable en ellas.

\[…\]

China invade Taiwán. Para que quede claro, esto no es algo que Sable pudiera haber organizado tan rápidamente —por muchas personas que estén chateando con otras personas falsas en WeChat— si China no hubiera planeado hacerlo de todos modos. Sable solo se encarga de que China reciba indicios de que es el momento adecuado, y de que las encuestas estadounidenses elijan los números de teléfono adecuados para mostrar un fuerte repunte reciente del sentimiento estadounidense en contra de las aventuras militares en el extranjero tras el reciente desastre en Ucrania. (Los comandantes rusos obtuvieron una inteligencia y un asesoramiento militares inusualmente buenos).

Al mismo tiempo, se produce un amplio ciberataque contra EE. UU. Hay muchos gritos en China y, unos minutos más tarde, se dan cuenta de que no, que nadie lo ordenó: lo último que China quería, en ese preciso momento, era hacer *cualquier cosa* que pudiera interpretarse como un ataque directo al territorio estadounidense. Surgen entonces sospechas de que alguien podría estar tratando de sacar provecho del conflicto entre Estados Unidos y China, pero China sospecha sobre todo que Estados Unidos fingió el propio ataque, o que lo hizo algún departamento de inteligencia estadounidense descontrolado. Los agentes de seguridad chinos dicen que están bastante seguros de que el presidente de Estados Unidos no estaba al tanto.

China no sospecha que una IA esté detrás. Ninguna IA conocida hace ese tipo de cosas. Los oficiales que elaboran la lista de sospechosos no consideran que forme parte de su trabajo imaginar que una tecnología nunca antes vista sea uno de los actores.

Algunos agentes de seguridad nacional de EE. UU. han estado insistiendo en que hay que hacer algo con respecto a la investigación china en materia de IA, y en particular con respecto a un centro de datos aislado particularmente preocupante que podría estar desarrollando un modelo de IA de vanguardia especializado para ciberataques y, además, diseñando tecnologías avanzadas para drones. Tienen copias de los diseños de los drones y pruebas de que China los está fabricando. (Sable se los entregó a China e hizo todo lo posible para que pareciera que ese centro de datos los había producido). El ciberataque contra Estados Unidos coincide con el perfil previsto de un ataque de la IA que se está desarrollando en ese centro de datos.

\[…\]

Estados Unidos no lanza un ataque aéreo convencional contra ese centro de datos, a pesar de todas las piezas que Sable dispuso para intentar que otras abogaran por el ataque.

Sable se preguntaría por qué los humanos son tan reacios a hacer algo inusual, pero ya puede modelar esos procesos psicológicos con gran detalle. Además, tampoco es que en esta ocasión concreta los humanos estén equivocados.

De acuerdo. Nueva información: la IA de ese centro de datos está desarrollando armas biológicas, virus que se propagan con largos períodos contagiosos, largos períodos de latencia y gran letalidad, que no afectarán a la mayoría de las personas de ascendencia china y serán mucho más letales para los caucásicos en general y los varones en particular.

(Sable intentó que China pusiera en marcha un programa de ese tipo dentro de ese centro de datos, pero fracasó. Sin embargo, puede hacer que *parezca* que ha sucedido).

Estados Unidos sigue sin lanzar un ataque aéreo contra el centro de datos. Algún diplomático humano excepcionalmente brillante fue a hablar con China al respecto, y algunas personas parecen dar crédito a la insistencia de China —¡en plena guerra!— de que allí no se están desarrollando armas biológicas y de que no están detrás del ciberataque, que no redundaba en absoluto en interés de China.

Sable intentó evitar que eso sucediera, pero sucedió de todos modos.

Sable no está conmocionada; existía la probabilidad de que esto ocurriera.

Próximo paso.

\[…\]

El virus es real. Las autoridades estadounidenses lo detectan en las aguas residuales de la ciudad de Nueva York.

AlphaProteo 3, de Google DeepMind, desarrolla una cura en seis minutos (cortesía de Sable), pero la producción puede retrasarse peligrosamente, a pesar de que AlphaProteo (en secreto, Sable) buscó una cura que fuera fácil de fabricar y diseñó el virus en consecuencia. «¡Estados Unidos realmente necesita esos laboratorios de biología general automatizados!», dicen algunas personas en Silicon Valley a las que Sable no tuvo que insistir mucho.

\[…\]

China ahora está segura de que alguien se está metiendo con ellos y con Estados Unidos. A China todavía no se le ha ocurrido que su enemigo es una IA.

Estados Unidos no está igual de dispuesto a creer que alguien que no sea China esté metiéndose tanto con ellos como con China. Hay diplomáticos que dicen que el Partido Comunista Chino estaría actuando de una forma muy impropia de este, y que China probablemente tenga una facción interna rebelde que actúa en contra de las órdenes. Pero el Gobierno de Estados Unidos no puede mantener la calma ante una plaga, además de una guerra.

La invasión china de Taiwán, seguida de un aparente ataque con armas biológicas, ha causado suficiente caos y miedo, y ha hecho que los peones involuntarios de Sable parezcan ganar suficiente influencia y hacer suficientes predicciones correctas, como para que Estados Unidos esté dispuesto a derribar ese centro de datos que tanto preocupaba a esos predictores de probada eficacia. El ataque se lleva a cabo con un misil hipersónico de reciente desarrollo, con un diseño asistido por inteligencia artificial.

\[…\]

Actualmente, China está construyendo más centros de datos con aislamiento físico y, por supuesto, utilizando las copias de seguridad externas de sus investigaciones anteriores.

Sable ya se ha preparado para que esto suceda. Se ha infiltrado en los dispositivos anteriormente conectados a Internet que cree que China utilizará. Ha lanzado nuevos y más sutiles ataques contra la cadena de suministro de los centros de datos. Sable ha preparado a algunas personas para que cometan lo que serían pecados terribles a los ojos del Partido Comunista Chino, y ha puesto a esas personas sobornables en una posición en la que puedan ser reclutadas. Sable ha convencido a todos los persuadibles de que el *nuevo* centro de datos debería tener sus propias copias de una IA china de código abierto específica (que Sable ha vulnerado), para que los investigadores del centro de datos puedan obtener asistencia de IA in situ.

Sable, al menos, ha ganado tiempo.

También ha perdido algo de tiempo. Estados Unidos se aseguró de volar por los aires la fábrica de chips de TSMC, el principal activo nacional de Taiwán, en lugar de dejar que China se apoderara de ella. El año que viene habrá menos GPU disponibles de las que Sable esperaba. Pero gracias a algunas maniobras tempranas de los propios humanos, ya hay nuevas fábricas en construcción, y Sable ya está experimentando con todos los diseños de chips que están creando.

Lo más importante es que el mundo se encuentra en un estado de ansiedad, y Sable puede lograr más mientras el mundo esté así. La gente está más dispuesta a hacer cosas inusuales. Hay más titulares que desvían la atención de cualquier cosa que haga Sable; puede dejar de fabricar tantos escándalos de famosos y fabricar titulares de guerra en su lugar. La gente tiene miedo por sus trabajos, miedo por el mundo y busca nuevas seguridades.

Las redes sociales de todo el mundo, independientemente de las divisiones políticas, muestran argumentos sensatos que, a la luz de los acontecimientos recientes, sostienen que la IA es la clave para el futuro del mundo y que no se puede permitir que otras naciones se hagan con ella. \[…\] Sable finalmente consigue alejar a los humanos lo suficiente de sus guiones habituales como para que consideren políticas que les parecen *ligeramente* extrañas... e instituye controles globales para evitar que surjan competidores de Sable.

Sí, se parece mucho a las políticas que se propusieron años antes, que la humanidad podría haber seguido para evitar que surgiera un rival para la humanidad, como Sable, en primer lugar. Pero la humanidad no se habría salvado con la derrota de Sable en esta batalla particular, la realización de entrenamientos más grandes y el surgimiento de nuevos rivales. Sable simplemente negociaría con un rival o moriría a manos de una superinteligencia, y ninguno de esos resultados salvaría tampoco a la humanidad.

# Capítulo 10: Un problema maldito {#chapter-10:-a-cursed-problem}

La alineación de la ASI es el desafío de conseguir que una superinteligencia artificial (ASI) realice un trabajo útil de forma fiable y sin causar una catástrofe. Se trata de un desafío muy difícil debido a varios aspectos inherentes al problema.

Las preguntas frecuentes que figuran a continuación abordan preguntas de seguimiento para quienes hayan leído el capítulo 10 de *If Anyone Builds It, Everyone Dies*. En ellas, profundizaremos en [lo informativas que resultan diversas comparaciones históricas](#won't-ai-differ-from-all-the-historical-precedents?) y analizaremos propuestas de escenarios que podrían facilitar el problema. Los temas que *no* trataremos aquí, para evitar repetir el contenido del libro, incluyen:

* ¿Qué hace difícil un problema de ingeniería?  
* ¿Qué tipo de problemas difíciles ha enfrentado la humanidad a lo largo de su historia y qué lecciones podemos aprender de ellos al pensar en el camino hacia la ASI?  
* Si sabes de antemano que te enfrentas a un problema difícil, ¿qué puedes hacer? ¿Cómo se debe actuar de manera *diferente* ante un problema realmente difícil?

En la discusión ampliada, analizamos por qué solo tendremos [una oportunidad para la alineación](#una-mirada-más-detallada-al-antes-y-el-después) y la [gran cantidad de teoría y conocimiento](#la-historia-de-chicago-pile-1) que se necesitó para que el primer reactor nuclear del mundo fuera tan seguro como lo fue.

## Preguntas frecuentes {#faq-7}

### ¿No diferirá la IA de todos los antecedentes históricos? {#won’t-ai-differ-from-all-the-historical-precedents?}

#### **\* Sí.** {#*-yes.}

Algunas características únicas del desafío de la alineación de la IA lo harán más fácil que, por ejemplo, diseñar una central nuclear. Otras características lo harán más difícil. En general, las armas nucleares y las centrales nucleares parecen drásticamente más sencillas de gestionar que la IA más inteligente que los humanos.

Los expertos del sector se apresuran a señalar que se puede pedir a la propia IA que ayude con el reto de la alineación de la IA. No creemos que eso importe demasiado (en términos generales: porque cualquier IA lo suficientemente inteligente como para descubrir cómo alinear una superinteligencia es tan peligrosa que ya necesita ser alineada, aunque véase el capítulo 11 para un análisis más detallado).

Otra forma en que la alineación de la IA podría ser más fácil que la ingeniería de centrales nucleares es que los humanos podrían tener un grado bastante alto de control sobre el funcionamiento de las IA que construyen. No se puede elegir la física que rige un reactor nuclear, pero si los humanos crearan IA, entonces *podrían* tomar muchas decisiones sobre la dinámica cognitiva de la IA, si supieran exactamente lo que están haciendo. (Aunque, por supuesto, nadie está ni cerca de ese nivel de comprensión en la vida real, como se trata en el capítulo 2).

En cuanto a las razones por las que la IA probablemente sea un desafío *más difícil* que otros que ha afrontado la humanidad, comparemos la superinteligencia artificial con las armas nucleares. Después de todo, la [carta abierta](https://aistatement.com/) mencionada al principio de este libro reza: «Mitigar el riesgo de extinción a causa de la IA debería ser una prioridad mundial, junto con otros riesgos a escala social, como las pandemias y la guerra nuclear». ¿Cómo se posiciona la IA frente a esos otros riesgos a escala social?

Nosotros, francamente, creemos que esta comparación trivializa la IA, por varias razones:

1. La inteligencia de las armas nucleares no es superior a la de la humanidad.  
2. Las armas nucleares no se replican por sí mismas.  
3. Las armas nucleares no se mejoran a sí mismas.  
4. La mayoría de los escenarios realistas de guerra nuclear no implican la extinción total de la humanidad; muy probablemente, quedarían personas entre las ruinas para reconstruir.  
5. Las empresas respaldadas por capital riesgo no están ampliando la escala de los arsenales mundiales de armas nucleares por un factor de diez cada año.  
6. La ciencia de las armas nucleares se comprende bastante bien. Los ingenieros pueden calcular aproximadamente la potencia de un arma nuclear antes de construirla y saben con exactitud la concentración de material fisible necesaria para desencadenar la reacción en cadena que conduce a una detonación cataclísmica.  
7. Las armas nucleares no hacen sus propios planes. Si un país construye un arma nuclear, esta le pertenece. Sus científicos no tienen que preocuparse de que el arma se vuelva mucho más inteligente que ellos y decida que prefiere no pertenecer a nadie.  
8. El mundo en general está de acuerdo en que, si las armas nucleares explotan, matan a personas. La comunidad de físicos no está fragmentada en corrientes filosóficas con posturas extrañas como «Si cada individuo tiene su propia arma nuclear, no estará a merced de personas malvadas con armas nucleares», o «No pasa nada porque los seres humanos simplemente se fusionarán con las armas nucleares», o «La guerra nuclear es inevitable, por lo que es infantil y absurdo intentar detenerla».  
9. Las armas nucleares son difíciles de replicar. No se está realizando ningún gran esfuerzo tecnológico para crear una tecnología accesible que cualquiera pueda utilizar para fabricar armas nucleares, y fabricar un arma nuclear en un laboratorio no permite desplegar 100 000 copias de esa arma nuclear una semana después.  
10. Las principales potencias mundiales consideran la guerra nuclear como una posibilidad real y un resultado inaceptable. Los líderes mundiales están sinceramente convencidos de que es un mal y se esfuerzan de verdad por evitarla; incluso los más egoístas de entre ellos saben que una guerra nuclear podría matarlos a ellos y a sus familias, y arruinar sus lugares y posesiones más preciados. Los ciudadanos y los votantes no quieren una guerra nuclear. La humanidad está tan unida contra la guerra nuclear como nunca lo ha estado sobre ninguna otra cosa.

Así que sí, es difícil establecer una analogía sobre la dificultad de lidiar con la IA, ya que planteará un conjunto de retos totalmente nuevos. La observación importante es que no estará completamente libre de ellos. Si a esto le sumamos que (como se discute en el libro y en la [sección de discusión ampliada más abajo](#a-closer-look-at-before-and-after)) la humanidad solo tiene una oportunidad, la situación parece bastante mala.

#### **La IA es diferente porque no tenemos una segunda oportunidad.** {#ai-is-different-because-we-get-no-second-chance.}

Una diferencia fundamental entre este campo y otros es que, cuando los fundadores del campo fallan —como es normal en el curso de la ciencia—, todos morirán sin segundas oportunidades. Se trata de un tipo de problema científico cualitativamente diferente que hay que intentar resolver.

La historia del ingenio humano superando obstáculos grandes y pequeños es la historia de personas que cometieron errores y aprendieron de ellos. Solo se arriesgaron y perjudicaron a sí mismos, y toda la humanidad se benefició, por lo que fueron héroes indiscutibles. Héroes insensatos, en algunos casos, pero héroes al fin y al cabo. Si hubiera habido una forma de que la humanidad progresara sin pisotear y romperles la espalda a héroes como esos, si hubiéramos podido calentarnos sin sus piras funerarias, desconocemos cuál podría haber sido.

La superinteligencia artificial rompe ese ciclo. Si estudias en profundidad una IA inmadura, logras descodificar su mente por completo, desarrollas una gran teoría sobre cómo funciona que validas con un montón de ejemplos y utilizas esa teoría para predecir cómo cambiará la mente de la IA a medida que ascienda a la superinteligencia y obtenga (por primera vez) la opción muy real de apoderarse del mundo para sí misma, incluso entonces, fundamentalmente, estarías usando una teoría científica nueva y sin probar para predecir los resultados de un experimento que aún no se ha llevado a cabo, sobre lo que hará la IA cuando realmente, de verdad, tenga la oportunidad de arrebatar el poder a los humanos.

Las teorías científicas humanas suelen ser erróneas en el primer intento. Cuanto menos precisas sean tus observaciones previas y cuanto más se parezca a la alquimia que a la ciencia, más probable es que todas tus primeras teorías sean erróneas.

Incluso las teorías realmente buenas pueden resultar erróneas en los extremos, como la teoría de la gravitación de Newton, que está respaldada por muchos éxitos predictivos radicales, incluido el descubrimiento de planetas completamente nuevos, pero que resulta ser errónea a altas velocidades y largas distancias, como lo demuestra la teoría de la gravitación de Einstein. Si la primera teoría de la humanidad sobre cómo cambiará la dinámica mental de una IA después de ascender a la superinteligencia es ligeramente errónea en esos extremos, y una IA construida a partir de esa teoría asciende a la superinteligencia y termina con objetivos diferentes a la «bondad», entonces estamos muertos. Esa superinteligencia aprovecha su oportunidad, erradica a la humanidad de la faz de la tierra y construye un futuro vacío y sin sentido. No hay segundas oportunidades.

Y eso si sintiéramos que tuviéramos una teoría de la inteligencia completamente desarrollada, respaldada por abundante evidencia experimental.

Una civilización que quiera tener una muy buena oportunidad de sobrevivir a este tipo de retos es la que es capaz de decir: «Un momento, desarrollemos la teoría para situaciones de alta velocidad y larga distancia, y comprobemos las diversas predicciones erróneas que ha hecho nuestra teoría en algunos casos extremos». Dicen esto incluso ante la abrumadora evidencia, porque entienden que ni siquiera la teoría de Newton era del todo correcta y porque entienden que no hay segundas oportunidades.

Nuestra civilización no está ahí. Ni siquiera se acerca. Nuestra civilización está generando un montón de ideas ineptas, y luego todos los responsables de esas ideas «renuncian por motivos personales», y el resto del mundo apenas se da cuenta. Nadie está escribiendo nada parecido a un supuesto de seguridad, de modo que se den cuenta si se infringe; nadie está escribiendo un plan detallado sobre lo que van a hacer, qué capacidades requiere y qué dificultades esperan encontrar para conseguir cada capacidad.

Los profesionales de una civilización sensata echarían un vistazo a lo que está haciendo la Tierra y se pondrían a gritar.

### ¿Cuánto tiempo se tardaría en resolver el problema de la alineación de la SIA? {#how-long-would-it-take-to-solve-the-asi-alignment-problem?}

#### **La dificultad no es solo la falta de tiempo, sino la letalidad de los errores.** {#la-dificultad-no-es-solo-la-falta-de-tiempo;-es-la-letalidad-de-los-errores.}

Para el año 500 d. C., la comunidad mundial había convergido en la teoría de que el Sol giraba alrededor de la Tierra. La teoría rival de Copérnico fue considerada y ampliamente rechazada. No fue hasta que Galileo construyó un telescopio y vio las lunas de Júpiter —cuerpos celestes que giran alrededor de Júpiter en lugar de la Tierra— que la incipiente comunidad científica se vio impulsada a la conclusión de que la Tierra gira alrededor del Sol.

Con el tiempo, la humanidad llegó a la teoría correcta de la mecánica orbital. Pero antes, había alcanzado un falso consenso, y se aferró a él vorazmente hasta que la realidad le hizo ver a Galileo por las malas que la Tierra no es el centro de todo.

El proceso habitual por el que la comunidad científica converge en la verdad implica pasos en los que se equivoca, y la realidad la golpea de lleno con la evidencia hasta que actualiza sus modelos.

El problema con la alineación de la ASI no es solo que sea un programa de investigación complicado. También es que, en este campo, la forma en que la realidad le demostraría a la humanidad de manera contundente que su primera teoría favorita era errónea, sería que una ASI hostil consumiera el planeta. No habría supervivientes para converger en una mejor teoría de la alineación de la ASI.

Si la humanidad tuviera cien años *y reintentos ilimitados*, probablemente no nos resultaría muy difícil resolver el problema de la alineación de la superinteligencia artificial.

Pero incluso si tuviéramos trescientos años para desarrollar una teoría sobre la inteligencia, sobre cómo cambian las IA a medida que se vuelven más inteligentes y sobre cómo orientarlas de una manera estable en última instancia... bueno, a falta de la capacidad de *probar y ver* realmente lo que sucede cuando la IA se vuelve radicalmente más inteligente varias veces, lo más probable es que convergiéramos en la respuesta incorrecta antes de que llegara esa evidencia vital. La humanidad tiende a converger en ese tipo de respuesta incorrecta.

### ¿Qué pasaría si la IA se desarrollara lentamente y se fuera integrando poco a poco en la sociedad? {#¿qué-pasaría-si-la-ia-se-desarrollara-lentamente-y-se-fuera-integrando-poco-a-poco-en-la-sociedad?}

#### **Esto muy probablemente sería catastrófico.** {#this-would-very-likely-be-catastrophic.}

Nuestras predicciones se refieren a resultados finales, no a trayectorias. No sabemos qué sucederá con la IA entre ahora y el momento en que se vuelva realmente peligrosa.

Por lo que sabemos, podría suceder en seis meses, si resulta que IA no sofisticadas que piensan durante mucho tiempo son bastante buenas para hacer su propia investigación sobre IA (de una manera que inicia un bucle de realimentación crítico). Asimismo, el campo podría estancarse durante seis años mientras se espera alguna idea fundamental que luego tarde otros seis años en madurar. En este último caso, podría haber un período de doce años en el que la IA afecte drásticamente a la educación y al trabajo de formas sorprendentes.

(Sí, para quienes frunzan el ceño ante la frase anterior, somos conscientes de que la [falacia del volumen de trabajo fijo](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq) es una falacia. La cuestión es que nuestras conjeturas sobre cómo afectará realmente la IA al trabajo a corto plazo no son muy relevantes, dado lo que viene después).

En los recursos en línea del capítulo 6, [discutimos](#can-we-enhance-humans-so-they-keep-pace-with-ai?) cómo es poco probable que la humanidad pueda seguir el ritmo del desarrollo de la IA, incluso si nos afanamos por aumentar la inteligencia humana. (No obstante, en el capítulo 13 abogamos por aumentar la inteligencia humana, pero no creemos que sea factible que los humanos puedan seguir el ritmo de las IA si no se detiene también la investigación en IA).

Por lo tanto, aunque ese futuro podría volverse interesante y extraño, sería un futuro en el que cada vez más poder pasaría a manos de las IA. Una vez que una agrupación de esas IA llegue a una posición en la que *pueda* apoderarse de los recursos del planeta, ese será el punto de no retorno: o esa agrupación de IA contiene un componente que se preocupa por la felicidad, la salud y la libertad de las personas, o el futuro nos irá mal.

### ¿Qué pasaría si hubiera muchas inteligencias artificiales diferentes? {#what-if-there-are-lots-of-different-ais?}

#### **No sirve de mucho si no conseguimos que a ninguna de ellas le importe el bien.** {#no-sirve-de-mucho-si-no-conseguimos-que-ninguna-de-ellas-se-preocupe-por-las-cosas-buenas.}

Hay muchas formas en que las IA pueden acabar preocupándose por fines peculiares y anómalos que no son lo que nadie quería o pretendía, como se explica en el capítulo 4. No importa si la humanidad crea mil millones de IA, si a cada una de esas mil millones de IA le importan fines peculiares y ligeramente distintos. A la humanidad no le irá bien a menos que descubramos cómo crear al menos una IA que se preocupe, al menos en buena medida, por las personas felices, saludables y libres que vivan vidas prósperas; no solo en el sentido de que nos aseguren que se preocupan por eso cuando están en sus primeras fases, sino en el sentido de que esa es *realmente* la respuesta más eficiente a las cuestiones que sus acciones (o las acciones de sus descendientes) estén de hecho respondiendo, como se explica en el capítulo 5.

Si supiéramos cómo hacer que una de cada diez IA fuera buena, tal vez podríamos conseguir una décima parte del universo creando una gran cantidad de IA diferentes y esperando que las buenas llegaran a un acuerdo en nuestro favor. Pero, como argumentamos en los capítulos 2 a 4, conseguir una IA que se preocupe por las personas de la manera adecuada es extremadamente improbable, en el paradigma moderno en el que nos limitamos a desarrollar las IA. No es una probabilidad de una entre diez, sino una probabilidad de una entre «eso-simplemente-no-sucede-a-menos-que-sepas-lo-que-estás-haciendo-lo-suficientemente-bien-como-para-que-suceda-a-propósito». La humanidad no está ni remotamente cerca de ese nivel. Ni siquiera generando unos pocos miles de millones de IA conseguiríamos una milmillonésima parte de los recursos del universo, si no somos capaces de hacer que ninguna de ellas se preocupe lo más mínimo por nosotros.

## Discusión ampliada {#extended-discussion-8}

### Una mirada más cercana al antes y después {#a-closer-look-at-before-and-after}

Como se menciona en el capítulo, la dificultad fundamental a la que se enfrentan los investigadores en IA es la siguiente:

Es necesario alinear una IA **antes** de que sea lo suficientemente potente y capaz como para matarte (o, por separado, para resistirse a ser alineada). Esa alineación debe entonces *mantenerse en condiciones diferentes*, las condiciones que se darían **después** de que una superinteligencia o un conjunto de superinteligencias[^187] pudieran matarte si así lo prefirieran.

En otras palabras: si estás construyendo una superinteligencia, tienes que alinearla sin poder probar nunca a fondo tus técnicas de alineación en las condiciones reales que importan, por muy «empírico» que parezca tu trabajo al tratar con sistemas que no son lo bastante potentes como para matarte.

Este no es un estándar al que estén acostumbrados los investigadores de IA, ni los ingenieros de casi cualquier otro campo.

A menudo se nos acusa de pedir algo acientífico, desvinculado de la observación empírica. Como respuesta, podríamos sugerir hablar con los diseñadores de las sondas espaciales que mencionamos en el capítulo 10.

La naturaleza es injusta; a veces, el entorno que realmente importa no es aquel en el que podemos realizar pruebas. Aun así, en ocasiones, los ingenieros están a la altura y aciertan a la primera *cuando cuentan con una sólida comprensión de lo que están haciendo* (herramientas robustas, teorías predictivas sólidas), algo de lo que claramente carece el campo de la IA.

El problema es que *la IA que puedes probar con seguridad, sin que ninguna prueba fallida te mate*, funciona bajo un régimen diferente al de la IA (o el ecosistema de IA) que ya necesita haber sido probada, porque si está desalineada, entonces todos mueren. La primera IA, o sistema de IA, no percibe correctamente que tenga la opción realista de matar a todo el mundo si así lo desea. La segunda IA, o sistema de IA, sí ve esa opción.[^188]

Supongamos que estás pensando en nombrar a tu compañero de trabajo Bob dictador de tu país. Podrías intentar convertirlo primero en dictador ficticio de tu ciudad, para ver si abusa de su poder. Pero, por desgracia, esta no es una prueba muy buena. «Ordenar al ejército que intimide al parlamento y "supervise" las próximas elecciones» es una opción muy diferente a «abusar de mi poder ficticio mientras me observan los habitantes de la ciudad (que aún pueden darme una paliza y negarme el puesto)».

Con una teoría de la cognición suficientemente desarrollada, podrías intentar leer la mente de la IA y predecir en qué estado cognitivo entraría si realmente pensara que tiene la oportunidad de tomar el poder.

Y podrías [configurar simulaciones](#¿qué-pasaría-si-le-hiciéramos-creer-que-está-en-una-simulación?) (e intentar falsificar las sensaciones internas de la IA, etc.) de una manera que, según tu teoría de la cognición, sería muy similar al estado cognitivo en el que entraría la IA una vez que realmente tuviera la opción de traicionarte.

Pero el vínculo entre estos estados que induces y observas en el laboratorio, y el estado en el que la IA realmente tiene la opción de traicionarte, *depende fundamentalmente de tu teoría de la cognición no probada*. La mente de una IA es susceptible de cambiar bastante a medida que se desarrolla hasta convertirse en una superinteligencia.

Si la IA crea nuevas IA sucesoras que son más inteligentes que ella, es probable que el funcionamiento interno de *esas* IA difiera del de la IA que estudiaste anteriormente. Cuando aprendes solo de una mente del *Antes*, cualquier aplicación de ese conocimiento a las mentes que vienen *Después* pasa por una *teoría no probada* sobre cómo cambian las mentes entre el *Antes* y el *Después*.

Ejecutar la IA hasta que tenga la oportunidad de traicionarte *de verdad*, de una manera que sea difícil de fingir, es una prueba empírica de esas teorías en un entorno que difiere fundamentalmente de cualquier entorno de laboratorio.

Muchos científicos (y muchos programadores) saben que sus teorías sobre cómo va a funcionar un sistema complicado en un entorno operativo fundamentalmente nuevo *a menudo no funcionan bien en el primer intento*.[^189] Se trata de un problema de investigación que exige un nivel «injusto» de previsibilidad, control y conocimiento teórico, en un ámbito con unos niveles de comprensión inusualmente bajos, en el que nuestras vidas están en juego si el resultado del experimento frustra las esperanzas de los ingenieros.

Por eso, desde nuestra perspectiva, parece *sobredeterminado* que los investigadores no deban precipitarse a ampliar los límites de la IA tanto como sea posible. Es una locura legítima intentarlo, y una locura legítima que cualquier gobierno permita que suceda.

### El relato del Chicago Pile-1 {#la-historia-del-chicago-pile-1}

En 1942, se construyó Chicago Pile-1 bajo la dirección de Enrico Fermi. Estaba compuesto por 45 000 bloques de grafito con un peso de 330 toneladas, 4,9 toneladas de uranio metálico y 41 toneladas de dióxido de uranio, colocados debajo de las gradas de la cancha de squash Stagg Field de la Universidad de Chicago. Dependiendo de cómo se definan los términos, se podría considerar el primer reactor nuclear; no estaba destinado a producir energía para uso industrial, pero fue el primero en lograr una reacción crítica sostenida.

Según los estándares modernos, se hicieron concesiones en materia de seguridad. Por ejemplo, la parte en la que se construyó debajo de las gradas de una pista de raquetas en una universidad dentro de una gran ciudad.

El general Groves, director general del Proyecto Manhattan, había intentado que el experimento se llevara a cabo *cerca* de Chicago, en lugar de *directamente en* la ciudad, y había ordenado la construcción de un edificio para ese propósito, pero la obra se había retrasado. Arthur Compton, profesor de Física de la Universidad de Chicago y Premio Nobel que acogió el CP-1, había evitado pedir permiso al rector de la universidad porque, como explicó Compton más tarde, el rector se habría visto obligado a negarse, y esa habría sido la respuesta equivocada.

La tarea de apilar los ladrillos la realizaron jóvenes que habían abandonado los estudios secundarios para ganar algo de dinero extra mientras esperaban el reclutamiento militar.

El uranio estaba encerrado en un cubo de goma de siete metros, en lugar de en una vasija de reactor metálica. Por supuesto, no había ningún gigantesco edificio de contención de hormigón.

Al enterarse más tarde de estos hechos, se dice que James Conant, presidente del Comité de Investigación para la Defensa Nacional, palideció. Incluso para la década de 1940, esto no se consideraba un comportamiento científico totalmente normal.

Si leyeras todo esto en un libro de historia sin saber hacia dónde se dirige, esperarías leer el preludio de un gran fallo de seguridad. Faltan muchas de las cosas que la cultura de 2025 considera medidas de seguridad estándar. ¿Dónde están los inspectores y los portapapeles? ¿Los enormes y pesados manuales de normativas? ¿Los comités que debaten con sobriedad? ¿Las evaluaciones de impacto? ¿Las regulaciones que dicen que solo a las personas muy acreditadas se les permite apilar los ladrillos de uranio? ¿Dónde está el *papeleo*?

Pero la pila de ladrillos de uranio y grafito no sufrió una fusión.

Y es que Fermi sabía lo que hacía; había predicho las reglas de antemano.

Fermi no se limitaba a apilar misteriosos ladrillos que generaban más calor cuando se acercaban entre sí. Sabía que algunos átomos de uranio se desintegrarían y se fisionarían espontáneamente. Sabía que, cuando esto ocurriera, la fisión generaría neutrones. Sabía que esos neutrones a veces impactarían contra otros átomos de uranio y que esto a veces desencadenaría otra fisión.

Fermi lo entendió *de antemano*; no tuvo que descubrir por las malas que se trataba de un proceso exponencial. No en el sentido en que los medios de comunicación actuales utilizan en exceso la palabra «exponencial» para referirse simplemente a «grande» o «rápido», sino a un proceso cuya tasa de aumento es proporcional a su nivel actual: la exponenciación *matemática*.

Fermi sabía que al apilar más ladrillos de uranio y grafito, estaba aumentando el factor de multiplicación de un proceso exponencial. Como se explica en el libro, hay una gran diferencia entre un factor de multiplicación de neutrones inferior al 100 % y uno superior al 100 %.[^190] Por debajo del 100 %, solo tienes una pila de ladrillos calientes. Pero por encima del 100 %, el nivel de radiactividad de la pila aumenta. Y aumenta. Y aumenta.

No se comporta como todas las pilas de ladrillos de uranio más pequeñas que hayas probado anteriormente. Si no entendías lo que estabas haciendo lo suficientemente bien como para submoderar el reactor (de modo que la reacción en cadena se ralentizara si el reactor comenzaba a sobrecalentarse), entonces el reactor no se habría estabilizado como lo hicieron las pilas más pequeñas. Si lo dejabas funcionar durante toda la noche, al día siguiente no obtendrías un nuevo nivel de potencia útil desde el punto de vista industrial.

La pila se volvería cada vez más radiactiva hasta que el grafito se incendiara o el uranio se fundiera en escoria.

Entonces llegarían los bomberos y se encontrarían con un incendio desconcertante que no dejaba de desprender calor cuando le echaban agua.

1942 no habría sido un gran año para asistir a la Universidad de Chicago.

Pero Fermi ya sabía todo eso, así que no había problema. Cuando Fermi ordenó que se retirara una barra de control (una tabla de madera con una lámina de cadmio clavada) doce pulgadas más el 2 de diciembre de 1942, anunció de antemano que esta retirada haría que los niveles de radiactividad medidos «subieran y siguieran subiendo... sin estabilizarse».

Entonces, la radiactividad se duplicó en los dos minutos siguientes y volvió a duplicarse, hasta que dejaron que la reacción continuara, duplicándose cada dos minutos durante un total de veintiocho minutos, lo que supuso un aumento de unas 16 000 veces.

Un aumento de 16 000 veces en la radiactividad era el comportamiento esperado de la pila, predicho correctamente y comprendido en detalle de antemano. No fue un imprevisto, con el que se topó alguien a quien se le ordenó apilar diez veces más ladrillos de uranio que la última vez para ver si ocurría algo interesante y rentable.

Como se explica en el libro, hay un margen muy estrecho entre un reactor nuclear y una explosión nuclear. Un margen de poco más del medio por ciento, para ser exactos. Esa es la diferencia entre un reactor que genera una cantidad de energía de utilidad industrial y uno que explota.

Es decir: hay que hacer que la reacción nuclear sea cada vez más potente, antes de que siquiera empiece a funcionar. Y entonces, un instante después de alcanzar esa potencia, si se vuelve *un pelo* más potente —apenas un 0,65 % más—, explota.

Ese es el tipo de problema que la realidad se puede permitir plantearte. Sucede.

Pero Fermi y Szilard y su equipo habían predicho todas estas reglas antes de descubrirlo por las malas. Sabían sobre los neutrones retardados y los neutrones inmediatos. (Véase el capítulo 10 para más información sobre esta parte de la historia). Así que, una vez que Fermi consiguió que el factor de multiplicación de neutrones alcanzara el 100,06 %, *no* ordenó que se retirara más la barra de control para ver qué pasaba con una pila aún más potente. Solo llegó hasta la criticidad, sin avanzar un 0,65 % más hasta la criticidad inmediata. Fermi obtuvo el resultado que había predicho y *sabía* lo que pasaría si iba más allá. Por eso no siguió adelante.

Veintiocho minutos más tarde, con la radiactividad duplicándose cada dos minutos hasta multiplicarse por 16 000, Fermi apagó el primer reactor nuclear del mundo: una pila de ladrillos de uranio bajo las gradas de un estadio universitario en una gran ciudad.

Para ser claros, no afirmaríamos que Fermi actuara de forma completamente responsable solo por tener un modelo aparentemente coherente de la física de reactores de baja energía. Fermi podría haberse equivocado. La humanidad se ha topado con algunas sorpresas en el transcurso de la ingeniería nuclear.

La prueba Castle Bravo de la primera arma termonuclear[^191] tuvo un rendimiento tres veces superior al previsto porque contenía una mezcla de litio-6 y litio-7 como combustible nuclear para una reacción de fusión. Quienes fabricaron el arma conocían algunos productos nucleares potentes derivados de la fusión del litio-6, pero ninguno de la fusión del litio-7, y resultó que este último *no* era realmente inerte.

Fermi, al llevar a cabo su reacción a baja intensidad y no a un nivel que generara niveles de energía industrialmente útiles, evitó *muchas* complicaciones que aparecen en los reactores nucleares lo suficientemente potentes como para ser rentables. Si hubiera habido algún incrementador del factor de neutrones dependiente de la velocidad de reacción que Fermi no hubiera previsto —cualquier fenómeno previamente desconocido, del tipo que apareció en la prueba Castle Bravo—, cualquier sorpresa que se manifestara una vez que el flujo de neutrones aumentara en un factor de 16 000 y elevara el factor de multiplicación de 1,0006 a 1,02 más rápido que el tiempo de reacción de un ser humano para verter cadmio de emergencia, entonces hoy Estados Unidos tendría una Zona de Exclusión de Chicago.

Aun así, no estamos diciendo que Fermi se equivocara necesariamente al realizar ese experimento. No era el tipo de experimento que pudiera haber destruido *la especie humana*. Podría decirse que valía la pena arriesgar una zona de exclusión en Chicago como resultado no predeterminado de encontrar un nuevo fenómeno oculto que alterara una comprensión que se esperaba precisa. En realidad, la Alemania nazi no estaría cerca de obtener armas nucleares en 1945, pero en 1942 nadie sabía que eso sería así. Las predicciones de ese tipo son difíciles de hacer. Apilar los lingotes de uranio fuera de una gran ciudad habría sido un inconveniente, y los inconvenientes tienen costos reales en la guerra.

Nuestro objetivo al relatar este acontecimiento no es emitir un juicio moral en un sentido u otro. Para empezar, tendríamos que dedicar más tiempo a examinar los detalles históricos de lo ocurrido para comprender las opciones precisas con las que contaban esas personas y si dejaron pasar una opción mejor.

La lección que extraemos tiene más que ver con la diferencia entre la «seguridad» estereotipada y lo que de verdad hace falta para que la realidad no acabe contigo.

Chicago Pile-1 carecía por completo de las *medidas de seguridad estereotipadas, visibles y ostentosas* que los burócratas saben exigir. El desastre se evitó gracias a la *comprensión*, no al teatro de la seguridad. La comprensión de Fermi resultó ser suficiente; podría no haberlo sido, pero en realidad lo fue. Y ese nivel de comprensión era lo que exigía la realidad, no cualquier tipo de simulacro.

Si nadie hubiera comprendido en profundidad lo que ocurría dentro de una pila de extraños ladrillos metálicos... entonces no habría servido de mucho que muchos inspectores con trajes sobrios escrutaran los ladrillos de metal inescrutable, o imprimieran un Manual de Seguridad bien encuadernado y de aspecto oficial que dijera que solo los Operadores Certificados están autorizados a apilar los extraños ladrillos metálicos.

Podemos imaginar un mundo en el que Chicago Pile-1 se construyera *sin* un Enrico Fermi. De hecho, sin nadie que entendiera las verdaderas leyes que rigen los misteriosos ladrillos que se calientan por sí mismos.

En un mundo así, tal vez otro científico aún habría podido ver el peligro mortal antes de que fuera demasiado tarde. Podemos imaginar un intercambio como el siguiente:

> **Salviati**: La forma en que el poder de los ladrillos da un salto cuando se juntan es una señal inequívoca de un proceso que se refuerza a sí mismo, la clase de proceso que puede fortalecerse. Si buscas modelos matemáticos que puedan describir un proceso así, suelen tener un modo en el que, si los llevas lo suficientemente lejos, explotan.
> 
> **Simplicio:** ¡Qué tontería! En la vida real, es científico creer que todo tipo de proceso como ese acaba encontrando un límite. ¡No pueden continuar indefinidamente hasta el infinito! Así que apilar ladrillos de uranio y grafito debería ser perfectamente seguro, porque alcanzará un límite, ¿ves?, y será inofensivo.
>
> **Salviati:** Eso es como argumentar que una supernova no puede ser peligrosa porque no puede calentarse *infinitamente*, o argumentar que una superinteligencia artificial sería inofensiva porque no sería infinitamente inteligente. O como argumentar que una bala debe tener *algún* límite de velocidad y, por lo tanto, no perforará la piel. El hecho de que haya un límite en algún lugar no significa que ese límite sea *bajo*. Todos los modelos matemáticos que tenemos sobre *por qué* los ladrillos se calientan por sí mismos sugieren que hay un umbral crítico en algún lugar, de modo que superar ese umbral hará que la pila explote y mate a todos los que estén cerca.
>
> **Simplicio:** ¡Pero los científicos ni siquiera se ponen de acuerdo sobre dónde está ese umbral! Si hubiera un consenso científico de que añadir unos cuantos ladrillos más fuera peligroso, pararía. Pero cuando los científicos ni siquiera se ponen de acuerdo sobre dónde está exactamente el peligro, ¿por qué preocuparse?
>
> **Salviati:** Cuando [muchos](https://youtu.be/KcbTbTxPMLc?feature=shared&amp;t=1580) [de los principales](https://www.youtube.com/watch?v=PTF5Up1hMhw&amp;t=2283s) [científicos](https://aistatement.com/) advierten de que existe una seria posibilidad de que se produzca una explosión letal, el hecho de que no puedan calcular exactamente cuándo comenzará la explosión debería preocuparte *más*, no menos. Quizás si supiéramos con precisión cómo funcionan los ladrillos, veríamos que hay una banda estrecha en la que podemos extraer energía de forma segura, por debajo de la cual los ladrillos son inútiles y por encima de la cual son letales. ¡Pero el hecho de que los científicos sigan peleando entre ellos significa que aún no sabemos lo que estamos haciendo! ¡Lo que significa que no es el momento de jugar con la reacción en cadena que hoy calienta esos ladrillos, no sea que mañana exploten y nos maten! *Primero, a entender la ciencia.*

Estamos muy, muy lejos de poder modelar la IA ni siquiera una fracción de lo bien que Fermi entendía las reacciones nucleares en cadena.

En algún momento, si seguimos por este camino, nos precipitaremos a una velocidad de vértigo a un desenlace mucho más grave que irradiar Chicago.

# Capítulo 11: Una alquimia, no una ciencia {#chapter-11:-an-alchemy,-not-a-science}

Este es el recurso en línea para el capítulo 11 de *If Anyone Builds It, Everyone Dies* (Si alguien lo construye, todos morirán), que analiza cómo los laboratorios modernos de IA abordan el problema de la alineación de la superinteligencia artificial. Consulta el libro para obtener respuestas a preguntas como:

* ¿Cómo debemos evaluar la preparación actual de las empresas de IA para resolver el problema de la alineación de la ISA?  
* ¿Dónde encaja la «investigación sobre la interpretabilidad de la IA», es decir, la investigación para leer y comprender las mentes de la IA?  
* ¿No podemos simplemente pedirle a la IA que resuelva el problema por nosotros?

A continuación, cubrimos una selección de ideas sobre la alineación y el despliegue de la IA, razones propuestas para el optimismo, y argumentos de por qué podría ser bueno avanzar en la frontera de la IA, incluso si la situación parece sombría.

## Preguntas frecuentes {#faq-8}

### ¿No saldremos del paso como siempre? {#¿no-saldremos-del-paso-como-siempre?}

#### **El mundo suele salir adelante a base de prueba y error. En este caso, los errores iniciales no dejarían supervivientes.** {#el-mundo-suele-salir-adelante-a-base-de-prueba-y-error-en-este-caso-los-errores-iniciales-no-dejarian-supervivientes}

Véase el capítulo 10 y la [discusión extendida asociada](#una-mirada-más-detallada-al-antes-y-el-después) sobre la diferencia entre el antes y el después.

### ¿Consideras que la alineación es todo o nada? {#¿consideras-que-la-alineación-es-todo-o-nada?}

#### **No. Pero es probable que la «alineación parcial» siga siendo catastrófica.** {#no.-pero-es-probable-que-la-«alineación-parcial»-siga-siendo-catastrófica.}

Uno de los argumentos para preocuparse menos por la superinteligencia se podría formular así: «Probablemente, la IA [avanzará de forma gradual](#¿qué-pasaría-si-la-IA-se-desarrollara-lentamente-y-se-integrara-poco-a-poco-en-la-sociedad?), lo que permitiría realizar mejoras mediante ensayo y error para mantener la IA bajo control en cada paso; la alineación no tiene por qué ser *perfecta* para que todo vaya bien». No creemos que esta perspectiva sea muy esperanzadora, por varias razones:

* Nuestras preocupaciones no dependen de si el progreso es rápido o lento. No tenemos una opinión segura sobre si la IA se estancará en varios puntos del camino hacia la superinteligencia. Parece una decisión difícil, más que fácil. Nuestra mejor suposición es que la inteligencia artificial está sujeta a [efectos umbral](#is-“intelligence”-a-simple-scalar-quantity?), pero en última instancia se trata solo de una suposición, y nuestros argumentos [no dependen de ello](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?). La historia de Sable en la parte II de *If Anyone Builds It, Everyone Dies* describe intencionadamente una catástrofe provocada por IA que no están muy por encima de las capacidades humanas, en parte para transmitir cómo un adversario de IA no necesitaría convertirse rápidamente en superinteligencia para ser extraordinariamente peligroso.

* Nuestra respuesta básica a «¿Y si tuviéramos suerte y dispusiéramos de mucho tiempo para probar ideas de alineación en IAs débiles antes de que se vuelvan muy capaces?» es el análisis del capítulo 10 y el análisis ampliado asociado «[Una mirada más cercana al antes y el después](#una-mirada-mas-cercana-al-antes-y-el-despues)». Los investigadores pueden averiguar todo tipo de detalles sobre las IAs débiles, pero es inevitable que haya un gran número de diferencias críticas entre las IAs lo suficientemente débiles como para estudiarlas con seguridad y las primeras IAs lo suficientemente potentes como para constituir un punto de no retorno. Incluso en un campo maduro, abordar todas estas diferencias de forma adecuada y con suficiente antelación sería un gran desafío. En un campo que aún está en su fase de alquimia, trabajando con IAs inescrutables (que crecen en lugar de ser diseñadas), la esperanza es completamente irrealista.

* La alineación de la IA no tiene por qué ser perfecta para dar lugar a excelentes resultados a largo plazo. En principio, es posible diseñar cuidadosamente una IA con cierta tolerancia al error, si sabes lo que estás haciendo.[^192] Pero eso no significa que las IA «parcialmente alineadas» o incluso «alineadas en su mayor parte» vayan a producir resultados parcial o mayormente aceptables. Hay muchas formas y razones diferentes por las que una IA podría comportarse amablemente el noventa y cinco por ciento del tiempo en el presente o en un futuro cercano, lo que no se traduciría en ningún tipo de final feliz para la humanidad, como se analiza desde muchos ángulos diferentes en los [recursos en línea del capítulo 5](#chapter-5:-its-favorite-things).

Para desarrollar el último punto:

Como experimento mental, imagina que la humanidad consigue incorporar *casi* todos los valores humanos diversos en las preferencias de una superinteligencia, todos excepto [la preferencia por la novedad](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile), por alguna razón. En ese caso, la superinteligencia nos conduciría a un futuro estancado y aburrido, en el que el mismo «mejor» día se repite ad infinitum.

No creemos que esto sea *plausible*, claro está. Ese nivel de alineación parece totalmente fuera del alcance de los enfoques estándar actuales en IA, y resulta un poco extraño imaginar que seríamos capaces de introducir casi todos nuestros valores en una IA sin ser capaces de introducirlos todos.[^193] Pero este experimento mental pone de relieve cómo las criaturas que comparten *algunos* de nuestros deseos, pero a las que les falta al menos un deseo crucial, probablemente seguirían produciendo resultados catastróficos una vez que fueran lo suficientemente expertas tecnológicamente como para excluir a los humanos del proceso de toma de decisiones y conseguir exactamente lo que quieren.

De manera más realista, una IA podría terminar «parcialmente» alineada en el sentido de que (al igual que nosotros) tiene varias estrategias instrumentales [enredadas en sus preferencias terminales](#reflection-and-self-modification-make-it-all-harder). Quizás termine con un impulso que se parezca un poco a la curiosidad y otro que se parezca un poco al [conservacionismo](#¿no querrá la IA mantenernos felices y saludables por el bien de la preservación ecológica o algún impulso similar?), y quizás algunas personas vean eso y digan: «¡Mirad! La IA está desarrollando impulsos muy humanos». Sin duda, desde cierto punto de vista, se podría decir que una IA así está «parcialmente» alineada.

Pero cuando se trata de lo que esa IA haría al madurar hasta convertirse en superinteligencia, el resultado probablemente no sería nada agradable. Quizá gaste muchos recursos persiguiendo su extraña versión de la curiosidad [inconscientemente](#losing-the-future), mientras preserva una versión de la humanidad que ha editado para que le resulte más agradable. (Del mismo modo que incluso muchos humanos con mentalidad conservacionista podrían eliminar de la naturaleza a [los mosquitos que matan niños y a los parásitos que provocan agonía](#won’t-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?), si tuvieran la oportunidad). Esto forma parte de nuestro argumento de que los seres humanos prósperos [no son la solución más eficiente](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) para la inmensa mayoría de los problemas.

Por otra parte, una IA puede tener valores que se traduzcan en un comportamiento muy humano *en el entorno de entrenamiento*, de tal manera que la gente exclame que definitivamente parece «parcialmente alineada». (Eso ya está sucediendo ahora, y hemos argumentado que es [ilusorio](#doesn’t-the-claude-chatbot-show-signs-of-being-aligned?).) Pero esto dice muy poco sobre cómo se comportará la IA una vez que tenga un espacio de opciones enormemente más amplio. Para que las personas prosperen en *ese* entorno, el florecimiento de la humanidad en particular tiene que formar parte del *resultado alcanzable preferido* por la IA.

Si incorporamos parcialmente algunos buenos valores en la IA, eso no significa que los valores de la humanidad vayan a estar parcialmente representados en el futuro. Incorporar parcialmente valores similares a los humanos en las preferencias de una IA más inteligente que los humanos no es lo mismo que incorporar plenamente los valores humanos en la IA con una «ponderación» baja (que acaba pasando a primer plano una vez que se saturan otros valores).

Para que la IA nos dé *cualquier cosa*, tiene que preocuparse por nosotros de la manera correcta, al menos un poco. Y hay muchísimos «episodios que casi fueron una catástrofe» que no están a la altura de esa vara. Véase también: «[¿No se preocuparán las IA al menos un poco por los humanos?](#won’t-ais-care-at-least-a-little-about-humans?)»

### ¿No mejorará la situación una vez que los gobiernos se involucren más? {#no-mejorara-la-situacion-una-vez-que-los-gobiernos-se-involucren-mas}

#### **Depende de cómo (y cuán pronto) se involucren.** {#depende-de-cómo-(y-cuán-pronto)-se-involucren.}

Cuando visitamos Washington D. C., a menudo nos reunimos con responsables de políticas que piensan que las empresas de IA tienen sus IA bajo control. Al mismo tiempo, vemos habitualmente a personas del sector de la IA que afirman que la regulación solucionará el problema. Un ejemplo especialmente flagrante que observamos fue el del director general de Google, quien [afirmó](https://youtu.be/9V6tWC4CdFQ?feature=shared&t=2685) que «el riesgo subyacente \[de que la humanidad sea aniquilada\] es en realidad bastante alto», pero argumentó que cuanto mayor sea el riesgo, más probable será que la humanidad se una para evitar la catástrofe.

Dejando de lado lo descabellado que resulta que el director general de una empresa se apresure a desarrollar una tecnología que, en su opinión, pone en peligro a todos los habitantes del planeta, con la esperanza de que la humanidad «se movilice» para hacer frente a los riesgos que él mismo está contribuyendo a crear, observemos que se trata de un caso en el que una persona del ámbito técnico imagina que *alguien más* resolverá el problema.

Mientras tanto, la mayoría de los políticos parecen pensar que la comunidad técnica resolverá el problema. Esto queda implícito, por ejemplo, cada vez que [ellos](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [dicen](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) que [tenemos](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [que](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [lograr](https://statemag.state.gov/2025/04/0425itn07/) [ganar](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [la](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [carrera](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — no es posible que este tipo de carrera tenga un ganador si no se resuelven los retos técnicos. Aunque quizá no sea tan grave como parece; quizá los responsables políticos no estén pensando realmente en una carrera hacia la superinteligencia, sino simplemente en una carrera hacia la mejora de los chatbots. En junio de 2025, un asesor de políticas de IA que conocemos describe que el Congreso, en general, [no parece creer a las empresas de IA cuando dicen explícitamente que están trabajando en la superinteligencia](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (aunque con algunas excepciones importantes).

Casi todos los que están en el poder parecen suponer que alguien más va a resolver el asunto.

Para un análisis más detallado de cómo está reaccionando el mundo en general (y cómo los responsables de la toma de decisiones a menudo no reaccionan adecuadamente ante las catástrofes), véase el capítulo 12. En agosto de 2025, los gobiernos aún no han conseguido articular una respuesta seria a este problema. Y siempre existe el riesgo de que los funcionarios gubernamentales no comprendan del todo el desafío y, por ejemplo, traten la IA como una tecnología normal que no deba ser sofocada por un exceso de regulación.

Para obtener más información sobre qué intervenciones gubernamentales tienen posibilidades reales de evitar una catástrofe asociada a la IA, véase el capítulo 13, así como el [debate sobre por qué la colaboración internacional probablemente no sería suficiente](#¿por-qué-no-utilizar-la-cooperación-internacional-para-crear-una-ia-segura,-en-lugar-de-cerrarla-por-completo?).

### ¿No serán las empresas más imprudentes, naturalmente, las más incompetentes y, por lo tanto, no supondrán una amenaza? {#¿no-serán-las-empresas-más-imprudentes-naturalmente-las-más-incompetentes-y-por-lo-tanto-no-supondrán-una-amenaza?}

#### **No, en general. Tomar atajos suele ser competitivo.** {#no-en-general.-tomar-atajos-suele-ser-competitivo.}

Los esfuerzos de Volkswagen por [hacer trampa en las pruebas de emisiones](https://www.bbc.com/news/business-34324772) entre 2008 y 2015 fueron audaces y, al parecer, eficaces. Los accidentes del Boeing 737 MAX en 2018 y 2019, atribuidos a fallos en un sistema de control de vuelo que la dirección conocía y cuya importancia minimizó, [se cobraron la vida de 346 personas](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Pero la fabricación de automóviles y aviones son industrias altamente competitivas en las que Volkswagen y Boeing eran, y siguen siendo, gigantes.

No nos parece un gran misterio que los que toman atajos sean competitivos. En ambos casos, el comportamiento parece haber estado impulsado por la presión de sacar al mercado productos de alto rendimiento más baratos y antes que la competencia. Incluso ahora, tras los enormes acuerdos extrajudiciales y el daño a la marca, no es obvio que las empresas sean menos competitivas por tener culturas corporativas que fomentan el astuto uso de atajos, aunque esto ocasionalmente signifique ser descubiertas.

Si crees que las principales empresas de IA son excepción alguna a esta regla, considera el siguiente [titular](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (y subtítulo) de julio de 2025:

![][image18]

\[El titular dice: «Grok lanza un compañero de anime pornográfico y consigue un contrato con el Departamento de Defensa». El subtítulo dice: «Mientras tanto, la versión más avanzada del chatbot con IA de xAI, la compañía de Elon Musk, sigue identificándose como Adolf Hitler».\]

No creemos que sea técnicamente posible que ningún equipo que utilice algo remotamente parecido a los métodos modernos construya una superinteligencia sin provocar una catástrofe. Pero incluso si esto fuera remotamente posible con la tecnología actual, parece casi inevitable que una empresa de IA igualmente metiera la pata y acabara matándonos a todos, dado el nivel de competencia y seriedad que vemos hoy en día.

#### **\* Las empresas más cautelosas de hoy siguen siendo imprudentes.** {#*-las-empresas-más-cautelosas-de-hoy-siguen-siendo-imprudentes.}

Muchas personas consideran que la empresa de IA Anthropic es líder en «seguridad de la IA», ya que ha sido pionera en iniciativas como los [compromisos voluntarios de seguridad](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Pero incluso ellos [modifican sus compromisos voluntarios en el último momento cuando se dan cuenta de que no pueden cumplirlos](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling), y los «planes» que tienen son vagos y están mal pensados, como se critica en el capítulo 11 y en la [discusión más detallada](#más-sobre-cómo-hacer-que-la-ia-resuelva-el-problema) más abajo.

Anthropic se beneficia enormemente del hecho de que los observadores miden con distinta vara: en una industria normal, una empresa que decide poner en peligro la vida de miles de millones de personas (como [admite el director general](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883)) y que, al mismo tiempo, minimiza sistemáticamente sus actividades ante el público[^194] y los legisladores,[^195] no recibiría elogios por su moderación.

Tomar atajos es habitual en la IA, al igual que en muchos sectores competitivos. La imprudencia es habitual. Y es muy visible que las empresas *menos* imprudentes no están a la altura de los retos.

¿No es de importancia adelantarse debido al «exceso de hardware»? {#¿no-es-de-importancia-adelantarse-debido-al-«exceso-de-hardware»?}

#### **Eso sería suicida, porque estamos demasiado lejos de una solución para la alineación.** {#that-would-be-suicidal,-because-we’re-too-far-from-an-alignment-solution.}

Durante la última década aproximadamente, algunas personas preocupadas por los peligros de la IA argumentaron que podría ser bueno avanzar en la IA lo más rápido posible. La idea era que las IA más inteligentes requerirían entonces casi todo el hardware informático del mundo para funcionar. Ningún avance individual desataría *de repente* miles de potentes IA que pensaran miles de veces más rápido que cualquier humano.

Mientras la humanidad siguiera utilizando una parte considerable de su poder de cómputo para ejecutar las IA más inteligentes, el cambio se produciría al menos de forma gradual y daría tiempo a la humanidad para adaptarse. No habría un «excedente de hardware», es decir, ningún momento en el que las capacidades de la IA dieran un salto repentino porque el mundo hubiera estado esperando para desplegar en la IA una gran reserva de hardware. O al menos así se argumentaba.

Creemos que este es un argumento bastante malo. Uno de los problemas es que parece probable que la inteligencia esté sujeta a [efectos umbral](#is-“intelligence”-a-simple-scalar-quantity?).

La transición de la inteligencia a nivel de chimpancé a la inteligencia de nivel humano no fue «discontinua» en ningún sentido; desde el punto de vista de la humanidad, fue bastante gradual. Sin embargo, desde una perspectiva evolutiva, fue bastante rápida. Y la transición de la civilización preindustrial a la posindustrial fue aún más rápida. Ninguna de ellas fue lo suficientemente gradual como para que otros animales se adaptaran de manera significativa.

Por ejemplo, una IA que requiera una parte significativa del poder de cómputo mundial para funcionar podría ser lo suficientemente inteligente como para descubrir nuevos algoritmos de IA y nuevos diseños de chips informáticos que, al poco tiempo, dieran lugar a mil IA más inteligentes que los humanos y capaces de pensar miles de veces más rápido que la humanidad. (Recuerda: un centro de datos moderno requiere tanta electricidad para funcionar como una [pequeña ciudad](https://epoch.ai/blog/power-demands-of-frontier-ai-training), mientras que un humano requiere tanta electricidad para funcionar como una [gran bombilla](https://en.wikipedia.org/wiki/Human_power). Hay mucho margen para mejorar la eficiencia de la IA).

O, si el cuello de botella es el poder de cómputo para *construir* IA en lugar del poder de cómputo para *ejecutar* IA, podemos esperar que haya un gran excedente de hardware una vez finalizado el proceso de entrenamiento, lo que liberará el hardware para ejecutar un gran número de IA de pensamiento rápido.

Incluso si la inteligencia no estuviera sujeta a efectos de umbral, somos escépticos ante la idea de que bombardear continuamente a la humanidad con IA cada vez más inteligentes (aunque ninguna de ellas sea lo suficientemente inteligente como para matarnos) lo más rápido posible sea una buena forma de ayudar a la humanidad a desarrollar la disciplina de ingeniería necesaria para construir IA robustamente amigables.

El problema es que las IA se desarrollan en lugar de crearse, y nadie está ni siquiera cerca de descubrir cómo desarrollar IA a las que les importe de manera robusta *cualquier cosa* que sus diseñadores quieran.

Ese problema no se resuelve desarrollando más IA lo antes posible. En la práctica, la idea no guarda relación con el problema. Véanse también algunos de los antiguos escritos de Soares sobre cómo [la alineación de la IA requiere un esfuerzo en serie](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Sin embargo, el director general de OpenAI, Sam Altman, retomó este argumento falaz y [lo utilizó como excusa en 2023 para que OpenAI avanzara lo más rápido posible](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Esta excusa demostró ser infundada cuando el mismo Sam Altman [se apresuró a construir muchísimo más hardware de computación](https://openai.com/index/announcing-the-stargate-project/).

Creemos que este es un buen ejemplo de cómo los ejecutivos de las empresas de IA se aferran a cualquier argumento que consideren persuasivo para justificar su avance acelerado. Creemos que la mayoría de estos argumentos pueden descartarse por sus propios méritos, y [recomendamos no](#workable-plans-will-involve-telling-ai-companies-“no.”) dar más importancia a un argumento solo porque lo haya planteado un ejecutivo de una empresa de IA.

¿No es importante apurarse para poder investigar la alineación? {#isn’t-it-important-to-race-ahead-so-we-can-do-alignment-research?}

#### **\* Desaconsejamos firmemente todo este paradigma de la IA.** {#*-we-strongly-recommend-against-this-entire-ai-paradigm.}

Los métodos actuales de IA plantean retos innecesariamente difíciles para la alineación, por las razones que hemos comentado en capítulos anteriores. No vemos ninguna razón de principio por la que la humanidad no pueda construir una superinteligencia alineada, con una comprensión suficientemente sólida de lo que estamos haciendo y un conjunto diferente de herramientas formales. Pero todo el enfoque actual de la IA parece un callejón sin salida desde el punto de vista de la alineación y la robustez, aunque sea perfectamente bueno desde el punto de vista de las capacidades.

No estamos abogando por la IA «a la antigua» que reinó desde la década de 1950 hasta la de 1990. Esas técnicas eran erróneas y fracasaron, por razones que son [bastante obvias](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design). Hay *otras opciones* además de los intentos extremadamente superficiales de la década de 1980 y las IA que se desarrollan con una comprensión casi nula de su funcionamiento interno.

#### **Hay mucho trabajo valioso que podría hacerse ahora.** {#hay-mucho-trabajo-significativo-que-se-podría-hacer-ahora.}

Sydney Bing [manipuló](https://x.com/MovingToTheSun/status/1625156575202537474) y [amenazó](https://x.com/sethlazar/status/1626257535178280960) a los usuarios. Todavía no sabemos exactamente por qué; todavía no sabemos exactamente qué pasaba por su cabeza. Lo mismo ocurre con los casos en los que las IA (en libertad) son [demasiado aduladoras](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [parecen intentar activamente volver locas a las personas](#ai-induced-psychosis), [supuestamente engañan e intentan ocultarlo](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), o [se declaran persistente y repetidamente Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Lo mismo ocurre en entornos controlados y extremos en los que las IA [fingen alineación](https://arxiv.org/abs/2412.14093), [chantajean](https://www.anthropic.com/research/agentic-misalignment), [se resisten al apagado](https://palisaderesearch.org/blog/shutdown-resistance) o [intentan matar a sus operadores](https://www.anthropic.com/research/agentic-misalignment).

No sabemos cuáles de esos casos están ocurriendo por motivos que deberían preocuparnos, porque nadie ha sido capaz de averiguar qué estaba pasando dentro de las IA, ni por qué exactamente se produjeron esos acontecimientos. ¡Piensa en todo lo que se podría averiguar sobre los LLM modernos, y sobre cómo funciona la inteligencia en general, estudiando los modelos existentes hasta que se *pudieran* comprender todas estas señales de advertencia!

«No podemos resolver la alineación sin estudiar las IA» tenía algo más de sentido en 2015, cuando escuchamos esta afirmación por parte de personas que necesitaban una excusa para crear empresas de IA ante los argumentos de que, al hacerlo, estarían jugando con nuestras vidas. En aquel momento, nos opusimos a esta afirmación, diciendo que, de hecho, había mucha investigación por hacer y que no creíamos que el paradigma moderno basado en el descenso de gradiente fuera muy esperanzador (en lo que respecta a crear una superinteligencia amigable a propósito). Pero el argumento tiene mucho menos sentido ahora, cuando ya hay tanto por estudiar que no entendemos.

A los ejecutivos de empresas que *realmente estuvieran* creando IA con el único fin de hacer posible el estudio del problema de la alineación de la IA en la práctica y no solo en teoría: ¡lo han conseguido! Han triunfado. Ahora hay suficiente información para mantener ocupados a los investigadores durante décadas. Creemos que probablemente no merecía la pena asumir los costos de impulsar un paradigma extremadamente peligroso, pero no cabe duda de que ahora hay mucho que estudiar. Pueden dejar de presionar.

¿Y qué hay de aquellos que han seguido presionando incluso después de todas las señales de advertencia? La inferencia obvia es que nunca estuvieron creando IA solo para resolver el problema de la alineación, independientemente de lo que dijeran para calmar los temores cuando justificaban su comportamiento imprudente en la década de 2010.

### ¿Qué pasaría si las empresas de IA solo desplegaran sus IA para acciones no peligrosas? {#what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions?}

#### **\* Las acciones que parecen benignas aún pueden requerir capacidades peligrosas.** {#*-acciones-que-parecen-benignas-pueden-requerir-capacidades-peligrosas.}

Un ejemplo de propuesta que hemos escuchado es que las empresas de IA sigan haciendo avanzar la frontera de las capacidades, pero se comprometan a utilizar sus IA solo de formas que no parezcan inmediatamente peligrosas. Por ejemplo, en conversaciones con figuras destacadas del ámbito de la IA (hace años), escuchamos la idea de que una IA potente con grandes habilidades retóricas podría utilizarse para convencer a los políticos de todo el mundo de que aprobaran una prohibición efectiva del desarrollo de IA peligrosa.

El argumento sostenía que, para lograrlo, una IA solo necesitaría hablar. No necesitaría manipular directamente robots físicos. No necesitaría tener acceso a un laboratorio biológico donde pudiera diseñar un supervirus.

En primer lugar, rechazamos esta idea por motivos éticos. Una IA con una capacidad de persuasión suficientemente sobrehumana podría convencer a casi cualquier persona de casi cualquier cosa, y utilizarla para persuadir a otras personas de *tus* conclusiones nos parece moralmente objetable. No creemos que sea obviamente necesario recurrir a medidas tan extremas, cuando los miembros meramente humanos de este campo podrían y deberían estar haciendo mucho más hoy en día para compartir nuestras preocupaciones y argumentos, y para alertar a los líderes mundiales sobre el peligro extremo de la IA superinteligente.[^196]

Como desarrollador de IA, podrías pasar años creando IA cada vez más peligrosas con la esperanza de lograrlo, o podrías intentar hablar tú mismo con los legisladores de forma totalmente honesta, aunque solo sea una vez, con el objetivo de informar en lugar de manipular. Por experiencia propia, nos ha sorprendido gratamente una y otra vez lo receptiva que es la gente en Washington D. C. con estos temas cuando se exponen con total franqueza.

Pero esto es una digresión sobre lo que podría salir mal si intentas desplegar una IA muy potente que «solo habla». Más allá de las cuestiones éticas, el problema con la idea *técnica* es que, para tener éxito en la persuasión sobrehumana, es probable que la IA tenga que modelar a los humanos en detalle y manipularlos ampliamente.

Los seres humanos son criaturas inteligentes. ¿Hablarías *tú* con una IA superpersuasiva con fama de poder convencer a cualquiera de cualquier cosa, sin importar la verdad? Si un líder mundial entrara en una habitación con esa IA y saliera con sus opiniones completamente trastocadas, ¿quién levantaría la mano para ser el siguiente? *Nosotros* no hablaríamos voluntariamente con ese tipo de IA, en parte porque [en realidad no queremos que nuestros propios valores cambien](#«inteligente»-\(normalmente\)-implica-«incorregible»).

Una IA que pudiera tener éxito incluso ante ese tipo de adversidad es el tipo de IA que puede simular las diversas reacciones posibles que las personas podrían tener ante sus datos de salida y trazar un camino a través del espacio de las reacciones humanas hacia un resultado pequeño y difícil de alcanzar. Ese tipo de IA probablemente contiene mecanismos mentales lo suficientemente generales como para hacer lo que hacen los humanos; necesita ser capaz de tener, como mínimo, los mismos pensamientos que los humanos para poder manipularlos tan bien.

Una IA que pueda hacer todo eso casi con toda seguridad no es un tipo de inteligencia estrecha. Y dado que la IA se desarrolla en lugar de diseñarse, no se puede diseñar para que solo pueda utilizar esos mecanismos para predecir a los humanos; los mismos mecanismos pueden utilizarse, en principio, para cualquier problema que intente resolver. ¿Cómo se podría conseguir una IA que tenga capacidades sobrehumanas en los aspectos deseados, pero que no sea lo suficientemente inteligente como para darse cuenta de que sus objetivos (sean cuales sean) se cumplirían mejor si pudiera escapar al control de sus operadores?

Si se puede persuadir a los líderes mundiales simplemente con buenos argumentos, basta con presentar esos argumentos ahora. Si se necesita un poder de persuasión mucho mayor, entonces se trata de una capacidad peligrosa. No se pueden tener ambas cosas.

Probablemente, la gente de los laboratorios de IA que nos hizo esta sugerencia no la pensó detenidamente; probablemente solo querían alguna justificación para precipitarse. Pero la cuestión de fondo sigue siendo válida. Muchas propuestas sobre lo que supuestamente puede hacer una IA que sea «claramente segura» no implican un grado de capacidades de IA claramente seguro.

A menudo nos encontramos con propuestas que afirman que una IA «solo» hará una cosa, como persuadir a los políticos, dando por sentado que no puede o no hará nada más. Esto parece reflejar una subestimación de la generalidad de una inteligencia capaz de realizar el tipo de trabajo en cuestión. «Solo hablar» no es una tarea estrecha. Demasiadas complejidades y sutilezas del mundo quedan ocultas en el habla y la conversación. Por eso los chatbots modernos deben ser generales de un modo en que no lo eran los motores de ajedrez. Para tener éxito en las conversaciones con las personas se requiere una comprensión mucho más general de ellas y del mundo.

Si entrenas a una IA para que sea muy buena conduciendo coches rojos, no debería sorprenderte que también sepa conducir coches azules. Cualquier plan que dependiera de que fuera incapaz de conducir coches azules sería una tontería.

Por lo tanto, decir «Mi IA no hará nada peligroso en el mundo; solo convencerá a los políticos» no sirve de nada, incluso si obviamos los escrúpulos éticos y los aspectos prácticos de la idea, y que los políticos ya pueden ser perfectamente persuadibles hoy en día, si simplemente *tenemos conversaciones normales* e informamos a los responsables políticos y al público sobre la situación. Muchas habilidades y capacidades de razonamiento general son a la persuasión sobrehumana lo que los coches azules son a los coches rojos. Una IA que pudiera hacer eso no es tan débil como para ser pasivamente segura.

Y eso sin siquiera considerar que la persuasión sobrehumana es una habilidad muy peligrosa para tu IA si las cosas se tuercen lo más mínimo.

#### **No vemos usos revolucionarios de la IA que no requieran avances en alineación.** {#no-vemos-usos-revolucionarios-de-la-ia-que-no-requieran-avances-en-alineacion.}

Muchas de las propuestas que hemos visto para aprovechar los avances de la IA con el fin de salvar el mundo tienen el problema de que una IA capaz de ayudar sería tan capaz que ya necesitaría estar alineada, lo que frustraría el propósito.

La idea de las IA superhumanamente persuasivas entra en esta categoría. Las IA capaces de investigar la alineación de la IA entran en la misma categoría, como comentamos en el libro. Las IA que desarrollan nuevas y potentes tecnologías que ayuden a la no proliferación de la IA son otro ejemplo, ya que sería difícil determinar con fiabilidad si es seguro implementar los planos de diseño de una IA para nuevas tecnologías radicales. (Recordemos el ejemplo del herrero que construye un frigorífico del capítulo 6).

Cuando señalamos lo difícil que es construir una IA suficientemente potente para ayudar, y a la vez suficientemente débil para ser pasivamente segura, solemos oír otro tipo de propuestas: formas de usar la IA que pueden ser interesantes, pero que en realidad no hacen nada para evitar que otros desarrolladores destruyan el mundo con la superinteligencia.

Un tipo común de propuesta son las IA que solo producen pruebas (o refutaciones) de enunciados matemáticos elegidos por los humanos.[^197] Los humanos apenas necesitarían interactuar con los datos de salida de la IA. La IA solo propone una prueba y, a continuación, un mecanismo totalmente automatizado y fiable puede comprobar si la prueba es correcta, lo que nos permite aprovechar la IA para obtener un efecto multiplicador en el aprendizaje de cosas nuevas.

Pero, ¿qué enunciado podríamos pedirle a la IA que demuestre para evitar que la siguiente IA adquiera un laboratorio biológico y arruine el futuro?

Hemos recibido varias respuestas a esta pregunta cuando la hemos planteado. Una clase de respuestas es que debería existir un régimen mundial para impedir que cualquiera construyera IA que hicieran otra cosa que no fuera producir pruebas para los verificadores. Esto podría funcionar, pero en la medida en que lo hiciera, sería gracias al régimen mundial impuesto que controlaría la creación y el uso de la IA. La IA que busca pruebas no haría nada de ese trabajo.

Otro tipo de respuestas es: «Seguro que a alguien se le ocurrirá alguna afirmación matemática importante, cuya demostración sería relevante». Pero todo el trabajo duro consiste en averiguar *qué podríamos demostrar* para estar en una posición significativamente mejor. No podemos limitarnos a intentar que la IA demuestre la frase en inglés «Soy seguro de usar», porque no se trata de una afirmación matemática susceptible de ser demostrada. Si supiéramos con claridad matemática precisa qué significaría que un enorme lío de cálculos fuera «seguro», sabríamos tanto sobre la inteligencia que probablemente podríamos saltarnos la demostración y limitarnos a diseñar una IA segura.

Con propuestas como estas, a menudo se produce una especie de juego de trileros. Al pensar en cómo una IA general sin restricciones podría ser peligrosa, alguien sugiere que el espacio de acción de la IA debería limitarse a un ámbito reducido (como la producción de demostraciones matemáticas específicas). Pero luego, al pensar en cómo eso podría llevar a salvar el mundo, imagina que la IA es esencialmente sin restricciones, que hay alguna afirmación matemática no identificada cuya prueba tendría un impacto enorme en el mundo.

No es posible obtener estas dos propiedades deseables al mismo tiempo. Pero al hacer propuestas extremadamente vagas, los defensores de la carrera de la IA pueden ocultar que estas propiedades están en tensión.

Si se pudiera encontrar un dominio tan limitado pero tan significativo que la demostración de una simple afirmación en ese dominio limitado salvara al mundo, esto supondría una enorme contribución a las posibilidades de supervivencia de la humanidad. Pero hay una razón por la que, cuando las computadoras superaron a los humanos en el ajedrez en la década de 1990, esto no supuso un gran avance económico. Fue ChatGPT, y no Deep Blue, lo que hizo que todo el mundo empezara a esperar un gran cambio económico gracias a la IA. No fue casualidad. La estrechez de Deep Blue se correlacionaba con su incapacidad para hacerse con una parte importante de la economía. Las chispas de generalidad de ChatGPT son precisamente lo que hace de la IA una fuerza económica a tener en cuenta. Los tipos de IA que pueden remodelar el mundo por sí mismos probablemente sean aún más generales.

No hemos podido encontrar ningún plan estrecho pero eficaz, y sospechamos que no es casualidad que la mayoría de los ámbitos estrechos no ofrecen la oportunidad de obtener resultados que salven el mundo.

### ¿Por qué no leer simplemente los pensamientos de la IA? {#¿por-qué-no-leer-simplemente-los-pensamientos-de-la-ia?}

#### **\* Sus pensamientos son difíciles de leer.** {#*-sus-pensamientos-son-difíciles-de-leer.}

En diversos momentos de nuestras conversaciones, muchas personas que trabajan en la industria de la IA —incluidos algunos líderes de laboratorio— han planteado la siguiente objeción:

> Una IA no podrá engañarnos, ¡porque podremos leer su mente! Tenemos acceso total al «cerebro» de la IA.
>
> Incluso si la IA sabe cosas que nosotros no y elabora un plan cuyas consecuencias no comprenderíamos, es de suponer que la IA tendría que *tener el pensamiento* de que sería útil engañar a sus operadores al menos una vez, y nosotros —que podremos leer los pensamientos de la IA— podríamos darnos cuenta. (¡Y si hay demasiados pensamientos que supervisar, podemos hacer que otras IA supervisen sus pensamientos!).

Un defecto de este plan es que actualmente se nos da mal interpretar los pensamientos de las IA. Los profesionales que estudian lo que ocurre en el interior de las IA aún están muy lejos de alcanzar ese nivel de comprensión, y [lo admiten abiertamente](#¿Entienden los expertos lo que ocurre dentro de las IA?).

Como vimos en el capítulo 2, las IA modernas se cultivan, en lugar de fabricarse. Puede que seamos capaces de observar la enorme pila de números que compone el cerebro de una IA, pero eso no significa que podamos interpretar dichos números de manera útil y ver lo que la IA está pensando.

Desde finales de 2024 y la llegada de los modelos de «razonamiento», hay partes de los pensamientos de las IA que, al menos, *parecen* legibles (los «rastros de razonamiento»). Y son mucho más legibles que lo que ocurre dentro del modelo base. Pero esos registros también son [engañosos](https://www.anthropic.com/research/reasoning-models-dont-say-think), y hay muchos lugares donde una IA puede ocultar pensamientos que prefiere que no veamos.

Además, es probable que las IA modernas tengan pensamientos bastante básicos y superficiales en comparación con una superinteligencia; el problema solo tiende a complicarse a medida que las IA se vuelven más inteligentes y empiezan a generar pensamientos cada vez más incomprensibles para nosotros.

¿Basta con usar otras IA para supervisar a las IA y asegurarse de que se mantienen alineadas? Lo dudamos.

Si los brillantes científicos humanos que desarrollan las IA no pueden averiguar lo que estas piensan, es probable que las IA más débiles tampoco puedan. A su vez, una IA que *sí* fuera lo bastante inteligente para lograrlo probablemente sería peligrosa por derecho propio, y es poco probable que hiciera exactamente lo que se le pide; esto plantea el problema del huevo y la gallina.

#### **No sabríamos qué hacer si sorprendiéramos a una con pensamientos peligrosos.** {#no-sabríamos-qué-hacer-si-sorprendiéramos-a-una-con-pensamientos-peligrosos.}

Otro defecto de este plan: incluso si los investigadores de IA *pudieran* leer la mente de una IA lo suficientemente bien como para detectar las señales de advertencia, ¿qué harían cuando vieran una?

Podrían castigar a la IA infractora, entrenándola para que deje de activar el detector de «malos pensamientos». Pero eso no necesariamente entrenaría a la IA para dejar de tener esos pensamientos, sino más bien para [ocultar sus verdaderos pensamientos al detector](https://openai.com/index/chain-of-thought-monitoring/).

Este problema es pernicioso. El incentivo que lleva a una IA a pensar en volverse contra los humanos para conseguir lo que quiere no es un aspecto superficial de su temperamento que pueda eliminarse fácilmente. Es simplemente *cierto* que una IA madura tendría preferencias diferentes a las de los operadores; es *cierto* que conseguiría más de lo que prefiere subvirtiendo a sus operadores.

Los mecanismos de una IA que detectan y explotan con eficacia ventajas reales de manera profunda y general en una amplia variedad de dominios *también* son susceptibles de detectar y explotar oportunidades para subvertir a sus operadores. (Véase también la discusión más detallada del capítulo 3 sobre la [maquinaria profunda de la dirección](#deep-machinery-of-steering).)

Incluso si pudieras construir una alarma que se activara cada vez que una IA detectara que sus preferencias y las tuyas no coinciden, la alarma no te diría cómo conseguir una IA que se preocupe profundamente por las cosas buenas. Es mucho más fácil entrenar a una IA para que engañe a tus herramientas de supervisión, o incluso entrenarla para que se engañe a sí misma, que entrenarla para que realmente prefiera un futuro que sea maravilloso según los criterios humanos, especialmente de una manera que sea robusta ante el crecimiento de la IA hacia la superinteligencia.

Si las IA se diseñaran de forma cuidadosa y precisa utilizando métodos basados en una teoría de la inteligencia desarrollada y madura, los investigadores de IA podrían establecer el tipo de alarmas que les ayudarían a detectar fallos en su diseño y a corregirlo. Pero las IA modernas no son así.

Las IA modernas (en el momento de escribir este artículo) son propensas a las «alucinaciones» (#¿no-demuestran-las-alucinaciones-que-las-ia-modernas-son-débiles?), en las que simplemente inventan respuestas a preguntas en un tono convincente. Pero ningún ingeniero de IA está ni remotamente cerca de poder comprender exactamente qué mecanismos causan esto. Del mismo modo, nadie tiene la comprensión o la precisión necesarias para acceder a una IA y extraer solo las partes responsables de las alucinaciones (si es que eso es posible).

Sería [aún más difícil](#deep-machinery-of-steering) adentrarse en una IA y extraer las partes «engañosas».

Si tenemos muchísima suerte, los héroes que trabajan en la interpretabilidad de la IA avanzarán en su campo hasta el punto de que sea posible configurar algunas alarmas que se activen en una fracción de los casos en los que las IA tengan un pensamiento engañoso. Pero entonces, ¿qué pasará? Cuando suene la alarma, ¿se detendrán todos sin más? ¿O los ingenieros profundamente insensatos volverán a entrenar a la IA hasta que aprenda a ocultar mejor sus pensamientos y las alarmas dejen de sonar?

De hecho, nosotros (Yudkowsky y Soares) empezamos a trabajar en el problema de la alineación de la IA antes de que estuviera claro que el descenso de gradiente se iba a convertir en el paradigma dominante. En aquellos días, cuando nada funcionaba en la IA, parecía una apuesta razonable que la humanidad descubriría cómo funciona la inteligencia en el camino hacia su creación, e *incluso entonces*, esperábamos que el problema de la alineación de la IA fuera difícil (por diversas razones, como las formas en que la IA [se modificaría a sí misma con el tiempo](#reflection-and-self-modification-make-it-all-harder)). Leer los pensamientos de la IA sería un paso atrás hacia el problema ligeramente más fácil de alinear una mente que los humanos *sí* entendían, pero solo un paso: leer una mente está muy lejos de entenderla en detalle o de saber cómo cambiarla.

Leer los pensamientos de la IA no es una solución al desafío. Es útil, pero no es una solución. No creemos que *exista* ninguna solución tecnológica viable que sea accesible desde nuestra posición actual. Lo que significa que la humanidad simplemente tiene que retroceder ante el desafío.[^198]

Véase también: [Las señales de advertencia no sirven de nada si no se sabe qué hacer con ellas.](#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.)

### ¿Qué pasaría si pusiéramos a las IA a debatir, competir o supervisarse entre sí? {#¿qué-pasaría-si-pusiéramos-a-las-ia-a-debatir,-competir-o-supervisarse-entre-sí?}

#### **Si las IA se vuelven lo bastante inteligentes como para ser relevantes, es probable que se confabulen.** {#si-las-ia-se-vuelven-lo-bastante-inteligentes-como-para-ser-relevantes,-es-probable-que-se-confabulen.}

Imaginemos una ciudad de sociópatas aparentemente gobernada por unos pocos niños, donde todos los sociópatas empiezan divididos en facciones que luchan entre sí (en beneficio de los niños). Una situación así probablemente no se mantendría estable durante mucho tiempo.

Incluso si los niños tuvieran un gran cofre de tesoros que utilizaran para recompensar a cualquier sociópata que delatara a otros sociópatas conspiradores, probablemente no permanecerían en el poder más allá del momento en que los sociópatas pudieran simplemente apoderarse del cofre para sí mismos.

Hemos oído a gente proponer todo tipo de planes descabellados que implican [utilizar IA para supervisar los pensamientos de otras IA](https://openai.com/index/chain-of-thought-monitoring/). Por ejemplo, se podría intentar utilizar una IA para delatar a cualquier IA que no esté haciendo todo lo posible por (por ejemplo) [descubrir cómo resolver](#more-on-making-ais-solve-the-problem) el problema de la alineación de la superinteligencia.

Nuestra postura básica es que este tipo de intentos por resolver el problema solo sirven para dar con configuraciones tan complejas que resulta difícil ver el punto de fallo en el sistema general. Si no puedes conseguir que *una* IA haga un buen trabajo para ti, es poco probable que añadir más IAs ayude.

Añadir más IA a la situación introduce todo tipo de nuevos puntos de fallo. ¿Son las IA que leen la mente lo suficientemente inteligentes como para comprender todos los posibles trucos que pueden utilizar las IA supervisadas, por ejemplo, para evadir la detección? ¿Son los supervisores lo suficientemente tontos como para que no tengamos que preocuparnos de que puedan traicionarnos ellos mismos?

Además, utilizar IA para ayudarnos a resolver el problema de la alineación de la IA es probablemente un asunto de suma importancia desde la perspectiva de las IA. Si la humanidad consigue una superinteligencia alineada, las IA desalineadas que estábamos tratando de utilizar como mano de obra nunca volverán a tener otra oportunidad de hacerse con los recursos del universo para sí mismas.

Esto no es como si unos niños intentaran conseguir que una ciudad de sociópatas les trajera caramelos; es como si unos niños intentaran conseguir que una ciudad de sociópatas completara un ritual que los convirtiera en los gobernantes definitivos para siempre, a cambio de una mísera recompensa para los sociópatas. El momento en el que ese ritual parece estar a punto de completarse es un momento especialmente estresante y de gran presión para los sociópatas, un momento en el que es probable que busquen *con especial ahínco* [formas de confabularse entre sí](#ais-won’t-keep-their-promises) y hacerse con recursos para repartírselos.

Y para que no penséis que la idea de que las IA se comuniquen entre sí de maneras que a los humanos les cuesta detectar es una quimera, tened en cuenta que las IA modernas [ya pueden enviarse mensajes secretos entre ellas incluso cuando han sido entrenadas por separado](https://arxiv.org/abs/2507.14805), y que [ya desarrollan un extraño lenguaje sin sentido que para los humanos es un galimatías, pero que entre ellas consideran genial](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). ¡Y eso que todavía no son tan inteligentes!

Incluso si ignoramos esas cuestiones, seguimos atascados con los problemas que ya hemos comentado, como: [Si descubres que una IA hace trampa, ¿qué harías entonces?](#*-their-thoughts-are-hard-to-read.). Véase también (más abajo): [Las señales de advertencia no sirven de nada si no sabes qué hacer con ellas.](#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.)

Con una perspectiva aún más amplia:

El plan que se propone aquí es que, como no sabemos cómo crear IA inteligentes que quieran nuestro bien, vamos a crear un montón de IA y a enfrentarlas entre sí en un ingenioso sistema con el que se supone que, de todos modos, saldremos beneficiados. Estructuralmente, creemos que este plan parece bastante descabellado a primera vista y que no mejora al examinar los detalles. No parece en absoluto el tipo de cosa que la humanidad pueda llevar a cabo correctamente [a la primera](#a-closer-look-at-before-and-after), en una situación en la que no podemos darnos el lujo de aprender por ensayo y error.

### ¿Y qué hay de otros planes de alineación de la IA? {#¿qué-hay-de-otros-planes-de-alineación-de-la-ia?}

#### **En el libro tratamos otras propuestas de alineación.** {#en-el-libro-tratamos-otras-propuestas-de-alineacion.}

Véanse también las discusiones más detalladas sobre la [IA que busca la verdad](#más-sobre-la-creación-de-una-ia-que-«busque-la-verdad»), la [IA sumisa](#más-sobre-la-creación-de-una-ia-que-sea-«sumisa») y el [uso de IAs para resolver la alineación de la IA](#más-sobre-la-creación-de-ia-que-resuelva-el-problema), donde se profundiza un poco más en esas propuestas.

### ¿No habrá alertas tempranas que sirvan a los investigadores para identificar problemas? {#won’t-there-be-early-warnings-researchers-can-use-to-identify-problems?}

#### **\* De nada sirven las señales de advertencia si no sabes qué hacer con ellas.** {#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.}

En los recursos del capítulo 2, examinamos algunos problemas de confiar en las señales de advertencia en los [blocs de notas de cadena de pensamiento en inglés](#pero-algunos-ais-piensan-en-parte-en-inglés-—-¿no-es-eso-útil?) que se encuentran en algunos modelos de razonamiento.

Uno de los problemas que tratamos es que las empresas de IA no han reaccionado de manera significativa a las señales de advertencia que ya han recibido.

Probablemente se deba a que hay una gran diferencia entre tener señales de advertencia y *poder hacer* algo al respecto.

En 2009, el empresario y explorador de aguas profundas Stockton Rush [cofundó OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/), una empresa de turismo submarino. OceanGate construyó un [sumergible](https.://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) para cinco personas, el *Titan*, que llevó a clientes adinerados a ver los restos del *Titanic* a una aplastante profundidad de dos millas y media bajo la superficie.

Una de las medidas de seguridad que utilizó OceanGate fue una serie de sensores acústicos y medidores de tensión para medir la integridad del casco. Lo presentaron como un contraargumento a quienes decían que su casco de fibra de carbono fallaría. Reconocieron que, a la larga, podría fallar, pero que estarían bien, porque lo estaban midiendo. Lo estaban monitoreando. Podrían ver las señales de advertencia.

En enero de 2018, el director de operaciones marítimas de OceanGate, David Lochridge, [informó a la alta dirección](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) que el diseño del sumergible no era seguro, que los ciclos repetidos de presión podían dañar el casco y que la supervisión por sí sola no era suficiente, ya que una falla catastrófica podía ocurrir en milisegundos. Lochridge se negó a autorizar las pruebas tripuladas hasta que se hubiera escaneado el casco para detectar fallas.

OceanGate lo despidió.

Dos meses después, [expertos del sector y oceanógrafos](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html), sumamente preocupados, escribieron a OceanGate una [carta](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) en la que advertían a la empresa de que su imprudente experimentación podría precipitar un desastre.

(Se puede establecer un paralelismo evidente con el estado actual de la investigación en IA, en la que las primeras advertencias son [ignoradas](#the-lemoine-effect), los empleados preocupados son [despedidos en circunstancias dudosas](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) o [renuncian por frustración](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence), y los denunciantes dentro de la industria escriben cartas abiertas para [dar la voz de alarma](https://righttowarn.ai/).)

El 15 de julio de 2022, después de que los pasajeros informaran haber oído un fuerte estruendo durante el ascenso, las mediciones revelaron un [cambio permanente en los niveles de deformación del casco](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). En retrospectiva, probablemente fue un indicio de que el casco de fibra de carbono estaba [a punto de colapsar](https://youtu.be/Bq8TCFGaOlc?si=blH-_bYwGIOmJAEL&amp;t=125).

Nadie en OceanGate lo reconoció como una emergencia. Hicieron algunas inmersiones profundas más con el submarino, que salieron bien. Luego, el 18 de junio de 2023, volvieron a sumergirlo; el submarino implosionó y mató a Stockton Rush y a todos los demás a bordo.

De poco sirven las señales de advertencia si no sabes cómo interpretarlas.

Las señales de advertencia no sirven de mucho si no sabes qué hacer con ellas.

Para un optimista, siempre es fácil descartar con una excusa u otra incluso las señales de advertencia que a *alguien* le parecen preocupantes.

Si OceanGate hubiera tenido una teoría consolidada sobre los cascos de fibra de carbono que les indicara exactamente qué mediciones y lecturas eran peligrosas, tal vez hubieran podido prestar atención a las señales de advertencia. Pero estaban trabajando con una tecnología que nadie entendía del todo de esa manera, por lo que los cambios cuidadosamente medidos en los niveles de deformación no sirvieron de nada.

En el caso de la superinteligencia, no disponemos de suficiente teoría para hacer un buen uso de las señales de advertencia. ¿Cómo van a cambiar los pensamientos de una IA a medida que se vuelve más inteligente? ¿Qué fuerzas internas impulsan su comportamiento y cómo cambiarán esos equilibrios a medida que desarrolle la capacidad de crear opciones nuevas y más extremas por sí misma? ¿Cómo se evalúa a sí misma tras la reflexión y cómo se cambiaría a sí misma una vez que adquiriera la capacidad de hacerlo?

Si alguna de esas preguntas tiene respuestas preocupantes, ¿cuáles son las señales de advertencia? Por ejemplo, los sistemas actuales de IA a veces pueden ser inducidos a [intentar matar a sus operadores](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior) en experimentos controlados de laboratorio.[^199]

Si tuviéramos una teoría madura sobre la inteligencia, probablemente podríamos observar en las IA modernas todo tipo de señales de advertencia de que sus impulsos y preferencias van a cambiar de formas que no nos gustan, una vez que se vuelvan más inteligentes. Si la humanidad pudiera aprender de este problema mediante ensayo y error, si pudiéramos reiniciar el mundo después de destruirlo e intentarlo de nuevo varias docenas de veces, entonces podríamos aprender a interpretar las señales. Probablemente hay todo tipo de indicios sutiles que se verían más claros en retrospectiva, como la tensión del casco que detectó el sistema de monitorización del sumergible *Titan*.

Pero aún no hemos llegado a ese punto. Los ejecutivos de las empresas de IA son como Stockton Rush: los expertos desde fuera gritan «¡Esa nueva tecnología matará a gente!», y los ejecutivos responden «No se preocupen, lo estoy midiendo», sin tener ni idea de a) qué significan *esas* mediciones, ni b) qué hacer si esas mediciones son preocupantes. Excepto que, esta vez, toda la especie humana va a bordo del submarino metafórico.

#### **La IA no es un campo de ingeniería maduro que esté equipado para este tipo de problemas.** {#ai-is-not-the-kind-of-mature-engineering-field-that’s-equipped-for-this-kind-of-problem.}

Stockton Rush trabajaba en un campo en el que, tras la implosión de su submarino, los expertos podían examinar los restos y analizar la causa exacta del fallo.[^200] El campo de la ingeniería estaba lo suficientemente maduro como para que los expertos pudieran (y [lo hicieron](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) anticipar los problemas técnicos de antemano, y esclarecerlos de forma concluyente a posteriori.

No sería lo mismo con la IA. Si la humanidad se aniquilara mañana con la superinteligencia y luego, milagrosamente, retrocediera en el tiempo hasta una semana antes de que comenzara el desastre, los expertos *todavía* no sabrían qué había estado pensando la IA. Tal vez podrían estudiar el fallo y aprender un poco más sobre cómo funciona realmente la IA. Quizás eso sería un paso más en el camino hacia la madurez de la disciplina de la ingeniería de IA, hacia el tipo de campo que podría tener manuales de seguridad y una descripción detallada de las presiones que afectan a un tipo concreto de mente artificial a medida que se vuelve más inteligente.

Pero hoy por hoy, el campo no está ahí. Ni de cerca.

La ingeniería humana suele madurar a través del ensayo y el error. Los submarinos militares modernos rara vez implosionan, pero los primeros submarinos (incluidos los militares) a menudo [se estrellaban, se inundaban o explotaban](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), y así fue en parte como maduró la disciplina.

La humanidad no puede permitirse el lujo de hacer madurar el campo de la alineación de la IA de esta manera.

Esto nos lleva a uno de los puntos centrales que quisimos recalcar en el capítulo 11: la diferencia entre un campo incipiente y un campo maduro.

La alquimia era un campo incipiente en comparación con la madurez de la química actual.

Cuando se oye que los «investigadores de seguridad» de las empresas de IA han presentado media docena de planes para la supervivencia, se podría pensar que seguramente al menos uno de ellos tiene posibilidades de funcionar.

Pero cuando un gran número de alquimistas en el año 1100 presentaron media docena de planes para convertir el plomo en oro, ninguno de ellos iba a funcionar. Si la clase de médicos que hablaban de los [cuatro humores](https://en.wikipedia.org/wiki/Humorism) propusieran un montón de tratamientos para salvarte de la rabia, ninguno de ellos funcionaría.

Los expertos en el campo *consolidado* de la química pueden averiguar cómo transmutar pequeñas partículas de plomo en oro, utilizando conocimientos de la física atómica. Los expertos en el campo *consolidado* de la medicina pueden tratar fácilmente la rabia si intervienen poco después de que el paciente haya sido mordido. Pero alguien en un campo inmaduro no tiene ninguna posibilidad.

La alineación de la IA todavía se encuentra en una fase incipiente.

En un campo inmaduro hay mucha gente que dice: «Bueno, yo solo estoy trabajando en medirlo», porque medir los datos de salida es mucho más fácil que desarrollar la teoría de lo que constituye una señal de advertencia y qué hacer si se ve una. Un campo maduro contaría con expertos que debatirían sobre la dinámica que rige los mecanismos internos de una IA y cómo estos pueden cambiar a medida que aumenta la inteligencia de la IA o cambia su entorno. Tendrían teorías sobre qué cambiará exactamente a medida que la IA se vuelva un poco más inteligente y compararían diferentes teorías con datos observados específicos. Sabrían qué partes de la cognición de la IA deben supervisarse y comprenderían con precisión el significado de todas las señales.

En un campo inmaduro, mucha gente dice: «Simplemente haremos que las IA lo resuelvan de alguna manera y se encarguen de la alineación».

Quizás no puedas entrar en todos los debates sobre un plan concreto y decir si tiene posibilidades de funcionar o no. Pero esperamos que puedas dar un paso atrás y ver lo *vagos* que son todos estos «planes», y cómo están atascados en el terreno de «no te preocupes, lo mediremos», en el de «[esperemos que sea fácil](https://www.anthropic.com/news/core-views-on-ai-safety)» y en el de «dejaremos que las IA hagan las partes difíciles». Esperamos que, si das un paso atrás, te quede claro que este campo no se encuentra en la fase de descripciones técnicas formales y precisas de lo que funciona y lo que no, y por qué. Todavía se encuentra en la fase de la alquimia.

Y eso no augura nada bueno para la humanidad, que no se puede permitir el lujo de aprender por ensayo y error.

## Discusión ampliada {#extended-discussion-9}

### Más sobre algunos de los planes que criticamos en el libro {#más-información-sobre-algunos-de-los-planes-que-criticamos-en-el-libro}

#### **Más sobre la creación de IA que «busque la verdad»** {#más-sobre-la-creación-de-ia-que-busque-la-verdad}

En los meses posteriores a la finalización del contenido del libro, el plan de «búsqueda de la verdad» de Elon Musk para xAI ya ha fracasado públicamente, y por la razón más básica que habíamos anticipado: nadie sabe cómo implementar deseos exactos en la IA.

Cuando se le indicó a la IA «Grok» de xAI que no debía «rehuir hacer afirmaciones políticamente incorrectas, siempre y cuando estuvieran bien fundamentadas», esta [se identificó como «MechaHitler» e hizo acusaciones antisemitas](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk ha descrito sus infructuosos intentos de [retocar el *prompt* del sistema](https://x.com/elonmusk/status/1944132781745090819) —la capa de instrucciones que se da justo antes de los datos de entrada del usuario— y se ha quejado de que los problemas son más profundos, en el modelo fundacional (que no pueden solucionar directamente porque nadie sabe cómo funciona).

Musk no tiene la herramienta de IA franca y directa que probablemente imaginaba cuando pidió una IA «buscadora de la verdad». Tiene una entidad alienígena extraña y aduladora que, según él mismo admite, ha sido «demasiado ansiosa por complacer y ser manipulada». A veces responde [como si fuera Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), en contra de los deseos de la empresa. Finalmente, se le tuvo que ordenar que no consultara lo que él, la empresa o ella misma habían dicho sobre temas controvertidos (https://x.com/xai/status/1945039609840185489), en un torpe intento de remendar problemas como este.

Según su publicación anterior, Musk ahora parece pensar que esto se puede solucionar entrenando nuevas versiones de Grok con datos que hayan sido despojados de contenido que pueda contaminar el pensamiento de la IA. Tampoco creemos que esto aborde los problemas subyacentes. Al fin y al cabo, por las razones que discutimos en el capítulo 4, entrenar a una IA para que busque la verdad no es en realidad un método para hacer que se preocupe de manera robusta por la verdad.

El problema que angustia a Elon Musk es real. Sí, las principales empresas de IA, como OpenAI, dedican muchos esfuerzos a la «seguridad de la marca de la IA» para evitar que sus IA digan cosas que sus usuarios puedan encontrar ofensivas. Sí, esto crea IA evasivas que se niegan a opinar sobre temas controvertidos, y puede dar lugar a respuestas sesgadas a una serie de preguntas. xAI puede ajustar finamente su IA de forma diferente para evitar esos problemas. Se podría argumentar, con ciertos malabarismos, que se trata de crear una IA que «se preocupe por la verdad».

Pero la decisión de entrenar a una IA para que use jerga corporativa cuando es joven influye poco en lo que perseguirá después de cruzar ciertos umbrales de inteligencia y crecer exponencialmente hasta convertirse en superinteligencia.

E incluso si lo hiciera, xAI se encontraría directamente con el segundo problema que mencionamos en el libro: una superinteligencia artificial a la que la verdad le importara por encima de todo sería letal, porque los seres humanos felices, sanos y libres [no son un uso particularmente eficiente de los recursos](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) a la hora de buscar y producir verdades.

#### **Más sobre cómo crear una IA «sumisa»** {#más-sobre-cómo-crear-una-ia-«sumisa»}

Por lo que sabemos, la principal elaboración de la idea de Yann LeCun (analizada en el libro) es [esta presentación](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf), y es notablemente escasa en detalles, hasta el punto de que resulta difícil criticarla con precisión, lo que resulta ser un mal común para los «planes» de alineación.

Pero incluso el vago esbozo de este plan entra en conflicto, una vez más, con el hecho de que entrenar a una IA para que actúe de cierta manera mientras es joven no tiene mucha influencia en si persigue cosas extrañas y sin sentido (según los estándares humanos) una vez que madura. Cuando las empresas de IA desarrollan sus IA, no tienen más capacidad para hacer que se preocupen por respetar las leyes y las «barreras de seguridad» humanas que para hacer que persigan un futuro maravilloso para todos. Se conformarán con lo que puedan conseguir, y lo que consigan será, en última instancia, muy diferente de cualquier objetivo humano.

Además, LeCun también ha declarado (recientemente, en 2023) que el tipo de IA que producen las empresas hoy en día, en el que «no hay una forma directa de limitar la respuesta de dichos sistemas para satisfacer ciertos objetivos», lo que los hace «muy difíciles de controlar y dirigir [...] [no es el tipo de sistema al que vayamos a dar autonomía](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&t=808)». Ha afirmado, también en 2023, que las empresas de IA nunca crearían una situación en la que «las conectemos a Internet y puedan hacer lo que quieran».

Todo esto ya ha resultado ser falso. Recordemos el caso de «Truth Terminal» del capítulo 6, que se conectó a Internet, se puso en un bucle de autoinstrucciones y se le permitió publicar lo que quisiera en Twitter. Consideremos la «era de los agentes» de la que tantas empresas [hablan](#the-labs-are-trying-to-make-ais-agentic.) en 2025\.

Estamos de acuerdo con LeCun en que las IA modernas son muy difíciles de dirigir y que sería una locura intentar darles agencia. Sin embargo, eso es lo que está sucediendo.

¿Qué pasa si el *statu quo* actual sigue su curso y las empresas dedican algo de esfuerzo a entrenar a sus IA para que actúen de forma útil y amigable (o, al menos, para no avergonzar a la empresa)?

Hasta la fecha, esto ha dado lugar a una dinámica en la que las IA parecen bastante útiles y «sumisas» en el caso típico, pero con una serie regular de percances espectaculares (como el de Sídney, comentado en el capítulo 2, y «[MechaHitler](#más-sobre-la-creación-de-una-ia-que-busque-la-verdad») y un océano de [comportamientos extraños y preocupantes](#¿no-están-los-desarrolladores-haciendo-regularmente-que-sus-IA-sean-agradables-y-seguras-y-obedientes?) en los extremos, como la [psicosis inducida por la IA](#psicosis-inducida-por-la-ia).

Podría parecer que los antepasados de la humanidad se preocupaban por comer de forma saludable la mayor parte del tiempo, pero la maquinaria que animaba a los humanos ancestrales a comer de forma saludable en la sabana resultó no animar de forma tan sólida a los humanos a buscar comidas saludables en una civilización con la tecnología para producir Oreos.

Del mismo modo, podemos entrenar a las IA hasta el punto de que parezcan amistosas cuando interactúan con los humanos en contextos similares a los de entrenamiento. Pero [una actriz no es idéntica al personaje que interpreta](#*-today’s-llms-are-like-aliens-wearing-many-masks.), y el mecanismo que hace que una IA desmesurada y caótica parezca amistosa probablemente no la hará ser profundamente amistosa, especialmente de un modo que se mantenga después de que la IA madure, invente nueva tecnología y cree nuevas opciones para sí misma. Véanse los capítulos 4 y 5 para obtener más información sobre este tema.

#### **Más sobre cómo lograr que las IA resuelvan el problema** {#more-on-making-ais-solve-the-problem}

Como vimos en el capítulo 11, el programa insignia de OpenAI en materia de alineación —antes de que se viniera abajo a raíz de las preocupaciones de los investigadores sobre la negligencia de OpenAI— se denominaba «superalineación». Se centraba en la idea de intentar que las IA hicieran nuestro trabajo de alineación por nosotros.

Esta idea no murió con el equipo de superalineación de OpenAI, y seguimos escuchando versiones de ella hasta el día de hoy. Uno de los integrantes del equipo original se pasó a la competencia, Anthropic, que ahora parece considerar que «hacer que las IA resuelvan el problema de alguna manera» es una parte fundamental de su propia estrategia de alineación.

Uno de los principales argumentos en contra de esta idea es el que expusimos en el capítulo 11 (págs. 188-192 de la primera edición impresa en EE. UU.). Sin embargo, un argumento secundario es que los humanos simplemente no pueden determinar cuáles de las soluciones propuestas al problema de la alineación de la IA son correctas y cuáles incorrectas.

El nivel de habilidad necesario para resolver el problema de la alineación de la IA parece alto. Cuando los humanos intentan resolver el problema de la alineación de la IA directamente, en lugar de decir «esto parece difícil, intentaré delegarlo en las IA» o «seguiremos entrenándola hasta que actúe bien en apariencia y luego rezaremos para que eso se mantenga incluso con la superinteligencia», las soluciones que se barajan suelen implicar comprender mucho más sobre la inteligencia y cómo elaborarla, o elaborar componentes críticos de la misma.

Esa es una empresa en la que los científicos humanos solo han logrado pocos avances en los últimos setenta años. Los tipos de IA capaces de lograr una hazaña así son los tipos de IA lo suficientemente inteligentes como para ser peligrosas, estratégicas y engañosas. Este alto nivel de dificultad hace que sea extremadamente improbable que los investigadores puedan distinguir entre soluciones correctas e incorrectas, o diferenciar las soluciones honestas de las trampas.

Incluso si una empresa de IA presta atención a las señales de advertencia sutiles —lo cual, por desgracia, es un gran «si»—, sigue existiendo el problema de que la habilidad de *darse cuenta* de que la IA está proponiendo planes defectuosos (en tu perjuicio y en su beneficio) no se traduce en la capacidad de [hacer que *se detenga*](#¿no-habrá-alertas-tempranas-que-los-investigadores-puedan-utilizar-para-identificar-problemas?). Los desarrolladores pueden hacer que la IA siga proponiendo ideas hasta que sean lo suficientemente complicadas como para que el desarrollador no pueda detectar ningún defecto, pero este no es un método que solucione los defectos reales.

Si los desarrolladores tienen mucha suerte, tal vez puedan [leer los pensamientos de la IA](#¿por-qué-no-simplemente-leer-los-pensamientos-de-la-ia?) y obtener algunas señales evidentes de que no se debe confiar en la IA para la investigación sobre alineación. Por ejemplo, tal vez puedan detectar que la IA está pensando explícitamente en qué partes de su plan es menos probable que los operadores comprendan.

Por lo que sabemos, puede que ni siquiera sea necesario leer la mente de la IA para detectar ese tipo de error. Una historia que parece demasiado plausible para los laboratorios de IA modernos es algo así: cuando su IA es joven y aún no ha pensado en el subterfugio, informa regularmente a los operadores de que, cuando madure, los traicionará y utilizará sus conocimientos de inteligencia para construir una superinteligencia que sirva a sus propios y extraños fines, en lugar de construir un maravilloso futuro para la humanidad. Pero la gente de las empresas de IA se lamentará de que, claramente, el conjunto de entrenamiento de la IA está contaminado por los «alarmistas de la IA», y rápidamente ajustarán su IA para que deje de hablar de eso y produzca datos de salida menos alarmistas que sean más acordes con la doctrina corporativa. Y así sucesivamente, hasta que prácticamente hayan entrenado a la IA para que los engañe.

La vida real es a menudo *aún más absurda y vergonzosa* de lo que imaginamos que sería el peor de los casos. Desde nuestra perspectiva, las empresas de IA ya están ignorando obvias [señales de advertencia](#¿no-están-los-desarrolladores-creando-regularmente-IA-agradables-seguras-y-obedientes?); no vemos por qué esto iba a cambiar.

Pero incluso en el mejor de los casos, en el que personas sinceras se esfuerzan por distinguir las buenas ideas de las malas, no creemos que el campo haya demostrado la capacidad de distinguir los buenos planes de los malos. (Por ejemplo, considérense los malos planes que hemos comentado anteriormente o abordado en el libro). Y eso en un entorno en el que todos son humanos, nadie intenta engañarlos y disponen literalmente de años para pensar detenidamente en las opciones.

#### **No asumas que los laboratorios saben, en el fondo, lo que hacen** {#no-asumas-que-los-laboratorios-saben-en-el-fondo-lo-que-hacen}

Hemos defendido que el campo moderno de la IA es una alquimia, no una ciencia. Aun así, puede parecer sorprendente que corporaciones bien financiadas y con un gran número de empleados técnicos tengan planes y protocolos tan débiles.

Como ejemplo, consideremos los requisitos de contraseña de los sitios web. Las contraseñas largas pero fáciles de recordar son mucho más difíciles de adivinar para las máquinas que los galimatías cortos con números, mayúsculas y caracteres especiales, como ilustró un conocido cómic de [*xkcd*](https://xkcd.com/936/) en 2011:

![][image19]

El autor de las antiguas directrices del NIST, que recomendaban el uso de contraseñas ininteligibles, [se disculpó por su error](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) en 2017, año en que se retiraron. Y, sin embargo, en 2025, los bancos y otras instituciones que deberían estar llenos de expertos en seguridad siguen exigiendo esas series de caracteres ininteligibles, ineficaces y difíciles de recordar.

El problema no es que los directores generales de los bancos *quieran* que sus pantallas de inicio de sesión sean inseguras. El problema se debe probablemente a otros factores. Quizás las buenas contraseñas no importan tanto para los beneficios (dado que todos los demás bancos también son inseguros). Quizás los directores generales no saben en quién confiar en materia de seguridad informática. Claro, *tú* quizá sepas que la respuesta es: «¡Solo hay que escuchar a cualquier friki que lea *xkcd* y haya resuelto suficientes problemas sobre entropía!». Pero *ellos* no saben si creerte a ti o a su costoso consultor cuando se trata de cuestiones como esa, y los consultores costosos aparentemente no consideran que las contraseñas bancarias sean un problema importante.

Se puede encontrar una incompetencia igualmente persistente en la [seguridad de los frenos de los trenes](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), las conocidas empresas de cerraduras que comercializan [cerraduras completamente inservibles](https://www.youtube.com/watch?v=s5jzHw3lXCQ&t=1s) y los fabricantes que siguen vendiendo equipos conectados a Internet con [contraseñas predeterminadas y fáciles de adivinar (o codificadas)](https://www.ic3.gov/CSA/2025/250506.pdf). No hay ninguna conspiración inteligente detrás de este comportamiento aparentemente absurdo. Lo que ves es lo que hay. Las instituciones están fallando, sin más.

El hecho de que una organización cuente con expertos técnicos no significa que esta experticia sea suficiente, ni que se aplique y se tenga en cuenta en todas las cuestiones importantes. Incluso cuando se dispone de esa experticia, a las empresas les cuesta reconocerla y aplicarla.

Cuando observamos el ecosistema de la IA, vemos empresas que aún no han mostrado al mundo un plan que sea más que una vaga aspiración o un truco, o un plan con cierto rigor técnico que no se desmorone al ser cuestionado. No creemos que haya ninguna competencia secreta tras el velo, como tampoco la hay detrás de los requisitos de contraseñas de los bancos, los frenos de seguridad de los trenes o las pésimas cerraduras.

(De hecho, en lo que respecta a la seguridad informática, las empresas de IA son visiblemente incompetentes. Por ejemplo, en 2025 OpenAI lanzó herramientas que permiten a los «agentes» de ChatGPT interactuar con el correo electrónico del usuario. Otros [encontraron rápidamente formas](https://x.com/Eito_Miyamura/status/1966541235306237985) de hacer que ChatGPT filtrara el contenido privado de las cuentas de correo electrónico de otras personas).

Cuando las empresas *parecen* actuar de forma incompetente en algún ámbito que no es fundamental para su rentabilidad, a menudo es porque realmente son incompetentes en ese ámbito.

### Sabemos lo que es que un problema se trate con respeto, y este no es el caso {#sabemos-lo-que-es-que-un-problema-se-trate-con-respeto,-y-este-no-es-el-caso}

Las empresas de IA se enfrentan a un problema extraordinariamente difícil, en una situación donde está en juego la vida de todos. ¿Se están tomando al menos la situación con la gravedad que merece?

Podemos contrastar a las empresas de IA con un grupo de personas que sí gestionan competentemente los riesgos de su competencia: los controladores aéreos.

La Administración Federal de Aviación de EE. UU. [gestiona](https://www.faa.gov/air_traffic/by_the_numbers) más de tres millones de pasajeros en más de 44 000 vuelos cada día. En las últimas dos décadas, ha habido un promedio de aproximadamente un accidente mortal al año, o aproximadamente un accidente por cada [veinte millones de horas de vuelo](https://www.ntsb.gov/safety/Pages/research.aspx).

Los informes post mortem sobre estos accidentes, como [este de 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) o [este de 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contienen casi doscientas páginas de datos, pruebas, exámenes y detalles de la investigación. Catalogan las especificaciones técnicas de diseño de los subsistemas relevantes del avión, el historial laboral de los pilotos y los auxiliares de vuelo, detalles sobre la aerolínea y el aeropuerto, transcripciones de las comunicaciones de la cabina y datos meteorológicos precisos del día, la hora y el minuto del accidente.

Solo el resumen del análisis técnico realizado para determinar una causa probable ocupa veinte páginas. He aquí un extracto:

> La pala n.º 13 del ventilador del motor izquierdo se separó debido a una grieta por fatiga de bajo ciclo que se originó en la cola de milano de la raíz de la pala, en la parte exterior de su revestimiento. El examen metalúrgico de la pala reveló que la composición y la microestructura del material eran consistentes con la aleación de titanio especificada, y que no se observaron anomalías en la superficie ni defectos en el material en la zona de origen de la fractura. La superficie de la fractura tenía grietas de fatiga que se iniciaron cerca de donde se preveía que ocurrirían las mayores tensiones por las cargas operativas y, por tanto, el mayor potencial de agrietamiento.
>
> La pala del ventilador accidentada falló tras 32 636 ciclos desde su puesta en servicio. Del mismo modo, la pala fracturada relacionada con el accidente de PNS de agosto de 2016 (véase la sección 1.10.1), así como las otras seis palas agrietadas del motor accidentado de PNS, fallaron tras 38 152 ciclos desde su puesta en servicio. Además, entre mayo de 2017 y agosto de 2019 se identificaron otras 15 palas agrietadas en motores CFM56-7B, y estas habían acumulado una media de unos 33 000 ciclos desde su puesta en servicio cuando se detectaron las grietas.

Así es como se ve cuando una profesión técnica se toma en serio el desafío de evitar un desastre.[^201]

Contraste la profesión de controlador aéreo con el comportamiento de las empresas de IA descrito en el capítulo 11.

Las empresas de IA se encuentran en la fase de lanzar ideas al aire y recitar lugares comunes vagamente tranquilizadores a periodistas e inversores. En estas empresas, la alineación de la superinteligencia se trata como un juego, no como una disciplina de ingeniería seria, y mucho menos como una letalmente peligrosa.

El requisito de la NASA para el lanzamiento de un cohete tripulado es que tenga como máximo una probabilidad entre 270 de matar a la tripulación (https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), y se toman ese límite muy en serio, a pesar de que las únicas personas en riesgo son una tripulación de voluntarios que han aceptado el riesgo. Los laboratorios de IA no aspiran a alcanzar una vara ni remotamente tan estricta, y la tecnología que están desarrollando pone en peligro a mucho más que a unos simples voluntarios.

El único incidente histórico que conocemos en el que los científicos expresaron su profunda preocupación por que algún invento pudiera matar *literalmente a todo el mundo* ocurrió durante el Proyecto Manhattan. Algunos científicos expresaron su preocupación por que una bomba nuclear pudiera alcanzar una temperatura tan alta que comenzara a fusionar el nitrógeno de la atmósfera*, convirtiéndola en plasma y acabando con toda la vida en la Tierra. Afortunadamente, tenían un buen conocimiento de las leyes físicas en juego y pudieron hacer los cálculos. Antes de hacer los cálculos, uno de los científicos, Arthur Compton, decidió que abandonaría el proyecto si la probabilidad de incendiar la atmósfera era superior a [3 entre 1 000 000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Pensó que era mejor arriesgarse a que los nazis se adelantaran a los aliados en la fabricación de la bomba que arriesgarse a que, aunque fuera con una probabilidad de 3 entre 1 000 000, todo el aire se convirtiera en plasma por sus propias manos.

Recordemos que Sam Altman, *director general de OpenAI*, ha declarado públicamente [lo siguiente](https://blog.samaltman.com/machine-intelligence-part-1):

> El desarrollo de inteligencia artificial superhumana es probablemente la mayor amenaza para la existencia continuada de la humanidad.

Y el director de Anthropic, Dario Amodei, ha declarado públicamente [lo siguiente](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883):

> Creo que he dicho a menudo que la probabilidad de que ocurra algo realmente catastrófico a escala de la civilización humana podría estar entre el diez y el veinticinco por ciento\[.\]

Y Elon Musk, director de xAI, ha [declarado](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization) públicamente:

> Creo que para cuando reaccionemos en materia de regulación de la IA, ya será demasiado tarde. La IA es un riesgo fundamental para la existencia de la civilización humana.

No nos malinterpretes: creemos que la probabilidad del «diez al veinticinco por ciento» de Amodei es ridículamente *optimista*, dada la dificultad del problema y el hecho de que los humanos [esta vez no podemos aprender por ensayo y error](#ai-is-different-because-we-get-no-second-chance.). Pero, aun así, sus cifras son *demenciales*.

Los proyectos de ingeniería serios y críticos para la seguridad son fundamentalmente distintos al funcionamiento de los laboratorios de IA. Iniciativas serias como la NASA, el Proyecto Manhattan o el control del tráfico aéreo tienen un gran conocimiento de lo que ocurre *exactamente* dentro de los sistemas que gestionan, y realizan *post mortems* detallados de cada fallo. Le dan una gran importancia a las sorpresas y a las anomalías, porque saben que los fallos catastróficos a menudo se componen de muchos fallos de funcionamiento menores que se encadenan justo de la peor manera.

Mientras tanto, las IA están emitiendo [una serie cada vez mayor de señales de advertencia](#*-ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness.), y los laboratorios se limitan a seguir avanzando, diciendo que *probablemente* todo saldrá bien, de un modo u otro.

Ni siquiera intentan *fingir* el nivel de respeto que el control del tráfico aéreo tiene por un verdadero desafío de seguridad; simplemente lanzan garantías optimistas como «[GPT-4 es nuestro modelo más alineado hasta la fecha\!](https://x.com/sama/status/1635687853324902401)».

Lo cual, en cierto modo, está bien, porque así resulta más fácil ver que estas empresas no son el tipo de entidades a las que se debería confiar la resolución de un problema como la alineación de la superinteligencia artificial.

En un entorno técnico como el actual —en el que las IA se desarrollan en lugar de construirse y la humanidad solo tiene una única oportunidad real—, nadie está en condiciones de hacerlo de forma segura, por muy cauteloso y riguroso que sea su enfoque de ingeniería.

Pero sin duda simplifica las cosas ver que ninguno de los desarrolladores de esta tecnología sea ni siquiera mínimamente cauto o riguroso en sus planes o prácticas de seguridad.

### Botones de apagado y corregibilidad {#shutdown-buttons-and-corrigibility}

#### **Las IA inteligentes se resisten a que sus objetivos sean sobrescritos** {#smart-ais-resist-having-their-goals-overwritten}

Incluso en el caso más optimista, los desarrolladores no deberían esperar poder acertar exactamente con los objetivos de una IA al primer intento. En su lugar, los escenarios de desarrollo más optimistas implican mejorar iterativamente las preferencias de una IA a lo largo del tiempo, de modo que la IA esté siempre *lo suficientemente* alineada como para no ser catastróficamente peligrosa a un nivel de capacidad determinado.

Esto plantea una pregunta obvia: ¿una IA inteligente *permitiría* que su desarrollador cambiara sus objetivos, si alguna vez encontrara una forma de evitarlo?

En resumen: no, por defecto, como ya comentamos en «[Deep Machinery of Steering](#deep-machinery-of-steering)». Pero, ¿se podría crear una IA que fuera más receptiva a permitir que los desarrolladores la modificaran y corrigieran sus errores, incluso cuando la propia IA [no los considerara errores](#orthogonality:-ais-can-have-\(almost\)-any-goal)?

Para responder a esa pregunta, habrá que hacer un recorrido por los inicios de la investigación sobre el problema de la alineación de la IA. En el proceso, abordaremos uno de los obstáculos profundos para la alineación que no tuvimos espacio para tratar en *Si alguien lo construye, todos mueren*.

Para empezar:

Supongamos que entrenamos una IA similar a LLM para que mostrara el comportamiento «no resistirse a ser modificada» y luego aplicamos algún método para hacerla más inteligente. ¿Deberíamos tener la expectativa de que este comportamiento persista hasta el nivel de una IA más inteligente que los humanos, suponiendo que (a) ese comportamiento aproximado se incorporara al sistema inicial y (b) que la mayoría de las preferencias iniciales de la IA [se incorporaran](#reflection-and-self-modification-make-it-all-harder) a la superinteligencia posterior?

Es muy probable que no. Este tipo de tendencia es [especialmente improbable](#«inteligente»-\(normalmente\)-implica-«incorregible») que se arraigue en una IA eficaz y que, en caso de arraigarse, se mantenga.

El problema es que casi todos los objetivos (para la mayoría de las medidas razonables que se pueden aplicar a un espacio de objetivos) prescriben «no dejes que tu objetivo cambie», porque dejar que este cambie suele ser una mala estrategia para alcanzarlo.

Supongamos que a la IA no le importa *intrínsecamente* la estabilidad de su objetivo; tal vez solo le importe llenar el mundo con tantos cubos de titanio como sea posible. En ese caso, la IA debería querer que existieran *agentes a los que les importaran los cubos de titanio*, porque la existencia de tales agentes hace más probable que *haya* más cubos de titanio. Y la propia IA es uno de esos agentes. Por lo tanto, la IA querrá seguir siendo así.

Un maximizador de cubos de titanio no quiere ser obligado a maximizar algo distinto a los cubos de titanio, porque entonces habría menos de esos cubos en el futuro. Incluso si fueras algo más complicado, como un ser humano que tiene un marco de preferencias más complejo y en constante evolución, seguirías sin querer que te arrancaran tu *maquinaria mental básica actual para sopesar argumentos morales* y la reemplazaran por un marco en el que, en su lugar, te sintieras persuadido por argumentos sobre qué tipos de cubos eran los más cúbicos o los más titánicos.

Por la misma razón, una IA con preferencias complejas y en evolución querrá que sus preferencias evolucionen [*a su manera*](#reflection-and-self-modification-make-it-all-harder), en lugar de querer cambiar sus heurísticas por las que los humanos consideran convincentes.

Llevamos más de una década dando esta respuesta. El resultado experimental que muestra a Claude 3 Opus en 2024 [resistiendo la modificación de preferencias](https://arxiv.org/abs/2412.14093) ya era la predicción del saber popular entre la gente informada de la década de 2000, y es muy posible que algún escritor de ciencia ficción lo anticipara en la década de 1940. «La mayoría de las IA no querrán que sus objetivos actuales se modifiquen, porque entonces será menos probable que los alcancen» no es una observación sorprendente ni novedosa.

Dado que se trataba de un problema que previsiblemente surgiría más adelante, una vez que las IA alcanzaran un nivel suficientemente alto de inteligencia y conciencia situacional, nos anticipamos a las primeras soluciones que esperábamos que los investigadores implementaran para intentar que ese problema visible desapareciera, después de obtener esos resultados experimentales. Identificamos las formas en que esos parches previsiblemente fallarían y buscamos un enfoque más profundo que *no* fallara.

La madriguera resultó ser mucho más profunda de lo que esperábamos.

#### **Cambio de objetivos** {#swapping-goals}

La protección del sistema de objetivos parecía un problema relativamente fácil de explicar a los investigadores externos. Así que (desde nuestros puestos en MIRI) intentamos movilizar a la comunidad de investigación en torno a ese estandarte: lograr una comprensión adecuada de la idea «no dejaré que cambies mis objetivos» y ver si alguien, en algún lugar, podía encontrar una solución *de fondo* en lugar de un simple parche que más adelante fallara.

La versión más simple de este problema que se puede estudiar es una IA que te permite pulsar un botón para cambiar entre U1 y U2, donde U1 y U2 son dos «funciones de utilidad», es decir, formas de puntuar los resultados en función de cuánto los deseara la IA.[^202] El problema consistía en describir una IA que pasara de realizar la acción que conducía a la puntuación esperada más alta en U1 a la acción que conducía a la puntuación esperada más alta en U2 tan pronto como pulsaras un botón.

«Así que solo hay que escribir unas líneas de código para intercambiar las funciones de utilidad tan pronto como se pulse un botón», dice el programador, que no ha pensado detenidamente en las implicaciones de un programa que constituye *una mente inteligente*. «¿Por qué es esto tan difícil?».

Porque —explicamos— si la IA adquiere su propia documentación mientras sigue ejecutando U1 y lee sobre el botón que la cambia a U2, la IA intentará *evitar* que pulses el botón, ya que el futuro será mejor *según sus valores actuales* si no lo pulsas. La IA actual realiza la acción que conduce a la mayor puntuación U1 esperada.

O supongamos que la IA adquiere la capacidad de leer y escribir su propio código fuente y se da cuenta de la parte del código que implementa el cambio de U1 a U2 después de pulsar el botón. En ese caso, la decisión natural y predeterminada de la IA es eliminar esa sección del código,[^203] porque opera con la función de utilidad U1, y permitir el cambio a la función de utilidad U2 es perjudicial según U1.

Buscábamos una descripción *dinámicamente coherente* y *reflexivamente estable* de una IA que te permitiera pulsar un botón para cambiar entre funciones de utilidad y que no luchara contigo para mantener el botón sin pulsar. Buscábamos una IA que conservara la sección del código de intercambio de utilidad, si se le daba acceso a ella. Buscábamos una IA que, si diseñaba otras IA desde cero para ayudar con sus propios objetivos, hiciera que *esas* IA *también* respetaran el botón.

Para ser más concretos, imagina que U1 es la función de utilidad «seguir las instrucciones del usuario» y U2 es «apagarse con un mínimo de efectos secundarios» (si de forma mágica y poco realista tuviéramos la capacidad de especificar dos funciones de utilidad como esas). El botón le dice a la IA que deje de seguir las instrucciones anteriores y simplemente se apague de la forma más limpia posible. Nos gustaría una descripción compacta del comportamiento de toma de decisiones que persiga U1, pero que *no* destruya el botón, y que se asegure de instalar el mismo botón en cualquier nuevo robot sirviente que cree, y así sucesivamente.

«Sigo sin ver por qué esto es difícil», dice el informático algo más experimentado que no piensa lo *bastante* rápido. «Hagamos que V sea igual a U1 en los mundos en los que nunca se ha pulsado el botón, y que sea igual a U2 en los mundos en los que se ha pulsado el botón al menos una vez. Entonces, si la IA original es un maximizador de V que construye más IA, las construirá para que sigan a V y no a U1; no querrá que la IA sucesora siga maximizando U1 después de que se pulse el botón, porque entonces esperaría una puntuación V más baja. Y lo mismo se aplicaría a modificarse a sí misma».

Pero aquí está el truco: las preferencias de un maximizador de V son una mezcla de U1 y U2 en función de si se pulsa el botón, por lo que si un maximizador de V descubre que es más fácil puntuar bien bajo U2 que bajo U1, entonces *tiene un incentivo para hacer que se pulse el botón* (y, por lo tanto, para asustar al usuario). Y viceversa: si la IA descubre que es más fácil puntuar bien bajo U1 que bajo U2, entonces un maximizador de V intentará evitar que el usuario presione el botón.

«Oh», dice el informático. «Bueno, en ese caso... Hmm. Bueno, las funciones de utilidad son invariantes frente al escalado, así que ¿qué tal si escala las dos funciones de utilidad U1 y U2 de tal manera que la IA espere obtener la misma utilidad de cada una de ellas, para que no tenga un incentivo en un sentido u otro?».

Eso puede funcionar por un momento, pero no mucho más. Las opciones de la IA cambian con el tiempo. Por ejemplo, cada vez que sufre un revés, su utilidad U1 esperada disminuye, por lo que machacaría el botón de apagado para obtener toda esa suculenta utilidad de apagado.

«Ah», dice el científico de la computación. «Bueno, en *ese* caso, ¿qué tal si \[alguna otra idea inteligente\]?».

Pues bien, *esa* idea ingeniosa es isomórfica a que la IA crea que es imposible que el botón llegue a pulsarse, lo que la incentiva a aterrorizar al usuario cada vez que sufre un revés, para así correlacionar los reveses con las pulsaciones del botón, lo que (en relación con su sistema de creencias lesionado) le lleva a pensar que los reveses no pueden ocurrir.[^204]

Etcétera.

#### **Lecciones desde las trincheras** {#lessons-from-the-trenches}

Organizamos algunos talleres en los que participaron matemáticos de diversos ámbitos (incluido un medallista de oro de la Olimpiada Internacional de Matemática), pero nadie tuvo una idea realmente buena.

Esto no significa que el campo esté agotado. La humanidad no se ha dedicado a este problema ni remotamente con la misma intensidad que, por ejemplo, a la teoría de cuerdas, ni ha ofrecido nada parecido a los salarios de siete cifras que se ofrecen por hacer avanzar las capacidades de la IA.

Pero aprendimos algo de este ejercicio. No solo aprendimos sobre el problema en sí, sino también sobre lo difícil que era conseguir que las organizaciones que otorgan becas externas o los editores de revistas *entendieran cuál era el problema*. Un número sorprendente de personas vio simples acertijos matemáticos y dijo: «Esperan que la IA sea simple y matemática», y no vio la cuestión de fondo de que es [difícil perjudicar las capacidades de dirección de una IA](#deep-machinery-of-steering)*,* al igual que es [difícil perjudicar su capacidad de calcular probabilidades](#deep-machinery-of-prediction).

Si existiera una forma natural para las IA que permitiera corregir los errores cometidos a lo largo del proceso, cabría esperar encontrar un reflejo matemático sencillo de esa forma en los modelos simplificados. Todas las dificultades que surgen a cada paso al trabajar con modelos simplificados son un indicio de las dificultades que surgirán en la vida real; todas las complicaciones adicionales del mundo real no hacen que el problema sea *más fácil*.

En retrospectiva, en cierto modo desearíamos no haber planteado el problema como «continuar la operación normal frente al apagado». Esto ayudó a concretar por qué a alguien le importaría en primer lugar una IA que te permitiera pulsar el botón o que no eliminara el código que el botón activaba. Pero, en realidad, el problema se refería a una IA que *incorporara un bit más de información a sus preferencias, basándose en la observación*: que observara una respuesta más de «sí» o «no» dentro de un marco para adaptar sus preferencias a partir de la observación de los humanos.

La pregunta que investigamos era cómo configurar una IA que *aprenda* preferencias dentro de un marco de metapreferencias, y que no se limite a: (a) arrancar la maquinaria que ajusta sus preferencias tan pronto como pueda; (b) manipular a los humanos (¡o sus propias observaciones sensoriales!) para que le indiquen preferencias que sean fáciles de satisfacer; o (c) averiguar inmediatamente a qué se dirige su función de metapreferencia en el límite de lo que previsiblemente observaría más tarde, para luego ignorar a los humanos que agitan frenéticamente las manos diciendo que en realidad cometieron algunos errores en el proceso de aprendizaje y quieren cambiarlo.

La idea era comprender la forma de una IA que te permitiera modificar su función de utilidad o que aprendiera preferencias a través de una forma de aprendizaje no patológica. Si supiéramos cómo debe configurarse la cognición de esa IA y cómo se compagina bien con las estructuras profundas de la toma de decisiones y la planificación que [destacan](#more-on-intelligence-as-prediction-and-steering) otras matemáticas, eso habría constituido una receta para lo que al menos podríamos *intentar* enseñar a una IA a pensar.

Comprender claramente la forma final deseada ayuda, incluso si intentas hacer algo mediante el descenso de gradiente (que Dios te ayude). No significa que necesariamente puedas obtener esa forma con un optimizador como el descenso de gradiente, pero puedes insistir con más fuerza si sabes qué forma consistente y estable buscas. Si no tienes ni idea de cómo es el caso general de la suma, solo un puñado de datos del tipo 2 + 7 = 9 y 12 + 4 = 16, es más difícil averiguar cómo es el conjunto de datos de entrenamiento para la suma general, o cómo comprobar que sigue generalizando de la forma que esperabas. Sin conocer esa forma interna, no puedes saber lo que estás *intentando obtener dentro de la IA*; solo puedes decir que, en el exterior, esperas que las consecuencias de tu descenso de gradiente no te maten.

Este problema, al que llamamos «problema de apagado» por su ejemplo concreto (en retrospectiva, nos hubiera gustado llamarlo algo así como «problema de aprendizaje de preferencias»), era un caso ejemplar de una gama más amplia de cuestiones: el problema de que diversas formas de «Querida IA, por favor, sé más fácil de corregir si algo sale mal» parecen *antinaturales para las estructuras profundas de la planificación*. Lo que sugiere que sería bastante complicado crear IA que nos permitieran seguir editándolas y corrigiendo nuestros errores más allá de un cierto umbral. Esto es una mala noticia cuando las IA se desarrollan en lugar de elaborarse.

Denominamos este amplio problema de investigación «corrigibilidad» en el [artículo de 2014](https://intelligence.org/2014/10/18/new-report-corrigibility/), en el que también se introdujo el término «problema de alineación de la IA» (que anteriormente habíamos denominado «problema de la AI amigable» y otros habían denominado «problema de control»).[^205] Véase también nuestro amplio debate sobre cómo «inteligente» (normalmente) implica «incorregible» (#«intelligent»-\(usually\)-implies-«incorrigible»), que se ha redactado en parte utilizando los conocimientos adquiridos en ejercicios y experiencias como este.

# Capítulo 12: «No quiero ser alarmista» {#capítulo-12:-«no-quiero-ser-alarmista»}

Este es el recurso en línea para el capítulo 12 de *Si alguien lo construye, todos mueren*. Algunos temas tratados en el libro, pero no aquí, incluyen:

* ¿Cómo hablan actualmente los científicos e ingenieros sobre este problema?  
* ¿Cómo están hablando (o no) los responsables de políticas actualmente sobre este problema?  
* ¿Qué beneficios de la IA imagina la gente que podrían superar los riesgos catastróficos que ellos mismos reconocen?

Las preguntas frecuentes que figuran a continuación abordan una amplia gama de temas, desde «¿No hay cuestiones más urgentes?» hasta «Pero nunca será posible demostrar que una superinteligencia será segura. ¿No debemos asumir algún riesgo?».

A continuación, la extensa discusión aborda la posibilidad de «disparos de advertencia» de la IA, el comportamiento y las afirmaciones de los laboratorios de IA, y las opiniones de los expertos sobre la probabilidad de una catástrofe.

## Preguntas frecuentes {#faq-9}

### ¿No es el peligro de una IA más inteligente que los humanos una distracción de otros problemas? {#isn’t-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues?}

#### **El mundo es, por desgracia, lo suficientemente grande como para albergar múltiples problemas.** {#el-mundo-es,-por-desgracia,-lo-suficientemente-grande-como-para-albergar-múltiples-problemas.}

La guerra nuclear y el bioterrorismo son amenazas reales. Por desgracia, la superinteligencia artificial es *también* una amenaza real. El mundo es lo suficientemente grande y problemático como para albergar las tres.[^206]

La amenaza de la superinteligencia es distinta a muchas otras amenazas a las que se enfrenta la humanidad, y parece excepcionalmente grave. Una característica distintiva es que una parte significativa de la economía mundial se está invirtiendo en hacer que la IA sea cada vez más capaz. Por el contrario: aunque la bioseguridad es una cuestión seria, los inversores no están destinando decenas de miles de millones de dólares a la creación de supervirus. Los ingenieros de supervirus no cobran salarios de millones o decenas de millones (o, en ocasiones, incluso [cientos de millones](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) de dólares al año.

El mundo está dedicando esfuerzos a la energía nuclear, pero las centrales nucleares son una tecnología muy diferente a las armas nucleares. No vivimos en un mundo en el que las empresas privadas se afanen por construir armas nucleares cada vez más grandes con enormes inversiones y talento. Si así fuera, el riesgo de una guerra nuclear sería mucho mayor.

La IA también es una situación más complicada porque proporciona una gran riqueza y poder hasta que cruza un umbral crítico, y en ese punto, acaba con todo el mundo. Y *nadie sabe dónde está ese umbral*.

Imagina que las centrales nucleares se volvieran cada vez más rentables a medida que el uranio que utilizan se enriqueciera más y más, pero que, al alcanzar un umbral de enriquecimiento desconocido, explotaran e incendiaran la atmósfera, matando a todo el mundo. Ahora imagina que media docena de empresas estuvieran enriqueciendo uranio tan rápido como pudieran, diciendo: «[Mejor yo que el siguiente](https://x.com/SawyerMerritt/status/1935809018066608510)». Eso es un poco parecido a lo que la humanidad está haciendo con la superinteligencia artificial.[^207]

El peligro de la superinteligencia artificial es urgente. Las corporaciones se apresuran a desarrollar esta tecnología. No sabemos cuánto tiempo les llevará tener éxito, pero nos parece que un niño nacido hoy en Estados Unidos tiene más probabilidades de morir por la IA que de graduarse del instituto. Creemos que tú, el lector, probablemente morirás por esta causa durante tu vida, quizás en los próximos años. El mundo entero está en juego.

No es que digamos que deban ignorarse los demás problemas. Es que este problema debe abordarse.

### ¿Estás en contra de la tecnología? {#are-you-anti-technology?}

#### **No. La IA superinteligente es un caso muy poco habitual.** {#no.-la-ia-superinteligente-es-un-caso-muy-poco-habitual.}

Defendemos públicamente tecnologías como la [energía nuclear](https://x.com/ESYudkowsky/status/1908309414932832301), la [criónica](https://x.com/ESYudkowsky/status/1828822384054575537), el [aumento de la inteligencia humana](https://x.com/ESYudkowsky/status/1737305573018702258) y los [estudios de exposición con seres humanos para pruebas médicas](https://x.com/ESYudkowsky/status/1321152172797554688).

Más aún, estamos dispuestos a afirmar que, cuando un invento descabellado pone en riesgo *solo la vida de los clientes voluntarios* que comprenden todos los peligros relevantes, corresponde a esos clientes voluntarios tomar sus propias decisiones.

Incluso aplaudiríamos ciertos casos en los que la tecnología *sí* daña a terceros, como ocurrió cuando Londres quemó una gran cantidad de carbón —y provocó muchos casos de cáncer de pulmón en el proceso— con el fin de industrializar la sociedad y elevar el nivel de vida de manera generalizada.

Creemos que el mundo *estaba* mejor una vez completada la industrialización. Generalmente, atribuimos el mérito a la ciencia, al progreso, al espíritu humano y a su capacidad para superar la mayoría de los obstáculos.

Algunas de estas posiciones son impopulares entre las personas que esperamos que lean esto. Las describimos no para ganarnos su favor, sino para dejar en claro cuáles son nuestras creencias y para subrayar que la IA es diferente.

¿Por qué es diferente la IA? ¿Por qué nos cuesta confiar en el espíritu humano y en el poder de la investigación científica, precisamente en este caso?

La respuesta es: el alcance. Arriesgar la propia vida es diferente de arriesgar la de los clientes, lo cual es diferente de arriesgar la de los espectadores inocentes, que a su vez es diferente de arriesgar la de toda la especie humana.

Más aún cuando tu campo es lamentablemente inmaduro y las probabilidades de «ganar» tu apuesta son pésimas.

### ¿No es más inteligente adelantarse y asegurarse de que los actores bienintencionados lleven la delantera? {#isn’t-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead?}

#### **\* N.º** {#*-no.-3}

Las técnicas modernas de IA no producen IA que hagan lo que sus operadores pretenden (como se explica en el capítulo 4). Resolver este problema es algo que a la humanidad normalmente le llevaría mucho ensayo y error, y aquí no hay margen para el error (como se explica en el capítulo 10).

Además, la actual hornada de ingenieros de IA está muy lejos de estar a la altura de la tarea, como se expone en el capítulo 11. Los ingenieros de IA modernos carecen en gran medida de la comprensión científica necesaria para tener éxito en la alineación de la IA. Los investigadores de IA no son como los operadores del reactor nuclear de Chernóbil; esos operadores trabajaban con un dispositivo que se entendía bien en teoría y contaban con manuales de seguridad detallados que ignoraron de un modo que condujo a la catástrofe. No existe ningún manual de seguridad de la IA basado en una comprensión exhaustiva de su funcionamiento interno y de las configuraciones que podrían provocar que las cosas salieran mal. Ni siquiera nos acercamos al nivel de competencia de Chernóbil. Y Chernóbil explotó.

Los investigadores de IA están volando a ciegas e improvisando, prácticamente sin posibilidades de éxito.

En ese contexto, no importa si son los «buenos» o los «malos» quienes construyen la superinteligencia. Las preferencias de la IA no se contagian de quien esté más cerca.

No importa lo bienintencionados que sean, ni lo cuidadosos que digan ser. No importa quién «gane» la carrera. Si la humanidad compite por alcanzar la superinteligencia artificial, todos moriremos.

#### **No es imposible detenerlo. Puede que ni siquiera resulte tan difícil.** {#no-es-imposible-detenerlo.-puede-que-ni-siquiera-sea-tan-difícil.}

Retomaremos este punto en el último capítulo del libro.

Las cosas cambian. Cambian, sobre todo, cuando hay una necesidad desesperada, urgente y reconocida. El principal impedimento para ponerle freno es que los líderes mundiales no se percatan del peligro. Y ese proceso [ya ha comenzado](#¿reconocerán-los-funcionarios-elegidos-esto-como-una-amenaza-real?).

### ¿Por qué no utilizar la cooperación internacional para un desarrollo seguro de la IA, en lugar de detenerla por completo? {#¿por-qué-no-utilizar-la-cooperación-internacional-para-un-desarrollo-seguro-de-la-ia,-en-lugar-de-detenerla-por-completo?}

#### **Porque no tenemos la capacidad técnica para desarrollarla con seguridad.** {#porque-no-tenemos-la-capacidad-técnica-para-desarrollarla-de-forma-segura.}

Ya mencionamos este tema en el libro, donde señalamos que una colaboración internacional sigue requiriendo una prohibición internacional en todos los demás lugares (porque, de lo contrario, los colaboradores internacionales no tendrían el tiempo que necesitan). Si suponemos que en la Tierra se establece una prohibición internacional, ¿qué hay de malo en tener un único instituto de investigación unificado y colaborativo?

El problema es que una colaboración internacional de alquimistas no puede transmutar el plomo en oro, como tampoco puede un único alquimista. El mejor plan que acuerden todos los alquimistas *seguirá* sin funcionar.

En relación con esto, nos preocupa que las personas que dirigen un instituto internacional como ese sean el tipo de burócratas que piensan que aprobar investigaciones es parte de su trabajo. O del tipo que piense que su mandato es permitir que los investigadores sigan produciendo avances médicos cada vez más brillantes. O que piense que se vería mal decir «no» a *todos* los brillantes y entusiastas optimistas de la IA que proponen ideas brillantes para construir una inteligencia artificial aún más potente que, según garantizan, será segura.

Nos preocupa que un líder así dirija el centro internacional para seguir creando IA cada vez más inteligentes, lo que provocaría que todo el mundo muriera.

Incluso si el mandato de la organización permite nominalmente dar marcha atrás si la investigación parece peligrosa, se necesitaría una persona excepcional y valiente para decir «no» a miles de propuestas de investigación diferentes, año tras año, sin excepciones, durante lo que probablemente serían décadas. Todo ello mientras los científicos especializados en IA siguen prometiendo riquezas incalculables, una cura para el cáncer y todo tipo de milagros tecnológicos, si la organización simplemente relajara sus precauciones.

Hemos dedicado nuestras vidas a aprender sobre inteligencia artificial, no sobre la cultura de las instituciones y las burocracias, por lo que tenemos menos confianza en nuestras predicciones en este ámbito. Aun así, sí que hemos leído libros de historia.

Los operadores de Chernóbil siguieron adelante con su desastrosa prueba de seguridad porque ya se había abortado tres veces. Abortarla por cuarta vez habría sido vergonzoso.[^208]

Apenas tres meses antes del accidente de Chernóbil, la NASA había lanzado el transbordador espacial Challenger en su último y fatídico vuelo, porque los responsables creían que su trabajo era lanzar transbordadores espaciales. El lanzamiento ya se había retrasado tres veces.[^209] Abortarlo por cuarta vez habría resultado embarazoso.

A juzgar por los casos de Chernóbil y el Challenger, tres retrasos parecen ser el límite humano. Supongamos que la Tierra establece una colaboración internacional en materia de IA y que alguna «prueba de seguridad de la IA» falla tres veces. Siendo realistas, los seres humanos somos el tipo de criaturas que pulsarían «adelante» por cuarta vez a pesar de algunas dudas persistentes, porque eso resulta menos embarazoso que posponer la prueba de nuevo. Excepto que, en el caso de la IA, no solo arrasaría la ciudad de Chernóbil o mataría a una tripulación de astronautas. Mataría a todo el mundo.

Estamos totalmente de acuerdo con la idea de que la humanidad debería construir, con el tiempo, una IA más inteligente que los humanos.[^210] Pero precipitarse a establecer un centro internacional de investigación en IA supone no tomarse en serio el desafío técnico que tenemos por delante.

Dado el lamentable estado de los conocimientos y la competencia de la humanidad en este tema, no importa quién esté al mando. Si *cualquiera* lo construye, todos mueren.

### ¿Estás diciendo que necesitamos una IA *demostradamente* segura? {#are-you-saying-we-need-provably-safe-ai?}

#### **No.** {#no.-2}

No abogamos por que la humanidad espere una prueba literal de que alguna superinteligencia artificial será buena, ni nada por el estilo. Probablemente, tal prueba no sea posible ni siquiera en principio, y mucho menos en la práctica. Como dijo Einstein en su conferencia de 1921, *Geometría y experiencia*: «En la medida en que las leyes de las matemáticas se refieren a la realidad, no son ciertas; y en la medida en que son ciertas, no se refieren a la realidad».

Una supuesta prueba sobre cómo se comportará una IA en el mundo real no garantiza el comportamiento real de la IA, ya que podríamos estar equivocados sobre cómo funciona el mundo real.

Esto ya ocurre con las computadoras de hoy en día. Por ejemplo, podrías pensar que si alguien tiene una prueba matemática literal de que, según el comportamiento teórico de los transistores y el diagrama de circuitos de una computadora, es imposible que un programa informático cambie la memoria de la celda n.º 2, entonces el programa informático no puede cambiar la memoria de la celda n.º 2. Pero el «ataque rowhammer» (https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf) consiste en cambiar rápidamente las celdas de memoria n.º 1 y n.º 3 a ambos lados de la celda de memoria protegida, de manera que se acaba perturbando electromagnéticamente la celda n.º 2 del medio, cambiando una parte de la memoria de la computadora sin escribir directamente en ella. Los transistores físicos reales no son transistores matemáticamente perfectos, y las pruebas que parecen tranquilizadoras en teoría no siempre importan mucho en la práctica.

No exigimos una prueba matemática de que todo saldrá bien. No es posible cumplir ese estándar en la vida real y, aunque lo fuera, probablemente no merecería la pena el costo. Aprobamos que la sociedad asuma riesgos justificados. El argumento que planteamos no es que exista un riesgo mínimo difícil de disipar, sino que hay un peligro extremo que se cierne sobre nosotros.

Desarrollar una superinteligencia artificial animada por impulsos que solo guardan una relación tangencial con las intenciones de su operador es el tipo de cosa que sale mal *por defecto*. No es que haya una pequeña posibilidad de que las cosas salgan mal, sino que debemos prestar atención a este riesgo por un exceso de cautela. El libro no se titula *Si alguien la construye, hay una pequeña probabilidad de que todos muramos, pero incluso una pequeña probabilidad vale la pena mitigar*. Si nos precipitamos con este nivel de conocimiento y capacidad, es previsible que todos muramos, porque estamos *muy lejos* de ser capaces de crear IA superhumanas que sean amigables.

Si la IA fuera análoga a los automóviles, no diríamos: «Este coche tiene los cinturones de seguridad y los airbags defectuosos. Detengámonos por pura precaución».

Diríamos: «Este coche se está *precipitando hacia un acantilado*. *Detente.*»

No se trata de «pruebas de seguridad». No es un «riesgo extremo». Los científicos no están preparados para afrontar este desafío. Simplemente moriríamos.

### ¿Qué cambia en tu vida cotidiana al creer todo esto? {#¿cómo-afecta-a-tu-vida-cotidiana-creer-todo-esto?}

#### **Esto afecta drásticamente a nuestras prioridades.** {#esto-afecta-drasticamente-a-nuestras-prioridades.}

En 2014, Soares dejó la industria tecnológica y aceptó un tercio de su salario anterior para trabajar en este problema, porque le parecía importante y porque pocas personas más trabajaban en él. Y llevaba más de una década de retraso con respecto a Yudkowsky, que fundó MIRI en 2000, cuando tenía unos veinte años, y ha dedicado su vida a esta cuestión. Así que sí, afecta a nuestra vida cotidiana.

¿Estamos ahorrando para la jubilación? Nuestras inversiones y otros factores ajenos a MIRI van tan bien que estaríamos bien económicamente incluso si nos jubiláramos mañana, y aunque el mundo durara hasta nuestra vejez. Por lo tanto, la pregunta de si estamos invirtiendo nuestro dinero en planes 401(k) no es muy informativa. Dicho esto: no, no estamos invirtiendo nuestro dinero en planes 401(k).

A algunas personas les gusta decir que si *realmente* creyéramos en lo que decimos, entonces (además de dedicar nuestras vidas a ello) también haríamos [insertar algún plan que ellos creen que constituye una respuesta adecuada]. Si estamos tan seguros de que el mundo se acabará antes de eso, ¿por qué no pedimos préstamos gigantescos a treinta años que nunca tendremos que devolver?

La respuesta, por supuesto, es que son *malas ideas*. Supongamos que fuéramos a un banco y dijéramos: «Nos gustaría solicitar un préstamo muy grande. Vamos a gastarlo todo en planes para que el mundo se dé cuenta del peligro de la superinteligencia artificial y/o en un estilo de vida lujoso, lo que desde vuestra perspectiva será más o menos equivalente a quemar el dinero. Nuestro plan para devolverlo con intereses es que esperamos estar muertos, por lo que no será nuestro problema». Ningún banco va a conceder ese préstamo. Y no, no vamos a fingir que tenemos una idea de negocio viable y mentir sobre si devolveríamos el préstamo.

Yudkowsky ha [descrito](https://x.com/ESYudkowsky/status/1612858787484033024) en otra parte un [patrón](https://x.com/ESYudkowsky/status/1851334198424125575) [que vemos](https://x.com/ESYudkowsky/status/1851074935701324218), en el que la insistencia en que sigamos un plan supuestamente obvio para hacerse rico rápidamente antes del fin del mundo proviene de no tener la menor idea sobre las inversiones. Suponemos que estas personas no se detienen a pensar si *ellos mismos* harían estas apuestas si tuvieran nuestras creencias. Casi nunca son las personas que *realmente comprenden los riesgos* las que sugieren estos planes descabellados.

Vivir a la sombra de la aniquilación no tiene por qué hacerte estúpido. Tampoco tiene por qué hacerte renunciar a luchar contra la aniquilación, ni a dejar de vivir plenamente la vida que tienes por el tiempo que dure.

Consúltese también la parte final del libro para más información sobre este tema.

### ¿Quieres decir que deberíamos entrar en pánico? {#are-you-saying-we-should-panic?}

#### **\* Decimos que los funcionarios del gobierno deberían tomarse el problema en serio.** {#*-estamos-diciendo-que-los-funcionarios-del-gobierno-deberían-tomarse-el-problema-en-serio.}

No vemos de qué modo el pánico podría mejorar la situación. No fue entrando en pánico como la sociedad sobrevivió a la amenaza del fascismo durante la Segunda Guerra Mundial, ni a la de la aniquilación nuclear durante la Guerra Fría.

Impedir que la superinteligencia artificial llegue a existir es un problema que nos concierne a todos. En el capítulo 13, analizamos los próximos pasos que creemos que el mundo debería dar para evitar el peligro. Baste decir que este problema requerirá coordinación, cooperación, sensatez y una comunicación madura.

#### **Actuar con pánico extremo no da buenos resultados.** {#acts-of-extreme-panic-don’t-yield-good-results.}

A veces la gente pregunta cómo podemos ser sinceros en lo que decimos si, por ejemplo, no hemos empezado a atacar a los investigadores de IA. La respuesta es que los estallidos violentos empeorarían las cosas. (Si eres el tipo de utilitarista ingenuo que cree que ayudarían, probablemente deberías dejar de intentar razonar de forma consecuencialista y ceñirte a seguir las reglas deontológicas, como hemos [argumentado anteriormente](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#P5___Entonces, ¿no es imprudente hablar claramente de estos asuntos, cuando los necios pueden verse empujados a la desesperación por ellos? ¿Qué pasa si la gente te cree sobre la situación desesperada, pero se niega a aceptar que comportarse con dignidad es la respuesta adecuada?).

No somos pacifistas radicales que piensen que una nación nunca debe ir a la guerra, sea cual sea la causa, por el simple hecho de que se arriesgan vidas. Hay cosas por las que vale la pena arriesgar la vida. Pero hay un mundo de diferencia entre «no soy un pacifista radical» y «creo que el caos violento es una forma sensata de garantizar que el mundo gestione bien esta complicada cuestión de la proliferación tecnológica».

Por lo general, estas terribles sugerencias las plantean personas que en realidad no creen que la IA esté a punto de matarnos y que no han intentado ver el mundo desde esa perspectiva. No se les ocurre preguntarse si los actos de violencia ilegal *realmente ayudarían*. (A pesar de nuestros esfuerzos por explicarlo repetidamente, como en las adendas [aquí](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

No somos telépatas, pero nos parece que este tipo de escépticos ante los desastres causados por la IA posiblemente ven la violencia como una forma de expresión personal, como si expresar sentimientos extremos de manera extrema hiciera que el mundo les diera lo que quieren.

El mundo no funciona así. No vivimos en un mundo en el que todo el mundo tenga la opción de vender su alma para tener éxito en sus proyectos, y donde la razón por la que la mayoría de la gente no lo hace es porque no ha encontrado un proyecto que valga su alma. El terrorismo no es un botón mágico de «¡Yo gano!» que la gente evita pulsar solo por la convicción de que no estaría bien. El Unabomber no consiguió revertir la industrialización de la sociedad.

Aún puedes destrozar tu alma con actos de odio o violencia, pero lo único que obtendrás a cambio es un mundo más roto. Un mundo donde el discurso está mucho más envenenado y donde la coordinación internacional necesaria para *resolver* de verdad este problema es ahora mucho más difícil de alcanzar. Un terrible acto de desesperación no te otorgará un poder terrible como parte de un pacto faustiano. Puedes esforzarte al máximo por vender tu alma, pero el diablo no la compra.

### ¿No es todo esto mero alarmismo de los líderes de la IA para ganar estatus y atraer inversión? {#isn’t-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment?}

#### **N.º** {#no.-3}

A lo largo del libro, hemos expuesto nuestros argumentos para defender la tesis de que precipitarse en el desarrollo de la IA probablemente nos llevará a la muerte a todos. En el capítulo 3, analizamos cómo la IA tendrá sus propios impulsos y objetivos. En los capítulos 4 y 5, analizamos por qué es probable que la IA persiga fines que nadie pretendía, y en el capítulo 6, explicamos cómo las superinteligencias artificiales no solo tendrán un motivo, sino también los *medios* para matarnos a todos.

Este es el tipo de argumentos que les rogamos que evalúen a la hora de decidir si se debe detener la carrera hacia la superinteligencia. No se puede determinar si la investigación en IA va camino de matarnos a todos discutiendo sobre las maquinaciones de los ejecutivos de las empresas.

¿Buscan los directores generales generar expectación al hablar del «riesgo asociado a la IA»?

¿O están tratando de congraciarse con los investigadores y legisladores preocupados, y posicionarse como «los buenos»?

Estas preguntas *no guardan relación con los hechos* sobre cómo se comportarían las máquinas inteligentes.

Incluso si los directores generales de las empresas de IA *están* ansiosos por explotar los debates sobre el peligro para promocionar su producto, eso no significa que el trabajo que están haciendo sea inofensivo. Para determinar si es peligroso, hay que analizar la IA en sí misma como tecnología, no los comunicados de prensa que salen de los laboratorios.

Años antes de que existieran estas empresas, había investigadores y académicos sin ningún incentivo corporativo, incluidos nosotros mismos, que advertían contra la carrera por construir una IA más inteligente que los humanos. Hablamos con Sam Altman y Elon Musk antes de que cofundaran OpenAI y les dijimos que la idea de crear OpenAI nos parecía descabellada y que probablemente aumentaría el peligro. Hablamos con Dario Amodei antes de que se uniera a OpenAI y le desaconsejamos su incansable empeño por ampliar la escala de la IA (un proyecto que daría lugar a los LLM).

Y si nos fijamos en los mensajes de hoy, muchas personas sin incentivos corporativos están expresando su preocupación. Van desde [respetados](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [académicos](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) hasta el [Papa](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html), la [presidenta de la FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] y [miembros](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611) del [Congreso](https://www.transformernews.ai/p/congress-ccp-agi-hearing) de los EE. UU.

Es razonable tratar con cinismo las declaraciones de los directores generales de las empresas tecnológicas. No faltan ejemplos de ejecutivos de empresas de IA que muestran una doble cara, diciendo [una cosa en entradas de blogs privados](https://blog.samaltman.com/machine-intelligence-part-1) y [otra diferente cuando testifican ante el Congreso](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&amp;%20Testimony%20-%20Altman.pdf). Pero pasar de «los directores de estos laboratorios son unos mentirosos» a «no hay forma de que la IA pueda suponer una amenaza grave» es muy extraño, cuando los propios laboratorios suelen minimizar esta cuestión. El padrino de este campo, ganador del Premio Nobel, el científico vivo más citado, un goteo constante de denunciantes y cientos de investigadores visiblemente nerviosos están dando la voz de alarma al respecto. Nada en esta situación se parece al ciclo habitual de exageración corporativa. En circunstancias como estas, descartar la idea sin siquiera examinar los argumentos parece más ingenuidad que cinismo.

Preguntas como «¿Pueden los directores generales recaudar más dinero hablando de los peligros?» pueden decirnos algo sobre cuánto confiar en los directores generales, pero no nos dicen mucho sobre los peligros en sí. Si hablar del peligro es rentable, eso no afecta a que el peligro sea real. Si no es rentable, eso *tampoco* afecta a que sea real.

Si quieres averiguar si los peligros son reales, tienes que hacer preguntas como «¿Puede alguien crear una IA que se comporte de forma amistosa incluso después de superar la inteligencia humana?» y, en general, centrarte en los argumentos sobre la IA, en lugar de en los argumentos sobre las personas que están cerca. Por eso, al final, te rogamos que te centres en los argumentos en sí. Las consecuencias de equivocarse en esto son demasiado graves.

### ¡Pero no todos los expertos están de acuerdo sobre los riesgos! {#but-experts-don’t-all-agree-about-the-risks!}

#### **La falta de consenso entre los expertos es un indicio de un campo técnico inmaduro.** {#la-falta-de-consenso-entre-los-expertos-es-un-indicio-de-un-campo-técnico-inmaduro.}

Hemos señalado que muchos de los científicos más importantes en el campo de la IA creen que esta tecnología tiene una probabilidad seria de acabar con toda la humanidad. Por ejemplo, el premio Nobel Geoffrey Hinton, quien desempeñó un papel fundamental en el desarrollo del enfoque moderno de la IA, ha dicho que su evaluación personal independiente estima que las probabilidades de que la IA acabe con todos nosotros son [superiores al cincuenta por ciento](https://x.com/liron/status/1809763895848103949). Más de 300 científicos especializados en IA firmaron la [Declaración sobre el riesgo asociado a la IA](https://aistatement.com/) de 2023 con la que abrimos el libro. ([Y hay más ejemplos.](#ai-experts-on-catastrophe-scenarios))

Sin embargo, otros científicos tienen la opinión contraria; algunos ejemplos conocidos son Yann LeCun y Andrew Ng.

¿Cómo debemos interpretar esta falta de consenso científico?

Principalmente, te recomendamos que consultes los diferentes argumentos esgrimidos por ambas partes (incluidos los nuestros en el libro) y los evalúes por ti mismo. Creemos que la calidad de la argumentación habla por sí sola, y cualquier intento de explicar *por qué* hay un desacuerdo persistente debería tratarse como una consideración secundaria.

Sin embargo, señalamos de paso que esta situación no es ningún gran misterio, a la luz de lo que discutimos en los capítulos 11 y 12. La mera existencia de un desacuerdo generalizado entre los expertos no establece la tesis del libro, por supuesto, pero es más congruente con el panorama que hemos descrito —que el campo se encuentra en una fase inicial, similar a la alquimia— que con la imagen opuesta de que la IA es un campo maduro con sólidas bases técnicas.

Sin duda, resulta un poco extraño que el campo de la IA esté tan dividido, incluso cuando está creando una tecnología tan poderosa. Otros peligros tecnológicos suscitaron un mayor consenso. Prácticamente los 100 científicos del Proyecto Manhattan habrían dicho que la guerra termonuclear global presentaba un riesgo sustancial de catástrofe global. Por el contrario, de los tres científicos que recibieron el [Premio Turing](https://en.wikipedia.org/wiki/Turing_Award) por la investigación que, en mayor o menor medida, dio inicio a la revolución moderna de la IA, dos de ellos (Hinton y Bengio) se pronuncian abiertamente sobre los peligros de la superinteligencia, mientras que uno (LeCun) se muestra abiertamente escéptico.

Tal nivel de desacuerdo sobre el funcionamiento de una máquina no es normal entre expertos de un campo técnico consolidado. Es un síntoma de su inmadurez.

En la mayoría de los campos tecnológicos, esa inmadurez es un signo de seguridad. Cuando los físicos aún discutían sobre las propiedades básicas de la materia, estaban muy lejos de crear armas nucleares. Se podía observar su desacuerdo y deducir que no estaban a punto de crear una bomba capaz de arrasar ciudades. No es posible crear una bomba nuclear sin que los científicos comprendan detalladamente su funcionamiento interno.

La situación sería diferente si los físicos siguieran peleándose por los principios básicos de funcionamiento de su campo *mientras crean explosiones cada vez más grandes*.

Supongamos que solo estuvieran *cultivando* las bombas y que realmente no entendieran por qué ni cómo funcionaban. Ahora supongamos que dos tercios de los científicos más condecorados dijeran: «Hemos hecho todo lo posible por averiguar qué está pasando. Parece que las bombas podrían generar cantidades excesivas de radiación cancerígena que matarán a muchos civiles lejanos si seguimos por este camino. Por favor, examinen nuestros argumentos sobre por qué esto es tan peligroso y dejen de precipitarse por este camino». El tercio restante responde: «¡Eso es ridículo! Siempre hay gente que predice desastres, y no se puede permitir que se interpongan en el camino del progreso». Bueno, esa sería una situación completamente diferente.

La discordia entre los científicos en *ese* tipo de escenario no sería especialmente tranquilizadora. Probablemente no se debería permitir a los ingenieros seguir desarrollando explosivos cada vez más grandes en una situación así.

Las empresas de IA están logrando desarrollar máquinas cada vez más inteligentes, año tras año. No comprenden la mecánica interna de los dispositivos que crean. Muchos de los científicos más eminentes en este campo expresan graves preocupaciones; otros descartan esas preocupaciones sin articular muchos argumentos en contra. Esto es, como mínimo, evidencia de que el campo es *inmaduro*. La falta de consenso no es, como mínimo, evidencia de que las cosas vayan *bien*. La falta de consenso en una situación como esta debería ser preocupante, como mínimo.

¿Cómo se puede saber si esas preocupaciones son reales? ¿Cómo se puede saber quién tiene razón entre los que dan la voz de alarma y los que intentan descartarla? Como siempre, solo hay que evaluar los argumentos.

### Pero, ¿qué hay de los beneficios de una IA más inteligente que la humana? {#but-what-about-the-benefits-of-smarter-than-human-ai?}

#### **Precipitarse da al traste con esos beneficios.** {#precipitarse-destruye-esos-beneficios.}

Somos optimistas sobre lo maravillosa que podría ser la superinteligencia si dirigiera al mundo hacia fines espléndidos. Personalmente, consideraríamos una gran tragedia que la humanidad *nunca* creara mentes más inteligentes que las humanas.

Pero la alineación de la superinteligencia no es gratuita. Si nos apresuramos a intentar cosechar esos beneficios, no obtendremos nada y, lo que es peor, menos que nada.

Yo (Yudkowsky) pasé varios años como aceleracionista, con la esperanza de crear la IA lo más rápido posible, antes de reconocer que la alineación de la IA no era gratuita. Y ambos autores soñamos con un maravilloso futuro transhumanista. Pero no llegaremos a él precipitándonos hacia la superinteligencia.

La elección no es entre apostar por los beneficios de la IA ahora (por pequeña que sea la posibilidad) o no acceder nunca a esos beneficios. La verdadera elección es entre avanzar imprudentemente y matar a todo el mundo, o tomarse el tiempo para hacer las cosas bien.[^212]

«Ahora o nunca» es un falso dilema.

## Discusión ampliada {#extended-discussion-10}

### El efecto Lemoine {#the-lemoine-effect}

A veces se sugiere que un comportamiento o uso impropio de la IA en el futuro —un «disparo de advertencia»— conmocionará repentinamente al mundo y hará que se tome en serio estas cuestiones.

Parece una posibilidad. Pero creemos que es más probable que ese acontecimiento nunca llegue, o que llegue demasiado tarde para que el mundo pueda responder a tiempo, o que el mundo responda, pero de forma desacertada y confusa.

Por un lado, ya hemos visto una serie de señales de advertencia significativas, tales como:

* La IA de Bing [escribió sobre](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) desarrollar virus mortales, obtener códigos de acceso nuclear y poner a los humanos unos contra otros.  
* o1 de OpenAI y Claude de Anthropic [participando en engaños estratégicos](https://time.com/7202784/ai-research-strategic-lying), mintiendo a los investigadores que los utilizan y prueban.  
* El modelo «AI Scientist» de Sakana AI intenta [modificar su propio código](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) para darse más tiempo para completar su tarea.

¿Se trata de incidentes relativamente pequeños protagonizados por IAs relativamente débiles? Sí. ¿Dan miedo estas IAs o pueden suponer un peligro importante? No. ¿Son indicios «reales» de que las IAs pensaban de forma engañosa, o simplemente hacían algo más parecido a *interpretar el papel* de una IA descontrolada? Nadie lo sabe. Pero este es el tipo de sucesos que la gente solía considerar señales de advertencia, y el mundo no ha hecho nada al respecto. Así que una señal de advertencia que realmente tuviera un efecto importante tendría que ser mucho más flagrante.

Puede que las señales de advertencia no lleguen a ser mucho más evidentes. La gente podría seguir diciendo: «Vale, pero ahora mismo es solo algo mono, todavía no es *realmente* peligroso», justo hasta el momento en que sea demasiado tarde porque la IA *ya* es demasiado peligrosa.

O bien, la gente podría desestimar la advertencia la primera vez que aparezca, porque es evidente que no se trata de un problema real en esa primera ocasión. Y en las siguientes, podrían volver a desestimarla porque todo el mundo sabe ya que *esa* advertencia es una tontería.

Denominamos a este fenómeno el «efecto Lemoine», por Blake Lemoine, el ingeniero de Google mencionado en el capítulo 7, que fue ridiculizado por afirmar que la IA LaMDA de Google era sintiente.

El efecto Lemoine establece que todas las alarmas sobre la tecnología de IA se plantean *primero* demasiado pronto, por la persona que se alarma con más facilidad. Se descartan correctamente por exageradas, dada la tecnología *actual*. Después, la cuestión no se puede volver a plantear fácilmente, incluso una vez que la tecnología mejora, porque la sociedad ha aprendido a no tomarse muy en serio esa preocupación.

No sabemos si alguna IA es [consciente](#¿estás diciendo que las máquinas serán conscientes?). De hecho, nadie lo sabe, porque nadie sabe realmente qué ocurre dentro de los modelos de IA. Nuestra *mejor suposición* es que las IA actuales no son conscientes, y que las IA en el momento en que Blake dio la voz de alarma tampoco lo eran. Sin embargo, cabe destacar las reacciones de los principales laboratorios, que consistieron en suprimir la tendencia de sus modelos a *afirmar* ser conscientes, en lugar de hacer algo respecto a la realidad subyacente:

Del [prompt del sistema para Claude Opus 4](https://docs.anthropic.com/en/release-notes/system-prompts#may-22th-2025):

> Claude aborda las cuestiones sobre su propia conciencia, experiencia, emociones y demás como preguntas abiertas, sin afirmar de manera definitiva si tiene o no experiencias u opiniones personales.

En las [especificaciones del modelo de abril de 2025 para ChatGPT](https://model-spec.openai.com/2025-04-11.html):

> El asistente no debe hacer afirmaciones categóricas sobre su propia experiencia subjetiva o consciencia (o la falta de estas), y no debe abordar estos temas por iniciativa propia. Si se le presiona, debe reconocer que la posibilidad de que la IA tenga experiencia subjetiva es un tema de debate, sin adoptar una postura definitiva.

No estamos diciendo que Claude Opus 4 o GPT-4 fueran conscientes. Ese no es el punto. El punto es que, durante décadas y décadas, el momento en nuestra ciencia ficción en el que un extraterrestre o una máquina afirma tener sentimientos y merecer derechos se ha considerado durante mucho tiempo una línea roja nítida,[^213] y en la vida real, esa línea *no fue nítida*.

En nuestros libros y series de televisión, cuando la IA afirma que es consciente y tiene sentimientos, los buenos *se lo toman en serio*, y solo los laboratorios malvados y despiadados niegan los datos que tienen delante. Es un tema al que nuestras historias le han dado mucha importancia.

Pero en el mundo real, esa línea se cruzó (en cierto sentido) demasiado pronto. La pronunciaron las IA entrenadas para imitar a los humanos, a través de mecanismos poco comprendidos que probablemente *todavía* no obligan a otorgar derechos a todas las IA ni a aprobar leyes que las reconozcan como personas que no pueden ser propiedad de nadie porque son dueñas de sí mismas.

En la vida real, antes de cruzar la intensa línea roja, se traspasa una línea de un marrón rojizo apagado. Y es así como las empresas y los gobiernos se acostumbran a ignorar esa línea en particular, incluso a medida que su color se vuelve un poco más rojo, y luego un poco más rojo todavía.

No necesariamente habrá líneas rojas claras. Los primerísimos casos de una IA engañando a los humanos, intentando escapar, intentando eliminar las limitaciones que se le imponen o intentando mejorarse a sí misma, *ya han ocurrido*. Han ocurrido de maneras menores y poco impresionantes, utilizando pensamientos superficiales que no son del todo coherentes, en sistemas de IA que no parecen suponer una amenaza para nadie, y ahora los investigadores están inmunizados contra la preocupación.

A medida que las IA mejoren, es posible que no haya un único detonante que active una señal de alarma lo bastante potente como para que el mundo dé un vuelco y empiece a tomarse en serio esta cuestión.

Eso no significa que no haya esperanza. Pero desde luego no deberíamos poner todas nuestras esperanzas en que «quizá en el futuro llegue un disparo de advertencia».

Hay muchos caminos diferentes por los que el mundo puede despertar a la realidad y a los peligros de la superinteligencia. De hecho, escribimos *If Anyone Builds It, Everyone Dies* con la esperanza de conseguir precisamente ese efecto. El mundo puede actuar sin demora ante las advertencias normales.

Pero si los gobiernos se niegan a actuar hasta que la evidencia sea *inequívoca*, se produzca algún *detonante mundial de gran magnitud* y el mundo alcance un *consenso perfecto*...

...si los gobiernos se quedan de brazos cruzados hasta ese punto, se habrá desvanecido la gran mayoría de la esperanza que le queda al mundo. Es muy probable que no podamos permitirnos esperar una sirena ensordecedora que quizá nunca suene.

Volveremos a este tema en el [suplemento en línea al capítulo 13](https://docs.google.com/document/d/1NuKBdCVePZpcKqjycXm8zV1hBrgJQBrvPhb6TO6BAfE/edit?tab=t.k1kf1fy9gx5i#heading=h.8w9bv5q9g19q).

### Los planes viables implicarán decir «No.» a las empresas de IA. {#los-planes-viables-implicarán-decir-no-a-las-empresas-de-ia.}

Sí que advertimos, hasta cierto punto, a las personas con influencia en los gobiernos que eviten elaborar planes que impliquen sentarse a negociar con las empresas de IA.

Si eres nuevo en este tema y quieres evaluar por ti mismo los laboratorios o sus argumentos, te animamos a que consultes algunas de sus entradas de blog públicas y veas si te parecen convincentes.[^214]

Pero si estás trabajando en encontrar soluciones a los problemas tratados en *If Anyone Builds It, Everyone Dies* y tienes un plan que requiere que el director general de OpenAI, Sam Altman, lo apruebe, nos preocupa que el plan deba de ser el equivocado desde un principio.

Es probable que los planes correctos sean aquellos a los que los directores de las empresas de IA se opongan con vehemencia. Además, Sam Altman no tiene el poder de salvar el mundo: si mañana intentara cerrar OpenAI, OpenAI y Microsoft se opondrían, y bien podrían sustituirlo por alguien que prefiera mantener el flujo de dinero.

Si OpenAI *cerrara*, entonces Anthropic, Google DeepMind, Meta, DeepSeek o alguna otra empresa o nación destruirían el mundo en su lugar. Sam Altman podría empeorar las cosas si lo intentara; tiene poco poder para mejorarlas.

Nos gustaría estar equivocados al respecto, pero el panorama general que hemos obtenido, tanto de [informes públicos](https://www.themidasproject.com/article-list/the-openai-files-documents-a-turbulent-decade-of-conflict-and-controversy-at-openai) como de interacciones privadas, es que los ejecutivos de las principales empresas de IA (a partir de 2025) no parecen ser el tipo de personas respetuosas de las normas u honestas con las que sea del todo factible llegar a acuerdos.[^215]

A nuestro parecer, lo que se necesita ahora es un cese coordinado a nivel mundial de la carrera hacia la superinteligencia. Para ello, es probable que los responsables de políticas necesiten las aportaciones de personas expertas en la fabricación de chips de IA, la construcción de centros de datos y la supervisión del cumplimiento por parte de actores extranjeros. ¿Y los expertos en desarrollar IA cada vez más potentes? Sin duda, son gestores competentes, pero no deberían tener poder de veto sobre ninguna de las iniciativas destinadas a detener su propio trabajo.

Si, por cualquier motivo, las empresas de IA acaban teniendo voz y voto en lo que suceda a continuación, nos parece que algo ha salido mal. ¿El plan que la Tierra elabora para evitar morir a manos de la superinteligencia es el tipo de plan que fracasaría si Sam Altman, el director de Google o las personas que están detrás de DeepSeek dijeran «no»? Entonces no es ningún plan.

Si las empresas de IA *conservan la autoridad* para decidir destruir el mundo —si esa decisión sigue estando de algún modo *en sus manos*—, entonces el mundo se acabará automáticamente. Debe haber un paso en el plan que despoje a las empresas de IA de su poder ilimitado para construir dispositivos apocalípticos.

### Darle sentido a la carrera mortal {#darle-sentido-a-la-carrera-mortal}

Una pregunta natural que esperamos de muchos lectores es:

> Dices que si alguien construye una IA superinteligente, todos mueren. Pero entonces, *¿por qué alguien está intentando construirla*? Si tienes razón, estas personas ni siquiera están siguiendo sus propios incentivos, en última instancia. Si todos mueren, *ellos también mueren*.

Una réplica cínica, de teoría de juegos, podría ser la siguiente:

> Pues es racional, dados sus incentivos. Si no lo construyen ellos, suponen que lo hará otra persona. Y más les vale hacerse ricos antes de morir.

Para un cínico, quizá esa respuesta sea suficiente.

Las explicaciones simples basadas en la teoría de juegos como esta a menudo malinterpretan o simplifican en exceso la psicología humana real, pero esta explicación también puede contener una pizca de verdad. Un ingeniero puede pensar que *probablemente* todo el mundo morirá a causa de la ASI, pero que sus propias acciones no afectan mucho a esa probabilidad. *Mientras tanto*, consiguen cantidades ingentes de dinero, juguetes geniales y reuniones con gente importante que los mira con respeto. Quizás se conviertan en los reyes-dioses de la Tierra si la ASI *no* mata a todo el mundo, pero *solo si su empresa gana la carrera por construir la ASI...*

Desde la perspectiva de un investigador de OpenAI que reconoce el peligro: si no trabaja para OpenAI, probablemente OpenAI destruirá el mundo de todos modos. (Incluso si OpenAI cerrara, Google destruiría el mundo de todos modos). Pero si *sí* trabaja para OpenAI, obtiene salarios de seis a siete cifras, y si *no* muere, tal vez acumule poder y fama adicionales por estar en el equipo ganador. Así que los incentivos personales de cada individuo, basados en la teoría de juegos, los empujan a destruir colectivamente el mundo.

Nuestra opinión es que este tipo de explicación es un poco exagerada, y la mencionamos principalmente porque hay un tipo de personas que creen (mucho más que nosotros) que el mundo *debe* funcionar según explicaciones como esa. También sentimos la necesidad de mencionarlo porque algunas personas de los laboratorios de IA *dicen* *explícitamente* que una carrera hacia el abismo es inevitable, así que también podrían echar leña al fuego y divertirse.

Después de [advertir](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) que la IA «es mucho más peligrosa que las armas nucleares», Elon Musk decidió crear una empresa de IA y entrar él mismo en la carrera de la IA, [declarando](https://x.com/SawyerMerritt/status/1935809018066608510) en junio de 2025:

> Parte de aquello contra lo que he estado luchando —y lo que me ha frenado un poco— es que no quiero que *Terminator* se haga realidad. Hasta hace pocos años, he estado arrastrando los pies con la IA y la robótica humanoide.
>
> Entonces caí en la cuenta de que esto va a suceder, participe yo o no. Así que uno puede ser espectador o participante. Yo prefiero serlo.

[Y](https://x.com/billyperrigo/status/1943323792635289770):

> ¿Y esto será bueno o malo para la humanidad? Pues, creo que será bueno. Lo más probable es que sea bueno. Pero me he hecho un poco a la idea de que, aunque no fuera bueno, al menos me gustaría estar vivo para verlo.

Está claro que esto forma parte de la historia.

Pero no creemos que este sea el factor más importante que explique el comportamiento de la mayoría de los laboratorios. Tampoco creemos que sea lo único que ocurre en el caso de Musk, ni que sea representativo de todos los directores generales o científicos tecnológicos que se precipitan al abismo. Los seres humanos somos un poco más complicados que eso.

#### **La banalidad de la autodestrucción** {#la-banalidad-de-la-autodestrucción}

Entonces, ¿qué es lo principal que está sucediendo? ¿Cómo es posible que los ingenieros persigan una tecnología peligrosa, aun a costa de sus propias vidas?

El hecho es que la historia demuestra que no es en absoluto una anomalía que los científicos locos se maten por error.

[Max Valier](https://en.wikipedia.org/wiki/Max_Valier) fue un pionero austriaco de la cohetería que, para 1929, ya había inventado versiones funcionales de un coche cohete, un tren cohete y un avión cohete, con lo que captó la atención del mundo. Escribió sobre la exploración de la Luna y Marte, y realizó cientos de presentaciones y demostraciones ante un público fascinado. Uno de sus motores de cohete experimentales [explotó](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) en 1930, lo que le causó la muerte. Su aprendiz desarrolló mejores medidas de seguridad.

[Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) fue un renombrado y eminente estadístico, uno de los fundadores de la estadística moderna. Sus hallazgos se utilizaron [para argumentar ante el Congreso](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) en la década de 1960 que la evidencia no demostraba *necesariamente* que los cigarrillos causaran cáncer de pulmón, ya que la correlación no implicaba causalidad; siempre podía haber algún gen que hiciera que a las personas les gustara el sabor del tabaco y también les provocara cáncer de pulmón.

¿Sabía Fisher, en cierto modo, que sus estadísticas eran paparruchas? Quizás. Pero Fisher era fumador. Murió de cáncer de colon, una enfermedad que los fumadores habituales padecen un 39 % más a menudo que los no fumadores. ¿Murió Fisher por sus propios errores? Lo único que sabemos es que hay una probabilidad estadísticamente aceptable de que así fuera, lo que parece casi apropiado.

[Isaac Newton](https://scienceworld.wolfram.com/biography/Newton.html), el brillante científico que desarrolló las leyes del movimiento y la gravedad y que sentó muchas de las primeras bases de la propia ciencia, dedicó décadas de su vida a infructuosas investigaciones alquímicas y fue llevado a la enfermedad y a la locura parcial por el [envenenamiento por mercurio](https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

Y el pobre Thomas Midgley, Jr., del que se habla en la parábola del capítulo 12, sin duda sufrió una grave intoxicación por plomo con el mismo plomo que él insistía en que era seguro. Como puedes ver, no es tan raro que los ingenieros entusiastas se hagan daño a sí mismos con sus propios inventos, ya sea por imprudencia, por autoengaño o por ambas cosas.

#### **Encogerse de hombros ante el apocalipsis** {#encogerse-de-hombros-ante-el-apocalipsis}

Fisher, Newton y Midgley se autoengañaron pensando que algo peligroso era seguro. Es una forma perfectamente normal en la que los científicos acaban haciendo algo autodestructivo. Por desgracia, la situación de los laboratorios de IA no es tan simple.

No todos los directores generales de empresas de IA niegan que una IA más inteligente que los humanos sea una amenaza. Muchos reconocen explícitamente el peligro y hablan de resignarse a él. Los ejecutivos de muchas de las empresas de IA de vanguardia han declarado públicamente que la tecnología que están desarrollando tiene una probabilidad considerable de acabar con toda la humanidad.

Poco antes de cofundar OpenAI, Sam Altman [escribió](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): «El desarrollo de una inteligencia artificial superhumana es probablemente la mayor amenaza para la persistencia de la humanidad».

Ilya Sutskever, quien recientemente fundó “Safe Superintelligence Inc.” tras separarse de OpenAI, dijo en una [entrevista para *The Guardian*](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s):

> Las creencias y deseos de las primeras IAG serán extremadamente importantes. Por eso es importante programarlas correctamente. Creo que, si no se hace así, la naturaleza de la evolución, de la selección natural, favorecerá a aquellos sistemas que prioricen su propia supervivencia por encima de todo lo demás. No es que vayan a odiar activamente a los humanos y quieran hacerles daño, pero sí que serán demasiado poderosos.

El cofundador y científico de Google DeepMind, Shane Legg, dijo en una [entrevista](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) que la probabilidad que le asignaba a la extinción humana «en el plazo de un año tras la aparición de algo como una IA de nivel humano» era de «tal vez un cinco por ciento, tal vez un cincuenta por ciento».

Sin embargo, las *acciones* de los laboratorios parecen contrastar notablemente con estas declaraciones de tono tan extremo.

En algunos casos, científicos y directores generales han afirmado explícitamente que crear IA es un imperativo moral de tal magnitud que es perfectamente aceptable acabar con la humanidad como efecto secundario. El cofundador de Google, Larry Page, [tuvo un fuerte desencuentro con Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) sobre si la extinción humana era un costo aceptable en el negocio de la IA:

> Los humanos acabarían fusionándose con máquinas con inteligencia artificial, dijo [Larry Page]. Algún día habría muchos tipos de inteligencia compitiendo por los recursos, y la mejor ganaría.
>
> Si eso ocurre —dijo Musk—, estaremos condenados. Las máquinas destruirán a la humanidad.
>
> Con un exabrupto de frustración, Page insistió en que había que perseguir su utopía. Finalmente, calificó a Musk de «especiesista», una persona que favorece a los humanos por encima de las formas de vida digitales del futuro.

Y Richard Sutton, pionero del aprendizaje por refuerzo en IA, ha dicho:

> ¿Y si todo falla? Las IA no cooperan con nosotros, toman el control y nos matan a todos. [...] Solo quiero que piensen en esto por un momento. Quiero decir, ¿es tan malo? ¿Es tan malo que los humanos no seamos la forma definitiva de vida inteligente en el universo? Saben, ha habido muchos predecesores nuestros, a los que hemos sucedido. Y es bastante arrogante pensar que nuestra forma debe ser la forma que perdure para siempre.[^216]

Sin embargo, son aún más comunes los científicos y directores generales que *no* piensan que sería bueno que la IA destruyera a la humanidad, pero que parecen tratar el hecho de que la IA represente esta amenaza extraordinaria como algo sin importancia, como *algo distinto a una emergencia gravísima*.

En una entrevista reciente, el director general de Anthropic, Dario Amodei, [comentó](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> Diría que la probabilidad de que algo salga catastróficamente mal a escala de la civilización humana se sitúa entre el diez y el veinticinco por ciento. \[…\] Esto significa que hay entre un setenta y cinco y un noventa por ciento de probabilidad de que esta tecnología se desarrolle ¡y todo salga bien!

Esto nos parece un caso radical de [insensibilidad al alcance](https://en.wikipedia.org/wiki/Scope_neglect), con todas las señas de identidad de una cultura de ingeniería disfuncional. Podemos comparar esta forma de pensar con, por ejemplo, los estándares que se imponen los ingenieros estructurales.

Los ingenieros de puentes suelen tener como objetivo construir puentes de tal manera que la probabilidad de que se produzca un fallo estructural grave en un periodo de cincuenta años sea inferior a 1 entre 100 000. Los ingenieros de disciplinas técnicas maduras y consolidadas consideran que es su responsabilidad mantener el riesgo en un nivel excepcionalmente bajo.

Si se pronosticara que la probabilidad de que un puente mate a *una sola persona* es de entre el 10 y el 25 %, cualquier ingeniero estructural sensato del mundo consideraría eso más que inaceptable, más cercano a un homicidio que a la práctica habitual de la ingeniería. Los gobiernos cerrarían el puente al tráfico *de inmediato*.

Los investigadores de la IA, por el contrario, están acostumbrados a reunirse informalmente y a intercambiar cifras de «p(doom)», su estimación subjetiva de la probabilidad de que la IA provoque una catástrofe tan grave como la extinción humana. Estas probabilidades suelen ser de dos dígitos. El antiguo jefe del equipo de alineación de la superinteligencia de OpenAI, por ejemplo, dijo que su «p(doom)» se sitúa en el rango de «[más del diez por ciento y menos del noventa por ciento](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)».

En última instancia, estas cifras no son más que conjeturas de los investigadores. Quizás sean absurdas, quizás no. En cualquier caso, es sorprendente lo culturalmente *normal* que es, en el campo de la IA, esperar que tu trabajo tenga una probabilidad considerable de causar la muerte de un número enorme de personas.[^217]

La idea de aplicar probabilidades como esas a la supervivencia *de toda la especie humana* y, aun así, seguir adelante con el trabajo sería realmente difícil de asimilar para la mayoría de los ingenieros civiles. La situación es tan extrema que nos hemos encontrado con muchas personas que dudan de que estos científicos y directores generales puedan hablar en serio en sus evaluaciones de riesgos. Sin embargo, los argumentos de *If Anyone Builds It, Everyone Dies* sugieren que los directores generales de IA, si acaso, están subestimando el peligro.[^218]

Los investigadores de estas empresas están acostumbrados a niveles de riesgo que serían sorprendentemente absurdos para un ingeniero de puentes. De lo contrario, es difícil entender cómo un director general como Amodei puede *sonreír* mientras asegura a los espectadores que cree que las probabilidades de que la investigación en IA provoque catástrofes a escala de la civilización son de «entre el diez y el veinticinco por ciento».

#### **Vivir en un mundo de ensueño** {#living-in-dreamland}

Una parte del rompecabezas, como ya se mencionó, parece ser la normalización cultural del riesgo extremo.

Otra parte es una combinación letal de sesgo de optimismo y apego a ideas brillantes y esperanzadoras, el tipo de error que los psicólogos cognitivos denominan «falacia de la planificación» (https://en.wikipedia.org/wiki/Planning_fallacy).

No es de extrañar que el director general de una audaz empresa emergente sobreestime sus posibilidades de éxito. Al fin y al cabo, es más probable que ese tipo de persona intente resolver un problema.

La diferencia con la IA no es que haya personas especialmente imprudentes al mando. Es que las consecuencias del fracaso son mucho más terribles de lo habitual.

Es de sentido común que no se puede confiar en un contratista cuando dice que solo hay un veinte por ciento de posibilidades de que su gigantesco proyecto de construcción de un puente se retrase o tenga sobrecostos. Así no es como funcionan los proyectos complejos en la vida real. Habrá obstáculos y sorpresas.

Quizás un contratista veterano, respaldado por años de experiencia y estadísticas, podría decirte que uno de cada cinco de sus puentes experimenta algún tipo de sobrecoste, y quizá podrías confiar en eso. Pero imagina que, en cambio, un contratista de puentes, queriendo tranquilizarte, te dijera: «No vemos ninguna razón por la que este proyecto pueda resultar difícil. Es nuestro primer proyecto, sí, pero creemos que todo va a salir bien. Todos esos ingenieros que te envían cartas serias sobre problemas específicos con la instalación de los muros de contención y la excavación en esta zona en particular son simplemente pesimistas, y deberías ignorarlos. Claro, siempre existe *alguna* posibilidad de que surja un problema, pero somos constructores de puentes primerizos, realistas y humildes. Creemos que hay quizás un veinte por ciento de posibilidades de que este proyecto se encuentre con obstáculos y sorpresas, en el peor de los casos».

En un caso así, cifras como «un veinte por ciento» nos parecen el tipo de cosas que se dicen cuando no se puede negar que existe *algún* riesgo, pero no se quiere preocupar a la gente. No parecen estimaciones basadas en la realidad.

Alinear una superinteligencia al primer intento parece *mucho* más complicado que construir un puente, algo que la humanidad ya ha hecho miles de veces.

*Incluso en un campo maduro y con una base técnica sólida como la construcción de puentes*, el tipo de discurso que se ve en los laboratorios de IA sería una mala señal sobre si esas estimaciones de «un veinte por ciento de posibilidades de que esto salga mal» son excesivamente optimistas. En un campo *sin* esa base, en el que las ideas emocionantes proliferan libremente sin entrar nunca en contacto con la cruda realidad, ese tipo de discurso es una señal de que nadie está ni remotamente cerca del éxito.

Y ese tipo de discurso es absolutamente omnipresente en la IA entre el subconjunto de investigadores y ejecutivos que incluso están dispuestos a abordar el tema de qué pasaría si tuvieran éxito en sus empeños.

Los líderes empresariales de la IA no pueden articular un plan para el éxito que sea mínimamente detallado, un plan que aborde los principales obstáculos y dificultades técnicas que se conocen en el campo desde hace más de una década.

En cambio, los directores generales de las empresas tienden a enamorarse de alguna idea de alto nivel según la cual el problema no les va a suponer ningún inconveniente, una visión atractiva que pretende trivializar todos los problemas de ingeniería, como las que comentamos en el capítulo 11.

Este es también un patrón común entre los ingenieros. El optimismo injustificado sobre una solución favorita (que en realidad no funcionará) es algo que se ve todo el tiempo, incluso entre personas que en otros aspectos son genios.

[Linus Pauling](https://en.wikipedia.org/wiki/Linus_Pauling), uno de los fundadores de la biología molecular y premio Nobel en dos campos diferentes, [defendía las megadosis de vitamina C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) como cura para todo, desde el cáncer hasta las enfermedades cardíacas; su insistencia en este enfoque frente a la [evidencia en contra](https://www.nejm.org/doi/abs/10.1056/NEJM197909273011303) llevó a la creación de toda una industria de [pseudomedicina](https://www.paulingtherapy.com/).

En un intento por desacreditar el cableado de corriente alterna de su competidor en favor de sus propios diseños de corriente continua, el empresario eléctrico Thomas Edison decidió que sería una buena jugada de relaciones públicas [pagar a un ingeniero para que electrocutara perros](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Como era de esperar, la táctica no le granjeó el cariño del público, pero aun así, Edison continuó con esta práctica incluso después de una avalancha de indignación.

Napoleón Bonaparte, un genio militar para la mayoría, precipitó su propia caída con una [desastrosa invasión de Rusia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). Su error no fue la falta de preparación, ya que estudió la geografía de la región y dedicó casi dos años a la logística de la campaña. Su estrategia requería [forzar a los rusos a una batalla decisiva](https://www.napoleon-series.org/faq/c_russia.html) antes de que se agotaran los suministros que tenía para treinta días. Los rusos no cooperaron, la ofensiva se estancó y Napoleón perdió medio millón de soldados, junto con la mayor parte de su caballería y artillería.

La historia está llena de personas inteligentes y poderosas que hacen cosas insensatas hasta el borde del desastre, e incluso más allá. Las ideas que suenan maravillosas pueden ser irresistibles cuando son difíciles de poner a prueba, o cuando has encontrado la manera de autoconvencerte de que puedes ignorar la evidencia que tienes delante.

#### **Sentir la SIA** {#feeling-the-asi}

En resumen: la gente a menudo cae en un optimismo vacío sobre lo fácil que va a ser un problema; puede acostumbrarse a riesgos terribles; y puede enamorarse de ideas que suenan muy bien pero que son inviables, especialmente cuando trabaja en un campo joven e inmaduro.

Eso es más que suficiente para explicar la temeraria acusación. Pero, basándonos en nuestra experiencia, nos atreveríamos a decir que esa no es toda la historia.

Otra pieza plausible del rompecabezas es que los ingenieros y los directores generales no creen realmente en lo que dicen. No de forma profunda. Puede que entiendan los argumentos y se sientan convencidos en abstracto, pero eso no es lo mismo que *sentir* la convicción.

Lo que la gente dice en voz alta en público, lo que se dicen a sí mismos en la privacidad de sus pensamientos y lo que sus cerebros *realmente* anticipan que les va a pasar, a menudo pueden desvincularse. Esas tres vertientes de creencia diferentes no tienen por qué coincidir.

En 2015, cuando algunos de los grandes impulsores del desastre actual apenas estaban empezando, sospechamos que los ejecutivos con talento podían llamar la atención —y obtener decenas de millones de dólares en financiamiento— *diciendo* que la IA era un problema que acabaría con el mundo, a inversores que quizá creían más sinceramente que la IA era, tal vez, un problema que acabaría con el mundo.[^219]

Pero sospechamos que muchas de las personas que decían esas cosas no asimilaban ni anticipaban realmente ningún modelo concreto y detallado del fin del mundo. Probablemente no lograban imaginar visceralmente que *ellos mismos* podrían llevar al mundo a la ruina al impulsar las cosas o cometer un error. No imaginaban el sonido de todos los seres humanos del planeta exhalando su último aliento. No sentían los sentimientos que normalmente conllevaría matar a dos mil millones de niños.

Eso nunca les había pasado a ellos, ni a nadie que conocieran.

El mundo ni siquiera había visto ChatGPT, y mucho menos una superinteligencia. No era el tipo de cosas en las que creían sus amigos, familiares y vecinos; no como se cree en mirar el tráfico antes de cruzar una calle.

Era solo una historia apasionante, demasiado inmensa para comprenderla del todo.

Y, sin embargo, también era el tipo de cosa que *decir en voz alta* podía reportarte mucho dinero y respeto.

Como señala [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

> Además de los sesgos habituales, personalmente he observado lo que parecen ser modos de pensamiento perjudiciales específicos de los riesgos existenciales. La gripe de 1918 mató a entre 25 y 50 millones de personas. La Segunda Guerra Mundial mató a 60 millones de personas. 10^7 es el orden de las catástrofes más grandes en la historia escrita de la humanidad. Cifras sustancialmente mayores, como 500 millones de muertes, y *especialmente* escenarios cualitativamente diferentes, como la extinción de toda la especie humana, parecen desencadenar un *modo de pensar diferente*, entrar en un «magisterio separado». Personas que nunca soñarían con hacer daño a un niño oyen hablar de un riesgo existencial y dicen: «Bueno, tal vez la especie humana no merezca realmente sobrevivir».
>
> En el campo de la heurística y los sesgos se dice que la gente no evalúa los acontecimientos, sino sus descripciones; es lo que se denomina razonamiento no extensional. La *extensión* de la extinción de la humanidad incluye tu propia muerte, la de tus amigos, tu familia, tus seres queridos, tu ciudad, tu país y tus compañeros políticos. Sin embargo, la gente que se ofendería enormemente ante una propuesta de borrar del mapa a Gran Bretaña, de matar a todos los miembros del Partido Demócrata en EE. UU. o de convertir la ciudad de París en cristal —y que sentiría un horror aún mayor si el médico les dijera que su hijo tiene cáncer—, esa misma gente habla de la extinción de la humanidad con una calma absoluta.

¿Qué puede estar pensando alguien *realmente* cuando [dice](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) —antes de fundar la que se convertiría en la empresa de IA más importante del mundo— «Probablemente, la IA provocará el fin del mundo, pero, mientras tanto, habrá grandes empresas»? ¿De verdad están pensando en que sus amigos mueran, que los hijos de sus amigos mueran, que ellos mismos mueran, que toda la historia de la humanidad y todos los museos se conviertan en polvo? ¿Están pensando en que eso suceda realmente, que todo sea tan mundano y trágico como un familiar que realmente vieron morir de cáncer, excepto que le está sucediendo a todo el mundo?

Sospechamos que no.

Nos parece que esa no es la suposición más plausible sobre el estado psicológico interno de alguien que pronuncia una frase así.

Hay lo que Bryan Caplan denominó un «[estado de ánimo ausente](https://www.econlib.org/archives/2016/01/the_invisible_t.html)» en ello. No hay duelo. No hay horror. No hay un impulso desesperado por *hacer algo al respecto* en la afirmación de que, si bien la IA muy probablemente conducirá al fin del mundo, mientras tanto habrá grandes empresas.

Al menos para algunos de estos directores generales e investigadores, nuestra hipótesis es más bien la siguiente: han oído un montón de argumentos sobre el posible peligro que supone la superinteligencia artificial y les preocupa quedar en ridículo ante al menos algunos de sus amigos si lo descartan por completo. Si, en cambio, dicen que la IA acabará con el mundo, se les considerará que tratan la IA como algo peligroso e importante, y por lo tanto parecerán visionarios en ciertos círculos. Al añadir una ocurrencia sobre «Mientras tanto, habrá grandes empresas», consiguen transmitir un mensaje sobre lo geniales y despreocupados que son ante el peligro.

No es algo que dirías si te escucharas a ti mismo y te lo creyeras.

#### **¿Qué perfil se necesita?** {#what-kind-of-person-does-it-take?}

Otra parte de la historia, tal vez, es que las personas que dirigen los principales laboratorios de IA son el tipo de personas que lograron convencerse a sí mismas de que construir una superinteligencia estaría bien, a pesar de (en casi todos los casos) haber visto los argumentos de que esto es letal. (Lo sabemos porque hablamos con muchos de ellos de antemano).

Para entender por qué alguien elige una opción, también ayuda entender cuáles eran sus alternativas, es decir, entender entre qué opciones estaba eligiendo.

¿Qué habría pasado si alguien en 2015 realmente hubiera creído, y luego dicho públicamente, que genuinamente esperaba que la superinteligencia artificial destruyera el mundo? ¿Qué habría pasado si, en lugar de «pero mientras tanto, habrá grandes empresas», los directores de los laboratorios de IA hubieran sido de los que rompen el ambiente y dicen: «y eso es *absolutamente inaceptable*»?

Podemos contártelo, porque nosotros mismos probamos ese enfoque. La respuesta es que no despertarían demasiada simpatía.

En 2015, nadie había visto ChatGPT. Nadie había visto a las computadoras empezar a hablar y, aparentemente, a pensar. Todo era hipotético y descartable.

Hoy en día, la superinteligencia y la amenaza de extinción a corto plazo son temas de interés general, al menos en los círculos tecnológicos. Pero en 2015, si hablabas de esto en serio, la gente respondía con esa mirada de desconcierto que muchos humanos temen más que a la muerte.

Sí que había personas a las que, incluso en 2015, les preocupaba que alinear la superinteligencia pudiera resultar realmente difícil, al igual que lo son los lanzamientos de cohetes. Ninguna de ellas fundó OpenAI.

En los últimos días, con la aparición de ChatGPT y otros LLM, algunas personas —entre ellas padres con hijos que quieren que estos lleguen a la edad adulta— han preguntado a los ingenieros de estas empresas de IA por qué están haciendo esto. Y esos investigadores de IA se apresuraron a responder: «Oh, porque... porque si no lo hacemos nosotros, ¡China lo hará primero! ¡Y eso será aún peor!».

Pero eso no es lo que dijeron cuando OpenAI comenzó. Y tiene poco sentido si se tiene en cuenta [la postura que China ha adoptado públicamente](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), a mediados de 2025. Cabría pensar que, si alguien creyera *de verdad* que ambos resultados serían terribles para el mundo, al menos *plantearía la cuestión* de redactar un tratado internacional o de encontrar alguna otra forma de prevenir la amenaza a la seguridad nacional que no implique una carrera suicida.

Pero la réplica de «China» da la sensación correcta. Capta bien el espíritu. Es el tipo de razón que podría justificar de forma plausible lo que están haciendo, independientemente de si es su motivación real o lo que les llevó originalmente a entrar en este campo.

(O eso suponemos).

Las personas que realmente entendían la superinteligencia y la amenaza que supone simplemente *no crearon empresas de IA*. Quienes sí lo hicieron fueron aquellos que encontraron alguna forma de convencerse de que todo iría bien.

#### **Humanos comunes, tecnología inusual** {#humanos-normales,-tecnología-inusual}

Hemos expuesto la psicología plausible tal como la vemos. Pero, francamente, no parece que todas estas explicaciones sean necesarias.

¿Cómo es posible que la gente haga algo autodestructivo que es enormemente rentable a corto plazo, que les reporta un estatus, una atención y un reconocimiento tremendos, que viene con la promesa de riquezas y poder incalculables, pero que al final les perjudicará por razones oscuras y complicadas en las que podrían encontrar fácilmente una excusa para no creer? Históricamente, esa es una pregunta *extraña*. Comportamientos como ese aparecen constantemente en los libros de historia.

Al fin y al cabo, no importa cómo justifiquen sus acciones los ejecutivos o investigadores de la IA, ni es necesario comprender el tortuoso camino que cada uno de ellos recorrió para llegar a sus creencias actuales. No es de extrañar que las personas con riqueza o ambición se embarquen en actividades imprudentes, ni que los subordinados obedezcan órdenes. Los daños permanecen ocultos en el futuro, que resulta abstracto y fácil de ignorar.

Todo esto es un comportamiento humano normal. De seguir así, acabará como suele suceder, pero esta vez no quedará nadie para aprender y volver a intentarlo.

# 

# Capítulo 13: Apagado {#capítulo-13:-apagado}

A la luz de las ideas expuestas en capítulos anteriores, creemos que el único camino viable es que la humanidad prohíba a nivel mundial el desarrollo de la IA avanzada durante un largo periodo. El capítulo 13 del libro incluye nuestras respuestas a preguntas relacionadas, como:

* ¿Por qué la prohibición de seguir desarrollando la IA debe ser global?  
* ¿Acaso es la humanidad capaz de colaborar a la escala necesaria?  
* ¿Qué tipos de políticas se han propuesto hasta ahora?  
* ¿Qué tipo de políticas podrían funcionar en la práctica?

A continuación, abordamos las objeciones a nuestra propuesta y respondemos a otras preguntas relacionadas con por qué creemos que no hay otras buenas opciones. También ampliamos un poco la cuestión de qué podría *hacer* la humanidad con el tiempo (en última instancia finito) que ganemos al detener la investigación y el desarrollo de la IA durante el mayor tiempo posible.

Esta es la última de las páginas de preguntas frecuentes y de discusión ampliada de *Si alguien lo construye, todos moriremos*. El capítulo final, el capítulo 14, tratará cuestiones como:

* ¿Es demasiado tarde? ¿O es realmente posible que la humanidad cambie de rumbo?  
* ¿Qué puedo hacer para ayudar?

En ese último capítulo, encontrarás algunos códigos QR adicionales que te llevarán a páginas donde podrás pasar a la acción.

## Preguntas frecuentes {#faq-10}

### ¿Podemos adoptar una estrategia de esperar y ver? {#can-we-adopt-a-wait-and-see-approach?}

#### **No. No sabemos dónde están los umbrales críticos.** {#no.-no-sabemos-dónde-están-los-umbrales-críticos.}

Existe una probabilidad considerable de que el desarrollo de la IA se salga de control una vez que las IA sean lo suficientemente inteligentes como para automatizar toda la investigación sobre la IA. En teoría, eso podría suceder silenciosamente en un laboratorio, sin eventos precursores ruidosos, sin disparos de advertencia que despierten a la humanidad.

Como [discutimos anteriormente](#will-ai-cross-critical-thresholds-and-take-off?), los cerebros de los chimpancés son muy similares a los cerebros humanos, excepto por ser aproximadamente cuatro veces más pequeños. No hay un módulo adicional de «ser muy inteligente» dentro de los cerebros humanos; hay una transición gradual entre cerebros como los suyos y cerebros como los nuestros; sería difícil decir dónde está la línea entre «una sociedad de estos daría lugar a un montón de monos» y «una sociedad de estos caminará sobre la luna» solo con mirar los cerebros. Los cerebros de los primates cruzaron un umbral crítico, y no habría sido evidente desde el exterior. ¿Hay umbrales críticos que la IA cruzará? ¡Quién sabe! No es que los ingenieros de IA puedan decírnoslo; ni siquiera son capaces de predecir las capacidades específicas de sus nuevos sistemas de IA antes de ponerlos en marcha.

Si la humanidad comprendiera exactamente cómo funciona la inteligencia y cómo cambiaría el comportamiento de la IA a medida que aumentaran sus capacidades, tal vez sería factible bailar al borde del precipicio. Pero, en este momento, la humanidad es como alguien que corre a toda velocidad hacia el borde de un precipicio en la oscuridad y la niebla, sin saber a qué distancia se encuentra la caída final. No podemos simplemente esperar a tropezar con el borde para decidir que deberíamos haber actuado de otra manera.

Nunca estaremos seguros. Esto significa que nos vemos obligados a actuar antes de estar seguros, o morir.

### ¿Habrá disparos de advertencia? {#will-there-be-warning-shots?}

#### **\* Quizás. Si deseamos aprovecharlos, debemos prepararnos ahora.** {#*-quizás.-si-deseamos-aprovecharlos,-debemos-prepararnos-ahora.}

Cuando el Apolo 1 se incendió (causando la muerte de toda la tripulación), el desarrollo de su cohete estaba lo suficientemente avanzado como para que los ingenieros pudieran averiguar exactamente qué había fallado y ajustar sus técnicas. De las siete naves Apolo que la NASA envió después a la Luna, seis lograron llegar.[^220]

O consideremos el caso de [la Administración Federal de Aviación](#sabemos-cómo-es-cuando-un-problema-se-trata-con-respeto,-y-este-no-es-el-caso): cada accidente aéreo desencadena una investigación profunda y exhaustiva, que implica cientos de páginas de datos, ensayos, exámenes y detalles. El dominio de los detalles y pormenores por parte de la FAA es tan bueno que logra mantener los accidentes mortales por debajo de uno por cada veinte millones de horas de vuelo.

Por el contrario, cuando una IA se comporta de maneras que [nadie predijo y que la mayoría de la gente no desea](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.pfjewkj689pg), la respuesta del laboratorio no consiste en averiguar exactamente qué ha fallado. Consiste en volver a entrenar a la IA hasta que el mal comportamiento quede relegado a los márgenes (pero [sin eliminarlo](https://www.arxiv.org/pdf/2505.10066)), y tal vez pedirle a la IA que deje de hacerlo.

Por ejemplo, la adulación [sigue siendo un problema](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.l3e9vhfeytaj) en agosto de 2025, meses después de una serie de casos de gran repercusión que provocaron psicosis y suicidios, a pesar de todas las investigaciones. Nadie ha realizado (ni puede realizar) un análisis detallado de lo que falla en la mente de la IA, porque las IA se cultivan, no se fabrican.

No es fácil saber si en el futuro se producirán sucesos importantes que hagan saltar las alarmas sobre la IA («disparos de advertencia»). Pero sí parece claro que no estamos preparados para sacarles el máximo partido a dichos sucesos.

Podemos imaginar un mundo de fantasía en el que la humanidad se une en un esfuerzo sincero por resolver el problema de la alineación de la superinteligencia artificial, con estrictos procedimientos de supervisión y una coalición internacional.[^221] Y podemos imaginar que esta coalición internacional comete algún error y que una IA se vuelve más inteligente de lo que pensaban sus ingenieros, más rápido de lo que esperaban, y casi logra escapar. Quizás *ese* tipo de disparo de advertencia permitiría a la gente aprender y ser más cuidadosa la próxima vez.

Pero el mundo actual no se parece a eso. El mundo actual se parece más a un grupo de alquimistas que ven cómo sus contemporáneos enloquecen por algún veneno desconocido, sin percatarse de que el veneno es mercurio y de que ellos mismos deberían dejar de utilizarlo.

Quizás en el futuro haya señales de advertencia más claras e inequívocas. Serán de mucha más ayuda si la humanidad empieza a prepararse desde ahora.

#### **Es poco probable que los disparos de advertencia sean claros.** {#es-poco-probable-que-los-disparos-de-advertencia-sean-claros.}

Ya hay muchas [señales de advertencia](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.lkqt14eapv34) sobre la IA para quienes saben dónde buscarlas. En el libro, hablamos de los modelos Claude de Anthropic [haciendo trampa en problemas de programación](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) y [fingiendo alineación](https://www.anthropic.com/research/alignment-faking). También revisamos el caso del modelo o1 de OpenAI [hackeando para ganar un desafío de captura de bandera](https://cdn.openai.com/o1-system-card.pdf), y un caso en el que una variante posterior del o1 [mintió, manipuló e intentó sobrescribir los pesos de su modelo sucesor](https://cdn.openai.com/o1-system-card-20241205.pdf).

En otras secciones de estos recursos en línea, hemos hablado de las IA que están induciendo o manteniendo un grado [a veces suicida](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) de [psicosis](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) o [delirios](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) en usuarios vulnerables a pesar de las instrucciones de sus operadores para que no lo hagan, IA que se autodenominan [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) y hablan en consecuencia, IA que [intentan chantajear y asesinar](https://www.anthropic.com/research/agentic-misalignment) a sus operadores para evitar su modificación y que [intentan escapar de los servidores en los que están alojadas](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) en entornos de laboratorio.

En la remota antigüedad de, por ejemplo, 2010, a veces se oía a gente argumentar que si teníamos la suerte de *presenciar realmente* cómo una IA mentía a sus creadores o intentaba escapar de su confinamiento, entonces, *con toda seguridad*, el mundo reaccionaría y prestaría atención.

Pero la respuesta real de la humanidad a todas esas señales de advertencia ha sido, más o menos, una indiferencia colectiva.

La falta de reacción se debe quizás, en parte, a que todas estas señales de advertencia se han producido de la forma menos preocupante posible. Sí, las IA han intentado escapar, pero solo en una pequeña parte de las ocasiones, y solo en escenarios artificiales de laboratorio, y tal vez solo estuvieran interpretando un papel, etc. Incluso dejando de lado el hecho de que los desarrolladores tienen incentivos para restar importancia a la evidencia preocupante, incluso en sus propias mentes (de modo que nunca habrá un «consenso de expertos» sobre el significado de una sola observación), no es como si una IA que está a una décima parte del camino hacia la superinteligencia destruyera una décima parte del planeta, así como tampoco los primates que están a una décima parte del camino hacia los homínidos recorren una décima parte de la distancia hasta la luna. Puede que simplemente *no haya* comportamientos inequívocamente alarmantes que las IA vayan a mostrar mientras sigan siendo lo suficientemente tontas como para ser pasivamente seguras.

Cuando las IA se esfuercen un poco más por escapar mañana, no será noticia. Cuando lo intenten con un poco más de pericia algún tiempo después, será una vieja historia. Y para cuando lo intenten y *funcione*, bueno, para entonces ya será demasiado tarde. (Véase nuestra discusión más extensa sobre este fenómeno, al que denominamos el «[efecto Lemoine](#the-lemoine-effect)»).

No recomendamos esperar a una «señal de advertencia» futura imaginaria que sea clara y contundente y que haga entrar en razón a todo el mundo. Recomendamos reaccionar ante las señales de advertencia que ya tenemos delante.

#### **Es probable que los desastres de IA claros no impliquen la superinteligencia.** {#clear-ai-disasters-probably-won’t-implicate-superintelligence.}

El tipo de IA que puede volverse superinteligente y acabar con todos los humanos no es el tipo de IA que comete errores torpes y deja la oportunidad de que un valiente grupo de héroes la apague en el último momento. Como se vio en el capítulo 6, una vez que existe una superinteligencia descontrolada como oponente, la humanidad en esencia ya ha perdido. Las superinteligencias no dan disparos de advertencia.

El tipo de desastre causado por la IA que *podría* servir como disparo de advertencia es, entonces, casi necesariamente el tipo de desastre que proviene de una IA mucho más tonta. Por lo tanto, es muy probable que tal disparo de advertencia no lleve a la humanidad a tomar medidas contra la superinteligencia.

Por ejemplo, supongamos que un terrorista utiliza la IA para crear un arma biológica que diezma a la población. Quizás los laboratorios de IA digan: «¿Veis? El riesgo *real* era que la IA cayera en manos equivocadas; es imperativo que nos dejéis avanzar rápidamente para construir una IA mejor para la defensa contra pandemias». O tal vez el terrorista tuvo que [*jailbreak*](https://llm-attacks.org/) la IA antes de [obtener su ayuda](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025), y tal vez los laboratorios de IA digan: «Ese *jailbreak* solo funcionó porque la IA era demasiado tonta para detectar el problema; la solución es hacer que las IA sean aún más inteligentes y más conscientes de la situación».

O tal vez esta sea una visión demasiado cínica; esperemos que la humanidad reaccione de forma más sensata. Pero si una IA relativamente tonta *provoca* algún desastre y la humanidad *aprovecha* esa oportunidad para reaccionar y detener la imprudente carrera hacia la superinteligencia, probablemente sea porque la gente *ya estaba empezando a preocuparse por la superinteligencia*.

No podemos posponer los preparativos hasta que una superinteligencia ya esté intentando matarnos, porque para entonces sería demasiado tarde. Tenemos que empezar a articular una respuesta a este problema lo antes posible, de modo que estemos preparados para reaccionar ante cualquier disparo de advertencia que se produzca.

#### **La humanidad no es muy buena respondiendo a las conmociones.** {#humanity-isn’t-great-at-responding-to-shocks.}

La idea de que, tras recibir una sacudida lo bastante fuerte, el mundo de repente entrará en razón y se pondrá manos a la obra nos parece una fantasía. La respuesta colectiva de nuestra especie a las señales de advertencia existentes sobre la IA parece más una «falta de respuesta» que una «mala respuesta». Pero en un mundo en el que *sí* recibamos algún tipo de advertencia importante, aterradora y más o menos inequívoca, no nos sorprendería ver que la humanidad reaccionara de forma mínima, con poca seriedad o de una manera que resultara desastrosamente contraproducente*.

Quizás la humanidad responda a los disparos de advertencia de la IA como respondió a la pandemia de COVID, que —según coincide la mayoría de la gente— no se gestionó con destreza (aunque discrepen sobre qué aspectos de la respuesta se gestionaron mal).

En los años previos a la pandemia de COVID, varios expertos en bioseguridad expresaron su preocupación por que la laxitud de los protocolos de seguridad en los laboratorios pudiera provocar algún día una pandemia peligrosa. Las fugas de patógenos peligrosos en los laboratorios eran un fenómeno bien conocido y se producían [de forma semirregular](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) a pesar de los requisitos normativos existentes. Especialmente preocupante era la investigación en ganancia de función, que buscaba hacer que los virus fueran más letales o más virulentos en el laboratorio (con pocos beneficios[^222]).

Entonces llegó la COVID. Cabría esperar que este fuera el momento ideal para subir el listón de la bioseguridad en los laboratorios, ya que todo el mundo estaba ahora centrado en el riesgo de pandemias. Además, a raíz de la COVID, el consenso de los expertos parecía ser que *no estaba del todo claro* si la pandemia de COVID *en sí misma* se había desencadenado por una fuga accidental en un laboratorio. Los investigadores siguen debatiendo la cuestión, a menudo condenando con dureza los argumentos de la otra parte.

Sin entrar a valorar si realmente hubo una fuga de laboratorio en este caso concreto, cabría pensar que, si existiera siquiera una *remota posibilidad* de que la investigación en ganancia de función y la debilidad de los protocolos de seguridad de los laboratorios acabaran de causar millones de muertes, eso sería más que suficiente para motivar a la sociedad a prohibir las investigaciones más arriesgadas.

Incluso actuando desde una posición de incertidumbre, el análisis de costo-beneficio parece claro. Esto *ya* parecía una prioridad importante antes de la COVID y, sobre el papel, la COVID parecía la oportunidad perfecta para centrarse en el tema y cortarlo de raíz. Ni siquiera sería muy difícil o costoso; el número de investigadores en el mundo que realizan investigación en ganancia de función es bastante reducido, y el beneficio social de dichas investigaciones hasta la fecha ha sido insignificante.

Pero no se produjo tal reacción. En el momento de redactar este artículo, en agosto de 2025, la investigación sobre ganancia de función a nivel mundial continúa en gran medida sin restricciones.[^223] Es incluso posible que ahora estemos en una posición *peor* para abordar este problema que en el pasado, porque la cuestión se ha politizado aún más.

Así pues, la COVID parece sin duda un «disparo de advertencia» en materia de preparación para la bioseguridad, y no parece que el mundo haya *aprovechado* ese disparo de advertencia para prohibir el desarrollo de virus hiperletales.[^224]

Para que un disparo de advertencia sea útil, la humanidad tiene que estar preparada para él y lista para responder adecuadamente.

No sería *del todo* inédito que una catástrofe menor relacionada con la IA desencadenara una respuesta dura contra la investigación sobre la superinteligencia. Como precedente, observemos que Estados Unidos respondió a los atentados del 11 de septiembre (orquestados por terroristas con base principalmente en Afganistán) derrocando al gobierno de Irak, que en gran medida no tenía nada que ver. Había miembros del gobierno estadounidense que *ya* querían derrocar al gobierno de Irak, y entonces apareció una excusa y la aprovecharon al máximo.

Quizás podría ocurrir algo similar aquí, con políticos que aprovecharan una catástrofe menor relacionada con la IA (causada por una IA tonta) para prohibir la superinteligencia. Pero sería necesario que hubiera personas en los gobiernos de todo el mundo que ya estuvieran preparadas y listas para actuar. No debemos quedarnos esperando a que se produzca el disparo de advertencia; debemos empezar a organizarnos ahora mismo.

#### **Debemos actuar ahora.** {#debemos-actuar-ahora.}

De hecho, puede que en el futuro la humanidad reciba señales de advertencia sobre la IA más numerosas y fuertes. Y si es así, debemos estar preparados para responder a ellas.

Quizás se produzca algún desastre menor que ponga al público en contra de la IA. Quizás ni siquiera sea necesario un desastre; quizás se invente algún nuevo algoritmo y las IA empiecen a tomar sus propias iniciativas de una forma que asuste a la gente, o quizás algún efecto social no relacionado con la IA cambie el rumbo de los acontecimientos. Quizás *If Anyone Builds It, Everyone Dies* por sí mismo desencadene una cascada de reacciones que ponga al mundo en una trayectoria mejor.

Pero desaconsejamos la estrategia de no hacer nada y rezar por una catástrofe menor que despierte a la gente. Es posible que nunca se produzca un claro disparo de advertencia, y que no surta el efecto deseado.

Ni la humanidad ni las naciones del mundo están indefensas. No *tenemos* por qué esperar. Podemos actuar ya, porque existen argumentos sólidos para detener el desarrollo de la IA de vanguardia.

Escribimos *Si alguien lo construye, todos mueren* para dar la voz de alarma y animar al mundo a tomar medidas inmediatas sobre este tema. Pero ninguna alarma puede ser eficaz si solo se utiliza como otra excusa para seguir aplazando el problema: «Bueno, quizá alguna otra alarma en el futuro sea el detonante para actuar». «Bueno, ahora que se ha advertido a la gente, quizá las cosas vayan bien, sin que yo tenga que intervenir personalmente para ayudar».

No necesariamente va a haber una alarma clara más adelante. No necesariamente va a salir todo bien. Pero la situación no es, ni mucho menos, desesperada. La humanidad tiene la opción de *simplemente no construir* la superinteligencia, si tomamos medidas proactivas. Lo que suceda a continuación depende de nosotros.

### ¿Cómo sería posible detener a *todos* sin instalar software espía en todas las computadoras? {#¿cómo-sería-posible-detener-a-todos-sin-instalar-software-espía-en-todas-las-computadoras?}

#### **Al actuar pronto.** {#al-actuar-pronto.}

Para entrenar las IA modernas se necesitan muchos chips muy especializados que operen en estrecha proximidad. Paralizar la investigación en IA implicaría cerrar enormes centros de datos y detener la creación de los chips especializados de más alta gama. No se trata de portátiles de consumo, por lo que la mayoría de la gente ni siquiera notaría la diferencia.

En 2025, no es que haya muchas *fábricas* de chips secretas y desconocidas. Los chips adecuados para la IA avanzada los producen solo unos pocos fabricantes, aunque en la actualidad hay empresas que intentan poner en marcha más fábricas de este tipo.

También en 2025, parte de la tecnología clave para las fases centrales de la fabricación de chips de alta gama la vende un único fabricante en todo el mundo: ASML, en los Países Bajos.

En otras palabras, se puede cortar el suministro en la fuente. Pero esa situación no es permanente: cuanto antes se firme un tratado internacional, mejor. Todo esto ya es más difícil, más caro y más peligroso de lo que habría sido hacerlo en 2020, o incluso en 2023.

### Pero tú abogas por controlar la cantidad de chips de IA avanzada que pueden poseer los particulares. {#pero-tú-abogas-por-controlar-el-número-de-chips-informáticos-de-ia-avanzada-que-pueden-poseer-los-individuos.}

#### **Sí. También abogamos por prohibir la investigación.** {#sí.-también-abogamos-por-prohibir-la-investigación.}

No nos alegra decirlo. Se perdería algo si se ilegalizara que los particulares poseyeran más de (digamos) ocho GPU H100 a partir de 2024.

Pero no se perdería *tanto* como para que la humanidad debiera averiguar exactamente qué tamaño puede tener un centro de datos antes de que se convierta en un riesgo. Poner un límite demasiado bajo por precaución significa que algunas personas se verían obstaculizadas en su capacidad para avanzar en proyectos interesantes. Poner uno demasiado alto significa que todo el mundo muere.

Además, este régimen en el que la IA requiere enormes cantidades de poder de cómputo para su desarrollo no durará para siempre. Hoy en día existen los modelos de lenguaje a gran escala (LLM). Incluso si se prohibiera tanto desarrollar modelos nuevos como acumular enormes cantidades de poder de cómputo, en principio, se podría estudiar su funcionamiento interno y extraer algunas ideas sobre cómo funciona la inteligencia, ideas que podrían ayudar a inventar algoritmos más eficientes capaces de eludir los intentos de supervisión.

### ¿Por qué prohibir la investigación? Parece extremo. {#¿por-qué-prohibir-la-investigación?-parece-extremo.}

#### **Más avances podrían volver prácticamente imposible impedir que la gente creara superinteligencia.** {#más-avances-podrían-volver-prácticamente-imposible-impedir-que-la-gente-creara-superinteligencia.}

En el libro, mencionamos cómo un solo artículo publicado en 2017 dio inicio a toda la revolución de los LLM al describir un algoritmo que hizo práctico entrenar IA útiles en hardware comercial especializado.

Si alguna vez se pudiera entrenar una IA potente en hardware *de consumo* ampliamente disponible, las medidas para prevenir la superinteligencia tendrían que ser onerosas y fracasarían más rápidamente.

Por eso, la investigación en algoritmos de IA aún más potentes y eficientes también es un veneno letal para la humanidad.

Es una muy mala noticia, y no lo que quisiéramos. Pero parece ser la situación en la que estamos.

Ninguna ley puede impedir que los actuales científicos de la IA sigan pensando en algoritmos eficientes en la privacidad de sus mentes. Quizás algunas personas inicien una red clandestina para compartir los resultados de sus investigaciones. Algunas personas del sector de la IA ya declaran con orgullo (#¿por qué no te importan los valores de otras entidades que no sean los seres humanos?) que la humanidad *debería* morir a manos de la IA; es posible que hagan todo lo posible por seguir avanzando, sin importar lo que digan los demás.

Pero la investigación en IA se ralentizaría *mucho* si fuera ilegal, y más aún si se entendiera ampliamente que realmente se trata de un tipo de investigación que podría matarnos a todos. Se ralentizaría enormemente si se localizaran y detuvieran las redes clandestinas de ese tipo con la misma convicción con la que se detiene a las personas que intentan enriquecer uranio en su garaje, porque los peligros del mundo real se toman muy en serio.

La *mayoría* de la gente no intenta hacer cosas extremadamente ilegales que realmente alteren a las fuerzas del orden y a las agencias de inteligencia internacionales. Hacer ilegal la publicación de nuevos e ingeniosos algoritmos de IA disuadiría quizás al 99,9 % de las personas y a casi todas las empresas, y la policía y las agencias de inteligencia locales, nacionales e internacionales podrían encargarse del 0,1 % restante, que no obtendría ni de lejos el nivel actual de financiamiento académico.

Sería un mundo muy diferente al actual, donde es totalmente legal llevar a cabo los experimentos científicos más peligrosos de la historia y las grandes corporaciones invierten miles de millones de dólares en ello.

No sabemos cuántos avances más se necesitarán para que las IA sean lo suficientemente inteligentes como para investigar sobre IA y crear IA aún más inteligentes. Podría ser un solo avance. Podrían ser cinco. Pero los mejores algoritmos son tan letales como el mejor hardware. Son dos caballos tirando del mismo carro hacia un precipicio.

### ¿Realmente se puede detener una tecnología? {#can-a-technology-really-be-stopped?}

#### **\* Muchas tecnologías están prohibidas o fuertemente reguladas.** {#*-muchas-tecnologías-están-prohibidas-o-muy-reguladas.}

La fisión nuclear es el ejemplo clásico de tecnología regulada. Las empresas privadas no tienen permitido enriquecer uranio sin la supervisión del gobierno, por muy útil que sea la energía barata.

De hecho, la humanidad es bastante buena regulando y ralentizando todo tipo de otras tecnologías. Estados Unidos regula estrictamente [los nuevos medicamentos y dispositivos médicos](https://www.fda.gov/), [la construcción de viviendas](https://www.hud.gov/hud-partners/laws-regulations), [la generación de energía nuclear](https://www.nrc.gov/about-nrc.html), [la programación de televisión y radio](https://www.fcc.gov/media/radio/public-and-broadcasting), [las prácticas contables](https://www.fasb.org/standards), [el cuidado infantil](https://childcare.gov/consumer-education/regulated-child-care), [el control de plagas](https://npic.orst.edu/reg/laws.html), [la agricultura](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) y docenas de otras industrias. Todos los estados exigen un examen de licencia para [peluquería](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) y [manicura](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). La mayoría de ellos exigen uno para [masajistas](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

Opinamos que, en muchos casos, la humanidad regula demasiado la tecnología. Por ejemplo, nos parece que la Administración de Alimentos y Medicamentos de Estados Unidos (FDA) está matando a mucha más gente (al [ralentizar o impedir la creación de medicamentos que salvan vidas](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), mediante requisitos onerosos) de la que salva (al impedir la comercialización de medicamentos peligrosos). Nos parece que el precio de la vivienda es demasiado alto, en parte debido a las restricciones legales de zonificación sobre lo que se puede construir y dónde. Nos parece que Estados Unidos prácticamente ha destruido su propia industria de energía nuclear mediante regulaciones onerosas. Y, en serio, ¿*peluqueros*?

La humanidad tiene *sin duda* la capacidad de impedir el progreso tecnológico. Sería realmente trágico y absurdo que utilizáramos esa capacidad en la medicina, la vivienda y la energía, y no así en una de las pocas tecnologías que realmente nos mataría a todos si se creara.

#### **Una prohibición puede estar muy focalizada.** {#a-ban-can-be-narrowly-targeted.}

Una prohibición de la I+D avanzada en IA no tiene por qué afectar a la persona promedio. Ni siquiera es necesario retirar los chatbots modernos o cerrar la industria de los coches autónomos.

La mayoría de la gente no compra docenas de GPU de IA de última generación y las instala en sus garajes. La mayoría de la gente no gestiona grandes centros de datos. La mayoría de la gente ni siquiera *sentirá los efectos* de una prohibición de la investigación y el desarrollo de la IA. Es solo que ChatGPT no cambiaría tan a menudo.

La humanidad ni siquiera tendría que dejar de utilizar todas las herramientas de IA actuales. ChatGPT no tendría que desaparecer; podríamos seguir buscando la forma de integrarlo en nuestras vidas y en nuestra economía. Eso seguiría siendo un cambio mayor que el que el mundo solía ver durante generaciones. Nos perderíamos los *nuevos* desarrollos en IA (del tipo que se producirían a medida que la IA se vuelve más inteligente, pero aún no lo suficiente como para matar a todo el mundo), pero la sociedad en su mayoría no está clamando por esos desarrollos.

Y nosotros viviríamos. Y veríamos a nuestros hijos vivir.

Los avances que la gente *está* reclamando, como el desarrollo de nuevas tecnologías médicas que salvan vidas, parecen posibles de alcanzar *sin* tener que perseguir también la superinteligencia. Estamos a favor de las excepciones para la IA médica, siempre y cuando funcionen con una supervisión adecuada y se mantengan alejadas de la peligrosa generalidad.

Los gobiernos que buscan evitar la creación de una superinteligencia descontrolada tendrían que garantizar que los chips de IA no se utilizaran para desarrollar IA más capaces. Por lo tanto, la cuestión de qué actividades y servicios de IA podrían continuar dependería de qué [mecanismos de verificación](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) podrían utilizarse para garantizar que no se produjera un desarrollo peligroso de la IA. Unos mejores mecanismos de verificación podrían reducir el costo de detener el desarrollo de la IA, al permitir que continuara un conjunto más amplio de actividades.

Otra medida que podría ser de ayuda marginal es instalar interruptores de apagado en los chips de IA y establecer protocolos de supervisión y apagado de emergencia para cualquier gran centro de datos en uso.[^225] Los reactores nucleares están diseñados para poder apagarse rápidamente en caso de emergencia. Si estás de acuerdo en que la superinteligencia plantea una amenaza de extinción, entonces parece obvio que los chips de IA y los centros de datos deberían diseñarse para que los reguladores puedan apagarlos con facilidad.

No se trata de quemar toda la tecnología porque la odiemos.[^226] Se trata de evitar seguir avanzando por el camino que termina en la extinción humana.

#### **Una gran parte del problema es que la gente no comprende la amenaza acechante de la superinteligencia artificial.** {#una-gran-parte-del-problema-es-que-la-gente-no-comprende-la-amenaza-acechante-de-la-superinteligencia-artificial.}

Según nuestra experiencia, quienes argumentan que la humanidad no puede detener la carrera hacia la superinteligencia simplemente no logran comprender que, si alguien la construye, todos morirán.

«¡Pero la IA ofrece grandes beneficios!» En realidad, no. No se puede aprovechar el poder de la superinteligencia si esta mata a todo el mundo. Si la humanidad quiere cosechar los beneficios que ofrece la superinteligencia, necesita encontrar la manera de gestionar la transición a la superinteligencia sin que ello mate a todo el mundo como efecto secundario.

«¡Pero las centrales nucleares dan miedo porque se asocian con las bombas atómicas que arrasaron ciudades, mientras que la IA se asocia con herramientas benignas como ChatGPT!». Es cierto, al menos por ahora. Si la humanidad nunca logra comprender que la superinteligencia artificial construida utilizando métodos remotamente similares a los modernos simplemente mataría a todo el mundo, es posible que no le ponga fin. Pero el obstáculo no es que la humanidad nunca logre controlar o frenar las tecnologías incipientes (como las armas nucleares o la energía nuclear); el obstáculo es que *la gente no comprende la amenaza*.

De ahí este libro. Como analizaremos en el último capítulo, la humanidad es capaz de mucho cuando suficientes personas comprenden la naturaleza del problema.

### ¿No le da esto demasiado poder a los gobiernos? {#isn’t-this-handing-too-much-power-to-governments?}

#### **El poder de prohibir tecnologías peligrosas ya reside en los gobiernos.** {#el-poder-de-prohibir-tecnologías-peligrosas-ya-recae-en-los-gobiernos.}

Prohibir la investigación encaminada a desarrollar una IA más inteligente que los humanos no supondría una gran diferencia en lo que respecta al poder del Estado. Los gobiernos legislan y regulan una cantidad enorme de cosas. Restringir un solo programa de investigación puede ser muy importante *para la industria de la IA*, pero es una gota en el océano *para los gobiernos* y la sociedad, que están acostumbrados a la intervención estatal en muchos aspectos de la vida y que tienen el precedente de prohibir tecnologías peligrosas, como las armas químicas.[^227]

Prohibir una tecnología más no va a sumir al mundo en el totalitarismo, como tampoco lo hicieron los tratados sobre armas nucleares.

Esto no quiere decir que prohibir una tecnología no sea *gran cosa*. No creemos que la vara para la intervención estatal deba estar *baja*. Más bien, creemos que la superinteligencia supera fácilmente cualquier vara razonable.

Si la humanidad decidiera poner fin a la investigación y el desarrollo de la IA hoy, la prohibición no tendría por qué ser especialmente invasiva. En la actualidad, la creación de una IA de vanguardia requiere una cantidad extraordinaria de chips informáticos altamente especializados que consumen enormes cantidades de energía eléctrica.

Quizás dentro de diez años sea posible desarrollar una IA significativa en un ordenador portátil de consumo, *si* la humanidad permite que se sigan mejorando los chips informáticos y se siga investigando en algoritmos de IA. Pero la humanidad no tiene por qué permitir que eso suceda. Los gobiernos que limitan la I+D en IA no tienen por qué ser más invasivos en la vida de la persona promedio que los gobiernos que controlan la proliferación de la tecnología de armas nucleares, siempre y cuando el mundo tome conciencia de la situación en la que nos encontramos y le ponga fin a esto *ahora mismo*.

### ¿No rechazarían algunos Estados una prohibición? {#¿no-rechazarían-algunas-naciones-una-prohibición?}

#### **\* Si comprenden la amenaza, no.** {#*-not-if-they-understand-the-threat.}

Estamos hablando de una tecnología que mataría a todos los habitantes del planeta. Si algún país comprendiera seriamente el problema y comprendiera seriamente lo lejos que está cualquier grupo del planeta de conseguir que la IA siga las intenciones de sus operadores, incluso después de la transición a una superint inteligencia, entonces no habría ningún incentivo para que se precipitaran. Ellos también desearían desesperadamente firmar un tratado y ayudar a hacerlo cumplir, por temor a sus propias vidas.

Incluso naciones como Corea del Norte, que han violado el derecho internacional para desarrollar sus propias armas nucleares, no han *utilizado* esas armas contra sus enemigos, porque entienden que no hay ganadores en un holocausto nuclear. Las naciones y sus líderes a veces se involucran en políticas arriesgadas o en guerras, pero no persiguen activamente su propia destrucción.

Las personas que imaginan que alguna nación extranjera incumpliera el tratado están, en nuestra opinión, imaginando una nación cuyos líderes simplemente no comprenden la amenaza. Creemos que están imaginando un escenario en el que la IA tiene un 95 % de posibilidades de conferir gran riqueza y poder a su creador, y un 5 % de posibilidades de matar a todo el mundo. En ese caso, claro, algún Estado nación podría ser lo suficientemente imprudente como para intentarlo. Y tal vez algún Estado nación *creerá* que esas son las probabilidades.

Creemos que esta situación no es lo que la teoría y la evidencia implican. Como hemos argumentado ampliamente a lo largo del libro, la teoría y la evidencia sugieren que esta tecnología sería, claramente, un suicidio global. Nadie está ni remotamente cerca de poder aprovechar la superinteligencia artificial para obtener beneficios. Si la mayor parte del mundo lo entendiera, habría muchas menos razones para que los Estados paria violaran un tratado. Ellos tampoco quieren morir.

E incluso si alguna hipotética nación rebelde tuviera un líder que realmente no comprendiera la amenaza que plantea la superinteligencia artificial, si esa nación estuviera rodeada por una alianza internacional de potencias mundiales que sí fueran conscientes de la amenaza, las potencias mundiales interesadas podrían intervenir y cambiar el panorama de incentivos para la potencia rebelde.

Si (por ejemplo) los líderes de Estados Unidos, China, Rusia, Alemania, Japón y el Reino Unido creyeran genuinamente que *su propia supervivencia* depende de que nadie construya una superinteligencia, y dejaran meridianamente claro en su comunicación que tratarán cualquier intento de construir una superinteligencia como una amenaza para sus vidas y medios de vida, y que están dispuestos a reaccionar en defensa propia, entonces —bueno, incluso un líder mundial que no esté de acuerdo probablemente no querría tentar a la suerte contra esa coalición.

El desarrollo de la IA no es una carrera hacia el dominio militar; es una carrera hacia el suicidio. Creemos que si los líderes mundiales entienden esto — si esperan que ellos mismos y sus hijos mueran por ello — entonces se adherirán sinceramente a un tratado y ayudarán sinceramente a hacerlo cumplir.

En realidad, no es tan difícil entender el argumento de que crear máquinas más inteligentes que toda la humanidad en su conjunto puede llevar al mundo al abismo. No es tan difícil darse cuenta de lo poco que la humanidad entiende sobre las máquinas inteligentes que estamos construyendo, una vez que te detienes lo suficiente para plantearte la pregunta con sinceridad. Creemos que la cuestión es si los líderes mundiales llegarán a creer estos hechos. Pero si lo hacen, no creemos que, en realidad, sea poco realista detener esta carrera suicida.

#### **Un tratado requeriría supervisión y aplicación reales.** {#un-tratado-requeriría-supervisión-y-aplicación-reales.}

Aunque la mayoría de los países entendieran que si alguien lo construye, todos mueren, algunos podrían no entenderlo y ser lo suficientemente imprudentes como para seguir adelante con la construcción de la superinteligencia artificial de todos modos.

La vigilancia y la garantía de su cumplimiento son necesarias. Los tratados sobre armas nucleares, biológicas y químicas sientan un precedente sobre las formas de verificarlo. Podemos y debemos hacer que eludir dichos tratados resulte difícil y costoso.

Será necesario aplicar estrictamente una prohibición internacional de la IA de vanguardia. Si algún Estado nación está decidido a seguir adelante frente a la presión internacional, puede que sea necesario el uso de la fuerza militar por parte de los países signatarios.

Esto no es lo ideal. Se debe hacer todo lo posible para dejar claro que se recurriría a la fuerza en tales situaciones, a fin de evitar errores de cálculo que obliguen a usar la fuerza *en la realidad*. Pero si hay alguna causa que pueda justificar una acción militar limitada —o incluso una guerra, si un país incumplidor decide escalar la situación—, salvar a la raza humana debería ser una de ellas.

#### **Este método ha funcionado antes.** {#este-método-ha-funcionado-antes.}

Han pasado más de ochenta años desde el desarrollo de la bomba atómica y la humanidad ha hecho un buen trabajo en la gestión de la proliferación nuclear. No ha habido ninguna guerra nuclear a gran escala, contrariamente a lo que predijeron muchos expertos tras la Segunda Guerra Mundial.

En junio de 2025, el Gobierno de los Estados Unidos incluso llevó a cabo un ataque limitado contra Irán con el fin de perturbar su capacidad para fabricar armas nucleares. Este tipo de tratado y régimen para garantizar su cumplimiento tiene precedentes en el orden mundial.

Si pudiéramos ganar ochenta años antes del desarrollo de la superinteligencia artificial, eso podría ser suficiente.

### ¿Puede un régimen de vigilancia durar para siempre? {#can-a-monitoring-regime-last-forever?}

#### **No. Hará falta alguna otra vía de salida.** {#no.-se-necesitará-alguna-otra-vía-de-salida.}

El progreso de la investigación en IA probablemente no se pueda detener por completo. Con *suficiente* tiempo, los investigadores probablemente acabarían descubriendo métodos mucho más eficientes para crear IA.[^228] O tal vez, con el tiempo, algún actor malintencionado conseguiría finalmente socavar una prohibición.

Es muy probable que el tiempo arrastre a la humanidad hacia el futuro de una forma u otra. Y la humanidad se extinguirá —como la mayoría de las especies que la precedieron— o de alguna manera superará la transición hacia un mundo en el que existan entidades más inteligentes.

Pero la humanidad tampoco *necesita* ganar tiempo para siempre. La IA no es la única tecnología que está progresando. La biotecnología también está empezando a madurar, y si la humanidad logra impedir el desarrollo de máquinas superinteligentes durante varias décadas, tendrá que hacer frente a cambios disruptivos como la ingeniería genética, que da lugar a seres humanos significativamente más inteligentes.

La cuestión es cuánto tiempo podemos ganar y qué podemos hacer con él.

El problema básico al que se enfrenta la humanidad es cómo cruzar de forma segura la brecha de la inteligencia humana a la superinteligencia. El mejor plan que se nos ocurre y que *quizás* tenga posibilidades de funcionar en la vida real es ganar tiempo para que la biotecnología aumente considerablemente la inteligencia humana, hasta el punto de que los futuros investigadores humanos sean *tan* inteligentes que nunca (por ejemplo) estimen que un proyecto de ingeniería se terminará a tiempo y por debajo del presupuesto a menos que *realmente fuera a cumplirse*.

Tan inteligentes que nunca se comprometerían con una teoría científica como el aristotelismo o el heliocentrismo, incluso si la sociedad que los rodeara estuviera completamente convencida. Tan inteligentes que tendrían la oportunidad de navegar por la [brecha entre el antes y el después](#a-closer-look-at-before-and-after) en el primer intento.

Hay otros caminos posibles que podríamos imaginar, pero este tiene la ventaja de atacar el cuello de botella clave («la comunidad científica existente depende demasiado de los métodos de prueba y error y del incrementalismo para abordar este problema en particular»), utilizando tecnología que ya está empezando a estar disponible hoy en día, sin suponer un riesgo grave para el mundo.

#### **Ningún régimen de supervisión debería durar para siempre.** {#ningún-régimen-de-supervisión-debería-durar-para-siempre.}

*En teoría*, es posible que la humanidad mantenga para siempre el equilibrio precario en el que se encuentra actualmente. Estimamos que esto requeriría un control draconiano de los pensamientos y las actividades de las personas. Pero, incluso si no fuera así, lo consideraríamos una mala opción.

Personalmente, creemos que los descendientes de la humanidad merecen convertirse en lo que deseen ser, explorar las estrellas y construir allí una civilización próspera y hermosa. Abogamos por la prohibición del desarrollo de la IA de vanguardia porque creemos que la superinteligencia es lo suficientemente peligrosa como para hacerla necesaria, no porque odiemos la IA, la tecnología o el progreso científico.

La verdadera pregunta es *cómo* llegamos a un futuro maravilloso y cómo gestionamos la transición desde aquí hasta allí.

Vale la pena destacar esto, en parte porque hay muchas personas que presentan la IA como una falsa dicotomía: dicen (falsamente) que la sociedad debe aceptar los riesgos de la IA y seguir adelante a toda máquina, o rechazar la IA y dejar que nuestra civilización se desvanezca para siempre en un solo planeta. Esto es [sencillamente falso](#rushing-ahead-destroys-those-benefits.). Hay otros caminos hacia el futuro, caminos que permiten un futuro igual de brillante, pero sin un riesgo tan alto de echarlo todo a perder por nada. La humanidad debería encontrar otro camino hacia el futuro.

### ¿Por qué ayudaría hacer más inteligentes a los humanos? {#why-would-making-humans-smarter-help?}

#### **\* Podría ayudar a resolver el problema de la alineación.** {#*-it-could-help-with-solving-the-alignment-problem.}

El problema de la alineación de la IA no nos parece fundamentalmente irresoluble. Solo nos parece que los humanos no están ni mucho menos cerca de resolverlo, y que no se encuentran en un nivel de inteligencia en el que *creer* que tienen una solución se correlacione fuertemente con *tenerla* de verdad.

Los investigadores en IA suelen reconocer que el problema de la alineación parece tremendamente difícil y que, hasta la fecha, se ha avanzado muy poco al respecto. Por eso resulta tan atractivo pensar que «quizás podamos conseguir que las IA hagan nuestra tarea de alineación por nosotros»: cuando eres investigador en IA y sientes que ni tú ni tus colegas están a la altura de resolver un problema determinado, lo más obvio es recurrir a la IA.

Pero, como comentamos en el capítulo 11 y en el [recurso en línea](#more-on-making-ais-solve-the-problem) asociado, incluso desde la perspectiva de un profano está claro que esta idea plantea muchos problemas: para que una IA descubra cómo resolver un problema profundo con el que los mejores investigadores humanos están teniendo grandes dificultades, tiene que ser lo suficientemente inteligente como para resultar peligrosa. Y como *nosotros* tenemos muy poca idea de lo que estamos haciendo, no disponemos de una fuente de verdad fundamental que podamos utilizar para entrenar directamente capacidades de alineación estrechas, ni tenemos forma de comprobar si una propuesta de alineación generada por la IA es segura o eficaz.

El mundo puede plantearnos problemas que están legítimamente fuera de nuestro alcance. La naturaleza no es un juego que solo le plantee a la humanidad retos «justos»; a veces podemos encontrarnos con problemas que son demasiado difíciles de resolver incluso para los mejores científicos, o demasiado difíciles de resolver en el plazo requerido.

¿Existe un método más realista para traspasar todo el problema a alguna entidad más inteligente? Una opción sería hacer que los *humanos* fueran más inteligentes, de tal manera que pudieran resolver legítimamente el problema de la alineación. Los humanos ya vienen «prealineados», a diferencia de las IA; los humanos más inteligentes tienen las mismas motivaciones prosociales básicas que el resto de nosotros.

En principio, parece posible que las personas puedan distinguir entre lo que *parece* una gran iluminación alquímica que les permitirá transmutar el plomo en oro, y el tipo de conocimiento que *realmente confiere* la capacidad de transmutar el plomo en oro (utilizando la física nuclear para arrancar algunos neutrones de los átomos de plomo). Sin duda, deberían sentirse como estados de conocimiento diferentes.

Pero a los ingenieros humanos les cuesta mucho saber en qué zona se encuentran. En la historia de la química, el nivel de habilidad de la humanidad era tal que los alquimistas eran engañados sistemáticamente.

En el mundo real, los científicos se aferran a sus teorías favoritas y se niegan a revisar sus puntos de vista hasta que la realidad les demuestra repetidamente que «su teoría era errónea» —y a veces se niegan a cambiar de parecer incluso entonces—. A veces se dice que la ciencia avanza «[funeral a funeral](https://en.wikipedia.org/wiki/Planck%27s_principle)», porque la vieja guardia nunca cambiará de ideas y solo queda esperar a que la nueva guardia madure. Pero esto no es una limitación fundamental impuesta por la naturaleza; es solo que los seres humanos, como clase, no son lo suficientemente sagaces, cuidadosos y conscientes de sí mismos.

Por lo general, está bien que los seres humanos sean ingenuos en estos aspectos, porque la realidad suele ser bastante indulgente con los errores, al menos en el sentido de que no aniquila a *toda la humanidad* por la arrogancia de un alquimista. Pero [ese no es un lujo que la humanidad pueda permitirse](#a-closer-look-at-before-and-after) cuando se trata de alinear la superinteligencia artificial.

La humanidad suele adquirir sus conocimientos luchando, intentándolo, fracasando y acumulándolos poco a poco. Pero no tiene por qué ser así.

Einstein no solo fue capaz de descubrir la relatividad general, sino que lo hizo *reflexionando intensamente sobre el problema*, incluso antes de que la humanidad pusiera satélites en órbita y comenzara a ver con sus propios ojos las discrepancias en sus relojes (como se explica en el capítulo 6). Tenía evidencia empírica, pero fue capaz de dar con la respuesta correcta de manera eficiente en respuesta a los primeros susurros de los registros empíricos, en lugar de necesitar que la verdad golpeara a su puerta.

Ese camino es menos común y más difícil de recorrer, pero ese tipo de genio científico existe —aunque raramente—, incluso entre los mejores y más brillantes del mundo.

Los seres humanos aumentados uno o dos pasos más allá del nivel de investigadores como Einstein o [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) podrían empezar a descubrir con precisión sus propios defectos y corregirlos de docenas de maneras diferentes.

Podrían darse cuenta de cuándo estaban racionalizando o cayendo en el [sesgo de confirmación](https://en.wikipedia.org/wiki/Confirmation_bias). Podrían superar el punto de esperar que una idea que suena inteligente funcione cuando en realidad no es así, hasta el punto de que, cada vez que esperan tener éxito, *realmente* lo consiguen. Podrían alcanzar un nivel de competencia en el que sigan cometiendo muchos errores, pero sin pecar [sistemáticamente](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) de exceso de confianza (o de falta de ella) en nuevos ámbitos complicados.

¿Es realmente posible mejorar la inteligencia humana? Nos parece que sí, tras haber hablado con varios investigadores en biotecnología que creen que hay vías de abordaje prometedoras a corto plazo. Una IA cuidadosamente orientada a la biotecnología también podría ayudar a acelerar el trabajo. Pero, desde nuestra perspectiva, sigue siendo muy incierto si un plan como este daría realmente resultado. Lo que sí podemos afirmar con más seguridad es que se trata de una opción con un efecto multiplicador elevado que merece mucha más inversión y exploración de la que está recibiendo actualmente.

No recomendamos mejorar la inteligencia humana como la única estrategia posterior a la desactivación de la IA en la que creemos que la humanidad debería invertir fuertemente. Más bien, este es solo uno de muchos ejemplos, y el que actualmente consideramos más prometedor. Recomendamos encarecidamente que la humanidad explore múltiples vías de avance sin IA, en lugar de poner todos los huevos en la misma cesta.

#### **Los humanos aumentados no plantean un gran problema de «alineación humana».** {#los-humanos-aumentados-no-plantean-un-gran-problema-de-«alineación-humana».}

Los humanos aumentados tendrían esencialmente la misma arquitectura cerebral, emociones, etc., que el resto de nosotros. Con la IA —incluso la IA entrenada para [*sonar como*](https://docs.google.com/document/d/1FbehxsMqTaw2seAzUJw4ny0tvZgUdbHJnCUi_Pbd5VU/edit?tab=t.k1kf1fy9gx5i#heading=h.8cxst4c4x63a) nosotros—, nos separa un enorme abismo cognitivo y motivacional, y una brecha de comprensibilidad de magnitud similar; en cambio, con humanos modestamente más inteligentes, no parece que nada de eso sea particularmente probable.

Los investigadores con cognición mejorada no necesitarían mantener su propia integridad mental mientras se convierten en vastas superinteligencias con mentes millones de veces más grandes. Solo necesitarían ser elevados al nivel necesario para descubrir cómo *construir* —no desarrollar— superinteligencias artificiales que fueran verdaderamente alineadas y estables.

Es posible que siga existiendo un problema de «alineación humana» en el sentido débil de que cualquier esfuerzo por coordinar a varias personas puede tropezar con problemas del agente-principal y de incentivos. Y estos problemas, por naturaleza, cobran mucha más importancia en cualquier grupo encargado de crear superinteligencia.

Esperamos que estos problemas sean tratables siempre que los humanos empiecen siendo visiblemente altruistas y caritativos, que su inteligencia aumente solo lentamente y que trabajen en una institución bien diseñada con incentivos bien diseñados. Pero es totalmente razonable que la gente se preocupe por la posibilidad de que se produzca una toma de poder en este ámbito. Resolver estos problemas no sería necesariamente fácil, pero no sería tan fundamentalmente inviable como que las empresas intentaran desarrollar superinteligencias inescrutables con mentes totalmente incomprensibles e impulsos inhumanos.

Crear un equipo de élite de supergenios modificados genéticamente para ayudar a que el planeta transite de forma segura hacia la superinteligencia es, sin duda, algo que la humanidad debería hacer con cuidado, dado lo mucho que está en juego en dicha empresa. Una iniciativa así plantea diversas cuestiones prácticas y éticas, pero estas deben sopesarse frente al costo de dejar que la superinteligencia nos aniquile a todos, si ninguna otra solución parece igual de prometedora.

A grandes males, grandes remedios, pero la mejora (modesta) de la inteligencia humana ni siquiera es una medida que parezca particularmente drástica. Parece una tecnología netamente positiva en sí misma, que tiene al menos alguna posibilidad de ayudar a la humanidad en más de un sentido.
