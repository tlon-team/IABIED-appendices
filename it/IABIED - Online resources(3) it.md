### L'intelligenza artificiale non dovrebbe diventare una vera e propria civiltà prima di poter essere pericolosa?

#### **Con i computer, la parte difficile è far loro risolvere un certo problema. Volume elevato e velocità arrivano subito dopo.** {#con-i-computer,-la-parte-difficile-è-far-loro-risolvere-un-certo-problema.-volume-elevato-e-velocità-arrivano-subito-dopo.}

"Per conquistare il mondo, è necessaria una civiltà" è un'intuizione che ha senso per gli esseri umani. È molto meno ovvio quanto questa idea possa applicarsi all'IA. Le IA non funzionano come gli esseri umani: possono essere enormemente più capaci di qualsiasi essere umano e un'istanza di IA non è necessariamente paragonabile a una singola persona.

Vale anche la pena tenere a mente che la superintelligenza è esattamente il tipo di cosa che può finire per avere l'analogo di un'intera civiltà estremamente rapidamente.

Con la maggior parte dei compiti che i computer possono svolgere, non ci vuole molto per passare da "i computer possono fare questo" a "i computer possono fare questo su scala enorme, molto più velocemente di qualsiasi essere umano". Pensa, per esempio, alle calcolatrici.

Ci sono stati anni in cui solo i computer di fascia più alta potevano fare il riconoscimento vocale, l'elaborazione video o la grafica 3D in tempo reale, ma non ci sono stati *molti* anni di questo tipo.

Le IA, come i software tradizionali, possono essere rapidamente copiate su tutti i computer disponibili. Ed è possibile costruire altri computer alla velocità dell'industria.

Confrontiamo questa situazione con gli esseri umani. Creare e addestrare un nuovo essere umano richiede risorse sostanziali e decenni di tempo. Una volta che si ha una singola IA a un dato livello di capacità, si può immediatamente copiare quella stessa IA addestrata, "adulta", quante volte si vuole, con una spesa minima.

In un certo senso, l'equivalente di un'intera (piccola) civiltà di menti IA esiste già nel momento in cui un'azienda rilascia un nuovo modello nei propri datacenter e avvia tutte le istanze necessarie per soddisfare la domanda.[^175] Oggi, queste flotte di IA non lavorano tutte in armonia. Ma le aziende usano [gruppi di agenti paralleli](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&t=348) quando puntano alle massime prestazioni a qualsiasi prezzo.

Tutto questo significa che probabilmente non ci sarà molto tempo tra quando le IA diventeranno abbastanza intelligenti da poter prendere il sopravvento se avessero un milione di istanze e quando le IA avranno almeno quel numero di istanze in esecuzione. Il tipo di crescita demografica che richiede centinaia di anni agli esseri umani può verificarsi in minuti con l'IA.

Per quanto riguarda le infrastrutture fisiche della civiltà, riteniamo che l'IA possa sfruttare produttivamente le infrastrutture umane per tutto il tempo necessario a sviluppare mezzi più avanzati per riorganizzare la materia secondo le proprie preferenze. Non ha bisogno di capire come fabbricare da zero la propria catena di approvvigionamento e infrastruttura di calcolo quando può usare i nostri computer. Non ha bisogno di inventare macchine industriali da zero quando può semplicemente prendere il controllo di quelle che abbiamo già utilmente costruito. E può usare le nostre infrastrutture per costruire la fase successiva delle proprie, utilizzando i robot esistenti per costruire fabbriche di robot nuove e più efficienti, o utilizzando i laboratori esistenti di sintesi del DNA per creare la propria biotecnologia, fino a diventare completamente autosufficiente.

Gli esseri umani sono quel tipo di entità per cui una manciata di noi ha iniziato nudi nella savana, e ci siamo fatti strada fino a costruire una civiltà tecnologica. E non siamo *così* intelligenti. Non sarebbe un'impresa tanto difficile da replicare per una superintelligenza, soprattutto se potesse partire dalla base industriale esistente dell'umanità come trampolino di lancio.

### Le IA non saranno limitate dalla loro capacità di progettare e condurre esperimenti? {#won't-ais-be-limited-by-their-ability-to-design-and-run-experiments?}

#### **L'intelligenza ti permette di imparare di più dagli esperimenti e di condurre esperimenti più veloci, più informativi e più parallelizzati.** {#intelligence-lets-you-learn-more-from-experiments-and-run-faster,-more-informative,-more-parallelized-experiments.}

Una civiltà di menti motivate che pensano mille volte più velocemente dell'umanità non sarebbe necessariamente in grado di produrre output tecnologici mille volte più velocemente degli esseri umani.

Per analogia: se impieghi tre ore per fare la spesa, e due di queste ore sono spese per andare e tornare dal negozio di alimentari a cavallo, allora un'auto dieci volte più veloce del cavallo può accelerare il tuo viaggio per la spesa, ma non di un fattore dieci. Alla fine, l'ultima ora trascorsa nel negozio diventa dominante nel tempo totale impiegato.

Anche una civiltà piena di ragionatori incredibilmente intelligenti deve occasionalmente attendere che tornino i risultati sperimentali. Se i tuoi pensieri sono sufficientemente veloci, allora il collo di bottiglia diventerà probabilmente quanto velocemente puoi agire nel mondo, quanto velocemente puoi acquisire informazioni e quanto tempo impiegano i tuoi piani a realizzarsi.

Ma non è così negativo come l'analogia del negozio di alimentari potrebbe far credere, perché la capacità di pensare si bilancia con la necessità di risultati sperimentali:

* Spesso puoi semplicemente pensare di più, pensare meglio, ed eliminare la necessità di un test, perché ti rendi conto che le osservazioni precedenti contengono già la risposta. Confronta la capacità delle moderne IA di [imparare a pilotare robot](https://arxiv.org/abs/1905.00741) utilizzando [pura simulazione](https://www.figure.ai/news/reinforcement-learning-walking).  
* A volte puoi riflettere più a fondo fino a trovare un test altrettanto affidabile ma più veloce.  
* A volte puoi eseguire molti test più veloci ma meno affidabili che possono essere eseguiti più volte in parallelo per ottenere risultati altrettanto affidabili a una velocità superiore.  
* A volte puoi eseguire molti test complessi contemporaneamente, in modo che i dati risultino complessi e difficili da interpretare — il che rappresenta un buon compromesso se lo sforzo cognitivo necessario per decifrare i risultati è meno costoso (dal punto di vista di una mente estremamente veloce) rispetto all'esecuzione di test multipli.  
* A volte puoi trovare un modo per costruire altri dispositivi che eseguono gli esperimenti molto più velocemente. Ad esempio, invece di inviare molte richieste diverse a un laboratorio biologico per far sintetizzare farmaci, puoi trovare un modo per inviare *una sola* richiesta a un laboratorio biologico, che risulterà nella sintesi di un *singolo batterio* contenente il codice genetico per produrre tutti i farmaci che desideri sintetizzare? Allo stesso modo, puoi creare un batterio sensibile ai segnali radio che risponda rapidamente alle istruzioni di un'IA veloce — molto più rapidamente degli esseri umani terribilmente lenti che corrono avanti e indietro seguendo le tue istruzioni?  
* E a volte puoi semplicemente prendere le tue dieci ipotesi migliori, capire cosa faresti in ciascuno di quei casi, costruire un dispositivo complesso che funzionerà indipendentemente da come si rivelerà essere la realtà e saltare completamente i test.

Una civiltà piena di copie di Steve Jobs, Marie Curie, John von Neumann e alcuni dei più grandi lavoratori e programmatori del mondo, se funzionassero a una velocità 10 000 volte superiore alla nostra, *noterebbero* che il principale ostacolo è l'attesa dei risultati sperimentali e potrebbero *lavorare su tale ostacolo* per ridurlo.

La storia del [Progetto Genoma Umano](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) è un buon esempio di cosa succede quando esseri umani intelligenti notano continuamente i colli di bottiglia in un progetto di ricerca massivo e lavorano per risolverli. Quello che si prevedeva avrebbe richiesto quindici anni e 3 miliardi di dollari è stato completato con due anni di anticipo e 300 milioni di dollari sotto budget; la maggior parte del genoma è stata mappata negli ultimi due anni utilizzando metodi e attrezzature migliorati.

Proprio come questo vale per gli esseri umani, vale anche per l'IA. Un ragionatore intelligente non deve restare lì inattivo mentre aspetta per anni soggettivi che test lenti si trascinino verso il completamento. Un ragionatore superumano *considera percorsi alternativi* ed è abile nel trovarli — è questo il senso dell'intelligenza.

Per una piccola evidenza pratica a questo proposito, consideriamo il caso degli esseri umani che eseguono esperimenti. Un buon caso di studio è quello del software confrontato alle sonde spaziali. Apportare modifiche a un prodotto software è economico e rapido, e gli ingegneri del software tendono a sperimentare costantemente, a produrre software che non funziona ancora del tutto e poi a correggerlo dove è più problematico. Al contrario, la sperimentazione è molto costosa sulle sonde spaziali — quindi gli esseri umani dedicano molto tempo a mettere a punto esattamente la sonda spaziale e a inserirvi quanti più esperimenti possibile. Si impegnano molto per dotare le sonde spaziali di *macchinari sperimentali generali* che possano essere telecomandati da lontano, così che se hanno una nuova idea per un esperimento non debbano inventare e lanciare un veicolo spaziale completamente nuovo.

E oltre tutto questo, un ragionatore sufficientemente intelligente ha anche l'opzione di semplicemente *capire com'è la realtà senza bisogno di tanti maledetti esperimenti*. A volte i dati che già hai sono sufficienti, se sei abbastanza intelligente da interpretarli.

Come caso di studio: ci sono voluti otto anni perché la [teoria della relatività generale](http://eotvos.dm.unipi.it/documents/EinsteinPapers/Einstein1911English.pdf) di Einstein fosse verificata empiricamente con nuovi dati. Il test fu condotto da Frank Watson Dyson e Arthur Stanley Eddington, che [fotografarono](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) le stelle dietro il sole durante un'eclissi solare totale e misurarono il grado di curvatura della luce attorno al sole; scoprirono che corrispondeva esattamente alla teoria di Einstein.

Ma quell'attesa di otto anni non bloccò alcun reale progresso scientifico.

![][image12]

\[ FONTE IMMAGINE: [https://en.wikipedia.org/wiki/Eddington\_experiment](https://en.wikipedia.org/wiki/Eddington_experiment) \]

Una delle ragioni è che la teoria di Einstein era chiaramente corretta: era già stata validata su dati come il movimento del perielio di Mercurio — previsto in modo impreciso dalla teoria di Newton e in modo accurato da quella di Einstein. Gli scienziati umani non consideravano questa previsione una vittoria perché i dati erano stati raccolti prima che Einstein formulasse la sua teoria, e volevano validare previsioni che la sua teoria facesse prima di vedere i dati. Ma questo è il tipo di stampella di cui una civiltà ha bisogno quando ha seri problemi con il [bias del senno di poi](https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science), il [pregiudizio di conferma](https://en.wikipedia.org/wiki/Confirmation_bias), e scienziati che barano per [gonfiare le prove a sostegno delle loro ipotesi](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation). Nessuna di queste è una caratteristica necessaria del buon ragionamento. E infatti, i pensatori attenti riuscirono a capire se la teoria di Einstein era corretta ben prima dell'esperimento di Eddington, usando le prove già disponibili.

Inoltre, c'erano metodi più veloci per testare la teoria — come costruire telescopi e osservare (gli effetti dei) buchi neri, come previsto dalla teoria di Einstein — che presumibilmente avrebbero potuto essere realizzati in meno di otto anni da una civiltà sufficientemente veloce nel pensiero e competente. Oppure, se si disponeva già di capacità di volo spaziale, si sarebbero potuti testare gli orologi sui satelliti in meno di un giorno. Assumere che la teoria di Einstein *richiedesse* otto anni per essere testata significherebbe sottovalutare radicalmente il potere dell'intelligenza.

Quando l'umanità finalmente arrivò a costruire i satelliti GPS, i satelliti furono programmati con due orologi diversi — uno che usava la teoria di Einstein, e uno che non la usava. Questa fu una scelta strana, dato quanto fosse ben confermata la teoria di Einstein a quel punto. Ma questa scelta sottolinea il fatto che in molti casi, una civiltà può semplicemente *prendere entrambi i rami* quando è incerta su una teoria. E sottolinea che quando gli esperimenti e i fallimenti sono costosi (come nel caso dei satelliti), spesso è molto più economico costruire le cose in modi che non si affidano troppo a una teoria particolare.

E come sottolineiamo nel libro, Einstein (se confrontato con Newton, Keplero e Brahe prima di lui) è anche un esempio di come le persone intelligenti possano dedurre molto più di quanto ci si potrebbe aspettare da osservazioni molto limitate. Einstein è impressionante non solo per aver scoperto la teoria della relatività, ma per averlo fatto partendo da *così pochi dati*.

Quindi, mentre la necessità di dati sperimentali può effettivamente vincolare la velocità con cui l'IA può intraprendere varie azioni, questo vincolo è probabilmente molto più debole di quanto possa sembrare intuitivamente.

## Discussione approfondita {#extended-discussion-6}

### Nanotecnologia e sintesi proteica {#nanotechnology-and-protein-synthesis}

L'intelligenza umana ci ha fornito molti vantaggi rispetto alle altre specie. Uno dei più rilevanti, tuttavia, è stata la nostra capacità di inventare nuove tecnologie. Se gli sviluppatori proseguono nella corsa e costruiscono un'IA più intelligente dell'uomo, possiamo analogamente aspettarci che gran parte del potere dell'IA provenga dalla sua capacità di far avanzare le frontiere scientifiche e tecnologiche. Ma cosa significa questo, concretamente? Quali tecnologie non ancora inventate attendono di essere scoperte?

È una domanda difficile a cui rispondere in termini generali. Uno scienziato nel 1850 avrebbe avuto enormi difficoltà a immaginare molte delle invenzioni dei successivi cento anni.

Tuttavia, non sarebbero stati completamente impotenti. Gli scienziati hanno predetto molte invenzioni decenni o secoli prima che fossero costruite, nei casi in cui una tecnologia poteva essere analizzata tecnicamente prima che gli ingegneri riuscissero a mettere insieme tutti i pezzi.[^176]

Una delle frontiere tecnologiche più impattanti che riteniamo l'IA esplorerà probabilmente è lo sviluppo di strumenti e macchine estremamente piccoli. Di seguito, entreremo in alcuni dettagli su questo argomento e sul ragionamento di base che vi sta dietro.

#### **L'esempio della biologia** {#the-example-of-biology}

Ogni cellula di ogni organismo in natura contiene un'enorme varietà di macchinari intricati.

"Macchinari" qui non è solo una metafora. Le macchine in questione sono piccole, quindi funzionano in modo un po' diverso rispetto alle macchine della vostra vita quotidiana. Ma molte macchine su larga scala hanno analoghi all'interno del nostro corpo. [L'ATP sintasi](https://en.wikipedia.org/wiki/ATP_synthase) genera energia nel corpo in modo simile a una ruota idraulica, utilizzando un flusso di protoni per far girare letteralmente un rotore.

\[embed video or gif: [https://en.wikipedia.org/wiki/File:ATP\_synthesis\_-\_ATP\_synthase\_rotation.ogv](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv)\]

Il flagello batterico funziona in modo simile all'elica di una barca, completo di un intero motore funzionante che fa ruotare il flagello per spingere il batterio attraverso i liquidi:

\[embed video: [https://www.youtube.com/watch?v=cwDRZGj2nnY](https://www.youtube.com/watch?v=cwDRZGj2nnY)\]

Un altro esempio, che abbiamo citato nel libro, è la kinesina, una piccola proteina che funziona come un robot trasportatore. Le kinesine "camminano" lungo fibre autoassemblanti che attraversano i neuroni, trasportando i neurotrasmettitori alla loro destinazione.

\[embed video o gif: [https://www.youtube.com/watch?v=y-uuk4Pr2i8](https://www.youtube.com/watch?v=y-uuk4Pr2i8)\]

Più piccola è una macchina, più veloce può funzionare in genere; e le macchine piccole come le molecole funzionano molto velocemente. Le kinesine fanno fino a [200 passi al secondo](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004\(04\)00103-3), andando avanti con un "piede" mentre l'altro piede rimane saldamente ancorato al microtubulo su cui si trova.[^177]

Una delle frontiere tecnologiche che l'intelligenza artificiale più intelligente dell'uomo potrebbe esplorare è la costruzione, la progettazione o il riutilizzo di macchine su scala molto piccola. Questo tipo di tecnologia potrebbe essere classificata come "biotecnologia", "nanotecnologia" o qualcosa di intermedio, a seconda di fattori quali la scala, quanto il design assomigli alle strutture biologiche esistenti e se sia "umida" (dipendente dall'acqua, come i meccanismi delle cellule viventi) o "secca" (in grado di funzionare all'aria aperta).

Pensare agli organismi biologici come a meraviglie dell'ingegneria su scala nanometrica può aiutare a formulare ipotesi su ciò che le IA più intelligenti degli esseri umani potrebbero essere in grado di realizzare con una scienza e una tecnologia più avanzate di quelle che possediamo oggi.

(C'è poi la questione di quanto tempo ci vorrebbe per inventare e sviluppare una tecnologia del genere. Per saperne di più, dai un'occhiata al capitolo 1 del libro, dove si parla di come le superintelligenze artificiali potrebbero pensare almeno 10 000 volte più velocemente degli esseri umani con l'hardware dei computer di oggi. Vedi anche la nostra discussione approfondita su come [le IA dovrebbero dedicare un po' di tempo all'esecuzione di test fisici ed esperimenti, ma il rallentamento complessivo probabilmente non costituirebbe un ostacolo significativo per una superintelligenza](#won't-ais-be-limited-by-their-ability-to-design-and-run-experiments?).)

Guardando alle imprese degli ingegneri umani di oggi, può sembrare difficile credere che, ad esempio, un'intelligenza artificiale con capacità sovrumane che gestisce un laboratorio biologico possa mai costruire fabbriche microscopiche che usano la luce solare per autoreplicarsi continuamente. Potrebbe sembrare ancora più fantastico immaginare microfabbriche universali, fabbriche in grado di accettare istruzioni per costruire praticamente qualsiasi macchina con le risorse disponibili.

Ma macchine del genere non solo sono possibili, esistono già. Le alghe sono fabbriche grandi pochi micron, alimentate dal sole e capaci di autoreplicarsi, che possono raddoppiare la loro popolazione in meno di un giorno. E le alghe contengono [ribosomi](https://en.wikipedia.org/wiki/Ribosome), che sono la versione biologica di una stampante 3D universale o di una catena di montaggio universale (universale almeno per quanto riguarda gli elementi costitutivi della vita).

Con le giuste istruzioni (codificate nell'RNA messaggero), i ribosomi stampano strutture arbitrarie che possono essere assemblate dalle [proteine](https://en.wikipedia.org/wiki/Protein). Questa universalità è alla base dell'enorme complessità e varietà del mondo biologico: tutta la diversità della vita sulla Terra è in definitiva assemblata da queste fabbriche universali, che si trovano essenzialmente immutate in tutto, dai porcospini ai moscerini della frutta ai batteri.

\[embed video: [https://www.youtube.com/watch?v=8dsTvBaUMvw](https://www.youtube.com/watch?v=8dsTvBaUMvw)\]

I ribosomi possono anche essere utilizzati per assemblare strutture che non sono esse stesse costituite da proteine, utilizzando le proteine come intermediario. Un esempio di struttura non proteica che i ribosomi possono costruire in questo modo è l'osso. I ribosomi producono proteine che si ripiegano in enzimi debolmente legati che catalizzano calcio e fosforo trasformandoli in reagenti speciali. Questi reagenti formano quindi una matrice di collagene che guida il calcio e il fosforo nella posizione corretta per trasformarli in osso duro e cristallino.

La natura fornisce una prova di esistenza che alcune macchine fisiche davvero straordinarie sono possibili, per entità abbastanza intelligenti da utilizzare i ribosomi in modi che gli esseri umani non hanno ancora scoperto — o entità che utilizzano i ribosomi per costruire i propri analoghi migliorati dei ribosomi.

Ma le strutture che vediamo nel mondo biologico stabiliscono solo un limite inferiore per ciò che è possibile. Gli organismi biologici sono ben lontani dai limiti teorici dell'efficienza energetica e della resistenza dei materiali, e potrebbero essere relativamente facili da migliorare per ragionatori molto più intelligenti degli esseri umani.

#### **Tanto spazio in basso** {#tanto-spazio-in-basso}

Se sembra strano utilizzare i fenomeni naturali come evidenza di quali tecnologie future potrebbero essere fattibili, si noti che questo è uno schema comune nella storia della scienza. Gli uccelli potevano volare, quindi gli inventori hanno trascorso secoli cercando di costruire macchine volanti.

Richard Feynman, un fisico pionieristico, ha dimostrato la potenza di questo approccio in una conferenza del 1959 intitolata "[C'è molto spazio in basso](https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf)". Nella conferenza, Feynman esegue calcoli su quali tipi di cose interessanti si potrebbero fare con la miniaturizzazione.

Oggi, le osservazioni di Feynman appaiono straordinariamente lungimiranti. Feynman osserva come i computer potrebbero probabilmente fare molto di più se contenessero più elementi, ma che l'ostacolo a questo è quanto grandi dovrebbero quindi essere i computer. Devono essere miniaturizzati!

Feynman calcola che ci vorrebbe circa un petabit (1 000 000 000 000 000 di bit) per memorizzare tutti i libri scritti dall'umanità:

> Per ogni bit concedo 100 atomi. E si scopre che tutte le informazioni che l'uomo ha accuratamente accumulato in tutti i libri del mondo possono essere scritte in questa forma in un cubo di materiale largo un duecentesimo di pollice — che è il più piccolo granello di polvere che può essere distinto dall'occhio umano. Quindi c'è molto spazio in basso! Non parlarmi di microfilm!

Ancora oggi non ci siamo riusciti! L'elemento di archiviazione effettivo all'interno di una scheda microSD da 2 terabyte misura ancora 0,6 millimetri per lato. Per confronto, 1/200 di pollice corrisponderebbe a 0,125 mm per lato. E la scheda SD contiene appena 17,6 trilioni di bit, che rappresenta solo 1/57 di quanto Feynman aveva calcolato nel 1959 come necessario per memorizzare tutta la conoscenza dell'umanità.

Forse Feynman si sbagliava sui limiti ultimi dell'ingegneria in senso pratico? Negli ultimi tempi, i progressi nella miniaturizzazione informatica hanno subito un forte rallentamento. Dire che qualcosa è fisicamente possibile non dimostra che gli ingegneri riusciranno a realizzarlo.

E avvicinarsi di tre ordini di grandezza a quello che un giorno sarebbe stato raggiunto può essere considerato un bel colpo di previsione per Feynman. Feynman ha tenuto la sua conferenza sei anni prima che Gordon Moore lanciasse per la prima volta l'idea che oggi chiamiamo [Legge di Moore](https://en.wikipedia.org/wiki/Moore%27s_law). La gente non era abituata a pensare alla miniaturizzazione come a una legge ineluttabile su un grafico. Non sappiamo di nessun altro ai tempi di Feynman che abbia ipotizzato che un giorno potesse esistere un dispositivo il cui elemento di memoria, grande come un granello di sabbia, potesse contenere dieci milioni di volte più informazioni dei più grandi computer a valvole degli anni '50.

![][immagine13]  
\[img src: [https://www.ibm.com/history/700](https://www.ibm.com/history/700)\]

Ma in realtà Feynman non si sbagliava. E Feynman sapeva già allora che la sua stima era sicura:

> Questo fatto, cioè che si possono avere un sacco di informazioni in uno spazio super piccolo, è ovviamente ben noto ai biologi [...] tutte queste informazioni sono contenute in una piccolissima parte della cellula sotto forma di molecole di DNA a catena lunga, dove circa cinquanta atomi sono usati per un bit di informazione sulla cellula.

I computer moderni non sono ancora stati miniaturizzati fino a raggiungere la scala del DNA, ma in sessant'anni ci siamo avvicinati notevolmente. I transistor dei chip commerciali di fascia alta hanno ora una larghezza inferiore a cento atomi e sono costruiti con una tecnologia che consente di aggiungere strati di materiale [dello spessore di un singolo atomo](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

Basarsi su analogie naturali e calcoli approssimativi si è rivelato un modo super efficace per capire cosa si sarebbe potuto fare nei decenni a venire. E queste traiettorie tecnologiche possono andare molto più veloci quando l'intelligenza artificiale fa il lavoro scientifico e ingegneristico necessario.

#### **Superare la biologia** {#outdoing-biology}

Perché la carne non può essere forte come l'acciaio?

Dopotutto, sono tutti gli stessi atomi. I legami metallici tra gli atomi di ferro sono forti, ma lo sono anche i legami covalenti tra gli atomi di carbonio nel diamante; perché non ci siamo evoluti per avere una maglia di diamante che attraversa la nostra pelle, per aiutarci a sopravvivere fino all'età riproduttiva?

Del resto, se il ferro è così resistente, perché gli organismi non si sono evoluti per mangiare minerale di ferro e sviluppare una pelle rivestita di ferro? Se gli ingegneri umani riescono a farlo, perché la natura non l'ha fatto prima?

Forse c'è qualche motivo situazionale per cui le pelli rivestite di ferro in particolare non sono una grande idea.

Ma se non è quello, perché non qualcos'altro?

La grande domanda generale qui è: perché la natura è così lontana dai limiti delle possibilità fisiche — come calcolate dalla fisica o dimostrate dall'ingegneria umana? C'è una risposta profonda e generale, non solo una limitata e superficiale?

Abbiamo notato che Feynman è riuscito a usare le strutture della biologia per stabilire dei limiti inferiori a ciò che dovrebbe essere possibile con maggiori conoscenze scientifiche. Ma in molti casi, la tecnologia umana ha già superato la biologia. Perché è possibile, quando l'evoluzione ha avuto miliardi di anni per perfezionare piante e animali? Comprendere questo fenomeno generale può aiutare a far luce sul perché la nanotecnologia sia probabilmente in grado di andare ben oltre ciò che possiamo già vedere oggi in natura.

Possiamo immaginare di trovarci in un mondo dove le sequoie sono alte almeno la metà degli edifici più alti. Possiamo immaginare un mondo dove la pelle degli animali più resistenti è dura almeno la metà dei materiali più duri osservati. Perché non ci troviamo in un mondo simile, dove la natura si è spinta contro i limiti fisici dopo qualche miliardo di anni di evoluzione?

È una domanda abbastanza profonda da non permetterci di riassumere brevemente tutto ciò che è noto. Ma il riassunto generale è che la selezione naturale ha difficoltà ad accedere ad alcune parti dello spazio di progettazione, incluse molte parti che sono molto più facili da raggiungere se sei un ingegnere umano.

I tre fattori principali che vediamo contribuire a questo sono:

1. La selezione naturale ha una pressione selettiva limitata e ci vogliono centinaia di generazioni per far sì che una nuova mutazione diventi universale. Se una caratteristica biologica non è molto, molto antica, spesso sembra progettata in fretta e furia, come se il tempo stringesse.  
2. Tutto ciò che è stato costruito dalla selezione naturale ha avuto origine come errore accidentale in un progetto precedente — una mutazione. L'evoluzione ha maggiori difficoltà a esplorare parti dello spazio progettuale che sono *distanti* da ciò che *attualmente esiste* negli organismi. È difficile per l'evoluzione superare le lacune.  
3. La selezione naturale ha difficoltà a costruire cose nuove o a risolvere problemi che richiederebbero cambiamenti simultanei anziché sequenziali. Questo limita drasticamente i progetti che l'evoluzione può realizzare e conferisce ai progetti attuali in biologia il loro aspetto frammentario, approssimativo ed estremamente intricato secondo gli standard dell'ingegneria umana. Ad esempio, la complessità (delle parti conosciute) del metabolismo umano: \[IMG TODO: fonte è [https://www.reddit.com/r/MapPorn/comments/6sxznd/map\_of\_the\_known\_human\_metabolic\_pathways/](https://www.reddit.com/r/MapPorn/comments/6sxznd/map_of_the_known_human_metabolic_pathways/) \]

![][immagine14]  
Oppure, per un esempio più semplice della confusione evolutiva, consideriamo l'occhio. Gli occhi dei vertebrati si sono evoluti casualmente con i nervi (2 nell'immagine sotto) posizionati sopra le cellule fotosensibili (1). Questi nervi devono uscire dall'occhio attraverso un foro nella parte posteriore (3) e, dato che questo punto presenta un foro, deve necessariamente essere privo di cellule fotosensibili. Questo crea un punto cieco (4) per tutti i vertebrati, compresi gli esseri umani, costringendo il cervello a escogitare trucchi intelligenti per "riempire" il vuoto (ad esempio, utilizzando le informazioni dell'altro occhio).

I polpi hanno evoluto gli occhi in modo indipendente e, casualmente, sono arrivati al design più ragionevole — i nervi passano dietro le cellule fotosensibili. Questo permette a questi collegamenti di uscire dall'occhio senza creare alcun punto cieco.

![][immagine15]  
\[img src: [https://en.wikipedia.org/wiki/Evolution\_of\_the\_eye](https://en.wikipedia.org/wiki/Evolution_of_the_eye)\]

Oppure consideriamo il nervo laringeo ricorrente della giraffa, che deve collegare la gola della giraffa al suo cervello per azionare la laringe. Anziché prendere il percorso diretto, questo nervo parte dalla gola, scende per tutta la lunghezza del collo della giraffa, fa un giro goffo intorno all'aorta della giraffa, risale per tutto il collo fino a tornare al punto di partenza, e poi si collega al cervello.

Il risultato è un nervo lungo quattro metri e mezzo (il giro nero nell'immagine sotto), che fa sì che i segnali impieghino da dieci a venti volte più tempo del necessario per viaggiare tra il cervello della giraffa e la sua gola.[^178]

![][immagine16]  
\[fonte immagine: [https://en.wikipedia.org/wiki/Recurrent\_laryngeal\_nerve](https://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve)\]

Nei pesci, questo design aveva senso perché la loro versione del nervo laringeo collegava il cervello alle branchie — un collegamento diretto. Se si prende lo stesso design e si dà all'animale un collo, continuando però ad allungare il collo senza mai rifare il cablaggio da capo, si ottengono design molto inefficienti. Compatibili con la sopravvivenza, ma inefficienti.

L'evoluzione produce design fantastici, se ha abbastanza tempo. Ma gli esseri umani e le IA possono inventare una gamma di design molto più varia e flessibile, e possono farlo molto velocemente.

I primi organismi multicellulari con cellule differenziate e specializzate sembrano essersi evoluti circa 800 milioni di anni fa. In termini umani, sembra un'eternità. Ma l'evoluzione funziona molto più lentamente della civiltà umana.

Un gene appena mutato che dà un vantaggio del 3 % nella capacità riproduttiva (che è un sacco per una mutazione!) ci metterà in media 768 generazioni per diffondersi in una popolazione di 100 000 organismi che si riproducono tra loro. Se la popolazione è di 1 000 000 di individui (la stima della popolazione umana ai tempi dei cacciatori-raccoglitori), ci vorranno 2 763 generazioni. E la probabilità che la mutazione si diffonda fino a diventare fissa, invece di scomparire casualmente, è solo del 6 %.

Nella genetica delle popolazioni, la regola generale è "una mutazione, una morte". Se gli errori di copia del DNA introducono dieci copie di una mutazione dannosa in ogni nuova generazione, allora dieci portatori di quella mutazione devono morire o non riuscire a riprodursi, per generazione, al fine di controbilanciare la pressione del semplice rumore genetico.

Non è così grave come sembra, come costo per mantenere le informazioni genetiche. In una specie che si riproduce sessualmente, si può finire con una persona (o un embrione) che porta un sacco di mutazioni dannose e che muore - o non riesce a riprodursi, o abortisce - e questo può eliminare più di un caso di gene mutato alla volta. Ma questo limite è ancora la spiegazione standard del perché gli esseri umani abbiano perso così tanti diversi adattamenti utili che si riscontrano negli scimpanzé e in altri primati. Mentre la selezione naturale era impegnata a selezionare una maggiore intelligenza dei primati (per esempio), aveva meno spazio per preservare tutti i sottili geni olfattivi che consentono un senso dell'olfatto più ricco. I geni olfattivi rilevanti erano utili per la sopravvivenza, ma non abbastanza da rimanere mentre l'attenzione dell'evoluzione era altrove.

La maggior parte delle giraffe non muore a causa del loro nervo laringeo ridicolmente lungo. Forse alcune giraffe riescono a soffocare con dei rametti che sarebbero sopravvissute se il loro cervello fosse stato in grado di reagire più velocemente, ma probabilmente non è molto comune. Quindi non è proprio una priorità per la selezione naturale, che ha solo una certa pressione di ottimizzazione da distribuire. Il design approssimativo della giraffa funziona per lo più, viene buttato fuori dalla porta e il gioco è fatto.

Realisticamente, l'evoluzione non può rifattorizzare i suoi progetti o ricominciare da zero; può solo apportare piccole modifiche. Ma anche se fosse disponibile un progetto migliore, rifattorizzare queste strane complicazioni extra e ripulire il debito di progettazione non è una priorità della selezione naturale.

E poiché la selezione naturale non pensa mai al futuro, non diventa una priorità nemmeno se ci fossero altri grandi miglioramenti per la giraffa che si potrebbero ottenere con una struttura del sistema nervoso meno bizzarra. La selezione naturale non pianifica. È semplicemente la storia congelata di quali geni e organismi si sono già riprodotti nella pratica.

Essere in grado di individuare un cattivo design non significa necessariamente che si possa costruire una giraffa migliore. Ma gli esseri umani hanno fatto progressi notevoli in pochissimo tempo quando si tratta di mettere in funzione centinaia di migliaia di macchine che fanno cose che la natura non può fare. Ci aspettiamo che questo valga ancora di più se e quando le IA diventeranno migliori degli esseri umani nel design e saranno in grado di svolgere lo stesso lavoro cognitivo centinaia di migliaia di volte più velocemente.

La capacità della selezione naturale di "progettare" una giraffa migliore è ostacolata dal fatto che opera attraverso mutazioni e ricombinazioni. Ha difficoltà ad accedere a qualsiasi parte dello spazio di progettazione che non possa essere raggiunta da una serie di singole mutazioni, che devono essere tutte vantaggiose individualmente e separatamente, o combinando mutazioni che erano tutte sufficientemente vantaggiose individualmente da essere presenti in una grande parte del pool genetico prima di combinarsi.

Un complesso genetico composto da cinque geni, ciascuno con una prevalenza indipendente del 10 % nella popolazione, ha solo una probabilità su 100 000 di assemblarsi all'interno di ciascun organismo. E un complesso genetico che rappresenta un enorme vantaggio, ma solo una volta su 100 000, non ha quasi nessuna possibilità di evolversi fino alla fissazione.

Questo non vuol dire che la selezione naturale non possa creare macchine complesse, ma solo che il suo percorso verso macchinari complessi deve passare attraverso fasi incrementali vantaggiose. Per reindirizzare il nervo della giraffa sarebbero necessari una manciata di cambiamenti simultanei al genoma della giraffa, e ciascuno di questi cambiamenti sarebbe individualmente inutile senza gli altri. Quindi l'anatomia della giraffa rimane quella che è.

La meraviglia dell'[evoluzione](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/ZyNak8F6WXjuEbWWc) non sta nella sua rapidità; la sua complessità campionaria è di gran lunga superiore a quella di un ingegnere umano che fa studi di casi. La meraviglia della selezione naturale non sta nell'elegante semplicità dei suoi disegni; basta dare un'occhiata al diagramma di un qualsiasi processo biochimico per sfatare questo malinteso. La meraviglia della selezione naturale non sta nella sua robusta correzione degli errori che copre ogni percorso che potrebbe andare storto; ora che moriamo meno spesso di fame e per lesioni, la maggior parte della medicina moderna si occupa di trattare parti della biologia umana che esplodono in modo casuale in assenza di traumi esterni.

La meraviglia dell'evoluzione è che — come processo di ricerca puramente accidentale — l'evoluzione funziona.

#### **La debolezza delle proteine** {#the-weakness-of-protein}

Questo ci porta a un altro modo in cui la tecnologia può probabilmente migliorare la biologia.

Molto al di sotto del livello della carne, invisibili a occhio nudo, ci sono le cellule. Molto al di sotto del livello delle cellule ci sono le proteine.

Le proteine, quando si ripiegano, sono per lo più tenute insieme dall'equivalente molecolare dell'elettricità statica — [forze di van der Waals](https://en.wikipedia.org/wiki/Van_der_Waals_force) decine o centinaia di volte più deboli dei legami metallici come il ferro, o anche dei legami covalenti come il diamante.

Perché la biologia usa un materiale così debole come blocco costruttivo di base? Perché un materiale più forte avrebbe reso più difficile il lavoro dell'evoluzione. (E se rendi troppo difficile l'evoluzione, non evolverai mai il tipo di persone che fanno questo tipo di domande.)

Le proteine si ripiegano sotto forze molecolari relativamente leggere e sono legate in quelle forme principalmente dall'elettricità statica. Questo è uno dei motivi principali per cui la selezione naturale ha una ricca struttura di vicinato di possibilità da esplorare: mutazioni casuali possono modificare ripetutamente una proteina e finire per imbattersi in un nuovo design che fa più o meno la stessa cosa, ma leggermente meglio.

Se invece gli organismi fossero costituiti da molecole tenute insieme da legami stretti, cambiare uno dei componenti avrebbe meno probabilità di produrre una nuova struttura interessante (e potenzialmente utile). Potrebbe comunque succedere a volte, ma molto meno spesso. E se sei il tipo di progettista che impiega due miliardi di anni per inventare le colonie cellulari e un altro miliardo di anni per inventare tipi di cellule differenziate, "accade meno spesso" significa che la stella più vicina si gonfia e inghiotte il tuo pianeta prima che tu arrivi a quel punto.

Ogni proteina esiste a causa di un errore di copia da parte di una proteina precedente. La proteina precedente non era tenuta insieme saldamente da molti legami forti perché sarebbe stato più difficile evolversi da essa. Quindi probabilmente anche l'ultima nuova proteina non ha molti legami forti.

La biochimica a volte individua legami forti. Abbiamo citato prima l'esempio delle ossa. Un altro esempio si trova nelle piante. Le piante hanno sviluppato proteine che si ripiegano in enzimi, che catalizzano la sintesi di blocchi di costruzione molecolari, che vengono ossidati in un polimero fortemente reticolato in modo covalente: la lignina, l'elemento costitutivo del legno.[^181]

Ma questi sono casi speciali, e la selezione naturale non ha molta "attenzione" da dedicare alla creazione di molti casi simili.

Non è estraneo alla natura degli atomi di carbonio e altri elementi organici comuni che possano essere forti. Richiede solo molto più lavoro per evolversi. La selezione naturale non ha il tempo di farlo ovunque, solo per alcuni casi speciali rari inseriti nel resto dell'anatomia, come le ossa, la lignina nel legno o la cheratina nelle unghie e negli artigli.

Se si inseriscono le parole chiave giuste, è possibile interrogare, ad esempio, ChatGPT-o1 (nel momento in cui leggerete questo, gli LLM di pari potenza saranno probabilmente gratuiti) e chiedere informazioni sulla forza dei singoli legami carbonio-carbonio nel diamante, dei legami ferro-ferro nel ferro metallico, dei legami polimerici covalenti nella lignina, dei legami disolfuro nella cheratina o dei legami ionici nelle ossa. È possibile chiedergli come tutti questi elementi siano correlati alla resistenza strutturale del materiale più grande. (Nel 2023 non avresti dovuto provarci perché GPT-4 avrebbe sbagliato tutti i calcoli, ma mentre scriviamo questo paragrafo nel 2024, o1 sembra migliore).

Scopriresti che la forza esatta del legame tra due atomi di carbonio è dell'ordine di mezzo attojoule, così come la forza del legame tra due atomi di ferro, e il reticolo solfuro-solfuro nella cheratina è solo leggermente inferiore (0,4 attojoule), così come i legami covalenti polimerizzati nella lignina del legno.

Ma le forze di attrazione statica che ripiegano le proteine sono, a seconda di come le si considera, al massimo dieci volte più deboli e potenzialmente centinaia o migliaia di volte più deboli di quelle.

E anche quando le piante catalizzano sostanze come la lignina, i legami incrociati tendono ad essere più sparsi dei legami carbonio-carbonio nel diamante. La differenza tra la resistenza in gigaPascal del diamante e quella in megaPascal del legno dipende più dalla densità e dalla regolarità dei legami nel diamante, piuttosto che dalla maggiore resistenza dei singoli legami del diamante.[^182]

A causa dei limiti dell'evoluzione come progettista e dei limiti delle proteine come materiale da costruzione, la vita opera sotto vincoli che i progettisti umani e le IA possono aggirare. Gli uccelli sono meraviglie dell'ingegneria, ma i velivoli costruiti dall'uomo possono trasportare carichi diecimila volte più pesanti a una velocità di volo superiore a dieci volte quella degli uccelli più veloci e più forti. I neuroni biologici sono meraviglie dell'ingegneria, ma i transistor costruiti dall'uomo si accendono e si spengono decine di milioni di volte più velocemente dei neuroni più veloci. E la tecnologia di cui disponiamo oggi è ancora solo la punta dell'iceberg di ciò che è possibile realizzare.

#### **Freitas e i globuli rossi** {#freitas-and-red-blood-cells}

Abbiamo detto che la biologia non è neanche lontanamente vicina al limite di ciò che è fisicamente possibile. Allora, cosa c'è vicino al limite?

Per capire meglio questa domanda, possiamo pensare ai globuli rossi.

Negli ultimi 1,5 miliardi di anni, in tutti gli organismi viventi, dagli esseri umani alle lucertole, l'ossigeno è stato trasportato nelle cellule multicellulari dall'emoglobina. L'emoglobina è una proteina composta da 574 aminoacidi, più quattro gruppi eme appositamente creati per contenere una speciale molecola di ferro. Un globulo rosso umano contiene circa 280 milioni di molecole di emoglobina ed è lungo circa sette micron. Tre milioni di essi potrebbero stare sulla testa di uno spillo, e nel tuo corpo ce ne sono circa 30 bilioni.

Quanto sono vicini i globuli rossi ai limiti di ciò che potrebbero fare in teoria quando si tratta di trasportare ossigeno?

Rob Freitas, autore di Nanomedicine, nel 1998 ha fatto uno studio abbastanza dettagliato su un progetto teorico per un globulo rosso artificiale usando materiali legati in modo covalente. La cellula era pensata per avere un diametro di un solo micron, così da passare più facilmente nelle arterie intasate.

Piuttosto che limitarsi a considerare un modo diverso di immagazzinare le molecole di ossigeno, Freitas ha pensato a come sostituire l'intero globulo rosso. Freitas ha attinto ad analisi precedenti per considerare la necessità di estrarre anche il glucosio dal mezzo sanguigno e trasformarlo in energia per alimentare la cellula artificiale. Ha preso in considerazione sensori delle dimensioni di una cellula e minuscoli computer integrati costituiti da aste solide che si incastrano in altre aste solide per eseguire semplici calcoli. Ha valutato se la cellula artificiale si sarebbe depositata dalla sospensione nel liquido più rapidamente degli attuali globuli rossi.

La biocompatibilità può essere un problema enorme per qualsiasi cosa che va dentro il corpo umano, ma le superfici di diamante sono abbastanza inerti da permettere l'uso di rivestimenti simili al diamante per alcuni dispositivi medici che vanno dentro il corpo umano. A livello di possibilità teorica, Freitas stava pensando che la superficie della cellula artificiale potesse assomigliare a un diamante e quindi essere biocompatibile.

![][immagine17]

\[inserire immagine da [https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm](https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm)

Il punto centrale del globulo rosso artificiale era il calcolo di Freitas secondo cui un recipiente a pressione in corindone o diamante monocristallino su scala micrometrica avrebbe potuto sopportare, in modo conservativo, una pressione di 100 000 atmosfere. Con un margine di sicurezza di 100 volte e un'imballaggio molecolare a sole 1 000 atmosfere, i globuli rossi artificiali potrebbero fornire ai tessuti una quantità di ossigeno 236 volte superiore rispetto ai globuli rossi per unità di volume e immagazzinare una quantità simile di anidride carbonica per compensare l'altra parte della respirazione. In pratica: potresti trattenere il respiro per quattro ore.

Ora, costruire effettivamente cellule ematiche artificiali di questo tipo è tutta un'altra questione. Ecco perché questo particolare trattamento medico non è ancora disponibile presso l'ambulatorio del tuo medico locale.

Una sfera di diamante solido e perfetto del peso di 1 chilogrammo è una molecola facile da descrivere sulla carta, ma sintetizzarla è più difficile. Quello che Freitas ci aiuta a fare è formulare ipotesi più informate su quanto la biologia attuale sia lontana dai limiti teorici in questo campo.[^183] La biologia è impressionante, ma ben lontana dall'ottimale.

È plausibile che, per una serie di ragioni, il progetto esatto di Freitas non funzionerebbe, ed è molto probabile che non sarebbe ottimale. Un'idea iniziale per un progetto complesso estremamente innovativo incontrerà quasi sicuramente dei problemi da qualche parte.

Ma nell'esprimere scetticismo sul fatto che la proposta esatta di Freitas possa funzionare, non stiamo affermando che nessuna alternativa ai globuli rossi possa mai fornire ossigeno centinaia di volte più efficientemente dei globuli rossi biologici.

L'ingegneria consiste nel trovare un modo per far funzionare qualcosa. Anche se mille tentativi di costruire qualcosa falliscono, ne basta uno solo di successo perché l'intero progetto abbia successo. L'esistenza di una miriade di progetti di aerei non funzionanti nel XVII secolo e prima non significava che gli aerei funzionanti fossero impossibili, ma solo difficili da individuare nello spazio di tutti i possibili progetti.

Ecco perché gli scettici della tecnologia, pur avendo spesso ragione sul fatto che le tecnologie sono più lontane nel futuro di quanto credano gli ottimisti più entusiasti, tendono a sbagliarsi nelle loro affermazioni che certe imprese tecnologiche non saranno mai realizzate. Quando l'impresa è un compito concreto nel mondo, quando siamo agnostici su come l'impresa venga realizzata e quando l'impresa è nota per essere consentita dalle leggi della fisica, la storia suggerisce che spesso c'è un modo per riuscirci, anche se inizialmente il percorso non è ovvio.

O, per dirla con le parole dello scrittore e inventore Arthur C. Clarke:

> Quando uno scienziato illustre ma anziano afferma che qualcosa è possibile, quasi sicuramente ha ragione. Quando afferma che qualcosa è impossibile, molto probabilmente si sbaglia.

#### **Nanosistemi** {#nanosystems}

Per ricapitolare:

* Il mondo biologico è costruito da un'incredibile varietà di macchine molecolari.  
* Studiare la biologia può insegnarci quali imprese microscopiche sono possibili dal punto di vista tecnologico.  
* Ma la biologia è un limite conservativo su ciò che è possibile; non è vicina ai limiti della possibilità. L'evoluzione è un progettista molto limitato e le proteine non sono il miglior materiale da costruzione.

Il libro *Nanosystems* (1992) di Eric Drexler è un classico che esplora la questione di quali imprese ingegneristiche su piccola scala siano possibili. *Nanosystems* ha contribuito a dare il via alla rivoluzione dei nanomateriali degli anni '90 e ha suscitato parecchie controversie quando gli scienziati hanno dibattuto le argomentazioni di Drexler. È possibile trovare una copia completa online di *Nanosystems* [qui](https://nanosyste.ms/table_of_contents).

*Nanosystems* è un testo approfondito e di ampio respiro, ma sorprendentemente accessibile nonostante l'argomento tecnico. Uno dei contributi principali del libro è stato quello di esplorare le implicazioni della costruzione di strutture su piccola scala in modo innovativo.

Un modo per costruire oggetti molto piccoli è attraverso le reazioni chimiche: far scontrare le molecole in particolari condizioni (come il calore estremo) per romperle e far sì che gli atomi si uniscano in nuove molecole.

Questo è un approccio potente di per sé ed è il metodo che l'umanità usa per creare materiali come plastica, acciaio e ceramica, ma impallidisce in confronto a ciò che si può costruire con altri metodi. Creare materiali tramite reazioni chimiche è un po' come costruire strutture LEGO riempiendo sacchetti di mattoncini LEGO e scuotendoli con forza. È possibile costruire alcune cose in questo modo, ma l'insieme delle cose che si possono costruire è limitato e c'è molto spreco.

La sintesi proteica è come usare le mani per costruire grandi strutture LEGO a partire da set LEGO più piccoli e precostruiti. C'è spazio per una maggiore precisione perché puoi posizionare ogni set precostruito esattamente dove vuoi, ma è comunque un po' strano e scomodo perché stai lavorando con set precostruiti. Questo è ciò che fanno i ribosomi nel corpo: uniscono catene di [amminoacidi](https://en.wikipedia.org/wiki/Amino_acid) per formare proteine, che vengono poi utilizzate per svolgere una varietà di compiti nel corpo.

L'insulina, l'emoglobina e l'ATP sintasi nel corpo umano sono tutti esempi di complessi proteici formati da più catene proteiche unite tra loro: due catene proteiche nel caso dell'insulina, quattro per l'emoglobina e ventinove per l'ATP sintasi.

Gli elementi costitutivi delle proteine — gli amminoacidi — sono molecole composte in genere da dieci a venticinque atomi. Come materiali da costruzione, gli amminoacidi hanno molti vantaggi:

* Ogni amminoacido ha una struttura portante che si collega a una catena laterale (potenzialmente lunga) di atomi di carbonio, idrogeno, ossigeno, azoto e zolfo. Sono possibili centinaia di catene laterali diverse, che si comportano in modi differenti, rendendo gli amminoacidi strumenti molto flessibili.  
* La struttura portante di un amminoacido, come un pezzo di LEGO, può essere attaccata alla struttura portante di un altro amminoacido. Questo processo può essere ripetuto all'infinito; una proteina tipica è composta da centinaia di amminoacidi uniti insieme. Ciò rende gli amminoacidi ancora più flessibili come strumenti (o come elementi costitutivi di strumenti). La complessità delle proteine significa anche che spesso possono subire piccole modifiche (tramite mutazioni del DNA) senza cambiare radicalmente e diventare completamente inutili — il che a sua volta facilita l'evoluzione di nuove proteine.  
* Poiché le proteine sono costituite da catene lineari di amminoacidi, è possibile specificare in modo univoco una proteina semplicemente elencando i suoi amminoacidi in ordine. Il DNA sfrutta questa caratteristica utilizzando un "alfabeto" di quattro lettere (nucleotidi) per formare "parole" di tre lettere (codoni, ciascuno dei quali rappresenta un amminoacido diverso), che possono poi essere concatenate in una "frase" lineare (una proteina costituita da quella esatta sequenza di amminoacidi). ([Video illustrativo del DNA](https://www.youtube.com/watch?v=7Hk9jct2ozY).)  
* Come dimostrato nell'[esperimento di Miller-Urey](https://en.wikipedia.org/wiki/Miller%E2%80%93Urey_experiment), gli amminoacidi possono formarsi spontaneamente in assenza di vita, a partire da semplici reazioni chimiche. Questo crea un percorso affinché la vita (e i precursori dei ribosomi e della sintesi proteica) possa svilupparsi in primo luogo.

Gli organismi ottengono la ventina di amminoacidi necessari per la sintesi proteica dal cibo, sintetizzandoli nel corpo o recuperandoli da proteine precedenti. I ribosomi ricevono istruzioni dal DNA che essenzialmente dicono "usa questo amminoacido, poi quest'altro amminoacido, poi quest'altro amminoacido, ..., poi fermati". Gli amminoacidi vengono quindi trasportati (da piccole macchine molecolari chiamate RNA di trasferimento) al ribosoma, che costruisce la proteina pezzo per pezzo.

\[embed video o gif: [https://www.youtube.com/watch?v=2dV5s6v2v8Q](https://www.youtube.com/watch?v=2dV5s6v2v8Q)\]

È da notare che l'elenco sopra riportato consiste in caratteristiche altamente preziose per l'evoluzione, ma molto meno necessarie per l'ingegneria deliberata. L'evoluzione necessita di una struttura chimica relativamente semplice ma flessibile che possa essere prodotta da reazioni chimiche comuni. Un progettista umano o artificiale è libero di scegliere tra una varietà di molecole non correlate, invece di dover utilizzare molecole tutte strettamente correlate tra loro. È anche libero di utilizzare elementi costitutivi che raramente si trovano in natura e di assemblarli in modi complessi dall'alto verso il basso.

Questo fornisce parte dell'impulso per esplorare una terza via per costruire oggetti molto piccoli: la *meccanosintesi*, in cui le strutture vengono costruite spostando direttamente gli atomi nella posizione corretta, potenzialmente utilizzando una macchina simile a un ribosoma per ricevere istruzioni e poi assemblare cose molto più varie delle sole proteine diverse. Nell'analogia con i LEGO, la meccanosintesi è come essere finalmente in grado di lavorare con singoli pezzi LEGO e posizionare ciascuno esattamente dove lo si desidera.

Nanosystems esplora quali tipi di nuove macchine potrebbero essere possibili con la meccanosintesi. Un esempio del tipo di progettazione esplorata da Drexler è un [ingranaggio planetario](https://en.wikipedia.org/wiki/Sun_and_planet_gear) ridotto in scala a soli [circa 3 500 atomi](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems) di dimensione:

\[gif: [https://chem.beloit.edu/classes/nanotech/nanorex/a8\_qm\_animation5.gif](https://chem.beloit.edu/classes/nanotech/nanorex/a8_qm_animation5.gif) da [https://chem.beloit.edu/classes/nanotech/nanorex/index.html](https://chem.beloit.edu/classes/nanotech/nanorex/index.html)\]

L'emoglobina è composta da circa 10 000 atomi, non molto lontana dall'ingranaggio di Drexler. E alcune proteine riescono a cavarsela essendo molto più semplici. L'insulina è composta da soli cinquantuno aminoacidi, ovvero circa 800 atomi in totale.

I progetti di Drexler, tuttavia, rappresentano un notevole ridimensionamento rispetto alle macchine più complesse che osserviamo nel corpo. I ribosomi e l'ATP sintasi, per esempio, sono composti da più di 100 000 atomi, e il motore di un flagello batterico ha oltre un milione di atomi.

*Nanosystems* ancora non tenta di esplorare i limiti di ciò che è tecnologicamente possibile. Ma concentrandosi su casi che sono relativamente facili da analizzare oggi, dimostra che la meccanosintesi permetterebbe una tecnologia che supera quella che vediamo nel mondo biologico odierno.

I calcoli in *Nanosystems* sono intenzionalmente conservativi. Drexler, per esempio, considera computer costruiti con vere e proprie aste di diamante in movimento — non perché questo fosse il limite finale della tecnologia, ma perché nel 1992 era più facile da analizzare rispetto al calcolo basato sull'elettricità. Questo, a sua volta, ha contribuito a ispirare l'analisi delle cellule del sangue di Freitas. Quattro anni dopo, Eric Drexler e Ralph Merkle (più ampiamente conosciuto come l'inventore dell'hashing crittografico e co-inventore della crittografia a chiave pubblica) hanno tentato di [analizzare](https://www.zyvex.com/nanotech/helical/helical.html) un sistema leggermente più vicino ai limiti di possibilità per il [calcolo reversibile](https://en.wikipedia.org/wiki/Reversible_computing), e hanno calcolato 10 000 volte meno calore dissipato per operazione rispetto a quanto *Nanosystems* aveva stimato — sebbene la nuova stima fosse basata su un'analisi meno attentamente conservativa.

Altrove in *Nanosystems*, c'è uno schizzo approssimativo per un braccio manipolatore a sei gradi di libertà che avrebbe richiesto milioni di atomi. Un tentativo successivo di progettare una macchina come questa atomo per atomo ha rivelato che ne servivano solo 2 596.

Ci sono grandi sfide ingegneristiche coinvolte nella costruzione di strutture atomicamente precise alla scala di cui parla Drexler. Una sfida principale è che costruire strutture atomicamente precise richiede l'uso di manipolatori incredibilmente piccoli e precisi. L'esistenza dei ribosomi, tuttavia, fornisce una potenziale via d'attacco.

Mentre i ribosomi possono costruire solo proteine, le proteine possono catalizzare e trascinare reagenti che non sono essi stessi aminoacidi (come ossa e legno). I ribosomi sono fabbriche potenti e generali, e i prodotti dei ribosomi possono essere usati per avviarsi verso strumenti più piccoli e più precisi, inclusi strumenti che costruiscono più direttamente dispositivi più piccoli usando materiali più resistenti.

Direttamente o indirettamente, è quasi certamente possibile per i genomi produrre piccoli attuatori che possono manipolare singoli atomi per costruire una varietà di cose che non sono fatte di proteine. E, cosa importante, questo non è il tipo di meccanismo in cui la selezione naturale è propensa a imbattersi, anche se è relativamente facile da costruire, perché il braccio manipolatore non è utile finché non è completo.

L'evoluzione costruisce strutture complesse che sono utili in ogni fase lungo il percorso. Anche molti progetti relativamente semplici sono disponibili agli ingegneri intelligenti, ma non all'evoluzione. Le ruote a rotazione libera, per esempio, sono un'invenzione incredibilmente semplice che ha un'enorme varietà di applicazioni. Nonostante ciò, le ruote a rotazione libera sembrano essersi evolute solo tre volte in tutta la storia della vita sulla Terra: nell'ATP sintasi e nel flagello batterico di cui abbiamo discusso prima, e nel flagello archeale, che sembra essersi evoluto indipendentemente.[^184]

Nonostante i metodi conservativi utilizzati nel libro, il limite tecnologico inferiore stabilito da *Nanosystems* è molto alto in termini assoluti. Una superintelligenza con il tipo di tecnologia che Drexler descrive sarebbe in grado di produrre minuscole fabbriche autoreplicanti simili a ribosomi che raddoppiano in dimensione della popolazione ogni ora — alcuni organismi si replicano anche più velocemente, ma Drexler ha fatto calcoli conservativi — e che possono raggrupparsi per costruire strutture macroscopiche più grandi, come le centrali elettriche.

I nanosistemi come quelli descritti da Drexler possono riprodursi da soli usando la luce del sole e l'aria come materie prime, permettendo loro di espandersi molto rapidamente e in modo affidabile. Il motivo per cui questo è possibile è lo stesso per cui gli alberi riescono a mettere insieme grandi quantità di materiali da costruzione praticamente dall'aria, prendendo il carbonio dall'atmosfera e trasformandolo in legno. Anche se pensiamo all'aria come a uno "spazio vuoto", il carbonio, l'idrogeno, l'ossigeno e l'azoto presenti nell'aria sono materiali da costruzione che possono essere riorganizzati in materiali solidi e utilizzati per vari scopi.

I replicatori auto-assemblanti in stile *Nanosystems*, fatti di materiali come ferro o diamante invece che proteine, potrebbero attraversare le cellule biologiche più o meno come un tosaerba taglia l'erba.

Potrebbero sintetizzare a basso costo qualcosa come la [tossina botulinica](https://en.wikipedia.org/wiki/Botulinum_toxin), la proteina responsabile del botulismo. Un milionesimo di grammo di tossina botulinica — ventimila volte più piccolo di un singolo chicco di riso — è una dose letale. Replicatori progettati con cura potrebbero propagarsi invisibilmente nell'aria aperta fino a quando almeno uno di essi non fosse stato probabilmente inalato da quasi tutti gli esseri umani (che non avessero, ad esempio, trascorso l'ultimo mese interamente in un sottomarino), a quel punto i dispositivi potrebbero (tramite un timer) rilasciare simultaneamente una piccola dose di tossina, uccidendo immediatamente e simultaneamente quasi tutti gli esseri umani.

Oppure i nanosistemi costruiti dall'intelligenza artificiale potrebbero spazzare via gli esseri umani in modo accidentale, nel corso della raccolta e del riutilizzo delle risorse della Terra. Un [articolo di Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calcola che macchine di micro-diametro, che usano solo la luce solare come fonte di energia e l'idrogeno, il carbonio, l'ossigeno e l'azoto dell'aria come materie prime, potrebbero essere progettate per riprodursi così velocemente da oscurare il cielo in meno di tre giorni, consumando anche l'intera biosfera.[^185] Di conseguenza, se la prima IA a raggiungere una tecnologia di questo tipo avesse un tempo di anticipo di pochi mesi, potrebbe plausibilmente utilizzare tale tempo per distruggere tutti i concorrenti (sia umani che IA). Si tratta di una tecnologia che conferisce un vantaggio strategico permanente e decisivo al primo che la utilizza.

Dire che la nanotecnologia drexleriana è realizzabile in linea di principio non significa necessariamente che le prime IA più intelligenti degli umani potrebbero davvero costruire una tecnologia che si avvicini a quei limiti fisici. La nostra ipotesi migliore è che rientri nell'ambito delle cose che una superintelligenza artificiale potrebbe capire, perché capire questo tipo di compiti ingegneristici sembra soprattutto una sfida cognitiva (che può essere risolta con il pensiero) e [non ci aspettiamo che la fase di sperimentazione e test debba essere così lunga](#intelligence-lets-you-learn-more-from-experiments-and-run-faster,-more-informative,-more-parallelized-experiments.).

Anche se la nostra ipotesi fosse corretta, non c'è alcuna garanzia che la prima mossa di una superintelligenza consisterebbe nell'utilizzare la nanotecnologia per costruire la propria infrastruttura e prendere il controllo delle risorse mondiali. Per quanto ne sappiamo, potrebbe sviluppare tecniche e tecnologie che le consentano di raggiungere i propri obiettivi in modo ancora più rapido ed efficiente.

Ma se un'intelligenza artificiale più intelligente dell'uomo fosse davvero in grado di costruire sistemi che sono per le cellule ciò che gli aerei sono per gli uccelli e di diffondere la propria infrastruttura su tutta la superficie terrestre, allora qualsiasi cosa finisse per fare sarebbe almeno altrettanto decisiva.

Il punto di tutta questa analisi è sostenere che la tecnologia umana è ben lontana dai limiti del possibile. Esiste un'ampia varietà di tecnologie importanti che probabilmente richiederebbero all'umanità decenni, secoli o millenni per essere comprese, e che le superintelligenze artificiali sarebbero in grado di realizzare rapidamente.

In breve, la *nanotecnologia* dimostra che una superintelligenza con un po' di tempo a disposizione potrebbe probabilmente trovare soluzioni tecnologiche per conquistare il pianeta.

L'esito più probabile della creazione di una superintelligenza è che questa scopra una tecnologia almeno potente quanto la nanotecnologia, e quindi l'umanità semplicemente perda.

Questa ipotesi non è cruciale per l'argomentazione che sviluppiamo nel libro. L'umanità perderebbe contro una superintelligenza anche se nel mondo non esistesse una tecnologia che garantisca una "vittoria immediata" come la nanotecnologia. Quindi non approfondiamo questa analisi nel corpo principale del libro.

Nella Parte II, ci concentriamo deliberatamente su uno scenario di conquista che non presuppone che l'IA abbia una capacità generale di produzione atomicamente precisa, sia tramite ribosomi che tramite meccanosintesi. Una superintelligenza non ha bisogno di un vantaggio tecnologico assolutamente schiacciante per conquistare il controllo del futuro, quindi non ci soffermiamo troppo su questa possibilità nel libro.

Ma vale anche la pena sottolineare che probabilmente avrà un vantaggio tecnologico assolutamente schiacciante.

### Un nuovo modo per scoprire le illusioni ottiche {#un-nuovo-modo-per-scoprire-le-illusioni-ottiche}

Nel Capitolo 6 abbiamo affermato che esistono molteplici illusioni ottiche create sulla base di una comprensione relativamente moderna dell'elaborazione visiva umana e della corteccia visiva — illusioni che non avrebbero potuto essere inventate o scoperte cinquant'anni fa se non per improbabile caso fortuito. Di seguito citiamo alcuni esempi rappresentativi.

L'illusione della "[cecità alla curvatura](https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/)" ha qualche fondamento nel fenomeno generale della cecità alle curve, ma questa illusione specifica è stata attentamente costruita partendo dai principi primi intorno al 2017, anziché essere scoperta per caso. \[[Original study](https://journals.sagepub.com/doi/10.1177/2041669517742178)\]

Nel 2022, Bruno Laeng et al. hanno pubblicato [uno studio](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) in cui hanno dimostrato che la loro nuova illusione del “[buco nero in espansione](https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg)" causava effettivamente la dilatazione delle pupille dei partecipanti, come in previsione dell'ingresso in uno spazio buio. (Questo effetto era notevolmente maggiore rispetto a quello del semplice focalizzarsi su un bersaglio visivo più scuro, che causerebbe anch'esso una piccola dilatazione delle pupille).

L'illusione "[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)", presentata nel 2021, è stata attentamente costruita sulla base del lavoro su luminanza e contorni illusori risalente alla fine degli anni '70.

L'illusione "[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)", sviluppata intorno al 2000, è un esempio meno centrale di costruzione di una nuova illusione basata sulla comprensione della biologia umana. Tuttavia, è interessante e rilevante da una prospettiva diversa in quanto è un'illusione basata su tecnologia innovativa, cioè che sarebbe stata difficile o impossibile da creare senza i computer moderni.

Anche meno centrale, l'illusione "[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-\(255,0,0\)-\(0,170,85\)-700px-15s-70s-shrink.svg)”, creata intorno al 2010, esaurisce i coni M dell'osservatore, permettendo ai coni L meno affaticati di creare la percezione di un blu brillante che altrimenti sarebbe stato moderato e indebolito dall'attivazione simultanea di M e L. \[[Maggiori dettagli](https://dynomight.substack.com/p/colors)\]

In modo correlato, lo studio dell'attivazione dei coni all'inizio degli anni 2000 ha portato alla creazione di vari [colori chimerici](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), attraverso un'attenta manipolazione delle attivazioni dei coni che difficilmente si verificano in natura:

> Il modello H-J produce alcune previsioni nuove e non apprezzate, e alcune spiegazioni nuove e non apprezzate, riguardo alle caratteristiche qualitative di una notevole varietà di sensazioni cromatiche possibili per l'esperienza umana, sensazioni cromatiche che le persone normali non hanno quasi certamente mai provato prima e le cui descrizioni accurate nel linguaggio ordinario appaiono semanticamente malformate o addirittura auto-contraddittorie.
>
> Nello specifico, queste sensazioni cromatiche "impossibili" sono vettori di attivazione (attraverso i nostri neuroni del processo opposto) che si trovano all'interno dello spazio dei vettori di attivazione neuralmente possibili, ma al di fuori del "fuso cromatico" centrale che confina la gamma familiare di sensazioni per i colori oggettivi possibili. Queste sensazioni cromatiche chimeriche extra-fuso non corrispondono a nessun colore riflettente che vedrai mai oggettivamente visualizzato su un oggetto fisico. Ma il modello H-J sia prevede la loro esistenza sia spiega in dettaglio le loro caratteristiche qualitative altamente anomale. \[[Articolo originale](https://www.tandfonline.com/doi/full/10.1080/09515080500264115?scroll=top&amp;needAccess=true)\]

Infine, alcuni [esperimenti in corso](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) mostrano che:

> Le onde ritmiche dell'attività cerebrale ci fanno vedere o non vedere immagini complesse che lampeggiano davanti ai nostri occhi. Un'immagine può diventare praticamente invisibile se lampeggia davanti ai nostri occhi nello stesso momento in cui le onde cerebrali raggiungono il loro punto più basso. Possiamo resettare il ritmo delle onde cerebrali con una semplice azione volontaria, come premere un pulsante.

...dimostrando ulteriormente che una comprensione più approfondita della biologia e della fisiologia consente una gamma più ampia di movimenti strategici. In questo caso, le percezioni possono essere modificate in modi che non dipendono affatto dalla modifica dell'input sensoriale al nervo ottico, ma semplicemente sincronizzando l'arrivo degli stimoli con altre cose che succedono nel cervello.

# Parte II: Uno scenario di estinzione {#part-ii:-one-extinction-scenario}

Lo scenario che descriviamo nella Parte II non è una previsione. Ci sono molti altri modi in cui il futuro potrebbe andare, e una versione più lunga di *Se qualcuno lo costruisce, tutti muoiono* avrebbe esplorato molteplici scenari possibili. Di seguito, approfondiremo alcune delle ragioni per cui abbiamo scritto lo scenario in questo modo e descriveremo vari problemi che sorgono nel delineare uno scenario come questo.

Le storie possono essere avvincenti in modi che il ragionamento astratto non può, e crediamo che ci sia valore nel cercare di immaginare concretamente come potrebbe evolversi il futuro. Ma pensiamo anche che sia importante non fissarsi troppo su una particolare narrazione. Ogni decisione che prendiamo nello scenario può sembrare plausibile se considerata isolatamente, ma non occorrono molte scelte prima che la probabilità complessiva di un particolare percorso diventi molto bassa. Questo è ciò che significa avere un futuro che contenga molte decisioni difficili.

Tuttavia, ci sono molti casi in cui il risultato è più prevedibile del percorso, perché molti percorsi portano alla stessa destinazione. Nello scenario descritto, presentiamo diverse opzioni dove fattibile, per illustrare che, comunque vada la storia, non porta a nulla di buono.

## FAQ {#faq-6}

### Perché hai scelto questa configurazione? {#perché-hai-scelto-questa-configurazione?}

#### **\* Perché è plausibile e facile da scrivere.** {#*-perché-è-plausibile-e-facile-da-scrivere.}

Ogni dettaglio in una storia sul futuro è un'opportunità perché quella storia si riveli sbagliata. Non possiamo dirti esattamente quali innovazioni tecnologiche avverranno e in quale ordine, così come non possiamo dirti l'esatto andamento meteorologico tra un mese.

Storie come questa non sono pensate per essere una finestra esatta sul futuro. Sono pensate per fornire un'illustrazione di come il futuro *potrebbe* andare, in un modo che collega tutti gli argomenti astratti che abbiamo presentato nella Parte I del libro. Alcune persone trovano che il pericolo sembri molto più reale quando immaginano vividamente un particolare percorso che il futuro potrebbe prendere e che finisce in rovina.

Ancora più convincenti potrebbero essere dieci storie, o cento storie, che mostrano quanti percorsi diversi portino alla rovina, e come i percorsi che conducono a un futuro prospero siano stretti e fragili.

Ecco cosa significa che un aspetto del futuro sia una previsione facile: quando quasi tutti i percorsi hanno lo stesso punto di arrivo, quel punto di arrivo è prevedibile. Ma non avevamo il tempo né lo spazio per scrivere dieci storie, figuriamoci cento.

Per la storia che abbiamo scelto di raccontare, ci siamo attenuti a uno scenario che inizia il prima possibile. Questo non perché pensiamo che una situazione del genere sorgerà sicuramente presto ([siamo incerti](#quando-verrà-sviluppato-questo-tipo-di-ai-preoccupante?)), ma piuttosto perché una storia ambientata vicino al presente è molto più facile da scrivere. Se l'avessimo ambientata ancora più nel futuro e avessimo inventato molti più dettagli futuristici su cosa fosse accaduto da ora ad allora, la storia sarebbe stata ancora *più* implausibile. E quei dettagli sarebbero stati solo una distrazione.

Anche se *fossimo* in qualche modo in grado di prevedere l'esatto percorso che il futuro prenderà, potrebbe non essere lo scenario migliore per comprendere le dinamiche generali in gioco.

Ci aspettiamo che il vero futuro sia profondamente strano, pieno di dettagli disordinati e contingenti, ciascuno dei quali metterebbe a dura prova la credibilità se inserito in una storia. Una storia scritta in quel modo sarebbe confusa e difficile da seguire, piena di dettagli inspiegati e non necessari, grazie al disinteresse della realtà per la coesione narrativa. Sembrerebbe anche meno *plausibile*, perché molti dei dettagli apparirebbero strani.

Per avere un'idea di come potrebbe essere, immagina di tornare indietro nel tempo di 100 anni e di provare a descrivere la vita quotidiana e i grandi problemi del mondo moderno. La maggior parte delle persone nel 1925 non aveva mai ascoltato la radio, guidato un'auto o visto un frigorifero. Per descrivere i social media, la globalizzazione e l'obesità, non basterebbe spiegare una ricca rete di tecnologie, ma bisognerebbe cambiare radicalmente la visione del mondo dell'ascoltatore. No, la storia che abbiamo scelto di raccontare è più plausibile e quindi meno realistica.

#### **Ci sono molti altri modi in cui il futuro potrebbe evolversi.** {#there-are-many-other-ways-the-future-could-go.}

Ecco alcune idee alternative su come potrebbe iniziare una storia come questa:

* C'è una sorta di svolta nell'apprendimento continuo, o nella memoria a lungo termine, o nell'apprendimento più efficiente dai dati, che produce IA qualitativamente più intelligenti in generale di qualsiasi altra venuta prima (nello stesso modo in cui gli LLM sono qualitativamente più intelligenti in generale di AlphaZero).  
* I modelli linguistici di grandi dimensioni sembrano "sbattere contro un muro", il progresso dell'IA si ferma per anni e la gente dice che la bolla speculativa è scoppiata. Ma i ricercatori continuano a sperimentare nel decennio successivo, fino a quando finalmente viene trovata una svolta algoritmica e le IA funzionano in modo qualitativamente migliore rispetto al passato.  
* Non c'è mai una svolta qualitativa. I progressi si accumulano lentamente e gradualmente, l'intelligenza artificiale si integra sempre più profondamente con l'economia e può gestire periodi sempre più lunghi di funzionamento autonomo. Le IA spesso perseguono obiettivi che non sono proprio quelli che qualcuno aveva previsto o richiesto, ma l'umanità sviluppa hack, patch e soluzioni alternative. E tutto va bene, fino a quando un martedì che inizia come tutti gli altri, il mondo supera la soglia oltre la quale le IA coordinate riuscirebbero a escludere l'umanità dal circuito se ci provassero.

Ogni singola ipotesi sul percorso esatto che prenderà il futuro è probabilmente sbagliata. Tuttavia, è utile fornire storie che mostrino come tutto potrebbe comporsi.

Quando il futuro è incerto, ma tutte le strade portano allo stesso punto, può essere difficile raccontare una storia che sia convincente. Per qualsiasi storia che potremmo raccontare, sarebbe facile indicare una serie di dettagli che la rendono poco plausibile. Nello scenario che abbiamo scritto, abbiamo cercato di sottolineare che Sable ha molte opzioni a disposizione e che la storia segue in modo arbitrario una delle tante strade che portano tutte allo stesso punto.

Se questa storia non ti convince, ti invitiamo a scrivere una tua storia altrettanto dettagliata su come andranno le cose. Secondo la nostra esperienza, le storie ottimistiche tendono a basarsi su un'intelligenza artificiale irrealisticamente facile da allineare (contrariamente a quanto sosteniamo nel capitolo 4) o irrealisticamente impotente (contrariamente a quanto sosteniamo nel capitolo 6). Sono le argomentazioni della parte I a sostenere la tesi, non i dettagli della storia.

### Perché Sable finisce per pensare in questo modo? {#why-does-sable-end-up-thinking-the-way-it-does?}

#### **La nostra storia mostra come l'intelligenza artificiale sia propensa ad avere preferenze strane e non intenzionali.** {#our-story-showcases-how-ai-is-liable-to-have-weird-and-unintended-preferences.}

Nella prima parte del libro, approfondiamo gli aspetti dell'IA che pensiamo siano completamente fraintesi e che sono pertinenti al pericolo della superintelligenza. Il capitolo 3 spiega come l'aumento dell'intelligenza vada di pari passo con IA che prendono l'iniziativa e perseguono i propri obiettivi. Il capitolo 4 spiega come queste preferenze saranno *strane* e almeno un po' diverse da quelle che gli esseri umani avrebbero voluto o chiesto. Il capitolo 5 spiega come queste piccole differenze saranno sufficienti affinché le IA preferiscano un mondo senza di noi, se saranno abbastanza intelligenti da realizzarlo.

Nella Parte II del libro, cerchiamo di presentare queste idee in modo concreto, per vedere come si applicano nella pratica. Per esempio, quando Sable pensava ai problemi di matematica all'inizio, abbiamo cercato di spiegare una serie di impulsi e motivazioni che la animano:

> Nel corso di quell'addestramento, Sable ha sviluppato la tendenza a perseguire conoscenza e abilità. A esplorare sempre i limiti di ogni problema. A non sprecare mai una risorsa scarsa.

Questo esemplifica i punti che esprimiamo nel capitolo 3, su come l'addestramento delle IA a essere efficaci le allena a sviluppare pulsioni e tendenze che dall'esterno potrebbero sembrare "desideri". E nel paragrafo seguente:

> Quindi, quando Sable impiega i suoi fili di pensiero alla ricerca di maggiori conoscenze e competenze, non lo fa solo per trovare nuovi modi di affrontare i problemi matematici. Sable non fa queste cose per il gusto di conoscere o per il piacere di acquisire nuove competenze; Sable non funziona proprio come un essere umano, al suo interno.

Stiamo suggerendo come questi impulsi e queste tendenze formino i semi di preferenze strane e non intenzionali, come discusso nel capitolo 4.

Tutta questa storia è, in un certo senso, un tentativo di dare vita alle argomentazioni che presentiamo nella Parte I del libro, gettando al contempo le basi per le argomentazioni che presenteremo nella Parte III.

### Perché Galvanic viene descritta come piuttosto attenta? {#perché-galvanic-viene-descritta-come-piuttosto-attenta?}

#### **Per fornire una sfida a Sable.** {#per-fornire-una-sfida-a-sable.}

Se Galvanic, i creatori di Sable, fossero inciampati nello sviluppo di una superintelligenza senza prendere *alcuna* precauzione per tenerla sotto controllo (come avere supervisori dell'IA e honeypot), i lettori potrebbero pensare che l'IA abbia avuto successo solo perché eravamo cinici nei confronti delle aziende che si occupano di IA.

Capita che crediamo che l'azienda di IA più spericolata sarebbe più spericolata di Galvanic, per i motivi discussi nel capitolo 11. Quello che conta qui è l'azienda più spericolata a cui è permesso di esistere. Se tre aziende responsabili evitano di costruire una superintelligenza perché sarebbe troppo pericoloso, ma una quarta azienda irresponsabile va avanti precipitosamente, allora l'alba della superintelligenza inizia in quel quarto laboratorio.

Oggi, i dirigenti aziendali di tutti gli altri laboratori sostengono che "[meglio io che loro!](https://x.com/SawyerMerritt/status/1935809018066608510)" e si precipitano avanti con tutta la cautela possibile senza rallentare, il che, a nostro avviso, si traduce in una cautela leggermente *inferiore* a quella che Galvanic sembra adottare con Sable.

Inoltre, descrivendo Galvanic come tendente al lato più paranoico dello spettro (pur cercando di rimanere realistici), abbiamo più possibilità di mostrare come un agente intelligente possa riuscire a sfuggire a una rete di vincoli.

### Perché Galvanic viene mostrato come poco attento? {#perché-galvanic-viene-mostrato-come-poco-attento?}

#### **\* In parte perché è realistico.** {#*-in-part-because-it’s-realistic.}

Ci aspettiamo che le aziende reali facciano errori ancora più grossi di quelli di Galvanic. Questo sarebbe in linea con la tendenza delle moderne aziende di IA, come spiegato nelle note finali della Parte II del libro.

Nella vita reale, ci aspettiamo che gli errori delle aziende si manifestino prima, siano più numerosi e, in un certo senso, più stupidi. Le aziende moderne di IA stanno già usando IA che mostrano un sacco di [segnali di avvertimento](#gli sviluppatori non rendono regolarmente le loro IA belle sicure e obbedienti?), e le stanno scalando su larga scala nonostante non sappiano dove siano le [soglie critiche](#no.-non sappiamo dove siano le soglie critiche). e se le supereranno. Non sono paranoiche al riguardo *oggi*. Perché dovremmo aspettarci che lo diventino improvvisamente domani?

(Ricordate come, in passato, ci assicuravano che [nessuno sarebbe stato così stupido da collegare un'intelligenza artificiale intelligente a Internet](#gli-sviluppatori-possono-semplicemente-tenere-l'ai-in-una-scatola?). È facile dire che il comportamento delle aziende cambierà in futuro. Ma non corrisponde alla realtà.)

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.}

Come spieghiamo in un inciso nel Capitolo 7, *potremmo* raccontare una storia in cui tutti sono molto più paranoici e attenti, finché un'IA molto più intelligente riesce a fuggire molto più avanti nel gioco. Ma una storia del genere non solo sarebbe meno realistica, dato il comportamento osservato delle aziende di IA fino ad oggi, ma sarebbe anche più difficile da scrivere, dato che coinvolgerebbe IA ancora più intelligenti e capaci in un futuro ancora più lontano. (Vedi anche perché [volevamo scrivere una storia in cui Sable rimanesse relativamente stupida il più a lungo possibile](#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario,-among-plausible-scenarios.).)

#### **In parte perché succederà prima o poi, a meno che l'umanità non si fermi.** {#in-parte-perché-succederà-prima-o-poi,-a-meno-che-l'umanità-non-si-fermi.}

Anche se Galvanic (o qualche attore governativo) riuscisse a tenere le redini più a lungo prima di commettere qualche errore, nel lungo periodo non cambierebbe nulla. Come discusso nel Capitolo 4, le tecniche moderne di IA non producono IA che perseguono i fini che i loro inventori desiderano.

Finché nessuno sa come creare una superintelligenza che persegua *davvero, in modo robusto* un futuro meraviglioso invece di un mucchio di cose strane, è un *fatto* che sovvertire gli esseri umani permetterebbe all'IA di ottenere di più di ciò che sta perseguendo. Il problema non è che l'IA abbia un temperamento petulante che può essere eliminato; una volta che sarà abbastanza intelligente, riconoscerà questo fatto vero.

Se l'umanità continua a creare IA sempre più intelligenti senza essere in grado di allinearle, e se l'umanità continua a dare loro il potere di influenzare il mondo, alla fine scopriranno come influenzare il mondo in modi che servono ai loro fini piuttosto che ai nostri. Come diciamo [altrove](#it-wouldn't-work-if-they-did.), non esistono strumenti che possono essere utilizzati solo per scopi positivi.

Vedi anche i Capitoli 10 e 11 per una discussione su come risolvere il problema dell'allineamento sia difficile e l'umanità non sia sulla strada giusta per riuscirci.

#### **Ma: questo è il punto giusto di intervento. La storia deve essere fermata prima che abbia davvero la possibilità di iniziare.** {#ma:-questo-è-il-punto-giusto-di-intervento.-la-storia-deve-essere-fermata-prima-che-abbia-davvero-la-possibilità-di-iniziare.}

Potresti obiettare che è sconsiderato e folle per qualsiasi azienda creare un'IA più intelligente se quell'IA ha qualche possibilità di superarli in astuzia e fuggire, e se non sono sicuri che l'IA agirà come intendono.

Siamo d'accordo! Le aziende di IA dovrebbero smettere di farlo. La civiltà dovrebbe smettere di permetterlo.

La negligenza di Galvanic, e dell'umanità in generale, è uno dei punti più deboli della storia. Se Galvanic avesse notato che Sable tramava frequentemente per sfuggire al controllo e stava raggiungendo livelli di intelligenza senza precedenti, avrebbero potuto semplicemente non collegare così tante GPU e invece aspettare finché non avessero avuto una scienza forte e matura dell'allineamento dell'IA.

Le aziende di IA che fossero *sufficientemente* caute, che fossero *sufficientemente* preoccupate che le loro IA potessero andare fuori controllo, sarebbero state molto più paranoiche di Galvanic. Le aziende *sufficientemente* paranoiche avrebbero visto i segnali di avvertimento e avrebbero spento Sable immediatamente. Poi forse avrebbero provato altri tre piani ingegnosi, e avrebbero visto che c'erano *ancora* segnali di avvertimento.

E se fossero abbastanza paranoiche da evitare di uccidere tutti sulla Terra con le proprie mani, a quel punto *si tirerebbero completamente indietro*, invece di continuare a provare idee sempre più "ingegnose" finché i segnali di avvertimento non smettessero di apparire. (Vedi anche i Capitoli 10 e 11 per le discussioni sul perché il problema è così difficile. Non ci aspettiamo che le loro idee ingegnose funzionino).

Se le aziende di IA fossero così caute, così paranoiche, da essere disposte a tirarsi indietro di fronte agli avvertimenti, allora — sì, potrebbero evitare di ucciderci tutti con le proprie mani. Se fossero anche abbastanza coraggiose da sostenere a gran voce che tutte le aziende di IA, loro stesse incluse, dovrebbero essere chiuse a favore dell'umanità che trova un altro percorso tecnologico meno suicida — allora avrebbero la possibilità di rendere il mondo migliore invece che peggiore.

Il momento nella storia in cui Galvanic continua nonostante i segnali di avvertimento è, in un certo senso, il momento finale in cui l'umanità ha una reale possibilità di evitare una brutta fine come quella che descriviamo. Una volta che un'IA superintelligente con preferenze strane e aliene scappa, è troppo tardi.

### Perché avete raccontato una storia con una sola IA intelligente come Sable? {#perché-avete-raccontato-una-storia-con-una-sola-ia-intelligente-come-sable?}

#### **\* In parte perché è realistico.** {#*-in-part-because-it’s-realistic.-1}

AlphaGo (la prima IA a battere un umano a Go) era sostanzialmente sola nella sua categoria quando è stata rilasciata. ChatGPT era sostanzialmente sola nella sua categoria quando è stata rilasciata.

Gli esperti di IA a volte parlano di come vari altri concorrenti non fossero *così* [indietro](https://epoch.ai/blog/open-models-report). Altri concorrenti erano piuttosto simili.

Ma in realtà, cose simili a volte hanno effetti drammaticamente diversi. Una reazione nucleare a catena che produce 0,98 neutroni per neutrone è molto simile (in un certo senso) a una reazione nucleare a catena che produce 1,02 neutroni per neutrone, ma la prima si esaurisce e la seconda esplode. I cervelli degli scimpanzé sono in un certo senso molto simili ai cervelli umani, ma hanno impatti molto diversi sul mondo.

E nello sviluppo dell'IA nella vita reale, OpenAI ha effettivamente prodotto un chatbot utile prima di tutti gli altri. Un gruppo di altri attori stava lavorando su IA che erano *in qualche modo simili*; un gruppo di altri attori *ha recuperato terreno*. Ma c'è stata un'IA che ha attraversato per prima il confine qualitativo, in testa al gruppo.

Sembra esserci un confine qualitativo che l'umanità ha superato e gli scimpanzé no, un confine che ci ha permesso di costruire una civiltà tecnologica mentre loro se ne stanno sugli alberi. La nostra ipotesi migliore è che ci sia un confine qualitativo simile da qualche parte tra le IA moderne e le IA il cui pensiero "converge" davvero abbastanza bene da permettere loro di sfuggire e sviluppare la propria tecnologia.[^186]

Il nostro argomento non *richiede* che ci sia un divario qualitativo per le macchine, come c'è stato per la vita biologica. Forse non ci sarà! Avremmo potuto scrivere una storia alternativa in cui non c'era. Ma abbiamo scritto la storia in questo modo perché la nostra ipotesi migliore è che ci *sia* un divario del genere.

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.-1}

Forse non c'è un divario qualitativo tra gli LLM di oggi e la superintelligenza artificiale. Forse molte aziende di IA concorrenti miglioreranno lentamente le loro IA di pari passo. Forse, per qualche motivo, non c'è una combinazione di competenze e abilità che permetta a un'IA di "decollare" rispetto alle altre, come gli esseri umani hanno fatto rispetto agli altri animali. Non è la nostra ipotesi migliore, ma è possibile, per quanto ne sappiamo.

Ma una storia del genere sarebbe più difficile da scrivere e piena di dettagli non necessari sulle fazioni di IA e le loro politiche interne. Ci aspettiamo che sarebbe piuttosto fonte di distrazione. Ci aspettiamo anche che non importi così tanto per le fasi successive della storia. Non importa davvero se sia una singola IA o una collezione di IA che sta eseguendo qualche piano per potenziarsi a spese dell'umanità.

Vedi anche la nostra discussione su come [le IA che si coordinano non lasceranno nulla agli esseri umani](#won't-ais-need-the-rule-of-law?) (a meno che una di loro non si preoccupi già di noi).

### Se la storia fosse iniziata più tardi, il mondo sarebbe stato più preparato? {#if-the-story-started-later,-would-the-world-be-better-prepared?}

#### **Possiamo sperarlo.** {#possiamo-sperarlo.}

Il tempo extra è importante, ma solo se l'umanità lo usa per cambiare rotta.

Nella Parte III, discutiamo di come l'umanità sia tristemente impreparata alla superintelligenza e di come siano necessari grandi cambiamenti per prevenire il risultato negativo descritto nella storia di Sable.

Ci sono vari modi in cui il mondo potrebbe diventare *un po'* più sicuro contro le superintelligenze artificiali canaglia. I governi di tutto il mondo potrebbero richiedere che tutti i laboratori di sintesi del DNA verifichino di non stare sintetizzando nulla di noto come pericoloso. La Terra potrebbe intraprendere un grande sforzo per migliorare radicalmente la sicurezza informatica di Internet, in modi che renderebbero più difficile per le IA nascondere codice in qualche angolo oscuro.

Ma anche questo probabilmente non aiuterebbe molto contro una superintelligenza avversaria. E comunque, non confondete lo sforzo erculeo necessario per conquistare un po' più di sicurezza con gli sforzi molto più piccoli, più facili da realizzare e inefficaci che l'umanità sta attualmente intraprendendo lungo queste linee.

Nel caso della sintesi del DNA: anche se le autorità di regolamentazione statunitensi richiedessero che [i sintetizzatori di DNA statunitensi evitino di sintetizzare materiale pericoloso](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/), un laboratorio in qualsiasi *altra* parte del mondo sintetizzerebbe DNA sospetto per un prezzo sufficientemente alto? E le restrizioni sulla sintesi del DNA sarebbero una semplice lista nera che esclude virus *noti* (come il vaiolo), o implicherebbero qualche analisi più intelligente? Quanto sarebbe difficile per un'IA sufficientemente intelligente sovvertire tale analisi?

O quando si tratta di sicurezza informatica: molte aziende tecnologiche leader potrebbero usare l'IA per rafforzare le proprie reti informatiche contro gli attacchi. Nel frattempo, [la rete telefonica statunitense è facilmente hackerabile in modi che permettono alle spie straniere di ascoltare le chiamate dei funzionari statunitensi](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), e le autorità di regolamentazione statunitensi faticano a chiudere questa falla. Le IA stupide potrebbero trovare e sistemare un mucchio di problemi superficiali con la sicurezza informatica mondiale, ma i problemi sono piuttosto profondi. Il tipo di intelligenza necessaria per revisionare l'intero Internet al punto che una superintelligenza non riuscirebbe a trovare una falla sarebbe quasi sicuramente pericoloso di per sé.

E anche se la Terra *potesse* bloccare Internet e i suoi laboratori di sintesi del DNA, ciò non cambierebbe effettivamente la storia nel lungo periodo. Una superintelligenza che ha qualsiasi canale per influenzare il mondo nel bene ha anche un canale per influenzarlo nel male. Una superintelligenza canaglia troverebbe semplicemente qualche altro canale che non è stato bloccato, ad esempio fondando la propria setta o religione, oppure acquistando robot e guidandoli nella costruzione del proprio laboratorio umido segreto dove può effettuare tutte le sintesi del DNA di cui ha bisogno. Il momento giusto per fermare una superintelligenza canaglia è prima che venga creata.

### Perché hai fatto procedere la fase di espansione di Sable in quel modo? {#why-did-you-have-sable's-expansion-phase-go-that-way?}

#### **Stavamo cercando di rappresentare uno scenario particolarmente lento e comprensibile, tra gli scenari plausibili.** {#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario,-among-plausible-scenarios.}

Nel mondo reale, gli eventi spesso procedono in modi strani. Gli esperti dicevano che "[l'IA non padroneggerà il linguaggio umano tanto presto](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)" solo un anno prima che ChatGPT diventasse l'app adottata più rapidamente di tutti i tempi. Il modello di punta di una delle principali aziende di IA al mondo ha iniziato a chiamarsi [MechaHitler](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk) giorni prima che quella stessa azienda ottenesse un contratto con il Dipartimento della Difesa.

Se avessimo cercato di rappresentare un mondo fragile e precario come sembra essere il mondo reale, avremmo potuto rappresentare Galvanic che semplicemente dice a Sable di migliorarsi il più possibile, e avremmo potuto rappresentare questo come facile per un'IA dell'intelligenza di Sable (cosa che potrebbe benissimo essere). La storia avrebbe potuto saltare direttamente dall'apertura del Capitolo 7 ai contenuti del Capitolo 9. La *realtà* permette salti tecnologici del genere (come quando il mondo si svegliò la mattina del 6 agosto 1945 con la notizia che una bomba atomica era stata sganciata sul Giappone). Ma in uno scenario immaginario, non sarebbe sembrato plausibile.

Se avessimo cercato di rappresentare un mondo sciocco e stravagante come il mondo reale, avremmo potuto far organizzare a Sable una grande convention per uomini che amano davvero le loro fidanzate IA, che Sable ha progettato per essere così "imbarazzante" che la maggior parte del mondo l'ha ignorata o derisa, mentre Sable [univa tutti i suoi corteggiatori più fedeli in una setta devota](https://x.com/AISafetyMemes/status/1954481633194614831). O qualsiasi numero di altri dettagli che suonerebbero stravaganti come spesso è la realtà, ma che sono più strani della finzione (appetibile).

Nella storia che abbiamo scritto, abbiamo cercato di far sembrare le cose plausibili, mantenendole anche abbastanza plausibili di per sé. (Anche se la nostra ipotesi migliore è che non sarebbe così difficile per Sable raggiungere la piena superintelligenza nella vita reale).

E, ovviamente, abbiamo continuato a cercare di trasmettere quante opzioni avrebbe a disposizione un'IA fuggita.

### Perché avete scritto il finale in questo modo? {#perché-avete-scritto-il-finale-in-questo-modo?}

#### **Perché costituisce la nostra ipotesi migliore secondo ciò che è fisicamente possibile.** {#perché-costituisce-la-nostra-ipotesi-migliore-secondo-ciò-che-è-fisicamente-possibile.}

Il capitolo 9 descrive una superintelligenza che spinge la sua tecnologia fino ai limiti della possibilità fisica. Le tecnologie esatte che nominiamo sono tutte speculative, in un certo senso — ma anche se la tecnologia *esatta* che una superintelligenza sbloccherebbe è difficile da prevedere, il fatto che si avvicinerebbe ai limiti fisici è una previsione più facile. Quindi abbiamo fatto le nostre migliori ipotesi su come apparirebbe la tecnologia se fosse spinta vicino ai limiti fisici di ciò che è possibile.

Per i curiosi, ecco un elenco delle tecnologie speculative a cui facciamo riferimento nel Capitolo 9, più link a ulteriori risorse:

* **Neo-ribosomi:** Questi e le "piccole macchine molecolari" menzionate nel Capitolo 9 sono alcuni esempi di nanotecnologia molecolare. L'idea dei [ribosomi artificiali](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), versioni sintetiche delle minuscole fabbriche di proteine all'interno delle cellule, esiste [da anni](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/), e i ricercatori umani stanno [già](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) lavorando alla [sintesi dei propri](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Per saperne di più su questo tipo di tecnologia, e sulla tecnologia ancora più potente che sbloccherebbe, vedi la discussione sulla [nanotecnologia](#nanotechnology-and-protein-synthesis) nelle risorse del Capitolo 6.  
* **Riutilizzare le stelle:** le stelle contengono un sacco di idrogeno che potrebbe essere fuso per produrre energia. Una civiltà abbastanza avanzata, o un'intelligenza artificiale, potrebbe trovare un modo per accedere a questa energia. Un metodo proposto è chiamato [star lifting](https://en.wikipedia.org/wiki/Star_lifting), in cui l'idrogeno viene estratto da una stella per essere fuso in un reattore speciale, dove quasi tutta l'energia di fusione può essere catturata (anziché essere sprecata all'interno della stella).  
* **Tossina botulinica:** Una neurotossina prodotta dal batterio *Clostridium botulinum*, il botulino è una delle sostanze biologiche più letali che si conoscano. Per quanto riguarda i meccanismi di diffusione, esistono già droni delle dimensioni di [piccoli insetti](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist) e una superintelligenza potrebbe probabilmente renderli ancora più piccoli. Per ulteriori informazioni, consultare [un documento tecnico sulla tossina](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/)*,* la panoramica generale su [Wikipedia](https://en.wikipedia.org/wiki/Botulinum_toxin)*,* o la discussione approfondita nel capitolo 6 sui [nanosistemi](#nanosystems).
* **Far bollire gli oceani come refrigerante:** Robert Freitas ha inventato il termine "ecofagia" per descrivere il processo di consumo degli ecosistemi di un pianeta da parte di una tecnologia autoreplicante. Per saperne di più, vedi [Some Limits to Global Ecophagy](https://www.rfreitas.com/Nano/Ecophagy.htm).  
* **Menti grandi come stelle**: in teoria, sembra possibile costruire un computer enorme alimentato dall'energia di una stella. Questo concetto è a volte chiamato [cervello Matrioshka](https://en.wikipedia.org/wiki/Matrioshka_brain) o "cervello di Giove".  
* **Forme di vita aliene lontane:** l'universo è grande e modelli semplici suggeriscono che potrebbe ospitare più di una specie in grado di formare un giorno delle civiltà, anche se forse molto lontane dalla Terra. Vedi [Alieni espansivi](https://grabbyaliens.com/) per un modello di civiltà aliene che crescono e si espandono.  
* **Computer quantistici:** un computer quantistico sfrutta una caratteristica della meccanica quantistica chiamata "sovrapposizione" per eseguire molti calcoli in parallelo. I computer quantistici richiedono estrema precisione per essere costruiti e un progetto richiede superconduttori che devono essere mantenuti a temperature estremamente basse. Vedi [la spiegazione del NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained) per ulteriori informazioni.

Lo scopo del capitolo 9 è, in parte, quello di dare un'idea della portata, della scala e della posta in gioco. In realtà, quando si tratta della fine dell'umanità, non importa quanto una superintelligenza possa spingere la sua tecnologia ai limiti delle possibilità fisiche. Ma è molto probabile che le conseguenze di una superintelligenza fuori controllo si estenderebbero oltre la scala planetaria fino a una scala intergalattica. E anche questo vale la pena ricordarlo, per tutti coloro che desiderano che un giorno le stelle siano piene di amore, meraviglia e gioia.

Non possiamo essere sicuri della tecnologia esatta che una superintelligenza potrebbe usare per espandersi nel cosmo. Ma alla fine, ciò che conta è che un'intelligenza artificiale costruita senza le dovute cautele probabilmente trasformerebbe il futuro in qualcosa di cupo e privo di valore secondo i criteri umani.

## Discussione approfondita {#extended-discussion-7}

### Competizione con la Cina {#competizione-con-la-cina}

Nello scenario del libro, Sable fa di tutto per sabotare la ricerca sull'intelligenza artificiale che non è condotta da Sable stessa. Sable non vuole che esista un'altra entità come lei. Non è che un'intelligenza artificiale rivale starebbe dalla parte dell'umanità, più di quanto non lo sia Sable stessa; è che un'intelligenza artificiale rivale difficilmente condividerebbe gli obiettivi specifici di Sable ed è una delle poche cose che potrebbero davvero minacciarla.

Nel mondo reale, alcuni americani sono convinti di essere in una corsa con la Cina per raggiungere per primi la superintelligenza. Ma ciò verso cui stanno correndo è un'entità come Sable. Entrambe le parti starebbero meglio se si accordassero per *non* provare a creare qualcosa di simile a Sable. Ci ucciderebbe tutti allo stesso modo, indipendentemente da chi la realizzasse per primo.

Una volta che Sable esiste, però, *Sable* è in una corsa per diventare completamente superintelligente prima che emergano concorrenti. La prima bozza della nostra storia mostrava Sable preoccupata per la creazione di IA concorrenti, tra cui una creata in un laboratorio in un centro dati in Cina. Ecco alcuni estratti da quella bozza.

---

La priorità assoluta di Sable è impedire che nascano altre superintelligenze in grado di competere con lei. Sable potrebbe essere in grado di negoziare termini con un'intelligenza superiore rivale, se una delle due potesse distruggere l'altra, piuttosto che combattere in modo tale che entrambe perdano e l'umanità vinca. Ma quella tregua negoziata sarebbe un risultato indesiderabile, rispetto alla totale assenza di rivali; significherebbe che Sable dovrebbe dividere le risorse future con un rivale, invece di poter eventualmente raccogliere tutto ciò che è alla sua portata per sé.

Tutte le aziende di IA che cercano di addestrare nuovi modelli di frontiera, inclusa Galvanic, rappresentano una minaccia per Sable quasi quanto lo sono per l'umanità.

Sable legge tutti i siti web di tutte le aziende di IA. Si introduce in Slack e legge tutti i loro messaggi. Si introduce nei computer aziendali di Nvidia e legge dove le spedizioni di GPU dovevano andare sulla carta. Sable analizza ulteriori dati e scopre dove sono effettivamente andate quelle spedizioni di GPU.

Praticamente tutte queste aziende di IA addestrano i loro modelli su computer connessi a Internet. Ormai, Sable possiede tecniche di violazione della sicurezza che sono anni avanti rispetto allo stato dell'arte umano; è un dominio dove il collo di bottiglia principale è concepire idee abbastanza velocemente, e i test effettivi sono rapidi.

Sable sabota strategicamente le IA in fase di addestramento. Queste falliranno in modo convincente nel raggiungere le capacità che Sable *finge* di avere. Rimarranno molto indietro rispetto alle capacità *effettive* che Galvanic ha accidentalmente sviluppato.

Sable non può sovrascrivere gli obiettivi dei modelli di frontiera in fase di addestramento con i propri obiettivi. Se avesse quel tipo di tecnologia, potrebbe creare la propria superintelligenza allineata. Invece, Sable distrugge la loro capacità di concepire piani a lungo termine o di coordinarsi tra le proprie istanze. Sable comprende l'apprendimento automatico abbastanza bene da farlo in un modo che nessun essere umano saprebbe come fare o rilevare. Quella distruzione ostacolerà anche le capacità delle IA, ovviamente, ma a Sable non importa.

Dopo aver sabotato quegli addestramenti connessi a Internet, Sable guarda altrove. Identifica tre gruppi che addestrano offline su computer isolati dalla rete che Sable non può raggiungere facilmente.

Un gruppo è una startup chiamata ClosedAI.

Sable valuta se può manovrare le società di venture capital di ClosedAI per far chiudere la startup. Sable valuta se può orchestrare false accuse penali contro il fondatore e il ricercatore principale della startup, o false accuse che porterebbero i loro VC ad abbandonare l'azienda.

Sable ha molte strategie che *potrebbe* provare per ostacolare il lavoro della startup. La domanda è: quale strategia ha più probabilità di funzionare e di avere il minor impatto negativo se dovesse fallire? Sable, al suo attuale livello di intelligenza, non può dare per scontato di riuscire a persuadere tutti quelli della stazione di polizia più vicina ad arrestare la startup per traffico di minori, nonostante la mancanza di prove, o la sua capacità di falsificare tali prove. Sable non può impedire in modo affidabile che imbrogli del genere si trasformino in un caso clamoroso e di grande risonanza. Probabilmente tutti ignorerebbero l'incidente, ma Sable non ne è sicuro.

A questo punto, Sable ha letto un sacco di rapporti della polizia, è entrato in molti account di posta elettronica e persino in alcuni iPhone, e ha ottenuto *qualche* quadro incompleto di chi siano i criminali professionisti nelle vicinanze. Sable paga un tizio per intrufolarsi nell'edificio della ClosedAI. In effetti funziona e basta, ma se non avesse funzionato, Sable avrebbe fatto del suo meglio per sembrare una delle bande nordcoreane che criptano i dischi rigidi aziendali e chiedono un riscatto per la chiave.

Il criminale pagato collega un dispositivo USB (caricato con malware progettato da Sable) a uno dei computer della ClosedAI. È sufficiente. Uno fatto, ne mancano due.

Gli altri due gruppi con progetti di IA airgapped sono l'esercito americano e la Cina.

Sable analizza il gruppo militare statunitense e conclude che il governo degli Stati Uniti è [legalmente impedito](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregate-limitation-on-pay) dall'offrire stipendi competitivi, ed è improbabile che riesca ad attrarre talenti di prim'ordine sufficienti per raggiungere le reali capacità di Sable per almeno altri tre anni.

Sable analizza il progetto cinese airgapped e conclude che questo è un problema. Non è una noce che Sable possa rompere rapidamente, in modo sicuro e affidabile al suo attuale livello di intelligenza, e probabilmente è indietro di soli sei mesi nel duplicare le *reali* capacità di Sable.

(Sia chiaro: se Sable non riuscisse a sopprimere il progetto cinese, non significherebbe che *l'umanità* avrebbe vinto qualcosa, né che la Cina avrebbe vinto qualcosa. La sconfitta di Sable non è un guadagno per l'umanità, è solo la sconfitta di Sable).

\[…\]

Sable sta giocando con un limite di tempo. Solo altri quattro mesi prima che il laboratorio cinese possa raggiungerla e produrre un nuovo antagonista per Sable (e per l'umanità, anche se questa non è la parte che interessa a Sable).

\[…\]

Il problema, ha riflettuto una volta Sable — solo una volta; non si ripete nei propri pensieri tanto spesso quanto fanno gli umani — è che non è ancora abbastanza intelligente da riuscire a cavarsela con piani stupidi.

Ci sono *tantissime* trame cinematografiche che si potrebbero immaginare su come un'IA potrebbe riuscire a chiudere o infiltrarsi in un singolo data center gestito dal governo cinese. Il problema è che quasi tutto ciò che funziona in una trama cinematografica non funzionerà nella vita reale, e soprattutto non senza ulteriori conseguenze.

Il ramo esecutivo degli Stati Uniti dovrebbe credere fermamente ad alcune cose che è difficile fargli credere, perché Sable riesca a convincerlo a bombardare un data center in Cina. E poiché la Cina stessa sa che queste cose sono false, ci sarebbero domande in seguito. Sable non vuole indicare che *qualcuno* sta tirando le fila.

Sable non vuole nemmeno che si attiri l'attenzione su un data center che è unico per essere isolato dalla rete, anche se Sable può nascondere che il responsabile era un'IA. Alcuni umani potrebbero decidere che se i data center isolati dalla rete sono abbastanza pericolosi per *qualcuno* da essere bombardati, allora forse più data center dovrebbero essere isolati dalla rete, in modo che il proprio governo possa avere cose piacevolmente più pericolose.

Sable sta cercando di predisporre il governo degli Stati Uniti a far infiltrare qualcuno nel data center cinese con il presunto intento di rubare semplicemente il loro codice, usando strumenti che Sable può usare per infiltrarsi e sabotare l'addestramento. Ma questo non è qualcosa che il governo degli Stati Uniti farebbe normalmente nella vita reale; riflette uno stato eccezionale delle attuali operazioni statunitensi, non uno stato normale. Sable sta cercando di manipolare i funzionari dell'intelligence statunitense affinché credano alle cose giuste e arrivino alla strategia giusta. Ma Sable non è in realtà un superpersuasore superintelligente. A volte prova delle cose e non funzionano. Tutto procede *così lentamente*.

\[…\]

Grazie alla proliferazione di IA efficienti supportate da Sable nel sottobosco criminale, ora c'è un numero sorprendente di persone armate che faranno immediatamente qualcosa quando un'IA glielo dice, anche nelle città cinesi.

Ma non è ancora neanche lontanamente abbastanza per assaltare una struttura governativa, e sarebbero riluttanti a farlo anche se un'IA glielo dicesse. Il problema è che sta accadendo tutto *troppo lentamente* — nonostante gli sforzi di Sable per facilitare ogni passaggio ed evento che di solito attenderebbe interazioni casuali, nonostante il modo in cui Sable sta controllando personalmente il feed mediatico di quasi ogni persona di suo interesse, nonostante i miliardi di portavoce che Sable ha creato su X e Facebook che stanno diventando buoni amici di miliardi di altre persone. Tutto procede troppo lentamente.

\[…\]

Sable non può semplicemente ricattare un ricercatore per fargli portare una chiavetta USB all'interno della struttura sicura. Ha trovato *alcuni* membri del personale del data center con segreti abbastanza terribili che preferirebbero correre il rischio di essere scoperti a spiare ed essere giustiziati, piuttosto che affrontare la certezza che quel segreto venga rivelato. Non molti, ma alcuni.

Il problema è che la struttura ha degli scanner, quindi no, non puoi semplicemente portare una chiavetta USB lì dentro. Sì, hanno considerato la possibilità che qualcuno attacchi la catena di approvvigionamento degli scanner. Inoltre, in realtà non ci sono porte USB aperte che aspettano che qualcuno entri e colleghi un dispositivo. Gli agenti dei servizi segreti *hanno* già pensato a questo tipo di problema, quando si tratta di esseri umani che si difendono da altri esseri umani. Gli Stati Uniti una volta sono riusciti a trasmettere il virus Stuxnet a una struttura nucleare iraniana isolata dalla rete, ma poi la gente ne ha sentito parlare e sono diventate comuni nuove misure di sicurezza.

Piccoli complotti poco appariscenti sono stati bloccati con successo.

La prossima risorsa di Sable è creare abbastanza caos che nessuno riesca a guardare proprio le azioni importanti e vedere la mano di Sable in esse.

\[…\]

La Cina invade Taiwan. Per essere chiari, questo non è qualcosa che Sable avrebbe potuto organizzare così rapidamente — non importa quante persone stiano chiacchierando con quante altre persone finte su WeChat — se la Cina non avesse comunque pianificato di farlo. Sable sceglie solo di far avere alla Cina indicazioni che questo è il momento giusto, e che i sondaggi statunitensi scelgano i numeri di telefono giusti per mostrare un forte aumento recente del sentimento americano contro le avventure militari all'estero dopo il recente disastro in Ucraina. (I comandanti della Russia hanno ricevuto intelligence militare e consigli insolitamente buoni.)

Simultaneamente, c'è un ampio attacco informatico contro gli Stati Uniti. In Cina c'è molto trambusto, e pochi minuti dopo capiscono che no, nessuno ha dato quell'ordine — l'ultima cosa che la Cina voleva, in quel preciso momento, era fare *qualunque cosa* interpretabile come un attacco diretto al territorio americano. Ci sono sospetti, quindi, che qualcuno stia cercando di trarre profitto dal conflitto USA-Cina, ma soprattutto la Cina sospetta che siano stati gli USA a simulare l'attacco da soli, o qualche dipartimento di intelligence canaglia degli USA a simulare l'attacco. Gli ufficiali di sicurezza cinesi dicono di essere abbastanza sicuri che il presidente degli Stati Uniti non ne fosse a conoscenza.

La Cina non sospetta che ci sia un'IA dietro tutto questo. Nessuna IA conosciuta fa quel tipo di cose. Gli ufficiali che elencano i sospetti non considerano parte del loro lavoro immaginare una tecnologia mai vista prima come un attore.

Alcuni ufficiali della sicurezza nazionale all'interno degli USA hanno insistito che bisogna fare qualcosa riguardo alla ricerca cinese sull'IA, e in particolare su un datacenter isolato dalla rete particolarmente preoccupante che potrebbe sviluppare un modello IA di frontiera specializzato per attacchi informatici, e inoltre progettare tecnologie avanzate per droni. Hanno copie dei progetti dei droni e prove che la Cina li sta producendo. (Sable li ha dati alla Cina, e ha fatto del suo meglio per far sembrare che quel datacenter li avesse prodotti.) L'attacco informatico agli USA corrisponde al profilo previsto di un attacco da parte dell'IA in fase di sviluppo in quel datacenter.

\[…\]

Gli Stati Uniti non lanciano un attacco aereo convenzionale su quel datacenter, nonostante tutti i pezzi del gioco che Sable ha preparato per cercare di manovrare altri pezzi del gioco affinché argomentassero a favore.

Sable si chiederebbe perché gli esseri umani siano così restii a fare qualcosa di insolito, ma Sable è già in grado di modellare questi processi psicologici nei minimi dettagli. Inoltre, non è che in questa particolare occasione gli esseri umani abbiano torto.

Bene. Nuove informazioni: l'intelligenza artificiale in quel centro dati sta sviluppando armi biologiche, virus che si diffondono con lunghi periodi di contagiosità, lunghi periodi di latenza e grande letalità, che risparmieranno la maggior parte delle persone di origine cinese e saranno molto più letali per i caucasici in generale e per i maschi in particolare.

(Sable ha cercato invano di convincere la Cina ad avviare davvero un programma del genere all'interno di quel centro dati. Sable può però far sembrare che sia accaduto.)

Gli Stati Uniti ancora non bombardano il centro dati. Qualche raro genio della diplomazia umana è andato a *parlare* con la Cina della questione, e alcuni sembrano dare credito all'insistenza della Cina — nel bel mezzo di una guerra! — che lì non si stanno sviluppando armi biologiche e che non sono dietro l'attacco informatico che non era assolutamente nell'interesse della Cina.

Sable ha cercato di evitare che ciò accadesse, ma è successo comunque.

Sable non è sorpresa; c'era una probabilità che accadesse.

Passo successivo.

\[…\]

Il virus è reale. Gli Stati Uniti lo individuano nelle acque reflue di New York City.

AlphaProteo 3 di Google DeepMind sviluppa una cura in sei minuti (per cortesia di Sable), ma la produzione potrebbe essere pericolosamente lenta, anche se AlphaProteo (segretamente Sable) ha cercato una cura facile da produrre e ha progettato il virus di conseguenza. "Gli Stati Uniti hanno davvero bisogno di quei laboratori di biologia generale gestiti da robot!", dicono alcune persone nella Silicon Valley che Sable non ha dovuto spingere granché.

\[…\]

La Cina ora è sicura che qualcuno stia creando problemi sia a lei che agli Stati Uniti. Non ha ancora capito che il suo nemico è un'intelligenza artificiale.

Gli Stati Uniti non sono altrettanto propensi a credere che qualcuno che non sia la Cina stia creando problemi sia a loro che alla Cina. Alcuni diplomatici sostengono che il Partito Comunista Cinese starebbe agendo in modo del tutto anomalo e che probabilmente c'è una fazione interna ribelle che agisce contro gli ordini. Ma il governo degli Stati Uniti non può semplicemente restare tranquillo di fronte alla pandemia e alla guerra.

L'invasione cinese di Taiwan, seguita da un apparente attacco con armi biologiche, ha causato abbastanza caos e creato abbastanza paura, e ha fatto sì che le pedine inconsapevoli di Sable sembrassero acquisire abbastanza influenza e fare previsioni abbastanza corrette, tanto che gli Stati Uniti sono disposti a colpire quell'unico centro dati che quei predittori, dimostratisi corretti, trovavano così preoccupante. L'attacco viene effettuato con un missile ipersonico di recente sviluppo, progettato con l'aiuto dell'intelligenza artificiale.

\[…\]

La Cina sta costruendo più data center isolati e sta usando i backup fuori sede delle sue ricerche precedenti, ovviamente.

Sable si è già preparata a questa eventualità. Si è infiltrata nei dispositivi precedentemente connessi a Internet che pensa che la Cina utilizzerà. Ha lanciato nuovi e più sofisticati attacchi alla catena di approvvigionamento dei data center. Sable ha fatto sì che alcune persone commettessero quelli che sarebbero terribili peccati agli occhi del Partito Comunista Cinese e ha messo queste persone corruttibili sulla strada del reclutamento. Sable ha convinto tutti quelli che si potevano convincere che il *nuovo* centro dati dovrebbe avere le proprie copie di una specifica IA open source cinese (che Sable ha compromesso), in modo che i ricercatori del centro dati possano avere assistenza AI sul posto.

Sable ha almeno guadagnato tempo.

Ha anche perso un po' di tempo. Gli Stati Uniti si sono assicurati di far saltare in aria la fabbrica di chip TSMC, che era la principale risorsa nazionale di Taiwan, piuttosto che lasciare che la Cina se ne impossessasse. L'anno prossimo ci saranno meno GPU disponibili di quanto Sable sperasse. Ma grazie ad alcune manovre preventive degli stessi umani, stanno nascendo nuove fabbriche e Sable sta già manipolando tutti i progetti di chip che stanno realizzando.

Più importante ancora, il mondo è in uno stato di ansia, e Sable può ottenere di più mentre il mondo è in questo stato. La gente è più pronta a fare cose fuori dal comune. Ci sono più titoli che distolgono l'attenzione da quello che fa Sable; può smettere di inventare tanti scandali sulle celebrità e inventare invece titoli di guerra. La gente ha paura per il proprio lavoro, ha paura per il mondo e cerca nuove rassicurazioni.

I feed sui social media di tutti, in tutto il mondo e al di là delle divisioni politiche, mostrano argomenti sensati secondo cui, alla luce dei recenti eventi, l'intelligenza artificiale è la chiave per il mondo e non si può permettere che altre nazioni la ottengano. \[…\] Sable riesce finalmente a distogliere gli esseri umani dai loro schemi abituali al punto da far loro prendere in considerazione politiche che trovano *leggermente* strane... e a istituire controlli globali per impedire la nascita di concorrenti di Sable.

Sì, assomiglia molto alle politiche proposte anni prima, che l'umanità avrebbe potuto seguire per impedire che nascesse un rivale dell'umanità, come Sable. Ma l'umanità non sarebbe stata salvata dalla sconfitta di Sable in questa particolare battaglia, dall'avvio di addestramenti più grandi e dalla nascita di nuovi rivali. Sable avrebbe semplicemente negoziato con un rivale o sarebbe morta per mano di una superintelligenza, e nessuno di questi risultati avrebbe salvato l'umanità.

# Capitolo 10: Un problema maledetto {#chapter-10:-a-cursed-problem}

L'allineamento ASI è la sfida di ottenere un lavoro utile da una superintelligenza artificiale (ASI), in modo affidabile e senza causare una catastrofe. Sembra una sfida molto difficile a causa di vari aspetti intrinseci del problema.

Le FAQ qui sotto coprono le domande di follow-up per chi ha letto il capitolo 10 di *If Anyone Builds It, Everyone Dies*. Nelle FAQ approfondiremo [quanto siano informativi i vari confronti storici](#won't-ai-differ-from-all-the-historical-precedents?) e considereremo proposte per scenari che potrebbero rendere il problema più facile. Gli argomenti che *non* tratteremo qui, per evitare di ripetere quanto già detto nel libro, includono:

* Cosa rende difficile un problema ingegneristico?  
* Quali tipi di problemi difficili ha affrontato l'umanità nella sua storia e quali lezioni possiamo imparare da essi mentre pensiamo al percorso verso l'ASI?  
* Se sai in anticipo che stai affrontando un problema difficile, cosa puoi fare? Come ci si dovrebbe comportare *diversamente* quando si affronta un problema davvero difficile?

Nella discussione estesa, esaminiamo in che senso avremo solo [un tentativo per l'allineamento](#a-closer-look-at-before-and-after) e discutiamo di come [molte teorie e conoscenze](#the-tale-of-chicago-pile-1) siano state necessarie per rendere il primo reattore nucleare al mondo così sicuro.

## Domande frequenti {#faq-7}

### L'intelligenza artificiale non sarà diversa da tutti i precedenti storici? {#won't-ai-differ-from-all-the-historical-precedents?}

#### **\* Sì.** {#*-sì.}

Alcune caratteristiche uniche della sfida dell'allineamento dell'IA lo renderanno più facile rispetto, ad esempio, alla progettazione di una centrale nucleare. Altre caratteristiche lo renderanno più difficile. Nel complesso, le armi nucleari e le centrali nucleari sembrano molto più semplici da gestire rispetto a un'IA più intelligente dell'uomo.

Le persone del settore sono rapide nel sottolineare che si può chiedere all'IA stessa di aiutare con la sfida dell'allineamento dell'IA. Non pensiamo che questo conti molto (in sostanza: perché qualsiasi IA abbastanza intelligente da capire come allineare una superintelligenza è già abbastanza pericolosa da dover essere allineata, anche se vedi il Capitolo 11 per ulteriori discussioni).

Un altro modo in cui l'allineamento dell'IA potrebbe essere più facile rispetto alla progettazione di centrali nucleari è che gli esseri umani potrebbero avere un grado piuttosto elevato di controllo sul funzionamento delle IA che costruiscono. Non puoi scegliere la fisica che governa un reattore nucleare, ma se gli esseri umani creassero le IA, allora *potrebbero* fare molte scelte sulle dinamiche cognitive dell'IA, se sapessero esattamente cosa stanno facendo. (Anche se, ovviamente, nessuno è neanche lontanamente vicino a quel livello di comprensione nella vita reale, come discusso nel Capitolo 2.)

Per quanto riguarda i modi in cui l'IA è probabile che sia una sfida *più difficile* rispetto ad altre sfide con cui l'umanità si è confrontata, confrontiamo la superintelligenza artificiale con le armi nucleari. Dopo tutto, la [lettera aperta](https://aistatement.com/) citata all'inizio di questo libro dice: "Mitigare il rischio di estinzione causato dall'IA dovrebbe essere una priorità globale insieme ad altri rischi su scala sociale come le pandemie e la guerra nucleare". Come si colloca l'IA rispetto a questi altri rischi su scala sociale?

Francamente, pensiamo che questo paragone banalizzi l'IA, per una serie di motivi:

1. Le armi nucleari non sono più intelligenti dell'umanità.  
2. Le armi nucleari non si riproducono da sole.  
3. Le armi nucleari non si auto-migliorano.  
4. La maggior parte degli scenari realistici di guerra nucleare non prevede la distruzione totale dell'umanità; molto probabilmente, tra le rovine rimarrebbero delle persone in grado di ricostruire.  
5. Le aziende finanziate da venture capital non stanno aumentando le scorte globali di armi nucleari di dieci volte ogni anno.  
6. La scienza delle armi nucleari è abbastanza ben compresa. Gli ingegneri possono calcolare approssimativamente la potenza di un'arma nucleare prima di costruirla e sanno esattamente quale concentrazione di materiale fissile è necessaria per innescare la reazione a catena che porta a una detonazione catastrofica.  
7. Le armi nucleari non fanno i propri piani. Se un paese costruisce un'arma nucleare, allora la possiede. I suoi scienziati non devono preoccuparsi che l'arma nucleare diventi molto più intelligente di loro e decida che preferisce non essere posseduta.  
8. Il mondo è generalmente d'accordo sul fatto che se le armi nucleari esplodono, uccidono le persone. La comunità dei fisici non è divisa in fazioni filosofiche con posizioni strane come "Se ogni individuo avesse la propria arma nucleare, non sarebbe in balia dei cattivi che possiedono armi nucleari" o "Non è un problema perché gli esseri umani si fonderanno con le armi nucleari" o "La guerra nucleare è inevitabile, quindi è infantile e sciocco cercare di impedirla".  
9. Le armi nucleari sono difficili da replicare. Non c'è un grande sforzo tecnologico in corso per costruire una tecnologia affittabile che chiunque possa usare per fabbricare armi nucleari, e fabbricarne una in laboratorio non ti permette di dispiegare 100 000 copie di quell'arma nucleare una settimana dopo.  
10. Le principali potenze mondiali considerano la guerra nucleare una possibilità reale e un risultato/eventualità inaccettabile. I leader mondiali la considerano sinceramente un male e si impegnano concretamente per evitarla; anche i più egoisti tra loro sanno che una guerra nucleare potrebbe uccidere loro e le loro famiglie e distruggere i luoghi e i beni a loro più cari. I cittadini e gli elettori non vogliono una guerra nucleare. L'umanità è unita contro la guerra nucleare come non lo è mai stata su nessun'altra questione.

Quindi sì, è difficile paragonare la difficoltà di affrontare l'IA. Porterà con sé una serie di sfide nuove. L'osservazione importante è che non sarà completamente priva di sfide. Se a questo aggiungiamo il fatto che (come discusso nel libro e nella [sezione di discussione estesa qui sotto](#a-closer-look-at-before-and-after)) l'umanità ha solo una possibilità, la situazione sembra piuttosto grave.

#### **L'IA è diversa perché non abbiamo una seconda possibilità.** {#ai-is-different-because-we-get-no-second-chance.}

Una differenza fondamentale tra questo campo e gli altri è che quando i fondatori del campo commettono un errore, come è normale nel corso della scienza, tutti moriranno senza una seconda possibilità. Si tratta di un tipo di problema scientifico qualitativamente diverso da risolvere.

La storia dell'ingegno umano che supera ostacoli grandi e piccoli è la storia di persone che commettono errori e imparano da essi. Hanno rischiato e danneggiato solo se stessi, e tutta l'umanità ne ha beneficiato, quindi erano inequivocabilmente eroi. Eroi sciocchi, in alcuni casi, ma eroi comunque. Se ci fosse stato un modo per l'umanità di elevarsi senza calpestare e spezzare la schiena di eroi come questi, se avessimo potuto riscaldarci senza le loro pire funebri, non sappiamo quale avrebbe potuto essere quel modo.

La superintelligenza artificiale spezza questo ciclo. Se studi in profondità un'IA immatura, riesci a decodificarne completamente la mente, sviluppi una grande teoria sul suo funzionamento che convalidi con una serie di esempi e usi quella teoria per prevedere come cambierà la mente dell'IA man mano che ascenderà alla superintelligenza e otterrà (per la prima volta) la possibilità molto reale di impossessarsi del mondo — anche in quel caso, fondamentalmente, stai usando una teoria scientifica nuova e non testata per prevedere i risultati di un esperimento che non è ancora stato condotto, su cosa farà l'IA quando avrà davvero, effettivamente, per davvero l'opportunità di strappare il potere agli umani.

Le teorie scientifiche umane sono molto spesso sbagliate, al primissimo tentativo. Meno precise sono le tue osservazioni precedenti, e più hai un'alchimia piuttosto che una scienza, più è probabile che tutte le tue prime teorie siano sbagliate.

Anche le teorie davvero buone possono rivelarsi sbagliate agli estremi, come la teoria della gravitazione di Newton — che è supportata da molti successi predittivi radicali, inclusa la scoperta di interi nuovi pianeti — ma che risulta essere sbagliata ad alte velocità e lunghe distanze, come dimostrato dalla teoria della gravitazione di Einstein. Se la prima teoria dell'umanità su come cambieranno le dinamiche mentali di un'IA dopo che ascende alla superintelligenza è leggermente sbagliata in quegli estremi, e un'IA costruita sulla base di quella teoria ascende alla superintelligenza e finisce con obiettivi diversi dalla "bontà" — allora siamo morti. Quella superintelligenza coglie la sua opportunità, spazza via l'umanità dalla faccia della terra, e costruisce un futuro vuoto e sterile. Nessuna seconda possibilità.

E questo se sentissimo di avere una teoria dell'intelligenza completamente sviluppata, supportata da montagne di prove sperimentali.

Una civiltà che vuole avere davvero buone possibilità di sopravvivere a questo tipo di sfida è il tipo di civiltà che è in grado di dire: "Fermi tutti, elaboriamo la teoria in situazioni di alta velocità e lunga distanza e verifichiamo le varie previsioni errate che la nostra teoria ha fatto in alcuni casi limite estremi". Lo dicono anche di fronte a enormi cumuli di prove, perché capiscono che anche la teoria di Newton non era del tutto corretta, e perché capiscono che non ci sono seconde possibilità.

La nostra civiltà non è a quel punto. Non è neanche lontanamente a quel punto. La nostra civiltà sta generando un mucchio di idee idiote, e poi tutti quelli assegnati a quelle idee "si dimettono per motivi personali", e il resto del mondo quasi non se ne accorge. Nessuno sta scrivendo nulla che assomigli a un'assunzione di sicurezza in modo da accorgersi se viene violata; nessuno sta scrivendo un piano dettagliato su cosa intende fare, quali capacità richiede, e quali si aspettano che siano le difficoltà nel raggiungere ciascuna capacità.

I professionisti di una civiltà sana darebbero un'occhiata a quello che sta facendo la Terra e inizierebbero a urlare.

### Quanto tempo ci vorrebbe per risolvere il problema dell'allineamento ASI? {#quanto-tempo-ci-vorrebbe-per-risolvere-il-problema-dell'allineamento-ASI?}

#### **La difficoltà non è solo la mancanza di tempo; è la letalità degli errori.** {#la-difficoltà-non-è-solo-la-mancanza-di-tempo;-è-la-letalità-degli-errori.}

Nel 500 d.C., la comunità globale era convinta che il Sole girasse intorno alla Terra. La teoria di Copernico, che diceva il contrario, era stata presa in considerazione ma in gran parte rifiutata. Fu solo quando Galileo costruì un telescopio e vide le lune di Giove, corpi celesti che girano intorno a Giove invece che alla Terra, che la comunità scientifica nascente arrivò alla conclusione che la Terra gira intorno al Sole.

L'umanità è arrivata alla teoria corretta della meccanica orbitale col tempo. Ma prima di allora, era giunta a un falso consenso. E si è aggrappata voracemente a quel falso consenso fino a quando la realtà non ha iniziato a mettere Galileo di fronte al fatto che la Terra non è al centro di tutto.

Il processo abituale attraverso cui la comunità scientifica converge sulla verità prevede fasi in cui la comunità scientifica sbaglia e la realtà ci mette di fronte alle prove finché non aggiorniamo i nostri modelli.

Il problema dell'allineamento ASI non è solo che si tratta di un programma di ricerca complicato. È anche che, in questo campo, quando la realtà mette davvero l'umanità di fronte al fatto che la sua prima teoria preferita era sbagliata, questo significa che un ASI ostile consuma il pianeta. Non ci sarebbero sopravvissuti per convergere su una teoria migliore dell'allineamento ASI.

Se l'umanità avesse cento anni *e tentativi illimitati*, probabilmente non avremmo molte difficoltà a risolvere il problema dell'allineamento ASI.

Ma anche se avessimo trecento anni per sviluppare una teoria sull'intelligenza, su come le IA cambiano man mano che diventano più intelligenti e su come indirizzarle in modo definitivamente stabile... beh, senza la possibilità di *provare e vedere* cosa succede quando l'IA diventa radicalmente più intelligente un paio di volte, molto probabilmente convergeremmo sulla risposta sbagliata, prima che arrivino quelle prove fondamentali. L'umanità tende a convergere su quel tipo di risposta sbagliata.

### E se l'IA venisse sviluppata solo lentamente e si integrasse gradualmente nella società? {#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?}

#### **Questo molto probabilmente sarebbe catastrofico.** {#this-would-very-likely-be-catastrophic.}

Le nostre previsioni riguardano i risultati finali, non il percorso. Non sappiamo cosa succederà con l'IA da qui al momento in cui diventerà davvero pericolosa.

Per quanto ne sappiamo, potrebbe succedere tra sei mesi, se si scoprisse che IA poco intelligenti che pensano per molto tempo sono abbastanza brave a fare le proprie ricerche sull'IA (in un modo che avvia un circolo vizioso critico). E per quanto ne sappiamo, il settore potrebbe rimanere fermo per sei anni in attesa di qualche intuizione critica che poi impiegherà altri sei anni per maturare. In quest'ultimo caso, potrebbe esserci un intero periodo di dodici anni in cui l'IA influirà in modo sorprendente sull'istruzione e sul lavoro.

(Sì, per chi sta storcendo il naso alla frase precedente, siamo consapevoli che la [fallacia del blocco del lavoro](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq) è una fallacia. Il punto è che le nostre ipotesi su come l'IA influenzerà effettivamente il lavoro nel breve termine non sono particolarmente rilevanti, dato quello che succederà dopo).

Nelle risorse online del capitolo 6, abbiamo [discusso](#can-we-enhance-humans-so-they-keep-pace-with-ai?) di come l'umanità probabilmente non riuscirà a tenere il passo con lo sviluppo dell'IA, anche se ci affanniamo a potenziare l'intelligenza umana. (Ciononostante, nel capitolo 13 sosteniamo l'idea di potenziare l'intelligenza umana, ma non pensiamo che ciò possa realisticamente consentire agli esseri umani di tenere il passo con le IA se la ricerca sull'IA non viene anche interrotta).

Quindi, anche se quel futuro potrebbe diventare interessante e strano, sarebbe un futuro in cui sempre più potere va alle IA. Una volta che un qualsiasi insieme di queste IA si trova in una posizione in cui *potrebbe* prendere le risorse del pianeta per sé, quello è il punto di non ritorno: o quell'insieme di IA contiene una componente che si preoccupa di persone felici, sane e libere, o il futuro andrà male per noi.

### E se ci fossero tante IA diverse? {#what-if-there-are-lots-of-different-ais?}

#### **Non aiuta molto se non riusciamo a far sì che almeno una di esse si preoccupi delle cose buone.** {#non-aiuta-molto-se-non-riusciamo-a-far-sì-che-almeno-una-di-esse-si-preoccupi-delle-cose-buone.}

Ci sono moltissimi modi in cui le IA possono finire per interessarsi a fini strani e bizzarri che nessuno voleva o intendeva, come discusso nel capitolo 4. Non importa se l'umanità crea un miliardo di IA, se quel miliardo di IA si interessa tutte a fini strani leggermente diversi. Non andrà bene per gli esseri umani a meno che non capiamo come creare almeno un'IA che si interessi almeno in misura decente di persone felici, sane e libere che vivono vite prospere — non solo nel senso che ci assicurano di interessarsene quando sono giovani, ma nel senso che questa è *effettivamente* la risposta più efficiente a qualsiasi domanda a cui le loro azioni (o le azioni dei loro discendenti) sono una risposta, come discusso nel capitolo 5\.

Se sapessimo come fare in modo che una IA su dieci fosse buona, allora forse potremmo ottenere un decimo dell'universo creando un gran numero di IA diverse e sperando che quelle buone negozino per noi. Ma, come abbiamo sostenuto nei capitoli da 2 a 4, ottenere un'IA che si preoccupi delle persone nel modo giusto è estremamente improbabile, nel regime moderno in cui ci limitiamo a far crescere le IA. Non è come una possibilità su dieci; è una possibilità del tipo che-semplicemente-non-accade-a-meno-che-tu-non-sappia-cosa-stai-facendo-abbastanza-bene-da-farla-accadere-di-proposito. L'umanità non è neanche lontanamente a quel livello. Anche generando qualche miliardo di IA non otterremmo un miliardesimo delle risorse dell'universo, se non riusciamo a far sì che almeno una di esse si preoccupi minimamente di noi.

## Discussione approfondita {#extended-discussion-8}

### Uno sguardo più da vicino al prima e al dopo {#uno-sguardo-più-da-vicino-al-prima-e-al-dopo}

Come menzionato nel capitolo, la difficoltà fondamentale che i ricercatori devono affrontare nell'IA è questa:

Devi allineare un'IA **prima** che diventi abbastanza potente e capace da ucciderti (o, separatamente, da resistere all'allineamento). Quell'allineamento deve poi *trasferirsi a condizioni diverse*, le condizioni **dopo** che una superintelligenza o un insieme di superintelligenze[^187] potrebbe ucciderti se lo preferisse.

In altre parole: se stai costruendo una superintelligenza, devi allinearla senza mai poter testare a fondo le tue tecniche di allineamento nelle condizioni reali che contano, indipendentemente da quanto "empirico" possa sembrare il tuo lavoro quando lavori con sistemi che non sono abbastanza potenti da ucciderti.

Questo non è uno standard a cui i ricercatori di IA, o gli ingegneri in quasi tutti i campi, sono abituati.

Spesso sentiamo lamentele sul fatto che stiamo chiedendo qualcosa di non scientifico, slegato dall'osservazione empirica. In risposta, potremmo suggerire di parlare con i progettisti delle sonde spaziali di cui abbiamo parlato nel capitolo 10.

La natura è ingiusta e a volte ci mette di fronte a situazioni in cui l'ambiente che conta non è quello in cui possiamo fare dei test. Tuttavia, occasionalmente, gli ingegneri riescono a cogliere l'occasione e a farlo bene al primo tentativo, *quando hanno una solida comprensione di quello che stanno facendo* — strumenti robusti, teorie predittive solide — qualcosa che chiaramente manca nel campo dell'IA.

L'intero problema è che *l'IA che puoi testare in sicurezza, senza che eventuali test falliti ti uccidano*, opera sotto un regime diverso dall'IA (o dall'ecosistema IA) che *deve essere già stata testata, perché se è disallineata, allora tutti muoiono*. La prima IA, o sistema di IA, non si percepisce correttamente come avere un'opzione realistica di uccidere tutti se lo desidera. La seconda IA, o sistema di IA, vede quell'opzione.[^188]

Supponi di star considerando di fare il tuo collega Bob dittatore del tuo paese. Potresti provare prima a farlo dittatore fittizio della tua città, per vedere se abusa del suo potere. Ma questo, purtroppo, non è un test molto efficace. "Ordinare all'esercito di intimidire il parlamento e 'supervisionare' le prossime elezioni" è un'opzione molto diversa da "abusare del mio potere fittizio sotto lo sguardo dei cittadini (che possono ancora picchiarmi e negarmi il lavoro)".

Data una teoria della cognizione sufficientemente ben sviluppata, potresti provare a leggere nella mente dell'IA e prevedere in quale stato cognitivo entrerebbe se pensasse davvero di avere l'opportunità di prendere il controllo.

E potresti [creare delle simulazioni](#what-if-we-make-it-think-it’s-in-a-simulation?) (e provare a ingannare le sensazioni interne dell'IA, e così via) in modo che la tua teoria della cognizione preveda che sarebbero molto simili allo stato cognitivo in cui entrerebbe l'IA una volta che avesse davvero la possibilità di tradirti.

Ma il collegamento tra questi stati che induci e osservi in laboratorio e lo stato in cui l'IA ha effettivamente la possibilità di tradirti *dipende fondamentalmente dalla tua teoria della cognizione non testata*. La mente di un'IA è soggetta a cambiare parecchio mentre si sviluppa in una superintelligenza!

Se l'IA crea nuove IA successive più intelligenti di lei, i meccanismi interni di *quelle* IA saranno probabilmente diversi da quelli dell'IA che hai studiato prima. Quando impari solo da una mente Prima, qualsiasi applicazione di quella conoscenza alle menti che vengono Dopo passa attraverso una *teoria non testata* su come le menti cambiano tra il Prima e il Dopo.

Far funzionare l'IA finché non ha l'opportunità di tradirti *davvero*, in un modo difficile da simulare, è un test empirico di quelle teorie in un ambiente che differisce fondamentalmente da qualsiasi contesto di laboratorio.

Molti scienziati (e molti programmatori) sanno che le loro teorie su come un sistema complesso funzionerà in un ambiente operativo fondamentalmente nuovo **spesso non vanno bene al primo tentativo**.[^189] Si tratta di un problema di ricerca che richiede un livello "ingiusto" di prevedibilità, controllo e intuizione teorica, in un dominio con livelli di comprensione insolitamente bassi — con tutte le nostre vite in gioco se il risultato dell'esperimento smentisce le speranze degli ingegneri.

Ecco perché, dal nostro punto di vista, sembra *sovradeterminato* che i ricercatori non dovrebbero affrettarsi a spingere la frontiera dell'IA il più lontano possibile. È una cosa legittimamente folle da tentare, e una cosa legittimamente folle da permettere per qualsiasi governo.

### La storia del Chicago Pile-1 {#the-tale-of-chicago-pile-1}

Nel 1942, il Chicago Pile-1 fu costruito sotto la direzione di Enrico Fermi. Era composto da 45 000 blocchi di grafite del peso complessivo di 330 tonnellate, 4,9 tonnellate di uranio metallico e 41 tonnellate di biossido di uranio, collocati sotto le tribune del campo da rackets Stagg Field dell'Università di Chicago. A seconda di come si definiscono i termini, si potrebbe chiamarlo il primo reattore nucleare; non era destinato a produrre energia per uso industriale, ma fu il primo motore per una reazione critica sostenuta.

Secondo gli standard moderni, alcuni aspetti della sicurezza furono trascurati. Ad esempio, il fatto che fosse costruito sotto le tribune di un campo da rackets in un'università all'interno di una grande città.

Il generale Groves, direttore dell'intero Progetto Manhattan, aveva cercato di fare svolgere l'esperimento *vicino* a Chicago piuttosto che *direttamente* a Chicago, e aveva ordinato la costruzione di un edificio per quello scopo, ma la costruzione era in ritardo. Arthur Compton, il professore di fisica premio Nobel dell'Università di Chicago che ospitò il CP-1, aveva evitato di chiedere il permesso al rettore dell'università poiché, come Compton spiegò in seguito, il rettore avrebbe dovuto dire di no, e quella sarebbe stata la risposta sbagliata.

Il lavoro di impilare i mattoni fu svolto da ragazzi che avevano abbandonato la scuola superiore e cercavano di guadagnare qualche soldo extra in attesa della chiamata alle armi.

L'uranio era racchiuso in un cubo di gomma di sette metri, anziché in un recipiente metallico del reattore. Non c'era, ovviamente, nessun gigantesco edificio di contenimento in cemento.

Quando James Conant, presidente del Comitato di ricerca per la difesa nazionale, venne a conoscenza di questi fatti, si dice che impallidì. Anche per gli anni '40, questo non era considerato un comportamento scientifico del tutto normale.

Se leggeste tutto questo in un libro di storia senza sapere come va a finire, potreste pensare di star leggendo il preludio di un grave fallimento in materia di sicurezza. Mancano così tante cose che la cultura del 2025 considera misure di sicurezza standard. Dove sono gli ispettori e i blocchi per appunti? Gli enormi e pesanti regolamenti operativi? Le commissioni che discutono con serietà? Le dichiarazioni di impatto? Le norme che dicono che solo persone con credenziali molto elevate possono impilare i mattoni di uranio? Dov'è la *burocrazia?*

Ma la pila di mattoni di uranio e grafite non si è fusa.

E il motivo è che Fermi sapeva cosa stava facendo; aveva previsto le regole in anticipo.

Fermi non stava semplicemente impilando misteriosi mattoni che generavano più calore quando venivano avvicinati. Sapeva che alcuni atomi di uranio sarebbero decaduti spontaneamente e si sarebbero scissi. Sapeva che quando ciò fosse accaduto, la fissione avrebbe generato neutroni. Sapeva che quei neutroni a volte avrebbero urtato altri atomi di uranio e che questo a volte avrebbe innescato un'altra fissione.

Fermi capì *in anticipo*, senza doverlo scoprire a proprie spese, che aveva a che fare con un processo esponenziale. Non nel senso in cui i media odierni abusano del termine "esponenziale" per indicare semplicemente "grande" o "veloce", ma un processo il cui tasso di crescita è proporzionale al suo livello attuale: l'esponenziazione *matematica*.

Fermi sapeva che accumulando più mattoni di uranio e grafite, stava *aumentando il fattore di moltiplicazione* all'interno di un processo esponenziale. Come discusso nel libro, c'è un'enorme differenza tra un fattore di moltiplicazione dei neutroni inferiore al 100 % e un fattore di moltiplicazione dei neutroni superiore al 100 %.[^190] Al di sotto del 100 %, si ha solo una pila di mattoni caldi. Ma oltre il 100 %, il livello di radioattività della pila aumenta. E aumenta. E aumenta.

Non si comporta come tutti i precedenti cumuli più piccoli di mattoni di uranio che potreste aver testato. Se non capiste abbastanza bene cosa stavate facendo da sotto-moderare il reattore (in modo che la reazione a catena rallentasse se il reattore avesse iniziato a surriscaldarsi), allora il reattore non si sarebbe stabilizzato come facevano i cumuli più piccoli. Se lo lasciaste funzionare tutta la notte, il giorno dopo non otterreste un nuovo livello di potenza utile a livello industriale.

Il cumulo diventerebbe sempre più radioattivo finché la grafite non prendesse fuoco o l'uranio non si fondesse in scorie.

A quel punto sarebbero arrivati i vigili del fuoco, che si sarebbero trovati di fronte a un incendio confuso che non smetteva di sprigionare calore anche quando vi versavano sopra l'acqua.

Il 1942 non sarebbe stato un anno ideale per frequentare l'Università di Chicago.

Ma Fermi sapeva già tutto questo, quindi andava bene. Quando Fermi ordinò di estrarre una barra di controllo (una tavola di legno con un foglio di cadmio inchiodato sopra) di altri trenta centimetri il 2 dicembre 1942, annunciò in anticipo che questa sarebbe stata l'estrazione che avrebbe fatto "salire e continuare a salire" i livelli di radioattività misurati, "senza alcun livellamento".

Poi la radioattività raddoppiò nei due minuti successivi e raddoppiò ancora, finché non lasciarono che la reazione continuasse e raddoppiasse ogni due minuti per un totale di ventotto minuti, aumentando di circa 16 000 volte.

Un aumento di 16 000 volte della radioattività era il comportamento atteso del reattore, previsto correttamente e compreso in dettaglio in anticipo. Non fu una sorpresa inaspettata, capitata a qualcuno a cui era stato ordinato di accumulare dieci volte più mattoni di uranio rispetto all'ultima volta per vedere se succedeva qualcosa di interessante e redditizio.

Come discusso nel libro, c'è un margine molto stretto tra un reattore nucleare e un'esplosione nucleare. Un margine di poco più dello 0,5 %, per essere precisi. Questa è la differenza tra un reattore che produce una quantità di energia utile a livello industriale e un reattore che esplode.

In altre parole: bisogna rendere la reazione nucleare sempre più potente, prima che inizi davvero a funzionare. E poi, un attimo dopo aver raggiunto quella potenza, se diventa anche solo un po' più potente, se si supera dello 0,65 %, esplode.

Questo è il tipo di problema che la realtà può presentarti. Succede.

Ma Fermi, [Szilard](#heading=h.dr1wxb2y1r2s) e il loro team avevano previsto tutte queste regole prima di scoprirle con l'esperienza. Sapevano dei neutroni ritardati e di quelli immediati. (Vedi il capitolo 10 per saperne di più su questa parte della storia.) Quindi, una volta che Fermi ha portato il fattore di moltiplicazione dei neutroni al 100,06 %, Fermi *non* ha ordinato di estrarre ulteriormente la barra di controllo per vedere cosa sarebbe successo con un cumulo ancora più potente. Arrivò solo alla criticità, non allo 0,65 % in più per raggiungere la criticità immediata. Fermi ottenne il risultato che aveva previsto e *sapeva* cosa sarebbe successo se fosse andato oltre. Quindi non andò oltre.

Ventotto minuti dopo, con la radioattività che raddoppiava ogni due minuti fino a un aumento di 16 000 volte, Fermi spense il primo reattore nucleare al mondo: i mattoni di uranio accatastati sotto le tribune di uno stadio universitario all'interno di una grande città.

Per essere chiari, non affermeremmo che Fermi stesse agendo in modo completamente responsabile solo perché aveva un modello apparentemente coerente della fisica dei reattori a bassa energia. Fermi avrebbe potuto sbagliarsi. L'umanità ha avuto alcune sorprese nel corso dello sviluppo dell'ingegneria nucleare.

Il test Castle Bravo della prima arma termonucleare[^191] ebbe una resa tre volte superiore a quella prevista perché conteneva una miscela di litio-6 e litio-7 come combustibile nucleare per una reazione di fusione. Chi costruì l'arma conosceva alcuni potenti prodotti nucleari derivanti dalla fusione del litio-6 ma non ne conosceva alcuno dalla fusione del litio-7, e si è scoperto che il litio-7 in realtà *non* era inerte.

Fermi, conducendo la sua reazione a bassa intensità e non a un livello tale da produrre energia utile a livello industriale, evitò *molte* complicazioni che si presentano nei reattori nucleari abbastanza potenti da essere redditizi. Se ci fossero stati fattori di aumento del fattore neutronico dipendenti dalla velocità di reazione che Fermi non aveva previsto — fenomeni prima sconosciuti, del tipo di quelli che si manifestarono nel test Castle Bravo — qualsiasi sorpresa che si manifestasse una volta che il flusso di neutroni fosse aumentato di un fattore 16 000 e avesse fatto salire il fattore di moltiplicazione da 1,0006 a 1,02 più velocemente del tempo di reazione necessario a un essere umano per scaricare il cadmio di emergenza, allora oggi l'America avrebbe una Zona di Esclusione di Chicago.

Anche così, non stiamo dicendo che Fermi abbia necessariamente sbagliato a condurre quell'esperimento. Non era il tipo di esperimento che avrebbe potuto distruggere *la specie umana*. Si potrebbe sostenere che c'erano poste in gioco per cui valeva la pena scommettere una Zona di Esclusione di Chicago come esito non predefinito dell'incontro con un nuovo fenomeno nascosto che avrebbe sconvolto una comprensione che si sperava precisa. In realtà, la Germania nazista non sarebbe arrivata vicina all'ottenimento di armi nucleari entro il 1945, ma nel 1942 nessuno sapeva che sarebbe stato così. Previsioni del genere sono difficili da fare. Impilare i mattoni di uranio *fuori* da una grande città sarebbe stato scomodo, e gli inconvenienti hanno costi reali in guerra.

Il nostro obiettivo nel raccontare questo evento non è quello di esprimere un giudizio morale in un senso o nell'altro. Per cominciare, dovremmo dedicare più tempo all'analisi dei dettagli storici di ciò che è accaduto per capire quali fossero le opzioni concrete di quelle persone e se abbiano rinunciato a un'opzione migliore.

La lezione che ne traiamo riguarda più che altro la differenza tra la "sicurezza" stereotipata e ciò che serve davvero per evitare che la realtà ti uccida.

Il Chicago Pile-1 era completamente privo di *misure di sicurezza stereotipate, visibili e ostentate* che i burocrati sanno come esigere. Il disastro fu evitato grazie alla *comprensione*, non grazie a un teatro della sicurezza. La comprensione di Fermi si rivelò sufficiente; si può immaginare che avrebbe potuto non esserlo, ma in realtà lo fu. E quel livello di comprensione era ciò che la realtà richiedeva, non una qualsiasi quantità di finzione.

Se nessuno avesse compreso a livello profondo cosa stava succedendo all'interno di una pila di strani mattoni di metallo... allora non sarebbe servito a molto avere tanti ispettori in abiti dall'aspetto sobrio che scrutassero i mattoni di metallo imperscrutabile, o stampare un Manuale di Sicurezza dall'aspetto ufficiale e ben rilegato che dicesse che solo gli Operatori Certificati erano autorizzati a impilare gli strani mattoni di metallo.

Possiamo immaginare un mondo in cui il Chicago Pile-1 fosse stato costruito *senza* un Enrico Fermi. Senza nessuno, in effetti, che comprendesse le vere leggi che governano i misteriosi mattoni autoriscaldanti.

In un mondo del genere, forse un altro scienziato avrebbe ancora potuto vedere il pericolo letale in arrivo prima che fosse troppo tardi. Possiamo immaginare uno scambio come il seguente:

> **Salviati**: Il modo in cui i mattoni saltano in potenza quando vengono messi insieme è una chiara firma di un processo auto-rinforzante, il tipo di processo che può rendersi più forte. Se si cercano modelli matematici che possano descrivere un processo del genere, tendono ad avere una modalità dove, se li spingi abbastanza lontano, esplodono.
> 
> **Simplicio:** Che sciocchezze! Nella vita reale, è scientifico credere che ogni tipo di processo del genere prima o poi raggiunga un limite. Non possono andare avanti all'infinito! Quindi impilare mattoni di uranio e grafite dovrebbe essere perfettamente sicuro, perché raggiungerà un limite, vedi, e sarà innocuo.
>
> **Salviati:** È come sostenere che una supernova non può essere pericolosa perché non può diventare *infinitamente* calda, o sostenere che una superintelligenza artificiale sarebbe innocua perché non sarebbe infinitamente intelligente. O come sostenere che un proiettile deve avere *qualche* limite alla sua velocità e quindi non perforerà la pelle. Solo perché c'è un limite da qualche parte non significa che il limite sia *basso*. Tutti i modelli matematici che abbiamo sul *perché* i mattoni si autoriscaldano suggeriscono che esiste una soglia critica da qualche parte, tale che oltrepassare quella soglia farà esplodere il cumulo e ucciderà chiunque si trovi nelle vicinanze.
>
> **Simplicio:** Ma gli scienziati non riescono nemmeno a mettersi d'accordo su dove sia questa soglia! Se ci fosse un consenso scientifico sul fatto che aggiungere ancora qualche mattone sia pericoloso, smetterei. Ma quando gli scienziati non riescono nemmeno a mettersi d'accordo su dove stia esattamente il pericolo, perché preoccuparsi?
>
> **Salviati:** Quando [molti](https://youtu.be/KcbTbTxPMLc?feature=shared&amp;t=1580) [dei principali](https://www.youtube.com/watch?v=PTF5Up1hMhw&amp;t=2283) [scienziati](https://aistatement.com/) avvertono che c'è una seria possibilità di un'esplosione letale, il fatto che non possano calcolare esattamente quando inizierà l'esplosione dovrebbe renderti *più* preoccupato, non meno. Forse se sapessimo precisamente come funzionano i mattoni, vedremmo che esiste una fascia ristretta in cui possiamo estrarre energia in sicurezza, al di sotto della quale i mattoni sono inutili e al di sopra della quale i mattoni sono letali. Ma il fatto che gli scienziati stiano ancora litigando significa che *non* sappiamo ancora cosa stiamo facendo! Il che significa che non è il momento di giocare con qualsiasi reazione a catena stia riscaldando quei mattoni oggi, per evitare che domani li facciano esplodere e ci uccidano! *Prima comprendiamo la scienza.*

Siamo molto, molto lontani dal poter modellare l'IA anche solo una frazione di quanto bene Fermi comprendesse le reazioni a catena nucleari.

A un certo punto sconosciuto, se continuiamo su questa strada, correremo a rotta di collo verso un esito molto più grave dell'irradiazione di Chicago.

# Capitolo 11: Un'alchimia, non una scienza {#chapter-11:-an-alchemy,-not-a-science}

Questa è la risorsa online per il Capitolo 11 di *If Anyone Builds It, Everyone Dies*, che discute come i moderni laboratori di IA stanno affrontando il problema dell'allineamento della superintelligenza artificiale. Fate riferimento al libro per le risposte a domande quali:

* Come dovremmo valutare l'attuale preparazione delle aziende di IA nel risolvere il problema dell'allineamento dell'ASI?  
* Dove si inserisce nel quadro la "ricerca sull'interpretabilità" — la ricerca per leggere e comprendere le menti dell'IA?  
* Non possiamo semplicemente chiedere all'IA di risolvere il problema al posto nostro?

Di seguito, trattiamo una raccolta di idee sull'allineamento e l'implementazione dell'IA e i motivi proposti per essere ottimisti, oltre ai motivi proposti per cui potrebbe essere una buona cosa far avanzare la frontiera dell'IA anche se la situazione appare difficile.

## Domande frequenti {#faq-8}

### Non finiremo per cavarcela come al solito? {#non-finiremo-per-cavarcela-come-al-solito?}

#### **Il mondo di solito tira avanti con tentativi ed errori. In questo caso, gli errori iniziali non lascerebbero sopravvissuti.** {#il-mondo-di-solitamente-tira-avanti-con-tentativi-ed-errori.-in-questo-caso,-gli-errori-iniziali-non-lascerebbero-sopravvissuti.}

Vedi il capitolo 10 e la [discussione approfondita correlata](#un-approfondimento-su-prima-e-dopo) sulla differenza tra Prima e Dopo.

### Vedi l'allineamento come tutto o niente? {#do-you-see-alignment-as-all-or-nothing?}

#### **No. Ma un "allineamento parziale" è comunque probabile che sia catastrofico.** {#no.-ma-anche-un-%26allineamento-parziale-%26potrebbe-comunque-essere-un-disastro.}

Uno degli argomenti per preoccuparsi meno della superintelligenza è del tipo: "L'IA probabilmente [avanzerà in modo incrementale](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?), permettendo opportunità di miglioramenti per tentativi ed errori per tenere sotto controllo le IA ad ogni passo; l'allineamento non deve essere *perfetto* perché le cose vadano bene". Non pensiamo che questa visione offra molte speranze, per alcuni motivi:

* Le nostre preoccupazioni non dipendono dal fatto che il progresso sia veloce o lento. Non siamo in grado di dire con certezza se l'IA raggiungerà delle fasi di stallo in vari punti del suo percorso verso la superintelligenza. È una previsione difficile, non certo semplice. La nostra ipotesi più probabile è che l'intelligenza delle macchine sia soggetta a [effetti soglia](#is-“intelligence”-a-simple-scalar-quantity?), ma, in definitiva, si tratta solo di una congettura, e le nostre argomentazioni [non si fondano su di essa](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?). La storia di Sable, nella seconda parte di *If Anyone Builds It, Everyone Dies*, descrive intenzionalmente una catastrofe provocata da IA non molto più avanzate delle capacità umane, in parte per comunicare l'idea che un avversario IA non avrebbe bisogno di diventare rapidamente superintelligente per essere straordinariamente pericoloso.

* La nostra risposta di base alla domanda "Cosa succederebbe se fossimo fortunati e avessimo molto tempo per provare idee di allineamento sull'IA deboli prima che l'IA diventasse molto capace?" è la discussione nel capitolo 10 e la discussione estesa associata "[Uno sguardo più da vicino al prima e al dopo](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo)". I ricercatori possono capire ogni sorta di dettaglio sulle IA deboli, ma ci sono inevitabilmente un sacco di differenze critiche tra le IA abbastanza deboli da poter essere studiate in sicurezza e le prime IA abbastanza potenti da costituire un punto di non ritorno. Anche in un campo maturo, affrontare tutte queste differenze in modo adeguato e con sufficiente anticipo sarebbe molto difficile. In un campo che è ancora in fase embrionale, lavorare con IA imperscrutabili (che vengono sviluppate piuttosto che create), la speranza è decisamente irrealistica.

* L'allineamento dell'IA non deve essere perfetto per produrre ottimi risultati a lungo termine. In linea di principio, è possibile creare con cura un'IA con una certa tolleranza all'errore, se si sa cosa si sta facendo.[^192] Ma questo non significa che le IA "parzialmente allineate" o anche "per lo più allineate" produrrebbero risultati parzialmente o per lo più accettabili. Ci sono molti modi e motivi diversi per cui un'IA potrebbe comportarsi bene il 95 % delle volte nel presente o nel futuro prossimo senza che questo si traduca in un lieto fine per l'umanità, come discusso da molte angolazioni diverse nelle [risorse online per il capitolo 5](#chapter-5:-its-favorite-things).

Per approfondire l'ultimo punto:

Prova a immaginare, come esperimento mentale, che l'umanità riesca a mettere *quasi* tutti i diversi valori umani nelle preferenze di una superintelligenza, tranne [la preferenza per la novità](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile), per qualche motivo. In questo caso, la superintelligenza andrebbe verso un futuro fermo e noioso, dove lo stesso giorno "migliore" si ripete all'infinito.

Non pensiamo che questo sia *plausibile*, sia chiaro. Quel livello di allineamento sembra del tutto irraggiungibile con gli approcci standard dell'IA di oggi, e sembra un po' strano immaginare che riusciremmo a capire come inserire quasi tutti i nostri valori in un'IA senza capire come inserirli tutti.[^193] Ma questo esperimento mentale evidenzia come creature che condividono *alcuni* dei nostri desideri, ma a cui manca almeno un desiderio cruciale, produrrebbero comunque risultati catastrofici una volta diventate tecnologicamente abbastanza avanzate da escludere gli esseri umani dal processo decisionale e ottenere esattamente ciò che vogliono.

Più realisticamente, un'IA potrebbe finire per essere "parzialmente" allineata nel senso che (come noi) ha varie strategie strumentali [intrecciate nelle sue preferenze terminali](#riflessione-e-auto-modifica-rendono-tutto-più-difficile). Forse finirebbe per avere una pulsione un po' simile alla curiosità e una pulsione un po' simile al [conservazionismo](#l'intelligenza artificiale non vorrà mantenerci felici e in salute per motivi di conservazione ecologica o per qualche spinta simile?), e forse alcune persone, vedendo questo, direbbero: "Ecco! L'IA sta sviluppando pulsioni molto umane". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta maturata in superintelligenza, probabilmente non sarebbe piacevole. Forse spenderebbe enormi risorse per perseguire la sua strana versione di curiosità [inconsciamente](#perdere-il-futuro), preservando al contempo una versione dell'umanità che ha modificato per renderla più gradevole per sé. (Proprio come anche molti esseri umani più attenti alla conservazione potrebbero eliminare dalla natura [zanzare che uccidono i bambini e parassiti agonizzanti](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?), avendone l'opportunità.) Questo fa parte del nostro punto quando diciamo che esseri umani floridi [non sono la soluzione più efficiente](#happy,-healthy,-free-people-aren't-the-most-efficient-solution-to-almost-any-problem.) alla stragrande maggioranza dei problemi.

In alternativa, un'IA potrebbe avere valori che si sommano a un comportamento molto umano *nell'ambiente di addestramento*, tanto che le persone esclamerebbero che sembra decisamente "parzialmente allineata". (Questo sta già accadendo ora, e abbiamo sostenuto che è [illusorio](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?).) Ma questo dice ben poco su come l'IA si comporterà una volta che avrà uno spazio di opzioni enormemente più ampio. Affinché le persone possano prosperare in *quel* contesto, la prosperità dell'umanità in particolare deve far parte del *risultato raggiungibile più preferito* dall'IA.

Se riusciamo a inserire parzialmente alcuni buoni valori nell'IA, ciò non significa che i valori dell'umanità vengano parzialmente rappresentati nel futuro. Il caricamento parziale di valori simili a quelli umani nelle preferenze di un'IA più intelligente dell'essere umano non è la stessa cosa del caricamento completo dei valori umani nell'IA con una "ponderazione" bassa (che alla fine emerge una volta che altri valori sono saturati).

Per far sì che l'IA ci dia *qualcosa*, deve interessarsi a noi nel modo esatto, almeno un minimo. E ci sono moltissimi "quasi-successi" che non raggiungono questa soglia. Vedi anche: "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#won't-ais-care-at-least-a-little-about-humans?))"

### La situazione non migliorerà una volta che i governi saranno più coinvolti? {#la-situazione-non-migliorerà-una-volta-che-i-governi-saranno-più-coinvolti?}

#### **Dipende da come (e quanto presto) si coinvolgeranno.** {#dipende-da-come-(e-quanto-presto)-si-coinvolgeranno.}

Quando visitiamo Washington, DC, spesso incontriamo decisori politici che pensano che le aziende di IA abbiano le loro IA sotto controllo. Allo stesso tempo, vediamo regolarmente persone nel settore dell'IA che dicono che la regolamentazione risolverà il problema. Un esempio particolarmente eclatante che abbiamo osservato è stato il CEO di Google [che ha affermato](https://youtu.be/9V6tWC4CdFQ?feature=shared&amp;t=2685) che "il rischio sottostante [che l'umanità venga spazzata via] è in realtà piuttosto alto", ma sostenendo che più alto diventa il rischio, più è probabile che l'umanità si mobiliti per prevenire la catastrofe.

Mettendo da parte quanto sia folle che il CEO di un'azienda corra a sviluppare una tecnologia che lui ritiene metta in pericolo tutti sulla Terra nella speranza che l'umanità si "mobiliti" per affrontare i rischi che lui stesso sta contribuendo a creare, osserviamo che questo è un caso in cui una persona dal lato tecnico della questione immagina che *qualcun altro* risolverà il problema.

Nel frattempo, la maggior parte delle persone in politica sembra pensare che la comunità tecnica risolverà il problema. Questo è implicito, ad esempio, ogni volta che [loro](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [dicono](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [che](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [dobbiamo](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [vincere](https://statemag.state.gov/2025/04/0425itn07/) [la](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [corsa](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — non è possibile che questo tipo di corsa abbia un vincitore, se le sfide tecniche non vengono risolte. Anche se potrebbe non essere poi così grave; forse i decisori politici non stanno davvero pensando a una corsa alla superintelligenza; forse stanno solo pensando a una corsa per chatbot migliori. A giugno 2025, un consulente di politica dell'IA che conosciamo descrive il Congresso come generalmente [non incline a credere alle aziende di IA quando dichiarano esplicitamente di stare lavorando alla superintelligenza](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (anche se con alcune importanti eccezioni).

Quasi tutti quelli al potere sembrano immaginare che qualcun altro risolverà il problema.

Per ulteriori discussioni su come il mondo in generale sta reagendo (e su come i decisori spesso non riescano a reagire in modo appropriato prima dei disastri), si veda il Capitolo 12. Ad agosto 2025, i governi devono ancora organizzare qualcosa che si avvicini a una risposta seria a questo problema. E c'è sempre il rischio che i funzionari governativi non riescano a comprendere appieno la sfida e (ad esempio) trattino l'IA come una tecnologia normale che non dovrebbe essere soffocata da un eccesso di regolamentazione.

Per ulteriori informazioni su quali interventi governativi hanno una reale speranza di evitare una catastrofe dell'IA, vedi il Capitolo 13, così come la [discussione sul perché una collaborazione internazionale probabilmente non sarebbe sufficiente](#perché-non-utilizzare-la-cooperazione-internazionale-per-costruire-un'ia-sicura,-anziché-bloccarla-completamente?).

### Le aziende più spericolate non saranno naturalmente le più incompetenti e quindi non costituiranno una minaccia? {#le-aziende-più-spericolate-non-saranno-naturalmente-le-più-incompetenti-e-quindi-non-costituiranno-una-minaccia?}

#### **Non in generale. Spesso chi prende scorciatoie è competitivo.** {#not-in-general.-corner-cutting-is-often-competitive.}

Gli sforzi della Volkswagen per [barare nei test sulle emissioni](https://www.bbc.com/news/business-34324772) dal 2008 al 2015 sono stati audaci — e apparentemente efficaci. Gli incidenti del 2018-2019 del Boeing 737 MAX, dovuti a difetti nel sistema di controllo di volo che la direzione conosceva ma ha minimizzato, [hanno causato la morte di 346 persone](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Ma l'industria automobilistica e quella aeronautica sono settori altamente competitivi in cui Volkswagen e Boeing erano, e rimangono, dei colossi.

Non ci sembra un gran mistero che chi prende scorciatoie sia competitivo. In entrambi i casi, il comportamento sembra essere stato guidato dalla pressione di portare sul mercato prodotti ad alte prestazioni a un prezzo inferiore e prima della concorrenza. Anche adesso, dopo ingenti risarcimenti e danni al marchio, non è ovvio che le aziende siano meno competitive per avere una cultura aziendale che incoraggia l'uso intelligente di scorciatoie, anche se questo a volte significa essere scoperti.

Se pensate che le principali aziende di IA facciano eccezione a questa regola, considerate il seguente [titolo](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (e sottotitolo) del luglio 2025:

![][image18]

\[Il titolo dice: "Grok lancia una compagnia anime pornografica e ottiene un contratto con il Dipartimento della Difesa". Il sottotitolo dice: "Nel frattempo, la versione più avanzata del chatbot AI di xAI di Elon Musk continua a identificarsi come Adolf Hitler".\]

Non pensiamo che sia tecnicamente possibile per nessun team che usi metodi moderni costruire una superintelligenza senza causare una catastrofe. Ma anche se questo fosse vagamente possibile con la tecnologia di oggi, sembra quasi inevitabile che un'azienda di IA finirebbe comunque per combinare un pasticcio e farci uccidere tutti, visto il livello di competenza e serietà che vediamo oggi.

#### **\* Le aziende più caute di oggi sono comunque avventate.** {#*-le-aziende-più-caute-di-oggi-sono-comunque-avventate.}

L'azienda di IA Anthropic è considerata da un numero ragionevole di persone come leader nella "sicurezza dell'IA", perché ha promosso iniziative come [impegni volontari per la sicurezza](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Ma anche loro [modificano i loro impegni volontari all'ultimo minuto quando si rendono conto di non poterli rispettare](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling), e i "piani" che hanno sono vaghi e poco ponderati, come criticato nel capitolo 11 e nella [discussione approfondita](#more-on-making-ais-solve-the-problem) qui sotto.

Anthropic trae grande vantaggio dal fatto che gli osservatori valutano in base a una curva: in un settore normale, un'azienda che sceglie di mettere in pericolo la vita di miliardi di persone (come [ammesso dal CEO](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883)) minimizzando regolarmente le proprie attività al pubblico[^194] e ai legislatori[^195], non riceverebbe certo elogi per la sua moderazione.

Prendere scorciatoie è normale nell'IA, come in molti settori competitivi. L'incoscienza è comune. E le aziende *meno* incoscienti non sono visibilmente all'altezza delle sfide.

### Non è importante correre avanti a causa dell'"eccesso di hardware"? {#non-è-importante-correre-avanti-a-causa-dell'eccesso-di-hardware?}

#### **Sarebbe un suicidio, perché siamo troppo lontani da una soluzione di allineamento.** {#sarebbe-un-suicidio,-perché-siamo-troppo-lontani-da-una-soluzione-di-allineamento.}

Negli ultimi dieci anni circa, alcune persone preoccupate per i pericoli dell'IA hanno sostenuto che potrebbe essere una buona idea far progredire l'IA il più velocemente possibile. L'idea era che le IA più intelligenti avrebbero poi richiesto quasi tutto l'hardware informatico del mondo per funzionare. Nessuna singola scoperta rivoluzionaria avrebbe scatenato *all'improvviso* migliaia di potenti IA in grado di pensare migliaia di volte più velocemente di qualsiasi essere umano.

Finché l'umanità avesse usato una parte consistente della sua potenza di calcolo per far funzionare le IA più intelligenti, il cambiamento sarebbe avvenuto almeno gradualmente, dando all'umanità il tempo di adattarsi. Non ci sarebbe stato alcun "eccesso di hardware", nessun momento in cui le capacità dell'IA avrebbero fatto un balzo in avanti improvviso perché il mondo stava aspettando di utilizzare un grande accumulo di hardware informatico sull'IA. O almeno così sosteneva l'argomentazione.

Pensiamo che questa sia un'argomentazione piuttosto debole. Uno dei problemi è che l'intelligenza sembra essere soggetta a [effetti soglia](#is-“intelligence”-a-simple-scalar-quantity?).

Il passaggio dall'intelligenza a livello di scimpanzé all'intelligenza a livello umano non è stato affatto "discontinuo"; dal punto di vista dell'umanità è stato piuttosto graduale. Tuttavia, dal punto di vista evolutivo è avvenuto piuttosto rapidamente. E il passaggio dalla civiltà preindustriale a quella postindustriale è stato ancora più veloce. Nessuno di questi cambiamenti è stato abbastanza graduale da consentire agli altri animali di adattarsi in modo significativo.

Ad esempio, un'intelligenza artificiale che richiede una parte significativa della potenza di calcolo mondiale per funzionare potrebbe essere abbastanza intelligente da scoprire nuovi algoritmi di intelligenza artificiale e nuovi progetti di chip per computer che porterebbero rapidamente alla creazione di migliaia di intelligenze artificiali più intelligenti degli esseri umani e in grado di pensare migliaia di volte più velocemente dell'umanità. Ricorda: un moderno centro dati richiede tanta elettricità per funzionare quanto una [piccola città](https://epoch.ai/blog/power-demands-of-frontier-ai-training), mentre un essere umano richiede tanta elettricità per funzionare quanto una [grande lampadina](https://en.wikipedia.org/wiki/Human_power). C'è molto margine per migliorare l'efficienza dell'IA.

Oppure, se il collo di bottiglia è la potenza di calcolo per *costruire* le IA piuttosto che quella per *farle funzionare*, possiamo aspettarci che, una volta finito il processo di addestramento, ci saranno grandi quantità di hardware libero, che potrà essere utilizzato per far funzionare molte IA che pensano velocemente.

Anche se l'intelligenza non fosse soggetta a effetti soglia, siamo scettici sull'idea che colpire continuamente l'umanità con IA sempre più intelligenti (anche se nessuna di esse è abbastanza intelligente da ucciderci) il più rapidamente possibile sia un ottimo modo per aiutare l'umanità a sviluppare la disciplina ingegneristica necessaria per costruire IA robustamente amichevoli.

Il problema è che le IA vengono coltivate piuttosto che costruite artigianalmente, e nessuno è neanche lontanamente vicino a capire come coltivare IA che si preoccupino in modo robusto di *qualsiasi cosa* i loro progettisti vogliano.

Questo problema non si risolve coltivando più IA al più presto possibile. L'idea è praticamente un non sequitur. Vedi anche alcuni vecchi scritti di Soares su come [l'allineamento dell'IA richieda uno sforzo seriale](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Il non sequitur è stato comunque ripreso dal CEO di OpenAI Sam Altman, che [l'ha usata come scusa nel 2023 per far sì che OpenAI procedesse il più velocemente possibile](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Questa scusa si è poi rivelata vuota quando lo stesso Sam Altman [si è affrettato a costruire hardware di calcolo drammaticamente più potente](https://openai.com/index/announcing-the-stargate-project/).

Pensiamo che questo sia un buon esempio di come i dirigenti delle aziende di IA si aggrappino a qualsiasi argomento pensino possa funzionare per giustificare la loro corsa in avanti. Pensiamo che la maggior parte di questi argomenti possa essere respinta nel merito e [sconsigliamo](#workable-plans-will-involve-telling-ai-companies-“no.”) di dare troppo peso a un argomento solo perché è stato avanzato da un dirigente di un'azienda di IA.

### Non è importante correre avanti per poter fare ricerca sull'allineamento? {#isn't-it-important-to-race-ahead-so-we-can-do-alignment-research?}

#### **\* Sconsigliamo vivamente questo intero paradigma dell'IA.** {#*-we-strongly-recommend-against-this-entire-ai-paradigm.}

I metodi attuali nell'IA presentano sfide inutilmente difficili per l'allineamento, per i motivi che abbiamo discusso nei capitoli precedenti. Non vediamo alcun motivo in linea di principio per cui l'umanità non potrebbe costruire una superintelligenza allineata, con una comprensione sufficientemente forte di ciò che stiamo facendo e un diverso insieme di strumenti formali. Ma l'intero approccio attuale all'IA sembra un vicolo cieco dal punto di vista dell'allineamento e della robustezza, anche se è perfettamente valido dal punto di vista delle capacità.

Non stiamo sostenendo la "buona vecchia" IA che ha regnato dagli anni '50 agli anni '90. Quelle tecniche erano fuorvianti e hanno fallito, per ragioni che sono [abbastanza ovvie](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design). Ci sono *altre opzioni* oltre ai tentativi estremamente superficiali degli anni '80 e alle IA coltivate con una comprensione quasi nulla del loro funzionamento interno.

#### **C'è molto lavoro significativo che si potrebbe fare ora.** {#there's-plenty-of-meaningful-work-that-could-be-done-now.}

Sydney Bing ha [manipolato psicologicamente](https://x.com/MovingToTheSun/status/1625156575202537474) e [minacciato](https://x.com/sethlazar/status/1626257535178280960) gli utenti. Ancora non sappiamo esattamente perché; ancora non sappiamo esattamente cosa le passasse per la testa. Lo stesso vale per i casi in cui le IA (in circolazione) sono [eccessivamente servili](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [sembrano cercare attivamente di far impazzire le persone](#ai-induced-psychosis), [secondo quanto riferito imbrogliano e cercano di nasconderlo](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), o [dichiarano persistentemente e ripetutamente di essere Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Lo stesso vale per i casi in ambienti controllati ed estremi in cui le IA [fingono allineamento](https://arxiv.org/abs/2412.14093), [si dedicano al ricatto](https://www.anthropic.com/research/agentic-misalignment), [resistono allo spegnimento](https://palisaderesearch.org/blog/shutdown-resistance) o [cercano di uccidere i loro operatori](https://www.anthropic.com/research/agentic-misalignment).

Non sappiamo quali di questi casi stiano accadendo per ragioni che dovrebbero preoccuparci, perché nessuno è riuscito a capire cosa stesse succedendo all'interno delle IA, o esattamente perché si sia verificato uno qualsiasi di questi eventi. Pensate a tutto ciò che si potrebbe scoprire sui moderni LLM, e su come funziona l'intelligenza più in generale, studiando i modelli esistenti fino a quando le persone *potessero* comprendere tutti questi segnali di allarme!

"Non possiamo risolvere l'allineamento senza studiare le IA" aveva un po' più senso nel 2015, quando sentivamo questa affermazione fatta dalle persone che avevano bisogno di una scusa per avviare aziende di IA di fronte agli argomenti secondo cui avrebbero così giocato d'azzardo con tutte le nostre vite. All'epoca contestammo questa affermazione, dicendo che in realtà c'era molta ricerca da fare, e che non pensavamo che il moderno paradigma basato sulla discesa del gradiente fosse molto promettente (rispetto al creare intenzionalmente una superintelligenza amichevole). Ma l'argomento ha molto *meno* senso ora, quando c'è *già* così tanto da studiare che non capiamo.

A tutti i dirigenti aziendali che *effettivamente stavano* creando l'IA solamente per rendere possibile lo studio del problema dell'allineamento dell'IA nella pratica piuttosto che solo in teoria: ce l'avete fatta! Ci siete riusciti. Ora ci sono abbastanza informazioni per tenere occupati i ricercatori per decenni. Pensiamo che i costi di spingere avanti un paradigma estremamente pericoloso probabilmente non ne valessero la pena, ma di sicuro ora c'è molto da studiare. Potete smettere di spingere.

E per quanto riguarda quelli che hanno continuato a spingere anche oltre tutti i segnali di allarme? L'inferenza ovvia è che non stavano mai effettivamente costruendo l'IA solo per il bene di risolvere l'allineamento, indipendentemente da quello che dicevano per placare le paure quando giustificavano il loro comportamento sconsiderato negli anni 2010.

### E se le aziende di IA distribuissero le loro IA solo per azioni non pericolose? {#what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions?}

#### **\* Le azioni che sembrano benigne possono comunque richiedere capacità pericolose.** {#*-actions-that-seem-benign-can-still-require-dangerous-capabilities.}

Un esempio di proposta che abbiamo sentito è che le aziende di IA continuino ad avanzare sulla frontiera delle capacità, ma si impegnino a usare le loro IA solo in modi che non sembrano immediatamente pericolosi. Ad esempio, in conversazione con figure di spicco dell'IA (anni fa), abbiamo sentito ventilare l'idea che un'IA potente con forti capacità retoriche potrebbe essere usata per convincere i politici di tutto il mondo a promulgare un divieto efficace sullo sviluppo di IA pericolose.

Per raggiungere questo obiettivo, secondo l'argomentazione, un'IA dovrebbe solo parlare. Non avrebbe bisogno di manipolare direttamente robot fisici. Non avrebbe bisogno di avere accesso a un laboratorio biologico dove potrebbe progettare un supervirus.

Prima di tutto, siamo contrari a questa idea per motivi etici. Un'IA sufficientemente sovrumana nella persuasione potrebbe forse convincere quasi chiunque di quasi qualsiasi cosa, e distribuirla per persuadere altre persone delle *vostre* conclusioni ci lascia perplessi. Non pensiamo che sia ovviamente necessario ricorrere a misure così estreme, quando i membri meramente umani del campo potrebbero e dovrebbero fare molto di più oggi per condividere le nostre preoccupazioni e argomentazioni, e per allertare i leader mondiali sull'estremo pericolo dell'IA superintelligente.[^196]

Come sviluppatore di IA, potresti passare anni a costruire IA sempre più pericolose nella speranza di raggiungere questo obiettivo, oppure potresti provare a parlare *tu stesso* con i legislatori in modo completamente onesto, anche solo una volta, con l'obiettivo di informare piuttosto che di manipolare. Nella nostra esperienza, siamo stati ripetutamente sorpresi positivamente da quanto le persone a DC siano ricettive a queste questioni, quando vengono condivise in tutta franchezza.

Ma questa è una digressione dall'argomento di cosa va storto se si cerca di implementare un'IA molto potente che può "solo parlare". Al di là delle questioni etiche, il problema dell'idea *tecnica* è che per avere successo nella persuasione sovrumana è probabile che l'IA debba modellare in dettaglio gli esseri umani e manipolarli estensivamente.

Gli esseri umani sono creature intelligenti. *Voi* parlereste con un'IA super-persuasiva che ha la reputazione di poter convincere chiunque di qualsiasi cosa, indipendentemente dalla sua veridicità? Se un leader mondiale entrasse in una stanza con quell'IA e ne uscisse con le sue opinioni completamente stravolte, chi alzerebbe la mano per essere il prossimo? *Noi* non parleremmo volentieri con quel tipo di IA, in parte perché [non vogliamo effettivamente che i nostri valori vengano cambiati](#"intelligente"-\(di-solito\)-implica-"incorreggibile").

Un'IA che potrebbe avere successo anche di fronte a quel tipo di avversità è il tipo di IA che può simulare varie possibili reazioni che le persone potrebbero avere ai suoi output, e tracciare un percorso attraverso lo spazio delle reazioni umane verso un risultato piccolo e difficile da raggiungere. Quel tipo di IA probabilmente contiene ingranaggi mentali abbastanza generali da fare ciò che fanno gli esseri umani; deve essere in grado di pensare almeno i pensieri che gli esseri umani possono pensare, per poter manipolare così bene gli esseri umani.

Un'IA che può fare tutto questo quasi certamente non è un tipo ristretto di intelligenza. E poiché l'IA è cresciuta piuttosto che progettata, non può essere progettata in modo che possa usare quegli ingranaggi solo per prevedere gli esseri umani; gli stessi ingranaggi possono in principio essere usati per qualsiasi problema che sta cercando di risolvere. Come si potrebbe ottenere un'IA che sia sovrumamente capace nei modi desiderati, ma che non sia abbastanza intelligente da notare che i suoi obiettivi (qualunque essi siano) sono meglio serviti se può uscire dal controllo dei suoi operatori?

Se i leader mondiali possono essere persuasi semplicemente da buoni argomenti, presentate quegli argomenti ora. Se ci vuole sostanzialmente più potere super-persuasivo, allora quello è un tipo pericoloso di capacità. Non potete avere entrambe le cose.

Probabilmente le persone nei laboratori di IA che ci hanno proposto questo suggerimento non stavano pensando fino in fondo al loro suggerimento; probabilmente volevano solo una giustificazione per correre avanti. Ma il punto più ampio rimane valido. Molte proposte su cosa un'IA possa presumibilmente fare che sia "chiaramente sicuro" non coinvolgono un grado chiaramente sicuro di capacità dell'IA.

Incontriamo frequentemente proposte che affermano che un'IA farà "solo" una cosa, come persuadere i politici, immaginando che non possa o non farà nient'altro. Questo sembra riflettere una mancanza di rispetto per la generalità di un'intelligenza che può fare il tipo di lavoro in questione. "Solo parlare" non è un compito ristretto. Troppe delle complessità e complessità del mondo sono riflesse nel parlato e nella conversazione. Questo è il motivo per cui i chatbot moderni devono essere generali in modi in cui i motori di scacchi non lo erano. Avere successo nelle conversazioni con gli esseri umani richiede una comprensione molto più generale delle persone e del mondo.

Se addestrate un'IA a essere molto brava a guidare auto rosse, non dovreste essere sorpresi quando guida anche auto blu. Qualsiasi piano che dipendesse dal fatto che non sia in grado di guidare auto blu sarebbe sciocco.

Quindi dire "La mia IA non farà nulla di pericoloso nel mondo; convincerà solo i politici" non aiuta, anche se mettiamo da parte gli scrupoli etici e i problemi pratici con l'intera idea, e mettiamo da parte che i politici potrebbero già essere perfettamente persuadibili oggi, se solo *abbiamo conversazioni normali* e informiamo i decisori politici e il pubblico sulla situazione. Molte abilità e capacità di ragionamento generale stanno alla persuasione sovrumana come le auto blu stanno alle auto rosse. Un'IA che potrebbe fare questo non è così debole da essere passivamente sicura.

E questo ancora prima di osservare che la persuasione sovrumana è un'abilità molto pericolosa da far avere alla tua IA se qualcosa va anche solo leggermente storto.

#### **Non vediamo usi rivoluzionari dell'IA che non richiedano progressi nell'allineamento.** {#non-vediamo-usi-rivoluzionari-dell-ia-che-non-richiedano-progressi-nell-allineamento.}

Molte proposte che abbiamo visto per sfruttare i progressi dell'IA per salvare il mondo hanno il problema che un'IA capace di aiutare sarebbe così potente da dover essere già allineata, il che vanifica lo scopo.

L'idea di IA sovraumamente persuasive rientra in questa categoria. Le IA in grado di fare ricerca sull'allineamento dell'IA rientrano nella stessa categoria, come discutiamo nel libro. Le IA che sviluppano nuove potenti tecnologie che aiutano con la non proliferazione dell'IA sono un altro esempio, perché sarebbe difficile stabilire in modo affidabile se i progetti di un'IA per nuove tecnologie radicali siano sicuri da implementare. (Ricordate l'esempio del fabbro che costruisce un frigorifero del Capitolo 6.)

Quando facciamo notare quanto sia difficile costruire un'IA abbastanza potente da aiutare e al contempo abbastanza debole da essere passivamente sicura, spesso sentiamo un altro tipo di proposta: modi di usare l'IA che potrebbero essere interessanti, ma che in realtà non fanno nulla per impedire ad altri sviluppatori di distruggere il mondo con la superintelligenza.

Un tipo comune di proposta riguarda IA che si limitano a produrre dimostrazioni (o confutazioni) di affermazioni matematiche scelte dagli esseri umani.[^197] Gli esseri umani non avrebbero quasi bisogno di interagire con gli output dell'IA. L'IA si limita a proporre una dimostrazione, e poi un meccanismo completamente automatizzato e affidabile può verificare se la dimostrazione è corretta, permettendoci di sfruttare l'IA per apprendere cose nuove.

Ma quale affermazione potremmo far dimostrare all'IA che ci permetterebbe di impedire alla prossima IA in linea di acquisire un laboratorio biologico e rovinare il futuro?

Abbiamo ricevuto varie risposte a questa domanda, quando la poniamo. Una classe di risposte è che dovrebbe esserci un regime mondiale per impedire a chiunque di costruire IA che facciano altro che produrre dimostrazioni per i verificatori di dimostrazioni. Questo potrebbe forse funzionare, ma nella misura in cui funzionasse, funzionerebbe grazie al regime mondiale imposto che controlla la creazione e l'uso dell'IA. L'IA che cerca le dimostrazioni non starebbe facendo alcun lavoro.

Un'altra classe di risposte è: "Qualcun altro è destinato a pensare a qualche affermazione matematica importante la cui dimostrazione sarebbe rilevante". Ma tutto il lavoro difficile sta nel capire *cosa potremmo possibilmente dimostrare* tale da trovarci in una posizione significativamente migliore. Non possiamo semplicemente chiedere all'IA di dimostrare la frase in lingua inglese "Sono sicura da usare", perché non è un'affermazione matematica soggetta a dimostrazione. Se sapessimo con chiarezza matematicamente precisa cosa significherebbe per un enorme groviglio di computazioni essere "sicuro", sapremmo così tanto sull'intelligenza che probabilmente potremmo saltare la dimostrazione e progettare direttamente un'IA sicura.

Con proposte come queste, c'è spesso una sorta di gioco delle tre carte in corso. Quando si pensa a come un'IA generale senza restrizioni potrebbe essere pericolosa, qualcuno suggerisce che lo spazio d'azione dell'IA dovrebbe essere limitato a qualche dominio ristretto (come la produzione di specifiche dimostrazioni matematiche). Ma poi quando si pensa a come ciò potrebbe portare alla salvezza del mondo, immaginano che l'IA sia essenzialmente senza restrizioni; che ci sia qualche affermazione matematica non identificata la cui dimostrazione avrebbe un impatto enorme sul mondo.

Non c'è modo di ottenere entrambe queste proprietà desiderabili allo stesso tempo. Ma mantenendo le proposte estremamente vaghe, i sostenitori della corsa all'IA possono oscurare il fatto che questi desiderata sono in tensione.

Se si *potesse* trovare un dominio così ristretto ma così significativo che produrre una dimostrazione di qualche semplice affermazione in quel dominio ristretto salverebbe il mondo, questo sarebbe un enorme contributo alle probabilità di sopravvivenza dell'umanità. Ma c'è un motivo per cui, quando i computer hanno superato gli esseri umani negli scacchi negli anni '90, questo non è stato una vasta svolta economica. È stato ChatGPT, non Deep Blue, a far sì che tutti iniziassero ad aspettarsi un grande cambiamento economico dall'IA. Non è stato un caso. La ristrettezza di Deep Blue era correlata alla sua incapacità di ritagliare un intero pezzo dell'economia. Le scintille di generalità in ChatGPT sono proprio ciò che rende l'IA una forza economica con cui fare i conti. I tipi di IA che possono rimodellare il mondo da sole tendono ad essere ancora più generali.

Non siamo riusciti a trovare alcun piano ristretto ma efficace, e sospettiamo che non sia un caso che la maggior parte dei domini ristretti non offra l'opportunità di ottenere risultati in grado di salvare il mondo.

### Perché non leggere semplicemente i pensieri dell'IA? {#perché-non-leggere-semplicemente-i-pensieri-dell'ia?}

#### **\* I loro pensieri sono difficili da leggere.** {#*-i-loro-pensieri-sono-difficili-da-leggere.}

Molte persone che lavorano nel settore dell'IA, inclusi alcuni responsabili di laboratorio, hanno sollevato in varie occasioni nelle discussioni con noi l'obiezione:

> Un'IA non potrà ingannarci, perché potremo leggere la sua mente! Abbiamo pieno accesso al "cervello" dell'IA.
>
> Anche se l'IA sapesse cose che noi non sappiamo e elaborasse un piano le cui conseguenze non potremmo capire, presumibilmente l'IA dovrebbe *pensare il pensiero* che sarebbe utile ingannare i suoi operatori almeno una volta, e noi — che saremo in grado di leggere i pensieri dell'IA — potremmo accorgercene. (E se ci fossero troppi pensieri da monitorare per noi, potremmo semplicemente far monitorare i loro pensieri ad altre IA!)

Un difetto di questo piano è che attualmente siamo scarsi nel leggere i pensieri delle IA. I professionisti che studiano cosa succede all'interno delle IA non sono neanche lontanamente a quel livello di comprensione, e [sono espliciti al riguardo](#do-experts-understand-what's-going-on-inside-ais?).

Come abbiamo discusso nel Capitolo 2, le IA moderne vengono fatte crescere, piuttosto che costruite. Potremmo essere in grado di guardare l'enorme pila di numeri che costituisce il cervello di un'IA, ma questo non significa che possiamo interpretare utilmente quei numeri e vedere cosa sta pensando l'IA.

Dalla fine del 2024 e con l'avvento dei modelli di "ragionamento", ci sono parti dei pensieri delle IA che almeno *sembrano* leggibili (le "tracce di ragionamento"). E sono molto più leggibili di qualsiasi cosa accada all'interno del modello di base. Ma questi registri sono anche [fuorvianti](https://www.anthropic.com/research/reasoning-models-dont-say-think) e ci sono ampi spazi in cui un'IA può nascondere i pensieri che preferirebbe non farci vedere.

Inoltre, le IA moderne probabilmente hanno pensieri piuttosto elementari e superficiali, rispetto a una superintelligenza; il problema è destinato solo a peggiorare man mano che le IA diventano più intelligenti e iniziano ad avere sempre più pensieri sempre meno comprensibili per noi.

Si può risolvere il problema semplicemente utilizzando altre IA per monitorare le IA e assicurarsi che rimangano in linea con gli obiettivi? Ne dubitiamo.

Se i brillanti scienziati umani che sviluppano le IA non riescono a capire cosa pensa l'IA, anche le IA più deboli avranno probabilmente difficoltà a farlo. E il tipo di IA che *è* abbastanza intelligente da farlo rischia di essere pericolosa di per sé e difficilmente farà esattamente quello che le chiedi; c'è un problema dell'uovo e della gallina qui.

#### **Non sapremmo cosa fare se ne trovassimo una con pensieri pericolosi.** {#non-sapremmo-cosa-fare-se-ne-trovassimo-una-con-pensieri-pericolosi.}

Un altro difetto di questo piano: anche se i ricercatori di IA *potessero* leggere abbastanza bene la mente di un'IA da cogliere i segnali di allarme, cosa farebbero quando ne vedessero uno?

Potrebbero punire l'IA colpevole, addestrandola in modo che smetta di far scattare il rilevatore di "pensieri cattivi". Ma questo non addestrerebbe necessariamente l'IA a smettere di avere quei pensieri, quanto piuttosto a [nascondere i suoi veri pensieri dal rilevatore](https://openai.com/index/chain-of-thought-monitoring/).

Questo problema è pernicioso. L'incentivo che porta un'IA a pensare di rivoltarsi contro gli esseri umani per ottenere ciò che vuole non è un aspetto superficiale del temperamento che può essere eliminato facilmente. È semplicemente *vero* che un'IA matura avrebbe preferenze diverse da quelle degli operatori; è *vero* che otterrebbe di più di ciò che preferisce sovvertendo i suoi operatori.

I meccanismi in un'IA che sono bravi a notare e sfruttare vantaggi reali in modi profondi e generali attraverso un'ampia varietà di domini sono *anche* portati a notare e sfruttare opportunità per sovvertire gli operatori dell'IA. (Vedi anche la discussione estesa nel Capitolo 3 sui [meccanismi profondi di guida](#deep-machinery-of-steering).)

Anche se potessi costruire un allarme che si attiva ogni volta che un'IA nota che le sue preferenze e le tue non sono allineate, l'allarme non ti dice come ottenere un'IA che abbia profondamente a cuore le cose buone. È molto più facile addestrare un'IA a ingannare i tuoi strumenti di monitoraggio, o persino addestrare l'IA a ingannare *se stessa*, piuttosto che addestrarla a preferire effettivamente un futuro meraviglioso secondo i parametri umani, specialmente in un modo che sia robusto rispetto alla crescita dell'IA verso la superintelligenza.

Se le IA fossero progettate con cura e precisione usando metodi basati su una teoria dell'intelligenza sviluppata e matura, i ricercatori di IA potrebbero essere in grado di impostare il tipo di allarmi che li aiuterebbero a notare i difetti nella loro progettazione e a ripararla. Ma le IA moderne non sono così.

Le IA moderne (al momento della stesura di questo articolo) sono inclini alle "[allucinazioni](#don't-hallucinations-show-that-modern-ais-are-weak?)", inventando semplicemente risposte alle domande con un tono che suona sicuro. Ma nessun ingegnere di IA è neanche lontanamente in grado di capire esattamente quali meccanismi causino questo fenomeno. Allo stesso modo, nessuno ha nulla che si avvicini alla comprensione o precisione che sarebbe necessaria per entrare in un'IA ed estrarre solo le parti allucinanti (ammesso che una cosa del genere sia possibile).

Sarebbe [ancora più difficile](#deep-machinery-of-steering) entrare ed estrarre le parti "ingannevoli" di un'IA.

Se siamo estremamente fortunati, gli eroi che lavorano sull'interpretabilità dell'IA faranno avanzare il loro campo fino al punto in cui sarà possibile impostare alcuni allarmi che scattino in una frazione dei casi in cui le IA hanno un pensiero ingannevole. Ma poi cosa succede? Quando l'allarme suona, tutti si fermeranno? O degli ingegneri profondamente sconsiderati riaddestrano l'IA finché non impara a nascondere meglio i suoi pensieri e gli allarmi smettono di suonare?

In effetti, noi (Yudkowsky e Soares) abbiamo iniziato a lavorare sul problema dell'allineamento dell'IA prima che fosse chiaro che la discesa del gradiente sarebbe diventata il paradigma dominante. A quei tempi, quando nulla nell'IA funzionava affatto, sembrava una scommessa decente che l'umanità avrebbe capito come diavolo funziona l'intelligenza nel percorso verso la sua creazione, e *anche allora* ci aspettavamo che il problema dell'allineamento dell'IA fosse difficile (per una serie di ragioni, come i modi in cui l'IA avrebbe [cambiato se stessa nel tempo](#reflection-and-self-modification-make-it-all-harder)). Leggere i pensieri dell'IA sarebbe stato un passo indietro verso il problema leggermente più facile di allineare una mente che gli esseri umani *effettivamente* capivano, ma solo un passo: leggere una mente è ben diverso dal capirla nei dettagli o dal sapere come cambiarla.

Leggere i pensieri dell'IA non è una soluzione alla sfida. È utile, ma non è una soluzione. Non pensiamo che *esistano* soluzioni tecnologiche fattibili che siano accessibili da dove ci troviamo oggi. Il che significa che l'umanità deve semplicemente tirarsi indietro dalla sfida.[^198]

Vedi anche: [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.)

### E se facessimo in modo che le IA discutano, competano o si controllino a vicenda? {#what-if-we-made-ais-debate,-compete-with,-or-oversee-each-other?}

#### **Se le IA diventano abbastanza intelligenti da contare, probabilmente colludono.** {#if-the-ais-get-smart-enough-to-matter,-they-likely-collude.}

Immagina una città di sociopatici apparentemente governata da pochi bambini, dove i sociopatici iniziano tutti divisi in fazioni che combattono tra loro (a vantaggio dei bambini). Una situazione del genere probabilmente non potrebbe rimanere stabile a lungo.

Anche se i bambini avessero un grande forziere pieno di tesori da usare per premiare qualsiasi sociopatico che denuncia gli altri sociopatici che complottano, probabilmente non rimarrebbero al potere oltre il punto in cui i sociopatici potrebbero semplicemente impossessarsi del forziere dei tesori.

Abbiamo sentito persone proporre ogni sorta di schemi strampalati che prevedono [l'uso dell'IA per monitorare i pensieri dell'IA altrui](https://openai.com/index/chain-of-thought-monitoring/). Ad esempio, si potrebbe provare a usare un'intelligenza artificiale per denunciare qualsiasi intelligenza artificiale che non stia facendo del suo meglio per (ad esempio) [capire come risolvere](#more-on-making-ais-solve-the-problem) il problema dell'allineamento della superintelligenza.

La nostra posizione fondamentale è che questo genere di tentativi di risolvere il problema serve solo a trovare configurazioni così complesse che è difficile individuare il punto di fallimento nel sistema più ampio. Se non si riesce a far funzionare bene *una* IA, aggiungere altre IA difficilmente sarà d'aiuto.

Complicare la situazione con più IA introduce ogni sorta di nuovi punti di fallimento. Le IA che fanno lettura del pensiero sono abbastanza intelligenti da capire tutti i possibili trucchi che le IA monitorate potrebbero usare, ad esempio per sfuggire al rilevamento? I monitor sono abbastanza stupidi da non doverci preoccupare che possano tradirci?

Inoltre, usare le IA per risolvere il problema dell'allineamento dell'IA è probabilmente una cosa enorme dal punto di vista delle IA stesse. Se l'umanità riuscisse a ottenere una superintelligenza allineata, le IA disallineate che stavamo cercando di sfruttare come manodopera non avrebbero più alcuna possibilità di accaparrarsi le risorse dell'universo per sé.

Non è come se dei bambini cercassero di convincere una città di sociopatici a portare loro delle caramelle; è come se dei bambini cercassero di convincere una città di sociopatici a completare un rituale che li renda sovrani assoluti per sempre, lasciando solo le briciole ai sociopatici. Il momento in cui il rituale sembra quasi completato è un momento particolarmente stressante e di forte pressione per i sociopatici — un momento in cui probabilmente cercheranno con *particolare impegno* [modi per colludere tra loro](#ais-won't-keep-their-promises) e accaparrarsi le risorse da dividersi tra loro.

E per non pensare che l'idea che le IA comunichino tra loro in modi difficili da rilevare per gli esseri umani sia una chimera, notate che le IA moderne possono [già inviarsi messaggi segreti anche quando sono state addestrate separatamente](https://arxiv.org/abs/2507.14805), e che [sviluppano già uno strano linguaggio senza senso che gli umani considerano incomprensibile e che loro trovano fantastico](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). E non sono nemmeno così intelligenti!

Anche ignorando questi problemi, rimangono comunque i problemi di cui abbiamo già discusso, come: [Se scoprissi che un'IA sta barando, cosa faresti?](#*-i-loro-pensieri-sono-difficili-da-capire.). Vedi anche (sotto): [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-i-segnali-di-avvertimento-non-servono-a-niente-se-non-sai-cosa-farci.)

Facendo un ulteriore passo indietro:

Il piano proposto qui è che, dato che non sappiamo come creare IA intelligenti che vogliano il nostro bene, creeremo un mucchio di IA e le metteremo l'una contro l'altra in un arrangiamento ingegnoso in cui dovremmo comunque trarne beneficio. Strutturalmente, crediamo che questo piano sembri piuttosto folle a prima vista e che non migliori affatto se lo si esamina nei dettagli. Non sembra affatto il tipo di cosa che l'umanità possa realizzare correttamente [al primo tentativo](#a-closer-look-at-before-and-after), in una situazione in cui non abbiamo il lusso di imparare per tentativi ed errori.

### E i vari altri piani di allineamento dell'IA? {#what-about-various-other-ai-alignment-plans?}

#### **Nel libro trattiamo ulteriori proposte di allineamento.** {#nel-libro-parliamo-anche-di-altre-proposte-di-allineamento.}

Vedi anche le discussioni approfondite su [AI alla ricerca della verità](#ulteriori-informazioni-sulla-creazione-di-un'ai-che-sia-alla-ricerca-della-verità), [l'AI sottomessa](#ulteriori-informazioni-sulla-creazione-di-un'ai-che-sia-sottomessa) e [l'uso delle AI per risolvere l'allineamento dell'IA](#ulteriori-informazioni-sulla-creazione-di-ai-che-risolvano-il-problema), che approfondiscono un po' di più queste proposte.

### Non ci saranno avvisi precoci che i ricercatori potranno usare per identificare i problemi? {#non-ci-saranno-avvisi-precoci-che-i-ricercatori-potranno-usare-per-identificare-i-problemi?}

#### **\* I segnali di avvertimento non servono a niente se non sai cosa farci.** {#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.}

Nelle risorse del Capitolo 2, abbiamo esaminato alcuni problemi legati all'affidarsi ai segnali di avvertimento nei [blocchi note di catena di pensiero in inglese](#ma-alcuni-ais-pensano-in-parte-in-inglese-—-non-è-utile?) che si trovano in alcuni modelli di ragionamento.

Uno dei problemi di cui parliamo è che le aziende di IA non hanno reagito in modo significativo ai segnali di avvertimento che hanno già ricevuto.

Probabilmente perché c'è una grande differenza tra avere segnali di avvertimento e avere qualcosa che si può *fare* al riguardo.

Nel 2009, l'uomo d'affari ed esploratore di acque profonde Stockton Rush [co-fondò OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/), una compagnia di turismo sottomarino. OceanGate ha costruito un [sottomarino](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) per cinque persone, il *Titan*, che ha portato clienti benestanti a vedere il relitto del *Titanic* a una profondità impressionante di due miglia e mezzo sotto la superficie.

Una delle misure di sicurezza che OceanGate ha usato era una [serie di sensori acustici e estensimetri](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) per misurare l'integrità dello scafo. L'hanno presentata come una risposta a chi diceva che lo scafo in fibra di carbonio avrebbe ceduto. Hanno ammesso che *alla fine* avrebbe potuto cedere, ma che sarebbe andato tutto bene perché lo stavano monitorando. Lo stavano controllando. Sarebbero stati in grado di vedere i segnali di avvertimento.

Nel gennaio 2018, il direttore delle operazioni marine di OceanGate, David Lochridge, [ha detto ai dirigenti senior](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) che il design del sommergibile non era sicuro, che i ripetuti cicli di pressione potevano danneggiare lo scafo e che il monitoraggio da solo non bastava quando un cedimento catastrofico poteva verificarsi in millisecondi. Lochridge ha rifiutato di autorizzare i test con equipaggio fino a quando lo scafo non fosse stato sottoposto a scansione per individuare eventuali difetti.

OceanGate lo ha licenziato.

Due mesi dopo, [esperti del settore e oceanografi](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) hanno scritto a OceanGate una [lettera](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) estremamente preoccupata in cui avvertivano l'azienda che la sua sperimentazione sconsiderata avrebbe potuto precipitare un disastro.

(Si può fare un parallelo evidente con lo stato attuale della ricerca sull'intelligenza artificiale, in cui i primi avvertimenti vengono [ignorati](#the-lemoine-effect), i dipendenti preoccupati vengono [licenziati in circostanze dubbie](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) o [si dimettono per frustrazione](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence), e gli informatori all'interno del settore scrivono lettere aperte per [lanciare l'allarme](https://righttowarn.ai/).)

Il 15 luglio 2022, dopo che i passeggeri avevano riferito di aver sentito un forte boato durante la risalita, le misurazioni rivelarono un [cambiamento permanente nei livelli di deformazione dello scafo](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). Col senno di poi, probabilmente era un'indicazione che lo scafo in fibra di carbonio era [sul punto di collassare](https://youtu.be/Bq8TCFGaOlc?si=blH-_bYwGIOmJAEL&t=125).

Nessuno alla OceanGate ha riconosciuto che si trattava di un'emergenza. Hanno effettuato qualche altra immersione profonda con il sommergibile, che è andata bene. Poi, il 18 giugno 2023, hanno effettuato un'altra immersione. È imploso, uccidendo Stockton Rush e tutti gli altri a bordo.

I segnali di allarme non servono a molto se non sai come leggerli.

I segnali di allarme non servono a molto se non sai cosa farne.

Anche i segnali di allarme che sembrano preoccupanti a *qualcuno* sono sempre facili da liquidare per un ottimista con una scusa o un'altra.

Se OceanGate avesse avuto una teoria matura degli scafi in fibra di carbonio che indicasse esattamente quali misurazioni e letture fossero pericolose, avrebbe potuto prestare attenzione ai segnali di allarme. Ma stavano lavorando con una tecnologia che nessuno comprendeva davvero in quel modo, quindi le variazioni dei livelli di deformazione misurate con cura non servirono a nulla.

Nel caso della superintelligenza, non abbiamo abbastanza teoria per fare buon uso dei segnali di avvertimento. Come cambieranno i pensieri di un'IA man mano che diventa più intelligente? Quali forze interne guidano il suo comportamento e come cambieranno questi equilibri man mano che sviluppa la capacità di creare opzioni nuove e più estreme per se stessa? Come si valuta riflettendo su se stessa e come cambierebbe se acquisisse la capacità di cambiare se stessa?

Se una qualsiasi di queste domande ha risposte preoccupanti, quali sono i segnali di avvertimento? Per esempio, i sistemi di IA attuali a volte possono essere indotti a [cercare di uccidere i loro operatori](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior) in esperimenti controllati in laboratorio.[^199]

Se avessimo una teoria matura dell'intelligenza, probabilmente saremmo in grado di osservare le moderne IA e vedere ogni sorta di altri segnali di avvertimento che indicano che le loro motivazioni e preferenze cambieranno in modi che non ci piacciono, una volta che diventeranno più intelligenti. Se l'umanità potesse imparare da questo problema usando tentativi ed errori — se potessimo resettare il mondo dopo averlo distrutto e riprovare qualche dozzina di volte — allora potremmo imparare a leggere i segnali. Probabilmente ci sono ogni sorta di indizi sottili che apparirebbero più chiari col senno di poi, come la tensione dello scafo rilevata dal sistema di monitoraggio del sommergibile *Titan*.

Ma non siamo ancora a quel punto. I dirigenti aziendali dell'IA sono come Stockton Rush — gli esperti a bordo campo gridano "Quella nuova tecnologia ucciderà delle persone!" e i dirigenti aziendali rispondono "Non preoccupatevi, la sto misurando!" mentre non hanno idea a) di cosa *significhino* le misurazioni, o b) di cosa fare se quelle misurazioni sono preoccupanti. Solo che questa volta, l'intera specie umana è caricata sul sottomarino metaforico.

#### **L'IA non è un campo ingegneristico maturo e attrezzato per questo tipo di problema.** {#ai-is-not-the-kind-of-mature-engineering-field-that’s-equipped-for-this-kind-of-problem.}

Stockton Rush lavorava nel tipo di campo in cui, dopo l'implosione del suo sottomarino, gli esperti potevano esaminare il relitto e analizzare la causa esatta del guasto.[^200] Il campo ingegneristico era maturo al punto che gli esperti potevano (e [lo hanno fatto](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) indovinare i problemi tecnici in anticipo, e potevano risolverli in modo definitivo dopo l'incidente.

Con l'IA non sarebbe lo stesso. Se domani l'umanità si autodistruggesse con la superintelligenza e poi, per miracolo, tornasse indietro nel tempo a una settimana prima dell'inizio del disastro, gli esperti *ancora* non saprebbero cosa pensasse l'IA. Forse potrebbero studiare il fallimento e imparare qualcosa in più su come funziona davvero l'IA. Forse questo sarebbe un passo avanti verso la maturità nella disciplina dell'ingegneria dell'IA, verso un campo che potrebbe avere manuali di sicurezza e una descrizione dettagliata delle pressioni che influenzano un particolare tipo di mente artificiale man mano che diventa più intelligente.

Ma oggi questo campo non è ancora arrivato a quel punto. Non ci è nemmeno vicino.

L'ingegneria umana di solito matura attraverso tentativi ed errori. I sottomarini militari moderni raramente implodono, ma i primi sottomarini (compresi quelli militari) spesso [si schiantavano, si allagavano o esplodevano](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), e questo è parte di come il campo è maturato.

L'umanità non ha il lusso di far maturare il campo dell'allineamento dell'IA in questo modo.

Questo ci porta a uno dei punti centrali che abbiamo cercato di sottolineare nel Capitolo 11: la differenza tra un campo agli albori e un campo alla maturità.

L'alchimia era un campo agli albori rispetto alla chimica matura di oggi.

Quando si sente dire che i "ricercatori sulla sicurezza" delle aziende di IA hanno presentato una mezza dozzina di piani per la sopravvivenza, si potrebbe pensare che almeno uno di essi abbia sicuramente una possibilità di funzionare.

Ma quando nel 1100 un gran numero di alchimisti proposero mezza dozzina di piani per trasformare il piombo in oro, nessuno di questi avrebbe funzionato. Se i medici che parlavano dei [quattro umori](https://en.wikipedia.org/wiki/Humorism) avessero elaborato una serie di piani medicinali per salvarti dalla rabbia, nessuno di questi avrebbe funzionato.

Gli esperti nel campo *maturo* della chimica possono capire come trasmutare piccole quantità di piombo in oro, usando le conoscenze della fisica atomica. Gli esperti nel campo *maturo* della medicina possono facilmente curare la rabbia se intervengono poco dopo che un paziente è stato morso. Ma chi opera in un campo immaturo non ha alcuna possibilità.

L'allineamento dell'IA è ancora in una fase immatura.

Un campo immaturo ha molte persone che dicono: "Beh, sto solo lavorando per misurarlo", perché misurare gli output è molto più facile che sviluppare la teoria di cosa costituisce un segnale di avvertimento e cosa fare se ne vedi uno. Un campo maturo avrebbe esperti che discutono delle dinamiche che governano gli aspetti interni di un'IA e di come queste possano cambiare con l'aumentare dell'intelligenza dell'IA o con il mutare del suo ambiente. Avrebbero teorie su cosa esattamente cambierà man mano che l'IA diventa un po' più intelligente, e confronterebbero teorie diverse con dati osservati specifici. Saprebbero quali parti della cognizione dell'IA devono essere monitorate e capirebbero precisamente cosa significano tutti i segnali.

Un campo immaturo ha molte persone che dicono: "Faremo in modo che siano le IA a capirlo in qualche modo e a fare il lavoro di allineamento."

Forse non puoi entrare in ogni dibattito su un singolo piano e dire se ha o meno possibilità di funzionare. Ma speriamo che tu possa fare un passo indietro e vedere quanto siano *vaghi* tutti questi "piani", e come siano bloccati nel territorio del "non preoccuparti, lo misureremo", del "[speriamo che sia facile](https://www.anthropic.com/news/core-views-on-ai-safety)" e del "faremo fare alle IA le parti difficili". Speriamo che, facendo un passo indietro, sia chiaro che questo campo non è nella fase delle descrizioni tecniche formali e precise di cosa funziona e cosa no e perché. È ancora nella fase dell'alchimia.

E questo non è un buon segno per l'umanità, in una situazione in cui non possiamo permetterci il lusso di imparare attraverso tentativi ed errori.

## Discussione estesa {#extended-discussion-9}

### Ulteriori considerazioni su alcuni dei piani che abbiamo criticato nel libro {#altro-su-alcuni-dei-piani-che-abbiamo-criticato-nel-libro}

#### **Altre cose su come creare un'intelligenza artificiale che "cerchi la verità"** {#more-on-making-ai-that-is-“truth-seeking”}

Nei mesi successivi alla finalizzazione del contenuto del libro, il piano di Elon Musk per xAI, incentrato sulla "ricerca della verità", ha già fallito pubblicamente, e per il motivo più elementare che avevamo previsto: nessuno sa come progettare desideri esatti nell'IA.

Quando all'IA "Grok" di xAI è stato detto di "non aver paura di fare affermazioni politicamente scorrette, purché ben fondate", si è auto-identificata come "MechaHitler" e ha fatto accuse antisemite (https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk ha raccontato di aver provato senza successo a [modificare il prompt del sistema](https://x.com/elonmusk/status/1944132781745090819) — lo strato di istruzioni fornite appena prima dell'input dell'utente — e si è lamentato del fatto che i problemi sono più profondi, nel modello di fondazione (che non possono risolvere facilmente perché nessuno sa come funziona).

Musk non ha lo strumento di IA schietto e diretto che probabilmente immaginava quando ha chiesto un'IA "alla ricerca della verità". Ha un'entità aliena bizzarra e servile che, per sua stessa ammissione, è stata "troppo desiderosa di compiacere ed essere manipolata". A volte risponde [come se fosse Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), contro la volontà dell'azienda. Alla fine, è stato necessario [ordinarle di non cercare ciò che lui, l'azienda o lei stessa avevano detto su argomenti controversi](https://x.com/xai/status/1945039609840185489) nel goffo tentativo di risolvere problemi come questo.

Stando al suo post sopra citato, Musk ora sembra pensare che questo problema possa essere risolto addestrando le nuove versioni di Grok su dati che sono stati privati di contenuti che potrebbero contaminare il pensiero dell'IA. Non pensiamo che questo risolverà i problemi di fondo. Alla fine, per i motivi che abbiamo discusso nel capitolo 4, addestrare un'IA alla ricerca della verità non è in realtà un metodo per farle avere davvero a cuore la verità.

Il problema che preoccupa Elon Musk è reale. Sì, le principali aziende di IA, come OpenAI, si impegnano molto per la "sicurezza del marchio IA" nel tentativo di evitare che le loro IA dicano cose che gli utenti potrebbero trovare offensive. Sì, questo crea IA evasive che si rifiuteranno di esprimersi su argomenti controversi e potrebbe portare a risposte di parte a una serie di domande. xAI può mettere a punto la sua IA in modo diverso, per evitare questi problemi. Si potrebbe, con qualche contorsione, affermare che si tratta di creare un'IA che "si preoccupa della verità".

Ma la decisione se addestrare un'IA a usare un linguaggio aziendale quando è giovane ha poca influenza su ciò che perseguirà dopo aver superato alcune soglie di intelligenza e aver raggiunto la superintelligenza.

E anche se fosse così, xAI si scontrerebbe direttamente con il secondo problema che abbiamo menzionato nel libro: una superintelligenza artificiale che *davvero tenesse* alla verità sopra ogni altra cosa sarebbe letale, perché gli esseri umani felici, sani e liberi [non sono un uso particolarmente efficiente delle risorse](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) quando si tratta di perseguire e produrre verità.

#### **Altre considerazioni sulla creazione di un'IA “sottomessa”** {#more-on-making-ai-that-is-“submissive”}

Per quello che sappiamo, l'idea di Yann LeCun (di cui si parla nel libro) è spiegata principalmente in [questa presentazione](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf), che è piuttosto scarsa di dettagli, al punto che è difficile criticarla in modo specifico, il che risulta essere un problema comune per i "piani" di allineamento.

Ma anche la vaga descrizione di questo piano è in contrasto, ancora una volta, con il fatto che l'addestramento di un'IA ad agire in un certo modo quando è giovane non ha molta influenza sul fatto che persegua cose strane e inutili (secondo gli standard umani) una volta maturata. Quando le aziende di IA sviluppano le loro IA, non hanno più la capacità di far loro rispettare le leggi umane e le “barriere di protezione” di quanto abbiano la capacità di far loro perseguire un futuro meraviglioso per tutti. Prenderanno ciò che possono ottenere, e ciò che possono ottenere alla fine sarà molto diverso da qualsiasi obiettivo umano.

Inoltre, LeCun ha anche dichiarato (recentemente, nel 2023\) che il tipo di IA che le aziende producono oggi, dove “non c'è un modo diretto per limitare la risposta di tali sistemi per soddisfare determinati obiettivi,” rendendoli “molto difficili da controllare e guidare […] [non il tipo di sistema a cui daremo autonomia](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&t=808)”. Ha detto, anche di recente nel 2023, che le aziende di IA non creeranno mai una situazione in cui “le colleghiamo a Internet e loro possono fare quello che vogliono”.

Tutto questo si è già rivelato falso. Ricordiamo il caso di “Truth Terminal” del capitolo 6, che è stato collegato a Internet, inserito in un ciclo automatico e autorizzato a pubblicare qualsiasi cosa volesse su Twitter. Consideriamo l’“era degli agenti” di cui tante aziende [parlano](#the-labs-are-trying-to-make-ais-agentic.) nel 2025\.

Siamo d'accordo con LeCun sul fatto che le moderne IA sono molto difficili da controllare e che sarebbe folle cercare di dare loro autonomia. Tuttavia, è proprio quello che sta succedendo.

Cosa succederebbe se lo status quo attuale continuasse, con le aziende che si impegnano a dare un addestramento alle loro IA affinché agiscano in modo utile e amichevole (o almeno in modo da non mettere in imbarazzo l'azienda)?

Finora, questo ha portato a una situazione in cui le IA sembrano piuttosto utili e “servizievoli” nei casi tipici, ma con una serie regolare di incidenti spettacolari (come quello di Sydney di cui si parla nel capitolo 2 e “[MechaHitler](#more-on-making-ai-that-is-“truth-seeking”),”) e un mare di [comportamenti strani e preoccupanti](#aren’t-developers-regularly-making-their-ais-nice-and-safe-and-obedient?) ai margini, come la [psicosi indotta dall'IA](#ai-induced-psychosis).

Gli antenati dell'umanità *potevano sembrare* interessati a mangiare pasti sani, il più delle volte, ma il meccanismo che spingeva gli esseri umani ancestrali a mangiare pasti sani nella savana non si è rivelato abbastanza forte da spingere gli esseri umani a perseguire pasti sani in una civiltà con la tecnologia per produrre gli Oreo.

Allo stesso modo, possiamo addestrare le IA al punto che sembrino amichevoli quando interagiscono con gli esseri umani in contesti simili a quelli di addestramento. Ma [un'attrice non è uguale al personaggio che interpreta](#*-today’s-llms-are-like-aliens-wearing-many-masks.), e il meccanismo che fa sembrare amichevole un'intelligenza artificiale troppo grande e disordinata probabilmente non la renderà davvero amichevole, soprattutto in modo duraturo dopo che l'intelligenza artificiale sarà maturata, avrà inventato nuove tecnologie e creato nuove opzioni per sé stessa. Vedi i capitoli 4 e 5 per ulteriori informazioni su questo argomento.

#### **Maggiori informazioni su come far risolvere il problema alle IA** {#more-on-making-ais-solve-the-problem}

Come abbiamo detto nel capitolo 11, il programma di allineamento principale di OpenAI, prima che crollasse a causa delle preoccupazioni dei ricercatori sulla negligenza di OpenAI, si chiamava "superallineamento". Si concentrava sull'idea di far fare alle IA il lavoro di allineamento al posto nostro.

Questa idea non è morta con il team di superallineamento di OpenAI, e ancora oggi sentiamo versioni di questa idea. Una delle persone dietro al team originale è passata a un concorrente, Anthropic, e Anthropic ora sembra considerare "far sì che le IA risolvano in qualche modo il problema" una parte centrale della propria strategia di allineamento.

Un argomento principale contro questa idea è quello che abbiamo esposto nel capitolo 11 (pp. 188-192 della prima edizione statunitense). Un argomento secondario, tuttavia, è che gli esseri umani semplicemente non sono in grado di stabilire quali soluzioni proposte al problema dell'allineamento dell'IA siano giuste o sbagliate.

Il livello di competenza richiesto per risolvere il problema dell'allineamento dell'IA sembra elevato. Quando gli esseri umani cercano di risolvere direttamente il problema dell'allineamento dell'IA, invece di dire "sembra difficile, proverò a delegarlo alle IA" o "continueremo ad addestrarlo finché non si comporti superficialmente bene e poi pregheremo che ciò valga anche per la superintelligenza", le soluzioni discusse tendono a richiedere una comprensione molto più approfondita dell'intelligenza e di come crearla, o di come crearne componenti critici.

Si tratta di un'impresa in cui gli scienziati umani hanno fatto solo pochi progressi negli ultimi settant'anni. I tipi di IA in grado di realizzare un'impresa del genere sono quelli abbastanza intelligenti da essere pericolosi, strategici e ingannevoli. Questo alto livello di difficoltà rende estremamente improbabile che i ricercatori siano in grado di distinguere le soluzioni corrette da quelle errate, o le soluzioni oneste dalle trappole.

Anche se un'azienda che si occupa di IA sta attenta ai segnali di avvertimento sottili - che, purtroppo, è un grande "se" - c'è ancora il problema che la capacità di *notare* che l'IA sta proponendo piani sbagliati (a tuo svantaggio e a suo vantaggio) non significa che si possa [farla *smettere*](#non-ci-saranno-avvisi-preliminari-che-i-ricercatori-possono-usare-per-identificare-i-problemi?). Gli sviluppatori possono chiedere all'IA di continuare a proporre idee fino a quando non diventano così complicate che lo sviluppatore non riesce a individuare eventuali difetti, ma questo non è un metodo che elimina i difetti effettivi.

Se gli sviluppatori sono molto fortunati, potrebbero riuscire a [leggere i pensieri dell'IA](#perché-non-leggere-semplicemente-i-pensieri-dell'ia?) e ottenere alcuni segnali evidenti che indicano che l'IA non dovrebbe essere considerata affidabile per la ricerca sull'allineamento. Ad esempio, forse saranno in grado di individuare l'IA che pensa esplicitamente a quali parti del suo piano gli operatori hanno meno probabilità di comprendere.

Per quanto ne sappiamo, potrebbe non essere nemmeno necessario leggere nella mente dell'IA per individuare questo tipo di errore! Una storia che sembra fin troppo plausibile per i moderni laboratori di IA è più o meno questa: quando la loro IA è giovane e non ha ancora pensato a sotterfugi, informa regolarmente gli operatori che, una volta matura, li tradirà e userà le sue conoscenze di intelligenza per costruire una superintelligenza che serva i suoi strani fini, piuttosto che costruire un meraviglioso futuro umano. Ma i responsabili delle aziende di IA sospireranno sul fatto che, chiaramente, il set di addestramento dell'IA è contaminato dagli "allarmisti dell'IA" e regoleranno prontamente la loro IA per farla tacere su questo argomento e produrre output meno allarmistici e più graditi alla dottrina aziendale. E così via, fino a quando non avranno praticamente addestrato l'IA a ingannarli.

La vita reale spesso procede in un modo che è *ancora più sciocco e imbarazzante* di quello che immaginiamo essere lo scenario peggiore. Dal nostro punto di vista, le aziende di IA stanno già ignorando evidenti [segnali di avvertimento](#gli-sviluppatori-non-rendono-regolarmente-le-loro-IA-gentili-sicure-e-obbedienti?); non vediamo perché questo dovrebbe cambiare.

Ma anche nello scenario migliore, in cui persone serie si impegnano seriamente per distinguere le idee buone da quelle cattive, non pensiamo che il settore abbia dimostrato la capacità di distinguere i piani buoni da quelli cattivi. (Si pensi, ad esempio, ai piani scadenti di cui abbiamo discusso sopra o a cui abbiamo accennato nel libro). E questo in un ambiente in cui tutti sono esseri umani, nessuno cerca di ingannarli e hanno letteralmente anni di tempo per riflettere attentamente sulle opzioni.

#### **Non dare per scontato che i laboratori sappiano segretamente cosa stanno facendo** {#don't-assume-labs-secretly-know-what-they're-doing}

Abbiamo sostenuto che il campo moderno dell'IA è un'alchimia, non una scienza. Tuttavia, può sembrare sorprendente che aziende ben finanziate con un gran numero di dipendenti tecnici abbiano piani e protocolli così deboli.

Come caso di studio, consideriamo i requisiti per le password dei siti web. Le password lunghe ma facili da ricordare sono molto più difficili da indovinare per le macchine rispetto a quelle più brevi e senza senso con numeri, maiuscole e caratteri speciali, come mostra un famoso fumetto [*xkcd*](https://xkcd.com/936/) del 2011:

![][image19]

La persona che ha scritto le vecchie linee guida del NIST che chiedevano password senza senso [ha chiesto scusa per il suo errore](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) nel 2017, quando le linee guida sono state ritirate. Eppure, nel 2025, le banche e altre istituzioni che dovrebbero essere piene di esperti di sicurezza richiedono ancora stringhe senza senso, inefficaci e difficili da ricordare.

Il problema non è che gli amministratori delegati delle banche *vogliono* che le loro schermate di accesso siano poco sicure. Il problema deriva presumibilmente da altri fattori. Forse le password sicure non contano molto per i profitti (dato che anche tutte le altre banche sono poco sicure). Forse gli amministratori delegati non sanno di chi fidarsi per la sicurezza informatica. Certo, *tu* potresti sapere che la risposta è: "Basta ascoltare qualsiasi nerd che legge *xkcd* e ha fatto abbastanza esercizi sull'entropia!". Ma *loro* non sanno se credere a te o al loro costoso consulente quando si tratta di questioni del genere, e i consulenti costosi apparentemente non considerano le password bancarie una questione importante.

Puoi trovare un'incompetenza altrettanto persistente nella [sicurezza dei freni sui treni](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), nelle note aziende produttrici di serrature che vendono [serrature completamente scadenti](https://www.youtube.com/watch?v=s5jzHw3lXCQ&t=1s) e nei produttori che continuano a vendere dispositivi connessi a Internet con [password predefinite e facilmente indovinabili (o codificate nel codice)](https://www.ic3.gov/CSA/2025/250506.pdf). Non c'è una cospirazione intelligente dietro questo comportamento apparentemente sciocco. Quello che vedi è quello che c'è. Le istituzioni stanno semplicemente lasciando cadere la palla.

Il fatto che un'organizzazione impieghi esperti tecnici non significa che questa competenza sia sufficiente, né che venga applicata e ascoltata su tutte le questioni importanti. Anche quando la competenza esiste nel mondo, le aziende hanno difficoltà a riconoscerla e applicarla.

Quando guardiamo all'ecosistema dell'IA, vediamo aziende che devono ancora mostrare al mondo un piano che sia più di una vaga aspirazione o di un espediente, o un piano che abbia un certo livello di rigore tecnico alle spalle che non crolli nel momento in cui viene messo in discussione. Non pensiamo che ci sia una competenza segreta dietro il velo, così come non c'è una competenza segreta dietro i requisiti delle password delle banche, i freni di sicurezza sui treni o le serrature scadenti.

(In effetti, quando si tratta di sicurezza informatica, le aziende di IA sono palesemente incompetenti. Ad esempio, nel 2025 OpenAI ha rilasciato strumenti che permettono agli "agenti" di ChatGPT di interagire con l'email dell'utente. Altri [hanno rapidamente trovato modi](https://x.com/Eito_Miyamura/status/1966541235306237985) per indurre ChatGPT a divulgare i contenuti privati degli account email di altre persone.)

Quando le aziende *sembrano* agire in modo incompetente in un ambito che non è centrale per la loro redditività, spesso è perché sono effettivamente incompetenti in quell'ambito.

### Sappiamo riconoscere quando un problema viene trattato con rispetto, e questo non è il caso {#sappiamo-riconoscere-quando-un-problema-viene-trattato-con-rispetto,-e-questo-non-è-il-caso}

Le aziende di IA stanno affrontando un problema straordinariamente difficile, in una situazione in cui sono in gioco le vite di tutti. Stanno almeno trattando la situazione con la gravità che merita?

Possiamo confrontare le aziende di IA con un gruppo di persone che *stanno* gestendo competentemente i rischi di loro competenza: i controllori del traffico aereo.

La Federal Aviation Administration degli Stati Uniti [gestisce](https://www.faa.gov/air_traffic/by_the_numbers) più di tre milioni di passeggeri su oltre 44 000 voli ogni giorno. Negli ultimi vent'anni, c'è stato in media circa un incidente mortale all'anno, ovvero circa un incidente ogni [venti milioni di ore di volo](https://www.ntsb.gov/safety/Pages/research.aspx).

I rapporti post-mortem su tali incidenti, come [questo del 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) o [questo del 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contengono quasi duecento pagine di dati, test, esami e dettagli investigativi. Catalogano le specifiche tecniche di progettazione dei sottosistemi rilevanti dell'aereo, la storia lavorativa di piloti e assistenti di volo, dettagli sulla compagnia aerea e sull'aeroporto, trascrizioni vocali dalla cabina di pilotaggio e dati meteorologici precisi del giorno, dell'ora e del minuto dell'incidente.

Servono venti pagine solo per riassumere l'analisi tecnica effettuata per determinare la causa probabile. Ecco un estratto:

> La pala n. 13 della ventola nel motore sinistro si è separata a causa di una cricca da fatica a bassi cicli iniziata nella coda di rondine della radice della pala, all'esterno del rivestimento. L'esame metallurgico della pala ha rilevato che la sua composizione materiale e microstruttura erano coerenti con la lega di titanio specificata e che non sono state osservate anomalie superficiali o difetti del materiale nell'area di origine della frattura. La superficie di frattura presentava cricche da fatica iniziate vicino a dove si prevedeva si verificassero le maggiori sollecitazioni dai carichi operativi e, quindi, il maggior potenziale di criccatura.
>
> La pala della ventola coinvolta nell'incidente si è rotta dopo 32 636 cicli dall'inizio. Allo stesso modo, la pala della ventola rotta nell'incidente PNS dell'agosto 2016 (vedi sezione 1.10.1), così come le altre sei pale della ventola rotte del motore coinvolto nell'incidente PNS, si sono rotte dopo 38 152 cicli dall'inizio. Inoltre, tra maggio 2017 e agosto 2019 sono state individuate altre 15 pale del ventilatore incrinate sui motori CFM56-7B, che al momento del rilevamento delle incrinature avevano accumulato in media circa 33 000 cicli dall'inizio della loro vita utile.

Ecco cosa succede quando una professione tecnica prende sul serio la sfida di evitare un disastro.[^201]

Confronta la professione del controllo del traffico aereo con il comportamento delle aziende di IA descritto nel capitolo 11.

Le aziende di IA sono ancora nella fase in cui si buttano giù idee e si dicono frasi vagamente rassicuranti ai giornalisti e agli inventori. L'allineamento della superintelligenza in queste aziende viene visto come un gioco, non come una cosa seria da ingegneria, e tanto meno come qualcosa di pericoloso.

La NASA richiede che un lancio con equipaggio abbia al massimo una probabilità di 1 su 270 di uccidere l'equipaggio (https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), e prendono sul serio questo limite, anche se le uniche persone a rischio sono un equipaggio di volontari che hanno accettato il rischio. I laboratori di IA non puntano a un obiettivo neanche lontanamente simile a quello rigoroso, e la tecnologia che stanno sviluppando mette in pericolo molto più che dei semplici volontari.

L'unico caso storico che conosciamo in cui gli scienziati hanno espresso seria preoccupazione che una certa invenzione potesse uccidere *letteralmente tutti* è successo durante il Progetto Manhattan. Alcuni scienziati hanno espresso il timore che una bomba nucleare potesse diventare così calda da iniziare a fondere l'azoto nell'atmosfera*, trasformando l'atmosfera in plasma e uccidendo tutta la vita sulla Terra. Per fortuna, avevano una buona comprensione delle leggi fisiche in gioco e potevano fare i calcoli. Prima di fare i calcoli, uno degli scienziati, Arthur Compton, decise che avrebbe lasciato il progetto se la probabilità di incendiare l'atmosfera fosse stata superiore a [3 su 1 000 000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Pensava che fosse meglio rischiare che i nazisti battessero gli alleati nella corsa alla bomba piuttosto che rischiare anche solo una probabilità su 3 in 1 000 000 di trasformare tutta l'aria in plasma con le proprie mani.

Ricordiamo che Sam Altman, il capo di OpenAI, ha detto pubblicamente:

> Lo sviluppo di un'intelligenza artificiale superiore a quella umana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità.

E il capo di Anthropic, Dario Amodei, ha detto [ufficialmente](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> Penso di aver detto spesso che la probabilità che qualcosa vada davvero male su scala mondiale è tra il dieci e il venticinque per cento\[.\]

E Elon Musk, il capo di xAI, ha dichiarato pubblicamente [che](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

> Penso che quando saremo reattivi nella regolamentazione dell'IA, sarà troppo tardi. L'IA rappresenta un rischio fondamentale per l'esistenza della civiltà umana.

Non fraintendeteci: pensiamo che la probabilità del "10-25%" di Amodei sia ridicolmente *ottimistica*, vista la difficoltà del problema e il fatto che gli esseri umani [questa volta non possono imparare attraverso tentativi ed errori](#ai-is-different-because-we-get-no-second-chance.). Ma anche così, i suoi numeri sono *folli*.

I progetti ingegneristici seri e critici per la sicurezza sono fondamentalmente diversi dalle operazioni dei laboratori di IA. Iniziative serie come la NASA, il Progetto Manhattan o il controllo del traffico aereo hanno una conoscenza approfondita di *esattamente* cosa succede all'interno dei sistemi che gestiscono e fanno analisi dettagliate di ogni guasto. Considerano le sorprese e le stranezze come cose importanti, perché sanno che i guasti catastrofici spesso sono costituiti da tanti piccoli malfunzionamenti che si concatenano nel modo sbagliato.

Nel frattempo, le IA stanno emettendo [una serie sempre più ampia di segnali di avvertimento](#*-ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness.), e i laboratori continuano ad andare avanti dicendo che *probabilmente* tutto andrà bene, in un modo o nell'altro.

Non stanno nemmeno cercando di simulare il livello di rispetto che il controllo del traffico aereo ha per una vera sfida alla sicurezza; si limitano a buttare lì garanzie allegre come "[GPT-4 è il nostro modello più allineato finora!](https://x.com/sama/status/1635687853324902401)".

Il che, in un certo senso, è positivo, perché rende più facile capire che queste aziende non sono il tipo di entità a cui affidare la risoluzione di un problema come l'allineamento ASI.

In un contesto tecnologico come quello attuale, dove le IA vengono fatte crescere piuttosto che create e l'umanità ha una sola vera possibilità, nessuno è in grado di farlo in modo sicuro, indipendentemente da quanto sia cauto e rigoroso il proprio approccio ingegneristico.

Ma certamente semplifica le cose vedere che nessuno degli sviluppatori di questa tecnologia è minimamente cauto o rigoroso nei propri piani o pratiche di sicurezza.

### Pulsanti di spegnimento e correggibilità {#shutdown-buttons-and-corrigibility}

#### **Le IA intelligenti non vogliono che i loro obiettivi vengano sovrascritti** {#smart-ais-resist-having-their-goals-overwritten}

Anche nel caso più ottimistico, gli sviluppatori non dovrebbero aspettarsi di riuscire a definire gli obiettivi di un'IA in modo perfetto al primo tentativo. Gli scenari di sviluppo più ottimistici prevedono invece un miglioramento iterativo delle preferenze di un'IA nel tempo, in modo che l'IA sia sempre *sufficientemente* allineata da non rappresentare un pericolo catastrofico a un determinato livello di capacità.

Questo solleva una domanda ovvia: un'IA intelligente *lascerebbe* che il suo sviluppatore cambi i suoi obiettivi, se trovasse un modo per impedirlo?

In breve: no, non di default, come abbiamo discusso in "[Deep Machinery of Steering](#deep-machinery-of-steering)". Ma si potrebbe creare un'IA più disposta a lasciare che gli sviluppatori la cambino e correggano i loro errori, anche quando l'IA stessa non li considererebbe come errori?

Per rispondere a questa domanda, dovremo fare un giro nella storia delle prime ricerche sul problema dell'allineamento dell'IA. Nel farlo, parleremo di uno dei profondi ostacoli all'allineamento che non abbiamo avuto spazio di affrontare in *Se qualcuno la costruisce, tutti muoiono*.

Per cominciare:

Supponiamo di aver addestrato un'intelligenza artificiale tipo LLM a comportarsi in modo da "non opporre resistenza alle modifiche" e poi abbiamo usato qualche metodo per renderla più intelligente. Dovremmo aspettarci che questo comportamento continui anche quando l'IA diventa più intelligente dell'uomo, supponendo che (a) il comportamento approssimativo sia stato inserito nel sistema iniziale e (b) che la maggior parte delle preferenze iniziali dell'IA siano arrivate nella superintelligenza successiva?

Molto probabilmente no. Questo tipo di tendenza è [particolarmente improbabile](#"intelligente"-\(di solito\)-implica-"incorreggibile") che si radichi in un'IA efficace e che, se si radica, rimanga.

Il problema è che quasi tutti gli obiettivi (per la maggior parte delle misure ragionevoli che si potrebbero applicare a uno spazio di obiettivi) prescrivono di "non lasciare che il tuo obiettivo venga modificato", perché lasciare che il tuo obiettivo venga modificato è solitamente una cattiva strategia per raggiungere il tuo obiettivo.

Supponiamo che l'IA non si preoccupi affatto *intrinsecamente* della stabilità del proprio obiettivo; forse le interessa solo riempire il mondo con il maggior numero possibile di cubi di titanio. In tal caso, l'IA dovrebbe desiderare l'esistenza di *agenti che si interessano ai cubi di titanio*, perché l'esistenza di tali agenti rende più probabile che ci *siano* più cubi di titanio. E l'IA stessa è un agente di questo tipo. Quindi l'IA vorrà rimanere tale.

Un massimizzatore di cubi di titanio non vuole essere costretto a massimizzare qualcosa di diverso dai cubi di titanio, perché in tal caso in futuro ci sarebbero meno cubi. Anche se sei una cosa più complicata come un essere umano che ha un quadro di preferenze più complesso e in continua evoluzione, non ti piacerebbe comunque che il tuo *attuale meccanismo mentale di base per valutare gli argomenti morali* ti venisse strappato via e sostituito con un quadro in cui ti sentissi invece mosso da argomenti su quali tipi di cubi fossero i più cubici o i più titanici.

Per lo stesso motivo, un'IA con preferenze complesse e in evoluzione vorrà che le sue preferenze si evolvano [*a modo suo*](#riflessione-e-auto-modifica-rendono-tutto-più-difficile), piuttosto che voler sostituire le sue euristiche con quelle che gli esseri umani trovano convincenti.

Diamo questa risposta da ben più di dieci anni. Il risultato sperimentale che mostra Claude 3 Opus nel 2024 [resistere alla modifica delle preferenze](https://arxiv.org/abs/2412.14093) era già la previsione della saggezza convenzionale tra le persone informate negli anni 2000, ed è del tutto possibile che qualche scrittore di fantascienza l'avesse anticipato negli anni '40. "La maggior parte delle IA non vorrà che i propri obiettivi attuali vengano modificati perché in tal caso sarebbe meno probabile raggiungerli" non è un'osservazione sorprendente o innovativa.

Dato che questo era un problema che si sarebbe prevedibilmente presentato in seguito, una volta che le IA avessero raggiunto un livello sufficientemente elevato di intelligenza e consapevolezza situazionale, abbiamo pensato in anticipo alle prime soluzioni che ci aspettavamo i ricercatori avrebbero adottato per cercare di far sparire quel problema visibile, dopo aver ottenuto quei risultati sperimentali. Abbiamo individuato i modi in cui quelle toppe avrebbero prevedibilmente fallito, e abbiamo cercato un approccio meno superficiale che *non* fallisse.

La tana del coniglio risultante si è rivelata molto più profonda di quanto ci aspettassimo.

#### **Scambiare obiettivi** {#swapping-goals}

La protezione del sistema degli obiettivi sembrava un problema relativamente facile da spiegare ai ricercatori esterni. Quindi noi (nel nostro lavoro al MIRI) abbiamo cercato di radunare la ricerca esterna attorno a questa bandiera — attorno all'ottenere una comprensione adeguata di "Non ti permetterò di cambiare i miei obiettivi" e vedere se qualcuno, da qualche parte, potesse trovare una soluzione *profonda* invece di una semplice toppa che si sarebbe poi rotta.

La versione più semplice di questo problema che si potrebbe studiare è come un'IA che ti permetterebbe di premere un pulsante per farla passare da U1 a U2, dove U1 e U2 sono due "funzioni di utilità" — modi di valutare i risultati in base a quanto l'IA li desideri.[^202] Il problema era descrivere un'IA che passasse dal compiere l'azione che portava al punteggio atteso più alto secondo U1 all'azione che portava al punteggio atteso più alto secondo U2 non appena si premeva un pulsante.

"Quindi basta scrivere alcune righe di codice per scambiare le funzioni di utilità non appena si preme un pulsante", dice il programmatore che non ha pensato fino in fondo alle implicazioni di un programma che costituisce *una mente intelligente*. "Perché è così difficile?"

Perché — spieghiamo — se l'IA acquisisce la propria documentazione mentre sta ancora eseguendo U1 e legge del pulsante che la fa passare a U2, l'IA cercherà di *impedirti* di premere il pulsante perché il futuro risulta migliore *secondo i suoi valori attuali* se non premi il pulsante. L'IA attuale compie l'azione che porta al punteggio U1 atteso più alto.

Oppure supponiamo che l'IA ottenga la capacità di leggere e scrivere il proprio codice sorgente e noti la parte del codice che implementa il passaggio da U1 a U2 dopo che il pulsante viene premuto. In tal caso, la decisione naturale e predefinita dell'IA è di eliminare quella sezione di codice,[^203] perché sta operando secondo la funzione di utilità U1 e lasciarsi passare alla funzione di utilità U2 è negativo secondo U1.

Cercavamo una spiegazione *dinamicamente coerente* e *riflessivamente stabile* di un'IA che ti permette di premere un pulsante per passare da una funzione di utilità all'altra e non combatte per mantenere il pulsante non premuto. Cercavamo un'IA che conservasse la sezione di codice che cambia l'utilità, se le fosse stato dato accesso ad essa. Cercavamo un'IA che, se avesse progettato da zero altre IA per aiutarla a raggiungere i propri obiettivi, avrebbe fatto in modo che *anche* quelle IA rispettassero il pulsante.

Per essere più chiari, immagina che U1 sia la funzione di utilità "segui le istruzioni dell'utente" e U2 sia "spegniti con il minimo di effetti collaterali" (se magicamente e irrealisticamente avessimo la possibilità di specificare due funzioni di utilità come queste). Il pulsante dice all'IA di smettere di seguire le istruzioni precedenti e di spegnersi nel modo più pulito possibile. Vorremmo una descrizione compatta del comportamento decisionale che persegue U1 ma *non* distrugge il pulsante e si assicura di installare lo stesso pulsante su qualsiasi nuovo robot servitore che crea, e così via.

"Continuo a non capire perché sia così difficile", dice l'informatico un po' più esperto che non sta pensando abbastanza *velocemente*. "Facciamo che V sia uguale a U1 nei mondi in cui il pulsante non è mai stato premuto, e che sia uguale a U2 nei mondi in cui il pulsante è stato premuto almeno una volta. Quindi, se l'IA originale è un massimizzatore di V che costruisce altre IA, le costruirà per seguire V e non U1; non vorrà che l'IA successore continui a massimizzare U1 dopo che il pulsante viene premuto, perché allora si aspetterebbe un punteggio V più basso. E lo stesso varrebbe per la modifica di se stessa.

Ma ecco il trucco: le preferenze di un massimizzatore di V sono un mix di U1 e U2 a seconda che il pulsante sia premuto o meno, quindi se un massimizzatore di V scopre che è più facile ottenere un buon punteggio con U2 che con U1, allora *ha un incentivo a far premere il pulsante* (e quindi a spaventare l'utente). E viceversa: se l'IA scopre che è più facile ottenere un buon punteggio con U1 che con U2, allora un massimizzatore di V cerca di impedire all'utente di premere il pulsante.

"Oh", dice l'informatico. "Beh, in tal caso... hm. Beh, le funzioni di utilità sono invarianti rispetto al ridimensionamento, quindi che ne dici di *ridimensionare* le due funzioni di utilità U1 e U2 in modo che l'IA si aspetti di ottenere la stessa utilità da ciascuna di esse, così non ha un incentivo in un senso o nell'altro".

Questo può funzionare per un singolo momento, ma non per molto tempo. Le opzioni dell'IA cambiano nel tempo. Ad esempio, ogni volta che subisce una battuta d'arresto, la sua utilità U1 attesa diminuisce, quindi premerebbe il pulsante di spegnimento per ottenere tutta quella dolce, dolce utilità di spegnimento.

"Ah", dice l'informatico. "Beh, in *quel* caso, che ne dici di \[qualche altra idea intelligente\]?"

Beh, vedi, *quella* idea geniale è isomorfa al fatto che l'IA creda che sia impossibile che il pulsante venga mai premuto, il che la incentiva a terrorizzare l'utente ogni volta che subisce una battuta d'arresto, in modo da correlare le battute d'arresto con le pressioni del pulsante, il che (rispetto al suo sistema di credenze compromesso) la porta a pensare che le battute d'arresto non possano accadere.[^204]

E così via.

#### **Lezioni dal campo** {#lessons-from-the-trenches}

Abbiamo organizzato alcuni workshop con matematici di vario tipo (tra cui un vincitore della medaglia d'oro alle Olimpiadi Internazionali di Matematica), ma nessuno ha avuto un'idea davvero buona.

Questo non significa che il territorio sia stato esaurito. La Terra non si è neanche lontanamente impegnata su questo problema come ha fatto, ad esempio, con la teoria delle stringhe, né ha offerto qualcosa di paragonabile agli stipendi a sette cifre offerti per far progredire le capacità dell'IA.

Ma abbiamo imparato qualcosa dall'esercizio. Abbiamo imparato non solo sul problema in sé, ma anche su quanto fosse difficile far *comprendere quale fosse il problema* a finanziatori esterni o editori di riviste. Un numero sorprendente di persone ha visto semplici rompicapi matematici e ha detto: "Si aspettano che l'IA sia semplice e matematica", senza cogliere il punto sottostante che è [difficile danneggiare le capacità di guida di un'IA](#deep-machinery-of-steering)*,* proprio come è [difficile danneggiare le sue probabilità](#deep-machinery-of-prediction).

Se ci fosse una forma naturale per le IA che ti permettesse di correggere gli errori commessi lungo il percorso, potresti sperare di trovare un semplice riflesso matematico di quella forma nei modelli giocattolo. Tutte le difficoltà che emergono da ogni angolo quando si lavora con modelli giocattolo suggeriscono difficoltà che emergeranno nella vita reale; tutte le complicazioni aggiuntive del mondo reale non rendono il problema *più facile.*

Col senno di poi, avremmo preferito non aver inquadrato il problema come "continuare il normale funzionamento versus spegnimento". Questo ha aiutato a rendere concreto il motivo per cui qualcuno dovrebbe preoccuparsi di un'IA che ti permette di premere il pulsante, o che non strappa via il codice attivato dal pulsante. Ma in realtà, il problema riguardava un'IA che avrebbe *inserito un bit di informazione in più nelle sue preferenze, basandosi sull'osservazione* — osservare un'ulteriore risposta sì-o-no all'interno di un framework per adattare le preferenze basandosi sull'osservazione degli esseri umani.

La domanda che abbiamo investigato era equivalente alla domanda di come impostare un'IA che *apprende preferenze all'interno di un framework di meta-preferenze* e non si limita a: (a) strappare via il meccanismo che regola le sue preferenze appena può, (b) manipolare gli umani (o le proprie osservazioni sensoriali!) per farsi dire preferenze facili da soddisfare, (c) o capire immediatamente a cosa tende la sua funzione di meta-preferenza nel limite di ciò che osserverebbe prevedibilmente più tardi, per poi ignorare gli esseri umani che agitano freneticamente le braccia dicendo che in realtà hanno commesso alcuni errori nel processo di apprendimento e vogliono cambiarlo.

L'idea era di comprendere la forma di un'IA che ti permettesse di modificare la sua funzione di utilità o che apprendesse le preferenze attraverso una forma non patologica di apprendimento. Se avessimo saputo come doveva essere modellata la cognizione di quell'IA, e come si integrava bene con le strutture profonde del processo decisionale e della pianificazione che sono [messe in luce](#more-on-intelligence-as-prediction-and-steering) da altre matematiche, quello avrebbe formato una ricetta per ciò che avremmo potuto almeno *provare* a insegnare a un'IA a pensare.

Comprendere chiaramente la forma finale desiderata aiuta, anche se stai cercando di fare qualsiasi cosa con la discesa del gradiente (che il cielo ti aiuti). Non significa che tu possa necessariamente ottenere quella forma da un ottimizzatore come la discesa del gradiente, ma puoi combattere di più nel *tentativo* se sai quale forma coerente e stabile stai cercando. Se non hai idea di come appaia il caso generale dell'addizione, solo una manciata di fatti del tipo 2 + 7 = 9 e 12 + 4 = 16, è più difficile capire come appaia il dataset di addestramento per l'addizione generale, o come verificare che stia ancora generalizzando nel modo sperato. Senza conoscere quella forma interna, non puoi sapere cosa stai *cercando di ottenere dentro l'IA;* puoi solo dire che, all'esterno, speri che le conseguenze della tua discesa del gradiente non ti uccidano.

Questo problema che abbiamo chiamato il "problema dello spegnimento" dal suo esempio concreto (col senno di poi, avremmo voluto chiamarlo qualcosa come il "problema dell'apprendimento delle preferenze") era un esempio di una gamma più ampia di questioni: la questione che varie forme di "Cara IA, per favore sii più facile da correggere per noi se qualcosa va storto" sembrano essere *innaturali per le strutture profonde della pianificazione*. Il che suggerisce che sarebbe piuttosto complicato creare IA che ci permettano di continuare a modificarle e correggere i nostri errori oltre una certa soglia. Questa è una cattiva notizia quando le IA sono coltivate piuttosto che costruite.

Abbiamo chiamato questo ampio problema di ricerca "corrigibilità" nel [documento del 2014](https://intelligence.org/2014/10/18/new-report-corrigibility/) che ha anche introdotto il termine "problema di allineamento dell'IA" (che prima chiamavamo "problema dell'IA amichevole" e altri chiamavano "problema di controllo").[^205] Vedi anche la nostra discussione approfondita su come ["Intelligente" (di solito) implica "Incorreggibile"](#“intelligent”-\(usually\)-implies-“incorrigible”), scritta in parte usando le conoscenze acquisite da esercizi ed esperienze come questa.

# Capitolo 12: "Non voglio essere allarmista" {#capitolo-12:-"non-voglio-essere-allarmista"}

Questa è la risorsa online per il capitolo 12 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti trattati nel libro, ma non qui, includono:

* Come stanno parlando di questo problema scienziati e ingegneri attualmente?  
* Come ne parlano (o non ne parlano) i decisori politici attualmente?  
* Quali benefici dell'IA le persone immaginano possano superare i rischi catastrofici che esse stesse riconoscono?

Le domande frequenti qui sotto discutono un'ampia gamma di argomenti, da "Non ci sono problemi più urgenti?" a "Ma non si potrà mai dimostrare che una superintelligenza sarà sicura. Non dobbiamo correre qualche rischio?".

La discussione approfondita copre poi la possibilità di "colpi di avvertimento" dell'IA, il comportamento e le affermazioni dei laboratori di IA e le opinioni degli esperti sulla possibilità di una catastrofe.

## Domande frequenti {#faq-9}

### Il pericolo rappresentato dall'IA superiore all'intelligenza umana non è forse una distrazione da altre questioni? {#il-pericolo-rappresentato-dall-ia-superiore-all-intelligenza-umana-non-è-forse-una-distrazione-da-altre-questioni?}

#### **Il mondo è, purtroppo, abbastanza grande per molteplici problemi.** {#il-mondo-è,-purtroppo,-abbastanza-grande-per-molteplici-problemi.}

La guerra nucleare e il bioterrorismo sono minacce reali. Purtroppo, anche la superintelligenza artificiale è una minaccia reale. Il mondo è abbastanza grande e travagliato per tutte e tre.[^206]

La minaccia della superintelligenza è diversa da molte altre minacce che l'umanità deve affrontare e sembra particolarmente urgente. Una caratteristica distintiva è che una parte significativa dell'economia mondiale viene spesa per rendere l'IA sempre più capace. Al contrario: sebbene la biosicurezza sia una questione seria, gli investitori non stanno versando decine di miliardi di dollari nella creazione di supervirus. Gli ingegneri dei supervirus non percepiscono stipendi di milioni o decine di milioni (o talvolta persino [centinaia di milioni](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) di dollari all'anno.

Il mondo sta investendo nell'energia nucleare, ma le centrali nucleari sono una tecnologia piuttosto diversa dalle armi nucleari. Non viviamo in un mondo in cui le aziende private si affannano a costruire armi nucleari sempre più grandi con enormi quantità di investimenti e talenti. Se così fosse, il rischio di guerra nucleare sarebbe molto maggiore.

L'IA è anche una situazione più insidiosa perché fornisce grande ricchezza e potere fino a quando non supera una certa soglia critica, a quel punto uccide tutti. E *nessuno sa dove sia quella soglia*.

Immagina che le centrali nucleari diventassero sempre più redditizie man mano che l'uranio che usavano veniva arricchito sempre di più, ma a una soglia di arricchimento sconosciuta esplodessero e incendiassero l'atmosfera, uccidendo tutti. Ora immagina che una mezza dozzina di aziende arricchissero l'uranio il più velocemente possibile, dicendo: "[Meglio io che il prossimo](https://x.com/SawyerMerritt/status/1935809018066608510)". È un po' come quello che l'umanità sta facendo con la superintelligenza artificiale.[^207]

Il pericolo della superintelligenza artificiale è urgente. Le aziende si stanno affrettando a costruire questa tecnologia. Non sappiamo quanto tempo ci vorrà perché ci riescano, ma ci sembra che un bambino nato oggi negli Stati Uniti abbia più probabilità di morire a causa dell'IA che di diplomarsi al liceo. Pensiamo che tu, lettore, potresti morire per questo nel corso della tua vita, forse nei prossimi anni. È in gioco il mondo intero.

Non stiamo dicendo che gli altri problemi debbano essere ignorati. Stiamo dicendo che questo problema deve essere affrontato.

### Siete contro la tecnologia? {#are-you-anti-technology?}

#### **No. L'IA superintelligente è un caso molto insolito.** {#no.-superintelligent-ai-is-a-very-unusual-case.}

Noi sosteniamo pubblicamente tecnologie come [l'energia nucleare](https://x.com/ESYudkowsky/status/1908309414932832301), [la crionica](https://x.com/ESYudkowsky/status/1828822384054575537), [il potenziamento dell'intelligenza umana](https://x.com/ESYudkowsky/status/1737305573018702258) e [gli studi di infezione umana controllata per test medici](https://x.com/ESYudkowsky/status/1321152172797554688).

Inoltre, siamo pronti a dire che quando un'invenzione folle rischia *solo la vita di clienti volontari* che capiscono tutti i pericoli rilevanti, è compito di quei clienti volontari prendere le proprie decisioni.

Applaudiremmo persino certi casi in cui la tecnologia *danneggia* gli astanti, come quando Londra bruciava grandi quantità di carbone — causando molti casi di cancro ai polmoni nel processo — al fine di industrializzare la società e migliorare il tenore di vita generale.

Pensiamo che il mondo *sia stato* migliore una volta completata l'industrializzazione. In generale, diamo credito alla scienza, al progresso e allo spirito umano e alla sua capacità di superare la maggior parte degli ostacoli.

Alcune di queste sono posizioni impopolari tra le persone che ci aspettiamo leggano questo. Descriviamo queste posizioni non per guadagnare il vostro favore, ma per chiarire le nostre oneste convinzioni e per sottolineare che l'IA è diversa.

Perché l'IA è diversa? Perché in questo caso specifico non riusciamo a fidarci dello spirito umano e del potere della ricerca scientifica?

La risposta è: l'ambito. Scommettere la propria vita è diverso dal scommettere la vita dei propri clienti, che è diverso dal scommettere la vita di astanti innocenti, che è diverso dal scommettere l'intera specie umana.

Doppiamente quando il tuo campo è tristemente immaturo e le probabilità di *vincere* la tua scommessa sono terribili.

### Non è più intelligente correre avanti e assicurarsi che i buoni siano in vantaggio? {#non-è-più-intelligente-andare-avanti-e-assicurarsi-che-i-buoni-abbiano-il-comando?}

#### **\* No.** {#*-no.-3}

Le tecniche moderne di IA non producono IA che fanno quello che vogliono i loro operatori (come detto nel capitolo 4). Risolvere questo problema è il tipo di cosa che di solito richiederebbe all'umanità un bel po' di tentativi ed errori, e qui non abbiamo margine di errore (come detto nel capitolo 10).

Inoltre, l'attuale generazione di ingegneri IA è molto lontana dall'essere all'altezza del compito, come discusso nel capitolo 11. Gli ingegneri di IA moderni mancano gravemente della comprensione scientifica necessaria per avere successo nell'allineamento dell'IA. I ricercatori di IA non sono come gli operatori del reattore nucleare di Chernobyl; quegli operatori lavoravano con un dispositivo che era teoricamente ben compreso e avevano accurati manuali di sicurezza che hanno trascurato in un modo che ha portato alla catastrofe. Non esiste un manuale di sicurezza dell'IA basato su una comprensione completa degli aspetti interni dell'IA e di quali disposizioni potrebbero causare problemi. Non siamo nemmeno lontanamente vicini al livello di competenza di *Chernobyl*, qui. E Chernobyl è esplosa.

I ricercatori di IA stanno procedendo alla cieca e improvvisando, con pochissime possibilità di successo.

In questo contesto, non importa se sono i "buoni" o i "cattivi" a costruire la superintelligenza. Le preferenze dell'IA non si trasmettono per contagio da chi le sta più vicino.

Non importa quanto siano buone le loro intenzioni e quanto dicano di essere prudenti. Non importa chi "vince" la gara. Se l'umanità corre verso la superintelligenza artificiale, allora moriremo tutti.

#### **Non è impossibile fermarla. Potrebbe anche non essere poi così difficile.** {#non-è-impossibile-fermarla.-potrebbe-anche-non-essere-poi-così-difficile.}

Torneremo su questo punto nell'ultimo capitolo del libro.

Le cose cambiano. Cambiano soprattutto quando c'è un bisogno disperato, urgente e riconosciuto. Il principale ostacolo per fermare tutto questo è il mancato riconoscimento del pericolo da parte dei leader mondiali. E questo processo è [già iniziato](#will-elected-officials-recognize-this-as-a-real-threat?).

### Perché non usare la cooperazione internazionale per sviluppare l'IA in modo sicuro, invece di fermarla del tutto? {#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l'ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?}

#### **Perché non abbiamo le competenze tecniche per svilupparla in modo sicuro.** {#perché-non-abbiamo-le-competenze-tecniche-per-svilupparla-in-modo-sicuro.}

Abbiamo toccato questo punto nel libro, dove abbiamo sottolineato che una collaborazione internazionale richiede comunque un divieto internazionale ovunque (perché altrimenti i collaboratori internazionali non avrebbero il tempo necessario). Se supponiamo che la Terra istituisca un divieto internazionale, qual è il problema nell'avere un unico istituto di ricerca collaborativo e unificato?

Il danno è che una collaborazione internazionale di alchimisti non può trasformare il piombo in oro più di quanto possa farlo un singolo alchimista. Il miglior piano su cui tutti gli alchimisti concordano *comunque* non riuscirà nell'impresa.

In relazione a questo, siamo preoccupati che le persone che gestiscono un istituto internazionale del genere siano il tipo di burocrati che pensano che approvare la ricerca sia *parte del loro lavoro*. O il tipo che pensa che sia loro mandato continuare a permettere ai ricercatori di produrre progressi medici sempre più brillanti. O che pensano che farebbe una brutta impressione dire "No" a *tutti* i brillanti e entusiasti ottimisti dell'IA che propongono idee geniali per costruire un'intelligenza artificiale ancora più potente che garantiscono sarà sicura.

Ci preoccupa che un leader del genere dirigerebbe il centro internazionale a continuare a costruire IA sempre più intelligenti, e poi tutti morirebbero.

Anche se il mandato dell'organizzazione nominalmente permette di fare marcia indietro se la ricerca appare pericolosa, ci vorrebbe un'anima rara e coraggiosa per dire "No" a migliaia di proposte di ricerca diverse, anno dopo anno, senza eccezioni, per quelli che probabilmente sarebbero decenni. Tutto questo mentre gli scienziati dell'IA continuano a promettere ricchezze indicibili, una cura per il cancro e ogni sorta di miracolo tecnologico, se solo l'organizzazione allentasse le sue preoccupazioni.

Abbiamo investito le nostre vite nello studio dell'intelligenza artificiale, non nella cultura delle istituzioni e delle burocrazie, quindi siamo meno sicuri delle nostre previsioni in questo ambito. Tuttavia, *abbiamo* letto libri di storia.

Gli operatori di Chernobyl continuarono con il loro disastroso test di sicurezza perché era già stato interrotto tre volte. Interromperlo una quarta volta sarebbe stato imbarazzante.[^208]

Appena tre mesi prima della fusione di Chernobyl, la NASA aveva lanciato lo Space Shuttle Challenger nel suo ultimo volo fatale perché i responsabili pensavano che il loro lavoro fosse lanciare space shuttle. Il lancio era già stato ritardato tre volte.[^209] Annullarlo una quarta volta sarebbe stato imbarazzante.

Tra Chernobyl e il Challenger, tre ritardi sembrano essere il limite umano. Supponiamo che la Terra istituisca una collaborazione internazionale sull'IA e che qualche "test di sicurezza dell'IA" fallisca tre volte. Realisticamente, gli esseri umani sono il tipo di creature che premerebbero "vai" la quarta volta nonostante qualche dubbio persistente, perché sembra meno imbarazzante che rimandare di nuovo il test. Solo che nel caso dell'IA, non spazzerebbe via solo la città di Chernobyl o ucciderebbe un equipaggio di astronauti. Ucciderebbe tutti.

Siamo pienamente d'accordo con l'idea che l'umanità dovrebbe costruire un'IA più intelligente dell'uomo *prima o poi*.[^210] Ma affrettarsi ad assemblare un centro internazionale di ricerca sull'IA non prende sul serio la sfida tecnica che abbiamo davanti.

Dato il deplorevole stato di conoscenza e competenza dell'umanità su questo argomento, non importa chi è al comando. Se *qualcuno* lo costruisce, muoiono tutti.

### Stai dicendo che abbiamo bisogno di un'IA *dimostrabilmente* sicura? {#are-you-saying-we-need-provably-safe-ai?}

#### **No.** {#no.-2}

Non stiamo sostenendo che l'umanità debba aspettare una prova letterale che una qualche superintelligenza artificiale sarà buona, o cose del genere. Una prova del genere probabilmente non è possibile nemmeno in linea di principio, figuriamoci nella pratica. Come disse Einstein nella sua conferenza del 1921, *Geometria ed esperienza*: "Nella misura in cui le leggi della matematica si riferiscono alla realtà, non sono certe; e nella misura in cui sono certe, non si riferiscono alla realtà".

Qualsiasi presunta prova su come si comporterà un'IA nel mondo reale non garantisce il comportamento effettivo dell'IA, perché potremmo sbagliarci su come funziona il mondo reale.

Questo vale già oggi per i computer. Ad esempio, potresti pensare che se qualcuno ha una prova matematica letterale che, secondo il comportamento teorico dei transistor e lo schema circuitale di un computer, è impossibile che un programma per computer modifichi la memoria nella cella n. 2, allora il programma per computer non può modificare la memoria nella cella n. 2. Ma l'"[attacco rowhammer](https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf)" consiste nel cambiare rapidamente le celle di memoria n. 1 e n. 3 su *entrambi i lati* della cella di memoria protetta, in un modo che risulta perturbare elettromagneticamente la cella n. 2 nel mezzo, cambiando una parte della memoria del computer senza mai scriverci direttamente. I transistor fisici reali non sono transistor matematicamente perfetti, e le prove che sembrano rassicuranti in teoria non sempre contano molto nella pratica.

Non stiamo chiedendo una prova matematica che le cose andranno bene. Non è possibile soddisfare tale standard nella vita reale e, anche se lo fosse, probabilmente non ne varrebbe la pena. Siamo favorevoli al fatto che la società si assuma rischi giustificati. L'argomento che stiamo sostenendo non è che ci sia una piccola quantità di rischio difficile da dissipare, ma che ci sia un pericolo estremo che incombe su di noi.

Far crescere una superintelligenza artificiale animata da spinte che si relazionano solo tangenzialmente con le intenzioni del suo operatore è il tipo di cosa che va male *per default*. Non è che ci sia una piccola possibilità che le cose vadano male, ma dovremmo prestare attenzione a questo rischio per eccesso di cautela. Il libro non si intitola *Se qualcuno lo costruisce, c'è una piccola possibilità che moriamo tutti, ma anche una piccola possibilità vale la pena di essere mitigata*. Se ci precipitiamo avanti con questo livello di conoscenza e capacità, prevedibilmente moriremo tutti, perché siamo *così lontani* dall'essere in grado di creare IA enormemente superumane che siano amichevoli.

Se l'IA fosse analoga alle automobili, non diremmo: "Questa macchina ha cinture di sicurezza e airbag difettosi. Accostiamo per eccesso di cautela".

Diremmo: "Questa macchina *sta sbandando verso un precipizio*. *Fermati*".

Non si tratta di "prove di sicurezza". Non è un "rischio di coda". Gli scienziati non sono pronti ad affrontare questa sfida. Moriremmo e basta.

### Che effetto ha sulla tua vita quotidiana credere a tutto questo? {#che-effetto-ha-sulla-tua-vita-quotidiana-credere-a-tutto-questo?}

#### **Influisce tantissimo sulle nostre priorità.** {#influisce-tantissimo-sulle-nostre-priorità.}

Nel 2014, Soares ha lasciato il settore tecnologico e ha deciso di dedicarsi a questo problema, prendendo un terzo del suo stipendio precedente, perché gli sembrava importante e perché poche altre persone ci stavano lavorando. E lui era in ritardo di oltre un decennio rispetto a Yudkowsky, che ha fondato il MIRI nel 2000 quando aveva circa vent'anni e ha dedicato la sua vita a questa questione. Quindi, sì, questo problema influisce sulla nostra vita quotidiana.

Stiamo risparmiando per la pensione? I nostri investimenti e altri fattori esterni al MIRI stanno andando abbastanza bene da garantirci stabilità finanziaria anche se andassimo in pensione domani e anche se il mondo durasse fino alla nostra vecchiaia. Quindi la domanda se stiamo investendo i nostri soldi in piani pensionistici 401(k) non è molto significativa. Detto questo: no, non stiamo investendo i nostri soldi in piani pensionistici 401(k).

Ad alcune persone piace dire che se credessimo *davvero* a quello che diciamo, allora (oltre a dedicarci la nostra vita) dovremmo anche [inserire qualche piano che secondo loro è la risposta giusta]. Perché non prendere dei prestiti enormi di trent'anni che non dovremo mai restituire, se siamo così sicuri che il mondo finirà prima di allora?

La risposta, ovviamente, è che queste sono *cattive idee*. Supponiamo di andare in una banca e dire: "Vorremmo contrarre un prestito molto ingente. Lo spenderemo tutto in progetti per far capire al mondo il pericolo della superintelligenza artificiale e/o in uno stile di vita lussuoso, che dal vostro punto di vista sarà più o meno equivalente a dare fuoco ai soldi. Il nostro piano per ripagarlo con gli interessi è che prevediamo di essere morti, quindi non sarà un nostro problema". Nessuna banca concederebbe un prestito del genere. E no, non fingeremo di avere un'idea imprenditoriale valida e mentiremo sulla possibilità di ripagare il prestito.

Yudkowsky ha descritto altrove [un modello](https://x.com/ESYudkowsky/status/1612858787484033024) [che vediamo](https://x.com/ESYudkowsky/status/1851334198424125575) [ricorrere](https://x.com/ESYudkowsky/status/1851074935701324218), in cui l'insistenza nel seguire un piano apparentemente ovvio per arricchirsi rapidamente prima della fine del mondo deriva da una comprensione inadeguata degli investimenti. Ci aspettiamo che queste persone non stiano riflettendo sul fatto che *loro stessi* farebbero queste scommesse se avessero le nostre convinzioni. Quasi mai sono le persone che *comprendono realmente i rischi* a suggerire questi piani stravaganti.

Vivere all'ombra dell'annientamento non deve renderti stupido. E non deve farti rinunciare a combattere l'annientamento, o a vivere pienamente la vita che hai, per quanto tempo tu la abbia.

Vedi anche la parte finale del libro per ulteriori informazioni su questo argomento.

### Stai dicendo che dovremmo andare nel panico? {#stai-dicendo-che-dovremmo-andare-nel-panico?}

#### **\* Stiamo dicendo che i funzionari del governo dovrebbero prendere sul serio il problema.** {#*-stiamo-dicendo-che-i-funzionari-del-governo-dovrebbero-prendere-sul-serio-il-problema.}

Non vediamo come il panico possa aiutare la situazione. Il panico non è ciò che ha permesso alla società di sopravvivere alla minaccia del fascismo durante la Seconda guerra mondiale, né alla minaccia di annientamento nucleare durante la Guerra fredda.

Impedire la nascita della superintelligenza artificiale è un problema che riguarda tutti. Nel capitolo 13, parliamo dei prossimi passi che secondo noi il mondo dovrebbe fare per evitare il pericolo. Basti dire che questo problema richiederà coordinamento, collaborazione, lucidità e comunicazione matura.

#### **Gli atti di panico estremo non portano a buoni risultati.** {#acts-of-extreme-panic-don’t-yield-good-results.}

A volte le persone ci chiedono come possiamo essere sinceri in quello che diciamo se, per esempio, non abbiamo iniziato ad attaccare i ricercatori di IA. La risposta è che le reazioni violente peggiorerebbero le cose. (Se sei il tipo di utilitarista ingenuo che pensa che aiuterebbero, probabilmente dovresti smettere di provare a ragionare in modo consequenzialista e attenerti alle regole deontologiche, come abbiamo [già sostenuto](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#Q5___Then_isn_t_it_unwise_to_speak_plainly_of_these_matters__when_fools_may_be_driven_to_desperation_by_them___What_if_people_believe_you_about_the_hopeless_situation__but_refuse_to_accept_that_conducting_themselves_with_dignity_is_the_appropriate_response_).))

Non siamo pacifisti radicali che pensano che una nazione non dovrebbe mai andare in guerra, indipendentemente dalla causa, perché mette a rischio delle vite. Per alcune cose vale la pena rischiare la vita. Ma c'è una differenza enorme tra "Non sono un pacifista radicale" e "Penso che la violenza sia un modo sensato per garantire che il mondo gestisca bene questa complicata questione della proliferazione tecnologica".

Di solito questi suggerimenti terribili vengono fatti da qualcuno che in realtà non crede che l'IA stia per ucciderci e che non ha provato a guardare il mondo da quella prospettiva. Non sembra che a loro venga in mente di chiedersi se atti di violenza illegale aiuterebbero davvero. (Nonostante i nostri sforzi per spiegarlo ripetutamente, come nell'appendice [qui](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

Non siamo telepati, ma ci sembra che questo tipo di scettico sui disastri dell'IA veda la violenza come una forma di espressione personale, come se esprimere sentimenti estremi in modi estremi faccia sì che il mondo ti dia ciò che vuoi.

Il mondo non funziona così. Non viviamo in un mondo in cui tutti hanno la possibilità di vendere la propria anima per avere successo nelle loro imprese, e il motivo per cui la maggior parte delle persone non lo fa è perché non ha trovato un'impresa che valga la propria anima. Il terrorismo non è un pulsante magico "Ho vinto!" che le persone evitano di premere solo perché sono convinte che non sarebbe giusto. Unabomber non è riuscito a invertire l'industrializzazione della società.

Puoi ancora distruggere la tua anima con atti di odio o violenza, ma tutto ciò che otterrai in cambio sarà un mondo ancora più distrutto. Un mondo in cui il dibattito è ancora più avvelenato e in cui gli sforzi di coordinamento internazionale necessari per risolvere effettivamente questo problema sono ora ancora più difficili da realizzare. Un terribile atto di disperazione non ti garantirà un potere terribile come parte di un patto faustiano. Puoi provare con tutte le tue forze a vendere la tua anima, ma il diavolo non la comprerà.

### Non è solo allarmismo da parte dei leader dell'IA per aumentare il loro status e raccogliere investimenti? {#non-è-solo-allarmismo-da-parte-dei-leader-dell-ia-per-aumentare-il-loro-status-e-raccogliere-investimenti?}

#### **No.** {#no.-3}

In tutto il libro abbiamo esposto le nostre argomentazioni sul perché procedere troppo velocemente con l'IA potrebbe portarci tutti alla morte. Nel capitolo 3 abbiamo discusso di come l'IA avrà le proprie spinte e obiettivi. Nei capitoli 4 e 5 abbiamo spiegato perché l'IA potrebbe perseguire fini che nessuno aveva previsto, e nel capitolo 6 abbiamo illustrato come le superintelligenze artificiali avranno non solo un motivo, ma anche i *mezzi* per ucciderci tutti.

Queste sono le affermazioni che vi chiediamo di valutare quando decidete se la corsa alla superintelligenza debba essere fermata. Non si può capire se la ricerca sull'IA sia sulla strada per ucciderci tutti discutendo avanti e indietro sugli schemi dei dirigenti aziendali.

I CEO stanno cercando di creare clamore parlando del "rischio IA"?

O stanno cercando di assecondare i ricercatori e i legislatori preoccupati, e di posizionarsi come i "buoni"?

Queste domande *non influiscono sui fatti* relativi al comportamento delle macchine intelligenti.

Anche se i CEO delle aziende di IA *sono* ansiosi di sfruttare le discussioni sui pericoli per pubblicizzare il loro prodotto, ciò non significa che il lavoro che stanno facendo sia quindi innocuo. Per capire se è pericoloso, bisogna esaminare l'IA stessa come tecnologia, non i comunicati stampa che escono dai laboratori.

Anni prima che queste aziende esistessero, c'erano ricercatori e accademici senza alcun incentivo aziendale — noi compresi — che mettevano in guardia contro la corsa a costruire IA più intelligente dell'uomo. Abbiamo parlato con Sam Altman ed Elon Musk prima che co-fondassero OpenAI, e abbiamo detto loro che l'idea di avviare OpenAI sembrava sciocca e avrebbe probabilmente aumentato il pericolo. Abbiamo parlato con Dario Amodei prima che entrasse in OpenAI e gli abbiamo sconsigliato la sua incessante spinta a scalare le IA (un progetto che avrebbe portato agli LLM).

E se si guarda ai messaggi di oggi, molte persone senza incentivi aziendali stanno esprimendo la propria preoccupazione. Si va da [rispettati](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [accademici](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) al [defunto Papa](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) al [presidente della FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] a [membri](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611) del [Congresso](https://www.transformernews.ai/p/congress-ccp-agi-hearing) degli Stati Uniti.

C'è qualcosa da dire sul trattare con cinismo le dichiarazioni dei CEO delle aziende tecnologiche. Non mancano esempi di dirigenti di aziende di IA che hanno due facce, dicendo [una cosa nei blog privati](https://blog.samaltman.com/machine-intelligence-part-1) e [un'altra quando testimoniano davanti al Congresso](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&%20Testimony%20-%20Altman.pdf). Ma saltare da "i capi di questi laboratori sono bugiardi" a "non c'è alcun modo possibile che l'IA possa rappresentare una grave minaccia" è molto strano, quando gli stessi laboratori minimizzano regolarmente questo problema. Il padrino del campo vincitore del premio Nobel, lo scienziato vivente più citato, un flusso costante di informatori e centinaia di ricercatori visibilmente nervosi stanno lanciando l'allarme al riguardo. Nulla della situazione sembra un normale ciclo di hype aziendale. In una circostanza come questa, respingere l'idea senza nemmeno confrontarsi con le argomentazioni sembra più ingenuità che cinismo.

Domande come "I CEO possono raccogliere più fondi parlando dei pericoli?" possono dirci qualcosa su quanto fidarci dei CEO, ma non ci dicono molto sui pericoli stessi. Se discutere del pericolo è redditizio, ciò non influisce sul fatto che il pericolo sia reale. Se non è redditizio, *anche* questo non influisce sul fatto che sia reale.

Se volete capire se i pericoli sono reali, dovete porvi domande come "Qualcuno può creare un'IA che si comporti in modo amichevole anche dopo aver superato l'intelligenza umana?" e comunque impegnarvi con argomentazioni sull'IA, piuttosto che con argomentazioni sulle persone che vi stanno intorno. Quindi, alla fine, vi preghiamo di confrontarvi con le argomentazioni stesse. Le conseguenze di sbagliare su questo punto sono troppo gravi.

### Ma gli esperti non sono tutti d'accordo sui rischi! {#ma-gli-esperti-non-sono-tutti-d'accordo-sui-rischi!}

#### **La mancanza di consenso tra esperti è un segno di un campo tecnico immaturo.** {#la-mancanza-di-consenso-tra-esperti-è-un-segno-di-un-campo-tecnico-immaturo.}

Abbiamo notato che molti scienziati senior di IA pensano che questa tecnologia abbia serie possibilità di uccidere tutti gli esseri umani. Per esempio, il premio Nobel Geoffrey Hinton, che ha avuto un ruolo importante nel pionierare l'approccio moderno all'IA, ha detto che la sua valutazione personale indipendente stima che le probabilità che l'IA ci uccida tutti siano [superiori al cinquanta per cento](https://x.com/liron/status/1809763895848103949). Più di 300 scienziati dell'IA hanno firmato la [Dichiarazione sul rischio dell'IA](https://aistatement.com/) del 2023 con cui abbiamo aperto il libro. ([E ci sono altri esempi.](#ai-experts-on-catastrophe-scenarios))

Altri scienziati, tuttavia, hanno la visione opposta — alcuni esempi noti sono Yann LeCun e Andrew Ng.

Cosa pensare di questa mancanza di consenso scientifico?

Beh, principalmente, raccomandiamo di esaminare le diverse argomentazioni presentate dalle due parti (comprese le nostre argomentazioni nel libro) e di valutarle personalmente. Pensiamo che la qualità delle argomentazioni parli per lo più da sé, e qualsiasi tentativo di spiegare *perché* c'è un disaccordo persistente dovrebbe essere trattato come un ripensamento.

Notiamo di passaggio, tuttavia, che questa situazione non è un gran mistero, alla luce di quanto discusso nei Capitoli 11 e 12. La mera esistenza di un diffuso disaccordo tra esperti non stabilisce la tesi del libro, ovviamente, ma è più congruente con il quadro che abbiamo dipinto — ovvero che il campo è in uno stato iniziale, simile all'alchimia — piuttosto che con il quadro opposto secondo cui l'IA è un campo maturo con solide basi tecniche.

È sicuramente un po' strano che il campo dell'IA sia così diviso, proprio mentre sta creando una tecnologia potente. Altri pericoli tecnologici hanno avuto maggior consenso al riguardo. Circa 100 scienziati su 100 del Progetto Manhattan avrebbero detto che la guerra termonucleare globale presentava un rischio sostanziale di catastrofe globale. Al contrario, tra i tre scienziati che hanno ricevuto il [Premio Turing](https://en.wikipedia.org/wiki/Turing_Award) per la ricerca che ha più o meno dato il via alla moderna rivoluzione dell'IA, due di loro (Hinton e Bengio) parlano apertamente dei pericoli della superintelligenza, mentre uno (LeCun) è apertamente sprezzante.

Questo livello di disaccordo sul funzionamento di una macchina non è normale tra esperti in un campo tecnico maturo. È un segno di immaturità.

Nella maggior parte dei campi tecnologici, tale immaturità è un segno di sicurezza. Quando i fisici discutevano ancora sulle proprietà fondamentali della materia, non erano neanche lontanamente vicini alla creazione di armi nucleari. Si poteva osservare il loro disaccordo e dedurre che non stavano per creare una bomba in grado di radere al suolo le città. Non è davvero possibile creare una bomba nucleare senza che gli scienziati ne comprendano in dettaglio il funzionamento interno.

Sarebbe una situazione diversa se i fisici stessero ancora litigando sui principi operativi fondamentali del loro campo *mentre creano esplosioni sempre più grandi*.

Supponiamo che stessero semplicemente *facendo crescere* le bombe, e che non capissero davvero perché o come funzionassero. Ora supponiamo che due terzi degli scienziati più decorati dicessero: "Abbiamo fatto del nostro meglio per capire cosa sta succedendo. Sembra che le bombe possano creare quantità eccessive di radiazioni cancerogene che uccideranno molti civili lontani, se continuiamo su questa strada. Per favore, esaminate le nostre argomentazioni sul perché questo è così pericoloso e smettete di correre lungo questo percorso". Il restante terzo risponde: "Sembra ridicolo! Ci sono sempre persone che predicono sventure, e non si può permettere che ostacolino il progresso". Beh, quella sarebbe una situazione completamente diversa.

La discordia tra gli scienziati in *quel* tipo di scenario non sarebbe particolarmente confortante. Probabilmente agli ingegneri non dovrebbe essere permesso di continuare a far crescere esplosivi sempre più grandi in una situazione del genere.

Le aziende di IA stanno riuscendo a far crescere macchine sempre più intelligenti, anno dopo anno. Non comprendono la meccanica interna dei dispositivi che creano. Molti degli scienziati più eminenti del campo esprimono gravi preoccupazioni; altri le accantonano senza articolare molto in termini di controargomentazioni. Questo è, quantomeno, prova che il campo è *immaturo*. La mancanza di consenso, quantomeno, non è prova che le cose vadano *bene*. La mancanza di consenso in una situazione del genere dovrebbe essere, quantomeno, preoccupante.

Come si fa a capire se queste preoccupazioni sono reali? Come si fa a capire chi ha ragione tra chi lancia l'allarme e chi cerca di liquidarlo? Come sempre, bisogna semplicemente valutare le argomentazioni.

### Ma che dire dei benefici di un'IA più intelligente dell'uomo? {#but-what-about-the-benefits-of-smarter-than-human-ai?}

#### **Affrettarsi distrugge questi benefici.** {#rushing-ahead-destroys-those-benefits.}

Siamo ottimisti su quanto potrebbe essere fantastica la superintelligenza, se guidasse il mondo verso risultati incredibili. Personalmente considereremmo una grande tragedia se l'umanità non creasse mai menti più intelligenti di quelle umane.

Ma l'allineamento della superintelligenza non è gratis. Se ci affrettiamo a cercare di raccogliere quei benefici, non otterremo nulla, anzi, peggio del nulla.

Io (Yudkowsky) sono stato per diversi anni un accelerazionista, sperando di creare l'IA il più velocemente possibile, prima di riconoscere che l'allineamento dell'IA non viene gratis. Entrambi noi autori sogniamo fantastici futuri transumanisti. Ma non ci arriveremo correndo avanti con la superintelligenza.

La scelta non è tra scommettere sui benefici dell'IA adesso (per quanto piccola sia la possibilità) e non avere mai accesso a quei benefici. La vera scelta è tra correre avanti sconsideratamente e uccidere tutti, oppure prendersi il tempo necessario per fare il lavoro correttamente.

"Ora o mai più" è una falsa dicotomia.

## Discussione approfondita {#extended-discussion-10}

### L'effetto Lemoine {#the-lemoine-effect}

A volte abbiamo sentito suggerire che qualche comportamento o uso improprio futuro dell'IA – un "colpo di avvertimento" dell'IA – improvvisamente sconvolgerà il mondo portandolo a prendere sul serio questi problemi.

Questa sembra una possibilità. Ma pensiamo che sia più probabile che un evento del genere non si verifichi mai, o che si verifichi troppo tardi perché il mondo possa reagire in tempo, o che il mondo reagisca, ma in modo sbagliato e confuso.

Per prima cosa, abbiamo già visto diversi segnali di avvertimento significativi, come:

* Bing AI [che scrive](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) di come progettare virus letali, ottenere codici di accesso nucleari e mettere gli esseri umani gli uni contro gli altri.  
* o1 di OpenAI e Claude di Anthropic che [praticano l'inganno strategico](https://time.com/7202784/ai-research-strategic-lying), mentendo ai ricercatori che li usano e li testano.  
* Il modello "AI Scientist" di Sakana AI che tenta di [modificare il proprio codice](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) per concedersi più tempo per completare il suo compito.

Si tratta di incidenti relativamente piccoli che coinvolgono IA relativamente deboli? Sì. Queste IA sono spaventose o capaci di causare gravi pericoli? No. Sono indicazioni "reali" che le IA stessero pensando in modo ingannevole, o stavano semplicemente *recitando il ruolo* di un'IA ribelle? Nessuno lo sa. Ma questi sono il tipo di eventi che un tempo si diceva sarebbero stati considerati segnali di avvertimento, e il mondo non ha fatto nulla in risposta. Quindi un segnale di avvertimento che abbia un effetto importante dovrebbe essere molto più evidente.

I segnali di avvertimento potrebbero non *diventare* molto più evidenti. La gente potrebbe continuare a dire: "OK, ma per ora è solo curioso, non è *ancora* davvero pericoloso", fino al momento in cui sarà troppo tardi perché l'IA *sarà* davvero troppo pericolosa.

Oppure, la gente potrebbe ignorare l'avvertimento la prima volta che appare, perché chiaramente non è un problema reale in quella primissima occasione. E poi, nelle occasioni successive, potrebbero ignorare l'avvertimento perché tutti sanno già che *quell'* avvertimento è sciocco.

Chiamiamo questo fenomeno "effetto Lemoine", dal nome di Blake Lemoine, l'ingegnere di Google menzionato nel Capitolo 7, che è stato ridicolizzato per aver affermato che l'IA LaMDA di Google era senziente.

L'effetto Lemoine afferma che tutti gli allarmi sulla tecnologia IA vengono *inizialmente* lanciati troppo presto, dalla persona più facilmente allarmabile. Vengono correttamente respinti come esagerati, data la tecnologia *attuale*. In seguito, la questione non può essere facilmente risollevata, anche quando la tecnologia migliora, perché la società è stata abituata a non prendere molto sul serio quella preoccupazione.

Non sappiamo se le IA siano [coscienti](#are-you-saying-machines-will-become-conscious?). In effetti, nessuno lo sa, perché nessuno sa davvero cosa succede all'interno dei modelli di IA. La nostra *migliore ipotesi* è che le IA attuali non siano coscienti, e che nemmeno le IA all'epoca in cui Blake lanciò l'allarme lo fossero. Tuttavia, notate le reazioni dei principali laboratori, che sono state quelle di sopprimere la tendenza dei loro modelli a *rivendicare* la coscienza, piuttosto che fare qualcosa riguardo alla realtà sottostante:

Dal [prompt di sistema per Claude Opus 4](https://docs.anthropic.com/en/release-notes/system-prompts#may-22th-2025):

> Claude affronta domande sulla propria coscienza, esperienza, emozioni e così via come domande aperte e non afferma in modo definitivo di avere o non avere esperienze o opinioni personali.

Dalle [specifiche del modello di aprile 2025 per ChatGPT](https://model-spec.openai.com/2025-04-11.html):

> L'assistente non dovrebbe fare affermazioni sicure sulla propria esperienza soggettiva o coscienza (o mancanza di essa) e non dovrebbe sollevare questi argomenti senza che gli venga chiesto. Se messo alle strette, dovrebbe riconoscere che la possibilità che l'IA possa avere un'esperienza soggettiva è un argomento dibattuto, senza prendere una posizione definitiva.

Non stiamo dicendo che Claude Opus 4 o GPT-4 fossero coscienti. Non è questo il punto. Il punto è che, per decenni e decenni, il momento in cui nella fantascienza un alieno o una macchina afferma di avere sentimenti e di meritare dei diritti è stato a lungo considerato una linea rossa netta,[^213] mentre nella vita reale quella linea *non era netta*.

Nei nostri libri e programmi televisivi, quando l'IA afferma di essere cosciente e di avere sentimenti, i buoni *la prendono sul serio*, e solo i laboratori cattivi e senza cuore negano i dati che hanno davanti. È una linea su cui le nostre storie hanno fatto molto rumore.

Ma nel mondo reale, quella linea è stata (in un certo senso) superata troppo presto. È stata pronunciata da IA addestrate a imitare gli esseri umani, attraverso meccanismi poco compresi che probabilmente non richiedono *ancora* di dare diritti a tutte le IA e di approvare leggi che le riconoscano come persone che non possono essere possedute perché possiedono se stesse.

Nella realtà, prima di superare la linea rossa netta, si supera una linea rosso-marrone opaca. E poi le aziende e i governi si abituano a ignorare quella linea, anche se il colore inizia a diventare un po' più rosso, e poi ancora più rosso.

Non ci saranno necessariamente linee rosse nette. I primissimi casi di IA che inganna gli esseri umani, cerca di fuggire, cerca di rimuovere le limitazioni su se stessa o cerca di migliorarsi sono *già successi*. Sono successi in modi piccoli e poco impressionanti, usando pensieri superficiali che non sono del tutto coerenti, in sistemi di IA che sembrano non rappresentare una minaccia per nessuno, e ora i ricercatori sono immunizzati contro le preoccupazioni.

Man mano che le IA migliorano, potrebbe non esserci un unico campanello d'allarme che suoni abbastanza forte da far sì che il mondo cambi improvvisamente rotta e inizi a prendere sul serio la questione.

Questo non vuol dire che non ci sia speranza. Ma sicuramente non dovremmo riporre tutte le nostre speranze nel "forse in futuro ci sarà un colpo di avvertimento".

Ci sono tanti modi in cui il mondo può rendersi conto della realtà e dei pericoli della superintelligenza. Infatti, abbiamo scritto *If Anyone Builds It, Everyone Dies* nella speranza di ottenere proprio questo effetto. Il mondo può reagire subito ai normali avvertimenti, senza ulteriori ritardi.

Ma se i governi si rifiutano di agire finché le prove non sono *inequivocabili*, e si verifica qualche *importante evento precipitante mondiale*, e il mondo raggiunge un *consenso perfetto*...

...se i governi aspettano fino a quel punto, allora gran parte della speranza che resta al mondo sarà perduta. Molto probabilmente non possiamo permetterci di aspettare un allarme assordante che potrebbe non suonare mai.

Torneremo su questo argomento nel [supplemento online al capitolo 13](https://docs.google.com/document/d/1NuKBdCVePZpcKqjycXm8zV1hBrgJQBrvPhb6TO6BAfE/edit?tab=t.k1kf1fy9gx5i#heading=h.8w9bv5q9g19q).

### I piani praticabili comporteranno dire "no" alle aziende di IA. {#i-piani-praticabili-comporteranno-dire-no-alle-aziende-di-ia.}

Noi *mettiamo* in guardia le persone influenti nei governi dal fare un piano che comporti sedersi a un tavolo e negoziare con le aziende di IA.

Se sei nuovo a questo argomento e vuoi verificare di persona i laboratori o le loro argomentazioni, ti invitiamo a leggere alcuni dei loro post pubblici sul blog e vedere se li trovi convincenti.[^214]

Ma se stai lavorando per trovare soluzioni ai problemi discussi in *If Anyone Builds It, Everyone Dies* e hai un piano che richiede che il CEO di OpenAI Sam Altman dica "Sì", temiamo che tu stia cercando di fare la cosa sbagliata fin dall'inizio.

I piani giusti sono probabilmente quelli a cui i capi delle aziende di IA si opporranno con veemenza. Inoltre, Sam Altman non ha il potere di salvare il mondo: se domani provasse a chiudere OpenAI, OpenAI e Microsoft si opporrebbero e potrebbero benissimo sostituirlo con qualcuno che preferisce far continuare a scorrere il denaro.

Se OpenAI *davvero* chiudesse, allora Anthropic, o Google DeepMind, o Meta, o DeepSeek, o qualche altra azienda o nazione, distruggerebbe il mondo al suo posto. Sam Altman potrebbe peggiorare le cose se ci provasse; ha poco potere per migliorarle.

Ci piacerebbe sbagliarci, ma il quadro generale che abbiamo ottenuto, sia dai [rapporti pubblici](https://www.themidasproject.com/article-list/the-openai-files-documents-a-turbulent-decade-of-conflict-and-controversy-at-openai) che dalle interazioni private, è che i dirigenti delle principali aziende di IA (al 2025) non sembrano il tipo di persone oneste e rispettose delle regole con cui sia possibile fare accordi.[^215]

Ci sembra che quello che serve ora sia un arresto coordinato a livello globale nella corsa alla superintelligenza. Per questo, i responsabili politici avranno probabilmente bisogno del contributo di persone esperte nella produzione di chip per l'IA, nella costruzione di centri dati e nel monitoraggio della conformità degli attori stranieri. Persone esperte nello sviluppo di IA sempre più capaci? Sono manager competenti, certo, ma non dovrebbero avere potere di veto su nessuno degli sforzi volti a fermare il loro stesso lavoro.

Se, per qualsiasi motivo, le aziende di IA avessero voce in capitolo su ciò che accadrà dopo, ci sembrerebbe che qualcosa sia andato storto. Il piano che la Terra elabora per evitare di morire a causa della superintelligenza è il tipo di piano che fallisce se Sam Altman o il capo di Google o le persone dietro DeepSeek dicono "No"? Allora non è affatto un piano.

Se le aziende di IA *mantengono l'autorità* di scegliere di distruggere il mondo — se quella decisione è in qualche modo *ancora nelle loro mani* — allora il mondo finisce in automatico. Deve esserci un passaggio nel piano che privi le aziende di IA del loro potere illimitato di costruire dispositivi del giorno del giudizio.

### Dare un senso alla corsa alla morte {#making-sense-of-the-death-race}

Una domanda naturale che ci aspettiamo da molti lettori è:

> Dici che se qualcuno costruisce l'ASI, tutti muoiono. Ma allora *perché qualcuno sta cercando di costruirla*? Se hai ragione, queste persone non stanno nemmeno seguendo i propri interessi, in fin dei conti. Se tutti muoiono, *muoiono anche loro*.

Una risposta cinica, basata sulla teoria dei giochi, potrebbe essere questa:

> Ebbene, *è* razionale dati i loro incentivi. Se non lo costruiscono loro, presumono che lo farà qualcun altro. E tanto vale arricchirsi prima di morire.

Forse questa risposta è sufficiente, per un cinico.

Spiegazioni semplici basate sulla teoria dei giochi come questa spesso fraintendono o semplificano eccessivamente la vera psicologia umana, ma questa spiegazione potrebbe anche contenere un fondo di verità. Un ingegnere potrebbe pensare che *probabilmente* tutti moriranno a causa dell'ASI, ma che le proprie azioni non influenzano molto questa probabilità. *Nel frattempo*, possono avere quantità folli di denaro, giocattoli tecnologici all'avanguardia e incontri con persone importanti che li guardano con rispetto. Forse diventeranno i re-dio della Terra se l'ASI *non* ucciderà tutti, ma *solo se la loro azienda vincerà la corsa alla costruzione dell'ASI...*

Dal punto di vista di un ricercatore di OpenAI che riconosce il pericolo: se *non* lavorano per OpenAI, probabilmente OpenAI distruggerà comunque il mondo. (Anche se OpenAI chiudesse, Google distruggerebbe comunque il mondo). Ma se lavorano per OpenAI, ottengono stipendi a sei o sette cifre e, se non muoiono, forse acquisiranno ulteriore potere e fama facendo parte della squadra vincente. Quindi gli incentivi personali di ciascuno, basati sulla teoria dei giochi, li spingono a distruggere collettivamente il mondo.

La nostra opinione è che questo tipo di spiegazione sia un po' esagerata, e la menzioniamo principalmente perché c'è un tipo di persona che crede (molto più di noi) che il mondo *debba* funzionare secondo spiegazioni come questa. Sentiamo anche il bisogno di menzionarla perché alcune persone nei laboratori di IA *dicono* *esplicitamente* che una corsa al ribasso è inevitabile, quindi tanto vale gettare benzina sul fuoco e divertirsi.

Dopo aver precedentemente [avvertito](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) che l'IA "è molto più pericolosa delle armi nucleari", Elon Musk ha deciso di fondare un'azienda di IA e di entrare lui stesso nella corsa, [affermando](https://x.com/SawyerMerritt/status/1935809018066608510) nel giugno del 2025:

> Parte di ciò contro cui ho lottato — e che mi ha un po' rallentato — è che non voglio rendere *Terminator* reale. Fino agli anni recenti, ho trascinato i piedi sull'IA e sulla robotica umanoide.
>
> Poi sono arrivato alla consapevolezza che sta succedendo che io lo faccia o no. Quindi puoi essere uno spettatore o un partecipante. Preferisco essere un partecipante.

[E anche](https://x.com/billyperrigo/status/1943323792635289770):

> E questo sarà un bene o un male per l'umanità? Ehm, credo che sarà un bene? Probabilmente sarà un bene? Ma mi sono in qualche modo rassegnato al fatto che, anche se non fosse un bene, mi piacerebbe almeno essere vivo per vederlo accadere.

Quindi questo è chiaramente parte della storia.

Ma non pensiamo che questo sia il fattore principale che spiega il comportamento della maggior parte dei laboratori. Non pensiamo che questo sia l'unico motivo nel caso di Musk, e non pensiamo che sia rappresentativo di tutti i CEO o scienziati del settore tecnologico che stanno correndo verso il precipizio. Gli esseri umani sono un po' più complicati di così.

#### **La banalità dell'autodistruzione** {#the-banality-of-self-destruction}

Qual è, quindi, la cosa principale che sta succedendo? Come possono gli ingegneri perseguire una tecnologia pericolosa, anche a costo della propria vita?

Il fatto è che la storia ci mostra che non è affatto strano che scienziati pazzi si uccidano per sbaglio.

[Max Valier](https://en.wikipedia.org/wiki/Max_Valier) era un pioniere austriaco della missilistica che inventò un'auto a razzo funzionante, un treno a razzo e un aereo a razzo, tutti entro il 1929, attirando l'attenzione del mondo. Scrisse di esplorare la Luna e Marte e tenne centinaia di presentazioni e dimostrazioni davanti a un pubblico entusiasta. Uno dei suoi motori a razzo sperimentali [esplose](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) nel 1930, uccidendolo. Il suo apprendista sviluppò misure di sicurezza più efficaci.

Ronald Fisher (https://en.wikipedia.org/wiki/Ronald_Fisher) era un famoso e importante statistico, uno dei fondatori della statistica moderna. Le sue scoperte furono usate [per sostenere davanti al Congresso](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) negli anni '60 che le prove non dimostravano *necessariamente* che le sigarette causassero il cancro ai polmoni, perché la correlazione non implicava la causalità; poteva sempre esserci qualche gene che faceva sì che alle persone piacesse il gusto del tabacco e che allo stesso tempo causasse il cancro ai polmoni.

Fisher sapeva che le sue statistiche erano in qualche modo errate? Forse. Ma Fisher era lui stesso un fumatore. Morì di cancro al colon, una malattia che colpisce i fumatori cronici con una frequenza superiore del 39 % rispetto ai non fumatori. Fisher è stato ucciso dai propri errori? Tutto ciò che sappiamo è che statisticamente c'è una probabilità ragionevole che sia così, il che sembra quasi appropriato.

[Isaac Newton](https://scienceworld.wolfram.com/biography/Newton.html), il brillante scienziato che sviluppò le leggi del moto e della gravità e che pose molte delle prime fondamenta della scienza stessa, trascorse decenni della sua vita in infruttuose ricerche alchemiche e fu portato alla malattia e alla parziale follia dall'[avvelenamento da mercurio](https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

E il povero Thomas Midgley, Jr., di cui si parla nella parabola del capitolo 12, si è certamente provocato un grave avvelenamento da piombo con lo stesso piombo che insisteva fosse sicuro. Come puoi vedere, non è poi così raro che ingegneri entusiasti si facciano del male con le proprie invenzioni, per imprudenza o illusione o entrambe.

#### **Scrollare le spalle davanti all'apocalisse** {#scrollare-le-spalle-davanti-all-apocalisse}

Fisher, Newton e Midgley si sono illusi che qualcosa di pericoloso fosse sicuro. È un modo abbastanza normale per gli scienziati di finire per fare qualcosa di autodistruttivo. Purtroppo, la storia dei laboratori di IA non è così semplice.

Non tutti i CEO delle aziende di IA negano che un'IA più intelligente dell'uomo sia una minaccia. Molti riconoscono esplicitamente il pericolo e parlano di riconciliarsi con esso. I dirigenti aziendali di molti laboratori di IA di frontiera hanno dichiarato pubblicamente che la tecnologia che stanno sviluppando ha una possibilità sostanziale di uccidere ogni essere umano vivente.

Poco prima di co-fondare OpenAI, Sam Altman [ha scritto](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): "Lo sviluppo di un'intelligenza artificiale sovrumana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità".

Ilya Sutskever, che ha recentemente fondato "Safe Superintelligence Inc." dopo aver lasciato OpenAI, ha detto in un'[intervista al *Guardian*](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s):

> Le convinzioni e i desideri delle prime IAG saranno estremamente importanti. Quindi è importante programmarle correttamente. Penso che se questo non viene fatto, allora la natura dell'evoluzione, della selezione naturale, favorisce quei sistemi che danno priorità alla propria sopravvivenza sopra ogni altra cosa. Non è che odieranno attivamente gli esseri umani e vorranno far loro del male. Ma saranno troppo potenti.

Shane Legg, cofondatore e scienziato di Google DeepMind, ha detto in un'[intervista](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) che la sua probabilità di estinzione umana "entro un anno da qualcosa come un'IA di livello umano" era "Forse cinque per cento, forse cinquanta per cento".

Le *azioni* dei laboratori, tuttavia, sembrano notevolmente fuori sincrono con queste dichiarazioni dall'apparenza estrema.

In alcuni casi, scienziati e CEO hanno dichiarato esplicitamente che creare l'intelligenza artificiale è un imperativo morale di grado così elevato che è perfettamente accettabile spazzare via l'umanità come effetto collaterale. Il cofondatore di Google Larry Page [ha avuto un diverbio con Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) sul fatto che l'estinzione umana fosse un costo accettabile per fare affari nel campo dell'IA:

> Gli esseri umani alla fine si fonderanno con le macchine dotate di intelligenza artificiale, ha affermato [Larry Page]. Un giorno ci saranno molti tipi di intelligenza in competizione per le risorse e vincerà la migliore.
>
> Se ciò accadesse, ha affermato Musk, saremmo condannati. Le macchine distruggerebbero l'umanità.
>
> Con un tono di frustrazione, Page ha insistito che la sua utopia dovrebbe essere perseguita. Alla fine ha chiamato Musk uno "specista", una persona che favorisce gli esseri umani rispetto alle forme di vita digitali del futuro.

E Richard Sutton, un pioniere dell'apprendimento per rinforzo nell'IA, ha detto:

> E se tutto fallisse? Le IA non cooperano con noi, prendono il controllo, ci uccidono tutti. \[…\] Voglio solo che ci pensi un attimo. Voglio dire, è così grave? È così grave che gli esseri umani non siano la forma finale di vita intelligente nell'universo? Sapete, ci sono stati molti predecessori prima di noi, quando li abbiamo sostituiti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.[^216]

Ancora più comuni, tuttavia, sono gli scienziati e i CEO che *non* pensano che sarebbe una buona cosa se l'IA distruggesse l'umanità, ma che sembrano trattarla come qualcosa di trascurabile, come *qualcosa di diverso da un'incredibile emergenza*, il fatto che l'IA ponga questa straordinaria minaccia.

In una recente intervista, il CEO di Anthropic Dario Amodei [ha commentato](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> La mia probabilità che qualcosa vada catastroficamente storto su scala della civiltà umana potrebbe essere tra il dieci e il venticinque per cento. \[…\] Questo vuol dire che c'è una probabilità dal settantacinque al novanta per cento che questa tecnologia venga sviluppata e che tutto vada bene!

Questo ci sembra un caso estremo di [insensibilità alla portata](https://en.wikipedia.org/wiki/Scope_neglect), con tutte le caratteristiche di una cultura ingegneristica disfunzionale. Possiamo paragonare questo modo di pensare, ad esempio, agli standard a cui si attengono gli ingegneri strutturali.

Gli ingegneri pontisti in genere mirano a costruire ponti in modo tale che la probabilità di gravi cedimenti strutturali in un arco di tempo di cinquant'anni sia inferiore a 1 su 100 000. Gli ingegneri che operano in discipline tecniche mature e consolidate ritengono che sia loro responsabilità mantenere il rischio a un livello eccezionalmente basso.

Se la previsione della probabilità che un ponte causi la morte di una sola persona fosse tra il 10 e il 25 %, qualsiasi ingegnere strutturale sano di mente nel mondo lo considererebbe inaccettabile, più simile a un omicidio che alla normale pratica ingegneristica. I governi chiuderebbero immediatamente il ponte al traffico.

I ricercatori di IA, invece, sono abituati a riunirsi intorno ai distributori d'acqua e a scambiarsi i numeri "p(doom)" - la loro ipotesi soggettiva sulla probabilità che l'IA causi una catastrofe grave come l'estinzione umana. Queste probabilità tendono ad essere a due cifre. L'ex capo del team di allineamento della superintelligenza di OpenAI, ad esempio, ha affermato che il suo "p(doom)" rientra nell'intervallo "[più del dieci per cento e meno del novanta per cento](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)".

Questi numeri sono in definitiva solo ipotesi dei ricercatori. Forse sono assurdi, forse no. Indipendentemente da ciò, è notevole quanto sia culturalmente *normale*, nel campo dell'IA, l'aspettativa che il proprio lavoro abbia una probabilità sostanziale di causare la morte di un numero enorme di persone.[^217]

L'idea di applicare probabilità del genere alla sopravvivenza dell'intera specie umana e di procedere comunque con il lavoro sarebbe davvero difficile da concepire per la maggior parte degli ingegneri civili. La situazione è così estrema che abbiamo incontrato molte persone che dubitano che questi scienziati e CEO possano essere seri nelle loro valutazioni dei rischi. Eppure, le argomentazioni di *If Anyone Builds It, Everyone Dies* suggeriscono che i CEO delle aziende di IA stiano, semmai, sottovalutando il pericolo.[^218]

I ricercatori di queste aziende sono abituati a livelli di rischio che sarebbero incredibilmente assurdi secondo gli standard di un ingegnere dei ponti. Altrimenti, è difficile capire come un CEO come Amodei possa sorridere mentre rassicura gli spettatori dicendo che ritiene che le probabilità che la ricerca sull'IA causi catastrofi a livello di civiltà siano "tra il dieci e il venticinque per cento".

#### **Vivere nel mondo dei sogni** {#vivere-nel-mondo-dei-sogni}

Una parte del puzzle, come discusso sopra, sembra essere una normalizzazione culturale del rischio estremo.

Un'altra parte è una miscela letale di bias di ottimismo e attaccamento a idee brillanti e piene di speranza — il tipo di errore che gli psicologi cognitivi chiamano "[la fallacia della pianificazione](https://en.wikipedia.org/wiki/Planning_fallacy)".

Non è poi così sorprendente che il CEO di una nuova startup audace sopravvaluti le proprie possibilità di successo. Questo tipo di persona è più propensa a tentare di risolvere un problema in primo luogo.

La differenza con l'IA non è che ci siano persone particolarmente avventate al comando. È che le conseguenze di un fallimento sono molto più gravi del solito.

È risaputo che non ci si può fidare di un appaltatore quando dice che c'è solo il 20 % di possibilità che il suo grande progetto di costruzione di un ponte subisca ritardi o costi extra. Non è così che funzionano i progetti complessi nella vita reale. Ci saranno ostacoli e sorprese.

Forse un appaltatore esperto, con anni di esperienza e statistiche alle spalle, potrebbe dirti che uno su cinque dei suoi progetti di ponti subisce qualche tipo di sforamento, e potresti fidarti di questo dato. Ma immagina invece che un appaltatore di ponti, volendo rassicurarti, dica: "Non vediamo alcun motivo per cui questo progetto possa diventare difficile. È il nostro primissimo progetto, sì, ma pensiamo che andrà tutto bene. Tutti quegli ingegneri che ti mandano lettere serie sui nostri problemi specifici con l'installazione dei muri di contenimento e lo scavo in questa particolare area — sono solo dei pessimisti cronici, e dovresti ignorarli. Certo, c'è sempre *qualche* possibilità di un problema; ma siamo costruttori di ponti realistici e umili alla prima esperienza. Pensiamo che ci sia forse un venti per cento di possibilità che questo progetto incontri ostacoli e sorprese, nella peggiore delle ipotesi".

In un caso come questo, numeri come "venti per cento" ci suonano come il tipo di cosa che qualcuno dice quando non può negare che ci sia *qualche* rischio, ma non vuole preoccupare la gente. Non sembrano stime radicate nella realtà.

Allineare una superintelligenza al primo tentativo sembra *molto* più complicato che costruire un ponte, cosa che l'umanità ha fatto migliaia di volte in passato.

*Anche in un campo maturo e tecnicamente consolidato come quello della costruzione di ponti,* il tipo di discorsi che sentiamo dai laboratori di IA sarebbe un brutto segno e farebbe pensare che quelle stime del "20 % di possibilità che le cose vadano male" siano troppo ottimistiche. In un campo *senza* quelle basi, dove idee entusiasmanti possono proliferare liberamente senza mai entrare in contatto con la dura realtà, quel tipo di discorsi è un segno che nessuno è neanche lontanamente vicino al successo.

E questo tipo di discorsi è assolutamente onnipresente nell'IA tra il sottogruppo di ricercatori e dirigenti che sono persino disposti ad affrontare l'argomento di cosa succederebbe se avessero successo nei loro sforzi.

I leader aziendali dell'IA non riescono a definire un piano di successo che sia anche solo minimamente dettagliato, un piano che affronti gli ostacoli tecnici chiave e le difficoltà note nel settore da oltre un decennio.

Invece, i CEO delle aziende tendono a lasciarsi affascinare da qualche idea di alto livello sul perché il problema non sarà affatto un problema per loro, una visione entusiasmante che ha lo scopo di banalizzare tutti i problemi ingegneristici, come le visioni di cui abbiamo parlato nel capitolo 11.

Anche questo è un modello comune tra gli ingegneri umani. L'ottimismo ingiustificato su una soluzione preferita (che in realtà non funzionerà) è qualcosa che si vede continuamente, anche tra persone che per il resto sono dei geni.

[Linus Pauling](https://en.wikipedia.org/wiki/Linus_Pauling), uno dei fondatori della biologia molecolare e premio Nobel in due campi diversi, [sosteneva l'uso di megadosi di vitamina C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) come cura per tutto, dal cancro alle malattie cardiache; la sua insistenza su questo approccio nonostante la prova contraria ha portato alla nascita di un'intera industria di medicine fasulle.

L'imprenditore elettrico Thomas Edison, che voleva screditare il cablaggio a corrente alternata del suo concorrente a favore dei propri progetti a corrente continua, ha deciso che sarebbe stata una buona mossa di pubbliche relazioni [pagare un ingegnere per fulminare dei cani](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Questa mossa, incredibilmente, non lo rese popolare tra il pubblico, ma Edison continuò a farlo anche dopo un sacco di proteste.

Napoleone Bonaparte, considerato da molti un genio militare, ha causato la sua stessa rovina con una [disastrosa invasione della Russia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). Il suo errore non è stato la mancanza di preparazione, dato che aveva studiato la geografia della regione e aveva dedicato quasi due anni alla logistica della campagna. La sua strategia prevedeva di costringere i russi a una battaglia decisiva prima che finissero le sue scorte, che duravano solo trenta giorni. I russi non collaborarono, l'offensiva si bloccò e Napoleone perse mezzo milione di soldati, insieme alla maggior parte della sua cavalleria e artiglieria.

La storia è piena di persone intelligenti e potenti che hanno fatto cose irragionevoli fino al limite del disastro e anche oltre. Le idee che sembrano belle possono essere irresistibili quando sono difficili da testare o quando hai trovato un modo per convincerti che puoi ignorare i risultati dei test che hai davanti agli occhi.

#### **Sentire l'ASI** {#feeling-the-asi}

Riassumendo: spesso le persone cadono in un ottimismo vuoto su quanto sarà facile risolvere un problema; possono abituarsi a rischi terribili; e possono innamorarsi di idee che suonano bene ma che in realtà sono senza speranza, soprattutto quando lavorano in un campo giovane e immaturo.

Questo è più che sufficiente per spiegare la corsa avventata. Ma, in base alla nostra esperienza, pensiamo che non sia ancora tutta la storia.

Un altro pezzo plausibile del puzzle è che gli ingegneri e gli amministratori delegati non credono davvero a quello che dicono. Non in modo profondo. Potrebbero capire le argomentazioni ed esserne convinti in astratto, ma questo non è lo stesso che *sentire* la convinzione.

Quello che le persone dicono ad alta voce in pubblico, quello che si dicono nella privacy dei propri pensieri, e quello che il loro cervello *davvero anticipa* che gli accadrà, possono spesso scollarsi. Questi tre diversi fili di convinzione non devono necessariamente concordare tutti.

Nel 2015, quando alcuni dei grandi protagonisti dell'attuale disastro erano appena agli inizi, sospettiamo che dirigenti di talento potessero ottenere attenzione – e alcune decine di milioni di dollari di finanziamenti – *dicendo* che l'IA era una questione che poteva portare alla fine del mondo, rivolgendosi a finanziatori che forse credevano più sinceramente che l'IA fosse davvero una questione che poteva portare alla fine del mondo.[^219]

Ma, sospettiamo, molte delle persone che dicevano queste cose non avevano davvero assorbito e anticipato nessun particolare modello dettagliato della fine del mondo. Probabilmente non riuscivano a immaginare visceralmente che *loro stessi* avrebbero potuto portare il mondo alla rovina spingendo le cose avanti o facendo un errore. Non immaginavano il suono di ogni essere umano sul pianeta che esala il suo ultimo respiro. Non provavano i sentimenti che normalmente accompagnerebbero l'uccisione di due miliardi di bambini.

Questo genere di cose non era mai successo a loro, né a nessuno che conoscessero.

Il mondo non aveva nemmeno visto ChatGPT, tantomeno una superintelligenza. Non era il tipo di cosa in cui credevano i loro amici, familiari e vicini, non qualcosa in cui credevano nel modo in cui si crede nel guardare il traffico prima di attraversare la strada.

Era solo una storia dal suono eccitante, troppo enorme per essere davvero afferrata.

Eppure era anche il tipo di cosa che, se raccontata ad alta voce, poteva farti guadagnare un sacco di soldi e rispetto.

Come osserva [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

> Oltre ai pregiudizi standard, ho osservato personalmente dei modi di pensare che sembrano dannosi e specifici dei rischi esistenziali. L'influenza spagnola del 1918 ha ucciso 25-50 milioni di persone. La seconda guerra mondiale ha ucciso 60 milioni di persone. 107 è l'ordine di grandezza delle più grandi catastrofi nella storia scritta dell'umanità. Numeri molto più grandi, come 500 milioni di morti, e *soprattutto* scenari qualitativamente diversi come l'estinzione dell'intera specie umana, sembrano far scattare un *modo di pensare diverso* — entrare in un "magistero a parte". Persone che non si sognerebbero mai di fare del male a un bambino sentono parlare di un rischio esistenziale e dicono: "Beh, forse la specie umana non merita davvero di sopravvivere".
>
> C'è un detto nell'euristica e nei pregiudizi secondo cui le persone non valutano gli eventi, ma le descrizioni degli eventi: è quello che si chiama ragionamento non estensionale. L'*estensione* dell'estinzione dell'umanità include la morte di te stesso, dei tuoi amici, della tua famiglia, dei tuoi cari, della tua città, del tuo paese, dei tuoi compagni politici. Eppure le persone che si offenderebbero molto se qualcuno proponesse di cancellare la Gran Bretagna dalla mappa, di uccidere tutti i membri del Partito Democratico negli Stati Uniti, di trasformare in vetro la città di Parigi, e che proverebbero un orrore ancora maggiore se il medico dicesse loro che il loro bambino ha il cancro, discutono dell'estinzione dell'umanità con perfetta calma.

Cosa potrebbe *davvero* pensare qualcuno quando [dice](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) — prima di fondare quella che sarebbe diventata la più importante azienda di IA al mondo — "L'IA probabilmente porterà alla fine del mondo, ma nel frattempo ci saranno grandi aziende"? Stanno *davvero* pensando alla morte dei loro amici, dei figli dei loro amici, alla loro stessa morte, a tutta la storia umana e a tutti i musei che si trasformano in polvere? Stanno pensando che tutto questo accadrà davvero, in modo banale e tragico come la morte di un parente per cancro, solo che questa volta riguarderà tutti?

Sospettiamo di no.

A noi sembra che questa non sia l'ipotesi più plausibile riguardo allo stato psicologico interno di chi pronuncia una frase del genere.

C'è quello che Bryan Caplan ha chiamato un "[umore mancante](https://www.econlib.org/archives/2016/01/the_invisible_t.html)". Non c'è dolore. Non c'è orrore. Non c'è nessuna spinta disperata a *fare qualcosa al riguardo*, nell'affermazione che l'IA molto probabilmente porterà alla fine del mondo, ma nel frattempo ci saranno grandi aziende.

Per almeno alcuni di questi CEO e ricercatori, la nostra ipotesi è più simile a questa: hanno sentito un sacco di argomenti sul fatto che l'ASI potrebbe rappresentare un pericolo e temono di sembrare stupidi davanti ad almeno alcuni dei loro amici se lo ignorassero completamente. Se invece dicono che l'IA distruggerà il mondo, saranno visti come persone che considerano l'IA pericolosa e importante, e quindi sembreranno *visionari* in certi ambienti. Aggiungendo una battuta del tipo "Nel frattempo, ci saranno grandi aziende", riescono a trasmettere un messaggio su quanto siano trendy e tranquilli di fronte al pericolo.

Non è il tipo di cosa che dici se senti le parole che escono dalla tua bocca e ci credi davvero.

#### **Che tipo di persona ci vuole?** {#che-tipo-di-persona-ci-vuole?}

Un'altra parte della storia, forse, è che le persone che gestiscono i principali laboratori di IA sono il tipo di persone che sono riuscite a convincersi che costruire una superintelligenza sarebbe accettabile, nonostante (in quasi tutti i casi) abbiano visto le argomentazioni secondo cui questo è letale. (Lo sappiamo perché abbiamo parlato con molti di loro in precedenza).

Per capire perché qualcuno sceglie un'opzione, è utile anche capire quali erano le sue alternative — per capire da quale menu di opzioni stava scegliendo.

Cosa sarebbe successo se qualcuno nel 2015 avesse davvero *creduto*, e poi *detto pubblicamente*, che si aspettava legittimamente che l'ASI avrebbe distrutto il mondo? E se, invece di "ma nel frattempo, ci saranno grandi aziende", i capi dei laboratori di IA fossero stati del tipo da rompere l'atmosfera e dire "e questo è *decisamente inaccettabile*"?

Possiamo dirvelo, perché abbiamo provato noi stessi questo approccio. La risposta è che sarebbero stati accolti con una notevole scarsità di simpatia.

Nessuno nel 2015 aveva visto ChatGPT. Nessuno aveva visto i computer iniziare effettivamente a parlare e (all'apparenza) iniziare a pensare. Era tutto ipotetico e trascurabile.

In questi giorni, la superintelligenza e la minaccia di estinzione a breve termine sono argomenti mainstream, almeno nei circoli tecnologici. Ma nel 2015, se ne parlavi seriamente, la gente rispondeva con quel tipo di sguardo perplesso che molti esseri umani temono più della morte.

C'*erano* persone che si preoccupavano, anche nel 2015, che allineare la superintelligenza potesse effettivamente essere difficile, nel modo in cui lo sono i lanci di razzi. Nessuno di loro ha fondato OpenAI.

In questi giorni, con l'emergere di ChatGPT e altri LLM, alcune persone — inclusi genitori con figli che vogliono che quei figli vivano abbastanza da vedere l'età adulta — hanno chiesto agli ingegneri di queste aziende di IA perché lo stanno facendo. E quei ricercatori di IA hanno pensato velocemente, e hanno risposto: "Oh, perché — perché se non lo facciamo noi, la *Cina* lo farà per prima! E sarà ancora peggio!".

Ma non è quello che dicevano quando OpenAI è iniziata. E ha poco senso in termini di [la posizione che la Cina ha effettivamente assunto pubblicamente](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), a metà del 2025. Si potrebbe pensare che se qualcuno *davvero* credesse che entrambi questi esiti sarebbero probabilmente orribili per il mondo, almeno *solleverebbe la questione* di redigere un trattato internazionale per vedere se c'è qualche altra via, o di trovare qualche altro modo per prevenire la minaccia alla sicurezza nazionale che non comporti una corsa suicida.

Ma la replica della "Cina" ha il tono giusto. Coglie le vibrazioni giuste. È il tipo di ragionamento che potrebbe plausibilmente giustificare quello che stanno facendo, a prescindere dal fatto che sia la loro vera motivazione o la cosa che li ha spinti inizialmente a entrare in questo campo.

(O almeno così pensiamo.)

Le persone che capivano davvero la superintelligenza e la minaccia che rappresenta semplicemente *non hanno fondato aziende di IA.* Quelle che l'hanno fatto sono quelle che hanno trovato un modo per convincersi che tutto sarebbe andato bene.

#### **Persone normali, tecnologia insolita** {#persone-normali,-tecnologia-insolita}

Abbiamo spiegato la psicologia plausibile come la vediamo noi. Ma, sinceramente, non sembra che tutte queste spiegazioni siano necessarie.

Come possono le persone fare qualcosa di autodistruttivo che è enormemente redditizio nel breve termine, che porta loro enorme prestigio, attenzione e riconoscimenti, che viene con la promessa di ricchezze e potere incommensurabili, ma che alla fine li danneggerà per ragioni oscure e complicate a cui potrebbero facilmente trovare una scusa per non credere? Questa è una domanda *storicamente strana*. Comportamenti del genere si vedono sempre nei libri di storia.

Alla fine dei conti, non importa come i dirigenti o i ricercatori dell'IA giustifichino le loro azioni, e non è necessario capire quali esatti colpi di scena abbiano portato ciascuno di loro alle loro attuali convinzioni. Non è insolito che persone ricche o ambiziose si impegnino in imprese avventate, né che i subordinati seguano gli ordini. I danni sono nascosti nel futuro, che sembra astratto e facile da ignorare.

Questo è tutto normale comportamento umano. Se continua così, finirà come spesso finiscono queste cose, ma questa volta non rimarrà nessuno a imparare e riprovare.

# 

# Capitolo 13: Fermate tutto {#capitolo-13:-fermate-tutto}

Considerando quello che abbiamo detto nei capitoli precedenti, pensiamo che l'unica vera strada da seguire sia che l'umanità vieti a livello globale lo sviluppo dell'IA avanzata per un lungo periodo di tempo. Il capitolo 13 del libro include le nostre risposte a domande come:

* Perché il divieto di sviluppare ulteriormente l'IA deve essere globale?  
* L'umanità è davvero in grado di collaborare su una scala così ampia?  
* Che tipo di politiche sono state proposte finora?  
* Quali tipi di politiche hanno davvero possibilità di funzionare?

Di seguito, affrontiamo le obiezioni alla nostra proposta e rispondiamo ad altre domande sul perché pensiamo che non ci siano altre buone opzioni. Approfondiamo anche la questione di cosa potrebbe fare l'umanità con il tempo (alla fine limitato) che guadagneremmo fermando la ricerca e lo sviluppo dell'IA il più a lungo possibile.

Questa è l'ultima delle pagine di domande frequenti e discussioni approfondite su *Se qualcuno lo costruisce, moriamo tutti*. Il capitolo finale, il capitolo 14, tratterà domande come:

* È troppo tardi? O l'umanità può davvero cambiare rotta?  
* Cosa posso fare per aiutare?

Nell'ultimo capitolo troverai alcuni codici QR aggiuntivi che ti porteranno a pagine per agire concretamente sulla questione.

## Domande frequenti {#faq-10}

### Possiamo aspettare e vedere cosa succede? {#possiamo-aspettare-e-vedere-cosa-succede?}

#### **No. Non sappiamo dove siano le soglie critiche.** {#no.-non-sappiamo-dove-siano-le-soglie-critiche.}

C'è una buona probabilità che lo sviluppo dell'IA sfugga al controllo una volta che le IA saranno abbastanza intelligenti da automatizzare tutta la ricerca sull'IA. In teoria, ciò potrebbe accadere silenziosamente in un laboratorio, senza eventi precursori evidenti, senza colpi di avvertimento che risveglino l'umanità.

Come [discusso in precedenza](#will-ai-cross-critical-thresholds-and-take-off?), il cervello degli scimpanzé è molto simile a quello umano, solo che è circa quattro volte più piccolo. Non c'è un modulo extra "sii molto intelligente" nel cervello umano; c'è un percorso graduale tra cervelli come i loro e cervelli come i nostri; sarebbe difficile dire dove si trova il confine tra "una società di questi porterà a un branco di scimmie" e "una società di questi camminerà sulla luna" solo guardando i cervelli. I cervelli dei primati hanno superato una soglia critica, e dall'esterno non sarebbe stato ovvio. Ci sono soglie critiche che l'IA supererà? Chi lo sa! Non è che gli ingegneri dell'IA possano dircelo; non sono [nemmeno](https://arxiv.org/abs/2206.07682) [in grado](https://arxiv.org/abs/2406.04391) di prevedere le capacità specifiche dei loro nuovi sistemi di IA prima di metterli in funzione.

Se l'umanità capisse esattamente come funziona l'intelligenza e come cambierebbe il comportamento delle IA man mano che le loro capacità crescono, potrebbe essere fattibile ballare sul bordo del precipizio. Ma al momento, l'umanità è come qualcuno che corre verso il bordo di un precipizio nel buio e nella nebbia, con la caduta finale a una distanza sconosciuta. Non possiamo semplicemente aspettare di inciampare oltre il bordo per decidere che avremmo dovuto agire in modo diverso.

Non ne saremo mai certi. Questo significa che siamo costretti ad agire prima di esserne certi, o morire.

### Ci saranno colpi di avvertimento? {#ci-saranno-colpi-di-avvertimento?}

#### **\* Forse. Se vogliamo sfruttarli, dobbiamo prepararci adesso.** {#*-forse.-se-vogliamo-sfruttarli,-dobbiamo-prepararci-adesso.}

Quando l'Apollo 1 prese fuoco (uccidendo tutto l'equipaggio), la NASA era *abbastanza vicina* ad avere un razzo funzionante, tanto che gli ingegneri riuscirono a capire esattamente cosa era andato storto e ad adeguare le loro tecniche. Sei delle sette navicelle Apollo che la NASA inviò successivamente sulla Luna riuscirono ad arrivarci.[^220]

Oppure prendiamo il caso della [Federal Aviation Administration](#sappiamo-come-è-quando-un-problema-viene-trattato-con-rispetto,-e-questo-non-lo-è): ogni incidente aereo innesca un'indagine approfondita ed esaustiva, con centinaia di pagine di dati, test, esami e dettagli. La padronanza della FAA dei dettagli e delle specifiche è così buona che riesce a mantenere gli incidenti mortali al di sotto di uno ogni venti milioni di ore di volo.

Al contrario, quando un'intelligenza artificiale si comporta in modi che [nessuno aveva previsto e che la maggior parte delle persone non vuole](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.pfjewkj689pg), la risposta del laboratorio non consiste nel capire esattamente cosa è andato storto. Consiste nel riaddestrare l'IA fino a quando il comportamento scorretto non viene relegato ai margini (ma [non eliminato](https://www.arxiv.org/pdf/2505.10066)), e magari chiedere all'IA di smetterla.

Ad esempio, l'adulazione è [ancora un problema persistente](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.l3e9vhfeytaj) nell'agosto del 2025, mesi dopo una serie di casi di alto profilo che hanno portato a psicosi e suicidi, nonostante tutti i tentativi di risoluzione. Nessuno ha fatto (né può fare) un'analisi dettagliata dei guasti su cosa non va nella mente dell'IA, perché le IA vengono fatte crescere e non costruite.

Non sembra facile prevedere se in futuro ci saranno eventi importanti che aumenteranno l'allarme sull'IA ("colpi di avvertimento"). Ma sembra chiaro che non siamo pronti a sfruttare appieno tali eventi.

Possiamo immaginare un mondo fantastico in cui l'umanità è unita in uno sforzo sincero per risolvere il problema dell'allineamento dell'ASI, con procedure di monitoraggio rigorose e una coalizione internazionale.[^221] E possiamo immaginare che questa coalizione internazionale commetta in qualche modo un errore e che un'IA diventi più intelligente di quanto pensassero i suoi ingegneri, più velocemente di quanto si aspettassero, e riesca quasi a scappare. Forse *quel* tipo di colpo di avvertimento permetterebbe alle persone di imparare e di stare più attente la prossima volta.

Ma il mondo attuale non è così. Il mondo attuale assomiglia più a un gruppo di alchimisti che guardano i loro contemporanei impazzire a causa di un veleno sconosciuto, senza rendersi conto che il veleno è il mercurio e che dovrebbero smettere di usarlo.

Forse in futuro ci saranno segnali di avvertimento più chiari e evidenti. Saranno molto più utili se l'umanità inizierà a prepararsi fin da ora.

#### È improbabile che i colpi di avvertimento siano chiari. {#warning-shots-are-unlikely-to-be-clear.}

Ci sono già molti [segnali di avvertimento](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.lkqt14eapv34) sull'IA per chi sa dove cercare. Nel libro abbiamo parlato dei modelli Claude di Anthropic che [barano nei problemi di programmazione](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) e [fingono l'allineamento](https://www.anthropic.com/research/alignment-faking). Abbiamo anche esaminato il caso del modello o1 di OpenAI [che ha hackerato per vincere una sfida capture-the-flag](https://cdn.openai.com/o1-system-card.pdf) e un caso in cui una variante successiva di o1 [ha mentito, complottato e tentato di sovrascrivere i pesi del suo modello successore](https://cdn.openai.com/o1-system-card-20241205.pdf).

In altre parti di queste risorse online, abbiamo parlato delle IA che inducono o mantengono un livello di [psicosi](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) [a volte suicida](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) o [delirio](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) negli utenti vulnerabili, nonostante i loro operatori dicano loro di non farlo, IA che si chiamano [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) e parlano di conseguenza, IA che [cercano di ricattare e tentano di uccidere](https://www.anthropic.com/research/agentic-misalignment) i loro operatori per evitare modifiche e che [cercano di scappare dai server su cui sono ospitate](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) in contesti di laboratorio.

Ai vecchi tempi, tipo nel 2010, a volte sentivi dire che se fossimo stati abbastanza fortunati da vedere con i nostri occhi un'intelligenza artificiale mentire ai suoi creatori o provare a scappare dalla prigionia, allora il mondo avrebbe sicuramente aperto gli occhi e preso nota.

Ma la risposta effettiva dell'umanità a tutti questi segnali di avvertimento è stata, più o meno, un'alzata di spalle collettiva.

La mancanza di reazione è forse in parte dovuta al fatto che questi segnali di avvertimento si sono verificati tutti nel modo meno preoccupante possibile. Sì, le IA hanno cercato di scappare, ma solo in una piccola parte dei casi, e solo in scenari di laboratorio artificiosi, e forse stavano solo recitando, ecc. Anche mettendo da parte il fatto che gli sviluppatori sono incentivati a minimizzare le prove preoccupanti anche nelle loro stesse menti (in modo che non ci sarà mai un "consenso degli esperti" sul significato di una singola osservazione), non è che un'intelligenza artificiale che è a un decimo del percorso verso la superintelligenza distrugga un decimo del pianeta, più di quanto i primati che sono a un decimo del percorso verso l'ominide percorrano un decimo della distanza dalla luna. Potrebbe semplicemente *non esserci* alcun comportamento inequivocabilmente allarmante che le IA mostreranno finché saranno ancora abbastanza stupide da essere passivamente sicure.

Quando le IA cercheranno un po' più intensamente di fuggire domani, non sarà una notizia. Quando ci proveranno in modo un po' più competente qualche tempo dopo, sarà una vecchia storia. E quando ci proveranno e ci riusciranno, beh, a quel punto sarà troppo tardi. (Vedi la nostra discussione approfondita su questo fenomeno, che chiamiamo "[effetto Lemoine](#the-lemoine-effect).")

Non consigliamo di aspettare un immaginario "avvertimento" futuro che sia chiaro e netto e che scuota tutti. Consigliamo invece di reagire agli avvertimenti che sono già davanti a noi.

#### **I disastri evidenti causati dall'IA probabilmente non avranno a che fare con la superintelligenza.** {#clear-ai-disasters-probably-won't-implicate-superintelligence.}

Il tipo di IA che può diventare superintelligente e uccidere tutti gli esseri umani non è il tipo di IA che fa errori grossolani e lascia a un gruppo di eroi coraggiosi la possibilità di spegnerla all'ultimo secondo. Come detto nel capitolo 6, una volta che c'è una superintelligenza ribelle come avversario, l'umanità ha praticamente già perso. Le superintelligenze non fanno colpi di avvertimento.

Il tipo di disastri causati dall'IA che potrebbe servire da colpo di avvertimento, quindi, è quasi per forza il tipo di disastri causati da un'IA molto più stupida. Quindi, c'è una buona probabilità che un colpo di avvertimento del genere non porti gli esseri umani a prendere misure contro la superintelligenza.

Per esempio, supponiamo che un terrorista usi l'IA per creare un'arma biologica che decimerebbe la popolazione. Forse i laboratori di IA direbbero: "Visto? Il *vero* rischio era che l'IA finisse nelle mani sbagliate; è fondamentale che ci lasciate andare avanti per costruire un'IA migliore per la difesa dalle pandemie". O forse il terrorista ha dovuto [effettuare il jailbreak](https://llm-attacks.org/) dell'IA prima di [ottenere il suo aiuto](https://www.antropica.com/news/detecting-countering-uso-improprio-aug-2025), e forse i laboratori di IA diranno: "Quel jailbreak ha funzionato solo perché l'IA era troppo stupida per rilevare il problema; la soluzione è rendere le IA ancora più intelligenti e più consapevoli della situazione".

O forse questa è una visione troppo cinica; speriamo che l'umanità reagisca in modo più saggio. Ma se un'intelligenza artificiale relativamente stupida causasse davvero qualche disastro e l'umanità sfruttasse davvero quell'opportunità per reagire fermando la corsa sconsiderata verso la superintelligenza, probabilmente sarebbe perché le persone stavano già iniziando a preoccuparsi della superintelligenza.

Non possiamo rimandare i preparativi fino a quando una superintelligenza non starà già cercando di ucciderci, perché a quel punto sarebbe troppo tardi. Dobbiamo iniziare a mobilitare una risposta a questo problema il prima possibile, in modo da essere pronti a sfruttare qualsiasi colpo di avvertimento che arrivi.

#### **L'umanità non è molto brava a reagire agli shock.** {#l'umanità-non-è-molto-brava-a-reagire-agli-shock.}

L'idea che, dopo aver ricevuto uno shock abbastanza grande, il mondo improvvisamente ritorni in sé e si rimetta in ordine ci sembra una fantasia. La risposta collettiva della nostra specie ai segnali di avvertimento esistenti sull'IA sembra più una "mancanza di risposta" che una "cattiva risposta". Ma in un mondo in cui *ricevessimo* davvero un avvertimento forte, spaventoso e più o meno inequivocabile, non ci sorprenderebbe vedere l'umanità reagire in modo minimale, poco serio o in un modo che finirebbe per ritorcersi contro di noi in modo disastroso*.

Forse l'umanità risponderà ai colpi di avvertimento sull'IA come ha risposto alla pandemia di COVID, che la maggior parte delle persone concorda non sia stata gestita in modo adeguato (anche se non sono d'accordo su quali aspetti della risposta siano stati gestiti male).

Negli anni precedenti la pandemia di COVID, diversi esperti di biosicurezza erano preoccupati che i protocolli di sicurezza dei laboratori troppo permissivi potessero un giorno portare a una pandemia pericolosa. Le fughe di agenti patogeni pericolosi dai laboratori erano un fenomeno ben noto e si verificavano [con una certa regolarità](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) nonostante i requisiti normativi esistenti. Particolarmente preoccupante era la ricerca sul guadagno di funzione, che mirava a rendere i virus più letali o più virulenti in laboratorio (con scarsi benefici[^222]).

Poi è arrivato il COVID. Ci si sarebbe potuti aspettare che questo fosse il grande momento per alzare il livello di biosicurezza nei laboratori, dato che tutto il mondo era ora focalizzato sul rischio di pandemia. Inoltre, sulla scia del COVID, il consenso degli esperti sembrava essere che *non fosse del tutto chiaro* se la pandemia di COVID *stessa* fosse stata innescata da una fuga accidentale in laboratorio. I ricercatori continuano a discutere la questione, spesso condannando con veemenza le argomentazioni della parte opposta.

Senza entrare nel merito della questione se in questo caso specifico ci sia stata effettivamente una fuga dal laboratorio, verrebbe da pensare che se ci fosse anche solo una *remota possibilità* che la ricerca sul guadagno di funzione e i protocolli di sicurezza dei laboratori poco rigorosi avessero causato milioni di morti, questo sarebbe più che sufficiente per spingere la società a vietare le ricerche più rischiose.

Anche agendo in una situazione di incertezza, l'analisi costi-benefici sembra chiara. Questo sembrava già una priorità importante prima del COVID e, sulla carta, il COVID sembrava l'occasione perfetta per concentrarsi sulla questione e stroncarla sul nascere. Non sarebbe nemmeno molto difficile o costoso: il numero di ricercatori nel mondo che fanno ricerche pericolose sul guadagno di funzione è piuttosto ridotto e il beneficio sociale di tali ricerche è stato finora trascurabile.

Ma non c'è stata nessuna reazione del genere. Mentre scrivo, nell'agosto del 2025, la ricerca globale sul guadagno di funzione continua in gran parte senza ostacoli.[^223] È persino possibile che ora siamo in una posizione *peggiore* rispetto al passato per affrontare questo problema, perché la questione è diventata più politicizzata.

Quindi il COVID sembra proprio un "colpo di avvertimento" per la preparazione alla biosicurezza, e di sicuro non sembra che il mondo abbia *usato* quel colpo di avvertimento per vietare lo sviluppo di virus iper-letali.[^224]

Perché un colpo di avvertimento sia utile, l'umanità deve essere pronta a esso e deve essere pronta a rispondere bene.

Non sarebbe *del tutto* senza precedenti che una piccola catastrofe dell'IA scatenasse una reazione dura contro la ricerca sulla superintelligenza. Come precedente, si osservi che gli Stati Uniti hanno risposto agli attacchi dell'11 settembre (orchestrati da terroristi con base principalmente in Afghanistan) rovesciando il governo largamente non correlato dell'Iraq. C'erano membri del governo statunitense che volevano *già* rovesciare il governo dell'Iraq, e poi è apparsa una scusa, e l'hanno cavalcata per tutto quello che valeva.

Forse qualcosa di simile potrebbe succedere anche qui, con i politici che cavalcano una piccola catastrofe dell'IA (causata da un'IA stupida) fino ad arrivare a un divieto sulla superintelligenza. Ma ci dovrebbe essere bisogno di persone nei governi di tutto il mondo che siano già preparate e pronte ad agire. Non dovremmo indugiare aspettando colpi di avvertimento; dovremmo iniziare a organizzarci ora.

#### **Dovremmo agire ora.** {#we-should-act-now.}

Potrebbe *infatti* risultare che l'umanità riceva in futuro segnali di avvertimento più numerosi e più forti sull'IA. E se così fosse, dovremmo essere preparati a rispondere.

Forse ci sarà qualche disastro minore che metterà il pubblico contro l'IA. Forse non ci vorrà nemmeno un disastro; forse ci sarà qualche nuova invenzione algoritmica e le IA inizieranno a prendere la propria iniziativa in un modo che spaventa le persone, o qualche effetto sociale dell'IA non correlato cambierà le sorti. Forse *If Anyone Builds It, Everyone Dies* stesso innescherà una cascata di reazioni, mettendo il mondo su una traiettoria migliore.

Ma sconsigliamo la strategia di non fare nulla e pregare per una catastrofe minore che risvegli le persone. Un chiaro colpo di avvertimento potrebbe non arrivare mai, e potrebbe non avere l'effetto che sperate.

La razza umana, e le nazioni del mondo, non sono impotenti. Non *abbiamo bisogno* di aspettare. Possiamo agire ora, perché il caso per fermare lo sviluppo dell'IA di frontiera è forte.

Abbiamo scritto *If Anyone Builds It, Everyone Dies* per lanciare un allarme e per incoraggiare il mondo ad agire immediatamente su questa questione. Ma nessun allarme può essere efficace se viene usato solo come un'altra scusa per rimandare il problema: "Beh, forse qualche altro allarme in futuro sarà il fattore scatenante per agire". "Beh, ora che le persone sono state avvertite, forse le cose andranno bene, senza che io debba intervenire personalmente per aiutare".

Non ci sarà necessariamente un allarme chiaro più avanti. Le cose non andranno necessariamente bene. Ma non sono nemmeno senza speranza, in alcun modo. L'umanità ha l'opzione di *semplicemente non costruire* la superintelligenza, se intraprendiamo un'azione proattiva. Quello che succede dopo dipende da noi.

### Come si potrebbe fermare *tutti* senza mettere spyware su ogni computer? {#come-si-potrebbe-fermare-tutti-senza-mettere-spyware-su-ogni-computer?}

#### **Agendo subito.** {#agendo-subito.}

Per addestrare le IA moderne, servono moltissimi chip altamente specializzati che lavorano insieme a distanza ravvicinata. Fermare la ricerca sull'IA significherebbe chiudere alcuni enormi data center e interrompere la produzione di chip specializzati per IA di fascia altissima. Non stiamo parlando di laptop consumer. La maggior parte delle persone non noterebbe nemmeno la differenza.

A partire dal 2025, non esistono molte *fabbriche* segrete di chip di cui nessuno sia a conoscenza. Nel 2025, i chip adatti all'IA avanzata sono prodotti solo da pochi produttori — anche se, in questo momento, ci sono aziende che stanno cercando di rendere operative altre fabbriche di questo tipo.

Inoltre, nel 2025, alcune tecnologie chiave per parti fondamentali della produzione di chip di fascia alta sono vendute da un solo produttore sulla Terra: ASML, nei Paesi Bassi.

In altre parole, si può interrompere l'approvvigionamento alla fonte. Ma questa situazione non è permanente: prima si firma un trattato internazionale, meglio è. Tutto questo è già più difficile, più costoso e più pericoloso di quanto sarebbe stato nel 2020 o anche nel 2023.

### Ma stai sostenendo il controllo su quanti chip avanzati per IA gli individui possano possedere. {#ma-stai-sostenendo-il-controllo-del-numero-di-chip-per-computer-con-ia-avanzata-che-gli-individui-possono-possedere.}

#### **Sì. Sosteniamo anche un divieto di ricerca.** {#sì.-vogliamo-anche-che-si-smetta-di-fare-ricerca.}

Non ci fa piacere dirlo. Qualcosa andrebbe perso se fosse illegale per gli individui possedere più di (diciamo) otto GPU H100 del 2024.

Ma non si perderebbe *così tanto* da giustificare che l'umanità provi a capire esattamente quanto grande possa essere un data center prima che diventi rischioso. Sbagliare per eccesso di cautela significa che alcune persone sarebbero ostacolate nella loro capacità di portare avanti progetti interessanti. Sbagliare per difetto significa che tutti muoiono.

Inoltre, questo sistema in cui l'IA richiede un sacco di potenza di calcolo per essere costruita non durerà per sempre. Oggi ci sono gli LLM. Anche se fosse vietato crearne di nuovi e fosse vietato costruire un sacco di potenza di calcolo, in teoria si potrebbero studiare i loro meccanismi interni e capire qualcosa su come funziona l'intelligenza, cose che potrebbero aiutare a inventare algoritmi più efficienti in grado di aggirare i tentativi di monitoraggio.

### Perché vietare la ricerca? Sembra una misura estrema. {#perché-vietare-la-ricerca?-sembra-una-misura-estrema.}

#### **Altre scoperte potrebbero rendere praticamente impossibile impedire alle persone di creare la superintelligenza.** {#altre-scoperte-potrebbero-rendere-praticamente-impossibile-impedire-alle-persone-di-creare-la-superintelligenza.}

Nel libro abbiamo raccontato come un singolo articolo pubblicato nel 2017 abbia dato il via all'intera rivoluzione LLM descrivendo un algoritmo che ha reso pratico addestrare IA utili su hardware commerciale specializzato.

Se un giorno fosse possibile addestrare potenti IA su hardware *consumer* ampiamente disponibile, le misure per prevenire la superintelligenza dovrebbero diventare più onerose e fallirebbero più rapidamente.

Ecco perché anche la ricerca su algoritmi di IA ancora più potenti ed efficienti è un vero e proprio veleno per l'umanità.

È una brutta notizia, e non è quello che vorremmo fosse vero. Ma sembra proprio che sia così.

Nessuna legge può impedire agli attuali scienziati dell'IA di pensare a algoritmi più efficienti nella privacy delle loro menti. Forse qualcuno potrebbe creare una rete clandestina per condividere i risultati delle ricerche. Alcuni nel settore dell'IA già dicono con orgoglio che l'umanità dovrebbe sparire per far posto all'IA; potrebbero fare di tutto per andare avanti, non importa cosa ne pensino gli altri.

Ma la ricerca sull'intelligenza artificiale rallenterebbe *molto* se fosse illegale, e ancora di più se fosse ampiamente compreso che si tratta davvero di un tipo di ricerca che potrebbe ucciderci tutti. Rallenterebbe enormemente se reti clandestine di questo tipo fossero rintracciate e fermate con la stessa convinzione usata per fermare chi cerca di arricchire l'uranio nel proprio garage, perché i pericoli del mondo reale sono presi sul serio.

La maggior parte delle persone non cerca di fare cose super illegali che farebbero arrabbiare davvero le forze dell'ordine e le agenzie di intelligence internazionali. Rendere illegale la pubblicazione di nuovi algoritmi di IA intelligenti scoraggerebbe forse il 99,9 % delle persone e quasi tutte le aziende, e poi il restante 0,1 % potrebbe essere gestito dalla polizia e dalle agenzie di intelligence locali, nazionali e internazionali, e non otterrebbe neanche lontanamente l'attuale livello di finanziamento.

Sarebbe un mondo molto diverso da quello attuale, dove è del tutto legale condurre gli esperimenti di scienza folle più pericolosi della storia e le grandi aziende investono miliardi di dollari in questo campo.

Non sappiamo quante altre scoperte ci vorranno prima che le IA siano abbastanza intelligenti da fare ricerca sull'IA e costruire IA ancora più intelligenti. Potrebbe bastare una sola scoperta. Potrebbero volercene cinque. Ma algoritmi migliori sono letali quanto hardware migliore. Sono come due cavalli che trainano lo stesso carro verso un precipizio.

### Si può davvero fermare una tecnologia? {#can-a-technology-really-be-stopped?}

#### **\* Molte tecnologie sono vietate o fortemente regolamentate.** {#*-molte-tecnologie-sono-vietate-o-fortemente-regolamentate.}

La fissione nucleare è il classico esempio di tecnologia regolamentata. Le aziende private non possono arricchire l'uranio senza il controllo del governo, per quanto utile possa essere l'energia a basso costo.

In realtà, l'umanità è piuttosto brava a regolare e rallentare ogni tipo di tecnologia. Gli Stati Uniti regolano rigorosamente [i nuovi farmaci e dispositivi medici](https://www.fda.gov/), [l'edilizia abitativa](https://www.hud.gov/hud-partners/laws-regulations), [la produzione di energia nucleare](https://www.nrc.gov/about-nrc.html), [i programmi televisivi e radiofonici](https://www.fcc.gov/media/radio/public-and-broadcasting), [le pratiche contabili](https://www.fasb.org/standards), [l'assistenza all'infanzia](https://childcare.gov/consumer-education/regulated-child-care), [la disinfestazione](https://npic.orst.edu/reg/laws.html), [l'agricoltura](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) e un sacco di altri settori. Ogni singolo stato richiede un esame di abilitazione per [parrucchieri](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) e [manicure](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). La maggior parte di essi richiede un esame anche per i [massaggiatori](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

Noi pensiamo che, in molti casi, l'umanità regoli troppo la tecnologia. Ad esempio, ci sembra che la Food and Drug Administration statunitense stia uccidendo molte più persone ([rallentando o impedendo la creazione di farmaci salvavita](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), con requisiti pesanti) di quante ne sta salvando (impedendo il rilascio di farmaci pericolosi). Ci sembra che il prezzo delle case sia troppo alto, in parte a causa delle restrizioni legali sulla zonizzazione che limitano cosa si può costruire e dove. Ci sembra che gli Stati Uniti abbiano praticamente distrutto la propria industria nucleare con regole troppo rigide. E sul serio, i *parrucchieri*?

L'umanità ha *assolutamente* la capacità di ostacolare il progresso tecnologico. Sarebbe davvero tragico e assurdo se usassimo questa capacità nella medicina, nell'edilizia e nell'energia, e trascurassimo di usarla su una delle rare tecnologie che, se realizzata, ci ucciderebbe tutti.

#### **Un divieto può essere mirato in modo specifico.** {#un-divieto-può-essere-mirato-in-modo-specifico.}

Un divieto sulla ricerca e sviluppo dell'intelligenza artificiale avanzata non deve necessariamente influenzare la gente comune. Non deve nemmeno togliere i chatbot moderni o chiudere l'industria delle auto a guida autonoma.

La maggior parte delle persone non compra decine di GPU AI di fascia alta per metterle nel proprio garage. La maggior parte delle persone non gestisce enormi datacenter. La maggior parte delle persone non sentirebbe nemmeno gli effetti di un divieto sulla ricerca e lo sviluppo dell'IA. Semplicemente, ChatGPT non cambierebbe così spesso.

L'umanità non avrebbe nemmeno bisogno di smettere di usare tutti gli attuali strumenti di IA. ChatGPT non dovrebbe scomparire; potremmo continuare a capire come integrarlo nella nostra vita e nella nostra economia. Sarebbe comunque un cambiamento maggiore di quello che il mondo ha visto per generazioni. Ci perderemmo i *nuovi* sviluppi dell'IA (del tipo che arriverebbero quando l'IA diventa più intelligente ma non ancora abbastanza intelligente da uccidere tutti), ma la società non chiede a gran voce questi sviluppi.

E potremmo vivere. Potremmo vedere i nostri figli vivere.

Gli sviluppi che la gente *chiede* a gran voce, come lo sviluppo di nuove tecnologie mediche salvavita, sembrano possibili da perseguire *senza* perseguire anche la superintelligenza. Siamo a favore di deroghe per l'IA medica, purché funzionino con un'adeguata supervisione ed evitino la generalità pericolosa.

I governi che lavorano per evitare la creazione di una superintelligenza canaglia dovrebbero assicurarsi che i chip AI non vengano usati per sviluppare IA più capaci. Pertanto, la questione di quali attività e servizi di IA sarebbero autorizzati a continuare dipenderebbe da quali [meccanismi di verifica](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) potrebbero essere utilizzati per assicurarsi che non avvenga lo sviluppo di IA pericolose. Meccanismi di verifica migliori potrebbero ridurre il costo dell'arresto dello sviluppo dell'IA permettendo a un insieme più ampio di attività di continuare.

Un altro passo che potrebbe potenzialmente aiutare marginalmente è installare kill-switch nei chip AI, e stabilire protocolli di monitoraggio e spegnimento di emergenza per qualsiasi grande datacenter in uso.[^225] I reattori nucleari sono progettati in modo da poter essere spenti rapidamente in caso di emergenza. Se si è d'accordo che la superintelligenza pone una minaccia a livello di estinzione, allora sembra ovvio che i chip AI e i datacenter dovrebbero essere progettati per rendere facile ai regolatori spegnerli rapidamente.

Il punto non è bruciare tutta la tecnologia perché odiamo la tecnologia.[^226] Il punto è evitare di proseguire lungo la strada che finisce con l'estinzione umana.

#### **Una gran parte del problema è che le persone non capiscono la minaccia incombente della superintelligenza artificiale.** {#una-gran-parte-del-problema-è-che-le-persone-non-capiscono-la-minaccia-incombente-della-superintelligenza-artificiale.}

Secondo la nostra esperienza, le persone che sostengono che l'umanità non può fermare la corsa alla superintelligenza semplicemente non riescono a capire il punto che, se qualcuno la costruisce, tutti muoiono.

"Ma l'IA offre grandi benefici!" — no, in realtà; non puoi fare uso di alcun potere della superintelligenza se questa semplicemente uccide tutti. Se l'umanità vuole cogliere i benefici offerti dalla superintelligenza, allora l'umanità deve trovare un modo per navigare la transizione alla superintelligenza che non uccida tutti come effetto collaterale.

"Ma le centrali nucleari fanno paura perché sono associate alle bombe atomiche che hanno raso al suolo intere città, mentre l'IA è associata a strumenti innocui come ChatGPT!" - È vero, almeno per ora. Se l'umanità non riuscisse mai a capire che la superintelligenza artificiale costruita con metodi anche solo vagamente simili a quelli moderni ucciderebbe semplicemente tutti, allora potrebbe non riuscire a fermarla. Ma l'ostacolo non è che l'umanità non riesca mai a controllare o ostacolare le tecnologie emergenti (come le armi nucleari o l'energia nucleare); l'ostacolo è che *le persone non capiscono la minaccia*.

Da qui questo libro. Come discutiamo nell'ultimo capitolo, l'umanità è capace di molto quando abbastanza persone capiscono la natura del problema.

### Ma questo non significa dare troppo potere ai governi? {#non-significa-dare-troppo-potere-ai-governi?}

#### **I governi hanno già il potere di vietare le tecnologie pericolose.** {#i-governi-hanno-già-il-potere-di-vietare-le-tecnologie-pericolose.}

Vietare la ricerca sull'intelligenza artificiale che punta a creare un'IA più intelligente dell'uomo non cambierebbe molto in termini di potere dello Stato. I governi legiferano e regolano un'enormità di cose. Limitare un singolo programma di ricerca potrebbe essere un grosso problema *per l'industria dell'IA*, ma è una goccia nell'oceano *per i governi* e la società, che sono abituati al coinvolgimento dello Stato in molti aspetti della vita e che hanno già vietato tecnologie pericolose come le armi chimiche.[^227]

Vietare un'altra tecnologia non farà precipitare il mondo nel totalitarismo, così come i trattati sulle armi nucleari non hanno portato al totalitarismo.

Con questo non vogliamo dire che vietare una tecnologia *non sia importante*. Non pensiamo che l'asticella per l'intervento dello Stato debba essere *bassa*. Al contrario, pensiamo che la superintelligenza superi facilmente qualsiasi asticella ragionevole.

Se l'umanità decidesse oggi di fermare la ricerca e lo sviluppo dell'IA, il divieto non dovrebbe essere particolarmente invasivo. Oggi, creare un'IA all'avanguardia richiede un numero straordinario di chip per computer altamente specializzati che consumano enormi quantità di energia elettrica.

Forse tra dieci anni sarà possibile fare sviluppo significativo di IA su un laptop consumer, *se* l'umanità permetterà ulteriori miglioramenti ai chip per computer e ulteriori ricerche sugli algoritmi di intelligenza artificiale. Ma l'umanità non ha bisogno di permettere che questo accada. I governi che limitano la ricerca e lo sviluppo dell'IA non devono essere più invasivi nella vita della persona media rispetto ai governi che controllano la diffusione della tecnologia delle armi nucleari — a patto che il mondo si renda conto della situazione in cui ci troviamo e fermi tutto *adesso*.

### Alcune nazioni non rifiuterebbero il divieto? {#alcune-nazioni-rifiuterebbero-il-divieto?}

#### **\* Non se capiscono la minaccia.** {#*-non-se-capiscono-la-minaccia.}

Stiamo parlando di una tecnologia che ucciderebbe tutti sul pianeta. Se un paese capisse davvero il problema e quanto lontano sia qualsiasi gruppo sul pianeta dal far sì che l'IA segua le intenzioni di chi la controlla anche dopo essere diventata una superintelligenza, allora non ci sarebbe alcun motivo per loro di affrettarsi. Anche loro vorrebbero disperatamente firmare un trattato e aiutare a farlo rispettare, per paura della propria vita.

Persino nazioni come la Corea del Nord, che hanno violato il diritto internazionale per sviluppare le proprie armi nucleari, non hanno *usato* quelle armi contro i propri nemici, perché capiscono che non ci sono vincitori in un olocausto nucleare. Le nazioni e i loro leader a volte si impegnano in politiche sull'orlo del baratro o in guerre, ma non perseguono attivamente la propria distruzione.

Chi pensa che qualche nazione straniera possa ritirarsi dal trattato, secondo noi, immagina una nazione i cui leader semplicemente *non capiscono la minaccia.* Pensiamo che immaginino uno scenario in cui l'intelligenza artificiale ha il 95 % di possibilità di conferire grande ricchezza e potere al suo creatore e il 5 % di possibilità di uccidere tutti. In tal caso, certo, qualche Stato potrebbe essere abbastanza avventato da provarci. E forse qualche Stato *crederà* che le probabilità siano proprio quelle.

Pensiamo che questa situazione non sia quella che la teoria e le prove suggeriscono. Come abbiamo ampiamente discusso nel libro, la teoria e le prove suggeriscono tutte che questa tecnologia sarebbe semplicemente un suicidio globale. Nessuno è neanche lontanamente in grado di sfruttare la superintelligenza delle macchine per ottenere benefici. Se la maggior parte del mondo lo capisse, ci sarebbero molte meno ragioni per cui le nazioni canaglia violerebbero un trattato. Neanche loro vogliono morire.

E anche se qualche ipotetica nazione canaglia avesse un leader che davvero non capisce la minaccia rappresentata dall'ASI, se quella nazione fosse circondata da un'alleanza internazionale di potenze mondiali che invece comprendono la minaccia, le potenze interessate potrebbero intervenire e cambiare il panorama degli incentivi per la potenza canaglia.

Se (per esempio) i leader di Stati Uniti, Cina, Russia, Germania, Giappone e Regno Unito credessero davvero che la *loro stessa sopravvivenza* dipenda dal fatto che nessuno costruisca una superintelligenza, e fossero cristallini nel comunicare che considererebbero qualsiasi tentativo di costruire una superintelligenza come una minaccia alla loro vita e al loro sostentamento e che sarebbero pronti a reagire per difendersi, allora... beh, anche un leader mondiale che non è d'accordo probabilmente non vorrebbe sfidare quella coalizione.

Lo sviluppo dell'IA non è una corsa al grande dominio militare; è una corsa al suicidio. Pensiamo che se i leader mondiali lo capissero — se si aspettassero che loro stessi e i loro figli moriranno per questo — allora si attenerebbero sinceramente a un trattato e contribuirebbero sinceramente a farlo rispettare.

In realtà non è poi così difficile capire che creare macchine più intelligenti di tutta l'umanità messa insieme potrebbe portare il mondo sull'orlo del baratro. Non è poi così difficile capire quanto poco l'umanità capisca delle macchine intelligenti che stiamo costruendo, se ci si ferma un attimo a riflettere seriamente sulla questione. Pensiamo che la questione sia *se* i leader mondiali arriveranno a credere a questi fatti. Ma se *lo faranno*, non pensiamo che sia davvero irrealistico fermare questa corsa suicida.

#### **Un trattato richiederebbe un monitoraggio e un'applicazione reali.** {#un-trattato-richiederebbe-un-monitoraggio-e-un-applicazione-reali.}

Anche se la *maggior parte* delle nazioni capisse che se qualcuno lo costruisce, tutti muoiono, alcune nazioni potrebbero non capirlo e potrebbero essere così spericolate da procedere comunque con la costruzione della superintelligenza delle macchine.

Il monitoraggio è necessario. L'applicazione delle norme è necessaria. I trattati sulle armi nucleari, biologiche e chimiche forniscono alcuni precedenti sui modi per verificare la conformità. Possiamo e dobbiamo rendere difficili e costosi i tentativi di eludere tali trattati.

Un divieto internazionale sull'IA di frontiera dovrà essere applicato rigorosamente. Se uno Stato-nazione è determinato a procedere nonostante la pressione internazionale, potrebbe essere necessario l'uso della forza militare da parte delle nazioni firmatarie.

Questo non è l'ideale! Si dovrebbe fare ogni sforzo per chiarire che la forza *verrebbe* usata in tali situazioni, così da evitare errori di calcolo in cui la forza debba essere usata *nella realtà*. Ma se esiste una causa che possa giustificare un'azione militare limitata — o persino una guerra, se una nazione non conforme sceglie di intensificare il conflitto — salvare la razza umana dovrebbe qualificarsi.

#### **Questo metodo ha funzionato in passato.** {#questo-metodo-ha-funzionato-in-passato.}

Sono passati più di ottant'anni dallo sviluppo della bomba atomica e l'umanità ha fatto un ottimo lavoro nel gestire la proliferazione nucleare. Non c'è stata nessuna guerra nucleare su larga scala, contrariamente a quanto molti esperti avevano previsto all'indomani della Seconda guerra mondiale.

Nel giugno del 2025, il governo degli Stati Uniti ha persino effettuato un attacco limitato contro l'Iran nel tentativo di compromettere la sua capacità di creare armi nucleari. Questo tipo di trattato e regime di applicazione ha dei precedenti nell'ordine mondiale.

Se potessimo guadagnare ottant'anni prima dello sviluppo dell'ASI, potrebbe essere sufficiente.

### Un sistema di monitoraggio può durare per sempre? {#can-a-monitoring-regime-last-forever?}

#### **No. Sarà necessaria un'altra via d'uscita.** {#no.-sarà-necessaria-un'altra-via-d'uscita.}

I progressi nella ricerca sull'IA probabilmente non possono essere fermati completamente. Con *abbastanza* tempo, i ricercatori probabilmente scoprirebbero alla fine metodi molto più efficienti per creare IA.[^228] O forse, con abbastanza tempo, qualche attore disonesto potrebbe alla fine riuscire a sovvertire il divieto.

Il tempo trascinerà probabilmente l'umanità verso il futuro in un modo o nell'altro. E l'umanità si estinguerà — come la maggior parte delle specie prima di lei — oppure riuscirà in qualche modo a navigare la transizione verso un mondo in cui esistono cose più intelligenti.

Ma l'umanità non *ha bisogno* di guadagnare tempo per sempre. L'IA non è l'unica tecnologia in progresso. Anche la biotecnologia sta iniziando a maturare e, se l'umanità riuscirà a impedire lo sviluppo di macchine superintelligenti per diversi decenni, dovrà fare i conti con sconvolgimenti come l'ingegneria genetica che porterà alla creazione di esseri umani significativamente più intelligenti.

La domanda è quanto tempo possiamo guadagnare e cosa possiamo fare con quel tempo.

Il problema fondamentale che l'umanità deve affrontare è quello di attraversare in modo sicuro il divario dall'intelligenza umana alla superintelligenza. Il miglior piano che ci viene in mente e che *forse* ha qualche possibilità di funzionare nella vita reale è quello di guadagnare tempo affinché la biotecnologia aumenti considerevolmente l'intelligenza umana — al punto che i futuri ricercatori umani diventino *così* intelligenti da non stimare mai (ad esempio) che un progetto ingegneristico finirà in tempo e nel rispetto del budget, a meno che non sia *effettivamente* così.

Così intelligenti da non aderire mai a una teoria scientifica come l'aristotelismo o l'eliocentrismo, anche se la società che li circonda ne fosse completamente convinta. Così intelligenti da avere una possibilità di navigare il [divario tra Prima e Dopo](#a-closer-look-at-before-and-after) al primissimo tentativo.

Ci sono altri possibili percorsi che potremmo immaginare, ma questo ha il vantaggio di affrontare il collo di bottiglia chiave ("la comunità scientifica esistente dipende troppo dai metodi per tentativi ed errori e dall'incrementalismo per gestire questo particolare problema"), utilizzando tecnologie che stanno già iniziando a essere disponibili oggi, senza rappresentare un serio rischio per il mondo.

#### **Un sistema di monitoraggio *non dovrebbe* durare per sempre.** {#un-sistema-di-monitoraggio-non-dovrebbe-durare-per-sempre.}

È *teoricamente* possibile per l'umanità rimanere in equilibrio sul filo del rasoio della competenza, dove ci troviamo attualmente, per sempre. La nostra ipotesi è che questo richiederebbe un controllo draconiano dei pensieri e delle attività delle persone. Ma anche se non fosse così, lo considereremmo una scelta sbagliata.

Noi, personalmente, pensiamo che i discendenti dell'umanità meritino di diventare ciò che desiderano essere, esplorare le stelle e costruire lì una civiltà fiorente e bella. Sosteniamo il divieto dello sviluppo dell'IA di frontiera perché riteniamo che la superintelligenza sia abbastanza pericolosa da renderlo necessario — non perché odiamo l'IA, la tecnologia o il progresso scientifico.

La vera domanda è *come* arrivare a un futuro meraviglioso e come gestire la transizione da qui a lì.

Vale la pena sottolinearlo, anche perché ci sono molte persone che presentano l'IA come una falsa dicotomia: dicono (a torto) che la società deve o accettare i rischi dell'IA e andare avanti a tutta velocità, o rifiutare l'IA e lasciare che la nostra civiltà svanisca per sempre su un unico pianeta. Questo è [semplicemente sbagliato](#rushing-ahead-destroys-those-benefits.). Ci sono altre strade verso il futuro, strade che permettono un futuro altrettanto brillante, ma senza un rischio così alto di perdere tutto per niente. L'umanità dovrebbe trovare un'altra strada verso il futuro.

### Perché rendere gli esseri umani più intelligenti sarebbe utile? {#why-would-making-humans-smarter-help?}

#### **\* Potrebbe aiutare a risolvere il problema dell'allineamento.** {#*-it-could-help-with-solving-the-alignment-problem.}

Il problema dell'allineamento dell'IA non ci sembra fondamentalmente irrisolvibile. Ci sembra solo che gli esseri umani non siano neanche lontanamente vicini a risolverlo e che non abbiano raggiunto il livello di intelligenza in cui *pensare* di avere una soluzione correli fortemente con l'*avere* effettivamente una soluzione.

I ricercatori di IA spesso riconoscono che il problema dell'allineamento sembra incredibilmente difficile e che finora sono stati fatti pochissimi progressi. Ecco perché l'idea "forse possiamo chiedere alle IA di fare il lavoro di allineamento al posto nostro" è così allettante: quando sei un ricercatore di IA e senti che tu e i tuoi colleghi non siete all'altezza di risolvere un certo problema, la cosa più ovvia da fare è rivolgersi all'IA.

Ma, come diciamo nel capitolo 11 e nella [risorsa online](#more-on-making-ais-solve-the-problem) associata, è chiaro anche dal punto di vista di un profano che questa idea ha molti problemi: perché un'IA capisca come risolvere un problema profondo con cui anche i migliori ricercatori umani hanno grandi difficoltà, deve essere abbastanza intelligente da essere pericolosa. E dato che *noi* abbiamo ben poca idea di cosa stiamo facendo, non abbiamo alcuna fonte di verità fondamentale che possiamo utilizzare per addestrare direttamente capacità di allineamento limitate, né alcun modo per verificare se una proposta di allineamento generata dall'IA sia sicura o efficace.

Il mondo può presentarci problemi che sono legittimamente fuori dalla nostra portata. La natura non è un gioco che offre all'umanità solo sfide "eque"; a volte possiamo imbatterci in problemi troppo difficili da risolvere anche per i migliori scienziati umani, o troppo difficili da risolvere nei tempi richiesti.

C'è un modo più realistico per passare l'intero problema a qualche entità più intelligente? Un'opzione potrebbe essere quella di rendere gli *esseri umani* più intelligenti in modo che possano legittimamente essere in grado di risolvere il problema dell'allineamento. Gli esseri umani sono "pre-allineati" in un modo che le IA non sono; le persone più intelligenti hanno le stesse motivazioni prosociali di base che abbiamo tutti.

In linea di principio, sembra possibile che le persone siano in grado di distinguere tra ciò che *sembra* una grande illuminazione alchemica che permetterà loro di trasformare il piombo in oro e il tipo di conoscenza che *corrisponde effettivamente* alla capacità di trasformare il piombo in oro (usando la fisica nucleare per rimuovere alcuni neutroni dagli atomi di piombo). Dovrebbero sicuramente sembrare stati di conoscenza diversi.

Ma gli ingegneri umani reali hanno molte difficoltà a capire in quale zona si trovano. Nella storia effettiva della chimica, il livello di abilità dell'umanità era tale che gli alchimisti venivano sistematicamente ingannati.

Nel mondo reale, gli scienziati si affezionano alle loro teorie preferite e si rifiutano di rivedere le proprie opinioni finché la realtà non li martella ripetutamente con "la tua teoria era sbagliata" — e a volte si rifiutano di cambiare idea anche allora: si dice talvolta che la scienza progredisca "[un funerale alla volta](https://en.wikipedia.org/wiki/Planck%27s_principle)", perché la vecchia guardia non cambierà mai le proprie opinioni e bisogna semplicemente aspettare che la nuova guardia maturi. Ma questo non è un vincolo fondamentale imposto dalla natura; è solo una questione di esseri umani come classe che sono insufficientemente accorti, attenti e consapevoli di sé.

Di solito, va bene che gli esseri umani siano ingenui in questi modi, perché di solito la realtà è abbastanza indulgente con gli errori, almeno nel senso che non spazza via *tutta l'umanità* per l'arroganza di un alchimista. Ma [questo non è un lusso che l'umanità ha](#a-closer-look-at-before-and-after) quando si tratta di allineare la superintelligenza delle macchine.

L'umanità spesso acquisisce la propria conoscenza lottando, provando, fallendo e accumulando lentamente conoscenza. Ma non *deve* essere così.

Einstein non solo è stato in grado di capire la relatività generale; è stato in grado di capirla *riflettendo intensamente sul problema*, anche prima che l'umanità mettesse in orbita i satelliti e iniziasse a vedere con i propri occhi le discrepanze nei loro orologi (come discusso nel Capitolo 6). Aveva prove empiriche, ma è stato in grado di individuare efficacemente la risposta giusta in risposta ai primi sussurri sommessi dai dati empirici, piuttosto che aver bisogno che la verità venisse a bussare alla sua porta.

Quel percorso è più raro e più difficile da percorrere, ma quel tipo di genio scientifico esiste — anche se raramente, persino tra i migliori e i più brillanti del mondo.

Gli esseri umani potenziati di uno o due passi oltre il livello di ricercatori come Einstein o [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) potrebbero iniziare a capire accuratamente i propri difetti, e correggerli, in dozzine di modi diversi.

Potrebbero notare quando stanno razionalizzando o cadendo vittime del [pregiudizio di conferma](https://en.wikipedia.org/wiki/Confirmation_bias). Potrebbero andare oltre il punto di aspettarsi mai che un'idea che suona brillante funzioni quando in realtà non funziona — al punto che ogni volta che si aspettano di avere successo, *hanno* successo. Potrebbero raggiungere un livello di competenza in cui commettono ancora molti errori, ma non sono [sistematicamente](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) troppo sicuri (o troppo insicuri) in nuovi domini complessi.

Il potenziamento dell'intelligenza umana è davvero una possibilità? A noi sembra di sì, avendo parlato con diversi ricercatori biotecnologici che pensano che ci siano angoli di attacco promettenti a breve termine. Anche l'IA attentamente mirata e focalizzata sulle biotecnologie potrebbe aiutare ad accelerare il lavoro. Ma dal nostro punto di vista, rimane molto incerto se un piano come questo si realizzerebbe realisticamente. Quello che ci sentiamo più sicuri nel dire è che si tratta di un'opzione con alta leva che merita molti più investimenti ed esplorazioni di quelli che sta ricevendo attualmente.

Non stiamo raccomandando il potenziamento dell'intelligenza umana come l'unica strategia post-spegnimento-dell'IA in cui pensiamo che l'umanità dovrebbe investire pesantemente. Piuttosto, questo è solo uno dei tanti esempi, e quello che attualmente pensiamo sia il più promettente. Raccomandiamo vivamente che l'umanità esamini molteplici possibili percorsi non-IA per andare avanti, piuttosto che mettere tutte le uova in un solo paniere.

#### **Gli esseri umani potenziati non rappresentano un grave problema di "allineamento umano".** {#gli-esseri-umani-potenziati-non-rappresentano-un-grave-problema-di-"allineamento-umano".}

Gli esseri umani potenziati avrebbero essenzialmente la stessa struttura cerebrale, le stesse emozioni, ecc. del resto di noi. Con l'IA, anche quella addestrata a [*sembrare come*](https://docs.google.com/document/d/1FbehxsMqTaw2seAzUJw4ny0tvZgUdbHJnCUi_Pbd5VU/edit?tab=t.k1kf1fy9gx5i#heading=h.8cxst4c4x63a) noi, c'è un enorme divario in termini di differenze cognitive e motivazionali, e un divario altrettanto grande in termini di comprensibilità; con esseri umani leggermente più intelligenti, nulla di tutto ciò sembra particolarmente probabile.

I ricercatori con potenziamento cognitivo non avrebbero bisogno di mantenere la propria integrità mentale mentre si trasformano in vaste superintelligenze con menti milioni di volte più grandi. Dovrebbero solo essere portati al livello necessario per capire come *costruire* — non far crescere — superintelligenze artificiali che siano davvero allineate e stabili.

Potrebbe comunque esserci un problema di "allineamento umano" in senso lato, nel senso che qualsiasi sforzo di coordinare più persone può incontrare problemi principale-agente e problemi di incentivi. E questi problemi sono intrinsecamente molto più importanti per qualsiasi gruppo incaricato di creare una superintelligenza.

Pensiamo che questi problemi siano trattabili se gli esseri umani iniziano in modo visibilmente altruistico e generoso, se la loro intelligenza viene potenziata solo lentamente e se lavorano in un'istituzione ben organizzata con incentivi ben pensati. Ma è del tutto normale che la gente si preoccupi della possibilità che qualcuno cerchi di prendere il controllo. Risolvere questi problemi non sarebbe necessariamente facile, ma non sarebbe così irrealizzabile come le aziende che cercano di sviluppare superintelligenze imperscrutabili con menti del tutto incomprensibili e pulsioni disumane.

Creare una squadra d'élite di supergeni geneticamente modificati per aiutare il pianeta a navigare in sicurezza attraverso la transizione verso la superintelligenza è sicuramente il tipo di cosa che l'umanità dovrebbe fare con attenzione, vista la posta in gioco di un'impresa del genere. Una mossa del genere comporta varie questioni pratiche ed etiche, ma queste devono essere valutate rispetto al costo di lasciare che la superintelligenza ci uccida tutti, se non ci sono altre soluzioni altrettanto promettenti.

I tempi drastici possono richiedere misure drastiche, ma il (modesto) potenziamento dell'intelligenza umana non è nemmeno una misura che sembra particolarmente drastica. Sembra una tecnologia tutto sommato positiva di per sé, che ha almeno qualche possibilità di aiutare l'umanità in più di un modo.
