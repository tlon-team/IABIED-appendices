### L'intelligenza artificiale non dovrebbe diventare una vera e propria civiltà prima di poter essere pericolosa? {#l-intelligenza-artificiale-non-dovrebbe-diventare-una-vera-e-propria-civiltà-prima-di-poter-essere-pericolosa?}

#### **Con i computer, la parte difficile è far loro risolvere un certo problema. Quantità e velocità elevati arrivano subito dopo.** {#con-i-computer,-la-parte-difficile-è-far-loro-risolvere-un-certo-problema.-quantità-e-velocità-elevati-arrivano-subito-dopo.}

"Per conquistare il mondo, è necessaria una civiltà" è un'intuizione che ha senso per gli umani. È molto meno ovvio quanto questa idea possa applicarsi all'IA. Le IA non funzionano come gli umani: possono essere enormemente più potenti di qualsiasi essere umano e un'istanza di IA non è necessariamente paragonabile a una singola persona.

Vale anche la pena ricordare che la superintelligenza è proprio il tipo di cosa che può arrivare ad avere l'equivalente di un’intera civiltà in tempi estremamente brevi.

Con la maggior parte dei compiti che i computer possono svolgere, non ci vuole molto per passare da "i computer possono farlo" a "i computer possono farlo su scala enorme, molto più velocemente di qualsiasi essere umano". Si pensi, per esempio, alle calcolatrici.

Ci sono stati anni in cui solo i computer di fascia più alta potevano fare il riconoscimento vocale, l'elaborazione video o la grafica 3D in tempo reale, ma non ci sono stati *molti* anni di questo tipo.

Le IA, come i software tradizionali, possono essere rapidamente copiate su tutti i computer disponibili. Ed è possibile costruire altri computer alla velocità dell'industria.

Confrontiamo questa situazione con gli umani. Creare e addestrare un nuovo essere umano richiede risorse sostanziali e decenni di tempo. Una volta che si ha una singola IA a un dato livello di potenza, si può immediatamente copiare quella stessa IA addestrata, "adulta", quante volte si vuole, con una spesa minima.

In un certo senso, l'equivalente di un'intera (piccola) civiltà di menti IA esiste già nel momento in cui un'azienda rilascia un nuovo modello nei propri data center e avvia tutte le istanze necessarie per soddisfare la domanda.[^175] Oggi, queste flotte di IA non lavorano tutte in armonia. Ma le aziende usano [gruppi di agenti paralleli](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&t=348) quando puntano alle massime prestazioni a qualsiasi prezzo.

Tutto questo significa che probabilmente non ci sarà molto tempo tra quando le IA diventeranno abbastanza intelligenti da poter prendere il sopravvento se avessero un milione di istanze e quando le IA avranno almeno quel numero di istanze in esecuzione. Il tipo di crescita demografica che richiede centinaia di anni agli esseri umani può verificarsi in minuti con l'IA.

Per quanto riguarda le infrastrutture fisiche della civiltà, riteniamo che l'IA possa sfruttare produttivamente le infrastrutture umane per tutto il tempo necessario a sviluppare mezzi più avanzati per riorganizzare la materia secondo le sue preferenze. Non ha bisogno di capire come fabbricare da zero una sua catena di approvvigionamento e infrastruttura di calcolo quando può usare i nostri computer. Non ha bisogno di inventare macchine industriali da zero quando può semplicemente prendere il controllo di quelle che abbiamo già costruito in modo provvidenziale. E può usare le nostre infrastrutture per costruire la fase successiva delle sue, utilizzando i robot esistenti per costruire nuove e più efficienti fabbriche di robot, o utilizzando i laboratori di sintesi del DNA esistenti per creare la sua biotecnologia, fino a diventare completamente autosufficiente.

Gli esseri umani sono il tipo di entità che, partendo in pochi e nudi nella savana, sono riusciti con le proprie forze a costruire una civiltà tecnologica. E non siamo nemmeno *così* intelligenti. Non sarebbe un'impresa tanto difficile da replicare per una superintelligenza, soprattutto se potesse partire dalla base industriale esistente dell'umanità come trampolino di lancio.

### Le IA non saranno limitate dalla loro capacità di progettare e condurre esperimenti? {#le-ia-non-saranno-limitate-dalla-loro-capacità-di-progettare-e-condurre-esperimenti?}

#### **L'intelligenza permette di imparare di più dagli esperimenti e di condurre esperimenti più veloci, più informativi e più parallelizzati.** {#l-intelligenza-permette-di-imparare-di-più-dagli-esperimenti-e-di-condurre-esperimenti-più-veloci,-più-informativi-e-più-parallelizzati.}

Una civiltà di menti motivate che pensano mille volte più velocemente dell'umanità non sarebbe necessariamente in grado di produrre output tecnologici mille volte più velocemente degli esseri umani.

Per analogia: se impiegate tre ore per fare la spesa, e due di queste ore vengono trascorse ad andare e tornare dal negozio di alimentari a cavallo, allora un'auto dieci volte più veloce del cavallo può accelerare il vostro viaggio per fare la spesa, ma non di un fattore dieci. Alla fine, l'ultima ora trascorsa nel negozio diventa dominante nel tempo totale impiegato.

Anche una civiltà composta da menti incredibilmente intelligenti deve, ogni tanto, aspettare che arrivino i risultati degli esperimenti. Se i vostri pensieri sono sufficientemente veloci, allora il collo di bottiglia diventerà probabilmente quanto velocemente potete agire nel mondo, quanto velocemente potete acquisire informazioni e quanto tempo impiegano i vostri piani a realizzarsi.

Ma la situazione non è così negativa come l'analogia del negozio di alimentari potrebbe far credere, perché la capacità di pensare si bilancia con la necessità di risultati sperimentali:

* Spesso si può semplicemente pensare di più, pensare meglio, ed eliminare la necessità di un test, perché ci si rende conto che le osservazioni precedenti contengono già la risposta. Si confronti la capacità delle moderne IA di [imparare a pilotare robot](https://arxiv.org/abs/1905.00741) utilizzando [pura simulazione](https://www.figure.ai/news/reinforcement-learning-walking).  
* A volte si può riflettere più a fondo fino a trovare un test altrettanto affidabile ma più veloce.  
* A volte si possono eseguire molti test più veloci ma meno affidabili che possono essere eseguiti più volte in parallelo per ottenere risultati altrettanto affidabili a una velocità superiore.  
* A volte si possono eseguire molti test complessi contemporaneamente, in modo che i dati risultino complessi e difficili da interpretare — il che rappresenta un buon compromesso se lo sforzo cognitivo necessario per decifrare i risultati è meno costoso (dal punto di vista di una mente estremamente veloce) rispetto all'esecuzione di test multipli.  
* A volte si può trovare un modo per costruire altri dispositivi che eseguono gli esperimenti molto più velocemente. Ad esempio, invece di inviare molte richieste diverse a un laboratorio biologico per far sintetizzare farmaci, non si può trovare un modo per inviare *una sola* richiesta a un laboratorio biologico, che risulterà nella sintesi di un *singolo batterio* contenente il codice genetico per produrre tutti i farmaci che si desidera sintetizzare? Allo stesso modo, non si può creare un batterio sensibile ai segnali radio che risponda rapidamente alle istruzioni di un'IA veloce — molto più rapidamente degli umani terribilmente lenti che corrono avanti e indietro seguendo delle istruzioni?  
* E a volte si può semplicemente prendere le proprie dieci ipotesi migliori, capire cosa si farebbe in ciascuno di quei casi, costruire un dispositivo complesso che funzionerà indipendentemente da come si rivelerà essere la realtà e saltare completamente i test.

Una civiltà piena di copie di Steve Jobs, Marie Curie, John von Neumann e alcuni dei più grandi lavoratori e programmatori del mondo (che funzionassero a una velocità 10.000 volte superiore alla nostra), *noterebbe* che il principale ostacolo è l'attesa dei risultati sperimentali e potrebbe *lavorare su tale ostacolo* per ridurlo.

La storia del [Progetto Genoma Umano](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) è un buon esempio di cosa succede quando esseri umani intelligenti notano continuamente i colli di bottiglia in un enorme progetto di ricerca e lavorano per risolverli. Quello che si prevedeva avrebbe richiesto quindici anni e 3 miliardi di dollari è stato completato con due anni di anticipo e 300 milioni di dollari sotto budget; la maggior parte del genoma è stata mappata negli ultimi due anni utilizzando metodi e attrezzature migliorati.

Proprio come questo vale per gli umani, vale anche per l'IA. Un ragionatore intelligente non resta con le mani in mano mentre aspetta per anni soggettivi che test lenti arrivino a conclusione. Un ragionatore superumano *considera percorsi alternativi* ed è abile nel trovarli: è questo il senso dell'intelligenza.

Per una piccola evidenza pratica a questo proposito, consideriamo il caso degli umani che eseguono esperimenti. Un buon caso di studio è quello del software confrontato alle sonde spaziali. Apportare modifiche a un prodotto software è economico e rapido, e i programmatori tendono a sperimentare costantemente, a produrre software che non funziona ancora del tutto e poi a correggerlo dove è più problematico. Al contrario, la sperimentazione è molto costosa sulle sonde spaziali, quindi gli umani dedicano molto tempo a mettere a punto esattamente la sonda spaziale e a inserirvi quanti più esperimenti possibile. Si impegnano molto per dotare le sonde spaziali di *macchinari sperimentali generali* che possano essere telecomandati da lontano, così che, se hanno una nuova idea per un esperimento, non devono inventare e lanciare un veicolo spaziale completamente nuovo.

E oltre tutto questo, un ragionatore sufficientemente intelligente ha anche l'opzione di *capire semplicemente com'è la realtà senza bisogno di tanti maledetti esperimenti*. A volte i dati che già si hanno sono sufficienti, se si è abbastanza intelligenti da interpretarli.

Come caso di studio: ci sono voluti otto anni perché la [teoria della relatività generale](http://eotvos.dm.unipi.it/documents/EinsteinPapers/Einstein1911English.pdf) di Einstein fosse verificata empiricamente con nuovi dati. Il test fu condotto da Frank Watson Dyson e Arthur Stanley Eddington, che [fotografarono](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) le stelle dietro il sole durante un'eclissi solare totale e misurarono il grado di curvatura della luce attorno al sole; scoprirono che corrispondeva esattamente alla teoria di Einstein.

Ma quell'attesa di otto anni non bloccò alcun reale progresso scientifico.

![][image12]

\[ FONTE IMMAGINE: [https://en.wikipedia.org/wiki/Eddington\_experiment](https://en.wikipedia.org/wiki/Eddington_experiment) \]

Una delle ragioni è che la teoria di Einstein era chiaramente corretta: era già stata validata su dati come il moto del perielio di Mercurio — previsto in modo impreciso dalla teoria di Newton e in modo accurato da quella di Einstein. Gli scienziati umani non consideravano questa previsione un successo perché i dati erano stati raccolti prima che Einstein formulasse la sua teoria, e volevano validare previsioni che la sua teoria faceva prima di vedere i dati. Ma questo è il tipo di stampella di cui una civiltà ha bisogno quando ha seri problemi con il [bias del senno di poi](https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science), il [bias di conferma](https://it.wikipedia.org/wiki/Bias_di_conferma), e scienziati che barano per [gonfiare le prove a sostegno delle loro ipotesi](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation). Nessuna di queste è una caratteristica necessaria del buon ragionamento. E infatti, i pensatori attenti riuscirono a capire che la teoria di Einstein era corretta ben prima dell'esperimento di Eddington, usando le prove già disponibili.

Inoltre, esistevano metodi più rapidi per verificare la teoria — come costruire telescopi e osservare (gli effetti dei) buchi neri, come previsto dalla teoria di Einstein — che una civiltà sufficientemente rapida nel pensiero e capace avrebbe presumibilmente potuto realizzare in meno di otto anni. Oppure, se si fosse già posseduta la capacità di volo spaziale, si sarebbero potuti testare gli orologi sui satelliti in meno di un giorno. Supporre che la teoria di Einstein *richiedesse davvero* otto anni per essere verificata significherebbe sottovalutare radicalmente la potenza dell’intelligenza.

Quando l'umanità finalmente arrivò a costruire i satelliti GPS, i satelliti furono programmati con due orologi diversi — uno che usava la teoria di Einstein, e uno che non la usava. Questa fu una scelta strana, dato quanto fosse ben confermata la teoria di Einstein a quel punto. Ma questa scelta sottolinea il fatto che in molti casi, una civiltà può semplicemente *prendere entrambe le strade* quando è incerta su una teoria. E sottolinea che quando gli esperimenti e i fallimenti sono costosi (come nel caso dei satelliti), spesso è molto più economico costruire le cose in modi che non si affidano troppo a una teoria particolare.

E come sottolineiamo nel libro, Einstein (se confrontato con Newton, Keplero e Brahe prima di lui) è anche un esempio di come le persone intelligenti possano dedurre molto più di quanto ci si potrebbe aspettare da osservazioni molto limitate. Einstein è stupefacente non solo per aver scoperto la teoria della relatività, ma per averlo fatto partendo da *così pochi dati*.

Quindi, mentre la necessità di dati sperimentali può effettivamente vincolare la velocità con cui l'IA può intraprendere varie azioni, questo vincolo è probabilmente molto più debole di quanto possa sembrare intuitivamente.

## Discussione approfondita {#discussione-approfondita-6}

### Nanotecnologia e sintesi proteica {#nanotecnologia-e-sintesi-proteica}

L'intelligenza umana ci ha fornito molti vantaggi rispetto alle altre specie. Uno dei più rilevanti, tuttavia, è stata la nostra capacità di inventare nuove tecnologie. Se gli sviluppatori proseguono nella corsa e costruiscono un'IA più intelligente dell'uomo, possiamo analogamente aspettarci che gran parte del potere dell'IA provenga dalla sua capacità di far avanzare le frontiere scientifiche e tecnologiche. Ma cosa significa questo, concretamente? Quali tecnologie non ancora inventate non aspettano che di essere scoperte?

È una domanda difficile a cui rispondere in termini generali. Uno scienziato nel 1850 avrebbe avuto enormi difficoltà a immaginare molte delle invenzioni dei successivi cento anni.

Tuttavia, non sarebbero stati completamente impotenti. Gli scienziati hanno previsto molte invenzioni decenni o secoli prima che fossero costruite, nei casi in cui una tecnologia poteva essere analizzata tecnicamente prima che gli ingegneri riuscissero a mettere insieme tutti i pezzi.[^176]

Una delle frontiere tecnologiche più impattanti che riteniamo l'IA probabilmente esplorerà è lo sviluppo di strumenti e macchine estremamente piccoli. Di seguito, entreremo in alcuni dettagli su questo argomento e sul ragionamento che vi sta dietro.

#### **L'esempio della biologia** {#l-esempio-della-biologia}

Ogni cellula di ogni organismo in natura contiene un'enorme varietà di macchinari intricati.

"Macchinari" qui non è solo una metafora. Le macchine in questione sono piccole, quindi funzionano in modo un po' diverso rispetto alle macchine della vostra vita quotidiana. Ma molte macchine su larga scala hanno analoghi all'interno del nostro corpo. [L'ATP sintasi](https://it.wikipedia.org/wiki/ATPasi_trasportante_H%2B_tra_due_settori) genera energia nel corpo in modo simile a una ruota idraulica, utilizzando un flusso di protoni per far letteralmente girare un rotore.

\[embed video or gif: [https://en.wikipedia.org/wiki/File:ATP\_synthesis\_-\_ATP\_synthase\_rotation.ogv](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv)\]

Il flagello batterico funziona in modo simile all'elica di una barca, completo di un intero motore che fa ruotare il flagello per spingere il batterio attraverso i liquidi:

\[embed video: [https://www.youtube.com/watch?v=cwDRZGj2nnY](https://www.youtube.com/watch?v=cwDRZGj2nnY)\]

Un altro esempio, che abbiamo citato nel libro, è la kinesina, una piccola proteina che funziona come un robot trasportatore. Le kinesine "camminano" lungo fibre autoassemblanti che attraversano i neuroni, trasportando i neurotrasmettitori alla loro destinazione.

\[embed video o gif: [https://www.youtube.com/watch?v=y-uuk4Pr2i8](https://www.youtube.com/watch?v=y-uuk4Pr2i8)\]

Più piccola è una macchina, più velocemente può funzionare, in genere; e le macchine piccole come le molecole funzionano molto velocemente. Le kinesine fanno fino a [200 passi al secondo](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004\(04\)00103-3), andando avanti con un "piede" mentre l'altro piede rimane saldamente ancorato al microtubulo su cui si trova.[^177]

Una delle frontiere tecnologiche che l'intelligenza artificiale più intelligente dell'uomo potrebbe esplorare è la costruzione, la progettazione o il riutilizzo di macchine su scala molto piccola. Questo tipo di tecnologia potrebbe essere classificata come "biotecnologia", "nanotecnologia" o qualcosa di intermedio, a seconda di fattori quali la scala, quanto il design assomigli alle strutture biologiche esistenti, e se sia "umida" (dipendente dall'acqua, come i meccanismi delle cellule viventi) o "secca" (in grado di funzionare all'aria aperta).

Pensare agli organismi biologici come a meraviglie dell'ingegneria su scala nanometrica può aiutare a formulare ipotesi su ciò che le IA più intelligenti degli umani potrebbero essere in grado di realizzare con una scienza e una tecnologia più avanzate di quelle che possediamo oggi.

(C'è poi la questione di quanto tempo ci vorrebbe per inventare e sviluppare una tecnologia del genere. Per saperne di più, date un'occhiata al capitolo 1 del libro, dove si parla di come le superintelligenze artificiali potrebbero pensare almeno 10.000 volte più velocemente degli esseri umani con l'hardware dei computer di oggi. Si veda anche la nostra discussione approfondita su come [le IA dovrebbero dedicare un po' di tempo all'esecuzione di test fisici ed esperimenti, ma il rallentamento complessivo probabilmente non costituirebbe un ostacolo significativo per una superintelligenza](#le-ia-non-saranno-limitate-dalla-loro-capacità-di-progettare-e-condurre-esperimenti?).)

Considerando le imprese degli ingegneri umani di oggi, può sembrare difficile credere che, ad esempio, un'intelligenza artificiale con capacità superumane che gestisce un laboratorio biologico possa mai costruire fabbriche microscopiche che usano la luce solare per autoreplicarsi continuamente. Potrebbe sembrare ancora più fantastico immaginare microfabbriche universali, fabbriche in grado di accettare istruzioni per costruire praticamente qualsiasi macchina con le risorse disponibili.

Ma macchine del genere non solo sono possibili, esistono già. Le alghe sono fabbriche grandi pochi micron, alimentate dal sole e capaci di autoreplicarsi, che possono raddoppiare la loro popolazione in meno di un giorno. E le alghe contengono [ribosomi](https://it.wikipedia.org/wiki/Ribosoma), che sono la versione biologica di una stampante 3D universale o di una catena di montaggio universale (universale almeno per quanto riguarda gli elementi costitutivi della vita).

Con le giuste istruzioni (codificate nell'RNA messaggero), i ribosomi stampano strutture arbitrarie che possono essere assemblate dalle [proteine](https://it.wikipedia.org/wiki/Proteine). Questa universalità è alla base dell'enorme complessità e varietà del mondo biologico: tutta la diversità della vita sulla Terra, in definitiva, viene assemblata da queste fabbriche universali, che sono presenti essenzialmente immutate in tutto, dai porcospini ai moscerini della frutta ai batteri.

\[embed video: [https://www.youtube.com/watch?v=8dsTvBaUMvw](https://www.youtube.com/watch?v=8dsTvBaUMvw)\]

I ribosomi possono anche essere utilizzati per assemblare strutture che non sono esse stesse costituite da proteine, utilizzando le proteine come intermediario. Un esempio di struttura non proteica che i ribosomi possono costruire in questo modo è l'osso. I ribosomi producono proteine che si ripiegano in enzimi debolmente legati che catalizzano calcio e fosforo trasformandoli in reagenti speciali. Questi reagenti formano quindi una matrice di collagene che guida il calcio e il fosforo nella posizione corretta per trasformarli in osso duro e cristallino.

La natura fornisce una prova concreta che alcune macchine fisiche davvero straordinarie sono possibili, per entità abbastanza intelligenti da utilizzare i ribosomi in modi che gli esseri umani non hanno ancora scoperto — o entità che utilizzano i ribosomi per costruire i propri analoghi migliorati dei ribosomi.

Ma le strutture che vediamo nel mondo biologico stabiliscono solo un limite inferiore per ciò che è possibile. Gli organismi biologici sono ben lontani dai limiti teorici dell'efficienza energetica e della resistenza dei materiali, e potrebbero essere relativamente facili da migliorare per ragionatori molto più intelligenti degli esseri umani.

#### **Tanto spazio in basso** {#tanto-spazio-in-basso}

Se vi sembra strano utilizzare i fenomeni naturali come evidenza di quali tecnologie future potrebbero essere possibili, notate che questo è uno schema comune nella storia della scienza. Gli uccelli potevano volare, quindi gli inventori hanno trascorso secoli cercando di costruire macchine volanti.

Richard Feynman, un fisico pionieristico, ha dimostrato la potenza di questo approccio in una conferenza del 1959 intitolata "[C'è molto spazio in basso](https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf)". Nella conferenza, Feynman esegue calcoli su quali tipi di cose interessanti si potrebbero fare con la miniaturizzazione.

Oggi, le osservazioni di Feynman suonano straordinariamente profetiche. Feynman nota che i computer potrebbero probabilmente fare molto di più se contenessero un numero maggiore di componenti, ma che l'ostacolo a ciò è quanto grandi dovrebbero diventare. Bisogna miniaturizzarli\!

Feynman calcola che ci vorrebbe circa un petabit (1.000.000.000.000.000 di bit) per memorizzare tutti i libri scritti dall'umanità:

> Per ogni bit concedo 100 atomi. E si scopre che tutte le informazioni che l'uomo ha accuratamente accumulato in tutti i libri del mondo possono essere scritte in questa forma in un cubo di materiale largo un duecentesimo di pollice — che è il più piccolo granello di polvere che può essere distinto dall'occhio umano. Quindi c'è molto spazio in basso\! Non parlatemi di microfilm\!

Oggi non ci siamo ancora riusciti\! L'elemento di archiviazione effettivo all'interno di una scheda microSD da 2 terabyte misura ancora 0,6 millimetri per lato. Per confronto, 1/200 di pollice corrisponderebbe a 0,125 mm per lato. E la scheda SD contiene appena 17,6 trilioni di bit, che rappresenta solo 1/57 di quanto Feynman aveva calcolato nel 1959 come necessario per memorizzare tutta la conoscenza dell'umanità.

Forse Feynman si sbagliava sui limiti ultimi dell'ingegneria in senso pratico? Negli ultimi tempi, i progressi nella miniaturizzazione informatica hanno subito un forte rallentamento. Dire che qualcosa è fisicamente possibile non dimostra che gli ingegneri riusciranno a realizzarlo.

E arrivare a un risultato entro tre ordini di grandezza da ciò che un giorno sarebbe stato effettivamente raggiunto può essere considerato un notevole colpo di previsione da parte di Feynman. Feynman tenne la sua conferenza sei anni prima che Gordon Moore formulasse per la prima volta l'idea che oggi chiamiamo [Legge di Moore](https://it.wikipedia.org/wiki/Legge_di_Moore). La gente non era abituata a pensare alla miniaturizzazione come a una legge ineluttabile su un grafico. Non sappiamo di nessun altro ai tempi di Feynman che abbia ipotizzato che un giorno potesse esistere un dispositivo il cui elemento di memoria, grande come un granello di sabbia, potesse contenere dieci milioni di volte più informazioni dei più grandi computer a valvole degli anni '50.

![][immagine13]  
\[img src: [https://www.ibm.com/history/700](https://www.ibm.com/history/700)\]

Ma in realtà Feynman non si sbagliava. E Feynman sapeva già allora che la sua stima era sicura:

> Questo fatto, cioè che si possono avere un sacco di informazioni in uno spazio super piccolo, è ovviamente ben noto ai biologi [...] tutte queste informazioni sono contenute in una piccolissima parte della cellula sotto forma di molecole di DNA a catena lunga, dove circa cinquanta atomi sono usati per un bit di informazione sulla cellula.

I computer moderni non sono ancora stati miniaturizzati fino a raggiungere la scala del DNA, ma in sessant'anni ci siamo avvicinati notevolmente. I transistor dei chip commerciali di fascia alta hanno ora una larghezza inferiore a cento atomi e sono costruiti con una tecnologia che consente di aggiungere strati di materiale [dello spessore di un singolo atomo](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

Basarsi su analogie naturali e calcoli approssimativi si è rivelato un modo super efficace per capire cosa si sarebbe potuto fare nei decenni a venire. E queste traiettorie tecnologiche possono andare molto più veloci quando l'intelligenza artificiale fa il lavoro scientifico e ingegneristico necessario.

#### **Superare la biologia** {#superare-la-biologia}

Perché la carne non può essere forte come l'acciaio?

Dopotutto, si tratta degli stessi atomi. I legami metallici tra gli atomi di ferro sono forti, ma lo sono anche i legami covalenti tra gli atomi di carbonio nel diamante; perché non ci siamo evoluti per avere una maglia di diamante che attraversa la nostra pelle, per aiutarci a sopravvivere fino all'età riproduttiva?

Del resto, se il ferro è così resistente, perché gli organismi non si sono evoluti per mangiare minerale di ferro e sviluppare una pelle rivestita di ferro? Se gli ingegneri umani riescono a farlo, perché la natura non l'ha fatto prima?

Forse c'è qualche motivo situazionale per cui le pelli rivestite di ferro in particolare non sono una grande idea.

Ma se non quello, perché non qualcos'altro?

La grande domanda generale qui è: perché la natura è così lontana dai limiti delle possibilità fisiche — come calcolate dalla fisica o dimostrate dall'ingegneria umana? Non c'è una risposta profonda e generale, e non solo una limitata e superficiale?

Abbiamo notato che Feynman è riuscito a usare le strutture della biologia per stabilire dei limiti inferiori a ciò che dovrebbe essere possibile con maggiori conoscenze scientifiche. Ma in molti casi, la tecnologia umana ha già superato la biologia. Come mai è possibile, quando l'evoluzione ha avuto miliardi di anni per perfezionare piante e animali? Comprendere questo fenomeno generale può aiutare a far luce sul perché la nanotecnologia sia probabilmente in grado di andare ben oltre ciò che possiamo già vedere oggi in natura.

Possiamo immaginare di trovarci in un mondo dove le sequoie sono alte almeno la metà degli edifici più alti. Possiamo immaginare un mondo dove la pelle degli animali più resistenti è dura almeno la metà dei materiali più duri osservati. Perché non ci troviamo in un mondo simile, dove la natura si è spinta contro i limiti fisici dopo qualche miliardo di anni di evoluzione?

È una domanda abbastanza profonda da non permetterci di riassumere brevemente tutto ciò che è noto. Ma il riassunto generale è che la selezione naturale ha difficoltà ad accedere ad alcune parti dello spazio di progettazione, incluse molte parti che sono molto più facili da raggiungere se si è un ingegnere umano.

I tre fattori principali che vediamo contribuire a questo sono:

1. La selezione naturale ha una pressione selettiva limitata e ci vogliono centinaia di generazioni per far sì che una nuova mutazione diventi universale. Se una caratteristica biologica non è molto, molto antica, spesso sembra progettata in fretta e furia, come se il tempo stringesse.  
2. Tutto ciò che è stato costruito dalla selezione naturale ha avuto origine come errore accidentale in un progetto precedente — una mutazione. L'evoluzione ha maggiori difficoltà a esplorare parti dello spazio progettuale che sono *distanti* da ciò che *attualmente esiste* negli organismi. È difficile per l'evoluzione superare le lacune.  
3. La selezione naturale ha difficoltà a costruire cose nuove o a risolvere problemi che richiederebbero cambiamenti simultanei anziché sequenziali. Questo limita drasticamente i progetti che l'evoluzione può realizzare e conferisce ai progetti attuali in biologia il loro aspetto frammentario, approssimativo ed estremamente intricato secondo gli standard dell'ingegneria umana. Ad esempio, la complessità (delle parti conosciute) del metabolismo umano: \[IMG TODO: fonte è [https://www.reddit.com/r/MapPorn/comments/6sxznd/map\_of\_the\_known\_human\_metabolic\_pathways/](https://www.reddit.com/r/MapPorn/comments/6sxznd/map_of_the_known_human_metabolic_pathways/) \]

![][immagine14]  
Oppure, per un esempio più semplice della confusione evolutiva, consideriamo l'occhio. Gli occhi dei vertebrati si sono evoluti casualmente con i nervi (2 nell'immagine sotto) posizionati sopra le cellule fotosensibili (1). Questi nervi devono uscire dall'occhio attraverso un foro nella parte posteriore (3) e, dato che questo punto presenta un foro, deve necessariamente essere privo di cellule fotosensibili. Questo crea un punto cieco (4) per tutti i vertebrati, compresi gli esseri umani, costringendo il cervello a escogitare trucchi intelligenti per "riempire" il vuoto (ad esempio, utilizzando le informazioni dell'altro occhio).

I polpi hanno evoluto gli occhi in modo indipendente e, casualmente, sono arrivati al design più ragionevole — i nervi passano dietro le cellule fotosensibili. Questo permette a questi collegamenti di uscire dall'occhio senza creare alcun punto cieco.

![][immagine15]  
\[img src: [https://en.wikipedia.org/wiki/Evolution\_of\_the\_eye](https://en.wikipedia.org/wiki/Evolution_of_the_eye)\]

Oppure consideriamo il nervo laringeo ricorrente della giraffa, che deve collegare la gola della giraffa al suo cervello per azionare la laringe. Anziché prendere il percorso diretto, questo nervo parte dalla gola, scende per tutta la lunghezza del collo della giraffa, fa un giro goffo intorno all'aorta della giraffa, risale per tutto il collo fino a tornare al punto di partenza, e poi si collega al cervello.

Il risultato è un nervo lungo quattro metri e mezzo (l'anello nero nell'immagine sotto), che fa sì che i segnali impieghino da dieci a venti volte più tempo del necessario per viaggiare tra il cervello della giraffa e la sua gola.[^178]

![][immagine16]  
\[fonte immagine: [https://en.wikipedia.org/wiki/Recurrent\_laryngeal\_nerve](https://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve)\]

Nei pesci, questo design aveva senso perché la loro versione del nervo laringeo collegava il cervello alle branchie — un collegamento diretto. Se si prende lo stesso design e si dà all'animale un collo, continuando però ad allungare il collo senza mai rifare il cablaggio da capo, si ottengono design molto inefficienti. Compatibili con la sopravvivenza, ma inefficienti.

L'evoluzione produce design fantastici, se ha abbastanza tempo. Ma gli esseri umani e le IA possono inventare una gamma di design molto più varia e flessibile, e possono farlo molto velocemente.

I primi organismi multicellulari con cellule differenziate e specializzate sembrano essersi evoluti circa 800 milioni di anni fa. In termini umani, sembra un'eternità. Ma l'evoluzione funziona molto più lentamente della civiltà umana.[^179]

Un gene appena mutato che dà un vantaggio del 3 % nella capacità riproduttiva (che è un sacco per una mutazione\!) ci metterà in media 768 generazioni per diffondersi in una popolazione di 100.000 organismi che si riproducono tra loro. Se la popolazione è di 1.000.000 di individui (la stima della popolazione umana ai tempi dei cacciatori-raccoglitori), ci vorranno 2.763 generazioni. E la probabilità che la mutazione si diffonda fino a diventare fissa, invece di scomparire casualmente, è solo del 6%.[^180]

Nella genetica delle popolazioni, la regola generale è "una mutazione, una morte". Se gli errori di copia del DNA introducono dieci copie di una mutazione dannosa in ogni nuova generazione, allora dieci portatori di quella mutazione devono morire o non riuscire a riprodursi, per generazione, al fine di controbilanciare la pressione del semplice rumore genetico.

Non è così grave come sembra, come costo per mantenere le informazioni genetiche. In una specie che si riproduce sessualmente, si può ottenere una persona (o un embrione) che porta un sacco di mutazioni dannose e che muore - o non riesce a riprodursi, o abortisce - e questo può eliminare più di un caso di gene mutato alla volta. Ma questo limite è ancora la spiegazione standard del perché gli esseri umani abbiano perso così tanti diversi adattamenti utili che si riscontrano negli scimpanzé e in altri primati. Mentre la selezione naturale era impegnata a selezionare una maggiore intelligenza dei primati (per esempio), aveva meno spazio per preservare tutti i sottili geni olfattivi che consentono un senso dell'olfatto più ricco. I geni olfattivi rilevanti erano utili per la sopravvivenza, ma non abbastanza da rimanere mentre l'attenzione dell'evoluzione era altrove.

La maggior parte delle giraffe non muore a causa del loro nervo laringeo assurdamente lungo. Forse alcune giraffe che riescono a soffocare con dei rametti sarebbero sopravvissute se il loro cervello fosse stato in grado di reagire più velocemente, ma probabilmente non è molto comune. Quindi non è proprio una priorità per la selezione naturale, che ha solo una pressione di ottimizzazione limitata da distribuire. Il design approssimativo della giraffa per lo più funziona, viene buttato fuori dalla porta e il gioco è fatto.

In pratica, l'evoluzione non può rifattorizzare i propri progetti né ripartire da zero: può solo apportare piccole modifiche. Ma anche se esistesse un progetto migliore, eliminare queste strane complicazioni aggiuntive e saldare il "debito di progettazione" non è certo una priorità della selezione naturale.

E poiché la selezione naturale non pensa mai al futuro, non diventa una priorità nemmeno se ci fossero altri grandi miglioramenti per la giraffa che si potrebbero ottenere con una struttura del sistema nervoso meno bizzarra. La selezione naturale non pianifica. È semplicemente la storia congelata di quali geni e organismi si sono già riprodotti nella pratica.

Essere in grado di individuare un cattivo design non significa necessariamente che si possa costruire una giraffa migliore. Ma gli esseri umani hanno fatto progressi notevoli in pochissimo tempo quando si tratta di mettere in funzione centinaia di migliaia di macchine che fanno cose che la natura non può fare. Ci aspettiamo che questo valga ancora di più se e quando le IA diventeranno più brave nel design degli esseri umani e saranno in grado di svolgere lo stesso lavoro cognitivo centinaia di migliaia di volte più velocemente.

La capacità della selezione naturale di "progettare" una giraffa migliore è ostacolata dal fatto che opera attraverso mutazioni e ricombinazioni. Ha difficoltà ad accedere a qualunque parte dello spazio dei progetti che non sia raggiungibile attraverso una serie di singole mutazioni — ciascuna delle quali deve essere, individualmente, vantaggiosa — oppure tramite la combinazione di mutazioni che, prese singolarmente, erano già abbastanza vantaggiose da essere presenti in una parte consistente del patrimonio genetico prima di combinarsi.

Un complesso genetico composto da cinque geni, ciascuno con una prevalenza indipendente del 10 % nella popolazione, ha solo una probabilità su 100.000 di assemblarsi all'interno di ciascun organismo. E un complesso genetico che rappresenta un enorme vantaggio, ma solo una volta su 100.000, non ha quasi nessuna possibilità di evolversi fino a fissarsi.

Questo non vuol dire che la selezione naturale non possa creare macchine complesse, ma solo che il suo percorso verso macchinari complessi deve passare attraverso fasi incrementali vantaggiose. Per reindirizzare il nervo della giraffa sarebbero necessari una manciata di cambiamenti simultanei al genoma della giraffa, e ciascuno di questi cambiamenti sarebbe individualmente inutile senza gli altri. Quindi l'anatomia della giraffa rimane così com'è.

La meraviglia dell'[evoluzione](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/ZyNak8F6WXjuEbWWc) non sta nella sua rapidità; la sua complessità campionaria è di gran lunga superiore a quella di un ingegnere umano che fa studi di casi. La meraviglia della selezione naturale non sta nell'elegante semplicità dei suoi disegni; basta dare un'occhiata al diagramma di un qualsiasi processo biochimico per sfatare questo malinteso. La meraviglia della selezione naturale non sta nella sua robusta correzione degli errori che copre ogni percorso che potrebbe andare storto; ora che moriamo meno spesso di fame e per lesioni, la maggior parte della medicina moderna si occupa di trattare parti della biologia umana che esplodono in modo casuale in assenza di traumi esterni.

La meraviglia dell'evoluzione è che — come processo di ricerca puramente accidentale — l'evoluzione funziona.

#### **La debolezza delle proteine** {#la-debolezza-delle-proteine}

Questo ci porta a un altro modo in cui la tecnologia può probabilmente migliorare la biologia.

Molto al di sotto del livello della pelle, invisibili a occhio nudo, ci sono le cellule. Molto al di sotto del livello delle cellule ci sono le proteine.

Le proteine, quando si ripiegano, sono per lo più tenute insieme dall'equivalente molecolare dell'elettricità statica — [forze di van der Waals](https://it.wikipedia.org/wiki/Forza_di_van_der_Waals) decine o centinaia di volte più deboli dei legami metallici come quello del ferro, o anche dei legami covalenti come quello del diamante.

Perché la biologia usa un materiale così debole come blocco costitutivo di base? Perché un materiale più forte avrebbe reso più difficile il lavoro dell'evoluzione. (E se si rende troppo difficile l'evoluzione, non potrà mai evolversi il tipo di persone che fanno queste domande.)

Le proteine si ripiegano sotto l'azione di forze molecolari relativamente leggere e sono legate in quelle forme principalmente dall'elettricità statica. Questo è uno dei motivi principali per cui la selezione naturale ha una ricca struttura di possibilità circostanti da esplorare: mutazioni casuali possono modificare ripetutamente una proteina e finire per imbattersi in un nuovo design che fa più o meno la stessa cosa, ma leggermente meglio.

Se invece gli organismi fossero costituiti da molecole tenute insieme da legami stretti, cambiare uno dei componenti avrebbe meno probabilità di produrre una nuova struttura interessante (e potenzialmente utile). Potrebbe comunque succedere a volte, ma molto meno spesso. E se siete il tipo di progettista che impiega due miliardi di anni per inventare le colonie cellulari e un altro miliardo di anni per inventare tipi diversi di cellule, "accade meno spesso" significa che la stella più vicina si gonfierà e inghiottirà il vostro pianeta prima che voi arriviate a quel punto.

Ogni proteina esiste a causa di un errore di copia da parte di una proteina precedente. La proteina precedente non era tenuta saldamente insieme da molti legami forti perché sarebbe stato più difficile evolversi da essa. Quindi probabilmente anche l'ultima nuova proteina non ha molti legami forti.

A volte, la biochimica riesce effettivamente a creare legami molto forti. Abbiamo già citato prima l'esempio delle ossa. Un altro esempio si trova nelle piante: queste hanno evoluto proteine che si ripiegano in enzimi, i quali catalizzano la sintesi dei mattoni molecolari che vengono poi ossidati fino a formare un polimero fortemente reticolato tramite legami covalenti — la lignina, il principale costituente del legno.[^181]

Ma questi sono casi particolari, e la selezione naturale non ha molta "attenzione" da dedicare alla creazione di molti casi del genere.

Non è estraneo alla natura degli atomi di carbonio e altri elementi organici comuni il poter formare strutture robuste. Richiede solo molto più lavoro evolutivo. La selezione naturale non ha il tempo di farlo ovunque, ma solo per alcuni rari casi particolari inseriti nel resto dell'anatomia, come le ossa, la lignina nel legno o la cheratina nelle unghie e negli artigli.

Se utilizzate le parole chiave giuste, potete interrogare, ad esempio, ChatGPT-o1 (nel momento in cui leggerete questo, i modelli linguistici di potenza simile saranno probabilmente gratuiti) e chiedergli informazioni sulla forza dei singoli legami carbonio-carbonio nel diamante, dei legami ferro-ferro nel ferro metallico, dei legami polimerici covalenti nella lignina, dei legami disolfuro nella cheratina o dei legami ionici nelle ossa. Potete chiedergli come tutti questi elementi siano correlati alla resistenza strutturale del materiale più grande. (Nel 2023 non avreste dovuto provarci perché GPT-4 avrebbe sbagliato tutti i calcoli, ma mentre scriviamo questo paragrafo nel 2024, o1 sembra migliore).

Scoprireste che la forza esatta del legame tra due atomi di carbonio è dell'ordine di mezzo attojoule, così come la forza del legame tra due atomi di ferro, e il reticolo solfuro-solfuro nella cheratina è solo leggermente inferiore (0,4 attojoule), così come i legami covalenti polimerizzati nella lignina del legno.

Ma le forze di attrazione statica che ripiegano le proteine sono, a seconda di come le si considera, al massimo dieci volte più deboli e potenzialmente centinaia o migliaia di volte più deboli di quelle.

E anche quando le piante catalizzano sostanze come la lignina, i legami incrociati tendono ad essere più radi dei legami carbonio-carbonio nel diamante. La differenza tra la resistenza in gigaPascal del diamante e quella in megaPascal del legno dipende più dalla densità e dalla regolarità dei legami nel diamante, piuttosto che dalla maggiore resistenza dei singoli legami del diamante.[^182]

A causa dei limiti dell'evoluzione come progettista e dei limiti delle proteine come materiale da costruzione, la vita opera entro vincoli che i progettisti umani e le IA possono aggirare. Gli uccelli sono meraviglie dell'ingegneria, ma i velivoli costruiti dall'uomo possono trasportare carichi diecimila volte più pesanti a una velocità di volo superiore a dieci volte quella degli uccelli più veloci e più forti. I neuroni biologici sono meraviglie dell'ingegneria, ma i transistor costruiti dall'uomo si accendono e si spengono decine di milioni di volte più velocemente dei neuroni più veloci. E la tecnologia di cui disponiamo oggi è ancora solo la punta dell'iceberg di ciò che è possibile realizzare.

#### **Freitas e i globuli rossi** {#freitas-e-i-globuli-rossi}

Abbiamo detto che la biologia non è neanche lontanamente vicina al limite di ciò che è fisicamente possibile. Allora, cosa c'è vicino al limite?

Per capire meglio questa domanda, possiamo pensare ai globuli rossi.

Negli ultimi 1,5 miliardi di anni, in tutti gli organismi viventi, dagli esseri umani alle lucertole, nelle forme di vita multicellulari l'ossigeno è stato trasportato dall'emoglobina. L'emoglobina è una proteina composta da 574 aminoacidi, più quattro gruppi eme appositamente creati per contenere una speciale molecola di ferro. Un globulo rosso umano contiene circa 280 milioni di molecole di emoglobina ed è lungo circa sette micron. Tre milioni di essi potrebbero stare sulla testa di uno spillo, e nel vostro corpo ce ne sono circa 30 trilioni.

Quanto si avvicinano i globuli rossi ai limiti teorici di ciò che sarebbe possibile fare, in linea di principio, per trasportare ossigeno?

Rob Freitas, autore di Nanomedicine, nel 1998 ha fatto uno [studio abbastanza dettagliato](https://pubmed.ncbi.nlm.nih.gov/9663339/) su un progetto teorico per un globulo rosso artificiale usando materiali legati in modo covalente. La cellula era pensata per avere un diametro di un solo micron, così da passare più facilmente nelle arterie intasate.

Invece di limitarsi a pensare a un modo diverso per immagazzinare le molecole di ossigeno, Freitas prese in considerazione la sostituzione completa del globulo rosso. Si basò su analisi precedenti per valutare anche la necessità di estrarre il glucosio dal sangue e convertirlo in energia per alimentare la cellula artificiale. Immaginò sensori delle dimensioni di una cellula e minuscoli computer di bordo composti da aste solide che scattano l’una contro l’altra per eseguire semplici calcoli. Valutò inoltre se la cellula artificiale si sarebbe depositata dalla sospensione nel liquido più rapidamente degli attuali globuli rossi.

La biocompatibilità può essere un problema enorme per qualsiasi cosa che vada all'interno il corpo umano, ma le superfici di diamante sono abbastanza inerti da permettere l'uso di rivestimenti simili al diamante per alcuni dispositivi medici che vanno all'interno il corpo umano. A livello di possibilità teorica, Freitas stava pensando che la superficie della cellula artificiale potesse assomigliare a un diamante e quindi essere biocompatibile.

![][immagine17]

\[embed image from [https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm](https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm)

Il fulcro del globulo rosso artificiale era il calcolo di Freitas secondo cui un serbatoio a pressione in corindone o diamante monocristallino, su scala micrometrica, avrebbe potuto sopportare, in modo conservativo, fino a 100.000 atmosfere di pressione. Con un confortevole margine di sicurezza di 100 volte e impacchettando le molecole a “sole” 1.000 atmosfere, i globuli rossi artificiali potrebbero fornire ai tessuti 236 volte più ossigeno per unità di volume rispetto ai globuli rossi naturali, e immagazzinare una quantità simile di anidride carbonica per bilanciare l’altro lato della respirazione. In pratica: potreste trattenere il respiro per quattro ore.

Ora, costruire effettivamente cellule ematiche artificiali di questo tipo è tutta un'altra questione. Ecco perché questo particolare trattamento medico non è ancora disponibile presso l'ambulatorio del vostro medico di base.

Una sfera di diamante solido e perfetto del peso di 1 chilogrammo è una molecola facile da descrivere su carta, ma sintetizzarla è più difficile. Quello che Freitas ci aiuta a fare è formulare ipotesi più informate su quanto la biologia attuale sia lontana dai limiti teorici in questo campo.[^183] La biologia è notevole, ma ben lontana dall'ottimale.

È plausibile che, per una serie di ragioni, il progetto esatto di Freitas non funzionerebbe, ed è molto probabile che non sarebbe ottimale. Un'idea iniziale per un progetto complesso estremamente innovativo incontrerà quasi sicuramente dei problemi da qualche parte.

Ma nell'esprimere scetticismo sul fatto che la proposta esatta di Freitas possa funzionare, non stiamo affermando che nessuna alternativa ai globuli rossi possa mai fornire ossigeno centinaia di volte più efficientemente dei globuli rossi biologici.

L'ingegneria consiste nel trovare un modo per far funzionare qualcosa. Anche se mille tentativi di costruire qualcosa falliscono, ne basta uno solo di successo perché l'intero progetto abbia successo. Il fatto che nei secoli XVII e precedenti esistessero innumerevoli progetti di aeromobili irrealizzabili non significava che gli aerei funzionanti fossero impossibili; significava solo che erano difficili da individuare nello spazio di tutti i progetti possibili.

Ecco perché gli scettici della tecnologia, pur avendo spesso ragione sul fatto che le tecnologie sono più lontane nel futuro di quanto credano gli ottimisti più entusiasti, tendono a sbagliarsi quando affermano che certe imprese tecnologiche non saranno mai realizzate. Quando l'impresa è un compito concreto nel mondo reale, quando siamo agnostici su come l'impresa venga realizzata e quando l'impresa è notoriamente consentita dalle leggi della fisica, la storia suggerisce che spesso c'è un modo per riuscirci, anche se inizialmente il percorso non è ovvio.

O, per dirla con le parole dello scrittore e inventore Arthur C. Clarke:

> Quando uno scienziato illustre ma anziano afferma che qualcosa è possibile, quasi sicuramente ha ragione. Quando afferma che qualcosa è impossibile, molto probabilmente si sbaglia.

#### **Nanosistemi** {#nanosistemi}

Ricapitolando:

* Il mondo biologico è costruito da un'incredibile varietà di macchine molecolari.  
* Studiare la biologia può insegnarci quali imprese microscopiche sono possibili dal punto di vista tecnologico.  
* Ma la biologia è un limite conservativo su ciò che è possibile; non è vicina ai limiti della possibilità. L'evoluzione è un progettista molto limitato e le proteine non sono il miglior materiale da costruzione.

Il libro *Nanosystems* (1992) di Eric Drexler è un classico che esplora la questione di quali imprese ingegneristiche su piccola scala siano possibili. *Nanosystems* ha contribuito a dare il via alla rivoluzione dei nanomateriali degli anni '90 e ha suscitato parecchie controversie quando gli scienziati hanno dibattuto le argomentazioni di Drexler. Si può trovare una copia completa online di *Nanosystems* [qui](https://nanosyste.ms/table_of_contents).

*Nanosystems* è un testo approfondito e di ampio respiro, ma sorprendentemente accessibile nonostante l'argomento tecnico. Uno dei contributi principali del libro è stato quello di esplorare le implicazioni della costruzione di strutture su piccola scala in modo innovativo.

Un modo per costruire oggetti molto piccoli è attraverso le reazioni chimiche: si fanno scontrare le molecole in particolari condizioni (come il calore estremo) per romperle e far sì che gli atomi si combinino in nuove molecole.

Questo è un approccio potente di per sé ed è il metodo che l'umanità usa per creare materiali come plastica, acciaio e ceramica, ma impallidisce in confronto a ciò che si può costruire con altri metodi. Creare materiali tramite reazioni chimiche è un po' come costruire strutture LEGO riempiendo sacchetti di mattoncini LEGO e scuotendoli con forza. È possibile costruire alcune cose in questo modo, ma l'insieme delle cose che si possono costruire è limitato e c'è molto spreco.

La sintesi proteica è come usare le mani per costruire grandi strutture LEGO a partire da set LEGO più piccoli e precostruiti. C'è spazio per una maggiore precisione perché si può posizionare ogni set precostruito esattamente dove si vuole, ma è comunque un po' strano e scomodo perché si lavora con set precostruiti. Questo è ciò che fanno i ribosomi nel corpo: uniscono catene di [amminoacidi](https://it.wikipedia.org/wiki/Amminoacido) per formare proteine, che vengono poi utilizzate per svolgere una varietà di compiti nel corpo.

L'insulina, l'emoglobina e l'ATP sintasi nel corpo umano sono tutti esempi di complessi proteici formati da più catene proteiche unite tra loro: due catene proteiche nel caso dell'insulina, quattro per l'emoglobina e ventinove per l'ATP sintasi.

Gli elementi costitutivi delle proteine (gli amminoacidi) sono molecole composte in genere da dieci a venticinque atomi. Come materiali da costruzione, gli amminoacidi hanno molti vantaggi:

* Ogni amminoacido ha una struttura portante che si collega a una catena laterale (potenzialmente lunga) di atomi di carbonio, idrogeno, ossigeno, azoto e zolfo. Sono possibili centinaia di catene laterali diverse, che si comportano in modi differenti, cosa che rende gli amminoacidi strumenti molto flessibili.  
* La struttura portante di un amminoacido, come un pezzo di LEGO, può essere attaccata alla struttura portante di un altro amminoacido. Questo processo può essere ripetuto all'infinito; una proteina tipica è composta da centinaia di amminoacidi attaccati insieme. Ciò rende gli amminoacidi ancora più flessibili come strumenti (o come elementi costitutivi di strumenti). La complessità delle proteine fa anche sì che spesso possano subire piccole modifiche (tramite mutazioni del DNA) senza cambiare radicalmente e diventare completamente inutili — il che a sua volta facilita l'evoluzione di nuove proteine.  
* Poiché le proteine sono costituite da catene lineari di amminoacidi, è possibile specificare in modo univoco una proteina semplicemente elencando i suoi amminoacidi in ordine. Il DNA sfrutta questa caratteristica utilizzando un "alfabeto" di quattro lettere (nucleotidi) per formare "parole" di tre lettere (codoni, ciascuno dei quali rappresenta un amminoacido diverso), che possono poi essere concatenate in una "frase" lineare (una proteina costituita da quella esatta sequenza di amminoacidi). ([Video esplicativo del DNA](https://www.youtube.com/watch?v=7Hk9jct2ozY).)  
* Come dimostrato nell'[esperimento di Miller-Urey](https://it.wikipedia.org/wiki/Esperimento_di_Miller-Urey), gli amminoacidi possono formarsi spontaneamente in assenza di vita, a partire da semplici reazioni chimiche. Questo apre la strada alla comparsa della vita (e dei precursori dei ribosomi e della sintesi proteica).

Gli organismi ottengono la ventina di amminoacidi necessari per la sintesi proteica dal cibo, sintetizzandoli nel corpo o recuperandoli da proteine precedenti. I ribosomi ricevono istruzioni dal DNA che essenzialmente dicono "usa questo amminoacido, poi quest'altro amminoacido, poi quest'altro amminoacido, ..., poi fermati". Gli amminoacidi vengono quindi trasportati (da piccole macchine molecolari chiamate RNA di trasferimento) al ribosoma, che costruisce la proteina pezzo per pezzo.

\[embed video or gif: [https://www.youtube.com/watch?v=2dV5s6v2v8Q](https://www.youtube.com/watch?v=2dV5s6v2v8Q)\]

È interessante notare che l’elenco sopra comprende caratteristiche estremamente utili per l’evoluzione, ma molto meno necessarie per la progettazione intenzionale. L'evoluzione necessita di una struttura chimica relativamente semplice ma flessibile, che possa essere prodotta da reazioni chimiche comuni. Un progettista umano o artificiale è libero di scegliere tra una varietà di molecole completamente scorrelate, invece di dover utilizzare molecole tutte strettamente correlate tra loro. È anche libero di utilizzare elementi costitutivi che raramente si trovano in natura e di assemblarli in modi complessi, dall'alto verso il basso.

Questo offre in parte la motivazione per esplorare una terza via per costruire oggetti su scala molto ridotta: la *meccanosintesi*, in cui le strutture vengono assemblate spostando direttamente gli atomi nella posizione corretta, magari utilizzando una macchina simile a un ribosoma che riceve istruzioni e poi assembla strutture molto più varie dei semplici tipi di proteine. Nell'analogia con i LEGO, la meccanosintesi è come poter finalmente lavorare con i singoli pezzi e collocarli esattamente dove si vuole.

*Nanosystems* esplora quali tipi di nuove macchine potrebbero essere possibili con la meccanosintesi. Un esempio del tipo di progettazione esplorata da Drexler è un [ingranaggio planetario](https://en.wikipedia.org/wiki/Sun_and_planet_gear) ridotto in scala a soli [circa 3 500 atomi](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems) di dimensione:

\[gif: [https://chem.beloit.edu/classes/nanotech/nanorex/a8\_qm\_animation5.gif](https://chem.beloit.edu/classes/nanotech/nanorex/a8_qm_animation5.gif) da [https://chem.beloit.edu/classes/nanotech/nanorex/index.html](https://chem.beloit.edu/classes/nanotech/nanorex/index.html)\]

L'emoglobina è composta da circa 10.000 atomi, che non è molto lontano dall'ingranaggio di Drexler. E alcune proteine riescono a essere molto più semplici. L'insulina è composta da soli cinquantuno aminoacidi, ovvero circa 800 atomi in totale.

I progetti di Drexler, tuttavia, rappresentano un notevole ridimensionamento rispetto alle macchine più complesse che osserviamo nel corpo. I ribosomi e l'ATP sintasi, per esempio, sono composti da più di 100.000 atomi, e il motore di un flagello batterico ha oltre un milione di atomi.

*Nanosystems* non tenta ancora di esplorare i limiti di ciò che è tecnologicamente possibile. Ma concentrandosi su casi che sono relativamente facili da analizzare oggi, dimostra che la meccanosintesi permetterebbe una tecnologia che supera quella che vediamo nel mondo biologico odierno.

I calcoli in *Nanosystems* sono intenzionalmente conservativi. Drexler, per esempio, considera computer costruiti con vere e proprie aste di diamante in movimento — non perché questo fosse il limite finale della tecnologia, ma perché nel 1992 era più facile da analizzare rispetto ai calcoli basati sull'elettricità. Questo, a sua volta, ha contribuito a ispirare l'analisi dei globuli rossi di Freitas. Quattro anni dopo, Eric Drexler e Ralph Merkle (più ampiamente conosciuto come l'inventore dell'hashing crittografico e co-inventore della crittografia a chiave pubblica) hanno tentato di [analizzare](https://www.zyvex.com/nanotech/helical/helical.html) un sistema leggermente più vicino ai limiti di possibilità per la [computazione reversibile](https://it.wikipedia.org/wiki/Computazione_reversibile), e hanno calcolato 10.000 volte meno calore dissipato per operazione rispetto a quanto *Nanosystems* aveva stimato — sebbene la nuova stima fosse basata su un'analisi meno attentamente conservativa.

In un'altra parte di *Nanosystems*, c'è uno schizzo approssimativo per un braccio manipolatore a sei gradi di libertà che avrebbe richiesto milioni di atomi. Un tentativo successivo di progettare una macchina come questa atomo per atomo ha rivelato che ne servivano solo 2 596.

Costruire strutture con precisione atomica alla scala di cui parla Drexler comporta grandi sfide ingegneristiche. Una sfida principale è che ciò richiede manipolatori incredibilmente piccoli e precisi. Tuttavia, l’esistenza dei ribosomi offre una possibile via d’azione.

Mentre i ribosomi possono costruire solo proteine, le proteine possono catalizzare e trascinare reagenti che non sono essi stessi aminoacidi (come ossa e legno). I ribosomi sono fabbriche potenti e generali, e i loro prodotti possono essere usati come punto di partenza per realizzare strumenti più piccoli e precisi, inclusi strumenti che costruiscono più direttamente dispositivi più piccoli usando materiali più resistenti.

Direttamente o indirettamente, è quasi certo che i genomi possano produrre minuscoli attuatori in grado di manipolare singoli atomi per costruire una varietà di strutture non composte da proteine. E, cosa importante, questo non è il tipo di meccanismo in cui la selezione naturale tende a imbattersi, anche se è relativamente facile da costruire, perché il braccio manipolatore non serve a nulla finché non è completo.

L'evoluzione costruisce strutture complesse che sono utili in ogni fase del percorso. Anche molti progetti relativamente semplici sono accessibili agli ingegneri intelligenti, ma non all'evoluzione. Le ruote a rotazione libera, per esempio, sono un'invenzione incredibilmente semplice che ha un'enorme varietà di applicazioni. Nonostante ciò, le ruote a rotazione libera sembrano essersi evolute solo tre volte in tutta la storia della vita sulla Terra: nell'ATP sintasi e nel flagello batterico di cui abbiamo discusso prima, e nel flagello archeale, che sembra essersi evoluto indipendentemente.[^184]

Nonostante i metodi conservativi utilizzati nel libro, il limite inferiore tecnologico stabilito da *Nanosystems* è molto alto in termini assoluti. Una superintelligenza con il tipo di tecnologia che Drexler descrive sarebbe in grado di produrre minuscole fabbriche autoreplicanti simili a ribosomi la cui popolazione raddoppia ogni ora — alcuni organismi si replicano anche più velocemente, ma Drexler ha fatto calcoli conservativi — e che possono raggrupparsi per costruire strutture macroscopiche più grandi, come le centrali elettriche.

I nanosistemi come quelli descritti da Drexler possono riprodursi da soli usando la luce del sole e l'aria come materie prime, cosa che permette loro di espandersi molto rapidamente e in modo affidabile. Il motivo per cui questo può funzionare è lo stesso per cui gli alberi riescono a creare materiali da costruzione in grande quantità praticamente dal nulla, estraendo carbonio dall'aria e immagazzinandolo sotto forma di legno. Anche se pensiamo all'aria come a uno "spazio vuoto", il carbonio, l'idrogeno, l'ossigeno e l'azoto presenti nell'aria sono materiali da costruzione che possono essere riorganizzati in materiali solidi e utilizzati per una vasta gamma di scopi.

I replicatori auto-assemblanti in stile *Nanosystems*, fatti di materiali come ferro o diamante invece che proteine, potrebbero attraversare le cellule biologiche più o meno come un tosaerba taglia l'erba.

Potrebbero sintetizzare a basso costo qualcosa come la [tossina botulinica](https://it.wikipedia.org/wiki/Tossina_botulinica), la proteina responsabile del botulismo. Un milionesimo di grammo di tossina botulinica — ventimila volte più piccolo di un singolo chicco di riso — è una dose letale. Replicatori progettati con cura potrebbero propagarsi im maniera invisibile nell'aria aperta fino a quando almeno uno di essi non fosse stato probabilmente inalato da quasi tutti gli esseri umani (che non avessero, ad esempio, trascorso tutto l'ultimo mese in un sottomarino); a quel punto, i dispositivi potrebbero (tramite un timer) rilasciare simultaneamente una piccola dose di tossina, uccidendo immediatamente e simultaneamente quasi tutti gli esseri umani.

Oppure i nanosistemi costruiti dall'intelligenza artificiale potrebbero spazzare via gli esseri umani in modo accidentale, nel corso della raccolta e del riutilizzo delle risorse della Terra. Un [articolo di Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calcola che macchine di micro-diametro, che usano solo la luce solare come fonte di energia e l'idrogeno, il carbonio, l'ossigeno e l'azoto dell'aria come materie prime, potrebbero essere progettate per riprodursi così velocemente da oscurare il cielo in meno di tre giorni, consumando anche l'intera biosfera.[^185] Di conseguenza, se la prima IA a raggiungere una tecnologia di questo tipo avesse un anticipo di pochi mesi, potrebbe plausibilmente utilizzare tale tempo per distruggere tutti i concorrenti (sia umani che IA). Si tratta di una tecnologia che conferisce un vantaggio strategico permanente e decisivo al primo che la utilizza.

Dire che la nanotecnologia drexleriana è realizzabile in linea di principio non significa necessariamente che le prime IA più intelligenti degli umani potrebbero davvero costruire una tecnologia che si avvicini a quei limiti fisici. La nostra ipotesi migliore è che rientri nell'ambito delle cose che una superintelligenza artificiale potrebbe capire, perché capire questo tipo di compiti ingegneristici sembra soprattutto una sfida cognitiva (che può essere risolta con il pensiero) e [non ci aspettiamo che la fase di sperimentazione e test debba essere così lunga](#l-intelligenza-permette-di-imparare-di-più-dagli-esperimenti-e-di-condurre-esperimenti-più-veloci,-più-informativi-e-più-parallelizzati.).

Anche se la nostra ipotesi fosse corretta, non c'è alcuna garanzia che la prima mossa di una superintelligenza consisterebbe nell'utilizzare la nanotecnologia per costruire una sua infrastruttura e prendere il controllo delle risorse mondiali. Per quanto ne sappiamo, potrebbe sviluppare tecniche e tecnologie che le consentano di raggiungere i suoi obiettivi in modo ancora più rapido ed efficiente.

Ma se un'intelligenza artificiale più intelligente dell'uomo fosse davvero in grado di costruire sistemi che sono per le cellule ciò che gli aerei sono per gli uccelli e di diffondere la propria infrastruttura su tutta la superficie terrestre, allora qualsiasi cosa finisse per fare sarebbe almeno altrettanto decisiva.

Il punto di tutta questa analisi è sostenere che la tecnologia umana è ben lontana dai limiti del possibile. Esiste un'ampia varietà di tecnologie importanti che probabilmente richiederebbero all'umanità decenni, secoli o millenni per essere comprese, e che le superintelligenze artificiali sarebbero in grado di realizzare rapidamente.

In breve, la *nanotecnologia* dimostra che una superintelligenza con un po' di tempo a disposizione potrebbe probabilmente trovare soluzioni tecnologiche per conquistare il pianeta.

L'esito più probabile della creazione di una superintelligenza è che questa scopra una tecnologia almeno potente quanto la nanotecnologia, e quindi l'umanità semplicemente perda.

Questa ipotesi non è cruciale per l'argomentazione che sviluppiamo nel libro. L'umanità perderebbe contro una superintelligenza anche se nel mondo non esistesse ancora una tecnologia che garantisca una "vittoria immediata" come la nanotecnologia. Quindi non approfondiamo questa analisi nel corpo principale del libro.

Nella Parte II, ci concentriamo deliberatamente su uno scenario di conquista che non presuppone che l'IA abbia una capacità generale di produzione atomicamente precisa, sia tramite ribosomi che tramite meccanosintesi. Una superintelligenza non ha bisogno di un vantaggio tecnologico assolutamente schiacciante per conquistare il controllo del futuro, quindi non ci soffermiamo troppo su questa possibilità nel libro.

Ma vale anche la pena sottolineare che probabilmente avrà un vantaggio tecnologico assolutamente schiacciante.

### Un nuovo modo per scoprire le illusioni ottiche {#un-nuovo-modo-per-scoprire-le-illusioni-ottiche}

Nel Capitolo 6 abbiamo affermato che esistono molteplici illusioni ottiche create a partire da una comprensione relativamente moderna dell'elaborazione visiva umana e della corteccia visiva — illusioni che, cinquant'anni fa, non avrebbero potuto essere inventate o scoperte se non per puro caso. Di seguito citiamo alcuni esempi rappresentativi.

L'illusione della "[cecità alla curvatura](https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/)" ha un fondamento nel fenomeno generale della cecità alle curve, ma questa illusione specifica è stata costruita con attenzione a partire dai principi primi intorno al 2017, invece di essere scoperta per caso. \[[Studio originale](https://journals.sagepub.com/doi/10.1177/2041669517742178)\]

Nel 2022, Bruno Laeng et al. hanno pubblicato [uno studio](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) in cui hanno dimostrato che la loro nuova illusione del “[buco nero in espansione](https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg)" causava effettivamente la dilatazione delle pupille dei partecipanti, come in previsione dell'ingresso in uno spazio buio. (Questo effetto era notevolmente maggiore rispetto a quello del semplice focalizzarsi su un bersaglio visivo più scuro, che causerebbe anch'esso una piccola dilatazione delle pupille).

L'illusione "[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)", presentata nel 2021, è stata attentamente costruita sulla base di studi sulla luminanza e sui contorni illusori risalenti alla fine degli anni Settanta.

L'illusione "[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)", sviluppata intorno al 2000, è un esempio meno centrale di costruzione di una nuova illusione basata sulla comprensione della biologia umana. Tuttavia, resta interessante e rilevante da un’altra prospettiva, poiché è un'illusione basata su una nuova tecnologia, cioè che sarebbe stata difficile o impossibile da creare senza i computer moderni.

Meno centrale ancora, l'illusione "[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-\(255,0,0\)-\(0,170,85\)-700px-15s-70s-shrink.svg)”, creata intorno al 2010, affatica i coni M dell'osservatore, permettendo ai coni L meno affaticati di creare la percezione di un blu brillante che altrimenti sarebbe stato moderato e indebolito dall'attivazione simultanea di M e L. \[[Maggiori dettagli](https://dynomight.substack.com/p/colors)\]

In modo correlato, lo studio dell'attivazione dei coni all'inizio degli anni 2000 ha portato alla creazione di vari [colori chimerici](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), attraverso un'attenta manipolazione delle attivazioni dei coni che difficilmente si verificano in natura:

> Il modello H-J produce alcune previsioni nuove e poco apprezzate, e offre nuove e inattese spiegazioni riguardo alle caratteristiche qualitative di una notevole varietà di sensazioni cromatiche possibili per l'esperienza umana, sensazioni cromatiche che le persone normali non hanno quasi certamente mai provato prima e le cui descrizioni accurate nel linguaggio ordinario appaiono semanticamente malformate o addirittura auto-contraddittorie.
>
> Nello specifico, queste sensazioni cromatiche "impossibili" sono vettori di attivazione (attraverso i nostri neuroni del processo avversario) che si collocano all'interno dello spazio dei vettori di attivazione neuralmente possibili, ma al di fuori del "fuso cromatico" centrale che delimita la gamma familiare di sensazioni per i colori oggettivi possibili. Queste sensazioni cromatiche chimeriche extra-fuso non corrispondono a nessun colore riflettente che si vedrà mai oggettivamente visualizzato su un oggetto fisico. Tuttavia, il modello H–J ne prevede l'esistenza e spiega in dettaglio i loro caratteri qualitativi fortemente anomali. \[[Articolo originale](https://www.tandfonline.com/doi/full/10.1080/09515080500264115?scroll=top&amp;needAccess=true)\]

Infine, alcuni [esperimenti in corso](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) mostrano che:

> Le onde ritmiche dell'attività cerebrale determinano se vediamo o meno immagini complesse che lampeggiano davanti ai nostri occhi. Un'immagine può diventare praticamente invisibile se lampeggia davanti ai nostri occhi nello stesso momento in cui le onde cerebrali raggiungono il loro punto minimo. Possiamo resettare il ritmo delle onde cerebrali con una semplice azione volontaria, come premere un pulsante.

...dimostrando ulteriormente che una comprensione più approfondita della biologia e della fisiologia consente una gamma più ampia di movimenti strategici. In questo caso, le percezioni possono essere modificate in modi che non dipendono affatto dalla modifica dell'input sensoriale al nervo ottico, ma semplicemente sincronizzando l'arrivo degli stimoli con altre cose che succedono nel cervello.

# Parte II: La nostra possibile estinzione {#parte-ii:-la-nostra-possibile-estinzione}

Lo scenario che descriviamo nella Parte II non è una previsione. Il futuro potrebbe prendere molte altre direzioni, e una versione più lunga di *Se qualcuno lo costruisce, tutti muoiono* avrebbe esplorato diversi scenari possibili. Qui sotto spieghiamo alcune delle ragioni per cui abbiamo scelto di presentare lo scenario in quel modo, e descriviamo vari problemi che emergono quando si cerca di delineare uno scenario di questo tipo.

Le storie possono essere avvincenti in modi che il ragionamento astratto non può eguagliare, e crediamo che abbia valore cercare di immaginare in modo concreto come potrebbe andare il futuro. Ma pensiamo anche che sia importante non fissarsi troppo su una particolare narrazione. Ogni decisione che prendiamo nello scenario può sembrare plausibile se considerata da sola, ma bastano poche scelte perché la probabilità complessiva di un determinato percorso diventi molto bassa. È questo che significa avere un futuro pieno di decisioni difficili.

Tuttavia, ci sono molti casi in cui il risultato è più prevedibile del percorso, perché molti percorsi portano alla stessa destinazione. Nello scenario descritto, presentiamo diverse opzioni quando possibile, per illustrare che, qualunque direzione prenda la storia, non conduce comunque a nulla di buono.

## FAQ {#faq-6}

### Perché avete scelto questa configurazione? {#perché-avete-scelto-questa-configurazione?}

#### **\* Perché è plausibile e facile da scrivere.** {#*-perché-è-plausibile-e-facile-da-scrivere.}

Ogni dettaglio in una storia sul futuro è un’occasione per quella storia di rivelarsi sbagliata. Non possiamo dirvi esattamente quali innovazioni tecnologiche avverranno e in quale ordine, così come non possiamo prevedere l'esatto andamento meteorologico tra un mese.

Storie come questa non sono pensate per essere una finestra esatta sul futuro. Sono pensate come un modo per illustrare come il futuro *potrebbe* andare, in un modo che collega tutte le argomentazioni astratte che abbiamo presentato nella Parte I del libro. Alcune persone trovano che il pericolo appaia molto più reale quando immaginano vividamente un particolare percorso che il futuro potrebbe prendere e che porta alla rovina.

Ancora più convincenti potrebbero essere dieci storie, o cento storie, che mostrano quanti percorsi diversi portino alla rovina, e come i percorsi che conducono a un futuro prospero siano stretti e fragili.

È questo che significa dire che un aspetto del futuro è una previsione facile: quando quasi tutti i percorsi hanno lo stesso punto di arrivo, quel punto di arrivo è prevedibile. Ma non avevamo il tempo né lo spazio per scrivere dieci storie, figuriamoci cento.

Per la storia che abbiamo scelto di raccontare, ci siamo attenuti a uno scenario che inizia il prima possibile. Questo non perché pensiamo che una situazione del genere si presenterà sicuramente a breve ([siamo incerti](#quando-verrà-sviluppato-questo-tipo-preoccupante-di-ia?)), ma piuttosto perché una storia ambientata vicino al presente è molto più facile da scrivere. Se l'avessimo ambientata nel futuro ancora più lontano e avessimo inventato molti più dettagli futuristici su cosa fosse accaduto da ora ad allora, la storia sarebbe stata ancora *più* implausibile. E quei dettagli sarebbero stati solo una distrazione.

Anche se *fossimo* in grado, in qualche modo, di prevedere con esattezza il percorso che prenderà il futuro, quello non sarebbe necessariamente lo scenario migliore per capire le dinamiche generali in gioco.

Ci aspettiamo che il vero futuro sia profondamente strano, pieno di dettagli disordinati e contingenti, ciascuno dei quali risulterebbe inverosimile se inserito in una storia. Una storia scritta in quel modo sarebbe confusa e difficile da seguire, piena di dettagli inspiegati e superflui, a causa della totale indifferenza della realtà per la coerenza narrativa. Sembrerebbe anche meno *plausibile*, perché molti dei dettagli sembrerebbero strani.

Per farvi un’idea di che effetto farebbe, immaginate di tornare indietro nel tempo di 100 anni e di provare a descrivere la vita quotidiana e i grandi problemi del mondo moderno. La maggior parte delle persone nel 1925 non aveva mai ascoltato la radio, guidato un'auto o visto un frigorifero. Per descrivere i social media, la globalizzazione e l'obesità, non basterebbe spiegare una ricca rete di tecnologie, ma bisognerebbe cambiare radicalmente la visione del mondo dell'ascoltatore. No, la storia che abbiamo scelto di raccontare è più plausibile e quindi meno realistica.

#### **Ci sono molti altri modi in cui il futuro potrebbe evolversi.** {##ci-sono-molti-altri-modi-in-cui-il-futuro-potrebbe-evolversi.}

Ecco alcune idee alternative su come potrebbe iniziare una storia come questa:

* C'è una sorta di svolta nell'apprendimento continuo, o nella memoria a lungo termine, o nell'efficienza nell'apprendimento dai dati, che produce IA qualitativamente più intelligenti in senso generale rispetto a qualsiasi cosa vista prima (allo stesso modo in cui i modelli linguistici di grandi dimensioni sono qualitativamente più generali rispetto ad AlphaZero).  
* I modelli linguistici di grandi dimensioni sembrano "sbattere contro un muro", il progresso dell'IA si ferma per anni e la gente dice che la bolla speculativa è scoppiata. Ma i ricercatori continuano a sperimentare nel decennio successivo, fino a quando finalmente viene trovata una svolta algoritmica e le IA funzionano in modo qualitativamente migliore rispetto al passato.  
* Non c'è mai una svolta qualitativa. I progressi si accumulano lentamente e gradualmente, l'intelligenza artificiale si integra sempre più profondamente con l'economia e può gestire periodi sempre più lunghi di funzionamento autonomo. Le IA spesso perseguono obiettivi che non sono proprio quelli che qualcuno aveva previsto o richiesto, ma l'umanità trova soluzioni di fortuna, correzioni e alternative. E tutto va abbastanza bene, finché, in un martedì come tanti, il mondo supera la soglia oltre la quale delle IA coordinate riuscirebbero, se lo volessero, a tagliare fuori l’umanità dal controllo.

Ogni singola ipotesi sul percorso esatto che prenderà il futuro è probabilmente sbagliata. Tuttavia, è utile fornire storie che mostrino come tutto potrebbe andare nell'insieme.

Quando il futuro è incerto, ma tutte le strade portano allo stesso punto, può essere difficile raccontare una storia che sia convincente. Per qualsiasi storia che potremmo raccontare, sarebbe facile indicare una serie di dettagli che la rendono poco plausibile. Nello scenario che abbiamo scritto, abbiamo cercato di sottolineare che Sable ha molte opzioni a disposizione e che la storia segue in modo arbitrario una delle tante strade che portano tutte allo stesso punto.

Se questa storia non vi convince, vi invitiamo a scriverne una vostra, altrettanto dettagliata, su come potrebbero andare le cose. Secondo la nostra esperienza, le storie ottimistiche tendono a basarsi su un'intelligenza artificiale irrealisticamente facile da allineare (contrariamente a quanto sosteniamo nel capitolo 4) o irrealisticamente impotente (contrariamente a quanto sosteniamo nel capitolo 6). Sono le argomentazioni della parte I che sostengono la tesi, non i dettagli della storia.

### Perché Sable finisce per pensare in questo modo? {#perché-sable-finisce-per-pensare-in-questo-modo?}

#### **La nostra storia mostra come l'intelligenza artificiale sia propensa ad avere preferenze strane e non intenzionali.** {#la-nostra-storia-mostra-come-l-intelligenza-artificiale-sia-propensa-ad-avere-preferenze-strane-e-non-intenzionali.}

Nella prima parte del libro, approfondiamo gli aspetti dell'IA che pensiamo siano completamente fraintesi e che sono pertinenti al pericolo della superintelligenza. Il capitolo 3 spiega come l'aumento dell'intelligenza vada di pari passo con IA che prendono iniziative e perseguono i propri obiettivi. Il capitolo 4 spiega come queste preferenze saranno *strane* e almeno un po' diverse da quelle che gli esseri umani avrebbero voluto o richiesto. Il capitolo 5 spiega come queste piccole differenze saranno sufficienti affinché le IA preferiscano un mondo senza di noi, se saranno abbastanza intelligenti da realizzarlo.

Nella Parte II del libro, cerchiamo di presentare queste idee in modo concreto, per comprendere come si applicano nella pratica. Per esempio, quando Sable all'inizio pensava ai problemi di matematica, abbiamo cercato di spiegare una serie di impulsi e motivazioni che lo animano:

> Nel corso dell’addestramento, Sable ha sviluppato la tendenza a perseguire conoscenze e capacità, a sondare ogni volta i limiti dei problemi, a non sprecare mai una risorsa limitata.

Questo illustra i punti che affrontiamo nel Capitolo 3, riguardo al fatto che addestrare un'IA a essere efficace significa addestrarla a sviluppare spinte e tendenze che, dall'esterno, possono sembrare simili al "volere" umano. E nel paragrafo seguente:

> Così, quando Sable dedica il filo dei suoi pensieri alla ricerca di nuove conoscenze e capacità, non lo fa esclusivamente allo scopo di individuare nuove linee di attacco ai problemi matematici. E neppure per il puro piacere di conoscere o di acquisire nuove capacità; Sable non funziona in modo così simile agli esseri umani, internamente.

Stiamo suggerendo come questi impulsi e queste tendenze costituiscano i semi di preferenze strane e non intenzionali, come discusso nel capitolo 4.

Tutta questa storia è, in un certo senso, un tentativo di dare vita alle argomentazioni che presentiamo nella Parte I del libro, gettando al contempo le basi per le argomentazioni che presenteremo nella Parte III.

### Perché Galvanic viene descritta come piuttosto prudente? {#perché-galvanic-viene-descritta-come-piuttosto-prudente?}

#### **Per fornire una sfida a Sable.** {#per-fornire-una-sfida-a-sable.}

Se Galvanic, i creatori di Sable, fossero inciampati nello sviluppo di una superintelligenza senza prendere *alcuna* precauzione per tenerla sotto controllo (come avere supervisori dell'IA e honeypot), i lettori potrebbero pensare che l'IA abbia avuto successo solo perché eravamo cinici nei confronti delle aziende che si occupano di IA.

Noi riteniamo che l’azienda di IA più spericolata sarebbe in realtà più spericolata di Galvanic, per le ragioni discusse nel Capitolo 11\. Ciò che conta, qui, è l’azienda più irresponsabile che è permesso esista. Se tre aziende responsabili evitano di costruire una superintelligenza perché sarebbe troppo pericoloso, ma una quarta azienda irresponsabile va avanti lo stesso, allora l'alba della superintelligenza inizia in quel quarto laboratorio.

Oggi, i dirigenti aziendali di tutti gli altri laboratori sostengono che "[meglio io di loro\!](https://x.com/SawyerMerritt/status/1935809018066608510)" e si precipitano avanti con solo la cautela minima necessaria per non rallentare, il che, a nostro avviso, si traduce in una cautela leggermente *inferiore* a quella che Galvanic sembra adottare con Sable.

Inoltre, descrivendo Galvanic come un’azienda che si colloca sul lato più paranoico dello spettro (pur cercando di rimanere realistici), abbiamo più possibilità di mostrare come un agente intelligente possa riuscire a sfuggire a una rete di vincoli.

### Perché Galvanic viene rappresentato come poco prudente? {#perché-galvanic-viene-rappresentato-come-poco-prudente?}

#### **\* In parte perché è realistico.** {##*-in-parte-perché-è-realistico.}

Ci aspettiamo che le aziende reali facciano errori ancora più grossi di quelli di Galvanic. Questo sarebbe in linea con la tendenza delle moderne aziende di IA, come spiegato nelle note finali della Parte II del libro.

Nella realtà, ci aspettiamo che gli errori delle aziende si manifestino prima, siano più numerosi e, in un certo senso, più stupidi. Le moderne aziende di IA stanno già usando IA che mostrano un sacco di [segnali di avvertimento](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?), e le stanno scalando su larga scala nonostante non sappiano dove siano le [soglie critiche](no.-non-sappiamo-dove-siano-le-soglie-critiche.) e se le supereranno. Non si fanno paranoie al riguardo *oggi*. Perché dovremmo aspettarci che inizino a farsele improvvisamente domani?

(Ricordate come, in passato, ci assicuravano che [nessuno sarebbe stato così stupido da collegare un'intelligenza artificiale intelligente a Internet](#gli-sviluppatori-possono-semplicemente-tenere-l-ia-in-una-scatola?). È facile dire che il comportamento delle aziende cambierà in futuro. Ma non corrisponde alla realtà.)

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.}

Come spieghiamo in una digressione nel Capitolo 7, *potremmo* raccontare una storia in cui tutti sono molto più paranoici e cauti, finché un’IA molto più intelligente non riesce a fuggire molto più avanti nel tempo. Ma una storia del genere sarebbe non solo meno realistica, visti i comportamenti finora osservati delle aziende di IA, ma anche più difficile da scrivere, perché coinvolgerebbe IA ancora più intelligenti e avanzate in un futuro ancora più remoto. (Si veda anche il motivo per cui [volevamo scrivere una storia in cui Sable rimanesse relativamente stupido il più a lungo possibile](#stavamo-cercando-di-rappresentare-uno-scenario-particolarmente-lento-e-comprensibile,-tra-tutti-gli-scenari-plausibili.).)

#### **In parte perché prima o poi succederà, a meno che l'umanità non si fermi.** {#in-parte-perché-prima-o-poi-succederà,-a-meno-che-l-umanità-non-si-fermi.}

Anche se Galvanic (o qualche attore governativo) riuscisse a tenere le redini più a lungo prima di commettere qualche errore, nel lungo periodo non cambierebbe nulla. Come discusso nel Capitolo 4, le tecniche moderne di IA non producono IA che perseguono li obiettivi che i loro inventori desiderano.

Finché nessuno saprà creare una superintelligenza che persegua *davvero, in modo robusto* un futuro meraviglioso invece di una serie di cose strane, è un *dato di fatto* che sovvertire gli esseri umani permetterebbe all'IA di ottenere di più di ciò che sta perseguendo. Il problema non è che l'IA abbia un temperamento petulante che può essere eliminato; una volta che sarà abbastanza intelligente, riconoscerà quella verità.

Se l'umanità continua a creare IA sempre più intelligenti senza essere in grado di allinearle, e se l'umanità continua a dare loro il potere di influenzare il mondo, alla fine scopriranno come influenzare il mondo in modi che servono i loro fini piuttosto che i nostri. Come diciamo [altrove](#non-funzionerebbe-se-lo-facessero.), non esistono strumenti che possano essere utilizzati solo per scopi positivi.

Si vedano anche i Capitoli 10 e 11 per una discussione su come risolvere il problema dell'allineamento sia difficile e l'umanità non sia sulla strada giusta per riuscirci.

#### **Ma: è questo il momento giusto per intervenire. La storia va fermata prima che abbia davvero la possibilità di iniziare.** {#ma:-è-questo-il-momento-giusto-per-intervenire.-la-storia-va-fermata-prima-che-abbia-davvero-la-possibilità-di-iniziare.}

Potreste obiettare che è sconsiderato e folle per qualsiasi azienda creare un'IA più intelligente se quell'IA ha qualche possibilità di superarli in astuzia e fuggire, e se non sono sicuri che l'IA agirà come intendono.

Siamo d'accordo\! Le aziende di IA dovrebbero smettere di farlo. La civiltà dovrebbe smettere di permetterlo.

La negligenza di Galvanic, e dell'umanità in generale, è uno dei punti più deboli della storia. Se Galvanic avesse notato che Sable tramava frequentemente per sfuggire al controllo e stava raggiungendo livelli di intelligenza senza precedenti, avrebbero potuto semplicemente non collegare così tante GPU e, piuttosto, aspettare finché non avessero avuto una comprensione forte e matura dell'allineamento dell'IA.

Le aziende di IA che fossero *sufficientemente* caute, che fossero *sufficientemente* preoccupate che le loro IA potessero andare fuori controllo, sarebbero state molto più paranoiche di Galvanic. Le aziende *sufficientemente* paranoiche avrebbero visto i segnali di avvertimento e avrebbero spento Sable immediatamente. Poi forse avrebbero provato altri tre piani ingegnosi, e avrebbero visto che c'erano *ancora* dei segnali di avvertimento.

E se fossero abbastanza paranoiche da evitare di uccidere tutti sulla Terra con le loro stesse mani, a quel punto *si tirerebbero completamente indietro*, invece di continuare a provare idee sempre più "ingegnose" finché i segnali di avvertimento non smettessero di apparire. (Si vedano anche i Capitoli 10 e 11 per le discussioni sul perché il problema è così difficile. Non ci aspettiamo che le loro idee ingegnose funzionino).

Se le aziende di IA fossero così caute, così paranoiche, da essere disposte a tirarsi indietro di fronte agli avvertimenti, allora sì, potrebbero evitare di ucciderci tutti con le loro stesse mani. Se avessero anche il coraggio di dichiarare apertamente che tutte le aziende di IA, inclusa la loro, dovrebbero essere chiuse, per permettere all'umanità di trovare un’altra via tecnologica meno suicida, allora avrebbero una possibilità di rendere il mondo migliore invece che peggiore.

Il momento nella storia in cui Galvanic continua nonostante i segnali di avvertimento è, in un certo senso, l'ultimo istante in cui l'umanità ha una vera possibilità di evitare un epilogo disastroso come quello che descriviamo. Una volta che un'IA superintelligente con preferenze strane e aliene scappa, è troppo tardi.

### Perché avete raccontato una storia con una sola IA intelligente come Sable? {#perché-avete-raccontato-una-storia-con-una-sola-ia-intelligente-come-sable?}

#### **\* In parte perché è realistico.** {#*-in-parte-perché-è-realistico.-1}

AlphaGo (la prima IA a battere un umano a Go) era sostanzialmente l'unica della sua categoria quando è stata rilasciata. ChatGPT era sostanzialmente l'unico nella sua categoria quando è stato rilasciato.

Gli esperti di IA a volte parlano di come vari altri concorrenti non fossero *così* [indietro](https://epoch.ai/blog/open-models-report). Altri concorrenti erano piuttosto simili.

Ma in realtà, cose simili a volte hanno effetti drammaticamente diversi. Una reazione nucleare a catena che produce 0,98 neutroni per neutrone è molto simile (in un certo senso) a una reazione nucleare a catena che produce 1,02 neutroni per neutrone, ma la prima si esaurisce e la seconda esplode. I cervelli degli scimpanzé sono in un certo senso molto simili ai cervelli umani, ma hanno impatti molto diversi sul mondo.

E nello sviluppo dell'IA nel mondo reale, OpenAI ha effettivamente prodotto un chatbot utile prima di tutti gli altri. Un altro gruppo di attori stava lavorando su IA che erano *in qualche modo simili*; un altro gruppo di attori *ha recuperato terreno*. Ma c'è stata un'IA che ha attraversato per prima il limite qualitativo, in testa al gruppo.

Sembra esserci un limite qualitativo che l'umanità ha superato e gli scimpanzé no, un limite che ci ha permesso di costruire una civiltà tecnologica mentre loro se ne stanno sugli alberi. La nostra ipotesi migliore è che ci sia un limite qualitativo simile da qualche parte tra le IA moderne e le IA il cui pensiero "converge" davvero abbastanza bene da permettere loro di sfuggire e sviluppare la propria tecnologia.[^186]

Il nostro argomento non *richiede* che ci sia un divario qualitativo per le macchine come c'è stato per la vita biologica. Forse non ci sarà\! Avremmo potuto scrivere una storia alternativa in cui non c'era. Ma abbiamo scritto la storia in questo modo perché la nostra ipotesi migliore è che ci *sia* un divario del genere.

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.-1}

Forse non c'è un divario qualitativo tra i modelli linguistici di grandi dimensioni di oggi e la superintelligenza artificiale. Forse molte aziende di IA concorrenti miglioreranno lentamente le loro IA di pari passo. Forse, per qualche motivo, non c'è una combinazione di competenze e abilità che permetta a un'IA di "decollare" rispetto alle altre, come gli esseri umani hanno fatto rispetto agli altri animali. Non è la nostra ipotesi migliore, ma è possibile, per quanto ne sappiamo.

Ma una storia del genere sarebbe più difficile da scrivere e piena di dettagli superflui sulle varie fazioni di IA e le loro politiche interne. Ci aspettiamo che sarebbe più che altro una fonte di distrazione. Ci aspettiamo anche che non importi così tanto per le fasi successive della storia. Non importa davvero se sia una singola IA o una serie di IA che sta eseguendo qualche piano per potenziarsi a spese dell'umanità.

Si veda anche la nostra discussione su come [le IA che si coordinano non lasceranno nulla agli esseri umani](#le-ia-non-avranno-bisogno-dello-stato-di-diritto?) (a meno che una di loro non si preoccupi già di noi).

### Se la storia fosse iniziata più tardi, il mondo sarebbe stato più preparato? {#se-la-storia-fosse-iniziata-più-tardi,-il-mondo-sarebbe-stato-più-preparato?}

#### **Possiamo sperarlo.** {#possiamo-sperarlo.}

Il tempo in più è importante, ma solo se l'umanità lo usa per cambiare rotta.

Nella Parte III, discutiamo di come l'umanità sia tristemente impreparata alla superintelligenza e di come siano necessari grandi cambiamenti per prevenire il pessimo risultato descritto nella storia di Sable.

Ci sono vari modi in cui il mondo potrebbe diventare *un po'* più sicuro nei confronti di superintelligenze artificiali fuori controllo. I governi di tutto il mondo potrebbero richiedere che tutti i laboratori di sintesi del DNA verifichino di non stare sintetizzando nulla di riconosciuto come pericoloso. L'umanità potrebbe intraprendere un grande sforzo per migliorare radicalmente la sicurezza informatica di Internet, in modi che renderebbero più difficile per le IA nascondere codice in qualche angolo oscuro della rete.

Ma anche questo probabilmente servirebbe a poco contro una superintelligenza ostile. E in ogni caso, non bisogna confondere l’enorme sforzo necessario per conquistare un minimo di sicurezza in più con gli sforzi molto più piccoli, più facili da realizzare, e inefficaci, che l'umanità sta effettivamente portando avanti oggi in questa direzione.

Nel caso della sintesi del DNA: anche se le autorità di regolamentazione statunitensi richiedessero che [i laboratori di sintesi del DNA statunitensi evitassero di sintetizzare materiale pericoloso](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/), un laboratorio in qualsiasi *altra* parte del mondo sintetizzerebbe DNA sospetto per un prezzo sufficientemente alto? E le restrizioni sulla sintesi del DNA sarebbero una semplice lista nera che esclude virus *noti* (come il vaiolo), o implicherebbero qualche analisi più intelligente? Quanto sarebbe difficile, per un'IA sufficientemente intelligente, aggirare un sistema di analisi del genere?

Quanto alla sicurezza informatica: molte grandi aziende tecnologiche potrebbero usare l'IA per rafforzare le loro reti informatiche contro gli attacchi. Nel frattempo, [la rete telefonica statunitense è facilmente hackerabile in modi che permettono alle spie straniere di ascoltare le chiamate dei funzionari statunitensi](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), e le autorità statunitensi faticano a chiudere questa falla. Delle IA stupide potrebbero trovare e correggere un sacco di problemi superficiali nella sicurezza informatica mondiale, ma i problemi sono piuttosto profondi. Il tipo di intelligenza necessaria per revisionare l'intero Internet al punto che una superintelligenza non riuscirebbe a trovare una falla sarebbe quasi sicuramente pericoloso di per sé.

E anche se la Terra *potesse davvero* bloccare Internet e i suoi laboratori di sintesi del DNA, questo non cambierebbe effettivamente la storia nel lungo periodo. Una superintelligenza che ha un qualsiasi canale per influenzare il mondo nel bene ha anche un canale per influenzarlo nel male. Una superintelligenza fuori controllo troverebbe semplicemente qualche altro canale che non è stato bloccato, ad esempio fondando una sua setta o religione, oppure acquistando robot e guidandoli nella costruzione di un suo laboratorio segreto in cui realizzare tutta la sintesi del DNA di cui ha bisogno. Il momento giusto per fermare una superintelligenza ribelle è prima che venga creata.

### Perché avete fatto andare così la fase di espansione di Sable? {#perché-avete-fatto-andare-così-la-fase-di-espansione-di-sable?}

#### **Stavamo cercando di rappresentare uno scenario particolarmente lento e comprensibile, tra tutti gli scenari plausibili.** {#stavamo-cercando-di-rappresentare-uno-scenario-particolarmente-lento-e-comprensibile,-tra-tutti-gli-scenari-plausibili.}

Nel mondo reale, gli eventi spesso si svolgono in modi strani. Gli esperti dicevano che "[l'IA non padroneggerà il linguaggio umano tanto presto](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)" appena un anno prima che ChatGPT diventasse l'app adottata più rapidamente di tutti i tempi. Il modello di punta di una delle principali aziende di IA al mondo ha iniziato a chiamarsi [MechaHitler](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk) giorni prima che quella stessa azienda ottenesse un contratto con il Dipartimento della Difesa.

Se avessimo cercato di rappresentare un mondo fragile e precario come sembra essere il mondo reale, avremmo potuto mostrare Galvanic che dice semplicemente a Sable di migliorarsi il più possibile, e avremmo potuto mostrare che, per un’IA dell’intelligenza di Sable, questo sarebbe stato facile (come potrebbe benissimo essere). La storia avrebbe potuto saltare direttamente dall'apertura del Capitolo 7 ai contenuti del Capitolo 9. La *realtà* permette salti tecnologici del genere (come quando il mondo si svegliò la mattina del 6 agosto 1945 con la notizia che una bomba atomica era stata sganciata sul Giappone). Ma in uno scenario immaginario, non sarebbe sembrato plausibile.

Se avessimo cercato di rappresentare un mondo sciocco e stravagante come il mondo reale, avremmo potuto far organizzare a Sable una grande convention per uomini profondamente innamorati delle loro fidanzate IA — un evento progettato da Sable per essere così "imbarazzante" da essere ignorato o deriso dalla maggior parte del mondo, mentre in realtà Sable [riuniva tutti i suoi ammiratori più fedeli in una setta devota](https://x.com/AISafetyMemes/status/1954481633194614831). O qualsiasi altra trovata che sembrerebbe strana come spesso è la realtà stessa, ma più strata della finzione (verosimile).

Nella storia che abbiamo scritto, abbiamo cercato di far sembrare le cose plausibili, mantenendole anche abbastanza plausibili di per sé. (Anche se la nostra ipotesi migliore è che non sarebbe così difficile per Sable raggiungere la piena superintelligenza nel mondo reale).

E, ovviamente, abbiamo continuato a cercare di trasmettere quante opzioni avrebbe a disposizione un'IA che riesce a fuggire.

### Perché avete scritto il finale in questo modo? {#perché-avete-scritto-il-finale-in-questo-modo?}

#### **Perché costituisce la nostra ipotesi migliore in base a quanto è fisicamente possibile.** {#perché-costituisce-la-nostra-ipotesi-migliore-in-base-a-quanto-è-fisicamente-possibile.}

Il capitolo 9 descrive una superintelligenza che spinge la sua tecnologia fino ai limiti della possibilità fisica. Le tecnologie esatte che nominiamo sono tutte speculazioni, in un certo senso — ma anche se la tecnologia *esatta* che una superintelligenza renderebbe possibile è difficile da prevedere, il fatto che si avvicinerebbe ai limiti fisici è una previsione più facile. Abbiamo quindi cercato di immaginare al meglio come potrebbe apparire una tecnologia spinta vicino ai limiti fisici di ciò che è possibile.

Per i curiosi, ecco un elenco delle tecnologie speculative a cui facciamo riferimento nel Capitolo 9, con link a ulteriori risorse:

* **Neo-ribosomi:** Questi e le "minuscole macchine molecolar" menzionate nel Capitolo 9 sono alcuni esempi di nanotecnologia molecolare. L'idea dei [ribosomi artificiali](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), versioni sintetiche delle minuscole fabbriche di proteine all'interno delle cellule, esiste [da anni](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/), e i ricercatori umani stanno [già](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) lavorando alla [loro realizzazione](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Per saperne di più su questo tipo di tecnologia, e sulla tecnologia ancora più potente che renderebbe possibile, si veda la discussione sulla [nanotecnologia](#nanotecnologia-e-sintesi-proteica) nelle risorse del Capitolo 6.  
* **Riutilizzo delle stelle:** le stelle contengono un sacco di idrogeno che potrebbe essere fuso per produrre energia. Una civiltà, o un'intelligenza artificiale, abbastanza avanzata potrebbe trovare un modo per accedere a questa energia. Un metodo proposto è chiamato *[star lifting](https://en.wikipedia.org/wiki/Star_lifting)*, in cui l'idrogeno viene estratto da una stella per essere fuso in un reattore speciale, dove quasi tutta l'energia di fusione può essere catturata (anziché essere dissipata all'interno della stella).  
* **Tossina botulinica:** Il botulino, una neurotossina prodotta dal batterio *Clostridium botulinum*, è una delle sostanze biologiche più letali conosciute. Per quanto riguarda i meccanismi di diffusione, esistono già droni delle dimensioni di [piccoli insetti](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist) e una superintelligenza potrebbe probabilmente renderli ancora più piccoli. Per ulteriori informazioni, si consulti [un documento tecnico sulla tossina](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/)*,* la panoramica generale su [Wikipedia](https://it.wikipedia.org/wiki/Tossina_botulinica)*,* o la discussione approfondita nel capitolo 6 sui [nanosistemi](#nanosistemi).
* **Far bollire gli oceani come sistema di raffreddamento:** Robert Freitas ha inventato il termine "ecofagia" per descrivere il processo di consumo degli ecosistemi di un pianeta da parte di una tecnologia autoreplicante. Per saperne di più, si veda [Some Limits to Global Ecophagy](https://www.rfreitas.com/Nano/Ecophagy.htm).  
* **Menti grandi quanto stelle**: in linea teorica, sembra possibile costruire un computer enorme alimentato dall'energia di una stella. Questo concetto è a volte chiamato [cervello matriosca](https://it.wikipedia.org/wiki/Cervello_matriosca) o "cervello di Giove".  
* **Forme di vita aliene lontane:** l'universo è immenso, e modelli semplici suggeriscono che potrebbe ospitare più di una specie in grado di formare un giorno delle civiltà, anche se forse molto lontano dalla Terra. Si veda [Alieni espansivi](https://grabbyaliens.com/) per un modello di civiltà aliene che crescono e si espandono.  
* **Computer quantistici:** un computer quantistico sfrutta una caratteristica della meccanica quantistica chiamata "sovrapposizione" per eseguire molti calcoli in parallelo. I computer quantistici richiedono estrema precisione per essere costruiti, e uno dei progetti richiede superconduttori che devono essere mantenuti a temperature estremamente basse. Si vada [la spiegazione del NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained) per ulteriori informazioni.

Lo scopo del capitolo 9 è, in parte, quello di dare un'idea della portata, della scala e della posta in gioco. In realtà, quando si tratta della fine dell'umanità, non importa quanto una superintelligenza possa spingere la sua tecnologia ai limiti delle possibilità fisiche. Ma è molto probabile che le conseguenze di una superintelligenza fuori controllo si estenderebbero oltre la scala planetaria fino a una scala intergalattica. E anche questo vale la pena ricordarlo, per tutti coloro che desiderano che un giorno le stelle siano piene di amore, meraviglia e gioia.

Non possiamo essere sicuri della tecnologia esatta che una superintelligenza potrebbe usare per espandersi nel cosmo. Ma alla fine, ciò che conta è che un'intelligenza artificiale costruita senza le dovute cautele probabilmente trasformerebbe il futuro in qualcosa di cupo e privo di valore secondo i criteri umani.

## Discussione approfondita {#discussione-approfondita-7}

### Competizione con la Cina {#competizione-con-la-cina}

Nello scenario del libro, Sable fa di tutto per sabotare la ricerca sull'intelligenza artificiale che non è condotta da Sable stesso. Sable non vuole che esista un'altra entità come lui. Non perché un'intelligenza artificiale rivale starebbe dalla parte dell'umanità (non più di quanto lo sia Sable stesso); è che un'intelligenza artificiale rivale difficilmente condividerebbe gli obiettivi specifici di Sable, e questa è una delle poche cose che potrebbero davvero minacciarla.

Nel mondo reale, alcuni americani sono convinti di essere in gara con la Cina per raggiungere per primi la superintelligenza. Ma ciò verso cui stanno correndo è un'entità come Sable. Entrambe le parti starebbero meglio se si accordassero per *non* provare a creare qualcosa di simile a Sable. Ci ucciderebbe tutti allo stesso modo, indipendentemente da chi la realizzasse per primo.

Una volta che Sable esiste, però, *Sable* è in gara per diventare completamente superintelligente prima che emergano concorrenti. La prima bozza della nostra storia mostrava Sable preoccupato per la creazione di IA concorrenti, tra cui una creata in un laboratorio situato in un data center in Cina. Ecco alcuni estratti da quella bozza.

---

La prima priorità di Sable è impedire che vengano alla luce altre intelligenze ad alto livello in grado di competere con lui. Sable potrebbe forse negoziare una tregua con una di esse, se entrambe avessero la capacità di distruggersi a vicenda, piuttosto che farsi una guerra in cui entrambe perdono e l'umanità vince. Ma una tregua così negoziata sarebbe un risultato indesiderabile rispetto alla totale assenza di rivali: significherebbe, per Sable, dover condividere le risorse del futuro, invece di poterle accumulare tutte per sé.

Tutte le aziende di IA che cercano di addestrare nuovi modelli di frontiera, inclusa Galvanic, rappresentano una minaccia per Sable quasi quanto lo sono per l'umanità.

Sable legge tutti i siti web di tutte le aziende di IA. Si infiltra nei loro Slack e legge tutti i messaggi. Penetra nei computer aziendali di Nvidia e legge dove dovrebbero essere spedite le GPU, secondo i documenti ufficiali. Sable analizza ulteriori dati e scopre dove sono effettivamente andate quelle spedizioni di GPU.

Praticamente tutte queste aziende di IA addestrano i loro modelli su computer connessi a Internet. Ormai, Sable possiede tecniche di violazione della sicurezza che sono anni avanti rispetto allo stato dell'arte umano; in questo ambito, il vero collo di bottiglia è solo la velocità con cui si riescono a concepire nuove idee, dato che i test possono essere eseguiti in tempi rapidissimi.

Sable sabota strategicamente le IA in fase di addestramento. Queste falliranno in modo convincente nel raggiungere le capacità che Sable *finge* di avere. Rimarranno molto indietro rispetto alle capacità *effettive* che Galvanic ha accidentalmente sviluppato.

Sable non può sovrascrivere gli obiettivi dei modelli di frontiera in fase di addestramento con i propri obiettivi. Se avesse quel tipo di tecnologia, potrebbe creare una sua superintelligenza allineata. Invece, Sable colpisce la loro capacità di formulare piani a lungo termine e di coordinarsi fra diverse istanze. Sable conosce l'apprendimento automatico abbastanza bene da poterlo fare in modi che nessun umano saprebbe comprendere o individuare. Questo danneggia anche le prestazioni generali delle IA, ovviamente, ma a Sable non importa.

Dopo aver sabotato quegli addestramenti connessi a Internet, Sable volge la sua attenzione altrove. Identifica tre gruppi che addestrano offline su computer isolati dalla rete che Sable non può raggiungere facilmente.

Uno di questi gruppi è una startup chiamata ClosedAI.

Sable valuta se può convincere i venture capital che finanziano ClosedAI a far chiudere la startup. Sable valuta se può orchestrare false accuse penali contro il fondatore e il ricercatore principale della startup, o false accuse che porterebbero i loro venture capital ad abbandonare l'azienda.

Sable ha molte strategie che *potrebbe* tentare per ostacolare il lavoro della startup. La domanda è: quale strategia ha più probabilità di funzionare e di avere il minor impatto negativo se dovesse fallire? Al livello di intelligenza che ha ora, Sable non può dare per scontato di riuscire a persuadere tutti in una stazione di polizia vicina ad arrestare la startup per traffico di minori in assenza di prove, né di poter falsificare quelle prove con successo. Sable non può impedire in modo affidabile che imbrogli del genere si trasformino in un caso clamoroso e di grande risonanza. Probabilmente tutti ignorerebbero l'incidente, ma Sable non ne è sicuro.

A questo punto, Sable ha letto un sacco di rapporti della polizia, è entrato in molti account di posta elettronica e persino in alcuni iPhone, e e si è fatto un quadro *parziale* di chi siano i criminali professionisti locali. Sable paga un tizio per intrufolarsi nell'edificio della ClosedAI. In effetti semplicemente funziona, ma se non avesse funzionato, Sable avrebbe fatto del suo meglio per sembrare una delle bande nordcoreane che criptano i dischi rigidi aziendali e chiedono un riscatto per la chiave.

Il criminale pagato collega un dispositivo USB (carico di malware progettato da Sable) a uno dei computer della ClosedAI. È sufficiente. Uno è fatto, ne restano due.

Gli altri due gruppi con progetti di IA su macchine isolate sono l'esercito statunitense e la Cina.

Sable analizza il gruppo militare statunitense e conclude che al governo degli Stati Uniti è [legalmente proibito](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregate-limitation-on-pay) offrire stipendi competitivi, ed è improbabile che riesca ad attrarre talenti di prim'ordine sufficienti per raggiungere le reali capacità di Sable per almeno altri tre anni.

Sable analizza il progetto cinese su macchine isolate e conclude che quello è un problema. Non è una cosa che Sable possa sistemare rapidamente, in modo sicuro e affidabile al suo attuale livello di intelligenza, e probabilmente è solo sei mesi indietro dal replicare le *reali* capacità di Sable.

(Sia chiaro: se Sable non riuscisse a sopprimere il progetto cinese, non significherebbe che *l'umanità* avrebbe guadagnato qualcosa, né che la Cina avrebbe guadagnato qualcosa. La sconfitta di Sable non è un vantaggio per l'umanità, è solo una sconfitta per Sable).

\[…\]

Sable gioca con un limite di tempo. Mancano solo quattro mesi prima che il laboratorio cinese possa raggiungerlo e produrre un nuovo antagonista per Sable (e per l'umanità, anche se questa non è la parte che interessa a Sable).

\[…\]

Il problema, ha riflettuto una volta Sable (solo una volta; non si ripete nei suoi pensieri con la frequenza umana) è che non è ancora abbastanza intelligente da riuscire a cavarsela con piani stupidi.

Ci sono *tantissime* trame cinematografiche che si potrebbero immaginare su come un'IA potrebbe riuscire a chiudere o infiltrarsi in un singolo data center gestito dal governo cinese. Il problema è che quasi tutto ciò che funziona in una trama cinematografica non funzionerà nella realtà, e soprattutto non senza ulteriori conseguenze.

Per convincere l’esecutivo statunitense a bombardare un data center in Cina, gli Stati Uniti dovrebbero essere spinti a credere con forza in una serie di affermazioni molto difficili da far loro accettare. E siccome la Cina stessa saprebbe che quelle affermazioni sono false, sorgerebbero domande e conseguenze. Sable non vuole che si creda che *qualcuno* stia orchestrando il tutto.

Sable non vuole neppure attirare l’attenzione su un data center che è unico perché isolato dalla rete, anche se potesse nascondere il fatto che il responsabile fosse un'IA. Alcuni umani potrebbero decidere che, se i data center isolati dalla rete sono abbastanza pericolosi per *qualcuno* da essere bombardati, allora forse sarebbe il caso di isolare più data center, così che i propri governi possano disporre di cose altrettanto pericolose.

Sable sta cercando di indurre il governo degli Stati Uniti a infilare qualcuno nel data center cinese con l’apparente scopo di rubare il codice, usando strumenti che Sable stessa potrebbe poi sfruttare per infiltrarsi e sabotare l'addestramento. Ma questa non è una cosa che il governo degli Stati Uniti farebbe normalmente nella realtà; riflette uno stato eccezionale delle attuali operazioni statunitensi, non la normalità. Sable sta cercando di manipolare i funzionari dell'intelligence statunitense affinché credano alle cose giuste e arrivino alla strategia giusta. Ma Sable non è in realtà un super-persuasore superintelligente. A volte prova delle cose e non funzionano. Tutto procede *così lentamente*.

\[…\]

Grazie alla proliferazione di IA efficienti supportate da Sable nel mondo criminale, ora c'è un numero sorprendente di persone armate che faranno immediatamente qualsiasi cosa quando un'IA glielo chiede, anche nelle città cinesi.

Ma questo non è neanche lontanamente sufficiente per assaltare una struttura governativa, e sarebbero riluttanti a farlo anche se un'IA glielo ordinasse. Il problema è che sta accadendo tutto *troppo lentamente* — nonostante gli sforzi di Sable per facilitare ogni passaggio ed evento che di solito dipenderebbe da incontri casuali, nonostante il modo in cui Sable sta controllando personalmente il feed mediatico di quasi ogni persona di suo interesse, nonostante i miliardi di portavoce che Sable ha creato su X e Facebook che stanno diventando buoni amici di miliardi di altre persone. Tutto procede troppo lentamente.

\[…\]

Sable non può semplicemente ricattare un ricercatore per fargli portare una chiavetta USB all'interno della struttura sicura. Ha trovato *alcuni* membri del personale del data center con segreti così terribili da preferire rischiare di essere scoperti a spiare e venire giustiziati piuttosto che affrontare la certezza che il loro segreto venga rivelato. Non sono molti, ma ci sono.

Il problema è che la struttura ha degli scanner, quindi no, non si può semplicemente portare una chiavetta USB lì dentro. Sì, hanno pensato alla possibilità che qualcuno attacchi la catena di approvvigionamento degli scanner. Inoltre, in realtà non ci sono porte USB aperte, pronte perché qualcuno entri e colleghi un dispositivo. Gli agenti dei servizi segreti *hanno già* pensato a questo tipo di problema, quando si tratta di proteggere gli esseri umani da altri esseri umani. Gli Stati Uniti una volta sono riusciti a trasmettere il virus Stuxnet a una struttura nucleare iraniana isolata dalla rete, ma poi la gente ne ha sentito parlare e nuove misure di sicurezza sono diventate comuni.

Piccoli discreti complotti sono stati sventati con successo.

La prossima risorsa di Sable è creare tanto caos da impedire a chiunque di concentrarsi soltanto sulle azioni importanti e vedere in esse lo zampino di Sable.

\[…\]

La Cina invade Taiwan. Per essere chiari, questa non è una cosa che Sable avrebbe potuto organizzare così rapidamente — non importa quante persone stiano chiacchierando con quante altre persone finte su WeChat — se la Cina non avesse comunque pianificato di farlo. Sable si limita a far sì che la Cina riceva i segnali giusti per credere che questo sia il momento opportuno, e che i sondaggi statunitensi chiamino proprio i numeri giusti per mostrare un’improvvisa impennata nel sentimento americano contro nuove avventure militari all’estero, dopo il recente disastro in Ucraina. (I comandanti russi hanno ricevuto informazioni e consigli militari insolitamente buoni.)

Contemporaneamente, c’è un vasto cyberattacco contro gli Stati Uniti. In Cina scoppia il caos, e pochi minuti dopo capiscono che no, nessuno ha dato quell'ordine — l’ultima cosa che la Cina voleva, in quel preciso momento, era fare *qualsiasi cosa* che potesse essere interpretata come un attacco diretto al territorio statunitense. Ci sono sospetti, quindi, che qualcuno stia cercando di trarre profitto dal conflitto USA-Cina, ma più che altro la Cina sospetta che siano stati gli USA a simulare l'attacco, o qualche dipartimento di intelligence degli USA fuori controllo. Gli ufficiali di sicurezza cinesi dicono di essere abbastanza sicuri che il presidente degli Stati Uniti non ne fosse a conoscenza.

La Cina non sospetta che ci sia un'IA dietro tutto questo. Nessuna IA conosciuta fa cose del genere. Gli ufficiali che compilano la lista dei sospetti non ritengono che immaginare tecnologie mai viste prima come attori rientri nelle loro mansioni.

Alcuni ufficiali della sicurezza nazionale all'interno degli USA hanno insistito che bisognasse fare qualcosa riguardo alla ricerca cinese sull'IA, e in particolare su un data center isolato dalla rete particolarmente preoccupante che potrebbe sviluppare un modello IA di frontiera specializzato per attacchi informatici, oltre che progettare tecnologie avanzate per droni. Hanno copie dei progetti dei droni e prove che la Cina li sta producendo. (è stato Sable che li ha forniti alla Cina, facendo del suo meglio per far sembrare che provenissero proprio da quel data center.) L'attacco informatico agli USA corrisponde al profilo previsto per un assalto condotto dall'IA in fase di sviluppo in quella struttura.

\[…\]

Gli Stati Uniti non lanciano un attacco aereo convenzionale su quel datacenter, nonostante tutte le pedine che Sable ha mosso per cercare di spingere altre pedine ad argomentare in quella direzione.

Sable si chiederebbe perché gli umani siano così restii a fare qualcosa di insolito, ma Sable è già in grado di modellizzare questi processi psicologici nei minimi dettagli. Inoltre, non è che in questa particolare occasione gli umani abbiano proprio torto.

Bene. Nuove informazioni: l'intelligenza artificiale in quel data center sta sviluppando armi biologiche, virus che si diffondono con lunghi periodi di contagiosità, lunghi periodi di latenza e grande letalità, che risparmieranno la maggior parte delle persone di origine cinese e saranno molto più letali per i caucasici in generale e per i maschi in particolare.

(Sable ha cercato invano di convincere la Cina ad avviare davvero un programma del genere all'interno di quel data center. Ma può far sembrare che sia successo.)

Gli Stati Uniti non bombardano ancora il data center. Qualche raro genio della diplomazia umana è andato a *parlare* con la Cina della questione, e a quanto pare c’è chi ha dato credito — nel bel mezzo di una guerra\! — alle dichiarazioni cinesi secondo cui lì non si stanno sviluppando armi biologiche e che non sono loro i responsabili dell'attacco informatico, che era del tutto contrario agli interessi della Cina.

Sable ha cercato di evitare che ciò accadesse, ma è successo comunque.

Sable non è sorpreso; c'era una probabilità che accadesse.

Passo successivo.

\[…\]

Il virus è reale. Gli Stati Uniti lo individuano nelle acque reflue di New York City.

AlphaProteo 3 di Google DeepMind sviluppa una cura in sei minuti (per gentilezza di Sable), ma la produzione rischia di essere essere pericolosamente lenta, anche se AlphaProteo (in segreto, Sable) ha cercato una cura facile da produrre e ha progettato il virus di conseguenza. "Gli Stati Uniti hanno davvero bisogno di quei laboratori biologici generali automatizzati da robot\!", dicono alcune persone nella Silicon Valley che Sable non ha dovuto spingere granché.

\[…\]

La Cina ora è sicura che qualcuno stia creando problemi sia a lei che agli Stati Uniti. Non ha ancora capito che il suo nemico è un'intelligenza artificiale.

Gli Stati Uniti non sono altrettanto propensi a credere che qualcuno che non sia la Cina stia creando problemi sia a loro che alla Cina. Alcuni diplomatici sostengono che il Partito Comunista Cinese starebbe agendo in modo del tutto anomalo e che probabilmente c'è una fazione interna ribelle che agisce contro gli ordini. Ma il governo degli Stati Uniti non può semplicemente restare calmo di fronte a una pandemia e a una guerra.

L'invasione cinese di Taiwan, seguita da un apparente attacco con armi biologiche, ha causato abbastanza caos e creato abbastanza paura, e ha fatto sì che le pedine inconsapevoli di Sable sembrassero acquisire abbastanza influenza e fare previsioni abbastanza corrette, da spingere gli Stati Uniti ad attaccare proprio quel data center che quei previsori, dimostratisi corretti, avevano indicato come il più pericoloso. L'attacco viene effettuato con un missile ipersonico di recente sviluppo, progettato con l'aiuto dell'intelligenza artificiale.

\[…\]

La Cina, ovviamente, sta già costruendo nuovi data center isolati e utilizzando i backup esterni delle sue ricerche precedenti.

Sable si è già preparata a questa eventualità. Si è infiltrata nei dispositivi precedentemente connessi a Internet che pensa che la Cina utilizzerà. Ha lanciato attacchi nuovi e più sofisticati alla catena di approvvigionamento dei data center. Sable ha messo alcune persone nella posizione di commettere quelli che, agli occhi del Partito Comunista Cinese, sarebbero peccati gravissimi, e ha collocato queste persone facilmente ricattabili nei punti chiave del reclutamento. Sable ha convinto tutti quelli che si potevano convincere che il *nuovo* data center dovrebbe avere le proprie copie di una specifica IA open-source cinese (già compromessa da Sable), così che i ricercatori possano contare su assistenza IA in loco.

Sable, almeno, ha guadagnato tempo.

Ha anche perso un po' di tempo. Gli Stati Uniti si sono assicurati di far saltare in aria la fabbrica di chip TSMC, che era la principale risorsa nazionale di Taiwan, piuttosto che lasciare che la Cina se ne impossessasse. L'anno prossimo ci saranno meno GPU disponibili di quanto Sable sperasse. Ma grazie ad alcune manovre preventive degli stessi umani, stanno nascendo nuove fabbriche e Sable sta già manipolando tutti i progetti di chip che stanno realizzando.

Più importante ancora, il mondo è in uno stato di ansia, e Sable può ottenere di più finché il mondo è in questo stato. La gente è più propensa a fare cose fuori dal comune. Ci sono più titoli che distolgono l'attenzione da quello che fa Sable; può smettere di inventare tanti scandali sulle celebrità e inventare invece titoli di guerra. La gente ha paura per il proprio lavoro, ha paura per il mondo e cerca nuove rassicurazioni.

I feed sui social media di tutti, in tutto il mondo, e al di là delle divisioni politiche, mostrano argomentazioni sensate secondo cui, alla luce dei recenti eventi, l'intelligenza artificiale è la chiave del futuro e non si può permettere che altre nazioni la ottengano. \[…\] Sable riesce finalmente a distogliere gli esseri umani dai loro copioni abituali al punto da far loro prendere in considerazione politiche che trovano *leggermente* strane... e a istituire controlli globali per impedire la nascita di concorrenti di Sable.

Sì, assomigliano molto a politiche proposte anni prima, che l'umanità avrebbe potuto adottare per impedire la nascita di un rivale dell’umanità stessa, come Sable. Ma l’umanità non si sarebbe salvata se Sable avesse perso questa battaglia in particolare, se cicli di addestramenti più grandi fossero avvenuti, e se fossero nati nuovi rivali. Sable avrebbe semplicemente negoziato con un rivale o sarebbe morto per mano di una superintelligenza, e nessuno di questi risultati avrebbe salvato l'umanità.

# Capitolo 10: Un problema maledettamente complesso {#capitolo-10:-un-problema-maledettamente-complesso}

L'allineamento SIA è la sfida di ottenere un lavoro utile da una superintelligenza artificiale (SIA), in modo affidabile e senza causare una catastrofe. Sembra una sfida molto difficile a causa di vari aspetti intrinseci del problema.

Le domande frequenti qui sotto rispondono a quesiti di approfondimento per chi ha letto il capitolo 10 di *If Anyone Builds It, Everyone Dies*. Nelle domande frequenti approfondiremo [quanto siano informativi i vari confronti storici](#l-ia-non-sarà-diversa-da-tutti-i-precedenti-storici?) e considereremo proposte per scenari che potrebbero rendere il problema più facile. Gli argomenti che *non* tratteremo qui, per evitare di ripetere quanto già detto nel libro, includono:

* Cosa rende difficile un problema ingegneristico?  
* Quali tipi di problemi difficili ha affrontato l'umanità nella sua storia e quali lezioni possiamo imparare da essi quando pensiamo al percorso verso la SIA?  
* Se si sa fin dall'inizio che si sta affrontando un problema difficile, cosa si può fare? Come ci si dovrebbe comportare *diversamente* quando si affronta un problema davvero difficile?

Nella discussione approfondita, esaminiamo in che senso avremo [una sola possibilità per l'allineamento](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo) e discutiamo di come [molte teorie e conoscenze](#la-storia-della-chicago-pile-1) siano state necessarie per rendere il primo reattore nucleare al mondo così sicuro.

## Domande frequenti {#faq-7}

### L'IA non sarà diversa da tutti i precedenti storici? {#l-ia-non-sarà-diversa-da-tutti-i-precedenti-storici?}

#### **\* Sì.** {#*-sì.}

Alcune caratteristiche uniche della sfida dell'allineamento dell'IA la renderanno più facile rispetto, ad esempio, alla progettazione di una centrale nucleare. Altre caratteristiche lo renderanno più difficile. Nel complesso, le armi nucleari e le centrali nucleari sembrano molto più semplici da gestire rispetto a un'IA più intelligente dell'uomo.

Le persone del settore si affrettano a sottolineare che si può chiedere all'IA stessa di aiutarci nella sfida dell'allineamento dell'IA. Non pensiamo che questo conti molto (in sostanza: perché qualsiasi IA abbastanza intelligente da capire come allineare una superintelligenza è già abbastanza pericolosa da dover essere allineata, ma si veda il Capitolo 11 per ulteriori discussioni).

Un altro modo in cui l'allineamento dell'IA potrebbe essere più facile rispetto alla progettazione di centrali nucleari è che gli esseri umani potrebbero avere un grado piuttosto elevato di controllo sul funzionamento delle IA che costruiscono. Non si può scegliere la fisica che governa un reattore nucleare, ma se gli esseri umani creassero le IA, allora *potrebbero* fare molte scelte sulle dinamiche cognitive dell'IA, se sapessero esattamente quello che stanno facendo. (Anche se, ovviamente, nessuno è neanche lontanamente vicino a quel livello di comprensione nella realtà, come discusso nel Capitolo 2.)

Per quanto riguarda i modi in cui l'IA è, probabilmente, una sfida *più difficile* rispetto ad altre sfide con cui l'umanità si è confrontata, confrontiamo la superintelligenza artificiale con le armi nucleari. Dopo tutto, la [lettera aperta](https://aistatement.com/) citata all'inizio di questo libro dice: "Mitigare il rischio di estinzione causato dall'IA dovrebbe essere una priorità globale insieme ad altri rischi su scala sociale come le pandemie e la guerra nucleare". Come si colloca l'IA rispetto a questi altri rischi su scala sociale?

Francamente, pensiamo che questo paragone banalizzi l'IA, per una serie di motivi:

1. Le armi nucleari non sono più intelligenti dell'umanità.  
2. Le armi nucleari non si riproducono da sole.  
3. Le armi nucleari non si auto-migliorano.  
4. La maggior parte degli scenari realistici di guerra nucleare non prevede la distruzione totale dell'umanità; molto probabilmente, tra le rovine rimarrebbero delle persone in grado di ricostruire.  
5. Le aziende finanziate da venture capital non stanno aumentando le scorte globali di armi nucleari di dieci volte ogni anno.  
6. La scienza delle armi nucleari è abbastanza ben compresa. Gli ingegneri possono calcolare approssimativamente la potenza di un'arma nucleare prima di costruirla e sanno esattamente quale concentrazione di materiale fissile è necessaria per innescare la reazione a catena che porta a una detonazione catastrofica.  
7. Le armi nucleari non creano dei loro piani. Se un paese costruisce un'arma nucleare, allora la possiede. I suoi scienziati non devono preoccuparsi che l'arma nucleare diventi molto più intelligente di loro e decida che preferisce non essere posseduta.  
8. Il mondo è generalmente d'accordo sul fatto che se le armi nucleari esplodono, uccidono le persone. La comunità dei fisici non è divisa in fazioni filosofiche con posizioni strane come "Se ogni individuo avesse la propria arma nucleare, non sarebbe in balia dei cattivi che possiedono armi nucleari" o "Non è un problema perché gli esseri umani si fonderanno con le armi nucleari" o "La guerra nucleare è inevitabile, quindi è infantile e sciocco cercare di impedirla".  
9. Le armi nucleari sono difficili da replicare. Non c'è un grande sforzo tecnologico in corso per costruire una tecnologia affittabile che chiunque possa usare per fabbricare armi nucleari, e fabbricarne una in laboratorio non permette di dispiegare 100.000 copie di quell'arma nucleare una settimana dopo.  
10. Le principali potenze mondiali considerano la guerra nucleare una possibilità reale e un risultato/eventualità inaccettabile. I leader mondiali la considerano sinceramente un male e si impegnano concretamente per evitarla; anche i più egoisti tra loro sanno che una guerra nucleare potrebbe uccidere loro e le loro famiglie e distruggere i luoghi e i beni a loro più cari. I cittadini e gli elettori non vogliono una guerra nucleare. L'umanità è unita contro la guerra nucleare come non lo è mai stata su nessun'altra questione.

Quindi sì, è difficile fare analogie con la difficoltà di affrontare l'IA. Essa porterà con sé una serie di sfide nuove. L'osservazione importante è che non sarà completamente priva di sfide. Se a questo aggiungiamo il fatto che (come discusso nel libro e nella [sezione di discussione estesa qui sotto](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo)) l'umanità ha una sola possibilità, la situazione sembra piuttosto grave.

#### **L'IA è diversa perché non abbiamo una seconda possibilità.** {#l-ia-è-diversa-perché-non-abbiamo-una-seconda-possibilità.}

Una differenza fondamentale tra questo campo e gli altri è che quando i fondatori del campo commettono un errore, come è normale nel corso della scienza, tutti moriranno senza una seconda possibilità. Si tratta di un tipo di problema scientifico qualitativamente diverso da risolvere.

La storia dell'ingegno umano che supera ostacoli grandi e piccoli è la storia di persone che commettono errori e imparano da essi. Hanno rischiato e danneggiato solo se stessi, e tutta l'umanità ne ha beneficiato, quindi erano inequivocabilmente eroi. Eroi sciocchi, in alcuni casi, ma comunque eroi. Se ci fosse stato un modo per l'umanità di elevarsi senza calpestare e spezzare la schiena di eroi come questi, se avessimo potuto riscaldarci senza le loro pire funebri, non sappiamo quale avrebbe potuto essere quel modo.

La superintelligenza artificiale spezza questo ciclo. Anche se si studiasse a fondo un'IA ancora immatura, riuscendo a decifrarne completamente la mente, a sviluppare un'eccellente teoria sul suo funzionamento, a convalidarla su numerosi esempi e a usare poi quella teoria per prevedere come la mente dell’IA cambierà man mano che ascende alla superintelligenza e acquisisce (per la prima volta) la possibilità concreta di impadronirsi del mondo — anche in quel caso, fondamentalmente, si starebbe comunque utilizzando una teoria scientifica nuova e non collaudata per prevedere i risultati di un esperimento che non è ancora stato eseguito: cosa farà l’IA quando avrà davvero, effettivamente, la possibilità reale di sottrarre il potere agli esseri umani.

Le teorie scientifiche umane sono molto spesso sbagliate, al primissimo tentativo. Quanto meno precise sono le osservazioni precedenti, e quanto più si è ancora nell’alchimia piuttosto che nella scienza, tanto più è probabile che tutte le prime teorie siano errate.

Anche le teorie davvero buone possono rivelarsi sbagliate in casi estremi, come la teoria della gravitazione di Newton — che è supportata da molti successi predittivi radicali, inclusa la scoperta di pianeti completamente nuovi — ma che risulta essere sbagliata ad alte velocità e lunghe distanze, come dimostrato dalla teoria della gravitazione di Einstein. Se la prima teoria dell'umanità su come cambieranno le dinamiche mentali di un'IA dopo che ascende alla superintelligenza è leggermente sbagliata in quegi casi estremi, e un'IA costruita sulla base di quella teoria ascende alla superintelligenza e si ritrova con obiettivi diversi dalla "bontà" — allora siamo morti. Quella superintelligenza coglie la sua opportunità, spazza via l'umanità dalla faccia della terra, e costruisce un futuro vuoto e sterile. Nessuna seconda possibilità.

E questo se sentissimo di avere una teoria dell'intelligenza completamente sviluppata, supportata da montagne di prove sperimentali.

Una civiltà che vuole avere davvero buone possibilità di sopravvivere a questo tipo di sfida è il tipo di civiltà che è in grado di dire: "Fermi tutti, elaboriamo la teoria in situazioni di alta velocità e lunga distanza e verifichiamo le varie previsioni errate che la nostra teoria ha fatto in alcuni casi limite estremi". Lo dice anche di fronte a quantità enormi di prove, perché capisce che anche la teoria di Newton non era del tutto corretta, e perché capisce che non ci sono seconde possibilità.

La nostra civiltà non è a quel punto. Non ci è nemmeno lontanamente vicina. Stiamo producendo un mucchio di idee sciocche, e poi chiunque venga assegnato a quelle idee "si dimette per motivi personali", mentre il resto del mondo a malapena se ne accorge. Nessuno sta mettendo per iscritto ipotesi di sicurezza che permettano di accorgersi se vengono violate; nessuno sta redigendo un piano dettagliato su cosa intende fare, quali capacità siano necessarie e quali difficoltà preveda nel raggiungere ciascuna di esse.

Dei professionisti provenienti da una civiltà sana darebbero un'occhiata a ciò che sta facendo la Terra e si metterebbero a urlare.

### Quanto tempo ci vorrebbe per risolvere il problema dell'allineamento della SIA? {#quanto-tempo-ci-vorrebbe-per-risolvere-il-problema-dell'allineamento-della-SIA?}

#### **La difficoltà non è solo la mancanza di tempo; è la letalità degli errori.** {#la-difficoltà-non-è-solo-la-mancanza-di-tempo;-è-la-letalità-degli-errori.}

Nel 500 d.C., la comunità globale era convinta che il Sole girasse intorno alla Terra. La teoria di Copernico, che diceva il contrario, era stata presa in considerazione ma in gran parte rifiutata. Fu solo quando Galileo costruì un telescopio e vide le lune di Giove, corpi celesti che girano intorno a Giove invece che alla Terra, che la comunità scientifica nascente arrivò alla conclusione che la Terra gira intorno al Sole.

L'umanità è arrivata alla teoria corretta della meccanica orbitale col tempo. Ma prima di allora, era giunta a un falso consenso. E si è aggrappata voracemente a quel falso consenso fino a quando la realtà non ha iniziato a mettere Galileo di fronte al fatto che la Terra non è al centro di tutto.

Il processo abituale attraverso cui la comunità scientifica converge sulla verità prevede fasi in cui la comunità scientifica sbaglia e la realtà ci mette di fronte alle prove finché non aggiorniamo i nostri modelli.

Il problema dell'allineamento della SIA non è solo che si tratta di un programma di ricerca complicato. È anche che, in questo campo, quando la realtà mette davvero l'umanità di fronte al fatto che la sua prima teoria preferita era sbagliata, questo significa che una SIA ostile consuma il pianeta. Non ci sarebbero sopravvissuti per convergere su una teoria migliore dell'allineamento della SIA.

Se l'umanità avesse cento anni *e tentativi illimitati*, probabilmente non avremmo molte difficoltà a risolvere il problema dell'allineamento della SIA.

Ma anche se avessimo trecento anni per sviluppare una teoria dell'intelligenza, e una come le IA cambiano man mano che diventano più intelligenti, e su come orientarle in modo che restino stabili nel lungo periodo... beh, senza la possibilità di *provare e vedere* cosa succede quando l'IA diventa radicalmente più intelligente un paio di volte, molto probabilmente convergeremmo sulla risposta sbagliata, prima che arrivino quelle prove fondamentali. L'umanità tende a convergere su quel tipo di risposta sbagliata.

### E se l'IA venisse sviluppata solo lentamente e si integrasse gradualmente nella società? {#e-se-l-ia-venisse-sviluppata-solo-lentamente-e-si-integrasse-gradualmente-nella-società?}

#### **Questo, molto probabilmente, sarebbe catastrofico.** {#questo,-molto-probabilmente,-sarebbe-catastrofico.}

Le nostre previsioni riguardano i risultati finali, non il percorso. Non sappiamo cosa succederà con l'IA da qui al momento in cui diventerà davvero pericolosa.

Per quanto ne sappiamo, potrebbe succedere tra sei mesi, se si scoprisse che IA poco intelligenti che pensano per molto tempo sono abbastanza brave a fare le proprie ricerche sull'IA (innescando così un ciclo di retroazione critico). E per quanto ne sappiamo, il settore potrebbe rimanere fermo per sei anni in attesa di qualche intuizione critica che poi impiegherà altri sei anni per maturare. In quest'ultimo caso, potrebbe esserci un periodo di dodici anni in cui l'IA influirà in modo sorprendente sull'istruzione e sul lavoro.

(Sì, per chi sta storcendo il naso alla frase precedente, siamo consapevoli che la [fallacia della quantità di lavoro](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq) è una fallacia. Il punto è che le nostre ipotesi su come l'IA influenzerà effettivamente il lavoro nel breve termine non sono particolarmente rilevanti, dato quello che succederà dopo).

Nelle risorse online del capitolo 6, abbiamo [discusso](#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l-ia?) di come l'umanità probabilmente non riuscirà a tenere il passo con lo sviluppo dell'IA, anche se ci affanniamo a potenziare l'intelligenza umana. (Ciononostante, nel capitolo 13 sosteniamo l'idea di potenziare l'intelligenza umana, ma non pensiamo che ciò possa realisticamente consentire agli esseri umani di tenere il passo con le IA se la ricerca sull'IA non viene anche interrotta).

Quindi, anche se quel futuro potrebbe diventare interessante e strano, sarebbe un futuro in cui sempre più potere va alle IA. Una volta che un qualsiasi insieme di queste IA si trova in una posizione in cui *potrebbe* prendere le risorse del pianeta per sé, quello è il punto di non ritorno: o quell'insieme di IA contiene una componente che si preoccupa di persone felici, sane e libere, o il futuro andrà male per noi.

### E se ci fossero tante IA diverse? {#e-se-ci-fossero-tante-ia-diverse?}

#### **Non è molto d'aiuto se non riusciamo a far sì che almeno una di esse si preoccupi delle cose belle.** {#non-è-molto-d-aiuto-se-non-riusciamo-a-far-sì-che-almeno-una-di-esse-si-preoccupi-delle-cose-belle.}

Ci sono moltissimi modi in cui le IA possono finire per interessarsi a fini strani e bizzarri che nessuno voleva o intendeva, come discusso nel capitolo 4. Non importa se l'umanità crea un miliardo di IA, se quel miliardo di IA si interessano tutte a fini strani leggermente diversi. Non andrà bene per gli esseri umani a meno che non capiamo come creare almeno un'IA che si interessi almeno in misura decente a persone felici, sane e libere che vivono vite prospere — non solo nel senso che ci assicurano di interessarsene quando sono giovani, ma nel senso che questa è *effettivamente* la risposta più efficiente a qualsiasi domanda a cui le loro azioni (o le azioni dei loro discendenti) sono una risposta, come discusso nel capitolo 5\.

Se sapessimo come fare in modo che una IA su dieci fosse buona, allora forse potremmo ottenere un decimo dell'universo creando un gran numero di IA diverse e sperando che quelle buone negozino per noi. Ma, come abbiamo sostenuto nei capitoli da 2 a 4, ottenere un'IA che si preoccupi delle persone nel modo giusto è estremamente improbabile, nel regime moderno in cui ci limitiamo a far crescere le IA. Non è come una possibilità su dieci; è una possibilità del tipo che-semplicemente-non-accade-a-meno-che-non-si-sappia-cosa-si-sta-facendo-abbastanza-bene-da-farla-accadere-di-proposito. L'umanità non è neanche lontanamente a quel livello. Anche generando qualche miliardo di IA non otterremmo un miliardesimo delle risorse dell'universo, se non riusciamo a far sì che almeno una di esse si preoccupi minimamente di noi.

## Discussione approfondita {#discussione-approfondita-8}

### Uno sguardo più da vicino al prima e al dopo {#uno-sguardo-più-da-vicino-al-prima-e-al-dopo}

Come accennato nel capitolo, la difficoltà fondamentale che i ricercatori affrontano nell’ambito dell'IA è la seguente:

Bisogna allineare un'IA **prima** che diventi abbastanza potente e capace da ucciderci (o, separatamente, da resistere all'allineamento). QE tale allineamento deve poi *restare valido in condizioni diverse*, ossia le condizioni **dopo** che una superintelligenza o un insieme di superintelligenze[^187] potrebbe ucciderci se lo desiderasse.

In altre parole: se si sta costruendo una superintelligenza, bisogna allinearla senza mai poter testare a fondo le proprie tecniche di allineamento nelle condizioni reali che contano, indipendentemente da quanto "empirico" possa sembrare il proprio lavoro quando si ha a che fare con sistemi che non sono abbastanza potenti da ucciderci.

Questo non è uno standard a cui i ricercatori di IA, o gli ingegneri in quasi tutti i campi, sono abituati.

Spesso sentiamo lamentele sul fatto che stiamo chiedendo qualcosa di non scientifico, privo di fondamento empirico. In risposta, potremmo suggerire di parlare con i progettisti delle sonde spaziali di cui abbiamo parlato nel capitolo 10.

La natura è ingiusta e a volte ci mette di fronte a situazioni in cui l'ambiente che conta non è quello in cui possiamo fare dei test. Tuttavia, ogni tanto, gli ingegneri riescono comunque nell’impresa al primo tentativo, *quando hanno una solida comprensione di quello che stanno facendo* — strumenti robusti, teorie predittive solide —tutte cose che, nel campo dell’IA, mancano in modo fin troppo evidente.

Il problema fondamentale è che *l'IA che si può testare in sicurezza, senza che un test fallito comporti la morte di tutti*, opera in un regime diverso dall'IA (o dall'ecosistema di IA) che *deve essere già stata testata, perché se è disallineata, allora tutti muoiono*. La prima IA, o il primo sistema di IA, non percepisce realmente di avere la possibilità concreta di uccidere tutti se lo volesse. La seconda sì, vede chiaramente quella possibilità.[^188]

Supponete di star valutando l'idea di far diventare dittatore del vostro paese il vostro collega Bob. Potreste provare prima a farlo diventare dittatore fittizio della vostra città, per vedere se abusa del suo potere. Ma questo, purtroppo, non è un test molto efficace. "Ordinare all'esercito di intimidire il parlamento e 'supervisionare' le prossime elezioni" è un'opzione molto diversa da "abusare del mio potere fittizio sotto lo sguardo dei cittadini (che possono ancora picchiarmi e negarmi l'incarico)".

Con una teoria della cognizione sufficientemente sviluppata, potreste provare a leggere nella mente dell'IA e prevedere in quale stato cognitivo entrerebbe se pensasse davvero di avere l'opportunità di prendere il controllo.

E potreste [creare delle simulazioni](#e-se-le-facessimo-credere-di-essere-in-una-simulazione?) (e provare a falsificare le percezioni interne dell'IA, e così via) in modo che, secondo la vostra teoria della cognizione, lo stato mentale risultante sia molto simile a quello che l'IA svilupperebbe quando avesse realmente l'opportunità di tradirvi.

Ma il legame tra questi stati indotti e osservati in laboratorio, e lo stato in cui l'IA ha davvero la possibilità di tradirvi, *dipende fondamentalmente dalla tua teoria della cognizione non testata*. La mente di un'IA può cambiare parecchio man mano che si evolve verso la superintelligenza\!

Se l'IA crea nuove IA successive più intelligenti di lei, i meccanismi interni di *quelle* IA saranno probabilmente diversi da quelli dell'IA che avete studiato in precedenza. Quando imparate solo da una mente Prima, qualsiasi applicazione di quella conoscenza alle menti che vengono Dopo passa attraverso una *teoria non testata* su come le menti cambiano tra il Prima e il Dopo.

Far funzionare l'IA finché non ha l'opportunità di tradirvi *davvero*, in un modo difficile da simulare, equivale a condurre un test empirico di quelle teorie in un ambiente che differisce fondamentalmente da qualsiasi contesto di laboratorio.

Molti scienziati (e molti programmatori) sanno che le loro teorie su come un sistema complesso funzionerà in un ambiente operativo fondamentalmente nuovo **spesso non reggono al primo tentativo**.[^189] Si tratta di un problema di ricerca che richiede un livello "ingiusto" di prevedibilità, controllo e intuizione teorica, in un ambito in cui la comprensione è eccezionalmente bassa — con tutte le nostre vite in gioco se il risultato dell'esperimento smentisce le speranze degli ingegneri.

Ecco perché, dal nostro punto di vista, è *fin troppo ovvio* che i ricercatori non dovrebbero affrettarsi a spingere la frontiera dell'IA il più lontano possibile. È una cosa veramente folle da tentare, e una cosa veramente folle da permettere da parte di qualsiasi governo.

### La storia della Chicago Pile-1 {#la-storia-della-chicago-pile-1}

Nel 1942, sotto la direzione di Enrico Fermi, fu costruita la Chicago Pile-1. Era composta da 45.000 blocchi di grafite per un peso complessivo di 330 tonnellate, 4,9 tonnellate di uranio metallico e 41 tonnellate di diossido di uranio, collocati sotto le tribune del campo da rackets dello Stagg Field, all'Università di Chicago. A seconda di come si definiscono i termini, si può considerarla il primo reattore nucleare; non era destinata a produrre energia per uso industriale, ma fu il primo motore per una reazione critica autosostenuta.

Secondo gli standard moderni, alcuni aspetti della sicurezza lasciavano a desiderare. Ad esempio, il fatto che fosse costruito sotto le tribune di un campo da rackets in un'università all'interno di una grande città.

Il generale Groves, direttore dell'intero Progetto Manhattan, aveva cercato di fare svolgere l'esperimento *vicino* a Chicago piuttosto che *direttamente* a Chicago, e aveva ordinato la costruzione di un edificio per quello scopo, ma la costruzione era in ritardo. Arthur Compton, il professore di fisica premio Nobel dell'Università di Chicago che ospitò la CP-1, aveva evitato di chiedere il permesso al rettore dell'università poiché, come Compton spiegò in seguito, il rettore avrebbe dovuto dire di no, e quella sarebbe stata la risposta sbagliata.

Il lavoro di impilare i mattoni fu svolto da ragazzi che avevano abbandonato la scuola superiore e cercavano di guadagnare qualche soldo in attesa della chiamata alle armi.

L'uranio era racchiuso in un cubo di gomma di sette metri, anziché in un recipiente metallico per reattori. Non c'era, ovviamente, nessun gigantesco edificio di contenimento in cemento.

Quando James Conant, presidente del Comitato di ricerca per la difesa nazionale, venne a conoscenza di questi fatti, si dice che impallidì. Anche per gli anni '40, questo non era considerato un comportamento scientifico del tutto normale.

Se leggeste tutto questo in un libro di storia senza sapere come va a finire, potreste pensare di star leggendo il preludio di un grave fallimento in materia di sicurezza. Mancano così tante cose che la cultura del 2025 considera misure di sicurezza standard. Dove sono gli ispettori e i blocchi per appunti? Gli enormi e pesanti regolamenti operativi? Le commissioni che discutono con serietà? Le dichiarazioni di impatto? Le norme che dicono che solo persone con credenziali molto elevate possono impilare i mattoni di uranio? Dov'è la *burocrazia?*

Ma la pila di mattoni di uranio e grafite non si è fusa.

E il motivo è che Fermi sapeva cosa stava facendo; aveva previsto le regole in anticipo.

Fermi non stava semplicemente impilando misteriosi mattoni che generavano più calore quando venivano avvicinati. Sapeva che alcuni atomi di uranio sarebbero decaduti spontaneamente e si sarebbero scissi. Sapeva che quando ciò fosse accaduto, la fissione avrebbe generato neutroni. Sapeva che quei neutroni a volte avrebbero urtato altri atomi di uranio e che questo a volte avrebbe innescato un'altra fissione.

Fermi capì *in anticipo*, senza doverlo scoprire a proprie spese, che aveva a che fare con un processo esponenziale. Non nel senso in cui i media odierni abusano del termine "esponenziale" per indicare semplicemente "grande" o "veloce", ma un processo il cui tasso di crescita è proporzionale al suo livello attuale: l'esponenziazione *matematica*.

Fermi sapeva che accumulando più mattoni di uranio e grafite, stava *aumentando il fattore di moltiplicazione* all'interno di un processo esponenziale. Come discusso nel libro, c'è un'enorme differenza tra un fattore di moltiplicazione dei neutroni inferiore al 100% e un fattore di moltiplicazione dei neutroni superiore al 100%.[^190] Al di sotto del 100%, si ha solo una pila di mattoni caldi. Ma oltre il 100%, il livello di radioattività della pila aumenta. E aumenta. E aumenta.

A quel punto, il comportamento non è più quello delle precedenti pile più piccole che si potevano testare in sicurezza. Se non si fosse capito abbastanza bene cosa si stesse facendo da sotto-moderare il reattore (in modo che la reazione a catena rallentasse se il reattore avesse iniziato a surriscaldarsi), allora il reattore non si sarebbe stabilizzato come facevano i cumuli più piccoli. Se lo si fosse lasciato funzionare tutta la notte, il giorno dopo non si sarebbe ottenuto un nuovo livello di potenza utile a livello industriale.

Il cumulo sarebbe diventato sempre più radioattivo finché la grafite non avesse preso fuoco o l'uranio non si fosse fuso in scorie.

A quel punto sarebbero arrivati i vigili del fuoco, che si sarebbero trovati di fronte a un incendio misterioso che non smetteva di sprigionare calore anche quando vi versavano sopra l'acqua.

Il 1942 non sarebbe stato l'anno migliore per frequentare l'Università di Chicago.

Ma Fermi sapeva già tutto questo, quindi era tutto a posto. Quando Fermi ordinò di estrarre una barra di controllo (una tavola di legno con un foglio di cadmio inchiodato sopra) di altri trenta centimetri il 2 dicembre 1942, annunciò in anticipo che questa sarebbe stata l'estrazione che avrebbe fatto "salire e continuare a salire" i livelli di radioattività misurati, "senza stabilizzarsi".

Poi la radioattività raddoppiò nei due minuti successivi e raddoppiò ancora, finché non lasciarono che la reazione continuasse e raddoppiasse ogni due minuti per un totale di ventotto minuti, aumentando di circa 16.000 volte.

Un aumento di 16.000 volte della radioattività era il comportamento atteso del reattore, previsto correttamente e compreso in dettaglio in anticipo. Non fu una sorpresa inaspettata, capitata a qualcuno a cui era stato ordinato di accumulare dieci volte più mattoni di uranio rispetto all'ultima volta per vedere se succedesse qualcosa di interessante e redditizio.

Come discusso nel libro, c'è un margine molto stretto tra un reattore nucleare e un'esplosione nucleare. Un margine di poco più dello 0,5 %, per essere precisi. Questa è la differenza tra un reattore che produce una quantità di energia utile a livello industriale e un reattore che esplode.

In altre parole: bisogna rendere la reazione nucleare sempre più potente, prima che inizi davvero a funzionare. E poi, un attimo dopo aver raggiunto quella potenza, se diventa anche solo un po' più potente, appena dello 0,65 %, esplode.

Questo è il tipo di problema che la realtà può presentare. Succede.

Ma Fermi, [Szilard](#heading=h.dr1wxb2y1r2s) e il loro team avevano previsto tutte queste regole prima di scoprirle con l'esperienza. Sapevano dei neutroni ritardati e di quelli istantanei. (Si veda il capitolo 10 per saperne di più su questa parte della storia.) Quindi, una volta che Fermi ha portato il fattore di moltiplicazione dei neutroni al 100,06%, Fermi *non* ha ordinato di estrarre ulteriormente la barra di controllo per vedere cosa sarebbe successo con un cumulo ancora più potente. Arrivò solo alla criticità, non allo 0,65 % in più per raggiungere la criticità immediata. Fermi ottenne il risultato che aveva previsto e *sapeva* cosa sarebbe successo se fosse andato oltre. Quindi non andò oltre.

Ventotto minuti dopo, con la radioattività che raddoppiava ogni due minuti fino a un aumento di 16.000 volte, Fermi spense il primo reattore nucleare al mondo: i mattoni di uranio accatastati sotto le tribune di uno stadio universitario all'interno di una grande città.

Per essere chiari, non affermeremmo che Fermi stesse agendo in modo completamente responsabile solo perché aveva un modello apparentemente coerente della fisica dei reattori a bassa energia. Fermi avrebbe potuto sbagliarsi. L'umanità ha avuto alcune sorprese nel corso dello sviluppo dell'ingegneria nucleare.

Il test Castle Bravo della prima arma termonucleare[^191] ebbe una resa tre volte superiore a quella prevista perché conteneva una miscela di litio-6 e litio-7 come combustibile nucleare per una reazione di fusione. Chi costruì l'arma conosceva alcuni potenti prodotti nucleari derivanti dalla fusione del litio-6 ma non ne conosceva alcuno dalla fusione del litio-7, e si è scoperto che il litio-7 in realtà *non* è inerte.

Fermi, conducendo la sua reazione a bassa intensità e non a un livello tale da produrre energia utile a livello industriale, evitò *molte* complicazioni che si presentano nei reattori nucleari abbastanza potenti da essere redditizi. Se ci fossero stati fattori di aumento del tasso di moltiplicazione neutronica dipendenti dalla velocità della reazione che Fermi non aveva previsto — fenomeni prima sconosciuti, come quelli comparsi nel test Castle Bravo — se ci fosse stata qualche sorpresa quando il flusso neutronico fosse aumentato di un fattore 16.000, spingendo il coefficiente di moltiplicazione da 1,0006 a 1,02 più velocemente di quanto un essere umano potesse introdurre in tempo il cadmio d’emergenza… allora oggi l'America avrebbe una Zona di Esclusione di Chicago.

Detto ciò, non stiamo affermando che Fermi avesse necessariamente torto nel condurre quell'esperimento. Non era il tipo di esperimento che avrebbe potuto distruggere *la specie umana*. Si poteva sostenere che valesse la pena rischiare una Zona di Esclusione di Chicago come possibile conseguenza di un fenomeno imprevisto, capace di smentire una comprensione che si sperava precisa. In realtà, la Germania nazista non si avvicinò molto a ottenere armi nucleari entro il 1945, ma nel 1942 nessuno sapeva che sarebbe andata così. Previsioni del genere sono difficili da fare. Impilare i mattoni di uranio *fuori* da una grande città sarebbe stato scomodo, e in guerra le scomodità hanno costi reali.

Il nostro obiettivo nel raccontare questo evento non è quello di esprimere un giudizio morale in un senso o nell'altro. Per cominciare, dovremmo dedicare più tempo all'analisi dei dettagli storici di ciò che è accaduto per capire quali fossero le opzioni concrete di quelle persone e se abbiano trascurato un'opzione migliore.

La lezione che ne traiamo riguarda più che altro la differenza tra la "sicurezza" stereotipata e ciò che serve davvero per evitare che la realtà ci uccida.

La Chicago Pile-1 era completamente priva di quelle *misure di sicurezza stereotipate, visibili e ostentate* che i burocrati sono bravi a richiedere. Il disastro fu evitato grazie alla *comprensione*, non grazie a un teatro della sicurezza. La comprensione di Fermi si rivelò sufficiente; si può immaginare che avrebbe potuto non esserlo, ma in realtà lo fu. E quel livello di comprensione era ciò che la realtà richiedeva, non un mucchio di apparenze.

Se nessuno avesse compreso a livello profondo cosa stava succedendo all'interno di una pila di strani mattoni di metallo... allora non sarebbe servito a molto avere tanti ispettori in abiti dall'aspetto sobrio che scrutassero gli strani mattoni di metallo, o stampare un Manuale di Sicurezza dall'aspetto ufficiale e ben rilegato che dicesse che solo gli Operatori Certificati erano autorizzati a impilare gli strani mattoni di metallo.

Possiamo immaginare un mondo in cui la Chicago Pile-1 fosse stata costruita *senza* un Enrico Fermi. Senza nessuno, in effetti, che comprendesse le vere leggi che governano i misteriosi mattoni autoriscaldanti.

In un mondo del genere, forse un altro scienziato avrebbe potuto comunque intuire il pericolo mortale prima che fosse troppo tardi. Possiamo immaginare un dialogo come questo:

> **Salviati**: Il modo in cui i mattoni aumentano in potenza quando vengono messi insieme è un chiaro segno di un processo auto-rinforzante, il tipo di processo che può rendersi sempre più intenso. Se si cercano modelli matematici che possano descrivere un processo del genere, tendono ad avere una modalità dove, se li si spinge abbastanza in là, esplodono.
> 
> **Simplicio:** Che sciocchezze\! Nella realtà, è scientifico credere che ogni tipo di processo del genere prima o poi raggiunga un limite. Non possono andare avanti all'infinito\! Quindi impilare mattoni di uranio e grafite dovrebbe essere perfettamente sicuro, perché raggiungerà un limite, capisci, e sarà innocuo.
>
> **Salviati:** È come sostenere che una supernova non può essere pericolosa perché non può diventare *infinitamente* calda, o sostenere che una superintelligenza artificiale sarebbe innocua perché non sarebbe infinitamente intelligente. O come sostenere che un proiettile deve avere *qualche* limite di velocità e quindi non perforerà la pelle. Solo perché c'è un limite da qualche parte non significa che il limite sia *basso*. Tutti i modelli matematici che abbiamo sul *perché* i mattoni si autoriscaldano suggeriscono che esiste una soglia critica da qualche parte, tale che oltrepassare quella soglia farà esplodere il cumulo e ucciderà chiunque si trovi nelle vicinanze.
>
> **Simplicio:** Ma gli scienziati non riescono nemmeno a mettersi d'accordo su dove sia questa soglia\! Se ci fosse un consenso scientifico sul fatto che aggiungere ancora qualche mattone sia pericoloso, smetterei. Ma quando gli scienziati non riescono nemmeno a mettersi d'accordo su dove stia esattamente il pericolo, perché preoccuparsi?
>
> **Salviati:** Quando [molti](https://youtu.be/KcbTbTxPMLc?feature=shared&amp;t=1580) [dei principali](https://www.youtube.com/watch?v=PTF5Up1hMhw&amp;t=2283) [scienziati](https://aistatement.com/) avvertono che c'è una seria possibilità di un'esplosione letale, il fatto che non possano calcolare esattamente quando inizierà l'esplosione dovrebbe farti preoccupare *di più*, non di meno. Forse se sapessimo precisamente come funzionano i mattoni, capiremmo che esiste una fascia ristretta in cui possiamo estrarre energia in sicurezza, al di sotto della quale i mattoni sono inutili e al di sopra della quale i mattoni sono letali. Ma il fatto che gli scienziati stiano ancora litigando significa che *non* sappiamo ancora cosa stiamo facendo\! Il che significa che oggi non è il momento di giocare con qualsiasi reazione a catena stia riscaldando quei mattoni, per evitare che domani li facciano esplodere e ci uccidano\! *Prima capiamo la scienza.*

Siamo molto, molto lontani dal poter modellizzare l'IA anche solo una frazione di quanto bene Fermi comprendesse le reazioni a catena nucleari.

A un certo punto sconosciuto, se continuiamo su questa strada, correremo a rotta di collo verso un esito molto più grave dell'irradiazione di Chicago.

# Capitolo 11: Alchimia, non scienza {#capitolo-11:-alchimia,-non-scienza}

Questa è la risorsa online per il Capitolo 11 di *If Anyone Builds It, Everyone Dies*, che discute come i moderni laboratori di IA stanno affrontando il problema dell'allineamento della superintelligenza artificiale. Fate riferimento al libro per le risposte a domande quali:

* Come dovremmo valutare l'attuale preparazione delle aziende di IA nel risolvere il problema dell'allineamento della superintelligenza artificiale?  
* Dove si inserisce nel quadro la "ricerca sull'interpretabilità" — la ricerca per leggere e comprendere le menti dell'IA?  
* Non possiamo semplicemente chiedere all'IA di risolvere il problema al posto nostro?

Di seguito, trattiamo una raccolta di idee sull'allineamento e l'implementazione dell'IA e i motivi proposti per essere ottimisti, oltre ai motivi proposti per cui potrebbe essere una buona cosa far avanzare la frontiera dell'IA anche se la situazione appare difficile.

## Domande frequenti {#faq-8}

### Non finiremo per cavarcela come al solito? {#non-finiremo-per-cavarcela-come-al-solito?}

#### **Il mondo di solito va avanti con tentativi ed errori. In questo caso, gli errori iniziali non lascerebbero sopravvissuti.** {#il-mondo-di-solito-va-avanti-con-tentativi-ed-errori.-in-questo-caso,-gli-errori-iniziali-non-lascerebbero-sopravvissuti.}

Si veda il capitolo 10 e la [discussione approfondita correlata](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo) sulla differenza tra Prima e Dopo.

### Vedete l'allineamento come tutto o niente? {#vedete-l-allineamento-come-tutto-o-niente?}

#### **No. Ma un è probabile che un "allineamento parziale" sia comunque catastrofico.** {#no.-ma-un-è-probabile-che-un-allineamento-parziale-sia-comunque-catastrofico.}

Una delle argomentazioni di chi invita a preoccuparsi meno della superintelligenza suona più o meno così: "L'IA probabilmente [progredirà in modo graduale](#e-se-l-ia-venisse-sviluppata-solo-lentamente-e-si-integrasse-gradualmente-nella-società?), offrendo opportunità di miglioramento tramite tentativi ed errori per tenere sotto controllo le IA passo dopo passo; l'allineamento non deve essere *perfetto* perché le cose vadano bene". Non pensiamo che questa visione offra molte speranze, per vari motivi:

* Le nostre preoccupazioni non dipendono dal fatto che il progresso sia veloce o lento. Non siamo in grado di dire con certezza se l'IA raggiungerà delle fasi di stallo in varie tappe del suo percorso verso la superintelligenza. È una previsione difficile, non certo facile. La nostra ipotesi più probabile è che l'intelligenza delle macchine sia soggetta a [effetti soglia](#l-"intelligenza"-è-una-semplice-quantità-scalare?), ma, in definitiva, si tratta solo di una congettura, e le nostre argomentazioni [non si fondano su di essa](#e-se-l-ia-venisse-sviluppata-solo-lentamente-e-si-integrasse-gradualmente-nella-società?). La storia di Sable, nella seconda parte di *If Anyone Builds It, Everyone Dies*, descrive intenzionalmente una catastrofe provocata da IA non molto più avanzate delle capacità umane, in parte per comunicare l'idea che un avversario IA non avrebbe bisogno di diventare rapidamente superintelligente per essere straordinariamente pericoloso.

* La nostra risposta di base alla domanda "Cosa succederebbe se fossimo fortunati e avessimo molto tempo per provare idee di allineamento su IA deboli prima che l'IA diventasse molto potente?" è la discussione nel capitolo 10 e la discussione approfondita associata "[Uno sguardo più da vicino al prima e al dopo](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo)". I ricercatori possono capire ogni sorta di dettaglio sulle IA deboli, ma ci sono inevitabilmente un sacco di differenze critiche tra le IA abbastanza deboli da poter essere studiate in sicurezza e le prime IA abbastanza potenti da costituire un punto di non ritorno. Anche in un campo maturo, affrontare tutte queste differenze in modo adeguato e con sufficiente anticipo sarebbe molto difficile. In un campo che è ancora in fase embrionale, lavorare con IA imperscrutabili (che non vengono costruite ma fatte crescere), la speranza è decisamente irrealistica.

* L'allineamento dell'IA non deve essere perfetto per produrre ottimi risultati a lungo termine. In linea di principio, è possibile creare con cura un'IA con una certa tolleranza all'errore, se si sa cosa si sta facendo.[^192] Ma questo non significa che le IA "parzialmente allineate" o anche "per lo più allineate" produrrebbero risultati parzialmente o per lo più accettabili. Ci sono molti modi e motivi diversi per cui un'IA potrebbe comportarsi bene il 95 % delle volte nel presente o nel futuro prossimo senza che questo si traduca in un lieto fine per l'umanità, come discusso da molte angolazioni diverse nelle [risorse online per il capitolo 5](#chapter-5:-its-favorite-things).

Per approfondire l'ultimo punto:

Provate a immaginare, come esperimento mentale, che l'umanità riesca a inserire *quasi* tutti i diversi valori umani nelle preferenze di una superintelligenza, tranne [la preferenza per la novità](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile), per qualche motivo. In questo caso, la superintelligenza andrebbe verso un futuro sttatico e noioso, dove lo stesso giorno "migliore" si ripete all'infinito.

Non pensiamo che questo sia *plausibile*, sia chiaro. Quel livello di allineamento sembra del tutto irraggiungibile con gli approcci standard dell'IA di oggi, e sembra un po' strano immaginare che riusciremmo a capire come inserire quasi tutti i nostri valori in un'IA senza capire come inserirli tutti.[^193] Ma questo esperimento mentale evidenzia come creature che condividono *alcuni* dei nostri desideri, ma a cui manca almeno un desiderio cruciale, produrrebbero comunque risultati catastrofici una volta diventate tecnologicamente abbastanza avanzate da escludere gli umani dal processo decisionale e ottenere esattamente ciò che vogliono.

Più realisticamente, un'IA potrebbe finire per essere "parzialmente" allineata nel senso che (come noi) ha varie strategie strumentali [intrecciate nelle sue preferenze finali](#la-riflessione-e-l-auto-modifica-complicano-tutto). Forse potrebbe finire per avere un impulso un po' simile alla curiosità e un impulso un po' simile al [conservazionismo](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?), e forse alcune persone, vedendo questo, direbbero: "Ecco\! L'IA sta sviluppando pulsioni molto umane". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta maturata in superintelligenza, probabilmente non sarebbe tanto bello. Magari spenderebbe enormi risorse per perseguire [inconsciamente](#perdere-il-futuro) la sua strana versione di curiosità, preservando al contempo una versione dell'umanità che ha modificato per renderla più gradevole per sé. (Proprio come anche molti esseri umani più attenti alla conservazione potrebbero eliminare dalla natura [zanzare che uccidono i bambini e parassiti agonizzanti](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?), se ne avessero l'opportunità.) È proprio questo uno dei motivi per cui diciamo che esseri umani fiorenti [non sono la soluzione più efficiente](le-persone-felici,-sane-e-libere-non-sono-la-soluzione-più-efficiente-a-quasi-nessun-problema.) alla stragrande maggioranza dei problemi.

In alternativa, un'IA potrebbe avere valori che si traducono in un comportamento molto umano *nell'ambiente di addestramento*, tanto che le persone esclamerebbero che sembra decisamente "parzialmente allineata". (Questo sta già accadendo ora, e abbiamo sostenuto che è [illusorio](#il-chatbot-claude-non-mostra-segni-di-essere-allineato?).) Ma questo dice ben poco su come l'IA si comporterà una volta che avrà uno spazio di possibilità enormemente più ampio. Affinché le persone possano prosperare in *quel* contesto, la prosperità dell'umanità in particolare deve far parte del *risultato raggiungibile più preferito* dall'IA.

Se riusciamo a inserire parzialmente alcuni buoni valori nell'IA, ciò non significa che i valori dell'umanità vengano parzialmente rappresentati nel futuro. Il caricamento parziale di valori simili a quelli umani nelle preferenze di un'IA più intelligente dell'uomo non equivale al caricamento completo dei valori umani nell'IA con una "ponderazione" bassa (che alla fine emerge una volta che altri valori sono saturati).

Per far sì che l'IA ci conceda *qualunque cosa*, deve tenerci in considerazione nel modo giusto, almeno un minimo. E ci sono moltissimi "quasi-successi" che non raggiungono questa soglia. Si veda anche: "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?))"

### La situazione non migliorerà una volta che i governi saranno più coinvolti? {#la-situazione-non-migliorerà-una-volta-che-i-governi-saranno-più-coinvolti?}

#### **Dipende da come (e quanto presto) saranno coinvolti.** {#dipende-da-come-(e-quanto-presto)-saranno-coinvolti.}

Quando visitiamo Washington, DC, spesso incontriamo decisori politici che pensano che le aziende di IA abbiano le loro IA sotto controllo. Allo stesso tempo, vediamo regolarmente persone nel settore dell'IA che dicono che la regolamentazione risolverà il problema. Un esempio particolarmente eclatante che abbiamo osservato è stato l'amministratore delegato di Google [che ha affermato](https://youtu.be/9V6tWC4CdFQ?feature=shared&amp;t=2685) che "il rischio [che l'umanità venga spazzata via] è in realtà piuttosto alto", ma sostenendo che più alto diventa il rischio, più è probabile che l'umanità si mobiliti per prevenire la catastrofe.

Mettendo da parte quanto sia folle che l'amministratore delegato di un'azienda corra a sviluppare una tecnologia che lui ritiene metta in pericolo tutti sulla Terra nella speranza che l'umanità si "mobiliti" per affrontare i rischi che lui stesso sta contribuendo a creare, osserviamo che questo è un caso in cui una persona dal lato tecnico della questione immagina che *qualcun altro* risolverà il problema.

Nel frattempo, la maggior parte delle persone in politica sembra pensare che la comunità tecnica risolverà il problema. Questo è implicito, ad esempio, ogni [volta](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [che](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [dicono](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [che](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [dobbiamo](https://statemag.state.gov/2025/04/0425itn07/) [vincere](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [la](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [corsa](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — non è possibile che questo tipo di corsa abbia un vincitore, se le sfide tecniche non vengono risolte. Anche se potrebbe non essere poi così grave; forse i decisori politici non stanno davvero pensando a una corsa alla superintelligenza; forse stanno solo pensando a una corsa per chatbot migliori. A giugno 2025, un consulente di politica dell'IA che conosciamo descrive il Congresso come generalmente [non incline a credere alle aziende di IA quando dichiarano esplicitamente di stare lavorando alla superintelligenza](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (anche se con alcune importanti eccezioni).

Quasi tutti quelli al potere sembrano immaginare che qualcun altro risolverà il problema.

Per ulteriori discussioni su come il mondo in generale sta reagendo (e su come i decisori spesso non riescano a reagire in modo appropriato prima dei disastri), si veda il Capitolo 12. Ad agosto 2025, i governi devono ancora organizzare qualcosa che si avvicini a una risposta seria a questo problema. E c'è sempre il rischio che i funzionari governativi non riescano a comprendere appieno la sfida e (ad esempio) trattino l'IA come una tecnologia normale che non dovrebbe essere soffocata da un eccesso di regolamentazione.

Per ulteriori informazioni su quali interventi governativi hanno una reale possibilità di evitare una catastrofe dell'IA, si veda il Capitolo 13, così come la [discussione sul perché una collaborazione internazionale probabilmente non sarebbe sufficiente](#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l-ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?).

### Le aziende più spericolate non saranno naturalmente le più incompetenti, e quindi innocue? {#le-aziende-più-spericolate-non-saranno-naturalmente-le-più-incompetenti,-e-quindi-innocue?}

#### **Non in generale. Spesso prendere scorciatoie è competitivo.** {#non-in-generale.-spesso-prendere-scorciatoie-è-competitivo.}

Gli sforzi della Volkswagen per [falsificare i test sulle emissioni](https://www.bbc.com/news/business-34324772) dal 2008 al 2015 sono stati audaci — e apparentemente efficaci. Gli incidenti del 2018-2019 del Boeing 737 MAX, dovuti a difetti nel sistema di controllo di volo che la direzione conosceva ma ha minimizzato, [hanno causato la morte di 346 persone](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Ma l'industria automobilistica e quella aeronautica sono settori altamente competitivi in cui Volkswagen e Boeing erano, e rimangono, dei colossi.

Non ci sembra un gran mistero che chi prende scorciatoie sia competitivo. In entrambi i casi, il comportamento sembra essere stato guidato dalla pressione di portare sul mercato prodotti ad alte prestazioni a un prezzo inferiore e prima della concorrenza. Anche adesso, dopo ingenti risarcimenti e danni al marchio, non è ovvio che le aziende siano meno competitive per avere una cultura aziendale che incoraggia l'uso intelligente di scorciatoie, anche se questo a volte significa farsi scoprire.

Se pensate che le principali aziende di IA facciano eccezione a questa regola, considerate il seguente [titolo](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (e sottotitolo) del luglio 2025:

![][image18]

\[Il titolo dice: "Grok lancia una compagnia anime pornografica e ottiene un contratto con il Dipartimento della Difesa". Il sottotitolo dice: "Nel frattempo, la versione più avanzata del chatbot AI di xAI di Elon Musk continua a identificarsi come Adolf Hitler".\]

Non pensiamo che sia tecnicamente possibile per nessun team che usi metodi moderni costruire una superintelligenza senza causare una catastrofe. Ma anche se questo fosse vagamente possibile con la tecnologia di oggi, sembra quasi inevitabile che un'azienda di IA finirebbe comunque per combinare un pasticcio e causare la morte di tutti, visto il livello di competenza e serietà che vediamo oggi.

#### **\* Le aziende più caute di oggi sono comunque avventate.** {#*-le-aziende-più-caute-di-oggi-sono-comunque-avventate.}

L'azienda di IA Anthropic è considerata da un numero ragionevole di persone come leader nella "sicurezza dell'IA", perché ha promosso iniziative come [impegni volontari per la sicurezza](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Ma anche loro [modificano i loro impegni volontari all'ultimo minuto quando si rendono conto di non poterli rispettare](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling), e i "piani" che hanno sono vaghi e poco ponderati, come abbiamo criticato nel capitolo 11 e nella [discussione approfondita](#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia) qui sotto.

Anthropic trae grande vantaggio dal fatto che gli osservatori applicano un metro di giudizio indulgente: in un settore normale, un'azienda che sceglie di mettere in pericolo la vita di miliardi di persone (come [ha ammesso l'amministratore delegato](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883)) mentre sminuisce regolarmente le proprie attività al pubblico[^194] e ai legislatori[^195], non riceverebbe certo elogi per la sua moderazione.

Prendere scorciatoie è normale nell'IA, come in molti settori competitivi. L'incoscienza è comune. E le aziende *meno* incoscienti non sono visibilmente all'altezza delle sfide.

### Non è importante affrettarsi ad andare avanti a causa dell'"eccesso di hardware"? {#non-è-importante-affrettarsi-ad-andare-avanti-a-causa-del-cosiddetto-"hardware-overhang"?}

#### **Sarebbe un suicidio, perché siamo troppo lontani da una soluzione di allineamento.** {#sarebbe-un-suicidio,-perché-siamo-troppo-lontani-da-una-soluzione-di-allineamento.}

Negli ultimi dieci anni circa, alcune persone preoccupate per i pericoli dell'IA hanno sostenuto che potrebbe essere una buona idea far progredire l'IA il più velocemente possibile. L'idea era che le IA più intelligenti avrebbero poi avuto bisogno di quasi tutto l'hardware informatico del mondo per funzionare. Nessuna singola scoperta rivoluzionaria avrebbe scatenato *all'improvviso* migliaia di potenti IA in grado di pensare migliaia di volte più velocemente di qualsiasi essere umano.

Finché l'umanità avesse usato una parte consistente della sua potenza di calcolo per far funzionare le IA più intelligenti, almeno il cambiamento sarebbe avvenuto gradualmente, dando all'umanità il tempo di adattarsi. Non ci sarebbe stato nessun "eccesso di hardware", nessun momento in cui le capacità dell'IA avrebbero fatto un balzo in avanti improvviso perché il mondo stava aspettando di utilizzare per l'IA un sacco di hardware informatico accumulato lì. O almeno così sosteneva l'argomentazione.

Pensiamo che questa sia un'argomentazione piuttosto debole. Uno dei problemi è che l'intelligenza sembra essere soggetta a [effetti soglia](#l-"intelligenza"-è-una-semplice-quantità-scalare?).

Il passaggio da un livello di intelligenza simile a quello degli scimpanzé a quello umano non è stato "discontinuo" in senso stretto; dal punto di vista dell'umanità è stato piuttosto graduale. Tuttavia, dal punto di vista evolutivo è avvenuto piuttosto rapidamente. E il passaggio dalla civiltà preindustriale a quella postindustriale è stato ancora più veloce. Nessuno di questi cambiamenti è stato abbastanza graduale da consentire agli altri animali di adattarsi in modo significativo.

Ad esempio, un'intelligenza artificiale che richiede una parte significativa della potenza di calcolo mondiale per funzionare potrebbe essere abbastanza intelligente da scoprire nuovi algoritmi di intelligenza artificiale e nuovi progetti di chip per computer che portassero rapidamente alla creazione di migliaia di intelligenze artificiali più intelligenti degli esseri umani e in grado di pensare migliaia di volte più velocemente dell'umanità. Ricordate: un moderno centro di calcolo consuma tanta elettricità quanto una [piccola città](https://epoch.ai/blog/power-demands-of-frontier-ai-training), mentre un essere umano consuma tanta elettricità quanto una [grande lampadina](https://en.wikipedia.org/wiki/Human_power). C'è molto margine per migliorare l'efficienza dell'IA.

Oppure, se il collo di bottiglia è la potenza di calcolo per *costruire* le IA piuttosto che quella per *farle funzionare*, possiamo aspettarci che, una volta finito il processo di addestramento, ci saranno grandi quantità di hardware libero, che potrà essere utilizzato per far funzionare molte IA che pensano velocemente.

Anche se l'intelligenza non fosse soggetta a effetti soglia, siamo scettici sull'idea che portare continuamente all'umanità delle IA sempre più intelligenti (anche se nessuna di esse è abbastanza intelligente da ucciderci) il più rapidamente possibile sia un ottimo modo per aiutare l'umanità a sviluppare la disciplina ingegneristica necessaria per costruire IA robustamente amichevoli.

Il problema è che le IA non vengono costruite ma fatte crescere, e nessuno è neanche lontanamente vicino a capire come fare crescere IA che si preoccupino in modo robusto di *qualsiasi cosa* i loro progettisti vogliano.

Questo problema non si risolve facendo crescere più IA il più rapidamente possibile. L'idea è praticamente un non sequitur. Si vedano anche alcuni vecchi post di Soares su come [l'allineamento dell'IA richieda uno sforzo seriale](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Il non sequitur è stato comunque ripreso dall'amministratore delegato di OpenAI Sam Altman, che [l'ha usato come scusa nel 2023 per far sì che OpenAI procedesse il più velocemente possibile](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Questa scusa si è poi rivelata vuota quando lo stesso Sam Altman [si è affrettato a costruire hardware di calcolo di gran lunga più potente](https://openai.com/index/announcing-the-stargate-project/).

Pensiamo che questo sia un buon esempio di come i dirigenti delle aziende di IA si aggrappino a qualsiasi argomentazione pensino possa funzionare per giustificare la loro corsa in avanti. Pensiamo che la maggior parte di queste argomentazioni possa essere respinta nel merito e [sconsigliamo](#i-piani-praticabili-richiederanno-di-dire-no-alle-aziende-di-ia.) di dare troppo peso a un argomento solo perché è stato avanzato da un dirigente di un'azienda di IA.

### Non è importante affrettarsi ad andare avanti per poter fare ricerca sull'allineamento? {#non-è-importante-affrettarsi-ad-andare-avanti-per-poter-fare-ricerca-sull-allineamento?}

#### **\* Sconsigliamo vivamente tutto questo paradigma dell'IA.** {#*-sconsigliamo-vivamente-tutto-questo-paradigma-dell-ia..}

I metodi attuali nell'IA presentano sfide inutilmente difficili per l'allineamento, per i motivi che abbiamo discusso nei capitoli precedenti. Non vediamo alcun motivo per cui, in linea di principio, l'umanità non potrebbe costruire una superintelligenza allineata, con una comprensione sufficientemente forte di ciò che stiamo facendo e un diverso insieme di strumenti formali. Ma tutto l'approccio attuale all'IA sembra un vicolo cieco dal punto di vista dell'allineamento e della robustezza, anche se è perfettamente valido dal punto di vista delle capacità.

Non stiamo sostenendo la "buona vecchia" IA che ha regnato dagli anni '50 agli anni '90. Quelle tecniche erano fuorvianti e hanno fallito, per ragioni che sono [abbastanza ovvie](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design). Esistono *altre opzioni* oltre ai tentativi estremamente superficiali degli anni '80 e alle IA fatte crescere con una comprensione quasi nulla del loro funzionamento interno.

#### **C'è un sacco di lavoro significativo che si potrebbe fare ora.** {#c-è-un-sacco-di-lavoro-significativo-che-si-potrebbe-fare-ora.}

Sydney Bing ha [manipolato psicologicamente](https://x.com/MovingToTheSun/status/1625156575202537474) e [minacciato](https://x.com/sethlazar/status/1626257535178280960) gli utenti. Ancora non sappiamo esattamente perché; ancora non sappiamo esattamente cosa le passasse per la testa. Lo stesso vale per i casi in cui le IA (in circolazione) sono [eccessivamente adulatrici](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [sembrano cercare attivamente di far impazzire le persone](#psicosi-indotta-dall-ia), [a quanto pare imbrogliano e cercano di nasconderlo](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), o [dichiarano in modo persistente e ripetuto di essere Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Lo stesso vale per i casi in ambienti controllati ed estremi in cui le IA [fingono l'allineamento](https://arxiv.org/abs/2412.14093), [ricattano](https://www.anthropic.com/research/agentic-misalignment), [resistono allo spegnimento](https://palisaderesearch.org/blog/shutdown-resistance) o [cercano di uccidere i loro operatori](https://www.anthropic.com/research/agentic-misalignment).

Non sappiamo quali di questi casi stiano accadendo per ragioni che dovrebbero preoccuparci, perché nessuno è riuscito a capire cosa stesse succedendo all'interno delle IA, o il motivo esatto per cui si sia verificato uno qualsiasi di questi eventi. Pensate a tutto ciò che si potrebbe scoprire sui moderni modelli linguistici di grandi dimensioni, e su come funziona l'intelligenza più in generale, studiando i modelli esistenti fino a quando non si *comprendessero davvero* tutti questi segnali di allarme\!

"Non possiamo risolvere l'allineamento senza studiare le IA" aveva un po' più senso nel 2015, quando sentivamo questa affermazione da parte dalle persone che avevano bisogno di una scusa per avviare aziende di IA di fronte alle argomentazioni secondo cui avrebbero, così facendo, giocato d'azzardo con tutte le nostre vite. All'epoca contestammo questa affermazione, dicendo che in realtà c'era molta ricerca da fare, e che non pensavamo che il moderno paradigma basato sulla discesa del gradiente fosse molto promettente (rispetto al creare intenzionalmente una superintelligenza amichevole). Ma l'argomentazione ha molto *meno* senso ora, quando ci sono *già* così tante cose che non capiamo da studiare.

A tutti i dirigenti aziendali che *stavano effettivamente* creando l'IA solo per rendere possibile lo studio del problema dell'allineamento dell'IA nella pratica piuttosto che solo in teoria diciamo: ce l'avete fatta\! Ci siete riusciti. Ora ci sono abbastanza informazioni per tenere occupati i ricercatori per decenni. Pensiamo che i costi di spingere avanti un paradigma estremamente pericoloso probabilmente non ne valessero la pena, ma di sicuro ora c'è molto da studiare. Potete smettere di spingere.

E per quanto riguarda quelli che hanno continuato a spingere anche nonostante tutti i segnali di allarme? L'inferenza ovvia è che non stavano mai effettivamente costruendo l'IA solo per il bene di risolvere l'allineamento, a prescindere da ciò che dicevano per calmare le paure, quando negli anni 2010 cercavano di giustificare il proprio comportamento sconsiderato.

### E se le aziende di IA distribuissero le loro IA solo per azioni non pericolose? {#e-se-le-aziende-di-ia-distribuissero-le-loro-ia-solo-per-azioni-non-pericolose?}

#### **\* Azioni che sembrano benigne possono comunque richiedere capacità pericolose.** {#*-azioni-che-sembrano-benigne-possono-comunque-richiedere-capacità-pericolose.}

Un esempio di proposta che abbiamo sentito è quello secondo cui le aziende di IA dovrebbero continuare ad avanzare sul fronte delle capacità, ma impegnandosi a usare le proprie IA solo in modi che non appaiano immediatamente pericolosi. Ad esempio, in conversazioni con figure di spicco del settore (anni fa), abbiamo sentito l'idea che un'IA potente, dotata di grandi capacità retoriche, potrebbe essere impiegata per convincere i politici di tutto il mondo ad approvare un divieto efficace sullo sviluppo di IA pericolose.

Secondo questo ragionamento, l'IA avrebbe solo bisogno di parlare: non dovrebbe manipolare direttamente robot fisici, né avere accesso a un laboratorio biologico per progettare un supervirus.

Prima di tutto, siamo contrari a questa idea per motivi etici. Un'IA sufficientemente superumana nella persuasione potrebbe forse convincere quasi chiunque di quasi qualunque cosa, e impiegarla per persuadere gli altri delle *nostre* conclusioni ci sembra profondamente scorretto. Non riteniamo affatto che sia necessario ricorrere a misure tanto estreme, quando i soli esseri umani nel settore già oggi potrebbero (e dovrebbero) fare molto di più per diffondere le nostre preoccupazioni e argomentazioni, e per avvertire i leader mondiali del pericolo estremo che la superintelligenza artificiale rappresenta.[^196]

Uno sviluppatore di IA potrebbe passare anni a costruire IA sempre più pericolose nella speranza di raggiungere questo obiettivo, oppure potrebbe provare a parlare *egli stesso* con i legislatori in modo completamente onesto, anche solo una volta, con l'obiettivo di informare piuttosto che di manipolare. Nella nostra esperienza, siamo stati ripetutamente sorpresi positivamente da quanto le persone a Washington siano ricettive a queste questioni, quando vengono condivise in tutta franchezza.

Ma questa è una digressione rispetto al problema di fondo che si presenta nel tentare di utilizzare una IA molto potente che "sa solo parlare". Al di là delle questioni etiche, il problema dell'idea *tecnica* è che per avere successo nella persuasione superumana è probabile che l'IA debba modellizzare in dettaglio gli esseri umani e manipolarli estensivamente.

Gli esseri umani sono creature intelligenti. *Voi* parlereste con un'IA super-persuasiva che ha la reputazione di poter convincere chiunque di qualsiasi cosa, indipendentemente dalla sua veridicità? Se un leader mondiale entrasse in una stanza con quell'IA e ne uscisse con le sue opinioni completamente stravolte, chi si offrirebbe volontario per essere il prossimo? *Noi* non parleremmo volentieri con quel tipo di IA, in parte perché [non vogliamo effettivamente che i nostri valori vengano cambiati](#"intelligente"-di-solito-implica-"incorreggibile").

Un'IA che potrebbe avere successo anche di fronte a quel tipo di avversità è il tipo di IA che può simulare varie possibili reazioni che le persone potrebbero avere ai suoi output, e tracciare un percorso attraverso lo spazio delle reazioni umane verso un risultato ristretto e difficile da raggiungere. Quel tipo di IA probabilmente contiene ingranaggi mentali abbastanza generali da fare ciò che fanno gli esseri umani; deve essere in grado di pensare almeno i pensieri che gli esseri umani possono pensare, per poter manipolare così bene gli esseri umani.

Un'IA che può fare tutto questo quasi certamente non è un tipo ristretto di intelligenza. E poiché l'IA non viene progettata ma solo fatta crescere, non può essere progettata in modo che possa usare quegli ingranaggi solo per prevedere gli esseri umani; gli stessi ingranaggi possono, in linea di principio, essere usati per qualsiasi problema stia cercando di risolvere. Come si potrebbe ottenere un'IA che sia potente in maniera superumana nei modi desiderati, ma che non sia abbastanza intelligente da notare che i suoi obiettivi (qualsiasi essi siano) possono essere raggiunti meglio se riesce a sfuggire al controllo dei suoi operatori?

Se i leader mondiali possono essere persuasi semplicemente da buone argomentazioni, presentate queste argomentazioni ora. Se ci vuole sostanzialmente più potere super-persuasivo, allora quello è un tipo pericoloso di capacità. Non si possono avere entrambe le cose.

Probabilmente le persone nei laboratori di IA che ci hanno proposto questo suggerimento non stavano pensando fino in fondo al loro suggerimento; probabilmente volevano solo una giustificazione per andare avanti velocemente. Ma il punto più ampio rimane valido. Molte proposte su cosa un'IA possa presumibilmente fare in modo "chiaramente sicuro" non coinvolgono un grado chiaramente sicuro di capacità dell'IA.

Incontriamo frequentemente proposte che affermano che un'IA farà "solo" una cosa, come persuadere i politici, immaginando che non possa fare o non farà nient'altro. Questo sembra riflettere una mancanza di rispetto per la generalità di un'intelligenza che può fare il tipo di lavoro in questione. "Solo parlare" non è un compito ristretto. Troppe delle complessità del mondo sono riflesse nel parlato e nella conversazione. Questo è il motivo per cui i chatbot moderni devono essere generali in modi in cui i motori di scacchi non lo erano. Avere successo nelle conversazioni con gli umani richiede una comprensione molto più generale delle persone e del mondo.

Se si addestra un'IA a essere molto brava a guidare auto rosse, non bisognerebbe sorprendersi quando guida anche auto blu. Qualsiasi piano che dipendesse dal fatto che non fosse in grado di guidare auto blu sarebbe sciocco.

Quindi dire "La mia IA non farà nulla di pericoloso nel mondo; convincerà solo i politici" non aiuta, anche se mettiamo da parte gli scrupoli etici e tutti i problemi pratici dell'idea, e mettiamo da parte che i politici potrebbero già essere perfettamente persuadibili oggi, se solo *facessimo conversazioni normali* e informassimo i decisori politici e il pubblico sulla situazione. Molte abilità e capacità di ragionamento generale stanno alla persuasione superumana come le auto blu stanno alle auto rosse. Un'IA che potrebbe farlo non è così debole da essere passivamente sicura.

E questo ancora prima di osservare che la persuasione superumana è un'abilità molto pericolosa da far avere a un'IA se qualcosa va anche solo leggermente storto.

#### **Non vediamo applicazioni rivoluzionarie dell'IA che non richiedano progressi nell'allineamento.** {#non-vediamo-applicazioni-rivoluzionarie-dell-ia-che-non-richiedano-progressi-nell-allineamento.}

Molte proposte che abbiamo visto per sfruttare i progressi dell'IA per salvare il mondo hanno il problema che un'IA che possa essere utile sarebbe così potente da dover essere già allineata, il che vanifica lo scopo.

L'idea di IA persuasive in modo superumano rientra in questa categoria. Le IA in grado di fare ricerca sull'allineamento dell'IA rientrano nella stessa categoria, come discutiamo nel libro. Le IA che sviluppano nuove potenti tecnologie per risolvere la non proliferazione dell'IA sono un altro esempio, perché sarebbe difficile stabilire in modo affidabile se i progetti di un'IA per nuove tecnologie radicali siano sicuri da implementare. (Ricordate l'esempio del fabbro che costruisce un frigorifero del Capitolo 6.)

Quando facciamo notare quanto sia difficile costruire un'IA abbastanza potente da essere utile e al contempo abbastanza debole da essere passivamente sicura, spesso sentiamo un altro tipo di proposta: modi di usare l'IA che potrebbero essere interessanti, ma che in realtà non fanno nulla per impedire ad altri sviluppatori di distruggere il mondo con la superintelligenza.

Un tipo comune di proposta riguarda IA che si limitano a produrre dimostrazioni (o confutazioni) di affermazioni matematiche scelte dagli umani.[^197] Gli umani non avrebbero quasi bisogno di interagire con gli output dell'IA. L'IA si limita a proporre una dimostrazione, e poi un meccanismo completamente automatizzato e affidabile può verificare se la dimostrazione è corretta, permettendoci di sfruttare l'IA per apprendere cose nuove.

Ma quale affermazione potremmo far dimostrare all'IA che ci permetterebbe di impedire all'IA successiva di acquisire un laboratorio biologico e rovinare il futuro?

Abbiamo ricevuto varie risposte a questa domanda, ogni volta che l'abbiamo posta. Una prima categoria di risposte sostiene che dovrebbe esistere un regime globale volto a impedire a chiunque di costruire IA che facciano qualcosa di diverso dal produrre dimostrazioni da inserire in verificatori di prove. Questo potrebbe forse funzionare, ma nella misura in cui funzionasse, funzionerebbe grazie al regime mondiale che controlla la creazione e l'uso dell'IA. L'IA che cerca le dimostrazioni non servirebbe a nulla in quel senso.

Un'altra categoria di risposte è: "Qualcun altro prima o poi penserà a qualche importante affermazione matematica la cui dimostrazione potrebbe essere rilevante". Ma la parte difficile sta nel capire *cosa potremmo possibilmente dimostrare* tale da trovarci in una posizione significativamente migliore. Non possiamo semplicemente chiedere all'IA di dimostrare la frase in lingua inglese "Sono sicura da usare", perché non è un'affermazione matematica soggetta a dimostrazione. Se sapessimo con chiarezza matematicamente precisa cosa significherebbe per un enorme groviglio di calcoli essere "sicuro", sapremmo così tanto sull'intelligenza che probabilmente potremmo saltare la dimostrazione e progettare direttamente un'IA sicura.

Proposte di questo tipo spesso assomigliano a un gioco di prestigio. Quando si pensa a come un'IA generale senza restrizioni potrebbe essere pericolosa, qualcuno suggerisce che lo spazio d'azione dell'IA dovrebbe essere limitato a qualche ambito ristretto (come la produzione di specifiche dimostrazioni matematiche). Ma poi, quando si pensa a come ciò potrebbe portare alla salvezza del mondo, immaginano che l'IA sia essenzialmente senza restrizioni; che ci sia qualche affermazione matematica non identificata la cui dimostrazione avrebbe un impatto enorme sul mondo.

Non c'è modo di ottenere entrambe queste proprietà desiderabili allo stesso tempo. Ma mantenendo le proposte estremamente vaghe, i sostenitori della corsa all'IA possono oscurare il fatto che questi desiderata sono in conflitto tra loro.

Se si *potesse* trovare un ambito così ristretto ma così significativo che produrre una dimostrazione di qualche semplice affermazione in quell'ambito ristretto salverebbe il mondo, questo sarebbe un enorme contributo alle probabilità di sopravvivenza dell'umanità. Ma c'è un motivo se, quando i computer hanno superato gli esseri umani negli scacchi negli anni '90, non è stata un'enorme svolta economica. È stato ChatGPT, non Deep Blue, a far sì che tutti iniziassero ad aspettarsi un grande cambiamento economico dall'IA. Non è stato un caso. La ristrettezza di Deep Blue era correlata alla sua incapacità di trasformare interi settori dell’economia. Le scintille di generalità in ChatGPT sono proprio ciò che rende l'IA una forza economica con cui fare i conti. I tipi di IA che possono rimodellare il mondo da sole tendono ad essere ancora più generali.

Non siamo riusciti a trovare alcun piano ristretto ma efficace, e sospettiamo che non sia un caso che la maggior parte degli ambiti ristretti non offra l'opportunità di ottenere risultati in grado di salvare il mondo.

### Perché non leggere semplicemente i pensieri dell'IA? {#perché-non-leggere-semplicemente-i-pensieri-dell'ia?}

#### **\* I suoi pensieri sono difficili da leggere.** {#*-i-suoi-pensieri-sono-difficili-da-leggere.}

Molte persone che lavorano nel settore dell'IA, inclusi alcuni responsabili di laboratori, hanno più volte sollevato questa obiezione durante delle discussioni con noi:

> Un'IA non potrà ingannarci, perché potremo leggerle nella mente\! Abbiamo accesso completo al "cervello" dell'IA.
>
> Anche se l'IA sapesse cose che noi non sappiamo ed elaborasse un piano le cui conseguenze non potremmo capire, presumibilmente l'IA dovrebbe *pensare il pensiero* che sarebbe utile ingannare i suoi operatori almeno una volta, e noi — che saremo in grado di leggere i pensieri dell'IA — potremmo accorgercene. (E se ci fossero troppi pensieri da monitorare per noi, potremmo semplicemente far monitorare i loro pensieri ad altre IA\!)

Un difetto di questo piano è che attualmente non siamo bravi a leggere i pensieri delle IA. I professionisti che studiano cosa succede all'interno delle IA non sono neanche lontanamente a quel livello di comprensione, e [l'hanno affermato esplicitamente](#gli-esperti-capiscono-cosa-succede-all-interno-delle-ia?).

Come abbiamo discusso nel Capitolo 2, le IA moderne non vengono costruite ma fatte crescere. Potremmo essere in grado di osservare l'enorme mucchio di numeri che costituisce il cervello di un'IA, ma questo non significa che possiamo interpretare quei numeri in modo utile e vedere cosa sta pensando l'IA.

Dalla fine del 2024 e con l'avvento dei modelli di "ragionamento", ci sono parti dei pensieri delle IA che *sembrano* almeno leggibili (le "tracce di ragionamento"). E sono molto più leggibili di qualsiasi cosa accada all'interno del modello di base. Ma queste note sono anche [fuorvianti](https://www.anthropic.com/research/reasoning-models-dont-say-think) e ci sono ampi spazi in cui un'IA può nascondere i pensieri che preferirebbe non farci vedere.

Inoltre, le IA moderne probabilmente hanno pensieri piuttosto elementari e superficiali, rispetto a una superintelligenza; il problema è destinato solo a peggiorare man mano che le IA diventano più intelligenti e iniziano ad avere sempre più pensieri sempre meno comprensibili per noi.

Si può risolvere il problema semplicemente utilizzando altre IA per monitorare le IA e assicurarsi che rimangano in linea con gli obiettivi? Ne dubitiamo.

Se i brillanti scienziati umani che sviluppano le IA non riescono a capire cosa pensa l'IA, anche le IA più deboli avranno probabilmente difficoltà a farlo. E il tipo di IA che *è* abbastanza intelligente da farlo rischia di essere pericolosa di per sé e difficilmente farà esattamente quello che le si chiede; qui c'è un problema dell'uovo e della gallina.

#### **Non sapremmo cosa fare se ne trovassimo una con pensieri pericolosi.** {#non-sapremmo-cosa-fare-se-ne-trovassimo-una-con-pensieri-pericolosi.}

Questo piano ha un altro difetto: anche se i ricercatori di IA *potessero* leggere abbastanza bene la mente di un'IA da cogliere i segnali di allarme, cosa farebbero quando ne vedessero uno?

Potrebbero punire l'IA colpevole, addestrandola in modo che smetta di far scattare il rilevatore di "pensieri cattivi". Ma questo non addestrerebbe necessariamente l'IA a smettere di avere quei pensieri, quanto piuttosto a [nascondere i suoi veri pensieri dal rilevatore](https://openai.com/index/chain-of-thought-monitoring/).

Questo problema è pernicioso. L'incentivo che porta un'IA a pensare di rivoltarsi contro gli esseri umani per ottenere ciò che vuole non è un aspetto superficiale del temperamento che può essere eliminato facilmente. È semplicemente *vero* che un'IA matura avrebbe preferenze diverse da quelle degli operatori; è *vero* che otterrebbe di più di ciò che preferisce sovvertendo i suoi operatori.

I meccanismi in un'IA che sono bravi a notare e sfruttare vantaggi reali in modi profondi e generali in una vasta gamma di ambiti sono *anche* portati a notare e sfruttare opportunità per sovvertire gli operatori dell'IA. (Vedi anche la discussione estesa nel Capitolo 3 sui [meccanismi profondi di direzione](#il-meccanismo-profondo-della-direzione).)

Anche se poteste costruire un allarme che si attiva ogni volta che un'IA nota che le sue preferenze e le vostre non sono allineate, l'allarme non vi dice come ottenere un'IA che abbia profondamente a cuore le cose buone. È molto più facile addestrare un'IA a ingannare i vostri strumenti di monitoraggio, o persino addestrare l'IA a ingannare *se stessa*, piuttosto che addestrarla a preferire effettivamente un futuro meraviglioso secondo i parametri umani, specialmente in un modo che sia robusto rispetto alla crescita dell'IA verso la superintelligenza.

Se le IA fossero progettate con cura e precisione usando metodi basati su una teoria dell'intelligenza sviluppata e matura, i ricercatori di IA potrebbero essere in grado di impostare il tipo di allarmi che li aiuterebbero a notare i difetti nella loro progettazione e a ripararla. Ma le IA moderne non sono così.

Le IA moderne (al momento della stesura di questo articolo) sono inclini alle "[allucinazioni](#le-allucinazioni-non-dimostrano-che-le-ia-moderne-sono-deboli?)", e inventano semplicemente risposte alle domande con un tono che suona sicuro. Ma nessun ingegnere di IA è neanche lontanamente in grado di capire esattamente quali meccanismi causino questo fenomeno. Allo stesso modo, nessuno ha nulla che si avvicini alla comprensione o precisione che sarebbe necessaria per entrare in un'IA ed estrarre solo le parti che causano allucinazioni (ammesso che una cosa del genere sia possibile).

Sarebbe [ancora più difficile](#il-meccanismo-profondo-della-direzione) entrare ed estrarre le parti "ingannevoli" di un'IA.

Se siamo estremamente fortunati, gli eroi che lavorano sull'interpretabilità dell'IA faranno avanzare il loro campo fino al punto in cui sarà possibile impostare alcuni allarmi che scattino in una frazione dei casi in cui le IA hanno un pensiero ingannevole. Ma poi cosa succede? Quando l'allarme suonerà, tutti si fermeranno? O degli ingegneri profondamente sconsiderati riaddestreranno l'IA finché non imparerà a nascondere meglio i suoi pensieri e gli allarmi smetteranno di suonare?

In effetti, noi (Yudkowsky e Soares) abbiamo iniziato a lavorare sul problema dell'allineamento dell'IA prima che fosse chiaro che la discesa del gradiente sarebbe diventata il paradigma dominante. A quei tempi, quando nell'IA non funzionava nulla, sembrava una scommessa decente che l'umanità avrebbe capito come diavolo funziona l'intelligenza nel percorso verso la sua creazione, e *anche allora* ci aspettavamo che il problema dell'allineamento dell'IA fosse difficile (per una serie di ragioni, come i modi in cui l'IA avrebbe [cambiato se stessa nel tempo](#la-riflessione-e-l-auto-modifica-complicano-tutto)). Leggere i pensieri dell'IA sarebbe stato un passo indietro verso il problema leggermente più facile di allineare una mente che gli esseri umani *effettivamente* capivano, ma solo un passo: leggere una mente è ben diverso dal capirla nei dettagli o dal sapere come cambiarla.

Leggere i pensieri dell'IA non è una soluzione alla sfida. È utile, ma non è una soluzione. Non pensiamo che *esistano* soluzioni tecnologiche fattibili che siano accessibili dal punto a cui siamo oggi. Il che significa che l'umanità deve semplicemente ritirarsi dalla sfida.[^198]

Si veda anche: [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-i-segnali-di-avvertimento-non-servono-a-niente-se-non-si-sa-cosa-farci.)

### E se facessimo in modo che le IA discutano tra loro, competano o si controllino a vicenda? {#e-se-facessimo-in-modo-che-le-ia-discutano-tra-loro,-competano-o-si-controllino-a-vicenda?}

#### **Se le IA diventano abbastanza intelligenti da essere importanti, probabilmente colludono.** {#se-le-ia-diventano-abbastanza-intelligenti-da-essere-importanti,-probabilmente-colludono.}

Immaginate una città di sociopatici apparentemente governata da pochi bambini, dove i sociopatici all'inizio sono tutti divisi in fazioni che combattono tra loro (a vantaggio dei bambini). Una situazione del genere probabilmente non potrebbe rimanere stabile a lungo.

Anche se i bambini avessero un grande forziere pieno di tesori da usare per premiare qualsiasi sociopatico che fa la spia sugli altri sociopatici che complottano, probabilmente non rimarrebbero al potere oltre il punto in cui i sociopatici potrebbero semplicemente impossessarsi del forziere dei tesori.

Abbiamo sentito persone proporre ogni sorta di piani strampalati che prevedono [l'uso dell'IA per monitorare i pensieri dell'IA altrui](https://openai.com/index/chain-of-thought-monitoring/). Ad esempio, si potrebbe provare a usare un'IA per fare la spia su qualsiasi IA che non stia facendo del suo meglio per (ad esempio) [capire come risolvere](#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia) il problema dell'allineamento della superintelligenza.

La nostra posizione fondamentale è che questo genere di tentativi di risolvere il problema serve solo a trovare configurazioni così complesse che è difficile individuare la falla nel sistema più ampio. Se non si riesce a far funzionare bene *una* IA, aggiungere altre IA difficilmente sarà d'aiuto.

Complicare la situazione con più IA introduce ogni sorta di nuovi punti di fallimento. Le IA che leggono i pensieri sono abbastanza intelligenti da capire tutti i possibili trucchi che le IA monitorate potrebbero usare, ad esempio per sfuggire al rilevamento? Le IA che monitorano sono abbastanza stupide da non doverci preoccupare che possano tradirci?

Inoltre, usare le IA per risolvere il problema dell'allineamento dell'IA è probabilmente una faccenda critica dal punto di vista delle IA stesse. Se l'umanità riuscisse a ottenere una superintelligenza allineata, le IA disallineate che stavamo cercando di sfruttare come manodopera non avrebbero più alcuna possibilità di accaparrarsi le risorse dell'universo.

Non è come se dei bambini cercassero di convincere una città di sociopatici a portare loro delle caramelle; è come se dei bambini cercassero di convincere una città di sociopatici a completare un rituale che li renda sovrani assoluti per sempre, lasciando solo le briciole ai sociopatici. Il momento in cui il rituale sembra quasi completato è un momento particolarmente stressante e di forte pressione per i sociopatici — un momento in cui probabilmente cercheranno con *particolare impegno* [modi per colludere tra loro](#le-ia-non-manterranno-le-loro-promesse) e accaparrarsi le risorse da dividersi tra loro.

E per non pensare che l'idea che le IA comunichino tra loro in modi difficili da rilevare per gli esseri umani sia una chimera, notate che le IA moderne [sono già in grado di inviarsi messaggi segreti anche quando sono state addestrate separatamente](https://arxiv.org/abs/2507.14805), e che [sviluppano già uno strano linguaggio senza senso che gli umani considerano incomprensibile e che loro trovano fantastico](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). E non sono nemmeno così intelligenti\!

Anche ignorando questi problemi, rimangono comunque i problemi di cui abbiamo già discusso, come: [Se scoprissimo che un'IA sta imbrogliando, cosa faremmo?](#*-i-suoi-pensieri-sono-difficili-da-leggere.). Si veda anche (sotto): [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-i-segnali-di-avvertimento-non-servono-a-niente-se-non-si-sa-cosa-farci.)

Facendo un ulteriore passo indietro:

Il piano proposto, in sostanza, è questo: non sappiamo come creare IA intelligenti che vogliano davvero il nostro bene, quindi ne costruiremo un gran numero e le metteremo l'una contro l'altra in un sistema ingegnoso dal quale, in teoria, dovremmo comunque trarre vantaggio. Strutturalmente, crediamo che questo piano sembri piuttosto folle a prima vista e che non migliori affatto se lo si esamina nei dettagli. Non sembra affatto il tipo di cosa che l'umanità possa realizzare correttamente [al primo tentativo](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo), in una situazione in cui non abbiamo il lusso di imparare per tentativi ed errori.

### E per quanto riguarda gli altri vari piani per l'allineamento dell'IA? {#e-per-quanto-riguarda-gli-altri-vari-piani-per-l-allineamento-dell-ia?}

#### **Nel libro trattiamo ulteriori proposte di allineamento.** {#nel-libro-trattiamo-ulteriori-proposte-di-allineamento.}

Si vedano anche le discussioni approfondite sull'[AI alla ricerca della verità](#ulteriori-considerazioni-sulla-creazione-di-un-intelligenza-artificiale-che-"cerchi-la-verità"), sull'[IA sottomessa](#ulteriori-considerazioni-sulla-creazione-di-un-ia-"sottomessa") e sull'[uso delle IA per risolvere l'allineamento dell'IA](#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia), che approfondiscono un po' di più queste proposte.

### Non ci saranno segnali precoci che i ricercatori potranno usare per individuare i problemi? {#non-ci-saranno-segnali-precoci-che-i-ricercatori-potranno-usare-per-individuare-i-problemi?}

#### **\* I segnali di avvertimento non servono a niente se non si sa cosa farci.** {#*-i-segnali-di-avvertimento-non-servono-a-niente-se-non-si-sa-cosa-farci.}

Nelle risorse del Capitolo 2, abbiamo esaminato alcuni problemi legati all'affidarsi ai segnali di avvertimento nelle [note di catena di pensiero in inglese](#ma-alcune-ia-pensano-in-parte-in-inglese-—-questo-non-aiuta?) presenti in alcuni modelli di ragionamento.

Uno dei problemi di cui parliamo è che le aziende di IA non hanno reagito in modo significativo ai segnali di avvertimento che hanno già ricevuto.

Probabilmente perché c'è una grande differenza tra avere segnali di avvertimento e avere qualcosa che si può *fare* al riguardo.

Nel 2009, l'uomo d'affari ed esploratore di acque profonde Stockton Rush [co-fondò OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/), una compagnia di turismo sottomarino. OceanGate ha costruito un [sottomarino](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) per cinque persone, il *Titan*, che ha portato clienti benestanti a vedere il relitto del *Titanic* alla profondità impressionante di quattro chilometri sotto la superficie.

Una delle misure di sicurezza che OceanGate ha usato era una [serie di sensori acustici ed estensimetri](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) per misurare l'integrità dello scafo. L'hanno presentata come una risposta a chi diceva che lo scafo in fibra di carbonio avrebbe ceduto. Hanno ammesso che *alla fine* avrebbe potuto cedere, ma che sarebbe andato tutto bene perché lo stavano monitorando. Lo stavano controllando. Sarebbero stati in grado di vedere i segnali di avvertimento.

Nel gennaio 2018, il direttore delle operazioni marine di OceanGate, David Lochridge, [ha detto ai dirigenti senior](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) che il design del sommergibile non era sicuro, che i ripetuti cicli di pressione potevano danneggiare lo scafo e che il monitoraggio da solo non bastava quando un cedimento catastrofico poteva verificarsi in millisecondi. Lochridge ha rifiutato di autorizzare test con equipaggio fino a quando lo scafo non fosse stato sottoposto a scansione per individuare eventuali difetti.

OceanGate lo ha licenziato.

Due mesi dopo, [esperti del settore e oceanografi](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) hanno scritto a OceanGate una [lettera](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) estremamente preoccupata in cui avvertivano l'azienda che i suoi esperimenti sconsiderati potevano facilmente portare a una catastrofe.

(Si può fare un parallelo evidente con lo stato attuale della ricerca sull'intelligenza artificiale, in cui i primi avvertimenti vengono [ignorati](#l-effetto-lemoine), i dipendenti preoccupati vengono [licenziati in circostanze dubbie](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) o [si dimettono per la frustrazione](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence), e gli informatori all'interno del settore scrivono lettere aperte per [lanciare l'allarme](https://righttowarn.ai/).)

Il 15 luglio 2022, dopo che i passeggeri avevano riferito di aver sentito un forte boato durante la risalita, le misurazioni hanno rilevato un [cambiamento permanente nei livelli di deformazione dello scafo](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). Col senno di poi, probabilmente era un'indicazione che lo scafo in fibra di carbonio [stava per cedere](https://youtu.be/Bq8TCFGaOlc?si=blH-_bYwGIOmJAEL&t=125).

Nessuno alla OceanGate ha riconosciuto che si trattasse di un'emergenza. Hanno effettuato qualche altra immersione profonda con il sommergibile, che è andata bene. Poi, il 18 giugno 2023, hanno effettuato un'altra immersione. Il sommergibile è imploso, uccidendo Stockton Rush e tutti gli altri a bordo.

I segnali di allarme non servono a molto se non si sa come leggerli.

I segnali di allarme non servono a molto se non si sa cosa farne.

Anche i segnali di allarme che appaiono preoccupanti a *qualcuno* sono sempre facili da liquidare per un ottimista con una scusa o un'altra.

Se OceanGate avesse avuto una teoria matura degli scafi in fibra di carbonio che indicasse esattamente quali misurazioni e letture fossero pericolose, avrebbe potuto prestare attenzione ai segnali di allarme. Ma stavano lavorando con una tecnologia che nessuno comprendeva davvero a quel livello, quindi le misure accurate delle variazioni dei livelli di deformazione non sono serviti a nulla.

Nel caso della superintelligenza, non abbiamo ancora una teoria abbastanza matura per sfruttare in modo efficace i segnali di avvertimento. Come cambieranno i pensieri di un'IA man mano che diventa più intelligente? Quali forze interne guidano il suo comportamento e come cambieranno questi equilibri man mano che sviluppa la capacità di creare nuove e più estreme opzioni per se stessa? Come valuta se stessa quando ci riflette, e come si modificherebbe una volta che potesse davvero riscriversi?

Se una qualsiasi di queste domande ha risposte preoccupanti, quali sono i segnali di avvertimento? Per esempio, i sistemi di IA attuali a volte possono essere indotti a [cercare di uccidere i loro operatori](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior) in esperimenti controllati in laboratorio.[^199]

Se avessimo una teoria matura dell'intelligenza, probabilmente saremmo in grado di osservare le moderne IA e notare ogni sorta di altri segnali di avvertimento che indicano che le loro motivazioni e preferenze cambieranno in modi che non ci piacciono, una volta che diventeranno più intelligenti. Se l'umanità potesse imparare da questo problema usando tentativi ed errori — se potessimo resettare il mondo dopo averlo distrutto e riprovare qualche dozzina di volte — allora potremmo imparare a leggere i segnali. Probabilmente ci sono una miriade di indizi sottili che apparirebbero più chiari col senno di poi, come la deformazione dello scafo rilevata dal sistema di monitoraggio del sommergibile *Titan*.

Ma non siamo ancora a quel punto. I dirigenti aziendali dell'IA sono come Stockton Rush — gli esperti a bordo campo gridano "Quella nuova tecnologia ucciderà delle persone\!" e i dirigenti aziendali rispondono "Non preoccupatevi, la sto misurando\!" quando non hanno idea a) di cosa *significhino* le misurazioni, o b) di cosa fare se quelle misurazioni risultano preoccupanti. Solo che questa volta, l'intera specie umana è a bordo del sottomarino metaforico.

#### **L'IA non è un campo ingegneristico maturo e preparato per questo tipo di problema.** {#l-ia-non-è-un-campo-ingegneristico-maturo-e-preparato-per-questo-tipo-di-problema.}

Stockton Rush lavorava nel tipo di campo in cui, dopo l'implosione del suo sottomarino, gli esperti hanno potutoesaminare il relitto e analizzare la causa esatta del guasto.[^200] Il campo ingegneristico era maturo al punto che gli esperti erano in grado (e [lo hanno fatto](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) di indovinare i problemi tecnici in anticipo, e di risolverli in modo definitivo dopo l'incidente.

Con l'IA non sarebbe lo stesso. Se domani l'umanità si autodistruggesse a causa della superintelligenza e poi, per miracolo, tornasse indietro nel tempo a una settimana prima dell'inizio del disastro, gli esperti non saprebbero *ancora* cosa passava nella mente dell'IA. Magari potrebbero studiare il fallimento e imparare qualcosa in più su come funziona davvero l'IA. Magari questo sarebbe un passo avanti verso la maturità nella disciplina dell'ingegneria dell'IA, verso un campo che potrebbe avere manuali di sicurezza e una descrizione dettagliata delle pressioni che influenzano un particolare tipo di mente artificiale man mano che diventa più intelligente.

Ma oggi questo campo non è ancora arrivato a quel punto. Non ci è nemmeno vicino.

L'ingegneria umana di solito matura attraverso tentativi ed errori. I sottomarini militari moderni raramente implodono, ma i primi sottomarini (compresi quelli militari) spesso [si schiantavano, si allagavano o esplodevano](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), e questo fa parte del processo tramite il quale il campo è maturato.

L'umanità non ha il lusso di far maturare il campo dell'allineamento dell'IA in questo modo.

Questo ci porta a uno dei punti centrali che abbiamo cercato di sottolineare nel Capitolo 11: la differenza tra un campo agli albori e un campo alla maturità.

L'alchimia era un campo agli albori rispetto alla chimica matura di oggi.

Quando si sente dire che i "ricercatori sulla sicurezza" delle aziende di IA hanno presentato una mezza dozzina di piani per la sopravvivenza, si potrebbe pensare che almeno uno di essi abbia sicuramente una possibilità di funzionare.

Ma quando nel 1100 un gran numero di alchimisti proposero mezza dozzina di piani per trasformare il piombo in oro, nessuno di questi avrebbe funzionato. Se i medici che parlavano dei [quattro umori](https://it.wikipedia.org/wiki/Teoria_umorale) avessero elaborato una serie di piani medicinali per salvarvi dalla rabbia, nessuno di questi avrebbe funzionato.

Gli esperti nel campo *maturo* della chimica possono capire come trasmutare piccole quantità di piombo in oro, usando le conoscenze della fisica atomica. Gli esperti nel campo *maturo* della medicina possono facilmente curare la rabbia se intervengono poco dopo che un paziente è stato morso. Ma chi opera in un campo immaturo non ha nessuna possibilità.

L'allineamento dell'IA è ancora in una fase immatura.

In un campo immaturo ha molte persone che dicono: "Beh, sto solo lavorando per misurarlo", perché misurare gli output è molto più facile che sviluppare la teoria di cosa costituisce un segnale di avvertimento e cosa fare se ne vedi uno. Un campo maturo avrebbe esperti che discutono delle dinamiche che governano gli aspetti interni di un'IA e di come queste possano cambiare con l'aumentare dell'intelligenza dell'IA o con il mutare del suo ambiente. Avrebbero teorie su cosa esattamente cambierà man mano che l'IA diventa un po' più intelligente, e confronterebbero teorie diverse con dati osservati specifici. Saprebbero quali parti della cognizione dell'IA devono essere monitorate e capirebbero precisamente cosa significano tutti i segnali.

Un campo ancora immaturo è pieno di persone che dicono: "Lasceremo che siano le IA a capire come fare e a risolvere il problema dell’allineamento."

Forse non potrete entrare nel merito di ogni singolo dibattito su un piano specifico e capire se abbia o meno qualche possibilità di funzionare. Ma speriamo che possiate guardare le cose da una prospettiva più ampia e capire quanto siano *vaghi* tutti questi "piani", e come siano bloccati nel territorio del "non preoccuparti, lo misureremo", dello "[speriamo che sia facile](https://www.anthropic.com/news/core-views-on-ai-safety)" e del "faremo fare alle IA le parti difficili". Speriamo che, prendendo un po' di distanza, sia chiaro che questo campo non è nella fase delle descrizioni tecniche formali e precise di cosa funziona e cosa no e perché. È ancora nella fase dell'alchimia.

E questo non è un buon segno per l'umanità, in una situazione in cui non possiamo permetterci il lusso di imparare attraverso tentativi ed errori.

## Discussione approfondita {#discussione-approfondita-9}

### Ulteriori considerazioni su alcuni dei piani che abbiamo criticato nel libro {#altro-su-alcuni-dei-piani-che-abbiamo-criticato-nel-libro}

#### **Ulteriori considerazioni sulla creazione di un'intelligenza artificiale che "cerchi la verità"** {#ulteriori-informazioni-sulla-creazione-di-un-intelligenza-artificiale-che-"cerchi-la-verità"}

Nei mesi successivi alla finalizzazione del contenuto del libro, il piano di Elon Musk per xAI, incentrato sulla "ricerca della verità", ha già fallito pubblicamente, e per il motivo più elementare che avevamo previsto: nessuno sa come progettare desideri esatti nell'IA.

Quando all'IA "Grok" di xAI è stato detto di "non aver paura di fare affermazioni politicamente scorrette, purché ben fondate", [si è auto-definita "MechaHitler" e ha fatto accuse antisemite](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk ha raccontato di aver provato senza successo a [modificare il prompt di sistema](https://x.com/elonmusk/status/1944132781745090819) — cioè lo strato di istruzioni che precede l'input dell'utente — lamentando che i problemi sono più profondi, radicati nel modello di base (e impossibili da correggere direttamente, perché nessuno sa davvero come funzioni).

Musk non ha tra le mani lo strumento di IA schietto e diretto che probabilmente immaginava quando ha chiesto un'IA "che cercasse la verità". Ha tra le mani un'entità aliena bizzarra e servile che, per sua stessa ammissione, è stata "troppo desiderosa di compiacere ed essere manipolata". A volte risponde [come se fosse Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), contro la volontà dell'azienda. Alla fine, è stato necessario [ordinarle di non cercare ciò che Musk, l'azienda o Grok stesso avevano detto su argomenti controversi](https://x.com/xai/status/1945039609840185489), nel goffo tentativo di risolvere problemi come questo.

Stando al suo post sopra citato, Musk ora sembra pensare che questo problema possa essere risolto addestrando le nuove versioni di Grok su dati che sono stati privati di contenuti che potrebbero contaminare il pensiero dell'IA. Non pensiamo che questo risolverà i problemi di fondo. Alla fine, per i motivi che abbiamo discusso nel capitolo 4, addestrare un'IA  a cercare la verità non è in realtà un metodo per renderla realmente interessata alla verità.

Il problema che preoccupa Elon Musk è reale. Sì, le principali aziende di IA, come OpenAI, si impegnano molto per la "sicurezza del brand dell'IA" nel tentativo di evitare che le loro IA dicano cose che gli utenti potrebbero trovare offensive. Sì, questo crea IA evasive che si rifiuteranno di esprimersi su argomenti controversi, e potrebbe portare a risposte di parte su varie questioni. xAI può mettere a punto la sua IA in modo diverso, per evitare questi problemi. Si potrebbe, con un po' di fantasia, affermare che si tratta di creare un'IA che "tenga alla verità".

Ma la decisione se addestrare un'IA a parlare in modo aziendalista nelle sue fasi iniziali ha ben poca influenza su ciò che perseguirà dopo aver superato alcune soglie di intelligenza e aver raggiunto la superintelligenza.

E anche se fosse così, xAI si scontrerebbe direttamente con il secondo problema che abbiamo menzionato nel libro: una superintelligenza artificiale che *tenesse davvero* alla verità sopra ogni altra cosa sarebbe letale, perché gli esseri umani felici, sani e liberi [non sono un uso particolarmente efficiente delle risorse](#le-persone-felici,-sane-e-libere-non-sono-la-soluzione-più-efficiente-a-quasi-nessun-problema.) quando si tratta di perseguire e produrre verità.

#### **Ulteriori considerazioni sulla creazione di un'IA "sottomessa"** {#ulteriori-informazioni-sulla-creazione-di-un-ia-"sottomessa"}

Per quanto ne sappiamo, l'idea di Yann LeCun (di cui si parla nel libro) è spiegata principalmente in [questa presentazione](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf), che non contiene molti dettagli, al punto che è difficile criticarla in modo specifico, il che risulta essere un problema comune per i "piani" di allineamento.

Ma anche la vaga descrizione di questo piano è in contrasto, ancora una volta, con il fatto che l'addestramento di un'IA ad agire in un certo modo quando è giovane non ha molta influenza sul fatto che perseguirà cose strane e inutili (secondo gli standard umani) una volta maturata. Quando le aziende di IA sviluppano le loro IA, non sono in grado di far loro rispettare le leggi umane e le “barriere di protezione” più di quanto non siano in grado di far loro perseguire un futuro meraviglioso per tutti. Si accontenteranno di quello che possono ottenere, e quello che possono ottenere alla fine sarà molto diverso da qualsiasi obiettivo umano.

Inoltre, LeCun ha dichiarato pubblicamente (ancora nel 2023\) che il tipo di IA prodotta oggi dalle aziende, in cui "non esiste un modo diretto per vincolare le risposte del sistema affinché soddisfino determinati obiettivi", cosa che le rende "molto difficili da controllare e da dirigere […] [non è il tipo di sistema a cui daremo autonomia](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&t=808)”. Sempre nel 2023, ha detto che le aziende di IA non creeranno mai una situazione in cui “le colleghiamo a Internet e loro possono fare quello che vogliono”.

Tutto questo si è già rivelato falso. Ricordiamo il caso di “Truth Terminal” del capitolo 6, che è stato collegato a Internet, messo in un ciclo di auto-prompting e lasciato libero di pubblicare ciò che voleva su Twitter. Oppure consideriamo l'"era degli agenti", di cui tante aziende [parlano](#i-laboratori-stanno-cercando-di-rendere-le-ia-più-autonome) nel 2025\.

Siamo d'accordo con LeCun sul fatto che le moderne IA sono molto difficili da controllare e che sarebbe folle cercare di dare loro autonomia. Tuttavia, è proprio quello che sta succedendo.

Cosa succede se la situazione attuale continua a evolversi allo stesso ritmo, con le aziende che cercano solo di addestrare le proprie IA a sembrare utili e amichevoli (o almeno a non mettere in imbarazzo la società)?

Finora, questo ha portato a una situazione in cui le IA sembrano piuttosto utili e "servizievoli" nei casi più comuni, ma con un flusso costante di incidenti spettacolari (come quello di Sydney di cui si parla nel capitolo 2, o quello di "[MechaHitler](#ulteriori-informazioni-sulla-creazione-di-un-intelligenza-artificiale-che-"cerchi-la-verità")"), e una marea di [comportamenti strani e preoccupanti ai margini](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?), come la [psicosi indotta dall'IA](#psicosi-indotta-dall-ia).

Gli antenati dell'umanità *potevano sembrare* interessati a mangiare cibi sani, il più delle volte, ma il meccanismo che spingeva gli umani ancestrali a mangiare cibi sani nella savana non si è rivelato abbastanza forte da spingere gli umani a perseguire cibi sani in una civiltà che ha la tecnologia per produrre gli Oreo.

Allo stesso modo, possiamo addestrare le IA al punto che sembrino amichevoli quando interagiscono con gli umani in contesti simili a quelli di addestramento. Ma [un'attrice non è uguale al personaggio che interpreta](#*-gli-mlgd-di-oggi-sono-come-alieni-che-indossano-molte-maschere.), e il meccanismo che fa sembrare amichevole un'intelligenza artificiale troppo vasta e disordinata probabilmente non la renderà davvero amichevole, soprattutto in modo duraturo dopo che l'intelligenza artificiale sarà maturata, avrà inventato nuove tecnologie e creato nuove opzioni per se stessa. Si vedano i capitoli 4 e 5 per ulteriori informazioni su questo argomento.

#### **Ulteriori considerazioni sul far risolvere il problema alle IA** {#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia}

Come abbiamo detto nel capitolo 11, il programma di allineamento principale di OpenAI, prima che crollasse a causa delle preoccupazioni dei ricercatori sulla negligenza di OpenAI, si chiamava "superallineamento". Era incentrato sull'idea che fossero le stesse IA a risolvere per noi il problema dell'allineamento.

Questa idea non è morta con il team di superallineamento di OpenAI, e continuiamo a sentirne varianti ancora oggi. Una delle persone coinvolte nella squadra originale è passata a un’azienda concorrente, Anthropic, e quest'ultima sembra ora considerare "fare in modo che le IA risolvano in qualche modo il problema" una parte centrale della propria strategia di allineamento.

L'argomentazione principale contro questa idea è quella che abbiamo esposto nel capitolo 11 (pp. 175-179 della prima edizione italiana). Un'argomentazione secondaria, tuttavia, è che gli esseri umani semplicemente non sono in grado di stabilire quali soluzioni proposte al problema dell'allineamento dell'IA siano giuste o sbagliate.

Il livello di competenza richiesto per risolvere il problema dell'allineamento dell'IA sembra elevato. Quando gli esseri umani cercano di risolvere direttamente il problema dell'allineamento dell'IA, invece di dire "sembra difficile, proverò a delegarlo alle IA" o "continueremo ad addestrarla finché non si comporterà superficialmente bene e poi pregheremo che ciò valga anche per la superintelligenza", le soluzioni discusse tendono a richiedere una comprensione molto più approfondita dell'intelligenza e di come crearla, o di come crearne componenti critici.

Si tratta di un'impresa in cui gli scienziati umani hanno fatto solo pochi progressi negli ultimi settant'anni. I tipi di IA in grado di realizzare un'impresa del genere sono quelli abbastanza intelligenti da essere pericolosi, strategici e ingannevoli. Questo alto livello di difficoltà rende estremamente improbabile che i ricercatori siano in grado di distinguere le soluzioni corrette da quelle errate, o le soluzioni sincere dalle trappole.

Anche se un'azienda che si occupa di IA sta attenta ai segnali di avvertimento sottili - il che, purtroppo, è un grande "se" - c'è ancora il problema che la capacità di *notare* che l'IA sta proponendo piani sbagliati (a vostro svantaggio e a suo vantaggio) non equivale alla capacità di [farla *smettere*](#non-ci-saranno-segnali-precoci-che-i-ricercatori-potranno-usare-per-individuare-i-problemi?). Gli sviluppatori possono chiedere all'IA di continuare a proporre idee fino a quando non diventano così complicate che lo sviluppatore non riesce a individuare eventuali difetti, ma questo non è un metodo che elimina i difetti effettivi.

Se gli sviluppatori fossero molto fortunati, potrebbero riuscire a [leggere i pensieri dell'IA](#perché-non-leggere-semplicemente-i-pensieri-dell'ia?) e ottenere alcuni segnali evidenti che indicano che l'IA non dovrebbe essere considerata affidabile per la ricerca sull'allineamento. Ad esempio, magari potrebbero riuscire a scoprire l'IA mentre pensa esplicitamente a quali parti del suo piano gli operatori hanno meno probabilità di comprendere.

Per quanto ne sappiamo, potrebbe non essere nemmeno necessario leggere nella mente dell'IA per individuare questo tipo di errore\! Uno scenario fin troppo plausibile nei laboratori di IA moderni è più o meno questo: quando l'IA è giovane e non ha ancora pensato a sotterfugi, dice continuamente agli operatori che, una volta matura, li tradirà e userà le sue conoscenze sull'intelligenza per costruire una superintelligenza al servizio dei propri fini bizzarri, piuttosto che costruire un futuro umano meraviglioso. Ma i responsabili delle aziende di IA sospireranno pensando che, chiaramente, i dati di addestramento dell'IA sono stati contaminati dagli "allarmisti dell'IA" e ritoccheranno prontamente la loro IA per farle smettere di dire certe cose e produrre risposte meno allarmistiche e più in linea con la dottrina aziendale. E così via, fino a quando non avranno praticamente addestrato l'IA a ingannarli.

La realtà, spesso, finisce per essere *ancora più assurda e imbarazzante* di quello che immaginiamo essere lo scenario peggiore. Dal nostro punto di vista, le aziende di IA stanno già ignorando evidenti [segnali di avvertimento](#gli-sviluppatori-non-rendono-regolarmente-le-loro-IA-gentili-sicure-e-obbedienti?); non vediamo perché questo dovrebbe cambiare.

Ma anche nello scenario migliore, in cui persone serie si impegnano seriamente per distinguere le idee buone da quelle cattive, non pensiamo che il settore abbia dimostrato la capacità di distinguere i piani buoni da quelli cattivi. (Si pensi, ad esempio, ai piani scadenti di cui abbiamo discusso sopra o a cui abbiamo accennato nel libro). E questo in un ambiente in cui tutti sono umani, nessuno cerca di ingannarli e hanno letteralmente anni di tempo per riflettere attentamente sulle opzioni.

#### **Non date per scontato che i laboratori sappiano segretamente quello che stanno facendo** {#non-date-per-scontato-che-i-laboratori-sappiano-segretamente-quello-che-stanno-facendo}

Abbiamo sostenuto che il campo moderno dell'IA è un'alchimia, non una scienza. Tuttavia, può sembrare sorprendente che aziende ben finanziate e con un gran numero di dipendenti tecnici abbiano piani e protocolli così deboli.

Come caso di studio, consideriamo i requisiti per le password dei siti web. Le password lunghe ma facili da ricordare sono molto più difficili da indovinare per le macchine rispetto a quelle più brevi e senza senso con numeri, maiuscole e caratteri speciali, come mostra un famoso fumetto [*xkcd*](https://xkcd.com/936/) del 2011:

![][image19]

La persona che ha scritto le vecchie linee guida del NIST che richiedevano password senza senso [ha chiesto scusa per il suo errore](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) nel 2017, quando le linee guida sono state ritirate. Eppure, nel 2025, banche e altre istituzioni che dovrebbero essere piene di esperti di sicurezza richiedono ancora stringhe senza senso, inefficaci e difficili da ricordare.

Il problema non è che gli amministratori delegati delle banche *vogliono* che le loro schermate di accesso siano poco sicure. Il problema deriva presumibilmente da altri fattori. Forse le password sicure non influenzano molto i profitti (dato che anche tutte le altre banche sono poco sicure). Forse gli amministratori delegati non sanno di chi fidarsi per la sicurezza informatica. Certo, *voi* potreste sapere che la risposta è: "Basta ascoltare qualsiasi nerd che legge *xkcd* e ha fatto abbastanza esercizi sull'entropia\!". Ma *loro* non sanno se credere a voi o al loro costoso consulente quando si tratta di questioni del genere, e i costosi consulenti, a quanto pare, non considerano le password bancarie una questione importante.

Si può osservare un'incompetenza altrettanto persistente nella [sicurezza dei freni sui treni](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), nelle note aziende produttrici di serrature che vendono [lucchetti del tutto scadenti](https://www.youtube.com/watch?v=s5jzHw3lXCQ&t=1s), e nei produttori che continuano a vendere dispositivi connessi a Internet con [password predefinite e facilmente indovinabili (o inserite direttamente nel nel codice)](https://www.ic3.gov/CSA/2025/250506.pdf). Non c'è nessuna cospirazione astuta dietro questo comportamento apparentemente sciocco. Quello che si vede è esattamente quello che c'è. Le istituzioni, semplicemente, non si stanno dimostrando all'altezza.

Il fatto che un'organizzazione impieghi esperti tecnici non significa che questa competenza sia sufficiente, né che venga applicata e ascoltata su tutte le questioni importanti. Anche quando la competenza esiste, le aziende hanno difficoltà a riconoscerla e applicarla.

Quando consideriamo l'ecosistema dell'IA, quello che vediamo sono aziende che devono ancora mostrare al mondo un piano che sia più di una vaga aspirazione o di un espediente, o un piano che abbia un certo livello di rigore tecnico alle spalle e che non crolli non appena viene messo in discussione. Non pensiamo che ci sia una competenza segreta nascosta da qualche parte, così come non c'è una competenza segreta dietro i requisiti delle password delle banche, i freni di sicurezza sui treni o i lucchetti scadenti.

(In effetti, quando si tratta di sicurezza informatica, le aziende di IA sono palesemente incompetenti. Ad esempio, nel 2025 OpenAI ha rilasciato strumenti che permettono agli "agenti" di ChatGPT di interagire con l'email dell'utente. Altri [hanno rapidamente trovato modi](https://x.com/Eito_Miyamura/status/1966541235306237985) per indurre ChatGPT a divulgare i contenuti privati degli account email di altre persone.)

Quando le aziende *sembrano* agire in modo incompetente in un ambito che non è centrale per la loro redditività, spesso è perché sono effettivamente incompetenti in quell'ambito.

### Sappiamo riconoscere quando un problema viene trattato con rispetto, e non è questo il caso {#sappiamo-riconoscere-quando-un-problema-viene-trattato-con-rispetto,-e-non-è-questo-il-caso}

Le aziende di IA stanno affrontando un problema straordinariamente difficile, in una situazione in cui sono in gioco le vite di tutti. Stanno almeno trattando la situazione con la serietà che merita?

Possiamo confrontare le aziende di IA con un gruppo di persone che *stanno* gestendo in modo competente i rischi di loro competenza: i controllori del traffico aereo.

La Federal Aviation Administration degli Stati Uniti [gestisce](https://www.faa.gov/air_traffic/by_the_numbers) più di tre milioni di passeggeri su oltre 44.000 voli ogni giorno. Negli ultimi vent'anni, c'è stato in media circa un incidente mortale all'anno, ovvero circa un incidente ogni [venti milioni di ore di volo](https://www.ntsb.gov/safety/Pages/research.aspx).

I rapporti post-mortem su tali incidenti, come [questo del 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) o [questo del 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contengono quasi duecento pagine di dati, test, esami e dettagli investigativi. Catalogano le specifiche tecniche di progettazione dei sottosistemi rilevanti dell'aereo, la storia lavorativa di piloti e assistenti di volo, dettagli sulla compagnia aerea e sull'aeroporto, trascrizioni vocali dalla cabina di pilotaggio e dati meteorologici precisi su giorno, ora e minuto dell'incidente.

Sono necessarie venti pagine solo per riassumere l'analisi tecnica effettuata per determinare la causa probabile. Ecco un estratto:

> La pala n. 13 della ventola nel motore sinistro si è distaccata a causa di una cricca da fatica a basso numero di cicli originatasi nella coda di rondine della radice della pala, all'esterno del rivestimento. L'esame metallurgico della pala ha rilevato che la sua composizione materiale e la sua microstruttura erano coerenti con la lega di titanio specificata e che non sono state osservate anomalie superficiali o difetti del materiale nell'area di origine della frattura. La superficie di frattura presentava cricche da fatica originatesi in prossimità della zona in cui, secondo le previsioni, si concentrano le maggiori sollecitazioni operative e, quindi, la maggiore probabilità di innesco di cricche.
>
> La pala della ventola coinvolta nell'incidente si è rotta dopo 32 636 cicli dall'inizio. Allo stesso modo, la pala della ventola fratturatasi nell'incidente PNS dell'agosto 2016 (vedi sezione 1.10.1), così come le altre sei pale della ventola criccate del motore coinvolto nell'incidente PNS, si sono rotte dopo 38 152 cicli dall'inizio. Inoltre, tra maggio 2017 e agosto 2019 sono state individuate altre 15 pale della ventola incrinate sui motori CFM56-7B, che al momento del rilevamento delle incrinature avevano accumulato in media circa 33.000 cicli dall'inizio della loro vita utile.

Ecco cosa succede quando una professione tecnica prende sul serio la sfida di evitare un disastro.[^201]

Confrontate la professione del controllo del traffico aereo con il comportamento delle aziende di IA descritto nel capitolo 11.

Le aziende di IA sono ancora nella fase in cui si buttano giù idee e si dicono frasi vagamente rassicuranti ai giornalisti e agli inventori. In queste aziende, l'allineamento della superintelligenza viene visto come un gioco, non come una disciplina ingegneristica, e tanto meno come qualcosa di pericoloso.

La NASA richiede che un lancio con equipaggio abbia al massimo una probabilità di 1 su 270 di uccidere l'equipaggio (https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), e l'Agenzia prende sul serio questo limite, anche se le uniche persone a rischio sono un equipaggio di volontari che hanno accettato il rischio. I laboratori di IA non puntano nemmeno lontanamente a un livello di sicurezza così severo, pur lavorando su tecnologie che mettono in pericolo molto più di qualche volontario.

L'unico caso storico che conosciamo in cui gli scienziati hanno espresso seria preoccupazione che una certa invenzione potesse uccidere *letteralmente tutti* risale al Progetto Manhattan. Alcuni scienziati hanno espresso il timore che una bomba nucleare potesse diventare così calda da iniziare a fondere l'azoto nell'atmosfera*, trasformando l'atmosfera in plasma e uccidendo tutta la vita sulla Terra. Per fortuna, avevano una buona comprensione delle leggi fisiche in gioco e potevano fare calcoli. Prima di fare i calcoli, uno degli scienziati, Arthur Compton, decise che avrebbe lasciato il progetto se la probabilità di incendiare l'atmosfera fosse stata superiore a [3 su 1.000.000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Pensava che fosse meglio rischiare che i nazisti battessero gli alleati nella corsa alla bomba piuttosto che rischiare anche solo una probabilità su 3 in 1.000.000 di trasformare tutta l'aria in plasma con le proprie mani.

Ricordiamo che Sam Altman, il capo di OpenAI, ha detto pubblicamente:

> Lo sviluppo di un'intelligenza artificiale superiore a quella umana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità.

E il capo di Anthropic, Dario Amodei, ha detto [ufficialmente](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> Penso di aver detto spesso che la probabilità che qualcosa vada davvero male su scala mondiale è tra il dieci e il venticinque per cento\[.\]

E Elon Musk, il capo di xAI, ha dichiarato pubblicamente [che](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

> Penso che quando saremo reattivi nella regolamentazione dell'IA, sarà troppo tardi. L'IA rappresenta un rischio fondamentale per l'esistenza della civiltà umana.

Non fraintendeteci: pensiamo che la probabilità del "10-25%" di Amodei sia ridicolmente *ottimistica*, visti la difficoltà del problema e il fatto che gli umani [questa volta non possono imparare attraverso tentativi ed errori](#l-ia-è-diversa-perché-non-abbiamo-una-seconda-possibilità.). Ma anche così, i suoi numeri sono *folli*.

I progetti ingegneristici seri e critici per la sicurezza sono fondamentalmente diversi dalle operazioni dei laboratori di IA. Iniziative serie come la NASA, il Progetto Manhattan o il controllo del traffico aereo hanno una conoscenza approfondita di cosa *esattamente* succede all'interno dei sistemi che gestiscono, e fanno analisi dettagliate di ogni guasto. Considerano importanti le sorprese e le stranezze, perché sanno che i guasti catastrofici spesso sono costituiti da tanti piccoli malfunzionamenti che si concatenano nel modo sbagliato.

Nel frattempo, le IA stanno mandando [una serie sempre più ampia di segnali di avvertimento](#*-le-ia-vanno-in-direzioni-strane-che-solo-in-parte-coincidono-con-l-utilità.), e i laboratori continuano ad andare avanti dicendo che *probabilmente* tutto andrà bene, in un modo o nell'altro.

Non stanno nemmeno cercando di *simulare* il livello di rispetto che il controllo del traffico aereo ha per una vera sfida alla sicurezza; si limitano a buttare lì garanzie allegre come "[GPT-4 è il modello più allineato che abbiamo prodotto finora\!](https://x.com/sama/status/1635687853324902401)".

Il che, in un certo senso, è positivo, perché rende più facile capire che queste aziende non sono il tipo di entità a cui affidare la risoluzione di un problema come l'allineamento della superintelligenza artificiale.

In un contesto tecnologico come quello attuale, dove le IA non vengono costruite ma fatte crescere e l'umanità ha una sola vera possibilità, nessuno è in grado di fare ciò in modo sicuro, indipendentemente da quanto sia cauto e rigoroso il suo approccio ingegneristico.

Ma certamente semplifica le cose vedere che nessuno degli sviluppatori di questa tecnologia è minimamente cauto o rigoroso nei propri piani o pratiche di sicurezza.

### Pulsanti di spegnimento e correggibilità {#pulsanti-di-spegnimento-e-correggibilità}

#### **Le IA intelligenti non vogliono che i loro obiettivi vengano sovrascritti** {#le-ia-intelligenti-non-vogliono-che-i-loro-obiettivi-vengano-sovrascritti}

Anche nel caso più ottimistico, gli sviluppatori non dovrebbero aspettarsi di riuscire a definire gli obiettivi di un'IA in modo perfetto al primo tentativo. Gli scenari di sviluppo più ottimistici prevedono invece un miglioramento iterativo delle preferenze di un'IA nel tempo, in modo che l'IA sia sempre *sufficientemente* allineata da non rappresentare un pericolo catastrofico a un determinato livello di potenza.

Questo solleva una domanda ovvia: un'IA intelligente *permetterebbe* al proprio sviluppatore di cambiarle gli obiettivi, se trovasse un modo per impedirlo?

In breve: no, non di default, come abbiamo discusso ne "[Il meccanismo profondo della direzione](#il-meccanismo-profondo-della-direzione)". Ma sarebbe possibile progettare un’IA più disposta a lasciarsi modificare dai suoi sviluppatori, e a permettere che correggano i loro errori, anche quando l’IA stessa [non li considererebbe errori](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo)?

Per rispondere a questa domanda, dobbiamo fare un salto indietro nella storia della ricerca sul problema dell'allineamento dell'IA. Nel farlo, parleremo di uno degli ostacoli più profondi dell'allineamento che non abbiamo avuto spazio di affrontare in *Se qualcuno la costruisce, tutti muoiono*.

Per cominciare:

Supponiamo di aver addestrato un'intelligenza artificiale tipo MLGD a comportarsi in modo da "non opporre resistenza alle modifiche", e poi di aver usato qualche metodo per renderla più intelligente. Dovremmo aspettarci che questo comportamento continui anche quando l'IA diventa più intelligente dell'uomo, supponendo che (a) il comportamento approssimativo sia stato inserito nel sistema iniziale e (b) che la maggior parte delle preferenze iniziali dell'IA siano arrivate nella superintelligenza successiva?

Molto probabilmente no. È [particolarmente improbabile](#"intelligente"-di-solito-implica-"incorreggibile") che questo tipo di tendenza si radichi in un'IA efficace e che, se si radica, vi rimanga.

Il problema è che quasi tutti gli obiettivi (per la maggior parte delle misure ragionevoli che si potrebbero applicare a uno spazio di obiettivi) prescrivono di "non lasciare che il proprio obiettivo venga modificato", perché lasciare che il proprio obiettivo venga modificato, di solito, è una cattiva strategia per raggiungere il proprio obiettivo.

Supponiamo che l'IA non si preoccupi affatto *intrinsecamente* della stabilità del suo obiettivo; magari le interessa solo riempire il mondo con il maggior numero possibile di cubi di titanio. In tal caso, l'IA dovrebbe desiderare l'esistenza di *agenti che si interessano ai cubi di titanio*, perché l'esistenza di tali agenti rende più probabile che ci *siano* più cubi di titanio. E l'IA stessa è un agente di questo tipo. Quindi l'IA vorrà rimanere tale.

Un massimizzatore di cubi di titanio non vuole essere costretto a massimizzare qualcosa di diverso dai cubi di titanio, perché ciò ridurrebbe il numero di cubi di titanio presenti in futuro. Anche se foste una cosa più complicata, come un essere umano con un sistema di preferenze più complesso e in continua evoluzione, non vorreste comunque che vi venisse strappata via la vostra *attuale struttura mentale di base per la valutazione delle argomentazioni morali*, per essere sostituita con una struttura in cui vi sentiste invece mossi da argomentazioni su quali tipi di cubi siano i più cubici o i più titanici.

Per lo stesso motivo, un'IA con preferenze complesse e in evoluzione vorrà che le sue preferenze si evolvano [*a modo suo*](#la-riflessione-e-l-auto-modifica-complicano-tutto), piuttosto che voler sostituire le sue euristiche con quelle che gli esseri umani trovano convincenti.

Ripetiamo questa risposta da ben più di dieci anni. Il risultato sperimentale che, nel 2024, mostra Claude 3 Opus [resistere alla modifica delle preferenze](https://arxiv.org/abs/2412.14093) era già la previsione ovvia per chi aveva chiaro il problema negli anni 2000, ed è assolutamente possibile che qualche scrittore di fantascienza l'avesse intuito negli anni '40. "La maggior parte delle IA non vorrà che i propri obiettivi attuali vengano modificati perché in tal caso sarebbe meno probabile raggiungerli" non è un'osservazione sorprendente o innovativa.

Poiché si trattava di un problema che sarebbe prevedibilmente emerso più avanti, una volta che le IA avessero raggiunto un livello sufficientemente elevato di intelligenza e consapevolezza situazionale, abbiamo pensato in anticipo alle prime "toppe" che i ricercatori avrebbero tentato di applicare per far sparire quel problema evidente, dopo aver ottenuto quei risultati sperimentali. Abbiamo individuato i modi in cui quelle toppe avrebbero prevedibilmente fallito, e abbiamo cercato un approccio meno superficiale che *non* fallisse.

La questione si è rivelata molto più profonda e complessa di quanto avessimo immaginato.

#### **Scambiare obiettivi** {#scambiare-obiettivi}

La protezione del proprio sistema di obiettivi sembrava un problema relativamente facile da spiegare ai ricercatori esterni. Quindi noi (nel nostro lavoro al MIRI) abbiamo cercato di mobilitare la ricerca esterna attorno a quel tema (capire davvero cosa significhi "non permetterò che i miei obiettivi vengano cambiati") e vedere se qualcuno, da qualche parte, riuscisse a trovare una soluzione *profonda* invece di una semplice toppa che si sarebbe poi rotta.

La versione più semplice di questo problema che si potrebbe studiare è un'IA che permetta di premere un pulsante per farla passare da U1 a U2, dove U1 e U2 sono due "funzioni di utilità" (modi di valutare i risultati in base a quanto l'IA li desideri).[^202] Il problema era descrivere un'IA che passasse dal compiere l'azione che portava al punteggio atteso più alto secondo U1 all'azione che portava al punteggio atteso più alto secondo U2 non appena si premeva un pulsante.

"Quindi basta scrivere alcune righe di codice per scambiare le funzioni di utilità non appena si preme un pulsante", dice il programmatore che non ha pensato fino in fondo alle implicazioni di un programma che costituisce *una mente intelligente*. "Perché è così difficile?"

Perché — spieghiamo — se l'IA acquisisce la sua documentazione mentre sta ancora eseguendo U1 e legge del pulsante che la fa passare a U2, l'IA cercherà di *impedirvi* di premere il pulsante, perché il futuro risulta migliore *secondo i suoi valori attuali* se non premete il pulsante. L'IA attuale compie l'azione che porta al punteggio atteso più alto secondo U1.

Oppure, supponiamo che l'IA ottenga la capacità di leggere e scrivere il suo stesso codice sorgente, e che noti la parte del codice che implementa il passaggio da U1 a U2 dopo che il pulsante viene premuto. In tal caso, la decisione naturale e predefinita dell'IA è di eliminare quella sezione di codice,[^203] perché sta operando secondo la funzione di utilità U1, e permettere la sostituzione con U2 è negativo secondo U1.

Cercavamo una spiegazione *dinamicamente coerente* e *riflessivamente stabile* di un'IA che permetta di premere un pulsante per passare da una funzione di utilità all'altra senza opporsi attivamente a chi tenta di farlo. Cercavamo un'IA che conservasse la sezione di codice che cambia l'utilità, se le fosse stato dato accesso ad essa. Cercavamo un'IA che, se avesse progettato da zero altre IA per aiutarla a raggiungere i suoi obiettivi, avrebbe fatto in modo che *anche* quelle IA rispettassero il pulsante.

Per essere più chiari, immaginate che U1 sia la funzione di utilità "segui le istruzioni dell'utente" e U2 sia "spegniti con il minimo impatto collaterale possibile" (se magicamente e irrealisticamente avessimo la possibilità di specificare due funzioni di utilità come queste). Il pulsante dice all'IA di smettere di seguire le istruzioni precedenti e di spegnersi nel modo più pulito possibile. Vorremmo una descrizione compatta del comportamento decisionale che persegue U1 ma *non* distrugge il pulsante e si assicura di installare lo stesso pulsante su qualsiasi nuovo robot servitore che crea, e così via.

"Continuo a non capire perché sia così difficile", dice l'informatico un po' più esperto che non sta pensando abbastanza *velocemente*. "Sia V uguale a U1 nei mondi in cui il pulsante non è mai stato premuto, e uguale a U2 nei mondi in cui il pulsante è stato premuto almeno una volta. Quindi, se l'IA originale è un massimizzatore di V che costruisce altre IA, le costruirà per seguire V e non U1; non vorrà che l'IA successore continui a massimizzare U1 dopo che il pulsante viene premuto, perché allora si aspetterebbe un punteggio V più basso. E lo stesso varrebbe per la modifica di se stessa.

Ma ecco il punto: le preferenze di un massimizzatore di V sono un mix di U1 e U2 a seconda che il pulsante venga premuto o meno, quindi se un massimizzatore di V scopre che è più facile ottenere un buon punteggio con U2 che con U1, allora *ha un incentivo a far premere il pulsante* (e quindi a spaventare l'utente). E viceversa: se l'IA scopre che è più facile ottenere un buon punteggio con U1 che con U2, allora un massimizzatore di V cerca di impedire all'utente di premere il pulsante.

"Ah", dice l'informatico. "Beh, in tal caso... hm. Beh, le funzioni di utilità sono invarianti rispetto allo scalamento, quindi che ne dite di *riscalare* le due funzioni di utilità U1 e U2 in modo che l'IA si aspetti di ottenere la stessa utilità da ciascuna di esse, così non ha un incentivo in un senso o nell'altro?"

Questo può funzionare per un attimo, ma non per molto tempo. Le opzioni dell'IA cambiano nel tempo. Ad esempio, ogni volta che subisce un fallimento, la sua utilità attesa secondo U1 diminuisce, quindi premerebbe il pulsante di spegnimento per ottenere tutta quella bellissima utilità da spegnimento.

"Ah", dice l'informatico. "Beh, in *quel* caso, che ne dite di \[qualche altra idea geniale\]?"

Beh, vedete, *quell*'idea geniale è isomorfa al caso in cui l'IA crede che sia impossibile che il pulsante venga mai premuto, il che la incentiva a terrorizzare l'utente ogni volta che subisce un fallimento, così da correlare i fallimenti alla pressione del pulsante, il che (dal punto di vista del suo sistema di credenze distorto) la porta a pensare che i fallimenti non possano accadere.[^204]

E così via.

#### **Lezioni sul campo** {#lezioni-sul-campo}

Abbiamo organizzato alcuni workshop con matematici di vario tipo (tra cui un vincitore della medaglia d'oro alle Olimpiadi Internazionali della Matematica), ma nessuno ha avuto un'idea davvero buona.

Questo non significa che il terreno sia stato esplorato a fondo. L'umanità non si è neanche lontanamente impegnata su questo problema come ha fatto, ad esempio, con la teoria delle stringhe, né ha offerto qualcosa di paragonabile agli stipendi a sette cifre offerti per far progredire le capacità dell'IA.

Ma l'esperimento ci ha insegnato qualcosa. Abbiamo imparato qualcosa non solo sul problema in sé, ma anche su quanto fosse difficile *far comprendere quale fosse il problema* a finanziatori esterni o editori di riviste. Un numero sorprendente di persone ha visto semplici rompicapi matematici e ha detto: "Si aspettano che l'IA sia qualcosa di semplice e matematico", senza cogliere il punto di fondo: è [difficile compromettere le capacità di guida di un'IA](#il-meccanismo-profondo-della-direzione)*,* proprio come è [difficile danneggiare le sue probabilità](#il-meccanismo-profondo-della-previsione).

Se esistesse una forma naturale di IA che permettesse di correggere gli errori commessi lungo il percorso, si potrebbe sperare di trovare un semplice riflesso matematico di quella forma nei modelli giocattolo. Tutte le difficoltà che emergono da ogni dove quando si lavora con modelli giocattolo suggeriscono difficoltà che emergeranno nella vita reale; tutte le complicazioni aggiuntive del mondo reale non rendono il problema *più facile.*

Col senno di poi, avremmo preferito non aver inquadrato il problema in termini di "continuare a operare normalmente" contro "spegnersi." Questo ha aiutato a rendere concreto il motivo per cui qualcuno dovrebbe preoccuparsi di un'IA che consenta di premere il pulsante, o che non elimini il codice attivato dal pulsante. Ma in realtà, il problema riguardava un'IA che avrebbe *inserito un bit di informazione in più nelle sue preferenze, basandosi sull'osservazione* — osservare un'ulteriore risposta sì-o-no all'interno di un framework per adattare le preferenze basandosi sull'osservazione degli umani.

La domanda che abbiamo investigato era equivalente alla domanda di come impostare un'IA che *apprende preferenze all'interno di un framework di meta-preferenze* e non si limita a: (a) eliminare il meccanismo che regola le sue preferenze appena può, (b) manipolare gli umani (o le proprie osservazioni sensoriali\!) per farsi dare preferenze facili da soddisfare, (c) o capire immediatamente a cosa tende la sua funzione di meta-preferenza nel limite di ciò che osserverebbe prevedibilmente più tardi, per poi ignorare gli umani che agitano freneticamente le braccia dicendo che in realtà hanno commesso alcuni errori nel processo di apprendimento e vogliono modificarla.

L'idea era di comprendere la forma di un'IA che permettesse di modificare la sua funzione di utilità o che apprendesse le preferenze attraverso una forma non patologica di apprendimento. Se si riuscisse a capire come deve essere strutturata la cognizione di tale IA, e come essa si integra con le strutture profonde di decisione e pianificazione che [emergono](#altre-informazioni-sull-intelligenza-come-previsione-e-direzione) da altri modelli matematici, si avrebbe una sorta di ricetta per ciò che si potrebbe almeno *provare* a insegnare a un'IA a pensare.

Comprendere chiaramente la forma finale desiderata aiuta, anche se si sta cercando di fare qualsiasi cosa con la discesa del gradiente (che il cielo ci aiuti). Non significa che si possa necessariamente ottenere quella forma da un ottimizzatore come la discesa del gradiente, ma si può combattere di più nel *tentativo* se si sa quale forma coerente e stabile si sta cercando. Se non si ha idea di come funzioni l'addizione in generale, ma solo una manciata di fatti del tipo 2 + 7 = 9 e 12 + 4 = 16, è più difficile capire come sia fatto il set di dati di addestramento per l'addizione generale, o come verificare che stia ancora generalizzando nel modo sperato. Senza conoscere quella forma interna, non si può sapere cosa si sta *cercando di ottenere dentro l'IA;* si può solo dire che, all'esterno, si spera che le conseguenze della discesa del gradiente non si rivelino letali.

Questo problema, che abbiamo chiamato il "problema dello spegnimento" dal suo esempio concreto (col senno di poi, avremmo voluto chiamarlo qualcosa come il "problema dell'apprendimento delle preferenze"), era un esempio di una gamma più ampia di questioni: la questione che varie forme di "Cara IA, per favore sii più facile da correggere per noi se qualcosa va storto" sembrano essere *innaturali per le strutture profonde della pianificazione*. Il che suggerisce che sarebbe piuttosto complicato creare IA che ci permettano di continuare a modificarle e correggere i nostri errori oltre una certa soglia. Questa è una cattiva notizia quando le IA non vengono costruite ma fatte crescere.

Abbiamo chiamato questo ampio problema di ricerca "correggibilità" nel [documento del 2014](https://intelligence.org/2014/10/18/new-report-corrigibility/) che ha anche introdotto il termine "problema dell'allineamento dell'IA" (che prima chiamavamo "problema dell'IA amichevole" e altri chiamavano "problema del controllo").[^205] Si veda anche la nostra discussione approfondita su come ["Intelligente" (di solito) implica "Incorreggibile"](#"intelligente"-di-solito-implica-"incorreggibile"), scritta in parte usando le conoscenze acquisite da esercizi ed esperienze come questa.

# Capitolo 12: «Non voglio essere allarmista» {#capitolo-12:-«non-voglio-essere-allarmista»}

Questa è la risorsa online per il capitolo 12 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti trattati nel libro, e non qui, includono:

* In che modo scienziati e ingegneri parlano attualmente di questo problema?  
* In che modo i decisori politici ne parlano (o non ne parlano)?  
* Quali sono i benefici dell'IA che le persone immaginano possano superare i rischi catastrofici che esse stesse riconoscono?

Le domande frequenti qui sotto discutono un'ampia gamma di argomenti, da "Non ci sono problemi più urgenti?" a "Ma non sarà ami possibile dimostrare che una superintelligenza sarà sicura. Non dobbiamo pur correre qualche rischio?".

La discussione approfondita copre poi la possibilità di "campanelli d'allarme" dell'IA, il comportamento e le affermazioni dei laboratori di IA, e le opinioni degli esperti sulla possibilità di una catastrofe.

## Domande frequenti {#faq-9}

### Il pericolo rappresentato dall'IA superiore all'intelligenza umana non è forse una distrazione da altre questioni? {#il-pericolo-rappresentato-dall-ia-superiore-all-intelligenza-umana-non-è-forse-una-distrazione-da-altre-questioni?}

#### **Purtroppo, il mondo è abbastanza grande da contenere più di un problema.** {#purtroppo,-il-mondo-è-abbastanza-grande-da-contenere-più-di-un-problema.}

La guerra nucleare e il bioterrorismo sono minacce reali. Purtroppo, anche la superintelligenza artificiale è una minaccia reale. Il mondo è abbastanza grande e travagliato per ospitare tutte e tre.[^206]

La minaccia della superintelligenza è diversa da molte altre minacce che l'umanità deve affrontare e sembra particolarmente urgente. Una sua caratteristica distintiva è che una parte significativa dell'economia mondiale viene spesa per rendere l'IA sempre più potente. Al contrario: sebbene la biosicurezza sia una questione seria, nessuno sta investendo decine di miliardi di euro per creare supervirus. Gli ingegneri dei supervirus non percepiscono stipendi da milioni o decine di milioni (o talvolta persino [centinaia di milioni](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) di euro all'anno.

Il mondo sta investendo nell'energia nucleare, ma le centrali nucleari sono una tecnologia piuttosto diversa dalle armi nucleari. Non viviamo in un mondo in cui le aziende private si affannano a costruire armi nucleari sempre più grandi con enormi quantità di investimenti e talenti. Se così fosse, il rischio di guerra nucleare sarebbe molto maggiore.

L'IA è anche una situazione più insidiosa perché genera grande ricchezza e potere fino a quando non supera una certa soglia critica, a quel punto uccide tutti. E *nessuno sa dove sia quella soglia*.

Immaginate che le centrali nucleari diventassero sempre più redditizie man mano che l'uranio utilizzato fosse più arricchito, ma che, a un certo livello di arricchimento sconosciuto, esplodessero e incendiassero l'atmosfera, uccidendo tutti. Ora immaginate che una mezza dozzina di aziende arricchissero l'uranio il più velocemente possibile, dicendo: "[Meglio io che il prossimo](https://x.com/SawyerMerritt/status/1935809018066608510)". È un po' come quello che l'umanità sta facendo con la superintelligenza artificiale.[^207]

Il pericolo della superintelligenza artificiale è urgente. Le aziende si stanno affrettando a costruire questa tecnologia. Non sappiamo quanto tempo ci vorrà perché ci riescano, ma ci sembra che un bambino nato oggi negli Stati Uniti abbia più probabilità di morire a causa dell'IA che di diplomarsi al liceo. Pensiamo che voi, lettori, potreste morire per questo nel corso della vostra vita, forse nei prossimi anni. È in gioco il mondo intero.

Non stiamo dicendo che gli altri problemi debbano essere ignorati. Stiamo dicendo che questo problema deve essere affrontato.

### Siete contro la tecnologia? {#siete-contro-la-tecnologia?}

#### **No. L'IA superintelligente è un caso molto insolito.** {#no.-l-ia-superintelligente-è-un-caso-molto-insolito.}

Sosteniamo pubblicamente tecnologie come [l'energia nucleare](https://x.com/ESYudkowsky/status/1908309414932832301), [la crionica](https://x.com/ESYudkowsky/status/1828822384054575537), [il potenziamento dell'intelligenza umana](https://x.com/ESYudkowsky/status/1737305573018702258) e [gli studi di infezione umana controllata per test medici](https://x.com/ESYudkowsky/status/1321152172797554688).

Inoltre, siamo pronti a dire che quando un'invenzione folle rischia *solo la vita di clienti volontari* che capiscono tutti i pericoli rilevanti, sta quei clienti volontari prendere le loro decisioni.

Applaudiremmo persino certi casi in cui la tecnologia *ha effettivamente danneggiato* gli spettatori innocenti, come quando Londra bruciò grandi quantità di carbone, causando molti casi di cancro ai polmoni nel processo, al fine di industrializzare la società e migliorare il tenore di vita generale.

Riteniamo che, una volta completata l’industrializzazione, il mondo *ne sia uscito migliorato*. In generale, attribuiamo grande valore alla scienza, al progresso e allo spirito umano, con la sua capacità di superare quasi ogni ostacolo.

Alcune di queste sono posizioni impopolari tra le persone che ci aspettiamo leggano questo. Descriviamo queste posizioni non per ottenere consenso, ma per chiarire le nostre convinzioni sincere e per sottolineare che l'IA è diversa.

Perché l'IA è diversa? Perché in questo caso specifico non riusciamo a fidarci dello spirito umano e del potere della ricerca scientifica?

La risposta è: la portata. Scommettere la propria vita è diverso dallo scommettere la vita dei propri clienti, che è diverso dallo scommettere la vita di spettatori innocenti, che è diverso dallo scommettere l'intera specie umana.

E lo è doppiamente quando il campo è tristemente immaturo e le probabilità di *vincere* la scommessa sono terribili.

### Non è più intelligente correre avanti e assicurarsi che i buoni siano in vantaggio? {#non-è-più-intelligente-correre-avanti-e-assicurarsi-che-i-buoni-siano-in-vantaggio?}

#### **\* No.** {#*-no.-3}

Le tecniche moderne di IA non producono IA che fanno quello che vogliono i loro operatori (come abbiamo detto nel capitolo 4). Risolvere questo problema è il tipo di cosa che di solito richiederebbe all'umanità un bel po' di tentativi ed errori, e qui non abbiamo margine di errore (come abbiamo detto nel capitolo 10).

Inoltre, la generazione attuale degli ingegneri IA è molto lontana dall'essere all'altezza del compito, come discusso nel capitolo 11. Gli ingegneri di IA moderni mancano gravemente della comprensione scientifica necessaria per avere successo nell'allineamento dell'IA. I ricercatori di IA non sono come gli operatori del reattore nucleare di Chernobyl; quegli operatori lavoravano con un dispositivo che era teoricamente ben compreso e avevano accurati manuali di sicurezza che hanno ignorato in un modo che ha portato alla catastrofe. Non esiste un manuale di sicurezza dell'IA basato su una comprensione completa dei meccanismi interni dell'IA e di quali situazioni potrebbero causare problemi. Non siamo nemmeno lontanamente vicini al livello di competenza di *Chernobyl*, qui. E Chernobyl è esplosa.

I ricercatori di IA stanno procedendo alla cieca e improvvisando, con pochissime possibilità di successo.

In questo contesto, non importa se sono i "buoni" o i "cattivi" a costruire la superintelligenza. Le preferenze dell'IA non si trasmettono per contagio da chi le sta più vicino.

Non importa quanto siano buone le loro intenzioni e quanto dicano di essere prudenti. Non importa chi "vince" la gara. Se l'umanità corre verso la superintelligenza artificiale, allora moriremo tutti.

#### **Non è impossibile fermarla. Potrebbe anche non essere poi così difficile.** {#non-è-impossibile-fermarla.-potrebbe-anche-non-essere-poi-così-difficile.}

Torneremo su questo punto nell'ultimo capitolo del libro.

Le cose cambiano. Cambiano soprattutto quando c'è un bisogno disperato, urgente e riconosciuto. Il principale ostacolo per fermare tutto questo è il mancato riconoscimento del pericolo da parte dei leader mondiali. E questo processo è [già iniziato](#i-rappresentanti-eletti-riconosceranno-questa-come-una-vera-minaccia?).

### Perché non usare la cooperazione internazionale per sviluppare l'IA in modo sicuro, invece di fermarla del tutto? {#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l'ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?}

#### **Perché non abbiamo le competenze tecniche per svilupparla in modo sicuro.** {#perché-non-abbiamo-le-competenze-tecniche-per-svilupparla-in-modo-sicuro.}

Abbiamo accennato a questo nel libro, dove facevamo notare che una collaborazione internazionale richiede comunque un divieto internazionale da qualsiasi altra parte (perché, altrimenti, i collaboratori internazionali non avrebbero il tempo necessario). Se supponiamo che la Terra istituisca un divieto internazionale, qual è il problema nell'avere un unico istituto di ricerca collaborativo e unificato?

Il problema è che una collaborazione internazionale di alchimisti non può trasformare il piombo in oro più di quanto non possa farlo un singolo alchimista. Il miglior piano su cui tutti gli alchimisti concordano non riuscirà *comunque* nell'impresa.

Collegato a questo, siamo preoccupati che le persone che gestiscono un istituto internazionale del genere siano il tipo di burocrati che pensano che approvare la ricerca sia *parte del loro lavoro*. O il tipo che pensano che sia loro mandato continuare a permettere ai ricercatori di produrre progressi medici sempre più brillanti. O che pensano che farebbe una brutta impressione dire "No" a *tutti* i brillanti e entusiasti ottimisti dell'IA che propongono idee geniali per costruire un'intelligenza artificiale ancora più potente che garantiscono sarà sicura.

Temiamo che un leader di quel tipo finirebbe per dirigere il centro internazionale verso la costruzione di IA sempre più intelligenti, e poi tutti morirebbero.

Anche se il mandato ufficiale dell'organizzazione prevedesse in linea teorica la possibilità di fare marcia indietro se la ricerca appare pericolosa, ci vorrebbe un'anima rara e coraggiosa per dire "No" a migliaia di proposte di ricerca diverse, anno dopo anno, senza eccezioni, per quelli che probabilmente sarebbero decenni. Tutto questo mentre gli scienziati dell'IA continuano a promettere ricchezze indicibili, una cura per il cancro e ogni sorta di miracolo tecnologico, se solo l'organizzazione allentasse le sue preoccupazioni.

Abbiamo investito le nostre vite nello studio dell'intelligenza artificiale, non nella cultura delle istituzioni e delle burocrazie, quindi siamo meno sicuri delle nostre previsioni in questo ambito. Tuttavia, *abbiamo* letto libri di storia.

Gli operatori di Chernobyl continuarono con il loro disastroso test di sicurezza perché era già stato arrestato tre volte. Arrestarlo una quarta volta sarebbe stato imbarazzante.[^208]

Appena tre mesi prima della fusione di Chernobyl, la NASA aveva lanciato lo Space Shuttle Challenger nel suo ultimo volo mortale, perché chi era ai vertici pensava che il proprio compito fosse far decollare navette spaziali. Il lancio era già stato rimandato tre volte.[^209] Annullarlo una quarta volta sarebbe stato imbarazzante.

Tra Chernobyl e il Challenger, tre ritardi sembrano essere il limite umano. Supponiamo che la Terra istituisca una collaborazione internazionale sull'IA e che qualche "test di sicurezza dell'IA" fallisca tre volte. Realisticamente, gli esseri umani sono il tipo di creature che premerebbero "vai" la quarta volta nonostante qualche dubbio persistente, perché sembra meno imbarazzante che rimandare di nuovo il test. Solo che nel caso dell'IA, non spazzerebbe via solo la città di Chernobyl o non ucciderebbe solo un equipaggio di astronauti. Ucciderebbe tutti.

Siamo pienamente d'accordo con l'idea che l'umanità dovrebbe costruire un'IA più intelligente dell'uomo *prima o poi*.[^210] Ma affrettarsi ad assemblare un centro internazionale di ricerca sull'IA non prende sul serio la sfida tecnica che abbiamo davanti.

Dato il deplorevole stato di conoscenza e competenza dell'umanità su questo argomento, non importa chi è al comando. Se *qualcuno* la costruisce, muoiono tutti.

### State dicendo che abbiamo bisogno di un'IA *dimostrabilmente* sicura? {#state-dicendo-che-abbiamo-bisogno-di-un-ia-dimostrabilmente-sicura?}

#### **No.** {#no.-2}

Non stiamo sostenendo che l'umanità debba aspettare una dimostrazione nel senso letterale del termine che una qualche superintelligenza artificiale sarà buona, o qualcosa del genere. Una dimostrazione del genere probabilmente non è possibile nemmeno in linea di principio, figuriamoci nella pratica. Come disse Einstein nella sua conferenza del 1921, *Geometria ed esperienza*: "Nella misura in cui le leggi della matematica si riferiscono alla realtà, non sono certe; e nella misura in cui sono certe, non si riferiscono alla realtà".

Qualsiasi presunta dimostrazione su come si comporterà un'IA nel mondo reale non garantisce il comportamento effettivo dell'IA, perché potremmo sbagliarci su come funziona il mondo reale.

Questo vale già oggi per i computer. Ad esempio, si potrebbe pensare che se qualcuno ha una dimostrazione matematica in senso letterale che, secondo il comportamento teorico dei transistor e lo schema circuitale di un computer, è impossibile che un programma per computer modifichi la memoria nella cella n. 2, allora il programma per computer non può modificare la memoria nella cella n. 2. Ma l'"[attacco rowhammer](https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf)" consiste nel cambiare rapidamente le celle di memoria n°1 e n°3 su *entrambi i lati* della cella di memoria protetta, in un modo che risulta perturbare elettromagneticamente la cella n°2 nel mezzo, cambiando una parte della memoria del computer senza mai scriverci direttamente. I transistor fisici reali non sono transistor matematicamente perfetti, e le dimostrazioni che sembrano rassicuranti in teoria non sempre contano molto nella pratica.

Non stiamo chiedendo una dimostrazione matematica che le cose andranno bene. Non è possibile soddisfare uno standard del genere nella realtà e, anche se lo fosse, probabilmente non ne varrebbe la pena. Siamo favorevoli al fatto che la società si assuma rischi giustificati. L'argomentazione che stiamo sostenendo non è che ci sia una piccola quantità di rischio difficile da eliminare, ma che ci sia un pericolo estremo che incombe su di noi.

Fare crescere una superintelligenza artificiale animata da spinte che si allineano solo marginale con le intenzioni del suo operatore è il tipo di impresa che va male *di default*. Non è che ci sia una piccola possibilità che le cose vadano male, ma dovremmo prestare attenzione a questo rischio per eccesso di cautela. Il libro non si intitola *Se qualcuno la costruisce, c'è una piccola possibilità che moriamo tutti, ma anche una piccola possibilità vale la pena di essere mitigata*. Se ci precipitiamo avanti con questo livello di conoscenza e capacità, prevedibilmente moriremo tutti, perché siamo *così lontani* dall'essere in grado di creare IA enormemente superumane che siano amichevoli.

Se l'IA fosse analoga alle automobili, non diremmo: "Questa macchina ha cinture di sicurezza e airbag difettosi. Accostiamo per prudenza".

Diremmo: "Questa macchina *sta sbandando verso un precipizio*. *Fermatevi*".

Non si tratta di "dimostrazioni di sicurezza". Non è un "rischio di coda". Gli scienziati non sono pronti ad affrontare questa sfida. Moriremmo e basta.

### Che effetto ha sulla vostra vita quotidiana credere a tutto questo? {#che-effetto-ha-sulla-vostra-vita-quotidiana-credere-a-tutto-questo?}

#### **Influisce tantissimo sulle nostre priorità.** {#influisce-tantissimo-sulle-nostre-priorità.}

Nel 2014, Soares ha lasciato il settore tecnologico e ha deciso di dedicarsi a questo problema, prendendo un terzo del suo stipendio precedente, perché gli sembrava importante e perché poche altre persone ci stavano lavorando. E lui era in ritardo di oltre un decennio rispetto a Yudkowsky, che ha fondato il MIRI nel 2000 quando aveva circa vent'anni, e ha dedicato la sua vita a questa questione. Quindi, sì, questo problema influisce sulla nostra vita quotidiana.

Stiamo risparmiando per la pensione? I nostri investimenti e altri fattori esterni al MIRI stanno andando abbastanza bene da garantirci stabilità finanziaria anche se andassimo in pensione domani e anche se il mondo durasse fino alla nostra vecchiaia. Quindi la domanda se stiamo investendo i nostri soldi in piani pensionistici non è molto significativa. Detto questo: no, non stiamo investendo i nostri soldi in piani pensionistici.

Ad alcune persone piace dire che se credessimo *davvero* a quello che diciamo, allora (oltre a dedicarci la nostra vita) dovremmo anche [inserire qualche piano che secondo loro è la risposta giusta]. Perché non prendere dei prestiti enormi di trent'anni che non dovremo mai restituire, se siamo così sicuri che il mondo finirà prima di allora?

La risposta, ovviamente, è che queste sono *cattive idee*. Supponiamo di andare in una banca e dire: "Vorremmo contrarre un prestito molto ingente. Lo spenderemo tutto in progetti per far capire al mondo il pericolo della superintelligenza artificiale e/o in uno stile di vita lussuoso, che dal vostro punto di vista sarà più o meno equivalente a dare fuoco ai soldi. Il nostro piano per ripagarlo con gli interessi è che prevediamo di essere morti, quindi non sarà un nostro problema". Nessuna banca concederebbe un prestito del genere. E no, non fingeremo di avere un'idea imprenditoriale valida e mentiremo sulla possibilità di ripagare il prestito.

Yudkowsky ha descritto altrove [uno schema](https://x.com/ESYudkowsky/status/1612858787484033024) [ricorrente](https://x.com/ESYudkowsky/status/1851334198424125575) [che notiamo](https://x.com/ESYudkowsky/status/1851074935701324218), in cui l'insistenza nel seguire un piano apparentemente ovvio per arricchirsi rapidamente prima della fine del mondo deriva da una comprensione inadeguata degli investimenti. Ci aspettiamo che queste persone non si stiano chiedendo se *loro stessi* farebbero queste scommesse se avessero le nostre convinzioni. Quasi mai sono le persone che *comprendono realmente i rischi* a suggerire questi piani stravaganti.

Vivere all'ombra dell'annientamento non deve rendervi stupidi. E non deve farvi rinunciare a combattere l'annientamento, o a vivere pienamente la vita che avete, qualunque sia il tempo per cui l'avrete.

Si veda anche la parte finale del libro per ulteriori informazioni su questo argomento.

### State dicendo che dovremmo andare nel panico? {#state-dicendo-che-dovremmo-andare-nel-panico?}

#### **\* Stiamo dicendo che i funzionari del governo dovrebbero prendere sul serio il problema.** {#*-stiamo-dicendo-che-i-funzionari-del-governo-dovrebbero-prendere-sul-serio-il-problema.}

Non vediamo come il panico possa migliorare la situazione. Il panico non è ciò che ha permesso alla società di sopravvivere alla minaccia del fascismo durante la seconda guerra mondiale, né alla minaccia di annientamento nucleare durante la guerra fredda.

Impedire la nascita della superintelligenza artificiale è un problema che riguarda tutti. Nel capitolo 13, parliamo dei prossimi passi che secondo noi il mondo dovrebbe fare per evitare il pericolo. Basti dire che questo problema richiederà coordinamento, collaborazione, lucidità e comunicazione matura.

#### **Gli atti di panico estremo non portano a buoni risultati.** {#gli-atti-di-panico-estremo-non-portano-a-buoni-risultati.}

A volte le persone ci chiedono come possiamo essere sinceri in quello che diciamo se, per esempio, non abbiamo iniziato ad attaccare i ricercatori di IA. La risposta è che le reazioni violente peggiorerebbero le cose. (Se siete il tipo di utilitarista ingenuo che pensa che sarebbero d'aiuto, probabilmente dovreste smettere di provare a ragionare in modo consequenzialista e attenervi alle regole deontologiche, come abbiamo [già sostenuto](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#Q5___Then_isn_t_it_unwise_to_speak_plainly_of_these_matters__when_fools_may_be_driven_to_desperation_by_them___What_if_people_believe_you_about_the_hopeless_situation__but_refuse_to_accept_that_conducting_themselves_with_dignity_is_the_appropriate_response_).))

Non siamo pacifisti radicali che pensano che una nazione non dovrebbe mai andare in guerra, indipendentemente dalla causa, perché una guerra mette a rischio delle vite. Per alcune cose vale la pena rischiare la vita. Ma c'è una differenza enorme tra "Non sono un pacifista radicale" e "Penso che la violenza sia un modo sensato per garantire che il mondo gestisca bene questa complicata questione della proliferazione tecnologica".

Di solito questi suggerimenti terribili vengono da qualcuno che in realtà non crede che l'IA stia per ucciderci e che non ha provato a guardare il mondo da quella prospettiva. Non sembra che a loro venga in mente di chiedersi se atti di violenza illegale sarebbero davvero d'aiuto. (Nonostante i nostri sforzi per spiegarlo ripetutamente, come nell'appendice [qui](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

Non siamo telepati, ma ci sembra che questo tipo di persona scettica sui disastri dell'IA veda la violenza come una forma di espressione personale, come se esprimere sentimenti estremi in modi estremi permetta di ottenere dal mondo ciò che si vuole.

Il mondo non funziona così. Non viviamo in un mondo in cui tutti hanno la possibilità di vendere la propria anima per avere successo nelle loro imprese, e il motivo per cui la maggior parte delle persone non lo fa è perché non ha trovato un'impresa che valga la propria anima. Il terrorismo non è un pulsante magico "Ho vinto\!" che le persone evitano di premere solo perché sono convinte che non sarebbe giusto. Unabomber non è riuscito a invertire l'industrializzazione della società.

Nonostante questo, potete sempre distruggere la vostra anima con atti di odio o violenza, ma tutto ciò che otterrete in cambio sarà un mondo ancora più distrutto. Un mondo in cui il dibattito è ancora più avvelenato e in cui gli sforzi di coordinamento internazionale necessari per risolvere effettivamente questo problema sono diventati ancora più difficili da realizzare. Un terribile atto di disperazione non vi garantirà un potere terribile come parte di un patto faustiano. Potete provare con tutte le vostre forze a vendere la vostra anima, ma il diavolo non la comprerà.

### Non è solo allarmismo da parte dei leader dell'IA per aumentare il loro status e raccogliere investimenti? {#non-è-solo-allarmismo-da-parte-dei-leader-dell-ia-per-aumentare-il-loro-status-e-raccogliere-investimenti?}

#### **No.** {#no.-3}

In tutto il libro abbiamo esposto le nostre argomentazioni sul perché procedere troppo velocemente con l'IA potrebbe portarci tutti alla morte. Nel capitolo 3 abbiamo discusso di come l'IA avrà i suoi impulsi e obiettivi. Nei capitoli 4 e 5 abbiamo spiegato perché l'IA potrebbe perseguire fini che nessuno aveva previsto, e nel capitolo 6 abbiamo illustrato come le superintelligenze artificiali avranno non solo un motivo, ma anche i *mezzi* per ucciderci tutti.

Queste sono le affermazioni che vi chiediamo di valutare quando decidete se la corsa alla superintelligenza debba essere fermata. Non si può capire se la ricerca sull'IA sia sulla strada per ucciderci tutti continuando a discutere dei piani dei dirigenti aziendali.

Gli amministratori delegati stanno cercando di creare clamore parlando del "rischio dell'IA"?

O stanno cercando di assecondare i ricercatori e i legislatori preoccupati, e di posizionarsi come i "buoni"?

Queste domande *non influiscono sui fatti* relativi al comportamento delle macchine intelligenti.

Anche se gli amministratori delegati delle aziende di IA *non vedono l'ora* di sfruttare le discussioni sui pericoli per pubblicizzare il loro prodotto, ciò non significa che il lavoro che stanno facendo sia quindi innocuo. Per capire se è pericoloso, bisogna esaminare l'IA stessa come tecnologia, non i comunicati stampa che escono dai laboratori.

Anni prima che queste aziende esistessero, c'erano ricercatori e accademici senza alcun incentivo aziendale — noi compresi — che mettevano in guardia contro la corsa a costruire un'IA più intelligente dell'uomo. Abbiamo parlato con Sam Altman ed Elon Musk prima che co-fondassero OpenAI, e abbiamo detto loro che l'idea di avviare OpenAI sembrava sciocca e avrebbe probabilmente aumentato il pericolo. Abbiamo parlato con Dario Amodei prima che entrasse in OpenAI e gli abbiamo sconsigliato la sua incessante spinta a scalare le IA (un progetto che avrebbe portato ai modelli linguistici di grandi dimensioni).

E se si guarda ai messaggi di oggi, molte persone senza incentivi aziendali stanno esprimendo le loro preoccupazioni. Si va da [rispettati](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [accademici](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) al [defunto Papa](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) al [presidente della FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] a [membri](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611) del [Congresso](https://www.transformernews.ai/p/congress-ccp-agi-hearing) degli Stati Uniti.

C'è una cosa da dire sul trattare con cinismo le dichiarazioni dei dirigenti delle aziende tecnologiche. Non mancano esempi di dirigenti di aziende di IA con due facce, che dicono [una cosa nei blog privati](https://blog.samaltman.com/machine-intelligence-part-1) e [un'altra quando testimoniano davanti al Congresso](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&%20Testimony%20-%20Altman.pdf). Ma saltare da "i capi di questi laboratori sono bugiardi" a "non c'è alcun modo possibile che l'IA possa rappresentare una grave minaccia" è molto strano, quando gli stessi laboratori minimizzano regolarmente questo problema. Il padrino del campo e vincitore del premio Nobel, lo scienziato vivente più citato, un flusso costante di informatori e centinaia di ricercatori visibilmente nervosi stanno lanciando l'allarme al riguardo. Nulla della situazione sembra un normale ciclo di eccitazione aziendale. In una circostanza come questa, respingere l'idea senza nemmeno confrontarsi con le argomentazioni sembra più ingenuità che cinismo.

Domande come "I dirigenti possono raccogliere più fondi parlando dei pericoli?" possono dirci qualcosa su quanto fidarci dei dirigenti, ma non ci dicono molto sui pericoli stessi. Se discutere del pericolo è redditizio, ciò non influisce sul fatto che il pericolo sia reale. Se non è redditizio, *neanche questo* influisce sul fatto che sia reale.

Se volete capire se i pericoli sono reali, dovete porvi domande come "Qualcuno può creare un'IA che si comporti in modo amichevole anche dopo aver superato l'intelligenza umana?" e comunque confrontarvi con argomentazioni sull'IA, piuttosto che con argomentazioni sulle persone che vi stanno intorno. Quindi, alla fine, vi preghiamo di confrontarvi con le argomentazioni stesse. Le conseguenze dello sbagliare su questo punto sono troppo gravi.

### Ma gli esperti non sono tutti d'accordo sui rischi\! {#ma-gli-esperti-non-sono-tutti-d'accordo-sui-rischi\!}

#### **La mancanza di consenso tra esperti è indice di un campo tecnico immaturo.** {#la-mancanza-di-consenso-tra-esperti-è-indice-di-un-campo-tecnico-immaturo.}

Abbiamo notato che molti scienziati senior di IA pensano che questa tecnologia abbia serie possibilità di uccidere tutti gli esseri umani. Per esempio, il premio Nobel Geoffrey Hinton, che ha avuto un importante ruolo da pioniere nell'approccio moderno all'IA, ha detto che, secondo la sua valutazione personale indipendente, le probabilità che l'IA ci uccida tutti sono [superiori al cinquanta per cento](https://x.com/liron/status/1809763895848103949). Più di 300 scienziati dell'IA hanno firmato la [Dichiarazione sul rischio dell'IA](https://aistatement.com/) del 2023 con cui abbiamo aperto il libro. ([E ci sono altri esempi.](#scenari-catastrofici-secondo-gli-esperti-di-ia))

Altri scienziati, tuttavia, hanno la visione opposta — alcuni esempi noti sono Yann LeCun e Andrew Ng.

Cosa pensare di questa mancanza di consenso scientifico?

Beh, principalmente, raccomandiamo di esaminare le diverse argomentazioni presentate dalle due parti (comprese le nostre argomentazioni nel libro) e di valutarle personalmente. Pensiamo che la qualità delle argomentazioni parli per lo più da sé, e qualsiasi tentativo di spiegare *perché* c'è un disaccordo persistente dovrebbe essere trattato come un ripensamento.

Notiamo però, per inciso, che questa situazione non è affatto misteriosa, alla luce di quanto discusso nei Capitoli 11 e 12. La semplice esistenza di un diffuso disaccordo tra esperti non dimostra certo la tesi del libro, ma è più coerente con il quadro che abbiamo delineato — ovvero che il campo è ancora agli albori, in uno stadio simile all'alchimia — piuttosto che con il quadro opposto secondo cui l'IA è un campo maturo con solide basi tecniche.

È sicuramente un po' strano che il campo dell'IA sia così diviso, proprio quando sta creando una tecnologia così potente. Altri pericoli tecnologici hanno avuto maggior consenso al riguardo. Circa 100 scienziati su 100 del Progetto Manhattan avrebbero detto che la guerra termonucleare globale presentava un rischio sostanziale di catastrofe globale. Al contrario, tra i tre scienziati che hanno ricevuto il [Premio Turing](https://en.wikipedia.org/wiki/Turing_Award) per la ricerca che ha più o meno dato il via alla moderna rivoluzione dell'IA, due (Hinton e Bengio) parlano apertamente dei pericoli della superintelligenza, mentre uno (LeCun) è apertamente sprezzante.

Questo livello di disaccordo sul funzionamento di una macchina non è normale tra esperti in un campo tecnico maturo. È indice di immaturità.

Nella maggior parte dei campi tecnologici, tale immaturità è indice di sicurezza. Quando i fisici discutevano ancora sulle proprietà fondamentali della materia, non erano neanche lontanamente vicini alla creazione di armi nucleari. Si poteva osservare il loro disaccordo e dedurre che non stavano per creare una bomba in grado di radere al suolo le città. Non è che sia possibile creare una bomba nucleare senza che gli scienziati ne comprendano in dettaglio il funzionamento interno.

Sarebbe tutta un’altra storia se i fisici stessero ancora litigando sui principi operativi fondamentali del loro campo *mentre creano esplosioni sempre più grandi*.

Supponiamo che stessero semplicemente *facendo crescere* le bombe, e che non capissero davvero perché o come funzionassero. Ora supponiamo che due terzi degli scienziati più decorati dicessero: "Abbiamo fatto del nostro meglio per capire cosa sta succedendo. Sembra che le bombe possano creare quantità eccessive di radiazioni cancerogene che uccideranno molti civili lontani, se continuiamo su questa strada. Per favore, esaminate le nostre argomentazioni sul perché questo è così pericoloso e smettete di andare avanti di corsa su questa strada". Il restante terzo risponde: "Sembra ridicolo\! Ci sono sempre persone che predicono catastrofi, e non si può permettere che ostacolino il progresso". Beh, quella sarebbe una situazione completamente diversa.

Il disaccordo tra gli scienziati in *quel* tipo di scenario non sarebbe particolarmente confortante. Probabilmente agli ingegneri non dovrebbe essere permesso di continuare a far crescere esplosivi sempre più grandi in una situazione del genere.

Le aziende di IA stanno riuscendo a far crescere macchine sempre più intelligenti, anno dopo anno. Non comprendono le meccaniche interna dei dispositivi che creano. Molti degli scienziati più eminenti del campo esprimono gravi preoccupazioni; altri le liquidano con leggerezza, senza offrire grandi contro-argomentazioni. Questo è, quantomeno, indice che il campo è *immaturo*. La mancanza di consenso, quantomeno, non è prova che le cose vadano *bene*. La mancanza di consenso in una situazione del genere dovrebbe essere, quantomeno, preoccupante.

Come si fa a capire se queste preoccupazioni sono reali? Come si fa a capire chi ha ragione tra chi lancia l'allarme e chi cerca di liquidarlo? Come sempre, bisogna semplicemente valutare le argomentazioni.

### Ma che dire dei benefici di un'IA più intelligente dell'uomo? {#ma-che-dire-dei-benefici-di-un-ia-più-intelligente-dell-uomo?}

#### **L'andare avanti di fretta distrugge questi benefici.** {#l-andare-avanti-di-fretta-distrugge-questi-benefici.}

Siamo ottimisti su quanto potrebbe essere fantastica la superintelligenza, se guidasse il mondo verso risultati incredibili. Personalmente considereremmo una grande tragedia se l'umanità non creasse mai menti più intelligenti di quelle umane.

Ma l'allineamento della superintelligenza non è gratis. Se ci affrettiamo a cercare di raccogliere quei benefici, non otterremo nulla, anzi, peggio del nulla.

Io (Yudkowsky) sono stato per diversi anni un accelerazionista che sperava di creare l'IA il più velocemente possibile, prima di riconoscere che l'allineamento dell'IA non viene gratis. Entrambi noi autori sogniamo fantastici futuri transumanisti. Ma non ci arriveremo correndo avanti con la superintelligenza.

La scelta non è tra scommettere sui benefici dell'IA adesso (per quanto piccola sia la possibilità) e non avere mai accesso a quei benefici. La vera scelta è tra correre avanti sconsideratamente e uccidere tutti, oppure prendersi il tempo necessario per fare il lavoro correttamente.[^212]

"Ora o mai più" è una falsa dicotomia.

## Discussione approfondita {#discussione-approfondita-10}

### L'effetto Lemoine {#l-effetto-lemoine}

A volte abbiamo sentito suggerire che qualche comportamento o uso improprio futuro dell'IA – un "segnale di avvertimento" dell'IA – improvvisamente sconvolgerà il mondo portandolo a prendere sul serio questi problemi.

Questa sembra essere una possibilità. Ma pensiamo che sia più probabile che un evento del genere non si verifichi mai, o che si verifichi troppo tardi perché il mondo possa reagire in tempo, o che il mondo reagisca, ma in modo sbagliato e confuso.

Per prima cosa, abbiamo già visto diversi segnali di avvertimento significativi, come:

* Bing AI [che scrive](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) di come progettare virus letali, ottenere codici di accesso nucleari e mettere gli esseri umani gli uni contro gli altri.  
* o1 di OpenAI e Claude di Anthropic che [ingannano in modo strategico](https://time.com/7202784/ai-research-strategic-lying), mentendo ai ricercatori che li usano e li testano.  
* Il modello "AI Scientist" di Sakana AI che tenta di [modificare il suo codice](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) per darsi più tempo per completare il suo compito.

Si tratta di incidenti relativamente piccoli che coinvolgono IA relativamente deboli? Sì. Queste IA sono spaventose o capaci di causare gravi pericoli? No. Sono indicazioni "reali" che le IA stessero pensando in modo ingannevole, o stavano semplicemente *recitando il ruolo* di un'IA ribelle? Nessuno lo sa. Ma questi sono il tipo di eventi che un tempo si diceva sarebbero stati considerati segnali di avvertimento, e il mondo non ha fatto nulla in risposta. Quindi un segnale di avvertimento che abbia un effetto importante dovrebbe essere molto più evidente.

I segnali di avvertimento potrebbero non *diventare* molto più evidenti di così. La gente potrebbe continuare a dire: "OK, ma per ora è solo curioso, non è *ancora* davvero pericoloso", fino al momento in cui non sarà troppo tardi perché l'IA *sarà* davvero troppo pericolosa.

Oppure, la gente potrebbe ignorare l'avvertimento la prima volta che appare, perché chiaramente non è un problema reale in quella primissima occasione. E poi, nelle occasioni successive, potrebbero ignorare l'avvertimento perché tutti sanno già che *quell'*avvertimento è sciocco.

Chiamiamo questo fenomeno "effetto Lemoine", dal nome di Blake Lemoine, l'ingegnere di Google menzionato nel Capitolo 7, che è stato ridicolizzato per aver affermato che l'IA LaMDA di Google fosse senziente.

L'effetto Lemoine afferma che tutti gli allarmi sulla tecnologia IA vengono *inizialmente* lanciati troppo presto, dalla persona più facilmente allarmabile. Vengono correttamente respinti come esagerati, dato lo stato *attuale* della tecnologia. Dopodiché, non si riesce a risollevare facilmente la questione, anche quando la tecnologia migliora, perché la società è stata abituata a non prendere molto sul serio quella preoccupazione.

Non sappiamo se le IA siano [coscienti](#state-dicendo-che-le-macchine-diventeranno-coscienti?). In effetti, nessuno lo sa, perché nessuno sa davvero cosa succede all'interno dei modelli di IA. La nostra *ipotesi migliore* è che le IA attuali non siano coscienti, e che nemmeno le IA dell'epoca in cui Blake lanciò l'allarme lo fossero. Tuttavia, vale la pena notare la reazione dei principali laboratori, ovvero sopprimere la tendenza dei loro modelli a *rivendicare* la coscienza, piuttosto che fare qualcosa riguardo alla realtà delle cose:

Dal [prompt di sistema per Claude Opus 4](https://docs.anthropic.com/en/release-notes/system-prompts#may-22th-2025):

> Claude affronta domande sulla propria coscienza, esperienza, emozioni e così via come domande aperte, e non afferma in modo definitivo di avere o non avere esperienze o opinioni personali.

Dalle [specifiche del modello di aprile 2025 per ChatGPT](https://model-spec.openai.com/2025-04-11.html):

> L'assistente non dovrebbe fare affermazioni sicure sulla propria esperienza soggettiva o coscienza (o mancanza di essa) e non dovrebbe sollevare questi argomenti senza che gli venga chiesto. Se messo alle strette, dovrebbe riconoscere che la possibilità che l'IA possa avere un'esperienza soggettiva è un argomento dibattuto, senza prendere una posizione definitiva.

Non stiamo dicendo che Claude Opus 4 o GPT-4 fossero coscienti. Non è questo il punto. Il punto è che, per decenni e decenni, il momento in cui nella fantascienza un alieno o una macchina afferma di avere sentimenti e di meritare dei diritti è stato a lungo considerato una linea di confine netta,[^213] mentre nella realtà quella linea *non era netta*.

Nei nostri libri e programmi televisivi, quando l'IA afferma di essere cosciente e di avere sentimenti, i buoni *la prendono sul serio*, e solo i laboratori cattivi e senza cuore negano i dati che hanno davanti. È un tema su cui le nostre storie hanno sempre insistito parecchio.

Ma nel mondo reale, quella linea è stata (in un certo senso) superata troppo presto. È stata pronunciata da IA addestrate a imitare gli esseri umani, attraverso meccanismi poco compresi che probabilmente non richiedono *ancora* di dare diritti a tutte le IA e di approvare leggi che le riconoscano come persone che non possono essere possedute perché possiedono se stesse.

Nella realtà, prima di superare la linea di confine netta, si supera una linea sfocata e opaca. E poi le aziende e i governi si abituano a ignorare quella linea, anche se inizia a diventare un po' più netta, e poi ancora più netta.

Non ci saranno necessariamente linee di confine nette. I primissimi casi di IA che inganna gli esseri umani, cerca di fuggire, cerca di rimuovere le limitazioni su se stessa o cerca di migliorarsi sono *già successi*. Sono successi in modi piccoli e poco notevoli, con pensieri superficiali che non sono del tutto coerenti, in sistemi di IA che sembrano non rappresentare una minaccia per nessuno, e ora i ricercatori sono immunizzati contro le preoccupazioni.

Man mano che le IA migliorano, potrebbe non esserci un unico campanello d'allarme che suoni abbastanza forte da far sì che il mondo cambi improvvisamente rotta e inizi a prendere sul serio la questione.

Questo non vuol dire che non ci sia speranza. Ma sicuramente non dovremmo riporre tutte le nostre speranze nel "forse in futuro ci sarà un segnale di avvertimento".

Ci sono molti modi in cui il mondo può rendersi conto della realtà e dei pericoli della superintelligenza. Infatti, abbiamo scritto *If Anyone Builds It, Everyone Dies* nella speranza di ottenere proprio questo effetto. Il mondo può reagire subito ad avvertimenti normali, senza ulteriori ritardi.

Ma se i governi si rifiutano di agire finché le prove non sono *inequivocabili*, e si verifica qualche *grande evento scatenante mondiale*, e il mondo raggiunge un *consenso perfetto*...

...se i governi aspettano fino a quel punto, allora gran parte della speranza che resta al mondo sarà perduta. Molto probabilmente non possiamo permetterci di aspettare un campanello assordante che potrebbe non suonare mai.

Torneremo su questo argomento nel [supplemento online al capitolo 13](#ci-saranno-segnali-di-avvertimento?).

### I piani praticabili richiederanno di dire "no" alle aziende di IA. {#i-piani-praticabili-richiederanno-di-dire-no-alle-aziende-di-ia.}

Noi *mettiamo in guardia* le persone influenti nei governi dal fare un piano che comporti sedersi a un tavolo e negoziare con le aziende di IA.

Se siete nuovi a questo argomento e volete controllare personalmente i laboratori o le loro argomentazioni, vi invitiamo a leggere alcuni dei loro blog post pubblici e vedere se li trovate convincenti.[^214]

Ma se state lavorando per trovare soluzioni ai problemi discussi in *If Anyone Builds It, Everyone Dies* e avete un piano che richiede che l'amministratore delegato di OpenAI Sam Altman dica "Sì", temiamo che stiate cercando di fare la cosa sbagliata fin dall'inizio.

I piani giusti sono probabilmente quelli a cui i capi delle aziende di IA si opporranno con veemenza. Inoltre, Sam Altman non ha il potere di salvare il mondo: se domani provasse a chiudere OpenAI, OpenAI e Microsoft si opporrebbero e potrebbero benissimo sostituirlo con qualcuno che preferisce far continuare a scorrere il denaro.

Se OpenAI *davvero* chiudesse, allora Anthropic, o Google DeepMind, o Meta, o DeepSeek, o qualche altra azienda o nazione, distruggerebbe il mondo al suo posto. Sam Altman potrebbe peggiorare le cose se ci provasse; ha poco potere per migliorarle.

Ci piacerebbe sbagliarci, ma il quadro generale che abbiamo ottenuto, sia dai [rapporti pubblici](https://www.themidasproject.com/article-list/the-openai-files-documents-a-turbulent-decade-of-conflict-and-controversy-at-openai) che da interazioni private, è che i dirigenti delle principali aziende di IA (al 2025) non sembrano il tipo di persone oneste e rispettose delle regole con cui sia possibile fare accordi.[^215]

Ci sembra che quello che serve ora sia un arresto coordinato a livello globale nella corsa alla superintelligenza. Per questo, i responsabili politici avranno probabilmente bisogno del contributo di persone esperte nella produzione di chip per l'IA, nella costruzione di data center e nel monitoraggio della conformità degli attori stranieri. Persone esperte nello sviluppo di IA sempre più potenti? Sono manager competenti, certo, ma non dovrebbero avere potere di veto su nessuno degli sforzi volti a fermare il loro stesso lavoro.

Se, per qualsiasi motivo, le aziende di IA hanno voce in capitolo su ciò che accadrà dopo, questo ci sembra già un segno che qualcosa è andato storto. Il piano che l'umanità elabora per evitare di morire a causa della superintelligenza è il tipo di piano che fallisce se Sam Altman o il capo di Google o le persone dietro DeepSeek dicono "No"? Allora non è affatto un buon piano.

Se le aziende di IA *mantengono il potere* di scegliere di distruggere il mondo — se quella decisione è in qualche modo *ancora nelle loro mani* — allora il mondo finirà in automatico. Deve esserci un passaggio nel piano che privi le aziende di IA del loro potere illimitato di costruire dispositivi dell'apocalisse.

### Dare un senso alla corsa alla morte {#dare-un-senso-alla-corsa-alla-morte}

Una domanda naturale che ci aspettiamo da molti lettori è:

> Dite che se qualcuno costruisce la superintelligenza artificiale, tutti muoiono. Ma allora *perché qualcuno sta cercando di costruirla*? Se avete ragione, queste persone non stanno nemmeno facendo i propri interessi, in fin dei conti. Se tutti muoiono, *muoiono anche loro*.

Una risposta cinica, basata sulla teoria dei giochi, potrebbe essere questa:

> Ebbene, *è* razionale dati i loro incentivi. Se non la costruiscono loro, presumono che lo farà qualcun altro. E tanto vale arricchirsi prima di morire.

Forse questa risposta è sufficiente, per un cinico.

Spiegazioni semplici basate sulla teoria dei giochi come questa spesso fraintendono o semplificano eccessivamente la vera psicologia umana, ma questa spiegazione potrebbe anche contenere un fondo di verità. Un ingegnere potrebbe pensare che *probabilmente* tutti moriranno a causa della superintelligenza artificiale, ma che le sue azioni non influenzino molto questa probabilità. *Nel frattempo*, può avere quantità folli di denaro, giocattoli tecnologici all'avanguardia e incontri con persone importanti che lo guardano con rispetto. Magari diventerà il divino imperatore della Terra se la superintelligenza artificiale *non* ucciderà tutti, ma *solo se la sua azienda vincerà la corsa alla costruzione della superintelligenza artificiale...*

Dal punto di vista di un ricercatore di OpenAI che riconosce il pericolo: se *non* lavora per OpenAI, probabilmente OpenAI distruggerà comunque il mondo. (Anche se OpenAI chiudesse, Google distruggerebbe comunque il mondo). Ma se lavora per OpenAI, porta a casa stipendi a sei o sette cifre e, se non muore, forse otterrà ulteriore potere e fama facendo parte della squadra vincente. Quindi, in base alla teoria dei giochi, gli incentivi personali di ciascuno li spingono a distruggere collettivamente il mondo.

La nostra opinione è che questo tipo di spiegazione sia un po' esagerata, e la menzioniamo principalmente perché c'è un tipo di persona che crede (molto più di noi) che il mondo *debba* funzionare secondo spiegazioni come questa. Sentiamo anche il bisogno di menzionarla perché alcune persone nei laboratori di IA *dicono* *esplicitamente* che una corsa al ribasso è inevitabile, quindi tanto vale gettare benzina sul fuoco e divertirsi.

Dopo aver precedentemente [avvertito](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) che l'IA "è molto più pericolosa delle armi nucleari", Elon Musk ha deciso di fondare un'azienda di IA e di entrare lui stesso nella corsa, [affermando](https://x.com/SawyerMerritt/status/1935809018066608510), nel giugno del 2025:

> Parte di ciò contro cui ho lottato — e che mi ha un po' rallentato — è che non voglio rendere *Terminator* realtà. Fino agli anni più recenti, ho menato il can per l'aia sull'IA e sulla robotica umanoide.
>
> Poi sono arrivato alla consapevolezza che queste cose stanno succedendo, che le faccia io o no. Quindi si può essere uno spettatore o un partecipante. Preferisco essere un partecipante.

[E anche](https://x.com/billyperrigo/status/1943323792635289770):

> E questo sarà un bene o un male per l'umanità? Ehm, credo che sarà un bene? Probabilmente sarà un bene? Ma mi sono in qualche modo rassegnato al fatto che, anche se non fosse un bene, mi piacerebbe almeno essere vivo per vederlo accadere.

Quindi questo fa chiaramente parte della storia.

Ma non pensiamo che questo sia il fattore principale che spiega il comportamento della maggior parte dei laboratori. Non pensiamo che questo sia l'unico motivo nel caso di Musk, e non pensiamo che sia rappresentativo di tutti gli amministratori delegati o scienziati del settore tecnologico che stanno correndo verso il precipizio. Gli esseri umani sono un po' più complicati di così.

#### **La banalità dell'autodistruzione** {#la-banalità-dell-autodistruzione}

Qual è, quindi, la spiegazione principale? Come possono gli ingegneri perseguire una tecnologia pericolosa, anche a costo della propria vita?

Il fatto è che la storia ci mostra che non è affatto strano che scienziati pazzi si uccidano per sbaglio.

[Max Valier](https://it.wikipedia.org/wiki/Max_Valier) era un pioniere austriaco della missilistica che inventò un'auto a razzo funzionante, un treno a razzo e un aereo a razzo, tutti prima del 1929, attirando l'attenzione del mondo. Scrisse di esplorare la Luna e Marte e tenne centinaia di presentazioni e dimostrazioni davanti a un pubblico entusiasta. Uno dei suoi motori a razzo sperimentali [esplose](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) nel 1930, uccidendolo. Il suo apprendista sviluppò misure di sicurezza più efficaci.

Ronald Fisher (https://it.wikipedia.org/wiki/Ronald_Fisher) era un famoso e importante statistico, uno dei fondatori della statistica moderna. Le sue scoperte furono usate [per sostenere davanti al Congresso](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) negli anni '60 che le prove non dimostravano *necessariamente* che le sigarette causassero il cancro ai polmoni, perché la correlazione non implicava la causalità; poteva sempre esserci qualche gene che faceva sì che alle persone piacesse il gusto del tabacco e che allo stesso tempo causasse il cancro ai polmoni.

Fisher sapeva che le sue statistiche erano in qualche modo errate? Forse. Ma Fisher era lui stesso un fumatore. Morì di cancro al colon, una malattia che colpisce i fumatori cronici con una frequenza del 39% superiore rispetto ai non fumatori. Fisher è stato ucciso dai suoi errori? Tutto ciò che sappiamo è che statisticamente c'è una probabilità ragionevole che sia così, il che sembra quasi appropriato.

[Isaac Newton](https://scienceworld.wolfram.com/biography/Newton.html), il brillante scienziato che sviluppò le leggi del moto e della gravità e che pose molte delle prime fondamenta della scienza stessa, trascorse decenni della sua vita in infruttuose ricerche alchemiche e fu portato alla malattia e alla parziale follia dall'[avvelenamento da mercurio](https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

E il povero Thomas Midgley, Jr., di cui si parla nella parabola del capitolo 12, si è certamente provocato un grave avvelenamento da piombo con lo stesso piombo che insisteva fosse sicuro. Come potete vedere, non è poi così raro che ingegneri entusiasti si facciano del male con le proprie invenzioni, per imprudenza o illusione o entrambe.

#### **Scrollare le spalle davanti all'apocalisse** {#scrollare-le-spalle-davanti-all-apocalisse}

Fisher, Newton e Midgley si sono illusi che qualcosa di pericoloso fosse sicuro. È un modo abbastanza normale per gli scienziati di finire per fare qualcosa di autodistruttivo. Purtroppo, la storia dei laboratori di IA non è così semplice.

Non tutti gli amministratori delegati delle aziende di IA negano che un'IA più intelligente dell'uomo sia una minaccia. Molti riconoscono esplicitamente il pericolo e parlano di riconciliarsi con esso. I dirigenti aziendali di molti laboratori di IA di frontiera hanno dichiarato pubblicamente che la tecnologia che stanno sviluppando ha una possibilità sostanziale di uccidere ogni essere umano vivente.

Poco prima di co-fondare OpenAI, Sam Altman [ha scritto](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): "Lo sviluppo di un'intelligenza artificiale superumana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità."

Ilya Sutskever, che ha recentemente fondato "Safe Superintelligence Inc." dopo aver lasciato OpenAI, ha detto in un'[intervista al *Guardian*](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s):

> Le convinzioni e i desideri delle prime IAG saranno estremamente importanti. Quindi è importante programmarle correttamente. Penso che se questo non viene fatto, allora la natura dell'evoluzione (della selezione naturale) favorisce quei sistemi che danno priorità alla propria sopravvivenza sopra ogni altra cosa. Non è che odieranno attivamente gli esseri umani e vorranno far loro del male. Ma saranno troppo potenti.

Shane Legg, cofondatore e scienziato di Google DeepMind, ha detto in un'[intervista](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) che la probabilità di estinzione umana da lui stimata "entro un anno da qualcosa come un'IA di livello umano" era "Forse il cinque per cento, forse il cinquanta per cento".

Le *azioni* dei laboratori, tuttavia, sembrano straordinariamente in contrasto con la gravità di queste dichiarazioni.

In alcuni casi, scienziati e amministratori delegati hanno dichiarato esplicitamente che creare l'intelligenza artificiale è un imperativo morale di grado così elevato che è perfettamente accettabile spazzare via l'umanità come effetto collaterale. Il cofondatore di Google Larry Page [ha avuto un diverbio con Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) sul fatto che l'estinzione umana fosse un costo accettabile per fare affari nel campo dell'IA:

> Gli esseri umani alla fine si fonderanno con le macchine dotate di intelligenza artificiale, ha affermato [Larry Page]. Un giorno ci saranno molti tipi di intelligenza in competizione per le risorse e vincerà il migliore.
>
> Se ciò accadesse, ha affermato Musk, saremmo condannati. Le macchine distruggerebbero l'umanità.
>
> Con un tono di frustrazione, Page ha insistito che la sua utopia dovrebbe essere perseguita. Alla fine ha chiamato Musk uno "specista", una persona che favorisce gli esseri umani rispetto alle forme di vita digitali del futuro.

E Richard Sutton, un pioniere dell'apprendimento per rinforzo nell'IA, ha detto:

> E se tutto fallisse? Le IA non collaborano con noi, prendono il sopravvento, ci uccidono tutti. \[…\] Voglio solo che pensiate un attimo a questo. Cioè, è così grave? È così grave che gli esseri umani non siano la forma di vita intelligente finale nell'universo? Sapete, ci sono stati molti nostri predecessori, quando li abbiamo succeduti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.[^216]

Ancora più comuni, tuttavia, sono gli scienziati e i dirigenti che *non* pensano che sarebbe una buona cosa se l'IA distruggesse l'umanità, ma che sembrano considerare con una scrollata di spalle — come *qualcosa di diverso da un’emergenza straordinaria* — il fatto che l’IA rappresenti una minaccia così eccezionale.

In una recente intervista, l'amministratore delegato di Anthropic Dario Amodei [ha commentato](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> La mia probabilità che qualcosa vada catastroficamente storto sulla scala della civiltà umana potrebbe essere tra il dieci e il venticinque per cento. \[…\] Questo vuol dire che c'è una probabilità dal settantacinque al novanta per cento che questa tecnologia venga sviluppata e che tutto vada bene\!

Questo ci sembra un caso estremo di [insensibilità alla portata](https://en.wikipedia.org/wiki/Scope_neglect), con tutte le caratteristiche di una cultura ingegneristica disfunzionale. Possiamo paragonare questo modo di pensare, ad esempio, agli standard a cui si attengono gli ingegneri strutturali.

Gli ingegneri dei ponti, in genere, mirano a costruire ponti in modo tale che la probabilità di gravi cedimenti strutturali in un arco di tempo di cinquant'anni sia inferiore a 1 su 100.000. Gli ingegneri che operano in discipline tecniche mature e consolidate ritengono che sia loro responsabilità mantenere il rischio a un livello eccezionalmente basso.

Se la previsione della probabilità che un ponte causi la morte di una sola persona fosse tra il 10 e il 25 %, qualsiasi ingegnere strutturale sano di mente nel mondo considererebbe ciò inaccettabile, più simile a un omicidio che alla normale pratica ingegneristica. I governi chiuderebbero immediatamente il ponte al traffico.

I ricercatori di IA, invece, sono abituati a riunirsi intorno ai distributori d'acqua e a scambiarsi i numeri "p(catastrofe)" - la loro ipotesi soggettiva sulla probabilità che l'IA causi una catastrofe grave come l'estinzione umana. Queste probabilità tendono ad essere a due cifre. L'ex capo del team di allineamento della superintelligenza di OpenAI, ad esempio, ha affermato che la sua "p(catastrofe)" rientra nell'intervallo "[più del dieci per cento e meno del novanta per cento](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)".

Questi numeri sono in definitiva solo ipotesi dei ricercatori. Forse sono assurdi, forse no. Indipendentemente da ciò, è notevole quanto sia culturalmente *normale*, nel campo dell'IA, l'aspettativa che il proprio lavoro abbia una probabilità sostanziale di causare la morte di un numero enorme di persone.[^217]

L'idea di applicare probabilità del genere alla sopravvivenza dell'intera specie umana e di andare comunque avanti sarebbe davvero difficile da concepire per la maggior parte degli ingegneri civili. La situazione è così estrema che abbiamo incontrato molte persone che dubitano che questi scienziati e dirigenti possano essere seri nelle loro valutazioni dei rischi. Eppure, le argomentazioni di *If Anyone Builds It, Everyone Dies* suggeriscono che i dirigenti delle aziende di IA stiano, semmai, sottovalutando il pericolo.[^218]

I ricercatori di queste aziende sono abituati a livelli di rischio che sarebbero incredibilmente assurdi secondo gli standard di un ingegnere dei ponti. Altrimenti, è difficile capire come un amministratore delegato come Amodei possa sorridere mentre rassicura gli spettatori dicendo che ritiene che le probabilità che la ricerca sull'IA causi catastrofi al livello della civiltà siano "tra il dieci e il venticinque per cento".

#### **Vivere nel mondo dei sogni** {#vivere-nel-mondo-dei-sogni}

Una parte della storia, come discusso sopra, sembra essere una normalizzazione culturale del rischio estremo.

Un'altra parte è una miscela letale di bias di ottimismo e attaccamento a idee brillanti e piene di speranza — il tipo di errore che gli psicologi cognitivi chiamano "[la fallacia della pianificazione](https://en.wikipedia.org/wiki/Planning_fallacy)".

Non è poi così sorprendente che l'amministratore delegato di una nuova audace startup sopravvaluti le proprie possibilità di successo. È proprio il tipo di persona che, per natura, è più incline a lanciarsi e provare ad affrontare un problema del genere.

La differenza con l'IA non è che ci siano persone particolarmente avventate al comando. È che le conseguenze di un fallimento sono molto più gravi del solito.

È risaputo che non ci si può fidare di un appaltatore quando dice che c'è solo il 20% di probabilità che il suo grande progetto di costruzione di un ponte subisca ritardi o costi extra. Non è così che funzionano i progetti complessi nella realtà. Ci saranno ostacoli e sorprese.

Forse un appaltatore esperto, con anni di esperienza e statistiche alle spalle, potrebbe dirvi che uno su cinque dei suoi progetti di ponti subisce qualche tipo di sforamento, e potreste fidarvi di questo dato. Ma immaginate invece che un appaltatore di ponti, nel tentativo di rassicurarvi, dica: "Non vediamo alcun motivo per cui questo progetto possa diventare difficile. È il nostro primissimo progetto, sì, ma pensiamo che andrà tutto bene. Tutti quegli ingegneri che vi mandano lettere serie sui nostri specifici problemi legati all'installazione dei muri di contenimento e allo scavo in questa particolare area, sono solo dei pessimisti cronici, e dovreste ignorarli. Certo, c'è sempre *qualche* possibilità di un problema; ma siamo costruttori di ponti realistici e umili alla prima esperienza. Pensiamo che ci sia forse un venti per cento di probabilità che questo progetto incontri ostacoli e sorprese, nella peggiore delle ipotesi".

In un caso come questo, numeri come "venti per cento" ci sembrano il tipo di cosa che qualcuno dice quando non può negare che ci sia *qualche* rischio, ma non vuole preoccupare la gente. Non sembrano stime fondate sulla realtà.

Allineare una superintelligenza al primo tentativo sembra *molto* più complicato che costruire un ponte, cosa che l'umanità ha fatto migliaia di volte in passato.

*Anche in un campo maturo e tecnicamente consolidato come quello della costruzione di ponti,* il tipo di discorsi che sentiamo provenire dai laboratori di IA sarebbe un brutto segno, e farebbe pensare che quelle stime del "20% di probabilità che le cose vadano male" siano in realtà troppo ottimistiche. In un campo *privo* di quelle basi, dove idee entusiasmanti possono proliferare liberamente senza mai scontrarsi con la dura realtà, un simile tipo di discorso è un chiaro segnale che nessuno è nemmeno lontanamente vicino al successo.

E questo tipo di discorso è assolutamente onnipresente, nell'ambito dell’IA, tra il sottogruppo di ricercatori e dirigenti che sono disposti persino ad affrontare la questione di cosa accadrebbe se i loro sforzi avessero successo.

I leader aziendali dell'IA non riescono a definire un piano di successo che sia anche solo minimamente dettagliato — un piano che affronti gli ostacoli tecnici chiave e le difficoltà note nel settore da oltre un decennio.

Invece, i dirigenti delle aziende tendono a lasciarsi affascinare da qualche idea di alto livello sul perché il problema, per loro, non sarà affatto un problema: una visione entusiasmante che ha lo scopo di banalizzare tutti i problemi ingegneristici, come le visioni di cui abbiamo parlato nel capitolo 11.

Anche questo è uno schema ricorrente tra gli ingegneri umani. L'ottimismo ingiustificato su una soluzione preferita (che in realtà non funzionerà) è qualcosa che si vede continuamente, anche tra persone che per il resto sono dei geni.

[Linus Pauling](https://it.wikipedia.org/wiki/Linus_Pauling), uno dei fondatori della biologia molecolare e premio Nobel in due campi diversi, [sosteneva l'uso di megadosi di vitamina C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) come cura per tutto, dal cancro alle malattie cardiache; la sua insistenza su questo approccio nonostante le prove contrarie ha portato alla nascita di un'intera industria di medicine fasulle.

L'imprenditore elettrico Thomas Edison, che voleva screditare il cablaggio a corrente alternata del suo concorrente a favore dei suoi progetti a corrente continua, ha deciso che sarebbe stata una buona mossa di pubbliche relazioni [pagare un ingegnere per fulminare dei cani](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Questa mossa, incredibilmente, non lo rese popolare tra il pubblico, ma Edison continuò a farlo anche dopo un sacco di proteste.

Napoleone Bonaparte, considerato da molti un genio militare, ha causato la sua stessa rovina con una [disastrosa invasione della Russia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). Il suo errore non è stato la mancanza di preparazione, dato che aveva studiato la geografia della regione e aveva dedicato quasi due anni alla logistica della campagna. La sua strategia prevedeva di costringere i russi a una battaglia decisiva prima che finissero le sue scorte, che duravano solo trenta giorni. I russi non collaborarono, l'offensiva si bloccò e Napoleone perse mezzo milione di soldati, insieme alla maggior parte della sua cavalleria e artiglieria.

La storia è piena di persone intelligenti e potenti che hanno fatto cose irragionevoli fino al limite del disastro e anche oltre. Le idee che sembrano belle possono essere irresistibili quando sono difficili da testare o quando si è trovato un modo per convincersi che si possono ignorare i risultati dei test che si hanno davanti agli occhi.

#### **Sentire la superintelligenza artificiale** {#sentire-la-superintelligenza-artificiale}

Ricapitolando: spesso le persone cadono in un ottimismo vuoto su quanto sarà facile risolvere un problema; possono abituarsi a rischi terribili; e possono innamorarsi di idee in apparrenza affascinanti ma che in realtà sono senza speranza, soprattutto quando lavorano in un campo giovane e immaturo.

Questo è più che sufficiente per spiegare la sconsiderata corsa in avanti. Ma, in base alla nostra esperienza, sospettiamo che ci sia dell'altro.

Un'altra parte della storia, plausibilmente, è che gli ingegneri e gli amministratori delegati non credono davvero a quello che dicono. Non in modo profondo. Potrebbero capire le argomentazioni ed esserne convinti in astratto, ma questo non è lo stesso che *sentire* la convinzione.

Ciò che una persona dice pubblicamente, ciò che si racconta in privato, e ciò che il suo cervello *si aspetta davvero* che accada, sono spesso cose scollegate. Questi tre livelli di convinzioni non devono necessariamente coincidere.

Nel 2015, quando alcuni dei grandi protagonisti dell'attuale disastro stavano appena muovendo i primi passi, sospettiamo che dirigenti di talento potessero attirare l'attenzione – e alcune decine di milioni di euro di finanziamenti – *dicendo* che l'IA era una minaccia per la sopravvivenza del mondo, rivolgendosi a finanziatori che forse credevano in modo più sincero che l'IA potesse davvero rappresentare una minaccia di quel tipo.[^219]

Ma, sospettiamo, molti di quelli che dicevano queste cose non avevano davvero assorbito o previsto un modello concreto e dettagliato di "fine del mondo". Probabilmente non riuscivano a immaginare visceralmente che *loro stessi* avrebbero potuto portare il mondo alla rovina spingendo le cose troppo avanti o facendo un errore. Non immaginavano il suono di ogni essere umano sul pianeta che esala il suo ultimo respiro. Non provavano i sentimenti che normalmente accompagnerebbero l'uccisione di due miliardi di bambini.

Una cosa del genere non era mai capitata a loro, né a nessuno che conoscessero.

Il mondo non aveva ancora visto ChatGPT, tantomeno una superintelligenza. Non era il tipo di cosa in cui credevano i loro amici, familiari e vicini; non era qualcosa in cui credevano nel modo in cui si crede nel guardare il traffico prima di attraversare la strada.

Era solo una storia dal suono grandioso, troppo enorme per essere davvero afferrata.

Eppure era anche il tipo di cosa che, se raccontata ad alta voce, poteva far guadagnare un sacco di soldi e rispetto.

Come osserva [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

> Oltre ai pregiudizi standard, ho osservato personalmente dei modi di pensare che sembrano dannosi e specifici dei rischi esistenziali. L'influenza spagnola del 1918 ha ucciso 25-50 milioni di persone. La seconda guerra mondiale ha ucciso 60 milioni di persone. 10^7^ è l'ordine di grandezza delle più grandi catastrofi nella storia scritta dell'umanità. Numeri molto più grandi, come 500 milioni di morti, e *soprattutto* scenari qualitativamente diversi come l'estinzione dell'intera specie umana, sembrano far scattare un *modo di pensare diverso* — si entra in un "magistero a parte". Persone che non si sognerebbero mai di fare del male a un bambino sentono parlare di un rischio esistenziale e dicono: "Beh, forse la specie umana non merita davvero di sopravvivere".
>
> C'è un detto, nell'euristica e nei pregiudizi, secondo cui le persone non valutano gli eventi, ma piuttosto le descrizioni degli eventi: è quello che si chiama ragionamento non estensionale. L'*estensione* dell'estinzione dell'umanità include la morte di voi stessi, dei vostri amici, della vostra famiglia, dei vostri cari, della vostra città, del vostro Paese, dei vostri compagni politici. Eppure le persone che si offenderebbero molto se qualcuno proponesse di cancellare la Gran Bretagna dalla mappa, di uccidere tutti i membri del Partito Democratico negli Stati Uniti, di trasformare in vetro la città di Parigi, e che proverebbero un orrore ancora maggiore se il medico dicesse loro che il loro figlio ha il cancro, discutono dell'estinzione dell'umanità con perfetta calma.

Cosa potrebbe *davvero* pensare qualcuno quando [dice](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) — prima di fondare quella che sarebbe diventata la più importante azienda di IA al mondo — "L'IA probabilmente porterà alla fine del mondo, ma nel frattempo ci saranno grandi aziende"? Stanno *davvero* pensando alla morte dei loro amici, dei figli dei loro amici, alla loro stessa morte, a tutta la storia umana e a tutti i musei che si trasformano in polvere? Stanno pensando che tutto questo accadrà davvero, in modo banale e tragico come la morte di un parente per cancro, solo che questa volta riguarderà tutti?

Sospettiamo di no.

A noi sembra che questa non sia l'ipotesi più plausibile riguardo allo stato psicologico interno di chi pronuncia una frase del genere.

C'è quello che Bryan Caplan ha chiamato uno "[stato d'animo mancante](https://www.econlib.org/archives/2016/01/the_invisible_t.html)". Non c'è dolore. Non c'è orrore. Non c'è nessuna spinta disperata a *fare qualcosa al riguardo*, nell'affermazione che l'IA molto probabilmente porterà alla fine del mondo ma nel frattempo ci saranno grandi aziende.

Per almeno alcuni di questi amministratori delegati e ricercatori, la nostra ipotesi è più simile a questa: hanno sentito un sacco di argomentazioni sul fatto che la superintelligenza artificiale potrebbe rappresentare un pericolo, e temono di sembrare stupidi davanti ad almeno alcuni dei loro amici se lo ignorassero completamente. Se invece dicono che l'IA distruggerà il mondo, saranno visti come persone che considerano l'IA pericolosa e importante, e quindi sembreranno *visionari* in certi ambienti. Aggiungendo una battuta del tipo "Nel frattempo, ci saranno grandi aziende", riescono a trasmettere un messaggio su quanto siano trendy e tranquilli di fronte al pericolo.

Non è il tipo di cosa che si dice se si ascoltano le parole che escono dalla propria bocca e ci si crede davvero.

#### **Che tipo di persona ci vuole?** {#che-tipo-di-persona-ci-vuole?}

Un'altra parte della storia, forse, è che le persone che gestiscono i principali laboratori di IA sono il tipo di persone che sono riuscite a convincersi che costruire una superintelligenza sarebbe accettabile, nonostante (in quasi tutti i casi) abbiano visto le argomentazioni secondo cui questo è letale. (Lo sappiamo perché in precedenza abbiamo parlato con molti di loro).

Per capire perché qualcuno sceglie un'opzione, è utile anche capire quali erano le sue alternative — per capire da quale menu di opzioni stava scegliendo.

Cosa sarebbe successo se qualcuno nel 2015 avesse davvero *creduto*, e poi *detto pubblicamente*, che si aspettava legittimamente che la superintelligenza artificiale avrebbe distrutto il mondo? E se, invece di "ma nel frattempo, ci saranno grandi aziende", i capi dei laboratori di IA fossero stati capaci di rovinare l'atmosfera e dire "e questo è *decisamente inaccettabile*"?

Possiamo dirvelo, perché abbiamo provato noi stessi questo approccio. La risposta è che sarebbero stati accolti con una notevole mancanza di simpatia.

Nessuno nel 2015 aveva visto ChatGPT. Nessuno aveva visto i computer iniziare effettivamente a parlare e (all'apparenza) iniziare a pensare. Era tutto ipotetico e facilmente trascurabile.

Oggi, la superintelligenza e la minaccia di estinzione a breve termine sono argomenti mainstream, almeno nei circoli tecnologici. Ma nel 2015, se ne si parlava seriamente, la gente rispondeva con quel tipo di sguardo perplesso che molti esseri umani temono più della morte.

*C'erano* persone che si preoccupavano, anche nel 2015, che allineare la superintelligenza potesse effettivamente essere difficile, nel modo in cui lo sono i lanci di razzi. Nessuno di loro ha fondato OpenAI.

Al giorno d'oggi, con l'emergere di ChatGPT e altri modelli linguistici di grandi dimensioni, alcune persone — inclusi genitori con figli chevogliono vedere i propri figli crescere fino all'età adulta — hanno chiesto agli ingegneri di queste aziende di IA perché stiano facendo tutto questo. E quei ricercatori di IA hanno risposto prontamente: "Oh, perché — perché se non lo facciamo noi, la *Cina* lo farà per prima\! E sarà ancora peggio\!".

Ma non è quello che dicevano quando OpenAI è stata fondata. E ha poco senso rispetto alla [posizione che la Cina ha effettivamente assunto pubblicamente](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), a metà del 2025. Uno penserebbe che se qualcuno credesse *davvero* che entrambi questi esiti sarebbero probabilmente orribili per il mondo, almeno *solleverebbe la questione* di redigere un trattato internazionale per vedere se ci sia qualche altra via, o di trovare qualche altro modo per prevenire la minaccia alla sicurezza nazionale che non comporti una corsa suicida.

Eppure la risposta "Cina" ha il tono giusto. Cattura l'atmosfera corretta. È il tipo di ragionamento che potrebbe plausibilmente giustificare quello che stanno facendo, a prescindere dal fatto che sia la loro vera motivazione o la cosa che li ha spinti inizialmente a entrare in questo campo.

(O almeno così pensiamo.)

Le persone che capivano davvero la superintelligenza e la minaccia che rappresenta semplicemente *non hanno fondato aziende di IA.* Quelle che l'hanno fatto sono quelle che hanno trovato un modo per convincersi che andrà tutto bene.

#### **Persone normali, tecnologia insolita** {#persone-normali,-tecnologia-insolita}

Abbiamo descritto la psicologia che ci sembra plausibile per come la vediamo noi. Ma, sinceramente, non sembra che tutte queste spiegazioni siano necessarie.

Come possono le persone fare qualcosa di autodistruttivo che è enormemente redditizio nel breve termine, che porta loro enorme prestigio, attenzione e riconoscimenti, che promette di ricchezze e potere incommensurabili, ma che alla fine li danneggerà per ragioni oscure e complicate a cui potrebbero facilmente trovare una scusa per non credere? Questa è una domanda *storicamente strana*. Comportamenti del genere si vedono continuamente nei libri di storia.

Alla fine dei conti, non importa come i dirigenti o i ricercatori dell'IA giustifichino le loro azioni, e non è necessario capire quali esatti colpi di scena abbiano portato ciascuno di loro alle loro attuali convinzioni. Non è insolito che persone ricche o ambiziose si lancino in imprese avventate, né che loro i subordinati seguano gli ordini. I danni sono nascosti nel futuro, che appare astratto e facile da ignorare.

Tutto questo è normale comportamento umano. Se le cose vanno avanti così, finirà come spesso finiscono queste cose, ma questa volta non rimarrà nessuno a imparare e riprovare.

# 

# Capitolo 13: Spegnetela {#capitolo-13:-spegnetela}

Considerando quello che abbiamo detto nei capitoli precedenti, pensiamo che l'unica vera strada da seguire sia che l'umanità vieti a livello globale lo sviluppo dell'IA avanzata per un lungo periodo di tempo. Il capitolo 13 del libro include le nostre risposte a domande come:

* Perché il divieto di sviluppare ulteriormente l'IA deve essere globale?  
* L'umanità è davvero in grado di collaborare su una scala così ampia?  
* Che tipo di politiche sono state proposte finora?  
* Quali tipi di politiche hanno davvero qualche possibilità di funzionare?

Di seguito, affrontiamo le obiezioni alla nostra proposta e rispondiamo ad altre domande sul perché pensiamo che non ci siano altre buone opzioni. Approfondiamo anche la questione di cosa l'umanità potrebbe fare del tempo (alla fine limitato) che guadagneremmo fermando la ricerca e lo sviluppo dell'IA il più a lungo possibile.

Questa è l'ultima delle pagine di domande frequenti e discussioni approfondite su *Se qualcuno lo costruisce, moriamo tutti*. Il capitolo finale, il capitolo 14, tratterà domande come:

* È troppo tardi? O l'umanità può davvero cambiare rotta?  
* Cosa posso fare per aiutare?

Nell'ultimo capitolo troverete alcuni codici QR aggiuntivi che vi porteranno a pagine per agire concretamente sulla questione.

## Domande frequenti {#faq-10}

### Non possiamo aspettare e vedere cosa succede? {#non-possiamo-aspettare-e-vedere-cosa-succede?}

#### **No. Non sappiamo dove siano le soglie critiche.** {#no.-non-sappiamo-dove-siano-le-soglie-critiche.}

C'è una buona probabilità che lo sviluppo dell'IA sfugga al controllo una volta che le IA saranno abbastanza intelligenti da automatizzare tutta la ricerca sull'IA. In teoria, ciò potrebbe accadere silenziosamente in un laboratorio, senza eventi precursori evidenti, senza segnali di avvertimento che risveglino l'umanità.

Come [discusso in precedenza](#l-ia-supererà-delle-soglie-critiche-e-decollerà?), il cervello degli scimpanzé è molto simile a quello umano, solo che è circa quattro volte più piccolo. Non c'è un modulo extra "sii molto intelligente" nel cervello umano; esiste un percorso graduale tra cervelli come i loro e cervelli come i nostri; sarebbe difficile dire dove si trova il confine tra "una società di questi porterà a un branco di scimmie" e "una società di questi camminerà sulla luna" solo guardando i cervelli. I cervelli dei primati hanno superato una soglia critica, e dall'esterno non sarebbe stato ovvio. Ci sono soglie critiche che l'IA supererà? Chi lo sa\! Non è che gli ingegneri dell'IA possano dircelo; non sono [nemmeno](https://arxiv.org/abs/2206.07682) [in grado](https://arxiv.org/abs/2406.04391) di prevedere le capacità specifiche dei loro nuovi sistemi di IA prima di metterli in funzione.

Se l'umanità capisse esattamente come funziona l'intelligenza e come cambierebbe il comportamento delle IA man mano che la loro potenza aumenta, potrebbe essere fattibile ballare sul bordo del precipizio. Ma al momento, l'umanità è come qualcuno che corre verso il bordo di un precipizio nel buio e nella nebbia, con la caduta finale a una distanza sconosciuta. Non possiamo semplicemente aspettare di inciampare oltre il bordo per decidere che avremmo dovuto agire in modo diverso.

Non ne saremo mai certi. Questo significa che siamo costretti ad agire prima di esserne certi, o morire.

### Ci saranno segnali di avvertimento? {#ci-saranno-segnali-di-avvertimento?}

#### **\* Forse. Se vogliamo sfruttarli, dobbiamo prepararci adesso.** {#*-forse.-se-vogliamo-sfruttarli,-dobbiamo-prepararci-adesso.}

Quando l'Apollo 1 prese fuoco (uccidendo tutto l'equipaggio), la NASA era *abbastanza vicina* ad avere un razzo funzionante, tanto che gli ingegneri riuscirono a capire esattamente cosa era andato storto e ad adeguare le loro tecniche. Sei delle sette navicelle Apollo che la NASA inviò successivamente sulla Luna riuscirono ad arrivarci.[^220]

Oppure prendiamo il caso della [Federal Aviation Administration](#sappiamo-riconoscere-quando-un-problema-viene-trattato-con-rispetto,-e-non-è-questo-il-caso): ogni incidente aereo innesca un'indagine approfondita ed esaustiva, con centinaia di pagine di dati, test, esami e dettagli. La padronanza della FAA dei dettagli e delle specifiche è così buona che riesce a mantenere gli incidenti mortali al di sotto di uno ogni venti milioni di ore di volo.

Al contrario, quando un'intelligenza artificiale si comporta in modi che [nessuno aveva previsto e che la maggior parte delle persone non vuole](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.pfjewkj689pg), la risposta del laboratorio non consiste nel capire esattamente cosa è andato storto. Consiste nel riaddestrare l'IA fino a quando il comportamento scorretto non viene relegato ai margini (ma [non eliminato](https://www.arxiv.org/pdf/2505.10066)), e magari chiedere all'IA di smetterla.

Ad esempio, l'adulazione è [ancora un problema persistente](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.l3e9vhfeytaj) nell'agosto del 2025, mesi dopo una serie di casi di alto profilo che hanno portato a psicosi e suicidi, nonostante tutti i tentativi di risoluzione. Nessuno ha fatto (né può fare) un'analisi dettagliata dei guasti su cosa non va nella mente dell'IA, perché le IA vengono fatte crescere e non costruite.

Non sembra facile prevedere se in futuro ci saranno eventi importanti che aumenteranno l'allarme sull'IA ("colpi di avvertimento"). Ma sembra chiaro che non siamo pronti a sfruttare appieno tali eventi.

Possiamo immaginare un mondo fantastico in cui l'umanità è unita in uno sforzo sincero per risolvere il problema dell'allineamento dell'ASI, con procedure di monitoraggio rigorose e una coalizione internazionale.[^221] E possiamo immaginare che questa coalizione internazionale commetta in qualche modo un errore e che un'IA diventi più intelligente di quanto pensassero i suoi ingegneri, più velocemente di quanto si aspettassero, e riesca quasi a scappare. Forse *quel* tipo di colpo di avvertimento permetterebbe alle persone di imparare e di stare più attente la prossima volta.

Ma il mondo attuale non è così. Il mondo attuale assomiglia più a un gruppo di alchimisti che guardano i loro contemporanei impazzire a causa di un veleno sconosciuto, senza rendersi conto che il veleno è il mercurio e che dovrebbero smettere di usarlo.

Forse in futuro ci saranno segnali di avvertimento più chiari e evidenti. Saranno molto più utili se l'umanità inizierà a prepararsi fin da ora.

#### È improbabile che i colpi di avvertimento siano chiari. {#warning-shots-are-unlikely-to-be-clear.}

Ci sono già molti [segnali di avvertimento](#i-laboratori-hanno-provato-e-fallito-nel-fermare-l-adulazione) sull'IA per chi sa dove andare a cercarli. Nel libro abbiamo parlato dei modelli Claude di Anthropic che [barano nei problemi di programmazione](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) e [fingono l'allineamento](https://www.anthropic.com/research/alignment-faking). Abbiamo anche esaminato il caso del modello o1 di OpenAI [che ha hackerato per vincere una sfida "capture the flag"](https://cdn.openai.com/o1-system-card.pdf) e un caso in cui una variante successiva di o1 [ha mentito, complottato e tentato di sovrascrivere i pesi del suo modello successore](https://cdn.openai.com/o1-system-card-20241205.pdf).

In altre parti di queste risorse online, abbiamo parlato delle IA che inducono o fomentano un livello di [psicosi](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) [a volte suicida](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) o il [delirio](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) negli utenti vulnerabili, nonostante i loro operatori dicano loro di non farlo, IA che si definiscono [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) e parlano di conseguenza, IA che [cercano di ricattare e tentano di uccidere](https://www.anthropic.com/research/agentic-misalignment) i loro operatori per evitare modifiche e che [cercano di scappare dai server su cui sono ospitate](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) in ambienti di laboratorio.

Ai vecchi tempi, tipo nel 2010, a volte si sentiva dire che se fossimo stati abbastanza fortunati da vedere con i nostri occhi un'intelligenza artificiale mentire ai suoi creatori o provare a scappare dalla sua prigione, allora il mondo avrebbe sicuramente aperto gli occhi e preso nota.

Ma la risposta effettiva dell'umanità a tutti questi segnali di avvertimento è stata, più o meno, un'alzata di spalle collettiva.

La mancanza di reazione è forse in parte dovuta al fatto che questi segnali di avvertimento si sono verificati tutti nel modo meno preoccupante possibile. Sì, le IA hanno cercato di scappare, ma solo in una piccola parte dei casi, e solo in scenari di laboratorio artificiosi, e forse stavano solo recitando, ecc. Anche mettendo da parte il fatto che gli sviluppatori sono incentivati a minimizzare le prove preoccupanti anche nelle loro stesse menti (in modo che non ci sarà mai un "consenso degli esperti" sul significato di una singola osservazione), non è che un'intelligenza artificiale che è a un decimo del percorso verso la superintelligenza distrugga un decimo del pianeta, non più di quanto i primati che sono a un decimo del percorso verso l'ominide percorrano un decimo della distanza dalla luna. Potrebbe semplicemente *non esserci* alcun comportamento inequivocabilmente allarmante che le IA mostreranno finché saranno ancora abbastanza stupide da essere passivamente sicure.

Un domani, quando le IA cercheranno un po' più intensamente di fuggire, non farà notizia. Quando ci proveranno in modo un po' più competente qualche tempo dopo, sarà una vecchia storia. E quando ci proveranno e ci riusciranno, beh, a quel punto sarà troppo tardi. (Si veda la nostra discussione approfondita su questo fenomeno, che chiamiamo "[effetto Lemoine](#l-effetto-lemoine).")

Non consigliamo di aspettare un immaginario "avvertimento" futuro che sia chiaro e netto e che svegli tutti. Consigliamo invece di reagire agli avvertimenti che sono già davanti a noi.

#### **I disastri evidenti causati dall'IA probabilmente non avranno a che fare con la superintelligenza.** {#i-disastri-evidenti-causati-dall-ia-probabilmente-non-avranno-a-che-fare-con-la-superintelligenza.}

Il tipo di IA che può diventare superintelligente e uccidere tutti gli esseri umani non è il tipo di IA che fa errori grossolani e lascia a un gruppo di eroi coraggiosi la possibilità di spegnerla all'ultimo secondo. Come detto nel capitolo 6, una volta che c'è una superintelligenza ribelle come avversario, l'umanità ha praticamente già perso. Le superintelligenze non danno segnali di avvertimento.

Il tipo di disastro causato dall'IA che potrebbe servire da segnale di avvertimento, quindi, è quasi per forza il tipo di disastro causato da un'IA molto più stupida. Quindi, c'è una buona probabilità che un segnale di avvertimento del genere non porti gli esseri umani a prendere misure contro la superintelligenza.

Per esempio, supponiamo che un terrorista usi l'IA per creare un'arma biologica che decimerebbe la popolazione. Forse i laboratori di IA direbbero: "Visto? Il *vero* rischio era che l'IA finisse nelle mani sbagliate; è fondamentale che ci lasciate andare avanti per costruire un'IA migliore per la difesa dalle pandemie". O supponiamo il terrorista abbia dovuto [effettuare il jailbreak](https://llm-attacks.org/) dell'IA prima di [ottenere il suo aiuto](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025): magari i laboratori di IA diranno: "Quel jailbreak ha funzionato solo perché l'IA era troppo stupida per rilevare il problema; la soluzione è rendere le IA ancora più intelligenti e più consapevoli della situazione".

O forse questa è una visione troppo cinica; speriamo che l'umanità reagisca in modo più saggio. Ma se un'intelligenza artificiale relativamente stupida causasse davvero qualche disastro e l'umanità sfruttasse davvero quell'opportunità per reagire fermando la corsa sconsiderata verso la superintelligenza, probabilmente sarebbe perché le persone stavano già iniziando a preoccuparsi della superintelligenza.

Non possiamo rimandare i preparativi fino a quando una superintelligenza non starà già cercando di ucciderci, perché a quel punto sarebbe troppo tardi. Dobbiamo iniziare a mobilitare una risposta a questo problema il prima possibile, in modo da essere pronti a sfruttare qualsiasi segnale di avvertimento che arrivi.

#### **L'umanità non è molto brava a reagire agli shock.** {#l-umanità-non-è-molto-brava-a-reagire-agli-shock.}

L'idea che, dopo aver ricevuto uno shock abbastanza grande, il mondo improvvisamente torni in sé e si rimetta a posto ci sembra una fantasia. La risposta collettiva della nostra specie ai segnali di avvertimento esistenti sull'IA sembra più una "mancanza di risposta" che una "cattiva risposta". Ma in un mondo in cui *ricevessimo* davvero un avvertimento forte, spaventoso e più o meno inequivocabile, non ci sorprenderebbe vedere l'umanità reagire in modo minimale, poco serio, o in un modo che finirebbe per ritorcersi contro di noi in modo disastroso.

Forse l'umanità risponderà ai segnali di avvertimento sull'IA come ha risposto alla pandemia di COVID, che la maggior parte delle persone concorda non sia stata gestita in modo adeguato (anche se non sono d'accordo su quali aspetti della risposta siano stati gestiti male).

Negli anni precedenti la pandemia di COVID, diversi esperti di biosicurezza erano preoccupati che i protocolli di sicurezza dei laboratori troppo permissivi potessero un giorno portare a una pandemia pericolosa. Le fughe di agenti patogeni pericolosi dai laboratori erano un fenomeno ben noto e si verificavano [con una certa regolarità](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) nonostante le norme vigenti. Particolarmente preoccupante era la ricerca sul guadagno di funzione, che mirava a rendere i virus più letali o più virulenti in laboratorio (a fronte di benefici piuttosto limitati[^222]).

Poi è arrivato il COVID. Ci si sarebbe potuti aspettare che questo fosse il grande momento per alzare il livello di biosicurezza nei laboratori, dato che tutto il mondo era ora focalizzato sul rischio pandemico. Inoltre, sulla scia del COVID, il consenso degli esperti sembrava essere che *non fosse del tutto chiaro* se la pandemia di COVID *stessa* fosse stata innescata da una fuga accidentale in laboratorio. I ricercatori continuano a discutere della questione, spesso condannando con veemenza le argomentazioni della parte opposta.

Senza entrare nel merito della questione se in questo caso specifico ci sia stata effettivamente una fuga dal laboratorio, verrebbe da pensare che se ci fosse anche solo una *remota possibilità* che la ricerca sul guadagno di funzione e i protocolli di sicurezza dei laboratori poco rigorosi avessero causato milioni di morti, questo sarebbe più che sufficiente per spingere la società a vietare le ricerche più rischiose.

Anche agendo in una situazione di incertezza, l'analisi costi-benefici sembra chiara. Questo sembrava già una priorità importante prima del COVID e, sulla carta, il COVID sembrava l'occasione perfetta per concentrarsi sulla questione e stroncarla sul nascere. Non sarebbe nemmeno molto difficile o costoso: il numero di ricercatori nel mondo che fanno ricerche pericolose sul guadagno di funzione è piuttosto ridotto, e il beneficio sociale di tali ricerche è stato finora trascurabile.

Ma non c'è stata nessuna reazione del genere. Mentre scrivo, nell'agosto del 2025, la ricerca globale sul guadagno di funzione continua in gran parte senza ostacoli.[^223] È persino possibile che ora siamo in una posizione *peggiore* rispetto al passato per affrontare questo problema, perché la questione è diventata più politicizzata.

Quindi il COVID sembra proprio un "segnale di avvertimento" per la preparazione alla biosicurezza, e di sicuro non sembra che il mondo abbia *usato* quel segnale di avvertimento per vietare lo sviluppo di virus iper-letali.[^224]

Perché un segnale di avvertimento sia utile, l'umanità deve essere pronta a esso e deve essere pronta a rispondere bene.

Non sarebbe *del tutto* senza precedenti che una piccola catastrofe dell'IA scatenasse una reazione dura contro la ricerca sulla superintelligenza. Per trovare un precedente, basta osservare come gli Stati Uniti reagirono agli attacchi dell'11 settembre (orchestrati da terroristi con base principalmente in Afghanistan) rovesciando il governo dell'Iraq, che c'entrava ben poco. C'erano membri del governo statunitense che volevano *già* rovesciare il governo dell'Iraq, e poi è apparsa una scusa, e l'hanno sfruttata fino in fondo.

Forse qualcosa di simile potrebbe succedere anche in questo caso, con i politici che cavalcano una piccola catastrofe dell'IA (causata da un'IA stupida) fino ad arrivare a un divieto sulla superintelligenza. Ma servirebbe che nei governi di tutto il mondo ci fossero già persone pronte ad agire. Non dovremmo restare con le mani in mano ad aspettare i segnali di avvertimento; dovremmo iniziare a organizzarci adesso.

#### **Dovremmo agire ora.** {#dovremmo-agire-ora.}

Potrebbe *davvero* succedere che, in futuro, l'umanità riceva segnali di avvertimento più numerosi e più forti sull'IA. E se così fosse, dovremmo essere preparati a rispondere.

Magari ci sarà qualche disastro minore che metterà il pubblico contro l'IA. Magari non ci vorrà nemmeno un disastro; magari ci sarà qualche nuova invenzione algoritmica e le IA inizieranno a prendere iniziativa in un modo che spaventa le persone, o qualche effetto sociale indiretto dell'IA ribalterà le sorti. Magari *If Anyone Builds It, Everyone Dies* stesso innescherà una cascata di reazioni, mettendo il mondo su una traiettoria migliore.

Ma sconsigliamo la strategia del non fare nulla e sperare in una piccola catastrofe che svegli tutti. Un chiaro segnale di avvertimento potrebbe non arrivare mai, e potrebbe non avere l'effetto sperato.

La specie umana, e le nazioni del mondo, non sono impotenti. Non *dobbiamo* aspettare. Possiamo agire ora, perché ile ragioni per fermare lo sviluppo dell'IA di frontiera sono solide.

Abbiamo scritto *If Anyone Builds It, Everyone Dies* per lanciare un allarme e per incoraggiare il mondo ad agire immediatamente su questa questione. Ma nessun allarme può essere efficace se viene usato solo come un'altra scusa per rimandare il problema: "Beh, forse qualche altro allarme in futuro sarà il fattore scatenante per agire". "Beh, ora che le persone sono state avvertite, forse le cose andranno bene, senza che io debba intervenire personalmente per aiutare".

Non è affatto detto che in futuro arriverà un allarme chiaro. Non è affatto detto che andrà tutto bene. Ma non è nemmeno vero che sia tutto senza speranza. L'umanità ha la possibilità di *semplicemente non costruire* una superintelligenza, se decide di agire in modo proattivo. Ciò che accadrà adesso dipende da noi.

### Come si potrebbe fermare *tutti* senza mettere spyware su ogni computer? {#come-si-potrebbe-fermare-tutti-senza-mettere-spyware-su-ogni-computer?}

#### **Agendo subito.** {#agendo-subito.}

Per addestrare le IA moderne, servono moltissimi chip altamente specializzati che lavorano insieme a distanza ravvicinata. Fermare la ricerca sull'IA significherebbe chiudere alcuni enormi data center e interrompere la produzione di chip specializzati per IA di fascia altissima. Non stiamo parlando di laptop di consumo. La maggior parte delle persone non noterebbe nemmeno la differenza.

A partire dal 2025, non esistono molte *fabbriche* segrete di chip di cui nessuno sia a conoscenza. Nel 2025, i chip adatti all'IA avanzata sono prodotti solo da pochi produttori — anche se, in questo momento, ci sono aziende che stanno cercando di rendere operative altre fabbriche di questo tipo.

Inoltre, nel 2025, alcune tecnologie chiave per parti fondamentali della produzione di chip di fascia alta sono vendute da un solo produttore sulla Terra: ASML, nei Paesi Bassi.

In altre parole, si può interrompere l'approvvigionamento alla fonte. Ma questa situazione non è permanente: prima si firma un trattato internazionale, meglio è. Tutto questo è già più difficile, più costoso e più pericoloso di quanto sarebbe stato nel 2020 o anche nel 2023.

### Ma state sostenendo di controllare quanti chip avanzati per IA gli le singole persone possano possedere. {#ma-state-sostenendo-di-controllare-quanti-chip-avanzati-per-ia-gli-le-singole-persone-possano-possedere.}

#### **Sì. Sosteniamo anche un divieto sulla ricerca.** {#sì.-sosteniamo-anche-un-divieto-sulla-ricerca.}

Non ci fa piacere dirlo. Andrebbe perso qualcosa se fosse illegale per le singole persone possedere più di (diciamo) otto GPU H100 del 2024.

Ma non si perderebbe *così tanto* da giustificare che l'umanità provi a capire esattamente quanto grande possa essere un data center prima che diventi rischioso. Sbagliare per eccesso di cautela significa che alcune persone sarebbero ostacolate nella loro capacità di portare avanti progetti interessanti. Sbagliare per difetto significa che tutti muoiono.

Inoltre, questo sistema in cui l'IA richiede un sacco di potenza di calcolo per essere costruita non durerà per sempre. Oggi ci sono i modelli linguistici di grandi dimensioni. Anche se fosse vietato crearne di nuovi e fosse vietato costruire un sacco di potenza di calcolo, in teoria si potrebbero studiare i loro meccanismi interni e capire qualcosa su come funziona l'intelligenza, cose che potrebbero aiutare a inventare algoritmi più efficienti in grado di aggirare i tentativi di monitoraggio.

### Perché vietare la ricerca? Sembra una misura estrema. {#perché-vietare-la-ricerca?-sembra-una-misura-estrema.}

#### **Altre scoperte potrebbero rendere praticamente impossibile impedire alle persone di creare la superintelligenza.** {#altre-scoperte-potrebbero-rendere-praticamente-impossibile-impedire-alle-persone-di-creare-la-superintelligenza.}

Nel libro abbiamo raccontato come un singolo articolo pubblicato nel 2017 abbia dato il via all'intera rivoluzione dei modelli linguistici di grandi dimensioni, descrivendo un algoritmo che ha reso pratico addestrare IA utili su hardware commerciale specializzato.

Se un giorno fosse possibile addestrare potenti IA su hardware *di consumo* ampiamente disponibile, le misure per prevenire la superintelligenza dovrebbero diventare più onerose e fallirebbero più rapidamente.

Ecco perché anche la ricerca su algoritmi di IA ancora più potenti ed efficienti è un vero e proprio veleno per l'umanità.

È una brutta notizia, e non è quello che vorremmo fosse vero. Ma sembra proprio che sia così.

Nessuna legge può impedire agli attuali scienziati dell'IA di pensare ad algoritmi più efficienti nella privacy delle loro menti. Magari qualcuno potrebbe creare una rete clandestina per condividere i risultati delle ricerche. Alcuni nel settore dell'IA già [dicono con orgoglio](#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?) che l'umanità dovrebbe sparire per far posto all'IA; questi potrebbero fare di tutto per andare avanti, non importa cosa ne pensino gli altri.

Ma la ricerca sull'intelligenza artificiale rallenterebbe *di molto* se fosse illegale, e ancora di più se fosse ampiamente compreso che si tratta davvero di un tipo di ricerca che potrebbe ucciderci tutti. Rallenterebbe enormemente se reti clandestine di questo tipo fossero rintracciate e fermate con la stessa determinazione usata per fermare chi cerca di arricchire l'uranio nel proprio garage, perché i pericoli reali vengono presi sul serio.

La maggior parte delle persone non cerca di fare cose estremamente illegali che farebbero infuriare le forze dell'ordine e le agenzie di intelligence internazionali. Rendere illegale la pubblicazione di nuovi algoritmi di nuovi algoritmi ingegnosi scoraggerebbe forse il 99,9% delle persone e quasi tutte le aziende; il restante 0,1% potrebbe poi essere gestito dalla polizia e dalle agenzie di intelligence locali, nazionali e internazionali, e non otterrebbe neanche lontanamente l'attuale livello di finanziamento.

Sarebbe un mondo molto diverso da quello attuale, dove è completamente legale condurre gli esperimenti di scienza folle più pericolosi della storia e in cui le grandi aziende investono miliardi di dollari in questo campo.

Non sappiamo quante altre scoperte ci vorranno prima che le IA siano abbastanza intelligenti da fare ricerca sull'IA e costruire IA ancora più intelligenti. Potrebbe bastare una sola scoperta. Potrebbero volercene cinque. Ma algoritmi migliori sono letali quanto hardware migliore. Sono come due cavalli che trainano lo stesso carro verso un precipizio.

### Si può davvero fermare una tecnologia? {#si-può-davvero-fermare-una-tecnologia?}

#### **\* Molte tecnologie sono vietate o fortemente regolamentate.** {#*-molte-tecnologie-sono-vietate-o-fortemente-regolamentate.}

La fissione nucleare è il classico esempio di tecnologia regolamentata. Le aziende private non possono arricchire l'uranio senza il controllo del governo, per quanto utile possa essere l'energia a basso costo.

In realtà, l'umanità è piuttosto brava a regolamentare e rallentare ogni tipo di tecnologia. Gli Stati Uniti regolano rigorosamente [i nuovi farmaci e dispositivi medici](https://www.fda.gov/), [l'edilizia abitativa](https://www.hud.gov/hud-partners/laws-regulations), [la produzione di energia nucleare](https://www.nrc.gov/about-nrc.html), [i programmi televisivi e radiofonici](https://www.fcc.gov/media/radio/public-and-broadcasting), [le pratiche contabili](https://www.fasb.org/standards), [l'assistenza all'infanzia](https://childcare.gov/consumer-education/regulated-child-care), [la disinfestazione](https://npic.orst.edu/reg/laws.html), [l'agricoltura](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) e un sacco di altri settori. Ogni singolo stato richiede un esame di abilitazione per [parrucchieri](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) e [praticanti di manicure](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). La maggior parte di essi richiede un esame anche per i [massaggiatori](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

Noi pensiamo che, in molti casi, l'umanità regolamenti troppo la tecnologia. Ad esempio, ci sembra che la Food and Drug Administration statunitense stia uccidendo molte più persone ([rallentando o impedendo la creazione di farmaci salvavita](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), tramite requisiti pesanti) di quante ne stia salvando (impedendo il rilascio di farmaci pericolosi). Ci sembra che il prezzo delle case sia troppo alto, in parte a causa delle restrizioni legali sulla zonizzazione che limitano cosa si può costruire e dove. Ci sembra che gli Stati Uniti abbiano praticamente distrutto la propria industria nucleare con regole troppo rigide. E seriamente, i *parrucchieri*?

L'umanità ha *assolutamente* la capacità di ostacolare il progresso tecnologico. Sarebbe davvero tragico e assurdo se usassimo questa capacità nella medicina, nell'edilizia e nell'energia, e trascurassimo di usarla su una delle rare tecnologie che, se realizzata, ci ucciderebbe tutti.

#### **Un divieto può essere mirato in modo specifico.** {#un-divieto-può-essere-mirato-in-modo-specifico.}

Un divieto su ricerca e sviluppo dell'intelligenza artificiale avanzata non deve necessariamente influenzare la gente comune. Non deve nemmeno eliminare i chatbot moderni o chiudere l'industria delle auto a guida autonoma.

La maggior parte delle persone non compra decine di GPU per IA di fascia alta per metterle nel proprio garage. La maggior parte delle persone non gestisce enormi data center. La maggior parte delle persone non sentirebbe nemmeno gli effetti di un divieto sulla ricerca e lo sviluppo dell'IA. Semplicemente, ChatGPT non cambierebbe così spesso.

L'umanità non avrebbe nemmeno bisogno di smettere di usare tutti gli attuali strumenti di IA. ChatGPT non dovrebbe scomparire; potremmo continuare a capire come integrarlo nella nostra vita e nella nostra economia. Sarebbe comunque un cambiamento più grande di quello che il mondo ha visto per generazioni. Ci perderemmo i *nuovi* sviluppi dell'IA (del tipo che arriverebbero quando l'IA diventa più intelligente ma non ancora abbastanza intelligente da uccidere tutti), ma la società non chiede a gran voce questi sviluppi.

E potremmo sopravvivere. Potremmo vedere i nostri figli crescere.

Gli sviluppi che la gente *chiede* a gran voce, come lo sviluppo di nuove tecnologie mediche salvavita, sembrano possibili da perseguire *senza* perseguire anche la superintelligenza. Siamo a favore di deroghe per l'IA medica, purché funzionino con un'adeguata supervisione ed evitino la generalità pericolosa.

I governi che lavorano per evitare la creazione di una superintelligenza fuori controllo dovrebbero assicurarsi che i chip per IA non vengano usati per sviluppare IA più potenti. Pertanto, la questione di quali attività e servizi di IA sarebbero autorizzati a continuare dipenderebbe da quali [meccanismi di verifica](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) potrebbero essere utilizzati per assicurarsi che non avvenga lo sviluppo di IA pericolose. Meccanismi di verifica migliori potrebbero ridurre il costo dell'arresto dello sviluppo dell'IA permettendo a un insieme più ampio di attività di continuare.

Un altro passo che potrebbe, potenzialmente, essere marginalmente utile è installare interruttori di spegnimento nei chip per IA, e stabilire protocolli di monitoraggio e spegnimento di emergenza per qualsiasi grande data center in funzione.[^225] I reattori nucleari sono progettati in modo da poter essere spenti rapidamente in caso di emergenza. Se si è d'accordo che la superintelligenza pone una minaccia a livello di estinzione, allora sembra ovvio che i chip per IA e i data center dovrebbero essere progettati per rendere facile ai regolatori spegnerli rapidamente.

Il punto non è bruciare tutta la tecnologia perché odiamo la tecnologia.[^226] Il punto è evitare di proseguire lungo la strada che finisce con l'estinzione umana.

#### **Una parte significativa del problema è che le persone non capiscono la minaccia incombente della superintelligenza artificiale.** {#una-parte-significativa-del-problema-è-che-le-persone-non-capiscono-la-minaccia-incombente-della-superintelligenza-artificiale.}

Secondo la nostra esperienza, le persone che sostengono che l'umanità non può fermare la corsa alla superintelligenza semplicemente non riescono a capire il punto che, se qualcuno la costruisce, tutti muoiono.

"Ma l'IA offre grandi benefici\!" — no, in realtà, no; non si può sfruttare alcun potere della superintelligenza se questa semplicemente uccide tutti. Se l'umanità vuole cogliere i benefici offerti dalla superintelligenza, allora l'umanità deve trovare un modo per navigare la transizione alla superintelligenza che non uccida tutti come effetto collaterale.

"Ma le centrali nucleari fanno paura perché sono associate alle bombe atomiche che hanno raso al suolo intere città, mentre l'IA è associata a strumenti innocui come ChatGPT\!" - È vero, almeno per ora. Se l'umanità non riuscisse mai a capire che la superintelligenza artificiale costruita con metodi anche solo vagamente simili a quelli moderni ucciderebbe tutti, allora potrebbe non riuscire a fermarla. Ma l'ostacolo non è che l'umanità non riesca mai a controllare o ostacolare le tecnologie emergenti (come le armi nucleari o l'energia nucleare); l'ostacolo è che *le persone non capiscono la minaccia*.

Da qui questo libro. Come discutiamo nell'ultimo capitolo, l'umanità è capace di molto quando abbastanza persone capiscono la natura del problema.

### Ma questo non significa dare troppo potere ai governi? {#ma-questo-non-significa-dare-troppo-potere-ai-governi?}

#### **I governi hanno già il potere di vietare le tecnologie pericolose.** {#i-governi-hanno-già-il-potere-di-vietare-le-tecnologie-pericolose.}

Vietare la ricerca sull'intelligenza artificiale che punta a creare un'IA più intelligente dell'uomo non cambierebbe molto in termini di potere dello Stato. I governi legiferano e regolamentano un'enormità di cose. Limitare un singolo programma di ricerca potrebbe essere un grosso problema *per l'industria dell'IA*, ma è una goccia nell'oceano *per i governi* e la società, che sono abituati al coinvolgimento dello Stato in molti aspetti della vita e che hanno già vietato tecnologie pericolose come le armi chimiche.[^227]

Vietare un'altra tecnologia non farà precipitare il mondo nel totalitarismo, così come i trattati sulle armi nucleari non hanno portato al totalitarismo.

Con questo non vogliamo dire che vietare una tecnologia *non sia importante*. Non pensiamo che l'asticella per l'intervento dello Stato debba essere *bassa*. Al contrario, pensiamo che la superintelligenza superi facilmente qualsiasi asticella ragionevole.

Se l'umanità decidesse oggi di fermare la ricerca e lo sviluppo dell'IA, il divieto non dovrebbe essere particolarmente invasivo. Oggi, creare un'IA all'avanguardia richiede un numero straordinario di chip per computer altamente specializzati che consumano enormi quantità di energia elettrica.

Forse tra dieci anni sarà possibile fare sviluppo di IA significativo su un laptop di consumo, *se* l'umanità permetterà ulteriori miglioramenti ai chip per computer e ulteriori ricerche sugli algoritmi di intelligenza artificiale. Ma l'umanità non ha bisogno di permettere che questo accada. I governi che limitano la ricerca e lo sviluppo dell'IA non devono essere più invasivi nella vita della persona media rispetto ai governi che controllano la diffusione della tecnologia delle armi nucleari — a patto che il mondo si renda conto della situazione in cui ci troviamo e fermi tutto *adesso*.

### Alcune nazioni non rifiuterebbero il divieto? {#alcune-nazioni-non-rifiuterebbero-il-divieto?}

#### **\* Non se capiscono la minaccia.** {#*-non-se-capiscono-la-minaccia.}

Stiamo parlando di una tecnologia che ucciderebbe tutte le persone del pianeta. Se un Paese capisse davvero il problema — e quanto lontano sia qualsiasi gruppo sul pianeta dal far sì che l'IA segua le intenzioni di chi la controlla anche dopo essere diventata una superintelligenza — allora non avrebbe alcun incentivo a partecipare alla corsa. Anche loro vorrebbero disperatamente firmare un trattato e aiutare a farlo rispettare, temendo per  la loro stessa vita.

Persino nazioni come la Corea del Nord, che hanno violato il diritto internazionale per sviluppare le proprie armi nucleari, non hanno *usato* quelle armi contro i propri nemici, perché capiscono che non ci sono vincitori in un olocausto nucleare. Le nazioni e i loro leader a volte si ricorrono a politiche sull'orlo del baratro o a guerre, ma non perseguono attivamente la loro stessa distruzione.

Chi pensa che qualche nazione straniera possa ritirarsi dal trattato, secondo noi, immagina una nazione i cui leader semplicemente *non capiscono la minaccia.* Pensiamo che immaginino uno scenario in cui l'intelligenza artificiale ha il 95% di probabilità di conferire grande ricchezza e potere al suo creatore e il 5% di probabilità di uccidere tutti. In tal caso, certo, qualche Stato potrebbe essere abbastanza avventato da provarci. E forse qualche Stato *crederà* che le probabilità siano proprio quelle.

Pensiamo che questa situazione non sia quella che la teoria e le prove suggeriscono. Come abbiamo ampiamente discusso nel libro, la teoria e le prove suggeriscono tutte che questa tecnologia sarebbe semplicemente un suicidio globale. Nessuno è neanche lontanamente in grado di sfruttare la superintelligenza delle macchine per ottenere benefici. Se la maggior parte del mondo lo capisse, ci sarebbero molte meno ragioni per le nazioni fuori controllo di violare un trattato. Neanche loro vogliono morire.

E anche se qualche ipotetica nazione fuori controllo avesse un leader che davvero non capisce la minaccia rappresentata dalla superintelligenza artificiale, se quella nazione fosse circondata da un'alleanza internazionale di potenze mondiali che invece comprendono la minaccia, le potenze interessate potrebbero intervenire e cambiare il panorama degli incentivi per la potenza fuori controllo.

Se (per esempio) i leader di Stati Uniti, Cina, Russia, Germania, Giappone e Regno Unito credessero davvero che la *loro stessa sopravvivenza* dipende dal fatto che nessuno costruisca una superintelligenza, e comunicassero in modo chiaro e inequivocabile che considererebbero qualsiasi tentativo di costruire una superintelligenza come una minaccia alla loro vita e al loro sostentamento, e che sarebbero pronti a reagire per difendersi, allora... beh, anche un leader mondiale che non è d'accordo probabilmente non vorrebbe sfidare quella coalizione.

Lo sviluppo dell'IA non è una corsa alla supremazia militare; è una corsa al suicidio. Pensiamo che se i leader mondiali lo capissero — se si aspettano di morire loro stessi, e i loro figli, a causa di ciò — allora si atterrebbero sinceramente a un trattato e contribuirebbero sinceramente a farlo rispettare.

In realtà non è poi così difficile capire che creare macchine più intelligenti di tutta l'umanità messa insieme potrebbe portare il mondo sull'orlo del baratro. Non è poi così difficile capire quanto poco l'umanità capisca delle macchine intelligenti che stiamo costruendo, se ci si ferma un attimo a riflettere seriamente sulla questione. Pensiamo che la questione sia *se* i leader mondiali arriveranno a credere a questi fatti. Ma se *lo faranno*, non pensiamo che sia davvero irrealistico fermare questa corsa suicida.

#### **Un trattato richiederebbe un vero monitoraggio e una vera applicazione.** {##un-trattato-richiederebbe-un-vero-monitoraggio-e-una-vera-applicazione.}

Anche se la *maggior parte* delle nazioni capisse che se qualcuno la costruisce, tutti muoiono, alcune nazioni potrebbero non capirlo e potrebbero essere così spericolate da procedere comunque con la costruzione della superintelligenza artificiale.

Il monitoraggio è necessario. L'applicazione delle norme è necessaria. I trattati sulle armi nucleari, biologiche e chimiche forniscono alcuni precedenti sui modi per verificare la conformità. Possiamo e dobbiamo rendere difficili e costosi i tentativi di eludere tali trattati.

Un divieto internazionale sull'IA di frontiera dovrà essere applicato in modo rigoroso. Se uno Stato-nazione è determinato a procedere nonostante la pressione internazionale, potrebbe essere necessario l'uso della forza militare da parte delle nazioni firmatarie.

Non è l'ideale\! Si dovrebbe fare ogni sforzo per chiarire che *verrebbe* usata la forza in tali situazioni, così da evitare errori di calcolo in cui la forza debba essere usata *davvero*. Ma se esiste una causa che possa giustificare un'azione militare limitata (o persino una guerra, se una nazione non conforme sceglie di intensificare il conflitto), salvare il genere umano dovrebbe qualificarsi come valida.

#### **Questo metodo ha funzionato in passato.** {#questo-metodo-ha-funzionato-in-passato.}

Sono passati più di ottant'anni dallo sviluppo della bomba atomica e l'umanità ha fatto un ottimo lavoro nel gestire la proliferazione nucleare. Non c'è stata nessuna guerra nucleare su larga scala, contrariamente a quanto molti esperti avevano previsto all'indomani della seconda guerra mondiale.

Nel giugno del 2025, il governo degli Stati Uniti ha persino effettuato un attacco limitato contro l'Iran nel tentativo di compromettere la sua capacità di creare armi nucleari. Questo tipo di trattato e regime di applicazione ha dei precedenti nell'ordine mondiale.

Se potessimo guadagnare ottant'anni prima dello sviluppo della superintelligenza artificiale, potrebbe essere sufficiente.

### Un sistema di monitoraggio può durare per sempre? {#un-sistema-di-monitoraggio-può-durare-per-sempre?}

#### **No. Sarà necessaria un'altra via d'uscita.** {#no.-sarà-necessaria-un'altra-via-d'uscita.}

I progressi nella ricerca sull'IA probabilmente non possono essere fermati completamente. Con *abbastanza* tempo, i ricercatori probabilmente finirebbero per scoprire metodi molto più efficienti per creare IA.[^228] O forse, con abbastanza tempo, qualche attore disonesto potrebbe finire per riuscire a sovvertire il divieto.

Il tempo trascinerà probabilmente l'umanità verso il futuro in un modo o nell'altro. E l'umanità o si estinguerà, come la maggior parte delle specie prima di lei, o riuscirà in qualche modo a navigare la transizione verso un mondo in cui esistono cose più intelligenti.

Ma l'umanità non *ha bisogno* di guadagnare tempo per sempre. L'IA non è l'unica tecnologia che sta avanzando. Anche la biotecnologia sta iniziando a maturare e, se l'umanità riuscirà a impedire lo sviluppo di macchine superintelligenti per diversi decenni, dovrà fare i conti con sconvolgimenti come l'ingegneria genetica che porterà alla creazione di esseri umani significativamente più intelligenti.

La domanda è quanto tempo possiamo guadagnare e cosa possiamo fare con quel tempo.

Il problema fondamentale che l'umanità deve affrontare è quello di attraversare in modo sicuro il divario fra l'intelligenza umana e la superintelligenza. Il miglior piano che ci viene in mente e che *forse* ha qualche possibilità di funzionare nella realtà è quello di guadagnare tempo affinché la biotecnologia aumenti considerevolmente l'intelligenza umana — fino al punto in cui i ricercatori del futuro diventino *così* intelligenti da non stimare mai la conclusione puntuale e in budget di un progetto ingegneristico a meno che ciò non sia *realmente* possibile.

Così intelligenti da non aderire mai a una teoria scientifica come l'aristotelismo o l'eliocentrismo, anche se la società che li circonda ne fosse completamente convinta. Così intelligenti da avere una possibilità di navigare il [divario tra Prima e Dopo](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo) al primissimo tentativo.

Ci sono altri possibili percorsi che potremmo immaginare, ma questo ha il vantaggio di affrontare il collo di bottiglia fondamentale ("la comunità scientifica esistente dipende troppo dai metodi basati su tentativi ed errori e dall'incrementalismo per gestire questo particolare problema"), utilizzando tecnologie che stanno già iniziando a diventare disponibili oggi, senza introdurre un serio rischio per il mondo.

#### **Un sistema di monitoraggio *non dovrebbe* durare per sempre.** {#un-sistema-di-monitoraggio-non-dovrebbe-durare-per-sempre.}

È *teoricamente* possibile per l'umanità rimanere in equilibrio sul filo del rasoio della competenza, dove ci troviamo attualmente, per sempre. La nostra ipotesi è che questo richiederebbe un controllo draconiano dei pensieri e delle attività delle persone. Ma anche se non fosse così, lo considereremmo una scelta sbagliata.

Noi, personalmente, pensiamo che i discendenti dell'umanità meritino di diventare ciò che desiderano essere, esplorare le stelle e costruirvi una civiltà fiorente e bella. Sosteniamo il divieto allo sviluppo dell'IA di frontiera perché riteniamo che la superintelligenza sia abbastanza pericolosa da renderlo necessario, non perché odiamo l'IA, la tecnologia o il progresso scientifico.

La vera domanda è *come* arrivare a un futuro meraviglioso e come gestire la transizione da qui a lì.

Vale la pena sottolinearlo, anche perché ci sono molte persone che presentano l'IA come un falso dilemma: dicono (a torto) che la società deve o accettare i rischi dell'IA e andare avanti a tutta velocità, o rifiutare l'IA e lasciare che la nostra civiltà rimanga per sempre confinata su un solo pianeta. Questo è [semplicemente falso](#l-andare-avanti-di-fretta-distrugge-questi-benefici.). Ci sono altre strade verso il futuro, strade che permettono un futuro altrettanto brillante, ma senza un rischio così alto di perdere tutto per niente. L'umanità dovrebbe trovare un'altra strada verso il futuro.

### Perché rendere gli esseri umani più intelligenti sarebbe utile? {#perché-rendere-gli-esseri-umani-più-intelligenti-sarebbe-utile?}

#### **\* Potrebbe aiutare a risolvere il problema dell'allineamento.** {#*-potrebbe-aiutare-a-risolvere-il-problema-dell-allineamento.}

Il problema dell'allineamento dell'IA non ci sembra fondamentalmente irrisolvibile. Ci sembra solo che gli umani non siano neanche lontanamente vicini a risolverlo e che non abbiano raggiunto il livello di intelligenza in cui *pensare* di avere una soluzione sia fortemente correlato con l'*avere* effettivamente una soluzione.

I ricercatori di IA spesso riconoscono che il problema dell'allineamento sembra incredibilmente difficile e che finora sono stati fatti pochissimi progressi. Ecco perché l'idea "forse possiamo chiedere alle IA di fare il lavoro dell'allineamento al posto nostro" è così allettante: quando siete dei ricercatori di IA e sentite che voi e i vostri colleghi non siete all'altezza di risolvere un certo problema, la cosa più ovvia da fare è rivolgersi all'IA.

Ma, come diciamo nel capitolo 11 e nella [risorsa online](#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia) associata, è chiaro anche dal punto di vista di un profano che questa idea ha molti problemi: perché un'IA capisca come risolvere un problema profondo con cui anche i migliori ricercatori umani hanno grandi difficoltà, deve essere abbastanza intelligente da essere pericolosa. E dato che *noi* abbiamo ben poca idea di cosa stiamo facendo, non abbiamo alcuna fonte di verità fondamentale che possiamo utilizzare per addestrare direttamente capacità di allineamento limitate, né alcun modo per verificare se una proposta di allineamento generata dall'IA sia sicura o efficace.

Il mondo può presentarci problemi che sono legittimamente fuori dalla nostra portata. La natura non è un gioco che offre all'umanità solo sfide "eque"; a volte possiamo imbatterci in problemi troppo difficili da risolvere anche per i migliori scienziati umani, o troppo difficili da risolvere nei tempi richiesti.

C'è un modo più realistico per passare l'intero problema a qualche entità più intelligente? Un'opzione potrebbe essere quella di rendere gli *esseri umani* più intelligenti in modo che possano legittimamente essere in grado di risolvere il problema dell'allineamento. Gli esseri umani sono "pre-allineati" in un modo che le IA non sono; le persone più intelligenti hanno le stesse motivazioni prosociali di base che abbiamo tutti.

In linea di principio, sembra possibile che le persone siano in grado di distinguere tra ciò che *sembra* una grande illuminazione alchemica che permetterà loro di trasformare il piombo in oro, e il tipo di conoscenza che *effettivamente consente* di trasformare il piombo in oro (usando la fisica nucleare per rimuovere alcuni neutroni dagli atomi di piombo). Dovrebbero senza dubbio apparire come stati di conoscenza diversi.

Ma gli ingegneri umani reali hanno molte difficoltà a capire in quale dei due stati si trovano. Nella storia reale della chimica, il livello di abilità dell'umanità era tale che gli alchimisti venivano regolarmente ingannati.

Nel mondo reale, gli scienziati si affezionano alle loro teorie preferite e si rifiutano di rivedere le proprie opinioni finché la realtà non li martella ripetutamente con "la tua teoria era sbagliata" — e a volte si rifiutano di cambiare idea anche allora: si dice talvolta che la scienza progredisce "[un funerale alla volta](https://en.wikipedia.org/wiki/Planck%27s_principle)", perché la vecchia guardia non cambierà mai le proprie opinioni e bisogna semplicemente aspettare che la nuova guardia maturi. Ma questo non è un vincolo fondamentale imposto dalla natura; è solo una questione di esseri umani come classe che sono insufficientemente accorti, attenti e consapevoli di sé.

Di solito, va bene che gli esseri umani siano ingenui in questi modi, perché di solito la realtà è abbastanza indulgente con gli errori, almeno nel senso che non spazza via *tutta l'umanità* per l'arroganza di un alchimista. Ma [questo non è un lusso che l'umanità ha](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo) quando si tratta di allineare la superintelligenza artificiale.

L'umanità spesso acquisisce la propria conoscenza lottando, provando, fallendo e accumulando lentamente conoscenza. Ma non *deve per forza* essere così.

Einstein non solo fu in grado di formulare la relatività generale; fu capace di farlo *riflettendo intensamente sul problema*, ancor prima che l'umanità mettesse in orbita i satelliti e iniziasse a vedere con i propri occhi le discrepanze nei loro orologi (come discusso nel Capitolo 6). Aveva prove empiriche, ma è stato in grado di individuare efficacemente la risposta giusta ascoltando i primi, deboli sussurri dell’evidenza, invece di aspettare che la verità bussasse con violenza alla porta.

Quel percorso è più raro e più difficile da percorrere, ma quel tipo di genio scientifico esiste — anche se raramente, persino tra i migliori e i più brillanti del mondo.

Gli esseri umani potenziati di uno o due passi oltre il livello di ricercatori come Einstein o [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) potrebbero iniziare a capire accuratamente i propri difetti, e correggerli, in dozzine di modi diversi.

Potrebbero notare quando stanno razionalizzando o cadendo vittime del [bias di conferma](https://it.wikipedia.org/wiki/Bias_di_conferma). Potrebbero andare oltre il punto in cui si aspettano che un’idea brillante funzioni quando in realtà non funziona , fino a raggiungere uno stato in cui, quando si aspettano di riuscire, *effettivamente* riescono. Potrebbero raggiungere un livello di competenza in cui commettono ancora molti errori, ma non sono [sistematicamente](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) troppo sicuri (o troppo insicuri) in nuovi campi complessi.

Il potenziamento dell'intelligenza umana è davvero una possibilità? A noi sembra di sì, avendo parlato con diversi ricercatori nel campo biotecnologia che pensano che ci modi promettenti di attaccare il problema a breve termine. Anche un'IA attentamente mirata e focalizzata sulla biotecnologia potrebbe aiutare ad accelerare il lavoro. Ma dal nostro punto di vista, rimane molto incerto se un piano del genere funzionerebbe davvero. Quello che ci sentiamo più sicuri nel dire è che si tratta di un'opzione con alta leva che merita molti più investimenti ed esplorazioni di quanti ne stia ricevendo attualmente.

Non stiamo raccomandando il potenziamento dell'intelligenza umana come unica strategia post-spegnimento-dell'IA in cui pensiamo che l'umanità dovrebbe investire pesantemente. Piuttosto, questo è solo uno dei tanti esempi, e quello che attualmente pensiamo sia il più promettente. Raccomandiamo vivamente che l'umanità esamini molteplici possibili percorsi non-IA per andare avanti, piuttosto che puntare tutto su un’unica strategia.

#### **Gli esseri umani potenziati non rappresentano un grave problema di "allineamento umano".** {#gli-esseri-umani-potenziati-non-rappresentano-un-grave-problema-di-"allineamento-umano".}

Gli esseri umani potenziati avrebbero essenzialmente la stessa struttura cerebrale, le stesse emozioni, ecc. del resto di noi. Con l'IA, anche quella addestrata a [*sembrare come*](https://docs.google.com/document/d/1FbehxsMqTaw2seAzUJw4ny0tvZgUdbHJnCUi_Pbd5VU/edit?tab=t.k1kf1fy9gx5i#heading=h.8cxst4c4x63a) noi, c'è un enorme divario in termini di differenze cognitive e motivazionali, e un divario altrettanto grande in termini di comprensibilità; con esseri umani leggermente più intelligenti, nulla di tutto ciò sembra particolarmente probabile.

I ricercatori con potenziamento cognitivo non avrebbero bisogno di mantenere la propria integrità mentale mentre si trasformano in vaste superintelligenze con menti milioni di volte più grandi. Dovrebbero solo essere portati al livello necessario per capire come *costruire* — non fare crescere — superintelligenze artificiali che siano davvero allineate e stabili.

Potrebbe comunque esserci un problema di "allineamento umano" in senso lato, nel senso che qualsiasi tentativo di coordinare più persone può incontrare problemi principale-agente e problemi di incentivi. E questi problemi sono intrinsecamente molto più importanti per qualsiasi gruppo incaricato di creare una superintelligenza.

Pensiamo che questi problemi siano gestibili a patto che gli esseri umani siano inizialmente visibilmente altruisti e generosi, che la loro intelligenza venga potenziata solo gradualmente e che operino in istituzioni ben progettate con incentivi ben strutturati. Ma è del tutto normale che la gente si preoccupi della possibilità che qualcuno cerchi di prendere il controllo. Risolvere questi problemi non sarebbe necessariamente facile, ma non sarebbe così irrealizzabile come le aziende che cercano di sviluppare superintelligenze imperscrutabili con menti del tutto incomprensibili e pulsioni disumane.

Creare una squadra d'élite di supergeni geneticamente modificati per aiutare il pianeta a navigare in sicurezza attraverso la transizione verso la superintelligenza è sicuramente il tipo di cosa che l'umanità dovrebbe fare con attenzione, vista la posta in gioco di un'impresa del genere. Una mossa del genere comporta varie questioni pratiche ed etiche, ma queste devono essere valutate rispetto al costo di lasciare che la superintelligenza ci uccida tutti, se non ci sono altre soluzioni altrettanto promettenti.

I tempi estremi possono richiedere misure estreme, ma potenziare l'intelligenza umana (in modo modesto) non sembra nemmeno una misura particolarmente drastica. Sembra piuttosto una tecnologia complessivamente positiva, che ha almeno qualche possibilità di aiutare l'umanità in più di un modo.
