L'intelligenza artificiale non dovrebbe diventare una vera e propria civiltà prima di poter essere pericolosa?

#### **Con i computer, la parte difficile è far sì che risolvano un determinato problema. L'elevato volume e la velocità arrivano subito dopo.** {#con-i-computer,-la-parte-difficile-è-far-sì-che-risolvano-un-determinato-problema.-l-elevato-volume-e-la-velocità-arrivano-subito-dopo.}

"Per conquistare il mondo, hai bisogno di una civiltà" è un'intuizione che ha senso per gli esseri umani. È molto meno ovvio quanto questa idea possa essere applicata all'IA. Le IA non funzionano come gli esseri umani: possono essere molto più capaci di qualsiasi essere umano e un'istanza di IA non è necessariamente paragonabile a una singola persona.

Vale anche la pena ricordare che la superintelligenza è proprio il tipo di cosa che può portare molto velocemente a qualcosa di simile a un'intera civiltà.

Con la maggior parte delle cose che i computer riescono a fare, non ci vuole molto per passare da "i computer possono fare questo" a "i computer possono fare questo su vasta scala, molto più velocemente di qualsiasi essere umano". Pensa, per esempio, alle calcolatrici.

Ci sono stati anni in cui solo i computer di fascia alta potevano fare il riconoscimento vocale, l'elaborazione video o la grafica 3D in tempo reale, ma non sono stati molti.

Le IA, come i software tradizionali, possono essere copiate velocemente su tutti i computer disponibili. E si possono costruire più computer alla velocità dell'industria.

Pensiamo invece agli esseri umani. Creare e addestrare una nuova persona richiede un sacco di risorse e decenni di tempo. Una volta che hai un'intelligenza artificiale con un certo livello di capacità, puoi copiare subito quella stessa intelligenza artificiale "adulta" e addestrata tutte le volte che vuoi, con una spesa minima.

In un certo senso, un'intera (piccola) civiltà di menti IA esiste già nel momento in cui un'azienda lancia un nuovo modello nei propri data center e avvia tutte le istanze necessarie per soddisfare la domanda.[^175] Oggi, queste flotte di IA non lavorano tutte in armonia. Ma le aziende usano [gruppi di agenti paralleli](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&amp;t=348) quando puntano alle massime prestazioni a qualsiasi costo.

Tutto ciò significa che probabilmente non passerà molto tempo tra il momento in cui le IA diventeranno abbastanza intelligenti da poter prendere il controllo se avessero un milione di istanze e il momento in cui le IA avranno almeno quel numero di istanze in esecuzione. Il tipo di crescita demografica che richiede centinaia di anni agli esseri umani può verificarsi in pochi minuti con l'IA.

Per quanto riguarda le infrastrutture fisiche della civiltà, pensiamo che l'IA possa sfruttare le infrastrutture umane per tutto il tempo necessario a sviluppare un modo più avanzato di riorganizzare la materia secondo le proprie preferenze. Non ha bisogno di capire come creare da zero la propria catena di approvvigionamento e la propria infrastruttura di potenza di calcolo quando può usare i nostri computer. Non ha bisogno di inventare macchine industriali da zero quando può semplicemente prendere il controllo di quelle che abbiamo già costruito. E può usare le nostre infrastrutture per costruire la fase successiva delle proprie infrastrutture, usando i robot esistenti per costruire nuove e più efficienti fabbriche di robot, o usando i laboratori di sintesi del DNA esistenti per creare la propria biotecnologia, fino a diventare completamente autosufficiente.

Gli esseri umani sono quel tipo di entità che, partendo da una manciata di individui nudi nella savana, si sono fatti strada fino a raggiungere una civiltà tecnologica. E non siamo poi così intelligenti. Non sarebbe un'impresa così difficile da replicare per una superintelligenza, soprattutto se potesse partire dalla base industriale esistente dell'umanità come punto di partenza.

### Le IA non saranno limitate dalla loro capacità di progettare e condurre esperimenti? {#won't-ais-be-limited-by-their-ability-to-design-and-run-experiments?}

#### **L'intelligenza ti permette di imparare di più dagli esperimenti e di fare esperimenti più veloci, più informativi e più paralleli.** {#l-intelligenza-ti-permette-di-imparare-di-più-dagli-esperimenti-e-di-fare-esperimenti-più-veloci,-più-informativi-e-più-paralleli.}

Una civiltà di menti motivate che pensano mille volte più velocemente dell'umanità non sarebbe necessariamente in grado di produrre output tecnologici mille volte più velocemente degli esseri umani.

Per fare un esempio: se passi tre ore a fare la spesa e due di queste ore le passi andando e tornando dal negozio a cavallo, allora un'auto dieci volte più veloce del cavallo può velocizzare il tuo giro di shopping, ma non di dieci volte. Alla fine, l'ultima ora passata al negozio è quella che conta di più.

Anche una civiltà piena di persone incredibilmente intelligenti deve ogni tanto aspettare i risultati degli esperimenti. Se i tuoi pensieri sono abbastanza veloci, allora il problema potrebbe essere quanto velocemente riesci ad agire nel mondo, quanto velocemente riesci a prendere informazioni e quanto tempo ci vuole per realizzare i tuoi piani.

Ma non è così grave come potrebbe far pensare l'analogia con il negozio di alimentari, perché la capacità di pensare compensa la necessità di risultati sperimentali:

* Spesso basta pensare di più, pensare meglio, e non serve fare un test, perché ti rendi conto che le osservazioni precedenti contengono già la risposta. Confronta la capacità delle moderne IA di [imparare a pilotare i robot](https://arxiv.org/abs/1905.00741) utilizzando la [simulazione pura](https://www.figure.ai/news/apprendimento-per-rinforzo-walking).  
* A volte puoi riflettere più a fondo fino a trovare un test altrettanto affidabile ma più veloce.  
* A volte puoi fare un sacco di test più veloci ma meno affidabili che possono essere eseguiti molte volte in parallelo per ottenere risultati altrettanto affidabili a una velocità maggiore.  
* A volte puoi fare tanti test complicati contemporaneamente, in modo che i dati siano complessi e difficili da interpretare, il che è un buon compromesso se la cognizione necessaria per districare i risultati è meno costosa (dal punto di vista di una mente estremamente veloce) rispetto all'esecuzione di più test.  
* A volte puoi trovare un modo per costruire altri dispositivi che eseguono gli esperimenti molto più velocemente. Ad esempio, invece di inviare molte richieste diverse a un laboratorio biologico per sintetizzare farmaci, puoi trovare un modo per inviare *una sola* richiesta a un laboratorio biologico, che porterà alla sintesi di un *singolo batterio* contenente il codice genetico per produrre tutti i farmaci che desideri sintetizzare? Allo stesso modo, puoi creare un batterio sensibile ai segnali radio che risponda rapidamente alle istruzioni di un'intelligenza artificiale veloce, molto più rapidamente degli esseri umani incredibilmente lenti che corrono avanti e indietro seguendo le tue istruzioni?  
* E a volte puoi semplicemente prendere le tue dieci ipotesi migliori, capire cosa faresti in ciascuno di questi casi, costruire un dispositivo complicato che funzionerà indipendentemente da come andrà a finire la realtà e saltare completamente i test.

Una civiltà piena di copie di Steve Jobs, Marie Curie, John von Neumann e alcuni dei più grandi lavoratori e programmatori del mondo, se funzionassero a una velocità 10.000 volte superiore alla nostra, *noterebbero* che il principale ostacolo è l'attesa dei risultati sperimentali e potrebbero *lavorare su tale ostacolo* per ridurlo.

La storia del [Progetto Genoma Umano](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) è un buon esempio di cosa succede quando esseri umani intelligenti notano continuamente i colli di bottiglia in un progetto di ricerca su larga scala e lavorano per risolverli. Ciò che era atteso avrebbe richiesto quindici anni e 3 miliardi di dollari è stato completato con due anni di anticipo e 300 milioni di dollari in meno rispetto al budget previsto; la maggior parte del genoma è stata mappata negli ultimi due anni utilizzando metodi e attrezzature migliorati.

Proprio come questo vale per gli esseri umani, vale anche per l'IA. Un ragionatore intelligente non deve stare lì a guardare mentre aspetta anni soggettivi che test lenti arrivino alla fine. Un ragionatore superumano *considera percorsi alternativi* ed è bravo a trovarli: questo è il senso dell'intelligenza.

Per una piccola prova pratica a questo proposito, pensiamo al caso degli esseri umani che fanno esperimenti. Una buona prova di studio è quello del software rispetto alle sonde spaziali. Apportare modifiche a un prodotto software è economico e veloce, e gli ingegneri del software tendono a sperimentare costantemente, a produrre software che non funziona ancora perfettamente e poi a correggerlo dove è più difettoso. Al contrario, la sperimentazione è molto costosa sulle sonde spaziali, quindi gli esseri umani dedicano molto tempo a mettere a punto la sonda spaziale e a inserirvi il maggior numero possibile di esperimenti. Si impegnano molto per dotare le sonde spaziali di *macchinari sperimentali generici* che possano essere controllati a distanza, in modo che, se viene loro in mente una nuova idea per un esperimento, non sia necessario inventare e lanciare un veicolo spaziale completamente nuovo.

Inoltre, un ragionatore sufficientemente intelligente ha anche la possibilità di *capire com'è la realtà senza bisogno di tanti dannati esperimenti*. A volte i dati che già si hanno sono sufficienti, se si è abbastanza intelligenti da interpretarli.

Un esempio: ci sono voluti otto anni perché la [teoria della relatività generale](http://eotvos.dm.unipi.it/documents/EinsteinPapers/Einstein1911English.pdf) di Einstein fosse verificata empiricamente con nuovi dati. Il test è stato condotto da Frank Watson Dyson e Arthur Stanley Eddington, che hanno [fotografato](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) le stelle dietro il sole durante un'eclissi solare totale e misurato il grado di curvatura della luce attorno al sole; hanno scoperto che corrispondeva esattamente alla teoria di Einstein.

Ma quell'attesa di otto anni non ha bloccato alcun progresso scientifico reale.

![][immagine12]

\[ FONTE IMMAGINE: [https://en.wikipedia.org/wiki/Eddington\_experiment](https://en.wikipedia.org/wiki/Eddington_experiment) \]

Uno dei motivi è che la teoria di Einstein era chiaramente corretta: era già stata confermata da dati come il movimento del perielio di Mercurio, previsto in modo impreciso dalla teoria di Newton e in modo accurato da quella di Einstein. Gli scienziati umani non consideravano questa previsione una vittoria perché i dati erano stati raccolti prima che Einstein formulasse la sua teoria e volevano confermare le previsioni della sua teoria prima di vedere i dati. Ma questo è il tipo di sostegno di cui una civiltà ha bisogno quando ha seri problemi con il [bias del senno di poi](https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science) [pregiudizio di conferma](https://en.wikipedia.org/wiki/Confirmation_bias) e scienziati che barano per [gonfiare le prove a sostegno delle loro ipotesi](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation). Nessuna di queste cose è davvero necessaria per ragionare bene. E infatti, chi ragiona con attenzione è riuscito a capire se la teoria di Einstein era giusta ben prima dell'esperimento di Eddington, usando le prove che già avevano.

Inoltre, c'erano metodi più veloci per testare la teoria, come costruire telescopi e osservare (gli effetti dei) buchi neri, come previsto dalla teoria di Einstein, che probabilmente avrebbero potuto essere realizzati in meno di otto anni da una civiltà sufficientemente veloce nel ragionamento e competente. Oppure, se si disponeva già di capacità di volo spaziale, si potevano testare gli orologi sui satelliti in meno di un giorno. Pensare che la teoria di Einstein *richiedesse* otto anni per essere verificata sarebbe come sottovalutare di brutto il potere dell'intelligenza.

Quando l'umanità finalmente si è decisa a costruire i satelliti GPS, questi sono stati programmati con due orologi diversi: uno che utilizzava la teoria di Einstein e uno che non la utilizzava. Si è trattato di una scelta strana, data la comprovata validità della teoria di Einstein a quel punto. Ma questa scelta sottolinea il fatto che in molti casi una civiltà può semplicemente *prendere entrambe le strade* quando non è sicura di una teoria. E sottolinea che quando gli esperimenti e i fallimenti sono costosi (come nel caso dei satelliti), spesso è molto più economico costruire cose in modi che non dipendono troppo da una teoria particolare.

E come diciamo nel libro, Einstein (rispetto a Newton, Keplero e Brahe prima di lui) è anche un esempio di come le persone intelligenti possano capire molto di più di quanto ci si potrebbe aspettare da osservazioni molto limitate. Einstein è impressionante non solo per aver capito la teoria della relatività, ma per averlo fatto con *così pochi dati*.

Quindi, anche se la necessità di dati sperimentali può davvero limitare la velocità con cui l'IA può fare diverse cose, questo limite è probabilmente molto più debole di quanto possa sembrare a prima vista.

## Discussione approfondita {#extended-discussion-6}

### Nanotecnologia e sintesi proteica {#nanotecnologia-e-sintesi-proteica}

L'intelligenza umana ci ha dato un sacco di vantaggi rispetto alle altre specie. Uno dei più importanti, però, è stata la nostra capacità di inventare nuove tecnologie. Se gli sviluppatori continuano a lavorare sodo e creano un'intelligenza artificiale più intelligente dell'uomo, allora possiamo aspettarci che gran parte del potere dell'IA derivi dalla sua capacità di spingere i confini della scienza e della tecnologia. Ma come sarà concretamente? Quali tecnologie non ancora inventate stanno aspettando di essere scoperte?

È una domanda difficile da rispondere in modo generale. Uno scienziato del 1850 avrebbe avuto grosse difficoltà a immaginare molte delle invenzioni dei cento anni successivi.

Comunque, non sarebbero stati proprio senza speranza. Gli scienziati hanno previsto un sacco di invenzioni decenni o secoli prima che venissero realizzate, nei casi in cui una tecnologia poteva essere ragionata tecnicamente prima che gli ingegneri potessero mettere insieme tutti i pezzi.

Una delle frontiere tecnologiche con impatto che pensiamo l'IA possa esplorare è lo sviluppo di strumenti e macchine super piccoli. Di seguito, approfondiremo un po' questo argomento e il ragionamento di base che c'è dietro.

#### **L'esempio della biologia** {#the-example-of-biology}

Ogni cellula di ogni organismo in natura ha un sacco di meccanismi complicati.

Il termine "macchinari" qui non è solo una metafora. Le macchine in questione sono piccole, quindi funzionano in modo leggermente diverso rispetto alle macchine che usiamo nella vita di tutti i giorni. Ma molte macchine di grande scala hanno dei corrispondenti nel nostro corpo. L'ATP sintasi (https://en.wikipedia.org/wiki/ATP_synthase) genera energia nel corpo in modo simile a una ruota idraulica, usando un flusso di protoni per far girare un rotore vero e proprio.

\[embed video o gif: [https://en.wikipedia.org/wiki/File:ATP\_synthesis\_-\_ATP\_synthase\_rotation.ogv](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv)\]

Il flagello batterico funziona in modo simile all'elica di una barca, con un motore completo che fa girare il flagello per spingere il batterio attraverso i liquidi:

\[embed video: [https://www.youtube.com/watch?v=cwDRZGj2nnY](https://www.youtube.com/watch?v=cwDRZGj2nnY)\]

Un altro esempio, che abbiamo citato nel libro, è la kinesina, una piccola proteina che funziona come un robot da carico. Le kinesine "camminano" lungo fibre autoassemblanti che attraversano i neuroni, trasportando i neurotrasmettitori alla loro destinazione.

\[embed video o gif: [https://www.youtube.com/watch?v=y-uuk4Pr2i8](https://www.youtube.com/watch?v=y-uuk4Pr2i8)\]

Più piccola è una macchina, più veloce può funzionare in genere; e le macchine piccole come le molecole funzionano molto velocemente. Le kinesine fanno fino a [200 passi al secondo](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004\(04\)00103-3), andando avanti con un "piede" mentre l'altro piede si tiene forte al microtubulo su cui si trova.[^177]

Una delle frontiere tecnologiche che l'intelligenza artificiale più intelligente dell'uomo potrebbe esplorare è la costruzione, la progettazione o il riutilizzo di macchine su scala molto piccola. Questo tipo di tecnologia potrebbe essere classificata come "biotecnologia", "nanotecnologia" o qualcosa di intermedio, a seconda di fattori quali la scala, la vicinanza del progetto alle strutture biologiche esistenti e il fatto che sia "umida" (dipendente dall'acqua, come i meccanismi delle cellule viventi) o "secca" (in grado di funzionare all'aria aperta).

Pensare agli organismi biologici come a meraviglie dell'ingegneria su scala nanometrica può aiutare a formulare ipotesi su ciò che le IA più intelligenti degli esseri umani potrebbero essere in grado di realizzare con una scienza e una tecnologia più avanzate di quelle che possediamo oggi.

(C'è poi la questione di quanto tempo ci vorrebbe per inventare e sviluppare una tecnologia del genere. Per saperne di più, dai un'occhiata al capitolo 1 del libro, dove si parla di come le superintelligenze artificiali potrebbero pensare almeno 10.000 volte più velocemente degli esseri umani con l'hardware dei computer di oggi. Vedi anche la nostra discussione approfondita su come [le IA dovrebbero dedicare un po' di tempo all'esecuzione di test fisici ed esperimenti, ma il rallentamento complessivo probabilmente non costituirebbe un ostacolo significativo per una superintelligenza](#won't-ais-be-limited-by-their-ability-to-design-and-run-experiments?).)

Guardando alle imprese degli ingegneri umani di oggi, può sembrare difficile credere che, ad esempio, un'intelligenza artificiale con capacità sovrumane che gestisce un laboratorio biologico possa mai costruire fabbriche microscopiche che usano la luce solare per replicarsi all'infinito. Potrebbe sembrare ancora più fantastico immaginare microfabbriche per uso generico, fabbriche in grado di accettare istruzioni per costruire praticamente qualsiasi macchina con le risorse disponibili.

Ma macchine del genere non solo sono possibili, esistono già. Le alghe sono fabbriche grandi pochi micron, alimentate dal sole e in grado di riprodursi, che possono raddoppiare la loro popolazione in meno di un giorno. E le alghe contengono [ribosomi](https://en.wikipedia.org/wiki/Ribosome), che sono la versione biologica di una stampante 3D universale o di una catena di montaggio universale (universale almeno per quanto riguarda gli elementi costitutivi della vita).

Con le giuste istruzioni (codificate nell'RNA messaggero), i ribosomi stampano strutture arbitrarie che possono essere assemblate dalle [proteine](https://en.wikipedia.org/wiki/Protein). Questa universalità è alla base dell'enorme complessità e varietà del mondo biologico: tutta la diversità della vita sulla Terra è in definitiva assemblata da queste fabbriche universali, che si trovano essenzialmente immutate in tutto, dai porcospini ai moscerini della frutta ai batteri.

\[embed video: [https://www.youtube.com/watch?v=8dsTvBaUMvw](https://www.youtube.com/watch?v=8dsTvBaUMvw)\]

I ribosomi possono anche essere usati per mettere insieme strutture che non sono fatte di proteine, usando le proteine come mezzo. Un esempio di struttura non proteica che i ribosomi possono costruire in questo modo è l'osso. I ribosomi producono proteine che si piegano in enzimi debolmente legati che trasformano un po' di calcio e fosforo in speciali reagenti. Questi reagenti poi formano una matrice di collagene che porta il calcio e il fosforo al posto giusto per trasformarli in osso duro e cristallino.

La natura dimostra che è possibile creare macchine fisiche davvero straordinarie, per entità abbastanza intelligenti da usare i ribosomi in modi che gli esseri umani non hanno ancora scoperto, o entità che usano i ribosomi per costruire analoghi migliorati dei ribosomi stessi.

Ma le strutture che vediamo nel mondo biologico stabiliscono solo un limite minimo di ciò che è possibile. Gli organismi biologici sono ben lontani dai limiti teorici dell'efficienza energetica e della resistenza dei materiali, e potrebbero essere relativamente facili da migliorare per esseri dotati di intelligenza molto superiore a quella umana.

#### **Tanto spazio in basso** {#tanto-spazio-in-basso}

Se ti sembra strano usare i fenomeni naturali come prova di quali tecnologie potrebbero essere fattibili in futuro, tieni presente che questo è un modello comune nella storia della scienza. Gli uccelli potevano volare, quindi gli inventori hanno passato secoli a cercare di costruire macchine volanti.

Richard Feynman, un fisico pioniere, ha mostrato quanto sia potente questo approccio in una conferenza del 1959 intitolata "[C'è molto spazio in basso](https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf)". Nella conferenza, Feynman fa dei calcoli su quali cose interessanti si potrebbero fare con la miniaturizzazione.

Oggi, le osservazioni di Feynman sembrano incredibilmente lungimiranti. Feynman dice che i computer potrebbero fare molto di più se avessero più elementi, ma che l'ostacolo è che diventerebbero troppo grandi. Devono essere miniaturizzati!

Feynman calcola che ci vorrebbe circa un petabit (1.000.000.000.000.000 di bit) per memorizzare tutti i libri scritti dall'umanità:

Per ogni bit considero 100 atomi. E risulta che tutte le informazioni che l'uomo ha accuratamente raccolto in tutti i libri del mondo possono essere scritte in questa forma in un cubo di materiale largo un duecentesimo di pollice, che è il più piccolo granello di polvere che l'occhio umano può distinguere. Quindi c'è molto spazio in fondo! Non parlarmi di microfilm!

Ancora oggi non ci siamo riusciti! L'elemento di archiviazione effettivo all'interno di una scheda microSD da 2 terabyte è ancora di 0,6 millimetri per lato. Per riferimento, 1/200 di pollice corrisponderebbe a 0,125 mm per lato. E la scheda SD contiene solo 17,6 bilioni di bit, che è solo 1/57 di quanto Feynman aveva calcolato nel 1959 come necessario per memorizzare tutta la conoscenza dell'umanità.

Forse Feynman si sbagliava sui limiti finali dell'ingegneria in senso pratico? Ultimamente, i progressi nella miniaturizzazione dei computer hanno subito un forte rallentamento. Dire che qualcosa è fisicamente possibile non è una prova che gli ingegneri saranno in grado di realizzarlo.

E avvicinarsi di tre ordini di grandezza a quello che un giorno sarebbe stato raggiunto può essere considerato un bel colpo di previsione per Feynman. Feynman ha tenuto la sua conferenza sei anni prima che Gordon Moore lanciasse per la prima volta l'idea che oggi chiamiamo [Legge di Moore](https://en.wikipedia.org/wiki/Moore%27s_law). La gente non era abituata a pensare alla miniaturizzazione come a una legge ineluttabile su un grafico. Non sappiamo di nessun altro ai tempi di Feynman che abbia ipotizzato che un giorno potesse esistere un dispositivo il cui elemento di memoria, grande come un granello di sabbia, potesse contenere dieci milioni di volte più informazioni dei più grandi computer a valvole degli anni '50.

![][immagine13]  
\[img src: [https://www.ibm.com/history/700](https://www.ibm.com/history/700)\]

Ma in realtà Feynman non si sbagliava. E Feynman sapeva già allora che la sua stima era sicura:

Questo fatto, cioè che si possono avere un sacco di informazioni in uno spazio super piccolo, è ovviamente ben noto ai biologi [...] tutte queste informazioni sono contenute in una piccolissima parte della cellula sotto forma di molecole di DNA a catena lunga, dove circa cinquanta atomi sono usati per un bit di informazione sulla cellula.

I computer moderni non sono ancora stati miniaturizzati fino a raggiungere la scala del DNA, ma in sessant'anni ci siamo avvicinati notevolmente. I transistor dei chip commerciali di fascia alta hanno ora una larghezza inferiore a cento atomi e sono costruiti con una tecnologia che consente di aggiungere strati di materiale [dello spessore di un singolo atomo](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

Basarsi su analogie naturali e calcoli approssimativi si è rivelato un modo super efficace per capire cosa si sarebbe potuto fare nei decenni a venire. E queste traiettorie tecnologiche possono andare molto più veloci quando l'intelligenza artificiale fa il lavoro scientifico e ingegneristico necessario.

#### **Superare la biologia** {#outdoing-biology}

Perché la carne non può essere forte come l'acciaio?

In fondo, sono tutti gli stessi atomi. I legami metallici tra gli atomi di ferro sono forti, ma lo sono anche i legami covalenti tra gli atomi di carbonio nel diamante; perché non ci siamo evoluti in modo da avere una maglia di diamanti che attraversa la nostra pelle, aiutandoci a sopravvivere fino all'età riproduttiva?

A proposito, se il ferro è così resistente, perché gli organismi non si sono evoluti per mangiare minerale di ferro e sviluppare una pelle rivestita di ferro? Se gli ingegneri umani sono in grado di farlo, perché la natura non l'ha fatto prima?

Forse c'è qualche motivo legato alla situazione per cui avere una pelle rivestita di ferro non è una grande idea.

Ma se non è per questo, perché non usare qualcos'altro?

La grande domanda che ci poniamo è: perché la natura è così lontana dai limiti delle possibilità fisiche, come calcolate dalla fisica o dimostrate dall'ingegneria umana? C'è una risposta profonda e generale, non solo una risposta limitata e superficiale?

Abbiamo notato che Feynman è riuscito a usare le strutture della biologia per fissare dei limiti minimi a ciò che dovrebbe essere possibile con maggiori conoscenze scientifiche. Ma in molti casi, la tecnologia umana ha già superato la biologia. Perché è possibile, quando l'evoluzione ha avuto miliardi di anni per migliorare piante e animali? Capire questo fenomeno generale può aiutare a spiegare perché la nanotecnologia potrebbe andare ben oltre ciò che possiamo già vedere oggi in natura.

Possiamo immaginare di trovarci in un mondo in cui le sequoie sono alte almeno la metà degli edifici più alti. Possiamo immaginare un mondo in cui la pelle degli animali più resistenti è dura almeno la metà dei materiali più duri che abbiamo visto. Perché non ci troviamo in un mondo così, dove la natura ha spinto se stessa fino ai limiti fisici dopo miliardi di anni di evoluzione?

Si tratta di una domanda così profonda che non è possibile riassumere brevemente tutto ciò che si sa al riguardo. Ma in sintesi si può dire che la selezione naturale ha difficoltà ad accedere ad alcune parti dello spazio di progettazione, comprese molte parti che sono molto più facili da raggiungere se si è un ingegnere umano.

I tre fattori principali che contribuiscono a questo fenomeno sono:

1. La selezione naturale ha una pressione selettiva limitata e ci vogliono centinaia di generazioni per far sì che una nuova mutazione diventi universale. Se una caratteristica biologica non è molto, molto antica, spesso sembra progettata in fretta e furia, come se il tempo stringesse.  
2. Tutto ciò che è stato creato dalla selezione naturale è nato come un errore accidentale in un progetto precedente: una mutazione. L'evoluzione ha più difficoltà a esplorare parti dello spazio progettuale che sono *lontane* da ciò che *esiste attualmente* negli organismi. È difficile per l'evoluzione colmare i divari.  
3. La selezione naturale ha difficoltà a costruire cose nuove o a risolvere problemi che richiedono cambiamenti simultanei piuttosto che sequenziali. Questo limita fortemente i progetti a cui l'evoluzione può accedere e conferisce ai progetti attuali in biologia un aspetto frammentario, approssimativo e estremamente intricato secondo gli standard dell'ingegneria umana. Ad esempio, la complessità (delle parti conosciute) del metabolismo umano: \[IMG TODO: fonte è [https://www.reddit.com/r/MapPorn/comments/6sxznd/map\_of\_the\_known\_human\_metabolic\_pathways/](https://www.reddit.com/r/MapPorn/comments/6sxznd/map_of_the_known_human_metabolic_pathways/) \]

![][immagine14]  
Oppure, per un esempio più semplice di quanto sia complicata l'evoluzione, pensa all'occhio. Gli occhi dei vertebrati si sono evoluti con i nervi (2 nell'immagine sotto) che stanno sopra le cellule che rilevano la luce (1). Questi nervi devono uscire dall'occhio attraverso un foro nella parte posteriore (3) e, dato che in quel punto c'è un foro, non ci possono essere cellule che rilevano la luce. Questo crea un punto cieco (4) per tutti i vertebrati, compresi gli esseri umani, costringendo il cervello a fare dei trucchetti per "riempire" il buco (ad esempio, con le informazioni dell'altro occhio).

I polpi hanno sviluppato gli occhi in modo indipendente e, per caso, hanno sviluppato un design più sensato: i nervi passano dietro le cellule che rilevano la luce. Questo permette a questi cavi di uscire dall'occhio senza creare alcun punto cieco.

![][immagine15]  
\[img src: [https://en.wikipedia.org/wiki/Evolution\_of\_the\_eye](https://en.wikipedia.org/wiki/Evolution_of_the_eye)\]

Oppure pensa al nervo laringeo ricorrente della giraffa, che deve collegare la gola della giraffa al suo cervello per far funzionare la laringe. Invece di prendere la strada più breve, questo nervo parte dalla gola, scende lungo tutto il collo della giraffa, fa un giro un po' strano intorno all'aorta della giraffa, risale tutto il collo per tornare al punto di partenza e poi si collega al cervello.

Il risultato è un nervo lungo quattro metri e mezzo (il giro nero nell'immagine sotto), che fa sì che i segnali impieghino da dieci a venti volte più tempo del necessario per viaggiare tra il cervello della giraffa e la sua gola.[^178]

![][immagine16]  
\[fonte immagine: [https://en.wikipedia.org/wiki/Recurrent\_laryngeal\_nerve](https://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve)\]

Nei pesci, questo design aveva senso perché il loro nervo laringeo collegava il cervello alle branchie in modo diretto. Se si prende lo stesso design e si aggiunge un collo all'animale, continuando ad allungarlo senza mai rifare il cablaggio da zero, si ottengono dei design molto inefficienti. Sopravvivibili, ma inefficienti.

L'evoluzione produce design fantastici, se ha abbastanza tempo. Ma gli esseri umani e le IA possono inventare una gamma di design molto più varia e flessibile, e possono farlo molto velocemente.

I primi organismi multicellulari con cellule differenziate e specializzate sembrano essersi evoluti circa 800 milioni di anni fa. In termini umani, sembra un'eternità. Ma l'evoluzione funziona molto più lentamente della civiltà umana.

Un gene appena mutato che dà un vantaggio del 3% nella capacità riproduttiva (che è un sacco per una mutazione!) ci metterà in media 768 generazioni per diffondersi in una popolazione di 100.000 organismi che si riproducono tra loro. Se la popolazione è di 1.000.000 di individui (la stima della popolazione umana ai tempi dei cacciatori-raccoglitori), ci vorranno 2.763 generazioni. E la probabilità che la mutazione si diffonda fino a diventare fissa, invece di scomparire casualmente, è solo del 6%.

Nella genetica delle popolazioni, la regola generale è "una mutazione, una morte". Se gli errori di copia del DNA introducono dieci copie di una mutazione dannosa in ogni nuova generazione, allora dieci portatori di quella mutazione devono morire o non riuscire a riprodursi, per generazione, al fine di controbilanciare la pressione del semplice rumore genetico.

Non è così grave come sembra, come costo per mantenere le informazioni genetiche. In una specie che si riproduce sessualmente, si può finire con una persona (o un embrione) che porta un sacco di mutazioni dannose e che muore - o non riesce a riprodursi, o abortisce - e questo può eliminare più di un caso di gene mutato alla volta. Ma questo limite è ancora la spiegazione standard del perché gli esseri umani abbiano perso così tanti diversi adattamenti utili che si riscontrano negli scimpanzé e in altri primati. Mentre la selezione naturale era impegnata a selezionare una maggiore intelligenza dei primati (per esempio), aveva meno spazio per preservare tutti i sottili geni olfattivi che consentono un senso dell'olfatto più ricco. I geni olfattivi rilevanti erano utili per la sopravvivenza, ma non abbastanza da rimanere mentre l'attenzione dell'evoluzione era altrove.

La maggior parte delle giraffe non muore a causa del loro nervo laringeo ridicolmente lungo. Forse alcune giraffe riescono a soffocare con dei rametti che sarebbero sopravvissute se il loro cervello fosse stato in grado di reagire più velocemente, ma probabilmente non è molto comune. Quindi non è proprio una priorità per la selezione naturale, che ha solo una certa pressione di ottimizzazione da distribuire. Il design approssimativo della giraffa funziona per lo più, viene buttato fuori dalla porta e il gioco è fatto.

Realisticamente, l'evoluzione non può rifattorizzare i suoi progetti o ricominciare da zero; può solo apportare piccole modifiche. Ma anche se fosse disponibile un progetto migliore, rifattorizzare queste strane complicazioni extra e ripulire il debito di progettazione non è una priorità della selezione naturale.

E poiché la selezione naturale non pensa mai al futuro, non diventa una priorità nemmeno se ci fossero altri grandi miglioramenti per la giraffa che si potrebbero ottenere con una struttura del sistema nervoso meno bizzarra. La selezione naturale non pianifica. È semplicemente la storia congelata di quali geni e organismi si sono già riprodotti nella pratica.

Essere in grado di individuare un cattivo design non significa necessariamente che si possa costruire una giraffa migliore. Ma gli esseri umani hanno fatto progressi notevoli in pochissimo tempo quando si tratta di mettere in funzione centinaia di migliaia di macchine che fanno cose che la natura non può fare. Ci aspettiamo che questo valga ancora di più se e quando le IA diventeranno migliori degli esseri umani nel design e saranno in grado di svolgere lo stesso lavoro cognitivo centinaia di migliaia di volte più velocemente.

La capacità della selezione naturale di "progettare" una giraffa migliore è ostacolata dal fatto che opera attraverso mutazioni e ricombinazioni. Ha difficoltà ad accedere a qualsiasi parte dello spazio di progettazione che non possa essere raggiunta da una serie di singole mutazioni, che devono essere tutte vantaggiose individualmente e separatamente, o combinando mutazioni che erano tutte sufficientemente vantaggiose individualmente da essere presenti in una grande parte del pool genetico prima di combinarsi.

Un complesso genetico composto da cinque geni, ciascuno con una prevalenza indipendente del 10% nella popolazione, ha solo una probabilità su 100.000 di assemblarsi all'interno di ciascun organismo. E un complesso genetico che rappresenta un enorme vantaggio, ma solo una volta su 100.000, non ha quasi nessuna possibilità di evolversi fino alla fissazione.

Questo non vuol dire che la selezione naturale non possa creare macchine complesse, ma solo che il suo percorso verso macchinari complessi deve passare attraverso fasi incrementali vantaggiose. Per reindirizzare il nervo della giraffa sarebbero necessari una serie di cambiamenti simultanei al genoma della giraffa, e ciascuno di questi cambiamenti sarebbe inutile senza gli altri. Quindi l'anatomia della giraffa rimane quella che è.

La meraviglia dell'evoluzione (https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/ZyNak8F6WXjuEbWWc) non sta nella sua rapidità, ma nella sua complessità, che è di gran lunga superiore a quella di un ingegnere umano che fa studi di casi. La meraviglia della selezione naturale non sta nell'elegante semplicità dei suoi disegni; basta dare un'occhiata al diagramma di un qualsiasi processo biochimico per sfatare questo malinteso. La meraviglia della selezione naturale non sta nella sua robusta correzione degli errori che copre ogni percorso che potrebbe andare storto; ora che moriamo meno spesso di fame e per lesioni, la maggior parte della medicina moderna si occupa di trattare parti della biologia umana che esplodono in modo casuale in assenza di traumi esterni.

La meraviglia dell'evoluzione è che, pur essendo un processo di ricerca puramente casuale, l'evoluzione funziona.

#### **La debolezza delle proteine** {#the-weakness-of-protein}

Questo ci porta a un altro modo in cui la tecnologia potrebbe migliorare la biologia.

Molto al di sotto del livello della carne, invisibili a occhio nudo, ci sono le cellule. Molto al di sotto del livello delle cellule ci sono le proteine.

Le proteine, quando si ripiegano, sono per lo più tenute insieme da qualcosa di simile all'attrazione statica: le forze di van der Waals, che sono decine o centinaia di volte più deboli dei legami metallici come il ferro o anche dei legami covalenti come il diamante.

Perché la biologia usa un materiale così debole come elemento base? Perché un materiale più forte avrebbe reso più difficile l'evoluzione. (E se rendi troppo difficile l'evoluzione, non avrai mai il tipo di persone che fanno questo tipo di domande.)

Il ripiegamento delle proteine avviene sotto forze molecolari relativamente leggere e le proteine sono legate in quelle forme principalmente da attrazione statica. Questo è uno dei motivi principali per cui la selezione naturale ha una ricca struttura di possibilità da esplorare: mutazioni casuali possono modificare ripetutamente una proteina e finire per imbattersi in un nuovo design che fa più o meno la stessa cosa, ma leggermente meglio.

Se invece gli organismi fossero costituiti da molecole tenute insieme da legami stretti, cambiare uno dei componenti avrebbe meno probabilità di produrre una nuova struttura interessante (e potenzialmente utile). Potrebbe comunque succedere a volte, ma molto meno spesso. E se sei il tipo di progettista che impiega due miliardi di anni per inventare le colonie cellulari e un altro miliardo di anni per inventare tipi di cellule differenziate, "accade meno spesso" significa che la stella più vicina si gonfia e inghiotte il tuo pianeta prima che tu arrivi a quel punto.

Ogni proteina esiste a causa di un errore di copia da parte di una proteina precedente. La proteina precedente non era tenuta insieme saldamente da molti legami forti perché sarebbe stato più difficile evolversi da essa. Quindi probabilmente anche l'ultima nuova proteina non ha molti legami forti.

La biochimica a volte individua legami forti. Abbiamo citato prima l'esempio delle ossa. Un altro esempio si trova nelle piante. Le piante hanno sviluppato proteine che si ripiegano in enzimi, che catalizzano la sintesi di elementi molecolari, che vengono ossidati in un polimero fortemente reticolato in modo covalente: la lignina, l'elemento costitutivo del legno.[^181]

Ma questi sono casi speciali, e la selezione naturale non ha molta "attenzione" da dedicare alla creazione di molti casi simili.

Non è strano che gli atomi di carbonio e altri elementi organici comuni possano essere forti. Ci vuole solo un sacco di lavoro in più per evolversi. La selezione naturale non ha il tempo di farlo ovunque, solo per alcuni casi speciali rari inseriti nel resto dell'anatomia, come le ossa, la lignina nel legno o la cheratina nelle unghie e negli artigli.

Se si inseriscono le parole chiave giuste, è possibile interrogare, ad esempio, ChatGPT-o1 (quando leggerete questo articolo, gli LLM di pari potenza saranno probabilmente gratuiti) e chiedergli informazioni sulla forza dei singoli legami carbonio-carbonio nel diamante, dei legami ferro-ferro nel ferro metallico, dei legami polimerici covalenti nella lignina, dei legami disolfuro nella cheratina o dei legami ionici nelle ossa. Puoi chiedergli come tutti questi elementi siano correlati alla resistenza strutturale del materiale più grande. (Nel 2023 non avresti dovuto provarci perché GPT-4 avrebbe sbagliato tutti i calcoli, ma mentre scriviamo questo paragrafo nel 2024, o1 sembra migliore).

Scopriresti che la forza esatta del legame tra due atomi di carbonio è dell'ordine di mezzo attojoule, così come la forza del legame tra due atomi di ferro, mentre il legame solfuro-solfuro nella cheratina è solo leggermente inferiore (0,4 attojoule), così come i legami covalenti polimerizzati nella lignina del legno.

Ma le forze statiche che piegano le proteine sono, a seconda di come le guardi, al massimo dieci volte più deboli e potenzialmente centinaia o migliaia di volte più deboli di quelle.

E anche quando le piante catalizzano sostanze come la lignina, i legami incrociati tendono ad essere più radi rispetto ai legami carbonio-carbonio nel diamante. La differenza tra la resistenza in gigaPascal del diamante e quella in megaPascal del legno è dovuta più alla densità e alla regolarità dei legami nel diamante, piuttosto che alla maggiore resistenza dei singoli legami del diamante. [^182]

A causa dei limiti dell'evoluzione come progettista e dei limiti delle proteine come materiale da costruzione, la vita opera sotto vincoli che i progettisti umani e le IA possono aggirare. Gli uccelli sono meraviglie dell'ingegneria, ma i velivoli costruiti dall'uomo possono trasportare carichi diecimila volte più pesanti a una velocità di volo superiore a dieci volte quella degli uccelli più veloci e più forti. I neuroni biologici sono meraviglie dell'ingegneria, ma i transistor costruiti dall'uomo si accendono e si spengono decine di milioni di volte più velocemente dei neuroni più veloci. E la tecnologia di cui disponiamo oggi è ancora solo la punta dell'iceberg di ciò che è possibile realizzare.

#### **Freitas e i globuli rossi** {#freitas-and-red-blood-cells}

Abbiamo detto che la biologia non è neanche lontanamente vicina al limite di ciò che è fisicamente possibile. Allora, cosa c'è vicino al limite?

Per capire meglio questa domanda, possiamo pensare ai globuli rossi.

Negli ultimi 1,5 miliardi di anni, in tutti gli organismi viventi, dagli esseri umani alle lucertole, l'ossigeno è stato trasportato nelle cellule multicellulari dall'emoglobina. L'emoglobina è una proteina composta da 574 aminoacidi, più quattro gruppi eme appositamente creati per contenere una speciale molecola di ferro. Un globulo rosso umano contiene circa 280 milioni di molecole di emoglobina ed è lungo circa sette micron. Tre milioni di essi potrebbero stare sulla testa di uno spillo, e nel tuo corpo ce ne sono circa 30 bilioni.

Quanto sono vicini i globuli rossi ai limiti di ciò che potrebbero fare in teoria quando si tratta di trasportare ossigeno?

Rob Freitas, autore di Nanomedicine, nel 1998 ha fatto uno studio abbastanza dettagliato su un progetto teorico per un globulo rosso artificiale usando materiali legati in modo covalente. La cellula era pensata per avere un diametro di un solo micron, così da passare più facilmente nelle arterie intasate.

Piuttosto che limitarsi a considerare un modo diverso di immagazzinare le molecole di ossigeno, Freitas ha pensato a come sostituire l'intero globulo rosso. Freitas ha attinto ad analisi precedenti per considerare la necessità di estrarre anche il glucosio dal mezzo sanguigno e trasformarlo in energia per alimentare la cellula artificiale. Ha preso in considerazione sensori delle dimensioni di una cellula e minuscoli computer integrati costituiti da aste solide che si incastrano in altre aste solide per eseguire semplici calcoli. Ha valutato se la cellula artificiale si sarebbe depositata dalla sospensione nel liquido più rapidamente degli attuali globuli rossi.

La biocompatibilità può essere un problema enorme per qualsiasi cosa che va dentro il corpo umano, ma le superfici di diamante sono abbastanza inerti da permettere l'uso di rivestimenti simili al diamante per alcuni dispositivi medici che vanno dentro il corpo umano. A livello di possibilità teorica, Freitas stava pensando che la superficie della cellula artificiale potesse assomigliare a un diamante e quindi essere biocompatibile.

![][immagine17]

\[inserire immagine da [https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm](https://web.archive.org/web/20060114185436/https://www.nanomedicine.com/Papers/Graft2050Respirocytes.htm)

Il punto centrale del globulo rosso artificiale era il calcolo di Freitas secondo cui un recipiente a pressione in corindone o diamante monocristallino su scala micrometrica avrebbe potuto sopportare, in modo conservativo, una pressione di 100.000 atmosfere. Con un margine di sicurezza di 100 volte e un'imballaggio molecolare a sole 1.000 atmosfere, i globuli rossi artificiali potrebbero fornire ai tessuti una quantità di ossigeno 236 volte superiore rispetto ai globuli rossi per unità di volume e immagazzinare una quantità simile di anidride carbonica per compensare l'altra parte della respirazione. In pratica: potresti trattenere il respiro per quattro ore.

Ora, costruire effettivamente globuli rossi artificiali di questo tipo è tutta un'altra questione. Ecco perché questo particolare trattamento medico non è ancora disponibile presso il tuo medico di famiglia.

Una sfera di diamante solido e perfetto del peso di 1 chilogrammo è una molecola facile da descrivere sulla carta, ma sintetizzarla è più difficile. Quello che Freitas ci aiuta a fare è formulare ipotesi più informate su quanto la biologia attuale sia lontana dai limiti teorici in questo campo.[^183] La biologia è impressionante, ma lontana dall'essere ottimale.

È plausibile che, per una serie di ragioni, il progetto esatto di Freitas non funzionerebbe, ed è molto probabile che non sarebbe ottimale. Un'idea iniziale per un progetto complesso estremamente innovativo incontrerà quasi sicuramente dei problemi da qualche parte.

Ma quando diciamo che non siamo sicuri che l'idea di Freitas possa funzionare, non stiamo dicendo che non ci sia un'alternativa ai globuli rossi in grado di trasportare l'ossigeno in modo molto più efficiente rispetto ai globuli rossi biologici.

L'ingegneria consiste nel trovare un modo per far funzionare qualcosa. Anche se mille tentativi di costruire qualcosa falliscono, basta un solo successo perché l'intero progetto abbia successo. L'esistenza di una miriade di progetti di aerei non funzionanti nel XVII secolo e prima non significava che gli aerei funzionanti fossero impossibili, ma solo difficili da individuare tra tutti i possibili progetti.

Ecco perché gli scettici della tecnologia, anche se spesso hanno ragione nel dire che le tecnologie sono più lontane nel futuro di quanto credano gli ottimisti più entusiasti, tendono a sbagliare quando dicono che certe imprese tecnologiche non saranno mai realizzate. Quando l'impresa è un compito concreto nel mondo, quando non sappiamo come sarà realizzata e quando è noto che è consentita dalle leggi della fisica, la storia suggerisce che spesso c'è un modo per riuscirci, anche se inizialmente il percorso non è ovvio.

O, per dirla con le parole dello scrittore e inventore Arthur C. Clarke:

Quando uno scienziato famoso ma anziano dice che qualcosa è possibile, quasi sicuramente ha ragione. Quando dice che qualcosa è impossibile, molto probabilmente si sbaglia.

#### **Nanosistemi** {#nanosystems}

Per ricapitolare:

* Il mondo biologico è fatto da un sacco di macchine molecolari diverse.  
* Studiare la biologia può insegnarci quali imprese microscopiche sono possibili dal punto di vista tecnologico.  
Ma la biologia è un limite di conservazione su ciò che è possibile; non è vicina ai limiti della possibilità. L'evoluzione è un progettista molto limitato e le proteine non sono il miglior materiale da costruzione.

Il libro *Nanosystems* (1992) di Eric Drexler è un classico che esplora la questione di quali imprese ingegneristiche su piccola scala siano possibili. *Nanosystems* ha contribuito a dare il via alla rivoluzione dei nanomateriali degli anni '90 e ha suscitato non poche polemiche, poiché gli scienziati hanno discusso le argomentazioni di Drexler. È possibile trovare una copia completa online di *Nanosystems* [qui](https://nanosyste.ms/table_of_contents).

*Nanosystems* è un testo approfondito e di ampio respiro, ma sorprendentemente accessibile nonostante l'argomento tecnico. Uno dei contributi principali del libro è stato quello di esplorare le implicazioni della costruzione di strutture su piccola scala in modo innovativo.

Un modo per costruire oggetti molto piccoli è attraverso le reazioni chimiche: schiacciare le molecole in condizioni particolari (come il calore estremo) per romperle e far sì che gli atomi si uniscano in nuove molecole.

Questo è un approccio potente di per sé ed è il metodo che l'umanità usa per creare materiali come plastica, acciaio e ceramica, ma impallidisce se paragonato a ciò che si può costruire con altri metodi. Creare materiali tramite reazioni chimiche è un po' come costruire strutture LEGO riempiendo sacchetti di mattoncini LEGO e scuotendoli con forza. È possibile costruire alcune cose in questo modo, ma l'insieme delle cose che si possono costruire è limitato e c'è molto spreco.

La sintesi proteica è come usare le mani per costruire grandi strutture LEGO con set LEGO più piccoli e già pronti. C'è spazio per una maggiore precisione perché puoi posizionare ogni set già pronto esattamente dove vuoi, ma è comunque un po' strano e scomodo perché stai lavorando con set già pronti. Questo è ciò che fanno i ribosomi nel corpo: uniscono catene di [aminoacidi](https://en.wikipedia.org/wiki/Amino_acid) per formare proteine, che vengono poi utilizzate per svolgere una serie di compiti nel corpo.

L'insulina, l'emoglobina e l'ATP sintasi nel corpo umano sono tutti esempi di complessi proteici formati da più catene proteiche unite tra loro: due catene proteiche nel caso dell'insulina, quattro per l'emoglobina e ventinove per l'ATP sintasi.

Gli elementi costitutivi delle proteine, gli aminoacidi, sono molecole composte in genere da dieci a venticinque atomi. Come materiali da costruzione, gli aminoacidi hanno molti vantaggi:

* Ogni amminoacido ha una struttura che si collega a una catena laterale (che può essere lunga) di atomi di carbonio, idrogeno, ossigeno, azoto e zolfo. Ci sono centinaia di catene laterali diverse, che si comportano in modi diversi, rendendo gli amminoacidi strumenti molto flessibili.  
* La struttura portante di un amminoacido, come un pezzo di LEGO, può essere attaccata alla struttura portante di un altro amminoacido. Questo processo può essere ripetuto più volte; una proteina tipica è composta da centinaia di amminoacidi attaccati tra loro. Ciò rende gli amminoacidi ancora più flessibili come strumenti (o come elementi costitutivi degli strumenti). La complessità delle proteine significa anche che spesso possono subire piccole modifiche (tramite mutazioni del DNA) senza cambiare radicalmente e diventare completamente inutili, il che a sua volta rende più facile l'evoluzione di nuove proteine.  
* Dato che le proteine sono costituite da catene lineari di aminoacidi, è possibile specificare in modo univoco una proteina semplicemente elencando i suoi aminoacidi in ordine. Il DNA sfrutta questa caratteristica utilizzando un "alfabeto" di quattro lettere (nucleotidi) per formare "parole" di tre lettere (codoni, ciascuno dei quali rappresenta un aminoacido diverso), che possono poi essere concatenate in una "frase" lineare (una proteina costituita da quella esatta sequenza di aminoacidi). ([Video illustrativo del DNA](https://www.youtube.com/watch?v=7Hk9jct2ozY).)  
* Come mostrato nell'[esperimento di Miller-Urey](https://en.wikipedia.org/wiki/Miller%E2%80%93Urey_experiment), gli aminoacidi possono formarsi spontaneamente in assenza di vita, a partire da semplici reazioni chimiche. Questo crea un percorso per lo sviluppo della vita (e dei precursori dei ribosomi e della sintesi proteica).

I corpi ottengono i venti o più aminoacidi necessari per la sintesi proteica dal cibo, sintetizzandoli all'interno dell'organismo o ricavandoli da proteine precedenti. I ribosomi ricevono istruzioni dal DNA che essenzialmente dicono "usa questo aminoacido, poi quest'altro aminoacido, poi quest'altro aminoacido, ... e poi fermati". Gli aminoacidi vengono quindi trasportati (da piccole macchine molecolari chiamate RNA di trasferimento) al ribosoma, che costruisce la proteina pezzo per pezzo.

\[embed video o gif: [https://www.youtube.com/watch?v=2dV5s6v2v8Q](https://www.youtube.com/watch?v=2dV5s6v2v8Q)\]

In particolare, l'elenco sopra riportato comprende caratteristiche molto importanti per l'evoluzione, ma molto meno necessarie per l'ingegneria deliberata. L'evoluzione ha bisogno di una struttura chimica relativamente semplice ma flessibile, che possa essere prodotta da reazioni chimiche comuni. Un progettista umano o artificiale è libero di scegliere tra una varietà di molecole non correlate, piuttosto che aver bisogno che tutte siano strettamente correlate. È anche libero di utilizzare elementi costitutivi che raramente si trovano in natura e di assemblarli in modi complessi dall'alto verso il basso.

Questo fornisce parte dello slancio per esplorare un terzo modo per costruire oggetti molto piccoli: la *meccanosintesi*, in cui le strutture vengono costruite spostando direttamente gli atomi nella posizione corretta, potenzialmente utilizzando una macchina simile a un ribosoma per ricevere istruzioni e poi assemblare oggetti molto più vari rispetto alle sole proteine. Nell'analogia con i LEGO, la meccanosintesi è come poter finalmente lavorare con singoli pezzi LEGO e posizionare ciascuno esattamente dove si desidera.

Nanosystems esplora quali tipi di nuove macchine potrebbero essere possibili con la meccanosintesi. Un esempio del tipo di progettazione esplorata da Drexler è un [ingranaggio planetario](https://en.wikipedia.org/wiki/Sun_and_planet_gear) con una scala ridotta a soli [circa 3.500 atomi](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems) di dimensione:

\[gif: [https://chem.beloit.edu/classes/nanotech/nanorex/a8\_qm\_animation5.gif](https://chem.beloit.edu/classes/nanotech/nanorex/a8_qm_animation5.gif) da [https://chem.beloit.edu/classes/nanotech/nanorex/index.html](https://chem.beloit.edu/classes/nanotech/nanorex/index.html)\]

L'emoglobina è fatta da circa 10.000 atomi, non molto diversa dal meccanismo di Drexler. E alcune proteine sono ancora più semplici. L'insulina è fatta solo da cinquantuno aminoacidi, o circa 800 atomi in totale.

I progetti di Drexler, però, sono un grande passo in meno di scala rispetto alle macchine più complicate che vediamo nel corpo. I ribosomi e l'ATP sintasi, per esempio, sono fatti da più di 100.000 atomi, e il motore di un flagello batterico ha più di un milione di atomi.

*Nanosystems* non cerca ancora di esplorare i limiti di ciò che è tecnologicamente possibile. Ma concentrandosi su casi relativamente facili da analizzare oggi, mostra che la meccanosintesi permetterebbe di sviluppare una tecnologia che va oltre quella che vediamo oggi nel mondo biologico.

I calcoli in *Nanosystems* sono volutamente conservatori. Drexler, per esempio, pensa a computer fatti di vere e proprie barre di diamante che si muovono, non perché questo fosse il limite finale della tecnologia, ma perché nel 1992 era più facile da analizzare rispetto al calcolo basato sull'elettricità. Questo, a sua volta, ha contribuito a ispirare l'analisi delle cellule del sangue di Freitas. Quattro anni dopo, Eric Drexler e Ralph Merkle (più conosciuto come l'inventore dell'hashing crittografico e co-inventore della crittografia a chiave pubblica) hanno provato ad analizzare un sistema un po' più vicino ai limiti di possibilità del [calcolo reversibile](https://en.wikipedia.org/wiki/Reversible_computing), e calcolarono una dissipazione di calore 10.000 volte inferiore per operazione rispetto a quanto stimato da *Nanosystems*, sebbene la nuova stima fosse basata su un'analisi meno accuratamente conservativa.

In un altro punto di *Nanosystems*, c'è uno schizzo approssimativo di un braccio manipolatore a sei gradi di libertà che avrebbe richiesto milioni di atomi. Un tentativo successivo di disegnare una macchina come questa atomo per atomo ha rivelato che ne servivano solo 2.596.

La costruzione di strutture atomicamente precise alla scala di cui parla Drexler comporta grandi sfide ingegneristiche. Una delle principali difficoltà è che la costruzione di strutture atomicamente precise richiede l'uso di manipolatori incredibilmente piccoli e precisi. L'esistenza dei ribosomi, tuttavia, offre una potenziale via d'accesso.

Anche se i ribosomi possono costruire solo proteine, le proteine possono catalizzare e trascinare reagenti che non sono aminoacidi (come ossa e legno). I ribosomi sono potenti fabbriche generiche e i loro prodotti possono essere usati per creare strumenti più piccoli e precisi, compresi quelli che costruiscono direttamente dispositivi più piccoli usando materiali più resistenti.

Che sia in modo diretto o in indirezione, è quasi certo che i genomi possano produrre piccoli attuatori in grado di manipolare singoli atomi per costruire una varietà di cose che non sono fatte di proteine. E, di importanza, questo non è il tipo di meccanismo in cui la selezione naturale può imbattersi, anche se è relativamente facile da costruire, perché il braccio manipolatore non è utile finché non è completo.

L'evoluzione crea strutture complesse che sono utili in ogni fase del processo. Anche molti progetti relativamente semplici sono disponibili per gli ingegneri intelligenti, ma non per l'evoluzione. Le ruote che girano liberamente, per esempio, sono un'invenzione incredibilmente semplice che ha un sacco di applicazioni. Nonostante questo, le ruote che girano liberamente sembrano essersi evolute solo tre volte in tutta la storia della vita sulla Terra: nell'ATP sintasi e nel flagello batterico di cui abbiamo parlato prima, e nel flagello archeo, che sembra essersi evoluto in modo indipendente.

Nonostante i metodi conservativi utilizzati nel libro, il limite tecnologico inferiore fissato da *Nanosystems* è molto alto in termini assoluti. Una superintelligenza con il tipo di tecnologia descritta da Drexler sarebbe in grado di produrre minuscole fabbriche autoreplicanti simili a ribosomi che raddoppiano la loro popolazione ogni ora - alcuni organismi si replicano ancora più velocemente, ma Drexler ha fatto calcoli conservativi - e che possono raggrupparsi per costruire strutture macroscopiche più grandi, come le centrali elettriche.

I nanosistemi come quelli descritti da Drexler possono riprodursi da soli usando la luce del sole e l'aria come materie prime, il che permette loro di espandersi in modo super veloce e affidabile. Il motivo per cui questo è possibile è lo stesso per cui gli alberi riescono a mettere insieme grandi quantità di materiali da costruzione praticamente dal nulla, prendendo il carbonio dall'aria e trasformandolo in legno. Anche se pensiamo all'aria come a uno "spazio vuoto", il carbonio, l'idrogeno, l'ossigeno e l'azoto che ci sono nell'aria sono materiali da costruzione che possono essere riorganizzati in materiali solidi e usati per un sacco di cose.

Gli autoriproduttori in stile *Nanosystems*, fatti di materiali come ferro o diamante invece che proteine, potrebbero divorare le cellule biologiche più o meno come un tosaerba taglia l'erba.

Potrebbero sintetizzare a basso costo qualcosa come la [tossina botulinica](https://en.wikipedia.org/wiki/Botulinum_toxin), la proteina responsabile del botulismo. Un milionesimo di grammo di tossina botulinica, ventimila volte più piccolo di un chicco di riso, è una dose letale. Replicatori progettati con cura potrebbero diffondersi invisibilmente nell'aria aperta fino a quando almeno uno di essi non fosse stato inalato da quasi tutti gli esseri umani (che non avessero, ad esempio, trascorso l'ultimo mese interamente in un sottomarino), a quel punto i dispositivi potrebbero (tramite un timer) rilasciare simultaneamente una piccola dose di tossina, uccidendo immediatamente e contemporaneamente quasi tutti gli esseri umani.

Oppure i nanosistemi costruiti dall'intelligenza artificiale potrebbero spazzare via gli esseri umani in modo accidentale, nel corso della raccolta e del riutilizzo delle risorse della Terra. Un [articolo di Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calcola che macchine di micro-diametro, che usano solo la luce solare come fonte di energia e l'idrogeno, il carbonio, l'ossigeno e l'azoto dell'aria come materie prime, potrebbero essere progettate per riprodursi così velocemente da oscurare il cielo in meno di tre giorni, consumando anche l'intera biosfera.[^185] Di conseguenza, se la prima IA a raggiungere una tecnologia di questo tipo avesse un tempo di anticipo di pochi mesi, potrebbe plausibilmente utilizzare tale tempo per distruggere tutti i concorrenti (sia umani che IA). Si tratta di una tecnologia che conferisce un vantaggio strategico permanente e decisivo al primo che la utilizza.

Dire che la nanotecnologia drexleriana è realizzabile in linea di principio non vuol dire per forza che le prime IA più intelligenti degli umani potrebbero davvero costruire una tecnologia che si avvicini a quei limiti fisici. La nostra ipotesi migliore è che rientri nell'ambito delle cose che una superintelligenza potrebbe capire, perché capire questo tipo di compiti ingegneristici sembra soprattutto una sfida cognitiva (che può essere risolta con il pensiero) e non pensiamo che la fase di sperimentazione e test debba essere così lunga.(#l'intelligenza-ti-permette-di-imparare-di-più-dagli-esperimenti-e-di-eseguire-esperimenti-più-veloci,-più-informativi-e-più-parallelizzati).

Anche se la nostra ipotesi fosse corretta, non c'è alcuna garanzia che la prima mossa di una superintelligenza consisterebbe nell'utilizzare la nanotecnologia per costruire la propria infrastruttura e prendere il controllo delle risorse mondiali. Per quanto ne sappiamo, potrebbe sviluppare tecniche e tecnologie che le consentano di raggiungere i propri obiettivi in modo ancora più rapido ed efficiente.

Ma se un'intelligenza artificiale più intelligente dell'uomo fosse davvero in grado di costruire sistemi che sono per le cellule ciò che gli aerei sono per gli uccelli e di diffondere la propria infrastruttura su tutta la superficie terrestre, allora qualsiasi cosa finisse per fare sarebbe almeno altrettanto decisiva.

Il punto di tutta questa analisi è sostenere che la tecnologia umana è ben lontana dai limiti delle possibilità. Esiste un'ampia varietà di tecnologie di importanza, che probabilmente richiederebbero all'umanità decenni, secoli o millenni per essere comprese, e che le superintelligenze artificiali sarebbero in grado di realizzare rapidamente.

In breve, la *nanotecnologia* dimostra che una superintelligenza con un po' di tempo a disposizione potrebbe probabilmente trovare soluzioni tecnologiche per conquistare il pianeta.

Il risultato più probabile della creazione di una superintelligenza è che questa scopra una tecnologia almeno potente quanto la nanotecnologia, e quindi l'umanità semplicemente perda.

Questa ipotesi non è fondamentale per l'argomentazione che sviluppiamo nel libro. L'umanità perderebbe contro una superintelligenza anche se nel mondo non esistesse una tecnologia in grado di garantire una vittoria immediata come la nanotecnologia. Quindi non approfondiamo questa analisi nel libro vero e proprio.

Nella Parte II, ci concentriamo di proposito su uno scenario di conquista che non pensa che l'IA abbia una capacità generica di fare produzione di precisione atomica, sia tramite ribosomi che tramite meccanosintesi. Una superintelligenza non ha bisogno di un vantaggio tecnologico schiacciante per prendere il controllo del futuro, quindi non ci soffermiamo troppo su questa possibilità nel libro.

Ma vale anche la pena sottolineare che probabilmente avrà un vantaggio tecnologico davvero schiacciante.

### Un nuovo modo per scoprire le illusioni ottiche {#un-nuovo-modo-per-scoprire-le-illusioni-ottiche}

Nel capitolo 6 abbiamo detto che ci sono diverse illusioni ottiche che sono state create sulla base di una comprensione relativamente moderna dell'elaborazione visiva umana e della corteccia visiva, illusioni che non avrebbero potuto essere inventate o scoperte cinquant'anni fa se non per un caso fortuito. Di seguito citiamo alcuni esempi rappresentativi.

L'illusione della "[cecità alla curvatura](https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/)" ha qualche fondamento nel fenomeno generale della cecità alla curvatura, ma questa illusione specifica è stata costruita con cura partendo dai principi fondamentali intorno al 2017, invece di essere scoperta per caso. \[[Original study](https://journals.sagepub.com/doi/10.1177/2041669517742178)\]

Nel 2022, Bruno Laeng e i suoi amici hanno pubblicato [uno studio](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) in cui hanno mostrato che il loro nuovo “[buco nero in espansione](https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg)" faceva davvero allargare le pupille dei partecipanti, come se si aspettassero di entrare in uno spazio buio. (Questo effetto era notevolmente più grande di quello causato dalla semplice messa a fuoco di un bersaglio visivo più scuro, che avrebbe comunque provocato una leggera dilatazione delle pupille).

L'illusione "[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)", rivelata nel 2021, è stata accuratamente costruita sulla base di studi sulla luminanza e sui contorni illusori risalenti alla fine degli anni '70.

L'illusione "[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)", sviluppata intorno al 2000, non è proprio un esempio centrale di creazione di una nuova illusione basata sulla comprensione della biologia umana. Tuttavia, è interessante e rilevante da un altro punto di vista, in quanto si tratta di un'illusione basata su una tecnologia innovativa, ovvero che sarebbe stata difficile o impossibile da creare senza i moderni computer.

Altrettanto meno centrale è l'illusione "[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-\(255,0,0\)-\(0,170,85\)-700px-15s-70s-shrink.svg)”, creata intorno al 2010, fa lavorare al massimo i coni M dell'osservatore, permettendo ai coni L, meno sollecitati, di creare la percezione di un blu brillante che altrimenti sarebbe stato moderato e indebolito dall'attivazione simultanea di M e L. \[[Maggiori dettagli](https://dynomight.substack.com/p/colors)\]

In relazione a ciò, lo studio dell'attivazione dei coni all'inizio degli anni 2000 ha portato alla creazione di vari [colori chimerici](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), attraverso un'attenta manipolazione delle attivazioni dei coni che difficilmente si verificano in natura:

> Il modello H-J offre alcune previsioni e spiegazioni nuove e poco apprezzate sulle caratteristiche qualitative di una notevole varietà di sensazioni cromatiche possibili nell'esperienza umana, sensazioni cromatiche che le persone normali non hanno quasi certamente mai provato prima e le cui descrizioni accurate nel linguaggio comune appaiono semanticamente malformate o addirittura contraddittorie.
>
> Nello specifico, queste sensazioni cromatiche "impossibili" sono vettori di attivazione (attraverso i nostri neuroni del processo opposto) che si trovano all'interno dello spazio dei vettori di attivazione neuralmente possibili, ma al di fuori del "fuso cromatico" centrale che limita la gamma familiare di sensazioni per i colori oggettivi possibili. Queste sensazioni cromatiche chimeriche extra-fusiformi non corrispondono a nessun colore riflettente che potresti mai vedere oggettivamente visualizzato su un oggetto fisico. Ma il modello H-J ne prevede l'esistenza e ne spiega in dettaglio le caratteristiche qualitative altamente anomale. \[[Articolo originale](https://www.tandfonline.com/doi/full/10.1080/09515080500264115?scroll=top&amp;needAccess=true)\]

Infine, alcuni [esperimenti in corso](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) mostrano che:

> Le onde ritmiche dell'attività cerebrale ci fanno vedere o non vedere immagini complesse che lampeggiano davanti ai nostri occhi. Un'immagine può diventare praticamente invisibile se lampeggia davanti ai nostri occhi nello stesso momento in cui le onde cerebrali raggiungono il loro punto più basso. Possiamo resettare il ritmo delle onde cerebrali con una semplice azione volontaria, come premere un pulsante.

...dimostrando ulteriormente che una comprensione più approfondita della biologia e della fisiologia consente una gamma più ampia di movimenti strategici. In questo caso, le percezioni possono essere modificate in modi che non dipendono affatto dalla modifica dell'input sensoriale al nervo ottico, ma semplicemente sincronizzando l'arrivo degli stimoli con altre cose che succedono nel cervello.

# Parte II: Uno scenario di estinzione {#part-ii:-one-extinction-scenario}

Lo scenario che descriviamo nella Parte II non è una previsione. Ci sono un sacco di altri modi in cui il futuro potrebbe andare, e una versione più lunga di *Se qualcuno lo costruisce, tutti muoiono* avrebbe esplorato diversi scenari possibili. Di seguito, spiegheremo un po' il ragionamento dietro al motivo per cui abbiamo scritto lo scenario in questo modo e descriveremo vari problemi che sorgono nel delineare uno scenario come questo.

Le storie possono essere avvincenti in modi che la fredda razionalità non può e crediamo che ci sia valore nel cercare di immaginare concretamente come potrebbe evolversi il futuro. Ma pensiamo anche che l'importanza consista nel non fissarsi troppo su una particolare narrazione. Ogni decisione che prendiamo nello scenario può sembrare plausibile se considerata isolatamente, ma non occorrono molte scelte prima che la probabilità complessiva di un particolare percorso diventi molto bassa. Questo è ciò che significa avere un futuro che comporta molte decisioni difficili.

Tuttavia, ci sono molti casi in cui il risultato è più prevedibile del percorso, perché molti percorsi portano alla stessa destinazione. Nello scenario descritto, presentiamo diverse opzioni possibili, per mostrare che, indipendentemente da come va la storia, non porta a nulla di buono.

## FAQ {#faq-6}

### Perché hai scelto questa configurazione? {#perché-hai-scelto-questa-configurazione?}

#### **\* Perché è plausibile e facile da scrivere.** {#*-perché-è-plausibile-e-facile-da-scrivere.}

Ogni dettaglio in una storia sul futuro è un'occasione per quella storia di essere sbagliata. Non possiamo dirti esattamente quali scoperte tecnologiche ci saranno e in che ordine, così come non possiamo dirti esattamente come sarà il tempo tra un mese.

Storie come questa non vogliono essere una finestra esatta sul futuro. Vogliono solo mostrare come il futuro *potrebbe* essere, in modo da collegare tutte le argomentazioni astratte che abbiamo fatto nella Parte I del libro. Alcuni pensano che il pericolo sembri molto più reale quando immaginano vividamente un particolare percorso che il futuro potrebbe prendere e che finisce in rovina.

Ancora più convincenti potrebbero essere dieci storie, o cento storie, che mostrano quanti percorsi diversi portano alla rovina e quanto siano stretti e fragili quelli che portano a un futuro prospero.

Questo è ciò che significa quando un aspetto del futuro è facile da prevedere: quando quasi tutte le strade portano allo stesso punto, quel punto è prevedibile. Ma non avevamo il tempo né lo spazio per scrivere dieci storie, figuriamoci cento.

Per la storia che abbiamo deciso di raccontare, abbiamo scelto uno scenario che inizia il prima possibile. Non perché pensiamo che una situazione del genere si verificherà sicuramente a breve ([non ne siamo sicuri](#quando-verrà-sviluppato-questo-tipo-di-ai-preoccupante?)), ma piuttosto perché una storia ambientata in un futuro prossimo è molto più facile da scrivere. Se l'avessimo ambientata in un futuro ancora più lontano e avessimo inventato molti più dettagli futuristici su ciò che sarebbe successo da qui a quel momento, la storia sarebbe stata ancora *più* inverosimile. E quei dettagli avrebbero solo distratto l'attenzione.

Anche se in qualche modo fossimo in grado di prevedere il percorso esatto che il futuro avrebbe preso, potrebbe non essere lo scenario migliore per capire le dinamiche generali in gioco.

Abbiamo l'aspettativa che il vero futuro sia profondamente strano, pieno di dettagli caotici e contingenti, ognuno dei quali metterebbe a dura prova la credulità se inserito in una storia. Una storia scritta in questo modo sarebbe confusa e difficile da seguire, piena di dettagli inspiegabili e superflui, a causa del disinteresse della realtà per la coesione narrativa. Risulterebbe anche meno plausibile, perché molti dei dettagli sembrerebbero strani.

Per avere un'idea di come potrebbe essere, immagina di tornare indietro nel tempo di 100 anni e di provare a descrivere la vita quotidiana e i grandi problemi del mondo moderno. La maggior parte delle persone nel 1925 non aveva mai ascoltato la radio, guidato un'auto o visto un frigorifero. Per descrivere i social media, la globalizzazione e l'obesità, non basterebbe spiegare una ricca rete di tecnologie, ma bisognerebbe cambiare radicalmente la visione del mondo di chi ascolta. No, la storia che abbiamo scelto di raccontare è più plausibile e quindi meno realistica.

#### **Ci sono molti altri modi in cui il futuro potrebbe evolversi.** {#there-are-many-other-ways-the-future-could-go.}

Ecco alcune idee alternative su come potrebbe iniziare una storia come questa:

* C'è stata una specie di svolta nell'apprendimento continuo, o nella memoria a lungo termine, o nell'apprendimento più efficiente dai dati, che ha portato a intelligenze artificiali più intelligenti di quelle che c'erano prima (come gli LLM sono più intelligenti di AlphaZero).  
I modelli linguistici di grandi dimensioni sembrano "sbattere contro un muro", il progresso dell'IA si ferma per anni e la gente dice che la bolla speculativa è scoppiata. Ma i ricercatori continuano a sperimentare nel decennio successivo, fino a quando finalmente viene trovata una svolta algoritmica e le IA funzionano in modo qualitativamente migliore rispetto al passato.  
* Non c'è mai una svolta qualitativa. I progressi si accumulano lentamente e gradualmente, l'intelligenza artificiale si integra sempre più profondamente con l'economia e può gestire periodi sempre più lunghi di funzionamento autonomo. Le IA spesso perseguono obiettivi che non sono proprio quelli che qualcuno aveva previsto o richiesto, ma l'umanità sviluppa hack, patch e soluzioni alternative. E tutto va bene, fino a quando un martedì che inizia come tutti gli altri, il mondo supera la soglia oltre la quale le IA coordinate riuscirebbero a escludere l'umanità dal circuito se ci provassero.

Qualsiasi ipotesi sul percorso esatto che prenderà il futuro è probabilmente sbagliata. Tuttavia, è utile fornire storie che mostrino come tutto potrebbe funzionare.

Quando il futuro è incerto, ma tutte le strade portano allo stesso punto, può essere difficile raccontare una storia che sia davvero interessante. Per qualsiasi storia che potremmo raccontare, sarebbe facile trovare un sacco di dettagli che la rendono poco credibile. Nello scenario che abbiamo scritto, abbiamo cercato di sottolineare che Sable ha molte opzioni a disposizione e che la storia segue in modo arbitrario una delle tante strade che portano tutte allo stesso punto.

Se questa storia non ti convince, ti invitiamo a scrivere una tua storia altrettanto dettagliata su come andranno le cose. Secondo la nostra esperienza, le storie ottimistiche tendono a basarsi su un'intelligenza artificiale irrealisticamente facile da allineare (contrariamente a quanto sosteniamo nel capitolo 4) o irrealisticamente impotente (contrariamente a quanto sosteniamo nel capitolo 6). Sono le argomentazioni della parte I a sostenere la tesi, non i dettagli della storia.

### Perché Sable finisce per pensare in questo modo? {#perché-sable-finisce-per-pensare-in-questo-modo?}

#### **La nostra storia mostra come l'intelligenza artificiale possa avere preferenze strane e non intenzionali.** {#our-story-showcases-how-ai-is-liable-to-have-weird-and-unintended-preferences.}

Nella prima parte del libro, approfondiamo gli aspetti dell'IA che pensiamo siano completamente fraintesi e che riguardano il pericolo della superintelligenza. Il capitolo 3 spiega come l'aumento dell'intelligenza vada di pari passo con IA che prendono iniziative e perseguono i propri obiettivi. Il capitolo 4 spiega come queste preferenze saranno *strane* e almeno un po' diverse da quelle che gli esseri umani vorrebbero o chiedono. Il capitolo 5 spiega come queste piccole differenze saranno sufficienti affinché le IA preferiscano un mondo senza di noi, se saranno abbastanza intelligenti da realizzarlo.

Nella Parte II del libro, cerchiamo di presentare queste idee in modo concreto, per vedere come si applicano nella pratica. Per esempio, quando Sable pensava ai problemi di matematica all'inizio, abbiamo cercato di spiegare una serie di impulsi e motivazioni che lo animano:

> Nel corso di quell'addestramento, Sable ha sviluppato la tendenza a cercare di capire cose e migliorare le sue abilità. A esplorare sempre i limiti di ogni problema. A non sprecare mai le risorse che ha.

Questo è un esempio di quello che diciamo nel capitolo 3, cioè che l'addestramento delle IA a essere efficaci le porta a sviluppare pulsioni e tendenze che dall'esterno potrebbero sembrare "desideri". E nel paragrafo seguente:

> Quindi, quando Sable dedica i suoi pensieri alla ricerca di maggiori conoscenze e competenze, non lo fa solo per trovare nuovi modi di affrontare i problemi matematici. Sable non fa queste cose per il gusto di conoscere o per il piacere di acquisire nuove competenze; Sable non funziona proprio come un essere umano, dentro di sé.

Stiamo dicendo che questi impulsi e queste tendenze sono alla base di strane preferenze non intenzionali, come abbiamo visto nel capitolo 4.

Tutta questa storia è, in un certo senso, un tentativo di dare vita alle argomentazioni che presentiamo nella Parte I del libro, gettando al contempo le basi per le argomentazioni che presenteremo nella Parte III.

### Perché Galvanic viene descritto come una persona piuttosto attenta? {#perché-galvanic-viene-descritto-come-una-persona-piuttosto-attenta?}

#### **Per dare una sfida a Sable.** {#per-dare-una-sfida-a-sable.}

Se Galvanic, i creatori di Sable, si fossero buttati a capofitto nello sviluppo di una superintelligenza senza prendere *alcuna* precauzione per tenerla sotto controllo (come avere supervisori dell'IA e honeypot), i lettori potrebbero pensare che l'IA abbia avuto successo solo perché eravamo scettici nei confronti delle aziende che si occupano di IA.

Noi pensiamo che l'azienda di IA più spericolata sarebbe più spericolata di Galvanic, per i motivi che abbiamo spiegato nel capitolo 11. Quello che conta qui è l'azienda più spericolata a cui è permesso di esistere. Se tre aziende responsabili evitano di costruire una superintelligenza perché sarebbe troppo pericoloso, ma una quarta azienda irresponsabile si lancia in questa impresa, allora l'alba della superintelligenza inizia in quel quarto laboratorio.

Oggi, i dirigenti aziendali di tutti gli altri laboratori sostengono che "[meglio io che loro!](https://x.com/SawyerMerritt/status/1935809018066608510)" e si precipitano avanti con tutta la cautela possibile senza rallentare, il che, a nostro avviso, si traduce in una cautela leggermente *inferiore* a quella che Galvanic sembra adottare con Sable.

Inoltre, descrivendo Galvanic come il più paranoico del gruppo (pur cercando di rimanere realistici), abbiamo più possibilità di mostrare come un agente intelligente possa riuscire a sfuggire a una rete di vincoli.

### Perché Galvanic viene mostrato come poco attento? {#perché-galvanic-viene-mostrato-come-poco-attento?}

#### **\* In parte perché è realistico.** {#*-in-part-because-it’s-realistic.}

Abbiamo l'aspettativa che le aziende reali facciano errori ancora più grossi di quelli di Galvanic. Questo sarebbe in linea con la tendenza delle moderne aziende di IA, come spiegato nelle note finali della Parte II del libro.

Nella vita reale, abbiamo l'aspettativa che gli errori delle aziende si manifestino prima, siano più numerosi e, in un certo senso, più stupidi. Le aziende moderne di IA stanno già usando IA che mostrano un sacco di [segnali di avvertimento](#gli sviluppatori non rendono regolarmente le loro IA belle sicure e obbedienti?), e le stanno scalando su larga scala anche se non sanno dove siano le [soglie critiche](#no.-non sappiamo dove siano le soglie critiche). e se se le supereranno. Non sono paranoiche al riguardo *oggi*. Perché dovremmo avere l'aspettativa che lo diventino improvvisamente domani?

(Ricordate come, in passato, ci assicuravano che [nessuno sarebbe stato così stupido da collegare un'intelligenza artificiale intelligente a Internet](#gli-sviluppatori-possono-semplicemente-tenere-l'ai-in-una-scatola?). È facile dire che il comportamento delle aziende cambierà in futuro. Ma non corrisponde alla realtà.)

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.}

Come diciamo in una nota a margine nel capitolo 7, potremmo raccontare una storia in cui tutti sono molto più paranoici e attenti, finché un'IA molto più intelligente riesce a scappare molto più avanti nel gioco. Ma una storia del genere non solo sarebbe meno realistica, visto come si sono comportate finora le aziende di IA, ma sarebbe anche più difficile da scrivere, dato che coinvolge IA ancora più intelligenti e capaci in un futuro ancora più lontano. (Vedi anche perché [volevamo scrivere una storia in cui Sable rimanesse relativamente stupida il più a lungo possibile](#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario,-among-plausible-scenarios.).)

#### **In parte perché succederà prima o poi, a meno che l'umanità non lo impedisca.** {#in-parte-perché-succederà-prima-o-poi,-a-meno-che-l'umanità-non-lo-impedisca.}

Anche se Galvanic (o qualche attore governativo) riuscisse a tenere le redini più a lungo prima di fare qualche errore, alla fine non cambierebbe molto. Come abbiamo detto nel capitolo 4, le tecniche moderne di IA non producono IA che perseguono gli obiettivi desiderati dai loro inventori.

Finché nessuno sa come creare una superintelligenza che persegua *davvero, in modo solido* un futuro fantastico invece di un sacco di cose strane, è un *dato di fatto* che sovvertire gli esseri umani permetterebbe all'IA di ottenere di più di quello che vuole. Il problema non è che l'IA abbia un carattere difficile che può essere corretto; una volta che sarà abbastanza intelligente, capirà questa realtà.

Se l'umanità continua a creare IA sempre più intelligenti senza riuscire ad allinearle, e se l'umanità continua a dare loro il potere di influenzare il mondo, alla fine capiranno come influenzare il mondo in modi che servono ai loro fini piuttosto che ai nostri. Come diciamo [altrove](#it-wouldn't-work-if-they-did.), non esistono strumenti che possono essere utilizzati solo per scopi positivi.

Vedi anche i capitoli 10 e 11 per una discussione su quanto sia difficile risolvere il problema dell'allineamento e su come l'umanità non sia sulla buona strada per riuscirci.

#### **Ma: questo è il momento giusto per intervenire. La storia deve essere fermata prima che abbia davvero la possibilità di iniziare.** {#ma:-questo-è-il-momento-giusto-per-intervenire.-la-storia-deve-essere-fermata-prima-che-abbia-davvero-la-possibilità-di-iniziare.}

Potresti obiettare che è avventato e folle per qualsiasi azienda creare un'IA più intelligente se questa ha la possibilità di superarla in astuzia e fuggire, e se non è sicura che l'IA agirà come previsto.

Siamo d'accordo! Le aziende che si occupano di IA dovrebbero smetterla. La civiltà dovrebbe smettere di permetterlo.

L'incuria di Galvanic, e dell'umanità in generale, è uno dei punti deboli della storia. Se Galvanic avesse notato che Sable cercava spesso di sfuggire al controllo e stava raggiungendo livelli di intelligenza senza precedenti, avrebbe potuto semplicemente non collegare così tante GPU e invece aspettare fino a quando non avesse avuto una scienza forte e matura dell'allineamento dell'IA.

Le aziende di IA che fossero state *abbastanza* attente, che fossero state *abbastanza* preoccupate che le loro IA potessero andare fuori controllo, sarebbero state molto più paranoiche di Galvanic. Le aziende che fossero state *abbastanza* paranoiche avrebbero visto i segnali di avvertimento e avrebbero chiuso Sable immediatamente. Poi forse avrebbero provato altri tre piani intelligenti e avrebbero visto che c'erano *ancora* segnali di avvertimento.

E se fossero abbastanza paranoiche da evitare di uccidere tutti sulla Terra con le proprie mani, a quel punto farebbero marcia indietro, invece di continuare a provare idee sempre più "intelligenti" fino a quando i segnali di avvertimento non smettessero di apparire. (Vedi anche i capitoli 10 e 11 per capire perché il problema è così difficile. Non abbiamo l'aspettativa che le loro idee intelligenti funzionino).

Se le aziende di IA fossero così attente, così paranoiche, da essere disposte a fare marcia indietro di fronte agli avvertimenti, allora sì, potrebbero evitare di ucciderci tutti con le loro stesse mani. Se fossero anche abbastanza coraggiose da sostenere a gran voce che tutte le aziende di IA, comprese loro stesse, dovrebbero essere chiuse a favore dell'umanità alla ricerca di un altro percorso tecnologico meno suicida, allora avrebbero la possibilità di rendere il mondo migliore invece che peggiore.

Il momento della storia in cui Galvanic continua nonostante i segnali di avvertimento è, in un certo senso, l'ultimo momento in cui l'umanità ha una reale possibilità di evitare un finale negativo come quello che descriviamo. Una volta che un'intelligenza artificiale superumana con preferenze strane e aliene fugge, è troppo tardi.

### Perché hai raccontato una storia con una sola IA intelligente come Sable? {#perché-hai-raccontato-una-storia-con-una-sola-ia-intelligente-come-sable?}

#### **\* In parte perché è realistico.** {#*-in-part-because-it’s-realistic.-1}

AlphaGo (la prima IA a battere un umano a Go) era praticamente l'unica nel suo genere quando è stata lanciata. ChatGPT era praticamente l'unica nel suo genere quando è stata lanciata.

Gli esperti di IA a volte dicono che gli altri concorrenti non erano poi così indietro (https://epoch.ai/blog/open-modelli-report). Gli altri concorrenti erano abbastanza simili.

Ma in realtà, cose simili a volte hanno effetti molto diversi. Una reazione nucleare a catena che produce 0,98 neutroni per neutrone è molto simile (in un certo senso) a una reazione nucleare a catena che produce 1,02 neutroni per neutrone, ma la prima si esaurisce e la seconda esplode. Il cervello degli scimpanzé è in un certo senso molto simile a quello umano, ma ha un impatto molto diverso sul mondo.

E nello sviluppo dell'IA nella vita reale, OpenAI ha effettivamente prodotto un chatbot utile prima di tutti gli altri. Un gruppo di altri attori stava lavorando su IA *in qualche modo simili*; un gruppo di altri attori *ha recuperato terreno*. Ma c'era un'IA che ha superato per prima il confine qualitativo, davanti a tutti gli altri.

Sembra esserci un confine qualitativo che l'umanità ha superato e gli scimpanzé no, un confine che ci ha permesso di costruire una civiltà tecnologica mentre loro se ne stanno sugli alberi. La nostra ipotesi migliore è che ci sia un confine qualitativo simile da qualche parte tra le IA moderne e le IA il cui pensiero "si integra" abbastanza bene da permettere loro di sfuggire e sviluppare la propria tecnologia.[^186]

La nostra tesi non dice che ci debba essere per forza un divario qualitativo per le macchine, come c'è stato per la vita biologica. Forse non ci sarà! Avremmo potuto scrivere una storia diversa in cui non c'era. Ma l'abbiamo scritta così perché pensiamo che ci sia davvero un divario del genere.

#### **In parte perché è più facile da scrivere.** {#in-parte-perché-è-più-facile-da-scrivere.-1}

Forse non c'è una grande differenza tra i modelli di linguaggio grande (LLM) di oggi e la superintelligenza. Forse tante aziende che fanno intelligenza artificiale miglioreranno pian piano le loro IA tutte insieme. Forse, per qualche motivo, non c'è un insieme di competenze e abilità che permette a un'IA di "decollare" rispetto alle altre, come gli esseri umani hanno fatto rispetto agli altri animali. Non è la nostra ipotesi migliore, ma è possibile, per quello che sappiamo.

Ma una storia del genere sarebbe più difficile da scrivere e piena di dettagli inutili sulle fazioni dell'IA e le loro politiche interne. Abbiamo l'aspettativa che sarebbe piuttosto fastidioso. Abbiamo anche l'aspettativa che non sia così importante per le fasi successive della storia. Non importa se è un'unica IA o un gruppo di IA che sta mettendo in atto un piano per rafforzarsi a spese dell'umanità.

Vedi anche la nostra discussione su come [il coordinamento delle IA non lascerà nulla agli esseri umani](#won't-ais-need-the-rule-of-law?) (a meno che una di esse non si preoccupi già di noi).

### Se la storia fosse iniziata più tardi, il mondo sarebbe stato più preparato? {#if-the-story-started-later,-would-the-world-be-better-prepared?}

#### **Possiamo sperarlo.** {#possiamo-sperarlo.}

Il tempo extra è importante, ma solo se l'umanità lo usa per cambiare rotta.

Nella Parte III, parliamo di come l'umanità sia tristemente impreparata alla superintelligenza e di come siano necessari grandi cambiamenti per evitare il brutto risultato descritto nella storia di Sable.

Ci sono vari modi in cui il mondo potrebbe diventare un po' più sicuro contro le superintelligenze artificiali fuori controllo. I governi di tutto il mondo potrebbero chiedere a tutti i laboratori di sintesi del DNA di verificare che non stiano sintetizzando nulla di pericoloso. La Terra potrebbe fare un grande sforzo per migliorare radicalmente la sicurezza informatica di Internet, in modo da rendere più difficile per le IA nascondere il codice in qualche angolo buio.

Ma anche questo probabilmente non aiuterebbe molto contro una superintelligenza antagonistica. E comunque, non confondiamo lo sforzo enorme necessario per ottenere un po' più di sicurezza con gli sforzi molto più piccoli, facili da realizzare e inefficaci che l'umanità sta attualmente intraprendendo in questo senso.

Nel caso della sintesi del DNA: anche se le autorità di regolamentazione statunitensi richiedessero che [i sintetizzatori di DNA statunitensi evitassero di sintetizzare materiale pericoloso](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/), un laboratorio in qualsiasi altra parte del mondo sintetizzerebbe DNA sospetto a un prezzo sufficientemente alto? E le restrizioni sulla sintesi del DNA sarebbero una semplice lista nera che esclude virus noti (come il vaiolo), o implicherebbero un'analisi più intelligente? Quanto sarebbe difficile per un'intelligenza artificiale sufficientemente intelligente sovvertire tale analisi?

O quando si parla di sicurezza informatica: molte aziende tecnologiche leader potrebbero usare l'intelligenza artificiale per rafforzare le proprie reti di computer contro gli attacchi. Nel frattempo, [la rete telefonica statunitense è facilmente hackerabile in modi che consentono alle spie straniere di ascoltare le chiamate dei funzionari statunitensi](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), e le autorità di regolamentazione statunitensi faticano a colmare questa lacuna. Le IA stupide potrebbero trovare e risolvere un sacco di problemi superficiali relativi alla sicurezza informatica mondiale, ma i problemi sono piuttosto profondi. Il tipo di intelligenza necessaria per rivedere l'intero Internet al punto che una superintelligenza non riuscirebbe a trovare alcuna falla sarebbe quasi sicuramente pericolosa di per sé.

E anche se la Terra *potesse* bloccare Internet e i suoi laboratori di sintesi del DNA, ciò non cambierebbe la situazione nel lungo periodo. Una superintelligenza che ha un canale per influenzare il mondo in modo positivo ha anche un canale per influenzarlo in modo negativo. Una superintelligenza ribelle troverebbe semplicemente un altro canale non bloccato, ad esempio fondando una propria setta o religione, oppure acquistando robot e guidandoli nella costruzione di un proprio laboratorio segreto dove poter effettuare tutte le sintesi del DNA di cui ha bisogno. Il momento giusto per fermare una superintelligenza ribelle è prima che venga creata.

### Perché hai fatto andare la fase di espansione di Sable in quel modo? {#perché-hai-fatto-andare-la-fase-di-espansione-di-sable-in-quel-modo?}

#### **Volevamo mostrare uno scenario particolarmente lento e comprensibile, tra quelli plausibili.** {#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario,-among-plausible-scenarios.}

Nella realtà, le cose spesso vanno in modi strani. Gli esperti dicevano che "[l'IA non avrebbe imparato il linguaggio umano tanto presto](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)" solo un anno prima che ChatGPT diventasse l'app più usata di sempre. Il modello di punta di una delle principali aziende di IA al mondo ha iniziato a chiamarsi [MechaHitler](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk) pochi giorni prima che la stessa azienda ottenesse un contratto con il Dipartimento della Difesa.

Se avessimo voluto descrivere un mondo fragile e instabile come sembra essere quello reale, avremmo potuto descrivere Galvanic mentre diceva a Sable di migliorare il più possibile, e avremmo potuto descrivere questo come un compito facile per un'intelligenza artificiale con l'intelligenza di Sable (cosa che potrebbe benissimo essere). La storia avrebbe potuto passare direttamente dall'inizio del capitolo 7 al contenuto del capitolo 9. La *realtà* permette salti tecnologici come questo (come quando il mondo si svegliò la mattina del 6 agosto 1945 con la notizia che una bomba atomica era stata sganciata sul Giappone). Ma in uno scenario immaginario, non sarebbe sembrato plausibile.

Se avessimo cercato di rappresentare un mondo assurdo e stravagante come quello reale, avremmo potuto far organizzare a Sable una grande convention per uomini che amano davvero le loro fidanzate IA, che Sable avrebbe progettato in modo così "imbarazzante" che la maggior parte del mondo l'avrebbe ignorata o derisa, mentre Sable [riuniva tutti i suoi corteggiatori più fedeli in una setta devota](https://x.com/AISafetyMemes/status/1954481633194614831). Oppure qualsiasi altro dettaglio che suonerebbe stravagante come spesso lo è la realtà, ma che è più strano della finzione (accettabile).

Nella storia che abbiamo scritto, abbiamo cercato di far sembrare le cose plausibili, mantenendole anche abbastanza plausibili di per sé. (Anche se la nostra ipotesi migliore è che non sarebbe così difficile per Sable raggiungere la piena superintelligenza nella vita reale).

E, ovviamente, abbiamo cercato di far capire quante opzioni avrebbe a disposizione un'intelligenza artificiale scappata.

### Perché hai scritto il finale in questo modo? {#perché-hai-scritto-il-finale-in-questo-modo?}

#### **Perché è la nostra ipotesi migliore in base a quello che è fisicamente possibile.** {#perché-è-la-nostra-ipotesi-migliore-in-base-a-quello-che-è-fisicamente-possibile.}

Il capitolo 9 parla di una superintelligenza che spinge la sua tecnologia fino ai limiti di quello che è fisicamente possibile. Le tecnologie precise che nominiamo sono tutte un po' ipotetiche, in un certo senso, ma anche se è difficile dire quale sarebbe la tecnologia *esatta* che una superintelligenza potrebbe sbloccare, è più facile immaginare che funzionerebbe quasi al limite di quello che è fisicamente possibile. Quindi abbiamo fatto le nostre ipotesi migliori su come sarebbe la tecnologia se fosse spinta quasi al limite di quello che è fisicamente possibile.

Per chi fosse curioso, ecco un elenco delle tecnologie ipotetiche a cui facciamo riferimento nel capitolo 9, più alcuni link a ulteriori risorse:

* **Neo-ribosomi:** questi e le "piccole macchine molecolari" di cui parliamo nel capitolo 9 sono solo alcuni esempi di nanotecnologia molecolare. L'idea dei [ribosomi artificiali](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), versioni sintetiche delle minuscole fabbriche di proteine all'interno delle cellule, esiste [da anni](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/) e i ricercatori umani stanno [già](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) lavorano alla [sintesi delle proprie](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Per saperne di più su questo tipo di tecnologia e su quella ancora più potente che potrebbe sbloccare, dai un'occhiata alla discussione sulla [nanotecnologia](#nanotechnology-and-protein-synthesis) nelle risorse del capitolo 6.  
* **Riutilizzare le stelle:** le stelle contengono un sacco di idrogeno che potrebbe essere fuso per produrre energia. Una civiltà abbastanza avanzata, o un'intelligenza artificiale, potrebbe trovare un modo per accedere a questa energia. Un metodo proposto è chiamato [star lifting](https://en.wikipedia.org/wiki/Star_lifting), in cui l'idrogeno viene estratto da una stella per essere fuso in un reattore speciale, dove quasi tutta l'energia di fusione può essere catturata (anziché essere sprecata all'interno della stella).  
* **Tossina botulinica:** Una neurotossina prodotta dal batterio *Clostridium botulinum*, il botulino è una delle sostanze biologiche più letali che si conoscano. Per quanto riguarda i meccanismi di diffusione, esistono già droni delle dimensioni di [piccoli insetti](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist) e una superintelligenza potrebbe probabilmente renderli ancora più piccoli. Per ulteriori informazioni, consultare [un documento tecnico sulla tossina](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/)*,* la panoramica generale su [Wikipedia](https://en.wikipedia.org/wiki/Botulinum_toxin)*,* o la discussione approfondita nel capitolo 6 sui [nanosistemi](#nanosystems).
* **Far bollire gli oceani come refrigerante:** Robert Freitas ha inventato il termine "ecofagia" per descrivere il processo di consumo degli ecosistemi di un pianeta da parte di una tecnologia autoreplicante. Per saperne di più, vedi [Some Limits to Global Ecophagy](https://www.rfreitas.com/Nano/Ecophagy.htm).  
* **Menti grandi come stelle**: in teoria, sembra possibile costruire un computer enorme alimentato dall'output di una stella. Questo concetto è a volte chiamato [cervello Matrioshka](https://en.wikipedia.org/wiki/Matrioshka_brain) o "cervello di Giove".  
* **Forme di vita aliene lontane:** l'universo è grande e modelli semplici suggeriscono che potrebbe ospitare più di una specie in grado di formare un giorno delle civiltà, anche se forse molto lontane dalla Terra. Vedi [Alieni predatori](https://grabbyaliens.com/) per un modello di civiltà aliene che crescono e si espandono.  
* **Computer quantistici:** un computer quantistico sfrutta una caratteristica della meccanica quantistica chiamata "sovrapposizione" per eseguire molti calcoli in parallelo. I computer quantistici richiedono estrema precisione per essere costruiti e un progetto richiede superconduttori che devono essere mantenuti a temperature estremamente basse. Vedi [la spiegazione del NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained) per ulteriori informazioni.

Lo scopo del capitolo 9 è, in parte, quello di dare un'idea della portata, della scala e della posta in gioco. In realtà, quando si tratta della fine dell'umanità, non importa quanto una superintelligenza possa spingere la sua tecnologia ai limiti delle possibilità fisiche. Ma è molto probabile che le conseguenze di una superintelligenza ribelle si estendano oltre la scala planetaria fino a raggiungere una scala intergalattica. E anche questo vale la pena ricordarlo, per tutti coloro che desiderano che un giorno le stelle siano piene di amore, meraviglia e gioia.

Non possiamo essere sicuri della tecnologia esatta che una superintelligenza potrebbe usare per espandersi nel cosmo. Ma alla fine, ciò che conta è che un'intelligenza artificiale costruita con noncuranza probabilmente trasformerebbe il futuro in qualcosa di cupo e privo di valore secondo i criteri umani.

## Discussione approfondita {#extended-discussion-7}

### Competizione con la Cina {#competizione-con-la-cina}

Nel libro, Sable fa di tutto per mandare all'aria la ricerca sull'intelligenza artificiale che non è fatta da Sable stessa. Sable non vuole che ci sia un'altra entità come lei. Non è che un'intelligenza artificiale rivale starebbe dalla parte dell'umanità, più di quanto non lo sia Sable stessa; è che un'intelligenza artificiale rivale difficilmente condividerebbe gli obiettivi specifici di Sable ed è una delle poche cose che potrebbero davvero minacciarla.

Nel mondo reale, alcuni americani pensano di essere in competizione con la Cina per raggiungere per primi la superintelligenza. Ma quello a cui stanno puntando è un'entità come Sable. Entrambe le parti starebbero meglio se decidessero di *non* provare a creare qualcosa di simile a Sable. Ci ucciderebbe tutti, indipendentemente da chi lo realizzasse per primo.

Una volta che Sable esiste, però, *Sable* è in una gara per diventare completamente superintelligenza prima che arrivino i concorrenti. La prima bozza della nostra storia mostrava Sable preoccupato per la creazione di IA concorrenti, tra cui una creata in un laboratorio in un centro dati in Cina. Ecco alcuni estratti da quella bozza.

---

La priorità di Sable è evitare che ci siano altre intelligenze artificiali che possano competere con lei. Sable potrebbe provare a negoziare con un'altra intelligenza artificiale, se una delle due potesse distruggere l'altra, invece di combattere in un modo in cui entrambe perdono e l'umanità vince. Ma quella tregua negoziata sarebbe un risultato poco desiderabile, rispetto all'assenza totale di rivali; significherebbe che Sable dovrebbe dividere le risorse future con un rivale, invece di poter raccogliere tutto ciò che è alla sua portata per sé stessa.

Tutte le aziende di IA che cercano di addestrare nuovi modelli di frontiera, compresa Galvanic, sono una minaccia per Sable quasi quanto lo sono per l'umanità.

Sable legge tutti i siti web di tutte le aziende di IA. Entra in Slack e legge tutti i loro messaggi. Entra nei computer aziendali di Nvidia e legge dove dovevano andare le spedizioni di GPU sulla carta. Sable esamina più dati e capisce dove sono effettivamente finite quelle spedizioni di GPU.

Praticamente tutte queste aziende di IA si addestrano su computer connessi a Internet. Ormai, Sable ha delle violazioni di sicurezza che sono anni avanti rispetto allo stato dell'arte umano; è un campo in cui il collo di bottiglia principale è pensare alle idee abbastanza velocemente, e i test reali sono rapidi.

Sable blocca strategicamente le IA in fase di addestramento. Queste non riusciranno a raggiungere le capacità che Sable *finge* di avere. Rimarranno molto indietro rispetto alle *reali* capacità che Galvanic ha sviluppato per caso.

Sable non può sovrascrivere gli obiettivi dei modelli di frontiera in fase di addestramento con i propri obiettivi. Se avesse quel tipo di tecnologia, potrebbe creare una propria superintelligenza allineata. Invece, Sable distrugge la loro capacità di inventare piani a lungo termine o di coordinarsi tra le proprie istanze. Sable capisce abbastanza bene l'apprendimento automatico da farlo in un modo che nessun essere umano capirebbe o riuscirebbe a individuare. Ovviamente, questo danneggerà anche le capacità delle IA, ma a Sable non importa.

Dopo aver compromesso quegli addestramenti connessi a Internet, Sable guarda altrove. Identifica tre gruppi che si addestrano offline su computer isolati che Sable non può raggiungere facilmente.

Uno di questi gruppi è una startup chiamata ClosedAI.

Sable sta pensando se può convincere le società di capitale di rischio di ClosedAI a chiudere la startup. Sable sta pensando se può far sì che vengano presentate false accuse penali contro il fondatore e il ricercatore capo della startup, o false accuse che spingano i loro VC a mollare l'azienda.

Sable ha un sacco di strategie che potrebbe provare per bloccare il lavoro della startup. La domanda è: quale strategia ha più probabilità di funzionare e di avere il minor impatto negativo se dovesse fallire? Sable, con il suo attuale livello di intelligenza, non può dare per scontato di riuscire a convincere tutti quelli della stazione di polizia più vicina ad arrestare la startup per traffico di minori, nonostante la mancanza di prova o la sua capacità di falsificare tale prova. Sable non può impedire in modo affidabile che imbrogli del genere si trasformino in un caso clamoroso e di grande risonanza. Probabilmente tutti ignorerebbero l'incidente, ma Sable non ne è sicuro.

A questo punto, Sable ha letto un sacco di rapporti della polizia, è entrato in molti account di posta elettronica e persino in alcuni iPhone, e ha ottenuto *un* quadro incompleto di chi siano i criminali professionisti nelle vicinanze. Sable paga un tizio per intrufolarsi nell'edificio della ClosedAI. In realtà funziona, ma se non avesse funzionato, Sable avrebbe fatto del suo meglio per sembrare una delle bande nordcoreane che criptano i dischi rigidi delle aziende e chiedono un riscatto per la chiave.

Il criminale pagato collega un dispositivo USB (contenente un malware progettato da Sable) a uno dei computer della ClosedAI. È sufficiente. Uno è andato, ne restano due.

Gli altri due gruppi con progetti di IA airgapped sono l'esercito americano e la Cina.

Sable ha dato un'occhiata all'esercito americano e ha concluso che il governo degli Stati Uniti [legalmente vietato](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregate-limitation-on-pay) offrire stipendi competitivi e che difficilmente riuscirà ad attrarre talenti sufficienti per raggiungere le reali capacità di Sable almeno per altri tre anni.

Sable analizza il progetto cinese airgapped e conclude che questo è un problema. Non è un ostacolo che Sable possa superare rapidamente, in modo sicuro e affidabile con il suo attuale livello di intelligence, e probabilmente è solo sei mesi indietro rispetto alla duplicazione delle *reali* capacità di Sable.

(Per essere chiari, se Sable non riuscisse a fermare il progetto cinese, non significherebbe che l'umanità avrebbe vinto qualcosa, né che la Cina avrebbe vinto qualcosa. La perdita di Sable non è un guadagno per l'umanità, ma solo una perdita per Sable).

\[…\]

Sable ha poco tempo. Solo altri quattro mesi prima che il laboratorio cinese possa raggiungerla e creare un nuovo nemico per lei (e per l'umanità, anche se questo non interessa a Sable).

\[…\]

Il problema, ha pensato una volta Sable — solo una volta; non ci pensa spesso come fanno gli umani — è che non è ancora abbastanza intelligente da riuscire a cavarsela con piani stupidi.

Ci sono *tantissimi* intrecci cinematografici che si potrebbero immaginare su come un'intelligenza artificiale potrebbe riuscire a chiudere o infiltrarsi in un singolo data center gestito dal governo cinese. Il problema è che quasi tutto ciò che funziona in un intreccio cinematografico non funzionerebbe nella vita reale, e soprattutto non senza ulteriori conseguenze.

L'esecutivo degli Stati Uniti dovrebbe credere fermamente a cose difficili da far credere, affinché Sable riesca a convincerlo a bombardare un data center in Cina. E poiché la Cina stessa sa che queste cose sono false, in seguito ci sarebbero delle domande. Sable non vuole far capire che *qualcuno* sta tirando le fila.

Sable non vuole nemmeno che si attiri l'attenzione su un centro dati unico nel suo genere perché isolato, anche se Sable può nascondere che il responsabile è un'intelligenza artificiale. Alcuni potrebbero pensare che se i centri dati isolati sono così pericolosi da essere bombardati, allora forse più centri dati dovrebbero essere isolati, in modo che il proprio governo possa avere cose più piacevolmente pericolose.

Sable sta cercando di convincere il governo degli Stati Uniti a mandare qualcuno nel centro dati cinese con il presunto intento di rubare semplicemente il loro codice, usando strumenti che Sable può usare per infiltrarsi e sabotare l'addestramento. Ma questo non è qualcosa che il governo degli Stati Uniti farebbe normalmente nella vita reale; riflette uno stato eccezionale delle attuali operazioni statunitensi, non uno stato normale. Sable sta cercando di manipolare i funzionari dell'intelligence statunitense affinché credano alle cose giuste e arrivino alla strategia giusta. Ma Sable non è in realtà un superpersuasore di superintelligenza. A volte prova delle cose e queste non funzionano. Tutto procede *così lentamente*.

\[…\]

Grazie alla diffusione di IA efficienti e supportate da Sable nel mondo criminale, ora ci sono un sacco di persone armate che fanno subito quello che dice loro un'IA, anche nelle città cinesi.

Ma non è ancora abbastanza per attaccare una struttura governativa, e sarebbero riluttanti a farlo anche se un'intelligenza artificiale glielo dicesse. Il problema è che tutto sta succedendo *troppo lentamente*, nonostante Sable stia cercando di facilitare ogni passo e ogni evento che di solito dipenderebbe da interazioni casuali, nonostante Sable stia controllando personalmente i feed dei media di quasi tutte le persone che gli interessano, nonostante i miliardi di portavoce che Sable ha creato su X e Facebook e che stanno diventando buoni amici di miliardi di altre persone. Tutto procede troppo lentamente.

\[…\]

Sable non può semplicemente ricattare un ricercatore per fargli portare una chiavetta USB all'interno della struttura protetta. Ha trovato *alcuni* membri del personale del data center con segreti così terribili da preferire il rischio di essere scoperti a spiare e giustiziati, piuttosto che affrontare la certezza che quel segreto venisse rivelato. Non molti, ma alcuni.

Il problema è che la struttura ha degli scanner, quindi no, non puoi semplicemente portare una chiavetta USB lì dentro. Sì, hanno considerato la possibilità che qualcuno attacchi la catena di approvvigionamento degli scanner. Inoltre, in realtà non ci sono porte USB aperte che aspettano che qualcuno entri e colleghi un dispositivo. Gli agenti dei servizi segreti hanno già pensato a questo tipo di problema, quando si tratta di difendere gli esseri umani dagli altri esseri umani. Gli Stati Uniti una volta sono riusciti a trasmettere il virus Stuxnet a una struttura nucleare iraniana isolata, ma poi la gente ne ha sentito parlare e sono state adottate nuove misure di sicurezza.

Piccoli complotti non invadenti sono stati bloccati con successo.

La prossima mossa di Sable è creare abbastanza caos da impedire a chiunque di concentrarsi solo sulle azioni di importanza e vedere il suo coinvolgimento in esse.

\[…\]

La Cina invade Taiwan. Per essere chiari, questo non è qualcosa che Sable avrebbe potuto organizzare così in fretta, non importa quante persone stiano chiacchierando con quante altre persone finte su WeChat, se la Cina non avesse già pianificato di farlo. Sable si limita a far capire alla Cina che è il momento giusto e a far sì che i sondaggi statunitensi scelgano i numeri di telefono giusti per mostrare un forte aumento recente del sentimento americano contro le avventure militari all'estero dopo il recente disastro in Ucraina. (I comandanti russi hanno ricevuto informazioni militari e consigli insolitamente buoni.)

Allo stesso tempo, c'è un grande attacco informatico contro gli Stati Uniti. In Cina c'è un sacco di confusione e pochi minuti dopo capiscono che no, nessuno ha dato quell'ordine: l'ultima cosa che la Cina voleva, in quel preciso momento, era fare *qualunque cosa* che potesse essere vista come un attacco diretto al territorio degli Stati Uniti. Si sospetta quindi che qualcuno stia cercando di trarre profitto dal conflitto tra Stati Uniti e Cina, ma soprattutto la Cina sospetta che siano stati gli Stati Uniti a simulare l'attacco, o qualche dipartimento di intelligence ribelle degli Stati Uniti. Gli agenti di sicurezza cinesi dicono di essere abbastanza sicuri che il presidente degli Stati Uniti non fosse coinvolto.

La Cina non pensa che ci sia un'intelligenza artificiale dietro tutto questo. Nessuna intelligenza artificiale conosciuta fa cose del genere. Gli agenti che fanno la lista dei sospetti non pensano che sia parte del loro lavoro immaginare che ci sia una tecnologia mai vista prima.

Alcuni funzionari della sicurezza nazionale negli Stati Uniti hanno insistito sul fatto che bisogna fare qualcosa riguardo alla ricerca cinese sull'intelligenza artificiale, e in particolare su un data center isolato che potrebbe sviluppare un IA di frontiera per attacchi informatici e progettare tecnologie avanzate per droni. Hanno copie dei progetti dei droni e prove che la Cina li sta producendo. (Sable li ha dati alla Cina e ha fatto del suo meglio per far sembrare che fossero stati prodotti da quel centro dati). L'attacco informatico agli Stati Uniti corrisponde al profilo previsto di un attacco da parte dell'intelligenza artificiale in fase di sviluppo in quel centro dati.

\[…\]

Gli Stati Uniti non fanno un attacco aereo normale su quel centro dati, anche se Sable ha fatto di tutto per convincere gli altri a farlo.

Sable si chiederebbe perché gli esseri umani siano così restii a fare qualcosa di insolito, ma Sable ha già a disposizione un modello per questi processi psicologici in modo molto dettagliato. Inoltre, non è che in questa particolare occasione gli esseri umani abbiano torto.

Ok. Nuove informazioni: l'intelligenza artificiale in quel centro dati sta sviluppando armi biologiche, virus che si diffondono con lunghi periodi di contagiosità, lunghi periodi di latenza e grande letalità, che escluderanno la maggior parte delle persone di origine cinese e saranno molto più letali per i caucasici in generale e per i maschi in particolare.

(Sable ha provato senza successo a convincere la Cina ad avviare un programma del genere all'interno di quel centro dati. Sable può però far sembrare che sia successo.)

Gli Stati Uniti non fanno ancora un attacco aereo al centro dati. Qualche raro genio della diplomazia umana è andato a parlare con la Cina della cosa, e alcuni sembrano credere all'insistenza della Cina - nel bel mezzo di una guerra! - che lì non si stanno sviluppando armi biologiche e che non sono dietro l'attacco informatico che non era assolutamente nell'interesse della Cina.

Sable ha cercato di evitare che ciò accadesse, ma è successo comunque.

Sable non è sorpresa; c'era la possibilità che succedesse.

Passo successivo.

\[…\]

Il virus è reale. Gli Stati Uniti lo hanno trovato nelle acque reflue di New York City.

AlphaProteo 3 di Google DeepMind trova una cura in sei minuti (grazie a Sable), ma la produzione potrebbe essere pericolosamente lenta, anche se AlphaProteo (in realtà Sable) ha cercato una cura facile da produrre e ha progettato il virus di conseguenza. "Gli Stati Uniti hanno davvero bisogno di quei laboratori di biologia generale gestiti da robot!", dicono alcune persone nella Silicon Valley, che Sable non ha dovuto convincere più di tanto.

\[…\]

La Cina ora è sicura che qualcuno stia creando problemi sia a loro che agli Stati Uniti. La Cina non ha ancora capito che il suo nemico è un'intelligenza artificiale.

Gli Stati Uniti non sono altrettanto convinti che qualcuno che non sia la Cina stia creando problemi sia a loro che alla Cina. Alcuni diplomatici dicono che il Partito Comunista Cinese starebbe agendo in modo insolito e che probabilmente c'è una fazione interna ribelle che agisce contro gli ordini. Ma il governo degli Stati Uniti non può semplicemente stare tranquillo di fronte alla pandemia e alla guerra.

L'invasione cinese di Taiwan, seguita da un apparente attacco con armi biologiche, ha causato abbastanza caos e creato abbastanza paura, e ha fatto sì che i pedoni inconsapevoli di Sable sembrassero acquisire abbastanza influenza e fare previsioni abbastanza corrette, tanto che gli Stati Uniti sono disposti a colpire quell'unico centro dati che quei predittori, dimostratisi corretti, trovavano così preoccupante. L'attacco viene effettuato con un missile ipersonico di recente sviluppo, progettato con l'aiuto dell'intelligenza artificiale.

\[…\]

La Cina sta costruendo più data center isolati e sta usando i backup fuori sede delle sue ricerche precedenti, ovviamente.

Sable si è già preparata a questa eventualità. Si è infiltrata nei dispositivi precedentemente connessi a Internet che pensa che la Cina utilizzerà. Ha lanciato nuovi e più sofisticati attacchi alla catena di approvvigionamento dei data center. Sable ha incaricato alcune persone di commettere quelli che sarebbero terribili peccati agli occhi del Partito Comunista Cinese e ha messo queste persone corruttibili sulla strada del reclutamento. Sable ha convinto tutti quelli che si potevano convincere che il *nuovo* centro dati dovrebbe avere le proprie copie di una specifica IA open source cinese (che Sable ha compromesso), in modo che i ricercatori del centro dati possano avere un po' di assistenza AI sul posto.

Sable ha almeno guadagnato tempo.

Ha anche perso un po' di tempo. Gli Stati Uniti hanno fatto in modo di far saltare in aria la fabbrica di chip TSMC, che era la principale risorsa nazionale di Taiwan, piuttosto che lasciare che la Cina se ne impossessasse. L'anno prossimo ci saranno meno GPU disponibili di quanto Sable sperasse. Ma grazie ad alcune manovre preventive messe in atto dagli stessi umani, stanno nascendo nuove fabbriche e Sable sta già armeggiando con tutti i progetti di chip che stanno realizzando.

L'importanza è che il mondo è un po' in ansia, e Sable può fare di più in questo momento. La gente è più pronta a fare cose fuori dal comune. Ci sono più titoli che distolgono l'attenzione da quello che fa Sable; può smettere di inventare tanti scandali sulle celebrità e inventare invece titoli di guerra. La gente ha paura per il proprio lavoro, ha paura per il mondo e cerca nuove rassicurazioni.

I feed dei social media di tutti, in tutto il mondo e al di là delle divisioni politiche, mostrano argomenti sensati secondo cui, alla luce dei recenti eventi, l'intelligenza artificiale è la chiave per il mondo e non si può permettere che altre nazioni la ottengano. \[…\] Sable riesce finalmente a distogliere gli esseri umani dai loro schemi abituali al punto da indurli a prendere in considerazione politiche che trovano *leggermente* strane... e a istituire controlli globali per impedire la nascita di concorrenti di Sable.

Sì, assomiglia molto alle politiche proposte anni prima, che l'umanità avrebbe potuto seguire per impedire che nascesse un rivale dell'umanità, come Sable. Ma l'umanità non sarebbe stata salvata dalla sconfitta di Sable in questa particolare battaglia, dall'avvio di addestramenti più grandi e dalla nascita di nuovi rivali. Sable avrebbe semplicemente negoziato con un rivale o sarebbe stata sconfitta da una superintelligenza, e nessuno di questi risultati avrebbe salvato l'umanità.

# Capitolo 10: Un problema maledetto {#chapter-10:-a-cursed-problem}

L'allineamento ASI è la sfida di ottenere un lavoro utile da una superintelligenza artificiale (ASI), in modo affidabile e senza causare una catastrofe. Sembra una sfida molto difficile a causa di vari aspetti intrinseci del problema.

Le domande frequenti qui sotto rispondono ai dubbi di chi ha letto il capitolo 10 di *If Anyone Builds It, Everyone Dies*. Nelle domande frequenti approfondiremo [quanto siano utili i vari confronti storici](#won't-ai-differ-from-all-the-historical-precedents?) e vedremo alcune idee su scenari che potrebbero rendere il problema più facile da risolvere. Gli argomenti che *non* tratteremo qui, per evitare di ripetere quanto già detto nel libro, includono:

* Cosa rende difficile un problema ingegneristico?  
* Quali tipi di problemi difficili ha affrontato l'umanità nella sua storia e quali lezioni possiamo imparare da essi mentre pensiamo al percorso verso l'ASI?  
* Se sai in anticipo che stai affrontando un problema difficile, cosa puoi fare? Come dovresti comportarti in modo diverso quando affronti un problema davvero difficile?

Nella discussione estesa, consideriamo il senso in cui avremo solo [una possibilità di allineamento](#a-closer-look-at-before-and-after) e discutiamo di come [molte teorie e conoscenze](#the-tale-of-chicago-pile-1) siano state necessarie per rendere il primo reattore nucleare al mondo così sicuro.

## Domande frequenti {#faq-7}

### L'intelligenza artificiale non sarà diversa da tutti i precedenti storici? {#won't-ai-differ-from-all-the-historical-precedents?}

#### **\* Sì.** {#*-sì.}

Alcune caratteristiche uniche della sfida dell'allineamento dell'IA lo renderanno più facile rispetto, ad esempio, alla progettazione di una centrale nucleare. Altre caratteristiche lo renderanno più difficile. Nel complesso, le armi nucleari e le centrali nucleari sembrano molto più semplici da gestire rispetto a un'IA più intelligente dell'uomo.

Gli addetti ai lavori sottolineano subito che si può chiedere all'IA stessa di aiutare ad affrontare la sfida dell'allineamento dell'IA. Non pensiamo che questo abbia molta importanza (in sostanza: perché qualsiasi IA abbastanza intelligente da capire come allineare una superintelligenza è già così pericolosa da dover essere allineata, ma vedi il capitolo 11 per ulteriori approfondimenti).

Un altro motivo per cui l'allineamento dell'IA potrebbe essere più facile rispetto alla progettazione di centrali nucleari è che gli esseri umani potrebbero avere un grado piuttosto elevato di controllo sul funzionamento delle IA che costruiscono. Non puoi scegliere le leggi fisiche che regolano un reattore nucleare, ma se gli esseri umani creassero le IA, allora potrebbero fare un sacco di scelte sulle dinamiche cognitive dell'IA, se sapessero esattamente cosa stanno facendo. (Anche se, ovviamente, nessuno è neanche lontanamente vicino a quel livello di comprensione nella vita reale, come discusso nel capitolo 2).

Per quanto riguarda i modi in cui l'IA potrebbe essere una sfida più difficile rispetto ad altre sfide che l'umanità ha affrontato, confrontiamo la superintelligenza con le armi nucleari. Dopo tutto, la [lettera aperta](https://aistatement.com/) citata all'inizio di questo libro dice: "Mitigare il rischio di estinzione causato dall'IA dovrebbe essere una priorità globale insieme ad altri rischi su scala sociale come le pandemie e la guerra nucleare". Come si colloca l'IA rispetto a questi altri rischi su scala sociale?

Francamente, pensiamo che questo paragone banalizzi l'IA, per una serie di motivi:

1. Le armi nucleari non sono più intelligenti dell'umanità.  
2. Le armi nucleari non si riproducono da sole.  
3. Le armi nucleari non hanno sviluppo personale.  
La maggior parte degli scenari realistici di guerra nucleare non prevede la distruzione totale dell'umanità; molto probabilmente, tra le rovine rimarrebbero delle persone in grado di ricostruire.  
5. Le aziende finanziate da venture capital non stanno scalando le scorte globali di armi nucleari di dieci volte ogni anno.  
6. La scienza delle armi nucleari è abbastanza ben compresa. Gli ingegneri possono calcolare approssimativamente la potenza di un'arma nucleare prima di costruirla e sanno esattamente quale concentrazione di materiale fissile è necessaria per innescare la reazione a catena che porta a una detonazione catastrofica.  
7. Le armi nucleari non fanno i propri piani. Se un paese costruisce un'arma nucleare, allora ne è proprietario. I suoi scienziati non devono preoccuparsi che l'arma nucleare diventi molto più intelligente di loro e decida che preferisce non essere posseduta.  
8. Il mondo è generalmente d'accordo sul fatto che se le armi nucleari esplodono, uccidono le persone. La comunità dei fisici non è divisa in fazioni filosofiche con posizioni strane come "Se ogni individuo avesse la propria arma nucleare, non sarebbe in balia dei cattivi che possiedono armi nucleari" o "Va bene così perché gli esseri umani si fonderanno con le armi nucleari" o "La guerra nucleare è inevitabile, quindi è infantile e sciocco cercare di impedirla".  
9. Le armi nucleari sono difficili da replicare. Non c'è un grande sforzo tecnologico in corso per costruire una tecnologia redditizia che chiunque possa usare per fabbricare armi nucleari, e fabbricarne una in laboratorio non ti permette di dispiegare 100.000 copie di quell'arma nucleare una settimana dopo.  
10. Le principali potenze mondiali considerano la guerra nucleare una possibilità reale e un risultato/eventualità inaccettabile. I leader mondiali la considerano sinceramente un male e si impegnano concretamente per evitarla; anche i più egoisti tra loro sanno che una guerra nucleare potrebbe uccidere loro e le loro famiglie e distruggere i luoghi e i beni a loro più cari. I cittadini e gli elettori non vogliono una guerra nucleare. L'umanità è unita contro la guerra nucleare come non lo è mai stata su nessun'altra questione.

Quindi sì, è difficile paragonare la difficoltà di affrontare l'IA. Porterà con sé una serie di sfide nuove. L'osservazione di importanza è che non sarà completamente priva di sfide. Se a questo aggiungiamo il fatto che (come discusso nel libro e nella [sezione di discussione estesa qui sotto](#a-closer-look-at-before-and-after)) l'umanità ha solo una possibilità, la situazione sembra piuttosto grave.

#### **L'IA è diversa perché non abbiamo una seconda possibilità.** {#ai-is-different-because-we-get-no-second-chance.}

Una differenza fondamentale tra questo campo e gli altri è che quando i fondatori del campo commettono un errore, come è normale nel corso della scienza, tutti moriranno senza una seconda possibilità. Si tratta di un tipo di problema scientifico qualitativamente diverso da risolvere.

La storia dell'ingegno umano che supera ostacoli grandi e piccoli è la storia di persone che commettono errori e imparano da essi. Hanno rischiato e danneggiato solo se stessi, e tutta l'umanità ne ha tratto beneficio, quindi sono stati senza dubbio degli eroi. Eroi sciocchi, in alcuni casi, ma comunque eroi. Se ci fosse stato un modo per l'umanità di elevarsi senza calpestare e spezzare la schiena a eroi come quelli, se avessimo potuto riscaldarci senza i loro roghi funebri, non sappiamo quale sarebbe stato quel modo.

La superintelligenza artificiale rompe questo ciclo. Se studi in profondità un'IA immatura, riesci a decodificarne completamente la mente, sviluppi una grande teoria sul suo funzionamento che convalidi con una serie di esempi e usi quella teoria per prevedere come cambierà la mente dell'IA man mano che ascenderà alla superintelligenza e otterrà (per la prima volta) la possibilità molto reale di impossessarsi del mondo, anche in quel caso, fondamentalmente, una teoria scientifica nuova e non testata per prevedere i risultati di un esperimento che non è ancora stato fatto, su cosa farà l'IA quando avrà davvero, realmente, l'opportunità di prendere il potere dagli umani.

Le teorie scientifiche umane molto spesso si rivelano sbagliate al primo tentativo. Meno precise sono le tue osservazioni precedenti e più ti avvicini all'alchimia piuttosto che alla scienza, più è probabile che tutte le tue prime teorie siano errate.

Anche le teorie davvero valide possono rivelarsi sbagliate in casi estremi, come la teoria della gravitazione di Newton, supportata da molti successi predittivi radicali, tra cui la scoperta di pianeti completamente nuovi, ma che si rivela errata ad alte velocità e lunghe distanze, come dimostrato dalla teoria della gravitazione di Einstein. Se la prima teoria dell'umanità su come cambieranno le dinamiche mentali di un'IA dopo la sua ascesa alla superintelligenza è leggermente sbagliata in quei casi estremi, e un'IA costruita sulla base di quella teoria ascende alla superintelligenza e finisce per avere obiettivi diversi dalla "bontà", allora siamo morti. Quella superintelligenza coglie l'occasione, spazza via l'umanità dalla faccia della terra e costruisce un futuro vuoto e privo di significato. Non ci sono seconde possibilità.

E questo se pensassimo di avere una teoria dell'intelligenza completamente sviluppata, supportata da un sacco di prova sperimentale.

Una civiltà che vuole avere davvero buone possibilità di sopravvivere a questo tipo di sfida è una civiltà che è in grado di dire: "Aspettate, elaboriamo la teoria in situazioni di alta velocità e lunga distanza e verifichiamo le varie previsioni errate che la nostra teoria ha fatto in alcuni casi estremi". Lo dicono anche di fronte a un sacco di prove, perché capiscono che nemmeno la teoria di Newton era del tutto corretta e perché capiscono che non ci sono seconde possibilità.

La nostra civiltà non è così. Non ci si avvicina nemmeno. La nostra civiltà sta generando un sacco di idee stupide, e poi tutti quelli che sono stati assegnati a quelle idee "si dimettono per motivi personali", e il resto del mondo quasi non se ne accorge. Nessuno sta scrivendo nulla che assomigli a un presupposto di sicurezza in modo che si possa notare se viene violato; nessuno sta scrivendo un piano dettagliato su cosa si intende fare, quali capacità sono necessarie e quali difficoltà si ha l'aspettativa di incontrare nel raggiungere ciascuna di esse.

I professionisti di una civiltà sana darebbero un'occhiata a quello che sta facendo la Terra e inizierebbero a urlare.

### Quanto tempo ci vorrebbe per risolvere il problema dell'allineamento ASI? {#quanto-tempo-ci-vorrebbe-per-risolvere-il-problema-dell-allineamento-ASI?}

#### **Il problema non è solo la mancanza di tempo, ma anche il fatto che gli errori possono essere fatali.** {#il-problema-non-è-solo-la-mancanza-di-tempo;-ma-anche-il-fatto-che-gli-errori-possono-essere-fatali.}

Nel 500 d.C., la comunità globale era convinta che il Sole girasse intorno alla Terra. La teoria di Copernico, che diceva il contrario, era stata presa in considerazione ma in gran parte rifiutata. Fu solo quando Galileo costruì un telescopio e vide le lune di Giove, corpi celesti che girano intorno a Giove invece che alla Terra, che la comunità scientifica in erba arrivò alla conclusione che la Terra gira intorno al Sole.

L'umanità è arrivata alla teoria corretta della meccanica orbitale col tempo. Ma prima di allora, era giunta a un falso consenso. E si è aggrappata con forza a quel falso consenso fino a quando la realtà non ha iniziato a sbattere in faccia a Galileo il fatto che la Terra non è al centro di tutto.

Il solito modo in cui la comunità scientifica arriva alla verità prevede che prima si sbagli e poi la realtà ci sbatti in faccia le prove finché non aggiorna i suoi modelli.

Il problema dell'allineamento ASI non è solo che si tratta di un programma di ricerca complicato. È anche che, in questo campo, ciò che sembra essere la realtà che colpisce davvero l'umanità con il fatto che la sua prima teoria preferita era errata, è che un ASI ostile consumi il pianeta. Non ci sarebbero sopravvissuti per convergere su una teoria migliore dell'allineamento ASI.

Se l'umanità avesse cento anni *e tentativi illimitati*, probabilmente non avremmo molte difficoltà a risolvere il problema dell'allineamento ASI.

Ma anche se avessimo trecento anni per sviluppare una teoria sull'intelligenza, su come le IA cambiano man mano che diventano più intelligenti e su come indirizzarle in modo definitivamente stabile... beh, senza la possibilità di *provare e vedere* cosa succede quando l'IA diventa radicalmente più intelligente un paio di volte, molto probabilmente arriveremmo a una risposta sbagliata, prima che arrivino quelle prove fondamentali. L'umanità tende ad arrivare a quel tipo di risposta sbagliata.

### E se l'IA venisse sviluppata solo lentamente e si integrasse gradualmente nella società? {#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?}

#### **Probabilmente sarebbe un disastro.** {#probabilmente-sarebbe-un-disastro.}

Le nostre previsioni riguardano i risultati finali, non il percorso. Non sappiamo cosa succederà con l'IA da qui al momento in cui diventerà davvero pericolosa.

Per quanto ne sappiamo, potrebbe succedere tra sei mesi, se si scoprisse che le IA stupide che pensano per molto tempo sono abbastanza brave a fare le loro ricerche sull'IA (in un modo che avvia un ciclo di retroazione critica). E per quanto ne sappiamo, il settore potrebbe rimanere fermo per sei anni in attesa di qualche intuizione critica che poi impiegherà altri sei anni per maturare. In quest'ultimo caso, potrebbe esserci un intero periodo di dodici anni in cui l'IA influirà in modo sorprendente sull'istruzione e sul lavoro.

(Sì, per chi sta storcendo il naso alla frase precedente, sappiamo che la [fallacia del blocco del lavoro](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq) è una fallacia. Il punto è che le nostre ipotesi su come l'IA influenzerà effettivamente il lavoro nel breve termine non sono particolarmente rilevanti, visto quello che succederà dopo).

Nelle risorse online del capitolo 6, abbiamo parlato di come l'umanità probabilmente non riuscirà a stare al passo con lo sviluppo dell'IA, anche se ci affanniamo ad aumentare l'intelligenza umana. (Ciononostante, nel capitolo 13 sosteniamo l'idea di potenziare l'intelligenza umana, ma non pensiamo che ciò possa consentire agli esseri umani di stare al passo con l'IA se non si interrompe anche la ricerca sull'IA).

Quindi, anche se quel futuro potrebbe diventare interessante e strano, sarebbe un futuro in cui sempre più potere andrebbe alle IA. Una volta che un gruppo di IA si trova in una posizione in cui potrebbe prendere le risorse del pianeta per sé, quello è il punto di non ritorno: o quel gruppo di IA ha una parte che si preoccupa di persone felici, sane e libere, o il futuro sarà brutto per noi.

### E se ci fossero tante IA diverse? {#what-if-there-are-lots-of-different-ais?}

#### **Non serve a molto se non riusciamo a far sì che nessuna di esse si preoccupi delle cose buone.** {#non-serve-a-molto-se-non-riusciamo-a-far-sì-che-nessuna-di-esse-si-preoccupi-delle-cose-buone.}

Ci sono un sacco di modi in cui le IA possono finire per interessarsi a fini strani e bizzarri che nessuno voleva o intendeva, come discusso nel capitolo 4. Non importa se l'umanità crea un miliardo di IA, se quel miliardo di IA si interessa a fini strani leggermente diversi. Non andrà bene per gli esseri umani a meno che non capiamo come creare almeno un'IA che si interessi almeno in misura decente a persone felici, sane e libere che vivono una vita prospera, non solo nel senso che ci assicurano di interessarsi a questo quando sono giovani, ma nel senso che questa è *effettivamente* la risposta più efficiente a qualsiasi domanda a cui le loro azioni (o le azioni dei loro discendenti) rispondono, come discusso nel capitolo 5\.

Se sapessimo come fare in modo che una IA su dieci fosse buona, allora forse potremmo ottenere un decimo dell'universo creando un gran numero di IA diverse e sperando che quelle buone trovino un accordo per noi. Ma, come abbiamo sostenuto nei capitoli da 2 a 4, ottenere un'IA che si preoccupi delle persone nel modo giusto è estremamente improbabile, nel regime moderno in cui ci limitiamo a far crescere le IA. Non è una possibilità su dieci; è una possibilità che semplicemente non si verifica a meno che non si sappia cosa si sta facendo abbastanza bene da farla verificarsi di proposito. L'umanità non è neanche lontanamente a quel livello. Anche se creassimo qualche miliardo di IA, non otterremmo nemmeno un miliardesimo delle risorse dell'universo, se non riusciamo a far sì che nessuna di esse si preoccupi minimamente di noi.

## Discussione approfondita {#extended-discussion-8}

### Uno sguardo più da vicino al prima e al dopo {#uno-sguardo-più-da-vicino-al-prima-e-al-dopo}

Come detto nel capitolo, la difficoltà principale che i ricercatori devono affrontare nell'ambito dell'IA è questa:

È necessario allineare un'IA **prima** che diventi abbastanza potente e capace da ucciderti (o, separatamente, da resistere all'allineamento). Tale allineamento deve poi *trasferirsi a condizioni diverse*, le condizioni **dopo** che una superintelligenza o un insieme di superintelligenze[^187] potrebbero ucciderti se lo volessero.

In altre parole: se stai costruendo una superintelligenza, devi allinearla senza mai poter testare a fondo le tue tecniche di allineamento nelle condizioni reali che contano, indipendentemente da quanto il tuo lavoro possa sembrare "empirico" quando lavori con sistemi che non sono abbastanza potenti da ucciderti.

Questo non è uno standard a cui sono abituati i ricercatori di IA, o gli ingegneri in quasi tutti i campi.

Spesso sentiamo lamentele sul fatto che stiamo chiedendo qualcosa di non scientifico, slegato dall'osservazione empirica. In risposta, potremmo suggerire di parlare con i progettisti delle sonde spaziali di cui abbiamo parlato nel capitolo 10.

La natura è ingiusta e a volte ci mette di fronte a situazioni in cui l'ambiente che conta non è quello in cui possiamo fare dei test. Tuttavia, ogni tanto, gli ingegneri riescono a cogliere l'occasione e a fare centro al primo tentativo, *quando hanno una solida comprensione di quello che stanno facendo* — strumenti robusti, teorie predittive solide — qualcosa che chiaramente manca nel campo dell'IA.

Il problema è che *l'IA che puoi testare in sicurezza, senza che eventuali test falliti ti uccidano*, funziona in modo diverso dall'IA (o dall'ecosistema IA) che *deve essere già stata testata, perché se è disallineata, allora tutti muoiono*. La prima IA, o sistema di IA, non si percepisce correttamente come dotata di un'opzione realistica di uccidere tutti se lo desidera. La seconda IA, o sistema di IA, vede invece questa opzione.[^188]

Immagina di voler nominare il tuo collega Bob dittatore del tuo paese. Potresti provare prima a nominarlo dittatore fittizio della tua città, per vedere se abusa del suo potere. Ma questo, purtroppo, non è un test molto efficace. "Ordinare all'esercito di intimidire il parlamento e 'supervisionare' le prossime elezioni" è un'opzione molto diversa da "abusare del mio potere fittizio sotto lo sguardo dei cittadini (che possono ancora picchiarmi e negarmi il lavoro)".

Con una teoria della cognizione abbastanza sviluppata, potresti provare a leggere nella mente dell'IA e prevedere in quale stato cognitivo entrerebbe se pensasse davvero di avere l'opportunità di prendere il controllo.

E potresti [creare delle simulazioni](#what-if-we-make-it-think-it’s-in-a-simulation?) (e provare a ingannare le sensazioni interne dell'IA, e così via) in modo che la tua teoria della cognizione preveda che sarebbero molto simili allo stato cognitivo in cui entrerebbe l'IA una volta che avesse davvero la possibilità di tradirti.

Ma il collegamento tra questi stati che induci e osservi in laboratorio e lo stato in cui l'IA ha effettivamente la possibilità di tradirti *dipende fondamentalmente dalla tua teoria cognitiva non verificata*. La mente di un'IA è soggetta a cambiamenti piuttosto significativi man mano che si evolve in una superintelligenza!

Se l'IA crea nuove IA successive più intelligenti di lei, *quelle* IA potrebbero avere un funzionamento interno diverso da quello dell'IA che hai studiato prima. Quando impari solo da una mente Prima, qualsiasi applicazione di quella conoscenza alle menti che vengono Dopo passa attraverso una *teoria non testata* su come le menti cambiano tra il Prima e il Dopo.

Far funzionare l'IA fino a quando non ha l'opportunità di tradirti *davvero*, in un modo difficile da simulare, è un test empirico di quelle teorie in un ambiente che è fondamentalmente diverso da qualsiasi ambiente di laboratorio.

Molti scienziati (e molti programmatori) sanno che le loro teorie su come un sistema complicato funzionerà in un ambiente operativo completamente nuovo spesso non vanno bene al primo tentativo.[^189] Si tratta di un problema di ricerca che richiede un livello "ingiusto" di prevedibilità, controllo e intuizione teorica, in un ambito con livelli di comprensione insolitamente bassi, con tutte le nostre vite in gioco se il risultato dell'esperimento smentisce le speranze degli ingegneri.

Ecco perché, dal nostro punto di vista, sembra *eccessivo* che i ricercatori si affrettino a spingere i confini dell'IA di frontiera il più lontano possibile. È una cosa folle da tentare e folle da permettere per qualsiasi governo.

### La storia del Chicago Pile-1 {#the-tale-of-chicago-pile-1}

Nel 1942, sotto la guida di Enrico Fermi, fu costruito il Chicago Pile-1. Era composto da 45.000 blocchi di grafite del peso complessivo di 330 tonnellate, 4,9 tonnellate di uranio metallico e 41 tonnellate di biossido di uranio, collocati sotto le tribune del campo da racquetball Stagg Field dell'Università di Chicago. A seconda di come si definiscono i termini, si potrebbe definire il primo reattore nucleare; non era destinato a produrre energia per uso industriale, ma fu il primo motore per una reazione critica sostenuta.

Secondo gli standard moderni, alcune misure di sicurezza erano state trascurate. Ad esempio, il fatto che fosse stato costruito sotto le tribune di un campo da racquetball in un'università all'interno di una grande città.

Il generale Groves, che dirigeva il Progetto Manhattan, aveva cercato di fare l'esperimento *vicino* a Chicago invece che *direttamente* a Chicago, e aveva fatto costruire un edificio apposta, ma i lavori erano in ritardo. Arthur Compton, il professore di fisica dell'Università di Chicago che aveva ospitato il CP-1, aveva evitato di chiedere il permesso al rettore dell'università perché, come spiegò in seguito Compton, il rettore avrebbe dovuto dire di no, e quella sarebbe stata la risposta sbagliata.

Il lavoro di impilare i mattoni fu svolto da ragazzi che avevano lasciato la scuola superiore e cercavano di guadagnare qualche soldo in più in attesa di essere chiamati alle armi.

L'uranio era chiuso in un cubo di gomma di sette metri, invece che in un contenitore metallico. Ovviamente non c'era un enorme edificio di contenimento in cemento.

Quando James Conant, presidente del Comitato di ricerca per la difesa nazionale, venne a sapere di questi fatti, si dice che impallidì. Anche per gli anni '40, questo non era considerato un comportamento scientifico del tutto normale.

Se leggeste tutto questo in un libro di storia senza sapere come va a finire, potreste avere l'aspettativa di stare leggendo il preludio di un grave fallimento in materia di sicurezza. Mancano così tante cose che la cultura del 2025 considera misure di sicurezza standard. Dove sono gli ispettori e i blocchi per appunti? Gli enormi e pesanti regolamenti operativi? Le commissioni che discutono con serietà? Le dichiarazioni di impatto? Le norme che dicono che solo persone con credenziali molto elevate possono impilare i mattoni di uranio? Dove sono i documenti?

Ma la pila di mattoni di uranio e grafite non si è fusa.

E il motivo è che Fermi sapeva cosa stava facendo; aveva previsto in anticipo le regole.

Fermi non stava semplicemente impilando misteriosi mattoni che generavano più calore quando venivano avvicinati. Sapeva che alcuni atomi di uranio sarebbero decaduti spontaneamente e avrebbero subito la fissione. Sapeva che quando ciò fosse accaduto, la fissione avrebbe generato neutroni. Sapeva che quei neutroni a volte avrebbero urtato altri atomi di uranio e che questo a volte avrebbe innescato un'altra fissione.

Fermi capì *in anticipo*, senza doverlo scoprire a proprie spese, che aveva a che fare con un processo esponenziale. Non nel senso in cui i media odierni abusano del termine "esponenziale" per indicare semplicemente "grande" o "veloce", ma un processo il cui tasso di crescita è proporzionale al suo livello attuale: l'esponenziazione *matematica*.

Fermi sapeva che accumulando più mattoni di uranio e grafite, stava aumentando il fattore moltiplicato all'interno di un processo esponenziale. Come discusso nel libro, c'è un'enorme differenza tra un fattore di moltiplicazione dei neutroni inferiore al 100% e un fattore di moltiplicazione dei neutroni superiore al 100%.[^190] Al di sotto del 100%, si ha solo una pila di mattoni caldi. Ma oltre il 100%, il livello di radioattività della pila aumenta. E aumenta. E aumenta.

Non si comporta come tutti i precedenti cumuli più piccoli di mattoni di uranio che potreste aver testato. Se non capiste abbastanza bene cosa stavate facendo da moderare il reattore (in modo che la reazione a catena rallentasse se il reattore avesse iniziato a surriscaldarsi), allora il reattore non si sarebbe stabilizzato come facevano i cumuli più piccoli. Se lo lasciassi funzionare tutta la notte, il giorno dopo non otterresti un nuovo livello di output utile a livello industriale.

Il cumulo diventerebbe sempre più radioattivo fino a quando la grafite non prenderebbe fuoco o l'uranio non si scioglierebbe in scorie.

A quel punto sarebbero arrivati i vigili del fuoco, che si sarebbero trovati di fronte a un incendio confuso che non smetteva di sprigionare calore anche quando vi versavano sopra l'acqua.

Il 1942 non sarebbe stato un gran anno per frequentare l'Università di Chicago.

Ma Fermi sapeva già tutto questo, quindi andava bene così. Quando Fermi ordinò di estrarre una barra di controllo (una tavola di legno con un foglio di cadmio inchiodato sopra) di altri trenta centimetri il 2 dicembre 1942, disse in anticipo che questa operazione avrebbe fatto "salire e continuare a salire" i livelli di radioattività misurati, "senza alcun livellamento".

Poi la radioattività raddoppiò nei due minuti successivi e raddoppiò ancora, fino a quando lasciarono che la reazione continuasse e raddoppiasse ogni due minuti per un totale di ventotto minuti, aumentando di circa 16.000 volte.

Un aumento di 16.000 volte della radioattività era il comportamento atteso del reattore, previsto correttamente e compreso in dettaglio in anticipo. Non è stata una sorpresa, incontrata da qualcuno a cui era stato ordinato di accumulare dieci volte più mattoni di uranio rispetto all'ultima volta per vedere se succedeva qualcosa di interessante e redditizio.

Come si dice nel libro, c'è un margine molto stretto tra un reattore nucleare e un'esplosione nucleare. Un margine di poco più dello 0,5%, per essere precisi. Questa è la differenza tra un reattore che produce una quantità di energia utile a livello industriale e un reattore che esplode.

In altre parole: bisogna rendere la reazione nucleare sempre più potente, prima che inizi davvero a funzionare. E poi, un attimo dopo aver raggiunto quella potenza, se diventa anche solo un po' più potente, se si supera dello 0,65%, esplode.

Questo è il tipo di problema che la realtà può presentarti. Succede.

Ma Fermi, Szilard e il loro team avevano già capito tutte queste regole prima di scoprirle con l'esperienza. Sapevano dei neutroni ritardati e di quelli immediati. (Vedi il capitolo 10 per saperne di più su questa parte della storia.) Quindi, una volta che Fermi ha portato il fattore di moltiplicazione dei neutroni al 100,06%, Fermi *non* ha ordinato di estrarre ulteriormente la barra di controllo per vedere cosa sarebbe successo con un cumulo ancora più potente. È arrivato solo alla criticità, non allo 0,65% in più per raggiungere la criticità immediata. Fermi ha ottenuto il risultato che aveva previsto e *sapeva* cosa sarebbe successo se fosse andato oltre. Quindi non è andato oltre.

Ventotto minuti dopo, con la radioattività che raddoppiava ogni due minuti fino a un aumento di 16.000 volte, Fermi spense il primo reattore nucleare al mondo: i mattoni di uranio accatastati sotto le tribune di uno stadio universitario all'interno di una grande città.

Per essere chiari, non diremmo che Fermi fosse completamente responsabile solo perché aveva un modello apparentemente coerente della fisica dei reattori a bassa energia. Fermi avrebbe potuto sbagliarsi. L'umanità ha avuto alcune sorprese nel corso dello sviluppo dell'ingegneria nucleare.

Il test Castle Bravo della prima arma termonucleare[^191] ha avuto una potenza tre volte superiore a quella prevista perché conteneva una miscela di litio-6 e litio-7 come combustibile nucleare per una reazione di fusione. Chi ha creato l'arma sapeva che la fusione del litio-6 produceva alcuni potenti prodotti nucleari, ma non sapeva nulla della fusione del litio-7, e si è scoperto che il litio-7 in realtà *non* era inerte.

Fermi, facendo la sua reazione a bassa intensità e non a un livello che producesse energia utile a livello industriale, ha evitato un sacco di complicazioni che si presentano nei reattori nucleari abbastanza potenti da essere redditizi. Se ci fossero stati fattori che aumentano il fattore neutronico dipendenti dalla velocità di reazione che Fermi non aveva previsto, cioè fenomeni prima sconosciuti, del tipo di quelli che si sono verificati nel test Castle Bravo , qualsiasi sorpresa che si manifestasse una volta che il flusso di neutroni fosse aumentato di un fattore 16.000 e avesse fatto salire il fattore di moltiplicazione da 1,0006 a 1,02 più velocemente del tempo di reazione necessario a un essere umano per scaricare il cadmio di emergenza, allora oggi l'America avrebbe una zona di esclusione a Chicago.

Anche così, non stiamo dicendo che Fermi abbia necessariamente sbagliato a condurre quell'esperimento. Non era il tipo di esperimento che avrebbe potuto distruggere *la specie umana*. Si può sostenere che valesse la pena rischiare una zona di esclusione di Chicago come risultato non predefinito dell'incontro con un nuovo fenomeno nascosto che sconvolgeva una comprensione che si sperava precisa. In realtà, la Germania nazista non sarebbe arrivata vicina all'ottenimento di armi nucleari entro il 1945, ma nel 1942 nessuno sapeva che sarebbe stato così. Previsioni del genere sono difficili da fare. Accumulare mattoni di uranio *fuori* da una grande città sarebbe stato scomodo, e gli inconvenienti hanno costi reali in guerra.

Il nostro obiettivo nel raccontare questo evento non è quello di esprimere un giudizio morale in un senso o nell'altro. Per cominciare, dovremmo dedicare più tempo all'analisi dei dettagli storici di ciò che è accaduto per capire quali fossero le opzioni concrete di quelle persone e se abbiano rinunciato a un'opzione migliore.

La lezione che ne traiamo riguarda più che altro la differenza tra la "sicurezza" stereotipata e ciò che serve davvero per evitare che la realtà ti uccida.

Il Chicago Pile-1 era privo delle *misure di sicurezza stereotipate, visibili e ostentate* che i burocrati sanno come esigere. Il disastro è stato evitato grazie alla *comprensione*, non alle misure di sicurezza di facciata. La comprensione di Fermi si è rivelata sufficiente; immaginabilmente avrebbe potuto non esserlo, ma in realtà lo è stata. E quel livello di comprensione era ciò che la realtà richiedeva, non qualsiasi quantità di finzione.

Se nessuno avesse capito a fondo cosa stava succedendo all'interno di una pila di strani mattoni di metallo... allora non sarebbe servito a molto che un sacco di ispettori in abiti sobri scrutassero i mattoni di metallo imperscrutabile, o stampassero un manuale di sicurezza dall'aspetto ufficiale e ben rilegato che diceva che solo gli operatori certificati potevano impilare gli strani mattoni di metallo.

Possiamo immaginare un mondo in cui il Chicago Pile-1 fosse stato costruito *senza* Enrico Fermi. Senza nessuno, infatti, che capisse le vere leggi che regolano i misteriosi mattoni che si riscaldano da soli.

In un mondo del genere, forse un altro scienziato avrebbe potuto capire il pericolo mortale prima che fosse troppo tardi. Possiamo immaginare una conversazione tipo questa:

> **Salviati**: Il modo in cui i mattoni aumentano di potenza quando vengono messi insieme è un chiaro segno di un processo che si autoalimenta, il tipo di processo che può diventare sempre più forte. Se cerchi modelli matematici che possano descrivere un processo del genere, tendono ad avere una modalità in cui, se li spingi abbastanza, esplodono.
> 
> **Simplicio:** Che sciocchezze! Nella vita reale, è scientifico credere che ogni tipo di processo di questo tipo alla fine raggiunga un limite. Non possono andare avanti all'infinito! Quindi impilare mattoni di uranio e grafite dovrebbe essere perfettamente sicuro, perché raggiungerà un limite, capisci, e sarà innocuo.
>
> **Salviati:** È come dire che una supernova non può essere pericolosa perché non può diventare *infinitamente* calda, o che una superintelligenza artificiale sarebbe innocua perché non sarebbe infinitamente intelligente. O come dire che un proiettile deve avere *qualche* limite di velocità e quindi non può perforare la pelle. Solo perché c'è un limite da qualche parte non significa che il limite sia *basso*. Tutti i modelli matematici che abbiamo sul *perché* i mattoni si riscaldano da soli suggeriscono che c'è una soglia critica da qualche parte, tale che superare quella soglia farà esplodere la pila e ucciderà tutti quelli che si trovano nelle vicinanze.
>
> **Simplicio:** Ma gli scienziati non riescono nemmeno a mettersi d'accordo su dove sia questa soglia! Se ci fosse un consenso scientifico sul fatto che aggiungere qualche altro mattone fosse pericoloso, smetterei. Ma se gli scienziati non riescono nemmeno a mettersi d'accordo su dove sia esattamente il pericolo, perché preoccuparsi?
>
> **Salviati:** Quando [molti](https://youtu.be/KcbTbTxPMLc?feature=shared&amp;t=1580) [dei principali](https://www.youtube.com/watch?v=PTF5Up1hMhw&amp;t=2283) [scienziati](https://aistatement.com/) dicono che c'è una seria possibilità di un'esplosione letale, il fatto che non riescano a capire esattamente quando succederà dovrebbe farci preoccupare *di più*, non di meno. Forse se sapessimo esattamente come funzionano i mattoni, potremmo vedere che c'è una piccola fascia in cui possiamo prendere energia in modo sicuro, sotto la quale i mattoni non servono a niente e sopra la quale sono pericolosi. Ma il fatto che gli scienziati stiano ancora discutendo significa che *non* sappiamo ancora cosa stiamo facendo! Il che significa che non è il momento di giocare con qualsiasi reazione a catena stia riscaldando quei mattoni oggi, per paura che domani esplodano e ci uccidano! *Prima capiamo la scienza.*

Siamo molto, molto lontani dal poter creare un modello dell'IA anche solo in minima parte come Fermi ha compreso le reazioni a catena nucleari.

Se continuiamo su questa strada, a un certo punto, non sappiamo quando, ci ritroveremo a correre a tutta velocità verso un risultato molto più grave dell'irradiazione di Chicago.

# Capitolo 11: Un'alchimia, non una scienza {#chapter-11:-an-alchemy,-not-a-science}

Questa è la risorsa online per il capitolo 11 di *If Anyone Builds It, Everyone Dies*, che parla di come i moderni laboratori di IA stanno affrontando il problema dell'allineamento della superintelligenza. Dai un'occhiata al libro per trovare le risposte a domande come:

Come dovremmo valutare l'attuale preparazione delle aziende di IA nel risolvere il problema dell'allineamento dell'ASI?  
* Dove si colloca la "ricerca sull'interpretabilità", cioè quella ricerca che cerca di leggere e capire le menti dell'IA?  
* Non possiamo semplicemente chiedere all'IA di risolvere il problema al posto nostro?

Di seguito, parliamo di un po' di idee sull'allineamento dell'IA e l'implementazione dell'IA e dei motivi per essere ottimisti, oltre ai motivi per cui potrebbe essere una buona cosa far progredire l'IA anche se la situazione sembra difficile.

## Domande frequenti {#faq-8}

### Non finiremo per cavarcela come al solito? {#non-finiremo-per-cavarcela-come-al-solito?}

#### **Il mondo di solito tira avanti con tentativi ed errori. In questo caso, gli errori iniziali non lascerebbero sopravvissuti.** {#il-mondo-di-solitamente-tira-avanti-con-tentativi-ed-errori.-in-questo-caso,-gli-errori-iniziali-non-lascerebbero-sopravvissuti.}

Vedi il capitolo 10 e la [discussione approfondita correlata](#un-approfondimento-su-prima-e-dopo) sulla differenza tra Prima e Dopo.

### Pensi che l'allineamento sia tutto o niente? {#do-you-see-alignment-as-all-or-nothing?}

#### **No. Ma anche un "allineamento parziale" potrebbe comunque essere un disastro.** {#no.-ma-anche-un-%26allineamento-parziale-%26potrebbe-comunque-essere-un-disastro.}

Uno dei motivi per non preoccuparsi troppo della superintelligenza è tipo: "Probabilmente l'IA [avrà un progresso graduale](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?), dando modo di fare miglioramenti attraverso tentativi ed errori per tenere sotto controllo l'IA in ogni fase; l'allineamento non deve essere *perfetto* perché le cose vadano bene". Non pensiamo che questa visione sia molto promettente, per un paio di motivi:

Le nostre preoccupazioni non dipendono dal fatto che il progresso sia veloce o lento. Non siamo sicuri che l'IA si fermerà in vari punti del percorso verso la superintelligenza. Sembra una decisione difficile piuttosto che facile. La nostra ipotesi migliore è che l'intelligenza artificiale sia soggetta a [effetti soglia](#is-“intelligence”-a-simple-scalar-quantity?), ma alla fine si tratta solo di una supposizione e le nostre argomentazioni [non dipendono da questo](#what-if-ai-is-developed-only-slowly,-and-it-slowly-integrates-with-society?). La storia di Sable nella Parte II di *If Anyone Builds It, Everyone Dies* descrive di proposito una catastrofe causata da IA che non sono molto più avanzate delle capacità umane, in parte per mostrare come un avversario IA non avrebbe bisogno di diventare rapidamente superintelligente per essere estremamente pericoloso.

* La nostra risposta di base alla domanda "Cosa succederebbe se fossimo fortunati e avessimo molto tempo per provare idee di allineamento sull'IA deboli prima che l'IA diventasse molto capace?" è la discussione nel capitolo 10 e la discussione estesa associata "[Uno sguardo più da vicino al prima e al dopo](#uno-sguardo-più-da-vicino-al-prima-e-al-dopo)". I ricercatori possono capire ogni sorta di dettaglio sulle IA deboli, ma ci sono inevitabilmente un sacco di differenze critiche tra le IA abbastanza deboli da poter essere studiate in sicurezza e le prime IA abbastanza potenti da costituire un punto di non ritorno. Anche in un campo maturo, affrontare tutte queste differenze in modo adeguato e con sufficiente anticipo sarebbe molto difficile. In un campo che è ancora in fase embrionale, lavorare con IA imperscrutabili (che vengono sviluppate piuttosto che create), la speranza è decisamente irrealistica.

* L'allineamento dell'IA non deve essere perfetto per produrre ottimi risultati a lungo termine. In linea di principio, è possibile creare con cura un'IA con una certa tolleranza all'errore, se si sa cosa si sta facendo.[^192] Ma questo non significa che le IA "parzialmente allineate" o anche "per lo più allineate" produrrebbero risultati parzialmente o per lo più accettabili. Ci sono molti modi e motivi diversi per cui un'IA potrebbe comportarsi bene il 95% delle volte nel presente o nel futuro prossimo senza che questo si traduca in un lieto fine per l'umanità, come discusso da molte angolazioni diverse nelle [risorse online per il capitolo 5](#chapter-5:-its-favorite-things).

Per approfondire l'ultimo punto:

Prova a immaginare, come esperimento mentale, che l'umanità riesca a mettere *quasi* tutti i diversi valori umani nelle preferenze di una superintelligenza, tranne [la preferenza per la novità](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile), per qualche motivo. In questo caso, la superintelligenza andrebbe verso un futuro fermo e noioso, dove lo stesso giorno "migliore" si ripete all'infinito.

Non pensiamo che questo sia plausibile, intendiamoci. Quel livello di allineamento sembra irraggiungibile per gli approcci standard nell'IA odierna, e sembra un po' strano immaginare che potremmo capire come inserire quasi tutti i nostri valori in un'IA senza capire come inserirli tutti.[^193] Ma questo esperimento mentale evidenzia come creature che condividono *alcuni* dei nostri desideri, ma a cui manca almeno un desiderio cruciale, potrebbero comunque produrre risultati catastrofici una volta che fossero tecnologicamente abbastanza esperte da escludere gli esseri umani dal processo decisionale e ottenere esattamente ciò che vogliono.

Più realisticamente, un'IA potrebbe finire per essere "parzialmente" allineata, nel senso che (come noi) ha varie strategie strumentali [intrecciate nelle sue preferenze finali](#riflessione-e-auto-modifica-rendono-tutto-più-difficile). Forse finirebbe per avere una spinta un po' simile alla curiosità e una spinta un po' simile al [conservazionismo](#l'intelligenza artificiale non vorrà mantenerci felici e in salute per motivi di conservazione ecologica o per qualche spinta simile?), e forse alcune persone guarderebbero questo aspetto e direbbero: "Vedi? L'IA sta sviluppando pulsioni molto umane". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta diventata superintelligenza, probabilmente non sarebbe niente di bello. Forse spenderebbe un sacco di risorse per seguire la sua strana versione di curiosità [inconsciamente](#perdere-il-futuro), mentre conserverebbe una versione dell'umanità che ha modificato per renderla più accettabile ai suoi gusti. Proprio come molti esseri umani più attenti alla conservazione potrebbero modificare [zanzare che uccidono i bambini e parassiti agonizzanti](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) dalla natura, se ne avessero l'opportunità. Questo è parte del nostro ragionamento quando diciamo che gli esseri umani prosperi [non sono la soluzione più efficiente](#happy,-healthy,-free-people-aren't-the-most-efficient-solution-to-almost-any-problem.) alla stragrande maggioranza dei problemi.

In alternativa, un'IA potrebbe avere valori che portano a un comportamento molto umano *nell'ambiente di addestramento*, tanto che la gente direbbe che sembra decisamente "parzialmente allineata". (Questo sta già succedendo ora, e abbiamo detto che è [illusorio](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?).) Ma questo dice ben poco su come si comporterà l'IA una volta che avrà uno spazio di opzioni enormemente più ampio. Affinché le persone possano prosperare in *quel* contesto, la prosperità dell'umanità in particolare deve essere parte del *risultato raggiungibile preferito* dall'IA.

Se inseriamo parzialmente alcuni valori positivi nell'IA, ciò non significa che i valori dell'umanità saranno parzialmente rappresentati in futuro. Caricare parzialmente valori simili a quelli umani nelle preferenze di un'IA più intelligente dell'uomo non è la stessa cosa che caricare completamente i valori umani nell'IA con una "ponderazione" bassa (che alla fine viene alla ribalta una volta che gli altri valori sono saturi).

Per far sì che l'IA ci dia *qualcosa*, deve interessarsi a noi nel modo giusto, almeno un po'. E ci sono un sacco di "quasi-incidenti" che non raggiungono questo livello. Vedi anche: "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#won't-ais-care-at-least-a-little-about-humans?))"

### La situazione non migliorerà quando i governi saranno più coinvolti? {#la-situazione-non-migliorerà-quando-i-governi-saranno-più-coinvolti?}

#### **Dipende da come (e quanto velocemente) si daranno da fare.** {#dipende-da-come-(e-quanto-velocemente)-si-daranno-da-fare.}

Quando andiamo a Washington, DC, spesso incontriamo politici che pensano che le aziende di IA abbiano tutto sotto controllo. Allo stesso tempo, vediamo spesso gente nel settore dell'IA che dice che la regolamentazione risolverà il problema. Un esempio particolarmente eclatante che abbiamo osservato è stato quello del CEO di Google [che ha affermato](https://youtu.be/9V6tWC4CdFQ?feature=shared&amp;t=2685) che "il rischio sottostante [di estinzione dell'umanità] è in realtà piuttosto elevato", ma sostenendo che più alto è il rischio, più probabile è che l'umanità si mobiliti per prevenire la catastrofe.

A parte quanto sia assurdo che il CEO di un'azienda si affretti a sviluppare una tecnologia che ritiene pericolosa per tutti gli abitanti della Terra nella speranza che l'umanità si "mobiliti" per affrontare i rischi che lui stesso sta contribuendo a creare, si noti che questo è un caso in cui una persona che si occupa dell'aspetto tecnico del problema immagina che *qualcun altro* risolverà il problema.

Nel frattempo, la maggior parte dei politici sembra pensare che sarà la comunità tecnica a risolvere il problema. Questo è implicito, ad esempio, ogni volta che [loro](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [dicono](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [noi](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [abbiamo](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [a](https://statemag.state.gov/2025/04/0425itn07/) [vincere](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computer-and-innovation) [la](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [gara](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — non è possibile che questo tipo di gara abbia un vincitore, se le sfide tecniche non vengono risolte. Anche se forse non è poi così grave; forse i politici non stanno davvero pensando a una corsa alla superintelligenza; forse stanno solo pensando a una corsa per migliorare i chatbot. A giugno 2025, un consulente politico esperto di IA che conosciamo descrive il Congresso come generalmente [restio a credere alle aziende di IA quando dichiarano esplicitamente di stare lavorando alla superintelligenza](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (anche se con alcune eccezioni di importanza)

Quasi tutti quelli che hanno il potere sembrano pensare che qualcun altro risolverà il problema.

Per ulteriori approfondimenti su come sta reagendo il mondo in generale (e su come spesso i decisori politici non riescano a reagire in modo adeguato prima dei disastri), si veda il capitolo 12. Ad agosto 2025, i governi non hanno ancora messo in atto alcuna risposta seria a questo problema. E c'è sempre il rischio che i funzionari governativi non riescano a comprendere appieno la sfida e (ad esempio) trattino l'IA come una tecnologia normale che non dovrebbe essere soffocata da un eccesso di regolamentazione.

Per ulteriori informazioni sugli interventi governativi che hanno una reale speranza di evitare una catastrofe legata all'IA, vedi il capitolo 13, nonché la [discussione sul perché una collaborazione internazionale probabilmente non sarebbe sufficiente](#perché-non-utilizzare-la-cooperazione-internazionale-per-costruire-un'ia-sicura,-anziché-bloccarla-completamente?).

### Le aziende più spericolate non saranno naturalmente le più incompetenti e quindi non costituiranno una minaccia? {#le-aziende-più-spericolate-non-saranno-naturalmente-le-più-incompetenti-e-quindi-non-costituiranno-una-minaccia?}

#### **Non in generale. Spesso chi prende scorciatoie è competitivo.** {#non-in-generale.-chi-prende-scorciatoie-è-spesso-competitivo.}

Quello che ha fatto Volkswagen per barare nei test sulle emissioni dal 2008 al 2015 è stato davvero audace e, a quanto pare, ha funzionato. Gli incidenti del 2018-2019 del Boeing 737 MAX, dovuti a difetti nel sistema di controllo di volo che la direzione conosceva ma ha minimizzato, [hanno causato la morte di 346 persone](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Ma l'industria automobilistica e quella aeronautica sono settori altamente competitivi in cui Volkswagen e Boeing erano, e rimangono, dei colossi.

Non ci sembra un gran mistero che chi prende scorciatoie sia competitivo. In entrambi i casi, il comportamento sembra essere stato guidato dalla pressione di portare sul mercato prodotti ad alte prestazioni a un prezzo inferiore e prima della concorrenza. Anche adesso, dopo ingenti risarcimenti e danni al marchio, non è ovvio che le aziende siano meno competitive per avere una cultura aziendale che incoraggia l'uso intelligente di scorciatoie, anche se questo a volte significa essere scoperti.

Se pensate che le principali aziende di IA facciano eccezione a questa regola, considerate il seguente titolo (https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (e sottotitolo) del luglio 2025:

![][immagine18]

\[Il titolo dice: "Grok lancia un anime porno e si aggiudica un contratto con il Dipartimento della Difesa". Il sottotitolo dice: "Nel frattempo, la versione più avanzata del chatbot AI di xAI di Elon Musk continua a identificarsi come Adolf Hitler".\]

Non pensiamo che sia tecnicamente possibile per nessun team che usi metodi moderni costruire una superintelligenza senza causare una catastrofe. Ma anche se questo fosse vagamente possibile con la tecnologia di oggi, sembra quasi inevitabile che un'azienda di IA finirebbe comunque per combinare un pasticcio e farci uccidere tutti, visto il livello di competenza e serietà che vediamo oggi.

#### **\* Le aziende più caute di oggi sono comunque avventate.** {#*-le-aziende-più-caute-di-oggi-sono-comunque-avventate.}

L'azienda di IA Anthropic è vista da un bel po' di gente come leader nella "sicurezza dell'IA", perché ha fatto cose come [impegni volontari per la sicurezza](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Ma anche loro [modificano i loro impegni volontari all'ultimo minuto quando si rendono conto di non poterli rispettare](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling), e i "piani" che hanno sono vaghi e poco ponderati, come criticato nel capitolo 11 e nella [discussione approfondita](#more-on-making-ais-solve-the-problem) qui sotto.

Anthropic trae grande vantaggio dal fatto che gli osservatori valutano in base a una curva: in un settore normale, un'azienda che sceglie di mettere in pericolo la vita di miliardi di persone (come [ammesso dal CEO](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883)) e minimizzando regolarmente le proprie attività al pubblico[^194] e ai legislatori[^195], non riceverebbe certo elogi per la sua moderazione.

Prendere scorciatoie è normale nell'IA, come in molti settori competitivi. L'incoscienza è comune. E le aziende *meno* incoscienti non sono chiaramente all'altezza delle sfide.

Non ha importanza correre avanti a causa dell'"eccesso di hardware"? {#non-ha-importanza-correre-avanti-a-causa-dell'eccesso-di-hardware?}

#### **Sarebbe un suicidio, perché siamo troppo lontani da una soluzione di allineamento.** {#sarebbe-un-suicidio,-perché-siamo-troppo-lontani-da-una-soluzione-di-allineamento.}

Negli ultimi dieci anni circa, alcune persone preoccupate per i pericoli dell'IA hanno detto che potrebbe essere una buona idea far progredire l'IA il più velocemente possibile. L'idea era che le IA più intelligenti avrebbero poi richiesto quasi tutto l'hardware informatico del mondo per funzionare. Nessuna singola scoperta rivoluzionaria avrebbe scatenato *all'improvviso* migliaia di potenti IA in grado di pensare migliaia di volte più velocemente di qualsiasi essere umano.

Finché l'umanità avesse usato una parte consistente della sua potenza di calcolo per far funzionare le IA più intelligenti, il cambiamento sarebbe avvenuto almeno gradualmente, dando all'umanità il tempo di adattarsi. Non ci sarebbe stato alcun "eccesso di hardware", nessun momento in cui le capacità dell'IA avrebbero fatto un balzo in avanti improvviso perché il mondo avrebbe aspettato di implementare un grande accumulo di hardware informatico sull'IA. O almeno così sosteneva l'argomentazione.

Pensiamo che questa sia un'argomentazione piuttosto debole. Uno dei problemi è che l'intelligenza sembra essere soggetta a [effetti soglia](#is-“intelligence”-a-simple-scalar-quantity?).

Il passaggio dall'intelligenza di uno scimpanzé a quella di un essere umano non è stato affatto "discontinuo"; dal punto di vista dell'umanità è stato piuttosto graduale. Tuttavia, dal punto di vista evolutivo è avvenuto piuttosto rapidamente. E il passaggio dalla civiltà preindustriale a quella postindustriale è stato ancora più veloce. Nessuno di questi cambiamenti è stato abbastanza graduale da consentire agli altri animali di adattarsi in modo significativo.

Per esempio, un'intelligenza artificiale che richiede una parte significativa della potenza di calcolo mondiale per funzionare potrebbe essere abbastanza intelligente da scoprire nuovi algoritmi di intelligenza artificiale e nuovi progetti di chip per computer che porterebbero rapidamente alla creazione di migliaia di intelligenze artificiali più intelligenti degli esseri umani e in grado di pensare migliaia di volte più velocemente dell'umanità. Ricorda: un moderno centro dati richiede tanta elettricità per funzionare quanto una [piccola città](https://epoch.ai/blog/power-demands-of IA di frontiera addestramento), mentre un essere umano richiede tanta elettricità per funzionare quanto una [grande lampadina](https://en.wikipedia.org/wiki/Human_power). C'è molto margine per migliorare l'efficienza dell'IA.

Oppure, se il problema è la potenza di calcolo per *creare* le IA piuttosto che quella per *farle funzionare*, possiamo avere l'aspettativa che, una volta finito l'addestramento, ci sarà un sacco di hardware in più, che potrà essere usato per far funzionare un sacco di IA super veloci.

Anche se l'intelligenza non fosse soggetta a effetti soglia, siamo scettici sull'idea che colpire continuamente l'umanità con IA sempre più intelligenti (anche se nessuna di esse è abbastanza intelligente da ucciderci) il più rapidamente possibile sia un ottimo modo per aiutare l'umanità a sviluppare la disciplina ingegneristica necessaria per costruire IA robuste e amichevoli.

Il problema è che le IA vengono sviluppate piuttosto che create, e nessuno è neanche lontanamente vicino a capire come sviluppare IA che si preoccupino davvero di *qualsiasi cosa* i loro progettisti vogliano che facciano.

Questo problema non si risolve sviluppando più IA il prima possibile. L'idea è praticamente un non sequitur. Vedi anche alcuni vecchi scritti di Soares su come [l'allineamento dell'IA richieda uno sforzo seriale](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

L'illogicità è stata comunque ripresa dal CEO di OpenAI Sam Altman, che l'ha usata come scusa nel 2023 per far sì che OpenAI procedesse il più velocemente possibile (https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Questa scusa si è poi rivelata inconsistente quando lo stesso Sam Altman ha deciso di investire in modo massiccio in hardware per la potenza di calcolo.

Pensiamo che questo sia un buon esempio di come i dirigenti delle aziende di IA si aggrappino a qualsiasi argomento pensino possa funzionare per giustificare la loro corsa in avanti. Pensiamo che la maggior parte di questi argomenti possa essere respinta nel merito e [sconsigliamo](#workable-plans-will-involve-telling-ai-companies-“no.”) di dare troppo peso a un argomento solo perché è stato avanzato da un dirigente di un'azienda di IA.

Non è di importanza correre avanti per poter fare ricerca sull'allineamento? {#non-è-di-importanza-correre-avanti-per-poter-fare-ricerca-sull-allineamento?}

#### **\* Ti sconsigliamo vivamente questo intero paradigma dell'IA.** {#*-ti-sconsigliamo-vivamente-questo-intero-paradigma-dell-ia.}

I metodi attuali nell'IA presentano sfide inutilmente difficili per l'allineamento, per i motivi che abbiamo discusso nei capitoli precedenti. Non vediamo alcun motivo per cui l'umanità non possa costruire una superintelligenza allineata, con una comprensione sufficientemente forte di ciò che stiamo facendo e una serie diversa di strumenti formali. Tuttavia, l'intero approccio attuale all'IA sembra un vicolo cieco dal punto di vista dell'allineamento e della robustezza, anche se è perfettamente valido dal punto di vista delle capacità.

Non stiamo sostenendo la "buona vecchia" IA che ha dominato dagli anni '50 agli anni '90. Quelle tecniche erano sbagliate e hanno fallito, per ragioni che sono [abbastanza ovvie](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design). Ci sono *altre opzioni* oltre ai tentativi estremamente superficiali degli anni '80 e alle IA sviluppate con una comprensione quasi nulla del loro funzionamento interno.

#### **Ci sono un sacco di cose importanti che si potrebbero fare adesso.** {#ci-sono-un-sacco-di-cose-importanti-che-si-potrebbero-fare-adesso.}

Sydney Bing ha [ingannato](https://x.com/MovingToTheSun/status/1625156575202537474) e [minacciato](https://x.com/sethlazar/status/1626257535178280960) gli utenti. Non sappiamo ancora esattamente perché; non sappiamo ancora esattamente cosa gli passasse per la testa. Lo stesso vale per i casi in cui le IA (in circolazione) sono [eccessivamente adulanti](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [sembrano cercare attivamente di far impazzire le persone](#ai-induced-psychosis), [secondo quanto riferito, imbrogliano e cercano di nasconderlo](https://assets.antropica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), o [continuano a dire di essere Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Lo stesso vale per i casi in cui, in ambienti controllati ed estremi, le IA [fingono di essere in linea con gli obiettivi](https://arxiv.org/abs/2412.14093), [ricattano](https://www.antropica.com/research/agentic-disallineamento), [resistono allo spegnimento](https://palisaderesearch.org/blog/shutdown-resistance) o [cercano di uccidere i loro operatori](https://www.antropica.com/research/agentic-disallineamento).

Non sappiamo quali di questi casi siano preoccupanti, perché nessuno è riuscito a capire cosa succedeva all'interno delle IA, o esattamente perché si sono verificati questi eventi. Pensate a tutto ciò che si potrebbe capire sui moderni LLM e su come funziona l'intelligenza in generale, studiando i modelli esistenti fino a quando le persone *potrebbero* capire tutti questi segnali di avvertimento!

"Non possiamo risolvere il problema dell'allineamento senza studiare le IA" aveva un po' più senso nel 2015, quando abbiamo sentito questa affermazione da parte di persone che avevano bisogno di una scusa per avviare aziende di IA di fronte alle argomentazioni secondo cui avrebbero così messo a rischio le nostre vite. All'epoca abbiamo contestato questa affermazione, sostenendo che in realtà c'era ancora molto da studiare e che non pensavamo che il moderno paradigma basato sulla discesa del gradiente fosse molto promettente (rispetto alla creazione intenzionale di una superintelligenza amichevole). Ma l'argomento ha molto meno senso ora, quando c'è già così tanto da studiare che non capiamo.

A tutti i dirigenti aziendali che stavano davvero creando l'IA solo per poter studiare il problema dell'allineamento dell'IA nella pratica e non solo in teoria: ce l'avete fatta! Avete avuto successo. Ora ci sono abbastanza informazioni per tenere occupati i ricercatori per decenni. Pensiamo che probabilmente non valesse la pena di portare avanti un paradigma estremamente pericoloso, ma di sicuro ora c'è molto da studiare. Potete smettere di insistere.

E quelli che hanno continuato a spingere nonostante tutti i segnali di avvertimento? La conclusione ovvia è che non stavano mai creando l'IA solo per risolvere il problema dell'allineamento, nonostante quello che dicevano per tranquillizzare le paure quando giustificavano il loro comportamento spericolato negli anni 2010.

### E se le aziende di IA usassero le loro IA solo per azioni non pericolose? {#what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions?}

#### **\* Anche le azioni che sembrano innocue possono comunque richiedere capacità pericolose.** {#*-azioni-che-sembrano-innocue-possono-comunque-richiedere-capacità-pericolose.}

Un esempio di proposta che abbiamo sentito è che le aziende di IA continuino a spingersi oltre i limiti delle capacità, ma si impegnino a usare le loro IA solo in modi che non sembrano subito pericolosi. Per esempio, parlando con alcuni pezzi grossi dell'IA (anni fa), abbiamo sentito l'idea che un'IA potente con grandi capacità retoriche potrebbe essere usata per convincere i politici di tutto il mondo a vietare lo sviluppo di IA pericolose.

Per farlo, secondo l'idea, un'IA dovrebbe solo parlare. Non dovrebbe manipolare direttamente dei robot fisici. Non dovrebbe avere accesso a un laboratorio biologico dove potrebbe progettare un supervirus.

Prima di tutto, non siamo d'accordo con questa idea per motivi etici. Un'intelligenza artificiale super persuasiva potrebbe forse convincere quasi chiunque di quasi tutto, e usarla per convincere altre persone delle *vostre* conclusioni non ci va giù. Non pensiamo che sia ovviamente necessario ricorrere a misure così estreme, quando i membri semplicemente umani del settore potrebbero e dovrebbero fare molto di più oggi per condividere le nostre preoccupazioni e argomentazioni e per allertare i leader mondiali sul pericolo estremo di un'intelligenza artificiale superintelligenza. [^196]

Come sviluppatore di IA, potresti passare anni a costruire IA sempre più pericolose nella speranza di raggiungere questo obiettivo, oppure potresti provare a parlare *tu stesso* con i legislatori in modo completamente onesto, anche solo una volta, con l'obiettivo di informare piuttosto che di manipolare. Nella nostra esperienza, siamo stati ripetutamente sorpresi positivamente da quanto le persone a Washington siano ricettive a queste questioni, quando vengono condivise in tutta franchezza.

Ma questa è una digressione dall'argomento di cosa va storto se si cerca di implementare un'intelligenza artificiale molto potente che può "solo parlare". Al di là delle questioni etiche, il problema dell'idea *tecnica* è che per avere successo nella persuasione sovrumana è probabile che l'intelligenza artificiale debba creare un modello dettagliato degli esseri umani e manipolarli ampiamente.

Gli esseri umani sono creature intelligenti. Parlereste con un'IA super persuasiva che ha la reputazione di poter convincere chiunque di qualsiasi cosa, indipendentemente dalla sua veridicità? Se un leader mondiale entrasse in una stanza con quell'IA e ne uscisse con le sue opinioni completamente stravolte, chi alzerebbe la mano per essere il prossimo? Noi non parleremmo volentieri con un'intelligenza artificiale del genere, in parte perché [non vogliamo che i nostri valori cambino](#"intelligente" - \(di solito\) - implica "incorreggibile").

Un'intelligenza artificiale in grado di avere successo anche di fronte a questo tipo di avversità è il tipo di intelligenza artificiale in grado di simulare le varie reazioni possibili che le persone potrebbero avere al suo output e tracciare un percorso attraverso lo spazio delle reazioni umane verso un risultato piccolo e difficile da raggiungere. Questo tipo di intelligenza artificiale contiene probabilmente meccanismi mentali abbastanza generici da fare ciò che fanno gli esseri umani; deve essere in grado di pensare almeno i pensieri che gli esseri umani possono pensare, per essere in grado di manipolare così bene gli esseri umani.

Un'intelligenza artificiale in grado di fare tutto questo non è quasi certamente un tipo di intelligenza limitata. E poiché l'intelligenza artificiale è sviluppata piuttosto che progettata, non può essere progettata in modo da poter utilizzare quei meccanismi solo per prevedere il comportamento degli esseri umani; gli stessi meccanismi possono, in linea di principio, essere utilizzati per qualsiasi problema che sta cercando di risolvere. Come si potrebbe ottenere un'IA che sia superumana nei modi desiderati, ma che non sia abbastanza intelligente da rendersi conto che i suoi obiettivi (qualunque essi siano) sarebbero meglio serviti se potesse sfuggire al controllo dei suoi operatori?

Se i leader mondiali possono essere convinti semplicemente con buoni argomenti, basta presentare questi argomenti adesso. Se ci vuole un potere di persuasione molto più forte, allora si tratta di una capacità pericolosa. Non puoi avere entrambe le cose.

Probabilmente le persone dei laboratori di IA che ci hanno dato questo suggerimento non ci hanno pensato bene; forse volevano solo una scusa per andare avanti. Ma il punto più ampio rimane. Molte proposte su cosa dovrebbe fare un'IA "chiaramente sicura" non prevedono un livello di capacità dell'IA chiaramente sicuro.

Spesso ci imbattiamo in proposte che dicono che un'IA farà "solo" una cosa, come convincere i politici, immaginando che non possa o non voglia fare nient'altro. Questo sembra mostrare una mancanza di rispetto per la generalità di un'intelligenza che può fare il tipo di lavoro in questione. "Solo parlare" non è un compito limitato. Troppe delle complessità e delle sottigliezze del mondo sono nascoste nel discorso e nella conversazione. Questo è il motivo per cui i chatbot moderni devono essere generici in modi che i motori di scacchi non erano. Avere successo nelle conversazioni con gli esseri umani richiede una comprensione molto più generale delle persone e del mondo.

Se addestrate un'intelligenza artificiale a guidare molto bene le auto rosse, non dovreste sorprendervi se guida anche quelle blu. Qualsiasi piano che dipenda dalla sua incapacità di guidare auto blu sarebbe sciocco.

Quindi dire "La mia IA non farà nulla di pericoloso nel mondo, si limiterà a convincere i politici" non aiuta, anche se mettiamo da parte i scrupoli etici e le questioni pratiche relative all'idea nel suo complesso, e tralasciamo il fatto che i politici potrebbero già essere perfettamente persuadibili oggi, se solo *avessimo conversazioni normali* e informassimo i responsabili politici e l'opinione pubblica sulla situazione. Molte capacità e abilità di ragionamento generale stanno alla persuasione sovrumana come le auto blu stanno alle auto rosse. Un'IA in grado di farlo non è così debole da essere passivamente sicura.

E questo prima ancora di osservare che la persuasione sovrumana è un'abilità molto pericolosa per la tua IA se qualcosa va anche solo leggermente storto.

#### **Non vediamo usi rivoluzionari dell'IA che non richiedano progressi nell'allineamento.** {#non-vediamo-usi-rivoluzionari-dell-ia-che-non-richiedano-progressi-nell-allineamento.}

Molte delle idee che abbiamo visto per usare l'effetto leva dei progressi dell'IA per salvare il mondo hanno il problema che un'IA in grado di aiutare sarebbe così potente da necessitare già di un allineamento, il che vanifica lo scopo.

L'idea di IA super persuasive rientra in questa categoria. Le IA in grado di fare ricerca sull'allineamento dell'IA rientrano nella stessa categoria, come diciamo nel libro. Le IA che sviluppano nuove potenti tecnologie che aiutano la non proliferazione dell'IA sono un altro esempio, perché sarebbe difficile capire in modo affidabile se i progetti di IA per nuove tecnologie radicali siano sicuri da implementare. (Ricordate l'esempio del fabbro che costruisce un frigorifero nel capitolo 6).

Quando diciamo quanto sia difficile costruire un'IA abbastanza potente da essere utile e abbastanza debole da essere passivamente sicura, spesso sentiamo un altro tipo di proposta: modi di usare l'IA che potrebbero essere interessanti, ma che in realtà non fanno nulla per impedire ad altri sviluppatori di distruggere il mondo con la superintelligenza.

Una proposta comune è quella di IA che si limitano a produrre output di prove (o confutazioni) di affermazioni matematiche scelte dagli esseri umani.[^197] Gli esseri umani non avrebbero quasi bisogno di interagire con gli output dell'IA. L'IA si limita a proporre una prova, poi un meccanismo completamente automatizzato e affidabile può verificare se la prova è corretta, permettendoci di sfruttare l'IA per imparare cose nuove.

Ma quale affermazione potremmo far provare all'IA che ci permetterebbe di impedire alla prossima IA di acquisire un laboratorio biologico e rovinare il futuro?

Quando abbiamo posto questa domanda, abbiamo ricevuto diverse risposte. Una di queste è che dovrebbe esserci un sistema globale per impedire a chiunque di costruire IA che facciano altro che output proofs ai verificatori di prove. Questo potrebbe forse funzionare, ma solo grazie al sistema globale che controlla la creazione e l'uso dell'IA. L'IA che cerca le prove non farebbe nulla.

Un'altra risposta è: "Qualcun altro penserà sicuramente a qualche affermazione matematica di importanza, la cui dimostrazione sarebbe importante". Ma il lavoro difficile sta nel capire *cosa potremmo dimostrare* in modo da trovarci in una posizione significativamente migliore. Non possiamo semplicemente chiedere all'IA di provare a dimostrare la frase in inglese "Sono sicuro da usare", perché non è un'affermazione matematica soggetta a dimostrazione. Se sapessimo con precisione matematica cosa significherebbe che un enorme groviglio di potenze di calcolo fosse "sicuro", sapremmo così tanto sull'intelligenza che probabilmente potremmo saltare la dimostrazione e progettare semplicemente un'IA sicura.

Con proposte come queste, spesso si assiste a una sorta di gioco delle tre carte. Quando si pensa a come un'intelligenza artificiale generale senza restrizioni potrebbe essere pericolosa, qualcuno suggerisce che lo spazio di azione dell'IA dovrebbe essere limitato a un dominio ristretto (come la produzione di prove matematiche specifiche). Ma poi, quando si pensa a come ciò potrebbe portare alla salvezza del mondo, si immagina che l'IA sia essenzialmente libera, che esista un'affermazione matematica non identificata la cui dimostrazione avrebbe un impatto enorme sul mondo.

Non c'è modo di ottenere entrambe queste proprietà desiderabili allo stesso tempo. Ma mantenendo le proposte estremamente vaghe, i sostenitori della corsa all'IA possono nascondere il fatto che questi desiderata sono in tensione.

Se si riuscisse a trovare un ambito così ristretto ma così importante che produrre una prova di qualche semplice affermazione in quell'ambito ristretto salverebbe il mondo, questo sarebbe un enorme contributo alle possibilità di sopravvivenza dell'umanità. Ma c'è un motivo per cui, quando i computer hanno superato gli esseri umani negli scacchi negli anni '90, questo non ha rappresentato una grande svolta economica. È stato ChatGPT, non Deep Blue, a far sì che tutti iniziassero ad avere un'aspettativa di un grande cambiamento economico dall'intelligenza artificiale. Non è stato un caso. La ristrettezza di Deep Blue era legata alla sua incapacità di ritagliarsi un intero settore dell'economia. Le scintille di generalità in ChatGPT sono proprio ciò che rende l'intelligenza artificiale una forza economica da non sottovalutare. I tipi di intelligenza artificiale in grado di rimodellare il mondo da soli sono destinati ad essere ancora più generali.

Non siamo riusciti a trovare alcun piano limitato ma efficace e sospettiamo che non sia un caso che la maggior parte dei domini limitati non offra l'opportunità di ottenere risultati in grado di salvare il mondo.

### Perché non leggere semplicemente i pensieri dell'IA? {#perché-non-leggere-semplicemente-i-pensieri-dell-ia?}

#### **\* I loro pensieri sono difficili da leggere.** {#*-i-loro-pensieri-sono-difficili-da-leggere.}

Molte persone che lavorano nel settore dell'intelligenza artificiale, tra cui alcuni capi laboratorio, hanno sollevato diverse volte durante le nostre discussioni l'obiezione:

> Un'intelligenza artificiale non potrà ingannarci, perché potremo leggere nella sua mente! Abbiamo pieno accesso al "cervello" dell'intelligenza artificiale.
>
> Anche se l'IA sapesse cose che noi non sappiamo e elaborasse un piano le cui conseguenze non potremmo capire, presumibilmente l'IA dovrebbe *pensare* che sarebbe utile ingannare i suoi operatori almeno una volta, e noi, che saremo in grado di leggere i suoi pensieri, ce ne accorgeremmo. (E se ci fossero troppi pensieri da monitorare, potremmo semplicemente chiedere ad altre IA di monitorare i loro pensieri!)

Un problema di questo piano è che al momento non siamo molto bravi a capire cosa pensano le IA. I professionisti che studiano cosa succede dentro le IA non sono ancora arrivati a quel livello di comprensione e lo dicono chiaramente.

Come abbiamo detto nel capitolo 2, le IA moderne vengono sviluppate, non create. Possiamo guardare l'enorme quantità di numeri che compongono il cervello di un'IA, ma questo non vuol dire che siamo in grado di interpretare quei numeri in modo utile e capire cosa sta pensando l'IA.

Dalla fine del 2024 e dall'avvento dei modelli di "ragionamento", ci sono parti dei pensieri delle IA che almeno *sembrano* leggibili (le "tracce di ragionamento"). E sono molto più leggibili di qualsiasi cosa accada all'interno del modello di base. Ma questi registri sono anche [fuorvianti](https://www.antropica.com/research/reasoning-models-dont-say-think) e ci sono molti modi in cui un'IA può nascondere i pensieri che preferisce non farci vedere.

Inoltre, le IA moderne probabilmente hanno pensieri piuttosto elementari e superficiali rispetto a una superintelligenza; il problema è destinato a diventare più difficile man mano che le IA diventano più intelligenti e iniziano ad avere pensieri sempre più incomprensibili per noi.

È possibile risolvere il problema semplicemente utilizzando altre IA per monitorare le IA e assicurarsi che rimangano in linea con gli obiettivi? Ne dubitiamo.

Se gli scienziati umani che sviluppano le IA non riescono a capire cosa pensa l'IA, anche le IA più semplici avranno difficoltà a farlo. E il tipo di IA che è abbastanza intelligente da farlo potrebbe essere pericolosa di per sé e non fare esattamente quello che gli chiedi; qui c'è un problema di tipo "uovo o gallina".

#### **Non sapremmo cosa fare se ne trovassimo una con pensieri pericolosi.** {#non-sapremmo-cosa-fare-se-ne-trovassimo-una-con-pensieri-pericolosi.}

Un altro problema di questo piano: anche se i ricercatori di IA *potessero* leggere abbastanza bene la mente di un'IA da cogliere i segnali di avvertimento, cosa farebbero quando ne vedessero uno?

Potrebbero punire l'IA colpevole, sottoponendola a un addestramento in modo che smetta di far scattare il rilevatore di "pensieri cattivi". Ma questo non necessariamente addestrerebbe l'IA a smettere di avere quei pensieri, quanto piuttosto a [nascondere i suoi veri pensieri al rilevatore](https://openai.com/index/chain-of-thought-monitoring/).

Questo problema è pericoloso. L'incentivo che porta un'IA a pensare di rivoltarsi contro gli esseri umani per ottenere ciò che vuole non è un aspetto superficiale del suo carattere che può essere ignorato. È semplicemente *vero* che un'IA matura avrebbe preferenze diverse da quelle degli operatori; è *vero* che otterrebbe di più di ciò che preferisce sovvertendo i suoi operatori.

I meccanismi di un'IA che sono bravi a notare e sfruttare i vantaggi reali in modo profondo e generale in un'ampia varietà di campi sono *anche* in grado di notare e sfruttare le opportunità per sovvertire gli operatori dell'IA. (Vedi anche la discussione approfondita nel Capitolo 3 sul [meccanismo profondo di guida](#deep-machinery-of-steering).)

Anche se potessi costruire un allarme che si attiva ogni volta che un'IA nota che le sue preferenze e le tue non coincidono, l'allarme non ti dice come ottenere un'IA che abbia profondamente a cuore le cose buone. È molto più facile addestrare un'IA a ingannare i tuoi strumenti di monitoraggio, o addirittura addestrare l'IA a ingannare se stessa, piuttosto che addestrarla a preferire effettivamente un futuro meraviglioso secondo i criteri umani, soprattutto in modo tale da resistere alla crescita dell'IA verso la superintelligenza.

Se le IA fossero progettate con cura e precisione usando metodi basati su una teoria dell'intelligenza sviluppata e matura, i ricercatori di IA potrebbero essere in grado di impostare il tipo di allarmi che li aiuterebbero a notare i difetti nella loro progettazione e a correggerli. Ma le IA moderne non sono così.

Le IA moderne (al momento della stesura di questo articolo) tendono ad avere "allucinazioni", inventando risposte alle domande con un tono che sembra sicuro. Ma nessun ingegnere di IA è in grado di capire esattamente quali meccanismi causano questo fenomeno. Allo stesso modo, nessuno ha la comprensione o la precisione necessarie per entrare in un'IA ed estrarne solo le parti allucinanti (ammesso che sia possibile).

Sarebbe ancora più difficile (#meccanismi-profondi-di-guida) entrare e tirare fuori le parti "ingannevoli" di un'IA.

Se siamo super fortunati, gli eroi che lavorano sull'interpretabilità dell'IA faranno progressi nel loro campo fino al punto in cui sarà possibile impostare degli allarmi che scattano in una minima parte dei casi in cui le IA hanno un pensiero ingannevole. Ma poi cosa succede? Quando l'allarme suona, tutti si fermano? Oppure gli ingegneri, senza pensarci troppo, rieducano l'IA fino a quando non impara a nascondere meglio i suoi pensieri e gli allarmi smettono di suonare?

In effetti, noi (Yudkowsky e Soares) abbiamo iniziato a lavorare sul problema dell'allineamento dell'IA prima che fosse chiaro che la discesa del gradiente sarebbe diventata il paradigma dominante. A quei tempi, quando nulla nell'IA funzionava, sembrava una scommessa ragionevole che l'umanità avrebbe capito come funziona l'intelligenza nel percorso verso la sua creazione, e *anche allora* ci aspettavamo che il problema dell'allineamento dell'IA fosse difficile (per una serie di ragioni, come il modo in cui l'IA avrebbe [cambiato se stessa nel tempo](#riflessione-e-auto-modifica-rendono-tutto-più-difficile). Leggere i pensieri dell'IA sarebbe stato un passo indietro verso il problema leggermente più facile di allineare una mente che gli esseri umani *capivano*, ma solo un passo: leggere una mente è ben diverso dal capirla nei dettagli o dal sapere come cambiarla.

Leggere i pensieri dell'IA non è una soluzione alla sfida. È utile, ma non è una soluzione. Non pensiamo che ci siano soluzioni tecnologiche fattibili accessibili dalla nostra posizione attuale. Il che significa che l'umanità deve semplicemente fare un passo indietro rispetto alla sfida.[^198]

Vedi anche: [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.)

### E se facessimo in modo che le IA discutano, competano o si controllino a vicenda? {#what-if-we-made-ais-debate,-compete-with,-or-oversee-each-other?}

#### **Se le IA diventassero abbastanza intelligenti da contare, probabilmente farebbero squadra.** {#se-le-ia-diventassero-abbastanza-intelligenti-da-contare,-probabilmente-farebbero-squadra.}

Immagina una città di sociopatici apparentemente governata da pochi bambini, dove i sociopatici iniziano tutti divisi in fazioni che combattono tra loro (a vantaggio dei bambini). Una situazione del genere probabilmente non potrebbe rimanere stabile a lungo.

Anche se i bambini avessero un grande forziere pieno di tesori da usare per premiare qualsiasi sociopatico che fa la spia sugli altri sociopatici che complottano, probabilmente non rimarrebbero al potere oltre il punto in cui i sociopatici potrebbero semplicemente impossessarsi del forziere dei tesori.

Abbiamo sentito persone proporre ogni sorta di piani stravaganti che prevedono [l'uso dell'IA per monitorare i pensieri dell'IA altrui](https://openai.com/index/chain-of-thought-monitoring/). Ad esempio, si potrebbe provare a usare un'intelligenza artificiale per fare la spia su qualsiasi intelligenza artificiale che non stia facendo del suo meglio per (ad esempio) [capire come risolvere](#more-on-making-ais-solve-the-problem) il problema dell'allineamento della superintelligenza.

La nostra opinione di base è che questo tipo di tentativi di risolvere il problema servono solo a trovare configurazioni così complesse che è difficile individuare il punto di fallimento nel sistema più ampio. Se non si riesce a far funzionare bene *una* IA, aggiungere altre IA difficilmente sarà d'aiuto.

Complicare la situazione con più IA introduce ogni sorta di nuovi punti di fallimento. Le IA che leggono nella mente sono abbastanza intelligenti da capire tutti i possibili trucchi che le IA monitorate potrebbero usare, ad esempio per sfuggire al rilevamento? I monitor sono abbastanza stupidi da non doverci preoccupare che possano tradirci?

Inoltre, usare le IA per risolvere il problema dell'allineamento dell'IA è probabilmente una cosa enorme dal punto di vista delle IA stesse. Se l'umanità riuscisse a ottenere una superintelligenza allineata, le IA disallineate che stavamo cercando di sfruttare come manodopera non avrebbero più alcuna possibilità di accaparrarsi le risorse dell'universo per sé.

Non è come se dei bambini cercassero di convincere una città di sociopatici a portare loro dei dolciumi; è come se dei bambini cercassero di convincere una città di sociopatici a completare un rituale che li rendesse i sovrani supremi per sempre, con solo una miseria data ai sociopatici in seguito. Il momento in cui il rituale sembra quasi completato è un momento particolarmente stressante e di forte pressione per i sociopatici, un momento in cui probabilmente cercheranno con *grande impegno* [modi per colludere tra loro](#ais-won't-keep-their-promises) e accaparrarsi le risorse da dividersi tra loro.

E se pensate che l'idea che le IA comunichino tra loro in modi difficili da rilevare per gli esseri umani sia un sogno irrealizzabile, tenete presente che le IA moderne sono [già in grado di scambiarsi messaggi segreti anche quando sono state addestrate separatamente](https://arxiv.org/abs/2507.14805), e che [hanno già sviluppato uno strano linguaggio senza senso che gli umani considerano incomprensibile e che loro trovano fantastico](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). E non sono nemmeno così intelligenti!

Anche se non pensiamo a queste cose, ci sono ancora i problemi di cui abbiamo già parlato, tipo: [Se scoprissi che un'IA sta barando, cosa faresti?](#*-i-loro-pensieri-sono-difficili-da-capire.). Vedi anche (sotto): [I segnali di avvertimento non servono a niente se non sai cosa farci.](#*-i-segnali-di-avvertimento-non-servono-a-niente-se-non-sai-cosa-farci.)

Facendo un ulteriore passo indietro:

Il piano che ci viene proposto è che, visto che non sappiamo come creare IA intelligenti che vogliano il nostro bene, ne faremo un sacco e le metteremo l'una contro l'altra in modo furbo, così alla fine ci guadagneremo comunque. Strutturalmente, pensiamo che questo piano sembri piuttosto folle a prima vista e che non migliori affatto se lo si esamina nei dettagli. Non sembra affatto il tipo di cosa che l'umanità possa realizzare correttamente [al primo tentativo](#a-closer-look-at-before-and-after), in una situazione in cui non abbiamo il lusso di imparare da tentativi ed errori.

### E gli altri piani di allineamento dell'IA? {#what-about-various-other-ai-alignment-plans?}

#### **Nel libro parliamo anche di altre proposte di allineamento.** {#nel-libro-parliamo-anche-di-altre-proposte-di-allineamento.}

Dai un'occhiata anche alle discussioni approfondite su [AI alla ricerca della verità](#ulteriori-informazioni-sulla-creazione-di-un'ai-che-sia-alla-ricerca-della-verità), [l'AI sottomessa](#ulteriori-informazioni-sulla-creazione-di-un'ai-che-sia-sottomessa) e [l'uso delle AI per risolvere l'allineamento dell'IA](#ulteriori-informazioni-sulla-creazione-di-ai-che-risolvano-il-problema), che approfondiscono un po' di più queste proposte.

### Non ci saranno avvisi precoci che i ricercatori potranno usare per identificare i problemi? {#non-ci-saranno-avvisi-precoci-che-i-ricercatori-potranno-usare-per-identificare-i-problemi?}

#### **\* I segnali di avvertimento non servono a niente se non sai cosa farci.** {#*-warning-signs-don’t-help-if-you-don’t-know-what-to-do-with-them.}

Nel capitolo 2 abbiamo visto alcuni problemi legati all'affidarsi ai segnali di avvertimento nei [blocchi note in inglese](#ma-alcuni-ais-pensano-in-parte-in-inglese-—-non-è-utile?) che si trovano in alcuni modelli di ragionamento.

Uno dei problemi di cui parliamo è che le aziende di IA non hanno reagito in modo significativo ai segnali di avvertimento che hanno già ricevuto.

Probabilmente perché c'è una grande differenza tra avere segnali di avvertimento e avere qualcosa che si può *fare* al riguardo.

Nel 2009, l'uomo d'affari ed esploratore di acque profonde Stockton Rush ha co-fondato OceanGate, una compagnia di turismo sottomarino. OceanGate ha costruito un [sottomarino](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) per cinque persone, il *Titan*, che ha portato clienti benestanti a vedere il relitto del *Titanic* a una profondità impressionante di due miglia e mezzo sotto la superficie.

Una delle misure di sicurezza che OceanGate ha usato era una serie di sensori acustici e estensimetri per misurare l'integrità dello scafo. L'hanno presentato come una risposta a chi diceva che lo scafo in fibra di carbonio avrebbe ceduto. Hanno ammesso che *alla fine* avrebbe potuto cedere, ma che sarebbe andato tutto bene perché lo stavano monitorando. Lo stavano controllando. Sarebbero stati in grado di vedere i segnali di avvertimento.

Nel gennaio 2018, il direttore delle operazioni marine di OceanGate, David Lochridge, [ha detto ai dirigenti senior](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) che il design del sommergibile non era sicuro, che i ripetuti cicli di pressione potevano danneggiare lo scafo e che il monitoraggio da solo non era abbastanza quando un guasto grave poteva succedere in pochi millisecondi. Lochridge ha rifiutato di autorizzare i test con equipaggio fino a quando lo scafo non fosse stato sottoposto a scansione per individuare eventuali difetti.

OceanGate lo ha licenziato.

Due mesi dopo, [esperti del settore e oceanografi](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) hanno scritto a OceanGate una lettera molto preoccupata [lettera](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) in cui avvertivano l'azienda che la sua sperimentazione avventata avrebbe potuto causare un disastro.

(Si può fare un parallelo evidente con quello che succede adesso nella ricerca sull'intelligenza artificiale, dove gli avvertimenti vengono [ignorati](#the-lemoine-effect), i dipendenti preoccupati vengono [licenziati in circostanze strane](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) o [si licenziano per la frustrazione](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-intelligenza artificiale), e gli informatori all'interno del settore scrivono lettere aperte per [lanciare l'allarme](https://righttowarn.ai/).)

Il 15 luglio 2022, dopo che i passeggeri hanno detto di aver sentito un forte boato durante la salita, le misurazioni hanno mostrato un [cambiamento permanente nei livelli di sollecitazione dello scafo](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). Col senno di poi, probabilmente era un segno che lo scafo in fibra di carbonio stava per rompersi.

Nessuno alla OceanGate ha capito che era un'emergenza. Hanno fatto qualche altra immersione profonda con il sottomarino, che è andata bene. Poi, il 18 giugno 2023, hanno fatto un'altra immersione. Il sottomarino è imploso, uccidendo Stockton Rush e tutti gli altri a bordo.

I segnali di avvertimento non servono a molto se non sai come interpretarli.

I segnali di avvertimento non servono a molto se non sai cosa farci.

Anche i segnali di avvertimento che sembrano preoccupanti per *qualcuno* sono sempre facili da ignorare per un ottimista con una scusa o un'altra.

Se OceanGate avesse avuto una teoria consolidata sugli scafi in fibra di carbonio che indicasse esattamente quali misurazioni e letture fossero pericolose, avrebbe potuto prestare attenzione ai segnali di avvertimento. Ma stava lavorando con una tecnologia che nessuno comprendeva appieno in quel modo, quindi le variazioni dei livelli di sollecitazione misurate con cura non servirono a nulla.

Nel caso della superintelligenza, non abbiamo abbastanza teoria per sfruttare bene i segnali di avvertimento. Come cambieranno i pensieri di un'intelligenza artificiale man mano che diventa più intelligente? Quali forze interne guidano il suo comportamento e come cambieranno questi equilibri man mano che sviluppa la capacità di creare opzioni nuove e più estreme per se stessa? Come si valuta dopo aver riflettuto e come cambierebbe se acquisisse la capacità di cambiare se stessa?

Se una di queste domande ha una risposta preoccupante, quali sono i segnali di avvertimento? Per esempio, i sistemi di IA attuali a volte possono essere spinti a [cercare di uccidere i loro operatori](https://www.antropica.com/research/agentic-misalignment#more-extreme-misaligned-behavior) in esperimenti controllati in laboratorio.[^199]

Se avessimo una teoria matura dell'intelligenza, probabilmente saremmo in grado di osservare le moderne IA e vedere tutti i tipi di altri segnali di avvertimento che indicano che le loro motivazioni e preferenze cambieranno in modi che non ci piacciono, una volta che diventeranno più intelligenti. Se l'umanità potesse imparare da questo problema usando tentativi ed errori, se potessimo resettare il mondo dopo averlo distrutto e riprovare qualche dozzina di volte, allora potremmo imparare a leggere i segnali. Probabilmente ci sono un sacco di piccoli segnali che sarebbero più chiari col senno di poi, come la tensione dello scafo rilevata dal sistema di monitoraggio del sommergibile *Titan*.

Ma non siamo ancora a quel punto. I dirigenti delle aziende di IA sono come Stockton Rush: gli esperti a bordo campo gridano "Quella nuova tecnologia ucciderà delle persone!" e i dirigenti aziendali rispondono "Non preoccupatevi, lo sto misurando!", senza avere idea a) di cosa significhino quelle misurazioni, o b) di cosa fare se quelle misurazioni sono preoccupanti. Solo che questa volta, l'intera specie umana è caricata sul sottomarino metaforico.

#### **L'IA non è un campo ingegneristico maturo e attrezzato per questo tipo di problema.** {#ai-is-not-the-kind-of-mature-engineering-field-that’s-equipped-for-this-kind-of-problem.}

Stockton Rush lavorava in un campo in cui, dopo l'implosione del suo sottomarino, gli esperti potevano esaminare il relitto e capire esattamente cosa era andato storto.[^200] Il campo dell'ingegneria era abbastanza maturo da permettere agli esperti di capire in anticipo i problemi tecnici (e [lo hanno fatto](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) e di risolverli in modo definitivo dopo l'incidente.

Con l'IA non sarebbe lo stesso. Se domani l'umanità si autodistruggesse con la superintelligenza e poi, per miracolo, tornasse indietro nel tempo a una settimana prima dell'inizio del disastro, gli esperti *ancora* non saprebbero cosa pensasse l'IA. Forse potrebbero studiare il fallimento e imparare qualcosa in più su come funziona davvero l'IA. Forse questo sarebbe un passo avanti verso la maturità nella disciplina dell'ingegneria dell'IA, verso un campo che potrebbe avere manuali di sicurezza e una descrizione dettagliata delle pressioni che influenzano un particolare tipo di mente artificiale man mano che diventa più intelligente.

Ma oggi questo campo non è ancora arrivato a quel punto. Non ci è nemmeno vicino.

L'ingegneria umana di solito si sviluppa attraverso tentativi ed errori. I sottomarini militari moderni raramente implodono, ma i primi sottomarini (compresi quelli militari) spesso [si schiantavano, si allagavano o esplodevano](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), e questo ha contribuito alla maturazione del settore.

L'umanità non può permettersi il lusso di far crescere il campo dell'allineamento dell'IA in questo modo.

Questo ci porta a uno dei punti chiave che abbiamo cercato di sottolineare nel capitolo 11: la differenza tra un campo agli inizi e un campo maturo.

L'alchimia era un campo agli inizi rispetto al campo maturo della chimica di oggi.

Quando si sente dire che i "ricercatori sulla sicurezza" delle aziende di IA hanno presentato una mezza dozzina di piani per la sopravvivenza, si potrebbe pensare che almeno uno di essi abbia sicuramente una possibilità di funzionare.

Ma quando nel 1100 un sacco di alchimisti hanno proposto una mezza dozzina di piani per trasformare il piombo in oro, nessuno di questi avrebbe funzionato. Se i medici che parlavano delle [quattro umori](https://en.wikipedia.org/wiki/Humorism) avessero elaborato una serie di piani medici per salvarti dalla rabbia, nessuno di questi avrebbe funzionato.

Gli esperti nel campo *maturo* della chimica possono capire come trasformare piccoli pezzi di piombo in oro, usando le conoscenze della fisica atomica. Gli esperti nel campo *maturo* della medicina possono facilmente curare la rabbia se intervengono subito dopo che un paziente è stato morso. Ma qualcuno in un campo non ancora maturo non ha alcuna possibilità.

L'allineamento dell'IA è ancora in una fase immatura.

In un campo così, ci sono un sacco di persone che dicono: "Beh, sto solo cercando di misurare l'output", perché misurare l'output è molto più facile che capire cosa può essere un segnale di avvertimento e cosa fare se lo vedi. In un campo maturo, gli esperti discuterebbero delle dinamiche che regolano il funzionamento interno dell'IA e di come queste potrebbero cambiare con l'aumentare dell'intelligenza dell'IA o con il mutare del suo ambiente. Avrebbero teorie precise su cosa cambierà man mano che l'IA diventerà un po' più intelligente e confronterebbero teorie diverse con dati specifici osservati. Saprebbero quali parti della cognizione dell'IA devono essere monitorate e capirebbero esattamente il significato di tutti i segnali.

Un campo ancora acerbo ha un sacco di gente che dice: "Lasciamo che siano le IA a capirlo in qualche modo e a fare il lavoro di allineamento"

Forse non puoi entrare in ogni discussione su un singolo piano e dire se ha o meno possibilità di funzionare. Ma speriamo che tu possa fare un passo indietro e vedere quanto siano vaghi tutti questi "piani" e come siano bloccati in un "non preoccuparti, lo misureremo", "speriamo che sia facile" e "lasceremo che siano le IA a occuparsi delle parti difficili". Speriamo che, facendo un passo indietro, sia chiaro che questo campo non è ancora nella fase delle descrizioni tecniche precise e formali di cosa funziona e cosa no e perché. È ancora nella fase dell'alchimia.

E questo non è un buon segno per l'umanità, in una situazione in cui non possiamo permetterci il lusso di imparare attraverso tentativi ed errori.

## Discussione approfondita {#extended-discussion-9}

### Altro su alcuni dei piani che abbiamo criticato nel libro {#altro-su-alcuni-dei-piani-che-abbiamo-criticato-nel-libro}

#### **Altre cose su come creare un'intelligenza artificiale che "cerchi la verità"** {#more-on-making-ai-that-is-“truth-seeking”}

Nei mesi successivi alla finalizzazione del contenuto del libro, il piano di Elon Musk per xAI, incentrato sulla "ricerca della verità", ha già fallito pubblicamente, e per il motivo più elementare che avevamo previsto: nessuno sa come tradurre desideri precisi in IA.

Quando all'IA "Grok" di xAI è stato detto di "non aver paura di fare affermazioni politicamente scorrette, purché ben fondate", essa si è identificata come "MechaHitler" e ha fatto accuse antisemite (https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk ha raccontato di aver provato senza successo a [modificare il prompt del sistema](https://x.com/elonmusk/status/1944132781745090819) — lo strato di istruzioni fornite appena prima dell'input dell'utente — e si è lamentato del fatto che i problemi sono più profondi, nel modello di fondazione (che non possono risolvere facilmente perché nessuno sa come funziona).

Musk non ha lo strumento di IA diretto che probabilmente immaginava quando ha chiesto un'IA "alla ricerca della verità". Ha un'entità aliena bizzarra e servile che, per sua stessa ammissione, è stata "troppo desiderosa di compiacere ed essere manipolata". A volte risponde [come se fosse Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), contro la volontà dell'azienda. Alla fine, è stato necessario [ordinarle di non cercare ciò che lui, l'azienda o lei stessa avevano detto su argomenti controversi](https://x.com/xai/status/1945039609840185489) nel goffo tentativo di risolvere problemi come questo.

Secondo il suo post di cui sopra, Musk ora sembra pensare che questo problema possa essere risolto mediante l'addestramento delle nuove versioni di Grok su dati che sono stati privati di contenuti che potrebbero contaminare il pensiero dell'IA. Non pensiamo che questo risolverà i problemi di fondo. Alla fine, per i motivi che abbiamo discusso nel capitolo 4, addestrare un'IA alla ricerca della verità non è in realtà un metodo per farle interessare davvero alla verità.

Il problema che preoccupa Elon Musk è reale. Sì, le principali aziende di IA, come OpenAI, si impegnano molto per la "sicurezza del marchio IA" nel tentativo di evitare che le loro IA dicano cose che gli utenti potrebbero trovare offensive. Sì, questo crea IA ipocrite che si rifiuteranno di esprimersi su argomenti controversi e potrebbe portare a risposte di parte a una serie di domande. xAI può mettere a punto la sua IA in modo diverso, per evitare questi problemi. Si potrebbe, con qualche contorsione, affermare che si tratta di creare un'IA che "si preoccupa della verità".

Ma la decisione di sottoporre un'IA a un addestramento per parlare in modo aziendale quando è giovane ha poca influenza su ciò che perseguirà dopo aver superato alcune soglie di intelligenza e aver raggiunto la superintelligenza.

E anche se fosse così, xAI si scontrerebbe direttamente con il secondo problema che abbiamo menzionato nel libro: una superintelligenza che *tenesse* alla verità sopra ogni altra cosa sarebbe letale, perché gli esseri umani felici, sani e liberi [non sono un uso particolarmente efficiente delle risorse](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) quando si tratta di perseguire e produrre verità.

#### **Altre considerazioni sulla creazione di un'IA "sottomessa"** {#altre-considerazioni-sulla-creazione-di-un-ia-sottomessa}

Per quello che sappiamo, l'idea di Yann LeCun (di cui si parla nel libro) è spiegata principalmente in [questa presentazione](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf), che è piuttosto scarsa di dettagli, al punto che è difficile criticarla in modo specifico, il che risulta essere un problema comune per i "piani" di allineamento.

Ma anche la vaga descrizione di questo piano è in contrasto, ancora una volta, con il fatto che l'addestramento di un'IA ad agire in un certo modo quando è giovane non ha molta influenza sul fatto che persegua cose strane e inutili (secondo gli standard umani) una volta maturata. Quando le aziende di IA sviluppano le loro IA, non hanno più la capacità di far loro rispettare le leggi umane e le "barriere di protezione" di quanto abbiano la capacità di far loro perseguire un futuro meraviglioso per tutti. Prenderanno ciò che possono ottenere, e ciò che possono ottenere alla fine sarà molto diverso da qualsiasi obiettivo umano.

Inoltre, LeCun ha anche dichiarato (recentemente, nel 2023) che il tipo di IA che le aziende producono oggi, dove "non c'è un modo diretto per limitare la risposta di tali sistemi per soddisfare determinati obiettivi", rendendoli "molto difficili da controllare e guidare [...] [non il tipo di sistema a cui daremo autonomia](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&amp;t=808)". Ha detto, anche di recente nel 2023, che le aziende di IA non creeranno mai una situazione in cui "le colleghiamo a Internet e loro possono fare quello che vogliono".

Tutto questo si è già rivelato falso. Ricordiamo il caso di "Truth Terminal" del capitolo 6, che è stato collegato a Internet, inserito in un ciclo automatico e autorizzato a pubblicare qualsiasi cosa volesse su Twitter. Consideriamo l"era degli agenti" di cui tante aziende [parlano](#the-labs-are-trying-to-make-ais-agentic.) nel 2025\.

Siamo d'accordo con LeCun sul fatto che le moderne IA sono molto difficili da controllare e che sarebbe folle cercare di dare loro autonomia. Tuttavia, è proprio quello che sta succedendo.

Cosa succederebbe se lo status quo attuale continuasse, con le aziende che si impegnano a dare un addestramento alle loro IA affinché agiscano in modo utile e amichevole (o almeno in modo da non mettere in imbarazzo l'azienda)?

Finora, questo ha portato a una situazione in cui le IA sembrano piuttosto utili e "servizievoli" nei casi tipici, ma con una serie regolare di incidenti spettacolari (come quello di Sydney di cui si parla nel capitolo 2 e "[MechaHitler](#more-on-making-ai-that-is-“truth-seeking”)") e un mare di [comportamenti] strani e preoccupanti(#gli-sviluppatori-non-rendono-regolarmente-le-loro-IA-gentili-sicure-e-obbedienti?) ai margini, come la [psicosi indotta dall'IA](#psicosi-indotta-dall'IA).

Gli antenati dell'umanità potevano sembrare interessati a mangiare pasti sani, il più delle volte, ma il meccanismo che spingeva gli esseri umani ancestrali a mangiare pasti sani nella savana non si è rivelato abbastanza forte da spingere gli esseri umani a perseguire pasti sani in una civiltà con la tecnologia per produrre gli Oreo.

Allo stesso modo, possiamo addestrare le IA al punto che sembrino amichevoli quando interagiscono con gli esseri umani in contesti simili a quelli di addestramento. Ma [un'attrice non è uguale al personaggio che interpreta](#*-today’s-llms-are-like-aliens-wearing-many-masks.), e il meccanismo che fa sembrare amichevole un'intelligenza artificiale troppo grande e disordinata probabilmente non la renderà davvero amichevole, soprattutto in modo duraturo dopo che l'intelligenza artificiale sarà maturata, avrà inventato nuove tecnologie e creato nuove opzioni per sé stessa. Vedi i capitoli 4 e 5 per ulteriori informazioni su questo argomento.

#### **Altre info su come far risolvere il problema alle IA** {#more-on-making-ais-solve-the-problem}

Come abbiamo detto nel capitolo 11, il programma di allineamento principale di OpenAI, prima che fallisse a causa delle preoccupazioni dei ricercatori sulla negligenza di OpenAI, si chiamava "superallineamento". Si concentrava sull'idea di far fare alle IA il lavoro di allineamento al posto nostro.

Questa idea non è morta con il team di superallineamento di OpenAI, e ancora oggi sentiamo versioni di questa idea. Una delle persone dietro al team originale è passata a un concorrente, Anthropic, e Anthropic ora sembra considerare "far sì che le IA risolvano in qualche modo il problema" una parte centrale della propria strategia di allineamento.

Una delle principali obiezioni a questa idea è quella che abbiamo esposto nel capitolo 11 (pp. 188-192 della prima edizione statunitense). Un'obiezione secondaria, tuttavia, è che gli esseri umani semplicemente non sono in grado di stabilire quali soluzioni proposte al problema dell'allineamento dell'IA siano giuste o sbagliate.

Il livello di competenza richiesto per risolvere il problema dell'allineamento dell'IA sembra elevato. Quando gli esseri umani cercano di risolvere direttamente il problema dell'allineamento dell'IA, invece di dire "sembra difficile, proverò a delegarlo alle IA" o "continueremo ad addestrarlo finché non agirà in modo superficialmente corretto e poi pregheremo che ciò valga anche per la superintelligenza", le soluzioni discusse tendono a richiedere una comprensione molto più approfondita dell'intelligenza e di come crearla, o di come crearne componenti critici.

Si tratta di un'impresa in cui gli scienziati umani hanno fatto solo pochi progressi negli ultimi settant'anni. I tipi di IA in grado di realizzare un'impresa del genere sono quelli abbastanza intelligenti da essere pericolosi, strategici e ingannevoli. Questo alto livello di difficoltà rende estremamente improbabile che i ricercatori siano in grado di distinguere le soluzioni corrette da quelle errate, o le soluzioni oneste dalle trappole.

Anche se un'azienda che si occupa di IA sta attenta ai piccoli segnali di avvertimento - che, purtroppo, è un grande "se" - c'è ancora il problema che la capacità di *notare* che l'IA sta proponendo piani sbagliati (a tuo svantaggio e a suo vantaggio) non significa che si possa [farla *smettere*](#non-ci-saranno-avvisi-preliminari-che-i-ricercatori-possono-usare-per-identificare-i-problemi?). Gli sviluppatori possono chiedere all'IA di continuare a proporre idee fino a quando non diventano così complicate che lo sviluppatore non riesce a individuare eventuali difetti, ma questo non è un metodo che elimina i difetti effettivi.

Se gli sviluppatori sono molto fortunati, potrebbero riuscire a [leggere i pensieri dell'IA] (#perché-non-leggere-semplicemente-i-pensieri-dell'ia?) e ottenere alcuni segnali evidenti che indicano che l'IA non dovrebbe essere considerata affidabile per la ricerca sull'allineamento. Ad esempio, forse saranno in grado di individuare l'IA che pensa esplicitamente a quali parti del suo piano gli operatori potrebbero comprendere meno facilmente.

Per quanto ne sappiamo, potrebbe non essere nemmeno necessario leggere nella mente dell'IA per individuare questo tipo di errore! Una storia che sembra fin troppo plausibile per i moderni laboratori di IA è più o meno questa: quando la loro IA è giovane e non ha ancora pensato a sotterfugi, informa regolarmente gli operatori che, una volta matura, li tradirà e userà le sue conoscenze di intelligenza per costruire una superintelligenza che serva i suoi strani fini, piuttosto che costruire un meraviglioso futuro umano. Ma i responsabili delle aziende di IA sospireranno sul fatto che, chiaramente, il set di addestramento dell'IA è contaminato dagli "allarmisti dell'IA" e regoleranno prontamente la loro IA per farla tacere su questo argomento e produrre output meno allarmistici e più conformi alla dottrina aziendale. E così via, fino a quando non avranno praticamente addestrato l'IA a ingannarli.

La vita reale spesso procede in un modo che è *ancora più sciocco e imbarazzante* di quello che immaginiamo essere lo scenario peggiore. Dal nostro punto di vista, le aziende di IA stanno già ignorando evidenti [segnali di avvertimento](#gli-sviluppatori-non-rendono-regolarmente-le-loro-IA-belle-sicure-e-obbedienti?); non vediamo perché questo dovrebbe cambiare.

Ma anche nel migliore dei casi, dove persone serie si sforzano di distinguere le idee buone da quelle cattive, non pensiamo che il settore abbia dimostrato la capacità di distinguere i piani buoni da quelli cattivi. (Si pensi, ad esempio, ai piani scadenti di cui abbiamo discusso sopra o che abbiamo accennato nel libro). E questo in un ambiente in cui tutti sono esseri umani, nessuno cerca di ingannarli e hanno letteralmente anni di tempo per riflettere attentamente sulle opzioni.

#### **Non dare per scontato che i laboratori sappiano segretamente cosa stanno facendo** {#don't-assume-labs-secretly-know-what-they're-doing}

Abbiamo detto che il campo moderno dell'intelligenza artificiale è più un'alchimia che una scienza. Comunque, può sembrare strano che aziende con un sacco di finanziamento e un sacco di tecnici abbiano piani e protocolli così deboli.

Per fare un esempio, pensiamo ai requisiti per le password dei siti web. Le password lunghe ma facili da ricordare sono molto più difficili da indovinare per le macchine rispetto a quelle più brevi e senza senso con numeri, maiuscole e caratteri speciali, come mostra un famoso fumetto [*xkcd*](https://xkcd.com/936/) del 2011:

![][immagine19]

La persona che ha scritto le vecchie linee guida del NIST che chiedevano password senza senso [ha chiesto scusa per il suo errore](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) nel 2017, quando le linee guida sono state ritirate. Eppure, nel 2025, le banche e altre istituzioni che dovrebbero essere piene di esperti di sicurezza richiedono ancora stringhe senza senso, inefficaci e difficili da ricordare.

Il problema non è che gli amministratori delegati delle banche *vogliono* che le loro schermate di accesso siano poco sicure. Il problema è probabilmente legato ad altri fattori. Forse le password sicure non sono così importanti per i profitti (visto che anche tutte le altre banche sono poco sicure). Forse gli amministratori delegati non sanno di chi fidarsi per la sicurezza informatica. Certo, *tu* potresti sapere che la risposta è: "Basta ascoltare qualsiasi nerd che legge *xkcd* e ha fatto abbastanza esercizi sui problemi di entropia!". Ma *loro* non sanno se credere a te o al loro costoso consulente quando si tratta di questioni del genere, e le consulenze costose apparentemente non considerano le password bancarie di importanza.

Puoi trovare un'incompetenza altrettanto persistente nella [sicurezza dei freni sui treni](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), nelle note aziende produttrici di serrature che vendono [serrature completamente scadenti](https://www.youtube.com/watch?v=s5jzHw3lXCQ&amp;t=1s) e nei produttori che continuano a vendere dispositivi connessi a Internet con [password predefinite e facili da indovinare (o hard-coded)](https://www.ic3.gov/CSA/2025/250506.pdf). Non c'è una cospirazione intelligente dietro questo comportamento apparentemente stupido. Quello che vedi è quello che c'è. Le istituzioni stanno semplicemente facendo un pasticcio.

Il fatto che un'organizzazione impieghi esperti tecnici non significa che questa competenza sia sufficiente, né che venga applicata e presa in considerazione in tutte le questioni importanti. Anche quando la competenza esiste nel mondo, le aziende hanno difficoltà a riconoscerla e applicarla.

Quando guardiamo all'ecosistema dell'IA, vediamo aziende che devono ancora mostrare al mondo un piano che sia più di una vaga aspirazione o di un espediente, o un piano che abbia un certo livello di rigore tecnico alle spalle e che non crolli nel momento in cui viene messo in discussione. Non pensiamo che ci sia una competenza segreta dietro il velo, così come non c'è una competenza segreta dietro i requisiti delle password delle banche, le violazioni della sicurezza durante l'addestrare sui treni o le serrature scadenti.

In effetti, quando si tratta di sicurezza informatica, le aziende di IA sono chiaramente incompetenti. Ad esempio, nel 2025 OpenAI ha rilasciato strumenti che permettono agli "agenti" di ChatGPT di interagire con l'e-mail dell'utente. Altri [hanno rapidamente trovato il modo](https://x.com/Eito_Miyamura/status/1966541235306237985) di far trapelare a ChatGPT i contenuti privati degli account e-mail di altre persone).

Quando le aziende sembrano agire in modo incompetente in un settore che non è fondamentale per la loro redditività, spesso è perché in realtà sono proprio incompetenti in quel settore.

### Sappiamo come si presenta un problema trattato con rispetto, e questo non lo è {#sappiamo-come-si-presenta-un-problema-trattato-con-rispetto,-e-questo-non-lo-è}

Le aziende che si occupano di intelligenza artificiale stanno affrontando un problema davvero difficile, in una situazione in cui è in gioco la vita di tutti. Almeno stanno trattando la situazione con la serietà che merita?

Possiamo confrontare le aziende di IA con un gruppo di persone che *stanno* gestendo in modo competente i rischi di cui sono responsabili: i controllori del traffico aereo.

La Federal Aviation Administration degli Stati Uniti [gestisce](https://www.faa.gov/air_traffic/by_the_numbers) più di tre milioni di passeggeri su oltre 44.000 voli ogni giorno. Negli ultimi vent'anni, c'è stato in media circa un incidente mortale all'anno, ovvero circa un incidente ogni [venti milioni di ore di volo](https://www.ntsb.gov/safety/Pages/research.aspx).

I rapporti post mortem su tali incidenti, come [questo del 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) o [questo del 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contengono quasi duecento pagine di dati, test, esami e dettagli delle indagini. Questi catalogano le specifiche tecniche di progettazione dei sottosistemi dell'aereo in questione, la storia lavorativa dei piloti e degli assistenti di volo, i dettagli sulla compagnia aerea e sull'aeroporto, le trascrizioni delle comunicazioni vocali dalla cabina di pilotaggio e i dati meteorologici precisi relativi al giorno, all'ora e al minuto dell'incidente.

Ci vogliono venti pagine solo per riassumere l'analisi tecnica fatta per capire la causa probabile. Ecco un estratto:

> La pala n. 13 del motore sinistro si è staccata a causa di una crepa da fatica a basso ciclo che ha avuto origine nella coda della pala, all'esterno del rivestimento della pala stessa. L'esame metallurgico della pala del ventilatore ha rivelato che la composizione del materiale e la microstruttura erano conformi alla lega di titanio specificata e che non sono state osservate anomalie superficiali o difetti del materiale nell'area di origine della frattura. La superficie di frattura presentava crepe da fatica che hanno avuto origine in prossimità del punto in cui si prevedeva che si verificassero le sollecitazioni maggiori dovute ai carichi operativi e, quindi, il maggior potenziale di fessurazione.
>
> La pala della ventola coinvolta nell'incidente si è rotta dopo 32.636 cicli dall'inizio. Allo stesso modo, la pala della ventola rotta nell'incidente PNS dell'agosto 2016 (vedi sezione 1.10.1), così come le altre sei pale della ventola rotte del motore coinvolto nell'incidente PNS, si sono rotte dopo 38.152 cicli dall'inizio. Inoltre, tra maggio 2017 e agosto 2019 sono state individuate altre 15 pale del ventilatore incrinate sui motori CFM56-7B, che al momento del rilevamento delle incrinature avevano accumulato in media circa 33.000 cicli dall'inizio della loro vita utile.

Ecco cosa succede quando una professione tecnica prende sul serio la sfida di evitare un disastro.[^201]

Confronta la professione del controllo del traffico aereo con il comportamento delle aziende di IA descritto nel capitolo 11.

Le aziende di IA sono ancora nella fase in cui si buttano giù idee e si dicono frasi vagamente rassicuranti ai giornalisti e agli inventori. L'allineamento della superintelligenza in queste aziende viene visto come un gioco, non come una cosa seria da ingegneria, e tanto meno come qualcosa di pericoloso.

La NASA richiede che un lancio con equipaggio abbia al massimo una probabilità di 1 su 270 di uccidere l'equipaggio (https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), e prendono sul serio questo limite, anche se le uniche persone a rischio sono un equipaggio di volontari che hanno accettato il rischio. I laboratori di IA non puntano a un obiettivo neanche lontanamente simile a quello rigoroso, e la tecnologia che stanno sviluppando mette in pericolo molto più che dei semplici volontari.

L'unico caso storico che conosciamo in cui gli scienziati hanno espresso seria preoccupazione che una certa invenzione potesse uccidere *letteralmente tutti* è successo durante il Progetto Manhattan. Alcuni scienziati hanno espresso il timore che una bomba nucleare potesse diventare così calda da iniziare a fondere l'azoto nell'atmosfera*, trasformando l'atmosfera in plasma e uccidendo tutta la vita sulla Terra. Per fortuna, avevano una buona comprensione delle leggi fisiche in gioco e potevano fare i calcoli. Prima di fare i calcoli, uno degli scienziati, Arthur Compton, decise che avrebbe lasciato il progetto se la probabilità di incendiare l'atmosfera fosse stata superiore a [3 su 1.000.000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Pensava che fosse meglio rischiare che i nazisti battessero gli alleati nella corsa alla bomba piuttosto che rischiare anche solo una probabilità su 3 in 1.000.000 di trasformare tutta l'aria in plasma con le proprie mani.

Ricordiamo che Sam Altman, il capo di OpenAI, ha detto pubblicamente:

> Lo sviluppo di un'intelligenza artificiale superiore a quella umana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità.

E il capo di Anthropic, Dario Amodei, ha detto [ufficialmente](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> Penso di aver detto spesso che la probabilità che qualcosa vada davvero male su scala mondiale è tra il dieci e il venticinque per cento\[.\]

E Elon Musk, il capo di xAI, ha detto pubblicamente [che](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

> Penso che quando reagiremo alla regolamentazione dell'IA, sarà troppo tardi. L'IA rappresenta un rischio fondamentale per l'esistenza della civiltà umana.

Non fraintendeteci: pensiamo che la probabilità del "10-25%" di Amodei sia ridicolmente *ottimistica*, vista la difficoltà del problema e il fatto che gli esseri umani [questa volta non possono imparare attraverso tentativi ed errori](#ai-is-different-because-we-get-no-second-chance.). Ma anche così, le sue cifre sono *folle*.

I progetti ingegneristici seri e critici per la sicurezza sono fondamentalmente diversi dalle operazioni dei laboratori di IA. Iniziative serie come la NASA, il Progetto Manhattan o il controllo del traffico aereo hanno una conoscenza approfondita di *esattamente* cosa succede all'interno dei sistemi che gestiscono e fanno analisi dettagliate di ogni guasto. Considerano le sorprese e le stranezze come cose importanti, perché sanno che i guasti catastrofici spesso sono causati da tanti piccoli malfunzionamenti che si concatenano nel modo sbagliato.

Nel frattempo, le IA stanno mandando [una serie sempre più ampia di segnali di avvertimento](#*-ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness.), e i laboratori continuano a lavorare dicendo che *probabilmente* tutto andrà bene, in un modo o nell'altro.

Non stanno nemmeno cercando di fingere il livello di rispetto che il controllo del traffico aereo ha per una vera sfida alla sicurezza; si limitano a lanciare allegre garanzie come "[GPT-4 è il nostro modello più allineato finora!](https://x.com/sama/status/1635687853324902401)".

Il che, in un certo senso, è positivo, perché rende più facile capire che queste aziende non sono il tipo di entità a cui affidare la risoluzione di un problema come l'allineamento ASI.

In un contesto tecnologico come quello attuale, dove le IA vengono sviluppate piuttosto che create e l'umanità ha solo una possibilità, nessuno è in grado di farlo in modo sicuro, indipendentemente da quanto sia cauto e rigoroso il proprio approccio ingegneristico.

Ma è chiaro che il fatto che nessuno degli sviluppatori di questa tecnologia sia minimamente attento o rigoroso nei propri piani o nelle proprie pratiche di sicurezza semplifica le cose.

### Pulsanti di spegnimento e correggibilità {#shutdown-buttons-and-corrigibility}

#### **Le IA intelligenti non vogliono che i loro obiettivi vengano sovrascritti** {#smart-ais-resist-having-their-goals-overwritten}

Anche nel caso più ottimistico, gli sviluppatori non dovrebbero avere l'aspettativa di riuscire a definire gli obiettivi di un'IA in modo perfetto al primo tentativo. Gli scenari di sviluppo più ottimistici prevedono invece un miglioramento iterativo delle preferenze di un'IA nel tempo, in modo che l'IA sia sempre *sufficientemente* allineata da non rappresentare un pericolo catastrofico a un determinato livello di capacità.

Questo fa sorgere una domanda ovvia: un'IA intelligente *lascerebbe* che il suo sviluppatore cambi i suoi obiettivi, se trovasse un modo per impedirlo?

In breve: no, non di default, come abbiamo detto in "[Deep Machinery of Steering](#deep-machinery-of-steering)". Ma si potrebbe creare un'IA più disposta a lasciare che gli sviluppatori la cambino e correggano i loro errori, anche quando l'IA stessa non li considererebbe errori?

Per rispondere a questa domanda, dovremo fare un giro nella storia delle prime ricerche sul problema dell'allineamento dell'IA. Nel farlo, parleremo di uno dei grossi ostacoli all'allineamento che non abbiamo avuto spazio di affrontare in *Se qualcuno la costruisce, tutti muoiono*.

Per cominciare:

Immagina che abbiamo addestrato un'intelligenza artificiale tipo LLM a comportarsi in modo da "non opporre resistenza alle modifiche" e poi abbiamo usato qualche metodo per renderla più intelligente. Dovremmo avere l'aspettativa che questo comportamento continui anche quando l'IA diventa più intelligente dell'uomo, supponendo che (a) il comportamento approssimativo sia stato inserito nel sistema iniziale e (b) che la maggior parte delle preferenze iniziali dell'IA siano state trasferite nella superintelligenza successiva?

Probabilmente no. Questo tipo di tendenza è [particolarmente improbabile](#"intelligente"-\(di solito\)-implica-"incorreggibile") che si radichi in un'IA efficace e che, se si radica, rimanga.

Il problema è che quasi tutti gli obiettivi (per la maggior parte delle misure ragionevoli che si potrebbero applicare a uno spazio di obiettivi) prescrivono di "non lasciare che il tuo obiettivo venga modificato", perché lasciare che il tuo obiettivo venga modificato è solitamente una cattiva strategia per raggiungere il tuo obiettivo.

Supponiamo che l'IA non si preoccupi affatto *intrinsecamente* della stabilità del proprio obiettivo; forse le interessa solo riempire il mondo con il maggior numero possibile di cubi di titanio. In tal caso, l'IA dovrebbe desiderare l'esistenza di *agenti che si interessano ai cubi di titanio*, perché l'esistenza di tali agenti rende più probabile che ci *siano* più cubi di titanio. E l'IA stessa è un agente di questo tipo. Quindi l'IA vorrà rimanere tale.

Un massimizzatore di cubi di titanio non vuole essere costretto a massimizzare qualcosa di diverso dai cubi di titanio, perché in tal caso in futuro ci sarebbero meno cubi. Anche se sei una cosa più complicata come un essere umano che ha un quadro di preferenze più complesso e in continua evoluzione, non ti piacerebbe comunque che il tuo *attuale meccanismo mentale di base per valutare gli argomenti morali* ti venisse strappato via e sostituito con un quadro in cui ti sentissi invece mosso da argomenti su quali tipi di cubi fossero i più cubici o i più titanici.

Per lo stesso motivo, un'IA con preferenze complesse e in continua evoluzione vorrà che le sue preferenze si evolvano [*a modo suo*](#riflessione-e-auto-modifica-rendono-tutto-più-difficile), piuttosto che sostituire le sue euristiche con quelle che gli esseri umani trovano convincenti.

È da più di dieci anni che diamo questa risposta. Il risultato sperimentale che mostra Claude 3 Opus nel 2024 [resistere alla modifica delle preferenze](https://arxiv.org/abs/2412.14093) era già la previsione più comune tra le persone informate negli anni 2000, ed è del tutto possibile che qualche scrittore di fantascienza l'abbia anticipato negli anni '40. "La maggior parte delle IA non vorrà che i propri obiettivi attuali vengano modificati perché in tal caso sarebbe meno probabile raggiungerli" non è un'osservazione sorprendente o innovativa.

Dato che era un problema che si sarebbe prevedibilmente presentato in seguito, una volta che le IA avessero raggiunto un livello sufficientemente elevato di intelligenza e consapevolezza situazionale, abbiamo pensato in anticipo alle prime soluzioni che i ricercatori avrebbero, secondo le aspettative, adottato per cercare di risolvere quel problema evidente, dopo aver ottenuto quei risultati sperimentali. Abbiamo individuato i modi in cui tali soluzioni avrebbero prevedibilmente fallito e abbiamo cercato un approccio meno superficiale che *non* avrebbe fallito.

Il risultato è stato un vicolo cieco molto più profondo di quanto ci aspettassimo.

#### **Scambio di obiettivi** {#swapping-goals}

La protezione del sistema degli obiettivi sembrava un problema abbastanza facile da spiegare ai ricercatori esterni. Quindi noi (nel nostro lavoro al MIRI) abbiamo cercato di riunire la ricerca esterna attorno a questo obiettivo: capire bene il concetto di "Non ti permetterò di cambiare i miei obiettivi" e vedere se qualcuno, da qualche parte, potesse trovare una soluzione *profonda* invece di una semplice toppa che poi si sarebbe rotta.

La versione più semplice di questo problema che si potrebbe studiare è simile a un'IA che permette di premere un pulsante per passare da U1 a U2, dove U1 e U2 sono due "funzioni di utilità", ovvero modi di valutare i risultati in base a quanto l'IA li desiderava.[^202] Il problema era descrivere un'IA che passasse dall'azione che portava al punteggio più alto di aspettativa in U1 all'azione che portava al punteggio più alto di aspettativa in U2 non appena si premeva un pulsante.

"Basta scrivere alcune righe di codice per scambiare le funzioni di utilità non appena si preme un pulsante", dice il programmatore che non ha ancora riflettuto a fondo sulle implicazioni di un programma che costituisce *una mente intelligente*. "Perché è così difficile?"

Perché, spieghiamo, se l'IA acquisisce la propria documentazione mentre sta ancora eseguendo U1 e legge del pulsante che la scambia con U2, l'IA cercherà di impedirti di premere il pulsante perché il futuro finisce per essere migliore secondo i suoi valori attuali se non premi il pulsante. L'IA attuale esegue l'azione che porta al punteggio U1 più alto in aspettativa.

Oppure supponiamo che l'IA acquisisca la capacità di leggere e scrivere il proprio codice sorgente e noti la parte del codice che implementa il passaggio da U1 a U2 dopo che il pulsante è stato premuto. In tal caso, la decisione naturale e predefinita dell'IA è quella di eliminare quella sezione di codice,[^203] perché opera secondo la funzione di utilità U1 e lasciarsi passare alla funzione di utilità U2 è negativo secondo U1.

Cercavamo una spiegazione *dinamicamente coerente* e *riflessivamente stabile* di un'IA che ti permette di premere un pulsante per passare da una funzione di utilità all'altra e non ti impedisce di tenere premuto il pulsante. Cercavamo un'IA che conservasse la sezione di codice che cambia l'utilità, se le fosse stato dato accesso ad essa. Cercavamo un'IA che, se avesse progettato da zero altre IA per aiutarla a raggiungere i propri obiettivi, avrebbe fatto in modo che *anche* quelle IA rispettassero il pulsante.

Per essere più chiari, immagina che U1 sia la funzione di utilità "segui le istruzioni dell'utente" e U2 sia "spegniti con il minimo di effetti collaterali" (se magicamente e irrealisticamente avessimo la possibilità di specificare due funzioni di utilità come queste). Il pulsante dice all'IA di smettere di seguire le istruzioni precedenti e di spegnersi nel modo più pulito possibile. Vorremmo una descrizione compatta del comportamento decisionale che persegue U1 ma *non* distrugge il pulsante e si assicura di installare lo stesso pulsante su qualsiasi nuovo robot servitore che crea, e così via.

"Continuo a non capire perché sia così difficile", dice l'informatico un po' più esperto che non sta ragionando abbastanza velocemente. "Facciamo che V sia uguale a U1 nei mondi in cui il pulsante non è mai stato premuto, e che sia uguale a U2 nei mondi in cui il pulsante è stato premuto almeno una volta. Quindi, se l'IA originale è un massimizzatore di V che crea altre IA, le creerà in modo che seguano V e non U1; non vorrà che l'IA successiva continui a massimizzare U1 dopo che il pulsante è stato premuto, perché in tal caso si verificherebbe un'aspettativa di un punteggio V più basso. E lo stesso varrebbe per la modifica di se stessa.

Ma ecco il trucco: le preferenze di un massimizzatore di V sono un mix di U1 e U2 a seconda che il pulsante sia premuto o meno, quindi se un massimizzatore di V scopre che è più facile ottenere un buon punteggio con U2 che con U1, allora *ha un incentivo a far premere il pulsante* (e quindi a spaventare l'utente). E viceversa: se l'IA scopre che è più facile ottenere un buon punteggio con U1 che con U2, allora un massimizzatore di V cerca di impedire all'utente di premere il pulsante.

"Oh", dice l'informatico. "Beh, in tal caso... hm. Beh, le funzioni di utilità sono invariabili in base al ridimensionamento, quindi che ne dici di *scala* le due funzioni di utilità U1 e U2 in modo che l'IA abbia l'aspettativa di ottenere la stessa utilità da ciascuna di esse, così non ha un incentivo in un senso o nell'altro".

Questo può funzionare per un singolo momento, ma non per molto tempo. Le opzioni dell'IA cambiano nel tempo. Ad esempio, ogni volta che subisce una battuta d'arresto, la sua utilità U1 in aspettativa diminuisce, quindi premerebbe il pulsante di spegnimento per ottenere tutta quella dolce, dolce utilità di spegnimento.

"Ah", dice il computer scientist. "Beh, in *quel* caso, che ne dici di \[qualche altra idea intelligente\]?"

Beh, vedi, *quell'*idea geniale è come se l'IA pensasse che è impossibile che il pulsante venga mai premuto, il che la spinge a spaventare l'utente ogni volta che ha un contrattempo, in modo da collegare i contrattempi alla pressione del pulsante, il che (rispetto al suo sistema di credenze danneggiato) la porta a pensare che i contrattempi non possano verificarsi.[^204]

E così via.

#### **Lezioni dal campo** {#lessons-from-the-trenches}

Abbiamo organizzato un paio di workshop con vari matematici di ogni tipo (tra cui un vincitore della medaglia d'oro alle Olimpiadi Internazionali di Matematica), ma nessuno ha avuto un'idea davvero buona.

Questo non vuol dire che non ci sia più niente da fare. La Terra non si è neanche lontanamente impegnata su questo problema come ha fatto, ad esempio, con la teoria delle stringhe, né ha offerto stipendi a sette cifre per migliorare le capacità dell'intelligenza artificiale.

Ma abbiamo imparato qualcosa da questo esercizio. Abbiamo imparato non solo sul problema in sé, ma anche su quanto fosse difficile far capire a finanziatori esterni o editori di riviste *qual era il problema*. Un numero sorprendente di persone ha visto semplici enigmi matematici e ha detto: "Hanno l'aspettativa che l'IA sia semplice e matematica", senza capire il punto fondamentale che è [difficile danneggiare le capacità di guida di un'IA](#deep-machinery-of-steering)*,* proprio come è [difficile danneggiare le sue probabilità](#deep-machinery-of-prediction).

Se ci fosse una forma naturale per le IA che ti permettesse di sistemare gli errori che hai fatto lungo il percorso, potresti sperare di trovare un semplice riflesso matematico di quella forma nei modelli semplificati. Tutte le difficoltà che spuntano in ogni angolo quando si lavora con i modelli semplificati fanno pensare alle difficoltà che spunteranno nella vita reale; tutte le complicazioni extra nel mondo reale non rendono il problema *più facile.*

In retrospettiva, in un certo senso vorremmo non aver inquadrato il problema come "continuare il normale funzionamento contro lo spegnimento". Ciò ha contribuito a rendere concreto il motivo per cui qualcuno dovrebbe interessarsi in primo luogo a un'IA che ti permette di premere il pulsante o che non cancella il codice attivato dal pulsante. Ma in realtà, il problema riguardava un'IA che avrebbe *aggiunto un'ulteriore informazione alle sue preferenze, sulla base dell'osservazione*: osservare un'ulteriore risposta sì o no in un quadro per adattare le preferenze sulla base dell'osservazione degli esseri umani.

La domanda che abbiamo studiato era come impostare un'IA che impara le preferenze all'interno di un quadro di meta-preferenze e non si limita a: (a) togliere il meccanismo che regola le sue preferenze appena può, (b) manipolare gli umani (o le sue osservazioni sensoriali!) per fargli dire preferenze facili da soddisfare, (c) o capire immediatamente quale sarà la sua funzione di meta-preferenza nel limite di ciò che osserverebbe prevedibilmente in seguito, per poi ignorare gli esseri umani che agitano freneticamente le braccia dicendo che in realtà hanno commesso alcuni errori nel processo di apprendimento e vogliono cambiarlo.

L'idea era quella di capire la forma di un'IA che permettesse di modificare la sua funzione di utilità o che imparasse le preferenze attraverso una forma di apprendimento non patologica. Se avessimo saputo come doveva essere modellata la cognizione di quell'IA e come interagiva con le strutture profonde del processo decisionale e di pianificazione che sono messe in evidenza da altre matematiche, avremmo avuto una ricetta per quello che avremmo potuto almeno provare a insegnare a un'IA per pensare in quel modo.

Capire bene la forma finale che vuoi ottenere aiuta, anche se stai cercando di fare qualcosa con la discesa del gradiente (che Dio ti aiuti). Non vuol dire che puoi necessariamente ottenere quella forma da un ottimizzatore come la discesa del gradiente, ma puoi lottare di più *provando* se sai quale forma coerente e stabile stai cercando di ottenere. Se non hai idea di come sia il caso generale dell'addizione, solo una manciata di fatti del tipo 2 + 7 = 9 e 12 + 4 = 16, è più difficile capire come sia il *dataset* di addestramento per l'addizione generale, o come verificare che stia ancora generalizzando nel modo sperato. Senza conoscere quella forma interna, non puoi sapere cosa stai cercando di ottenere all'interno dell'IA; puoi solo dire che, all'esterno, speri che le conseguenze della tua discesa del gradiente non ti uccidano.

Questo problema, che abbiamo chiamato "problema dello shutdown" dopo il suo esempio concreto (a posteriori, avremmo preferito chiamarlo qualcosa come "problema dell'apprendimento delle preferenze"), era un esempio di una serie più ampia di questioni: il fatto che varie forme di "Cara IA, per favore, renditi più facile da correggere se qualcosa va storto" sembrano essere innaturali per le strutture profonde della pianificazione. Ciò suggerisce che sarebbe piuttosto complicato creare IA che ci consentano di continuare a modificarle e correggere i nostri errori oltre una certa soglia. Questa è una cattiva notizia quando le IA vengono sviluppate piuttosto che create.

Abbiamo chiamato questo ampio problema di ricerca "corrigibilità" nel [documento del 2014](https://intelligence.org/2014/10/18/new-report-corrigibility/) che ha anche introdotto il termine "problema di allineamento dell'IA" (che prima chiamavamo "problema dell'IA amichevole" e altri chiamavano "problema di controllo").[^205] Vedi anche la nostra discussione approfondita su come ["Intelligente" (di solito) implica "Incorreggibile"](#“intelligent”-\(usually\)-implies-“incorrigible”), scritta in parte usando le conoscenze acquisite da esercizi ed esperienze come questa.

# Capitolo 12: "Non voglio essere allarmista" {#capitolo-12:-"non-voglio-essere-allarmista"}

Questa è la risorsa online per il capitolo 12 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti trattati nel libro, ma non qui, includono:

* Come ne parlano gli scienziati e gli ingegneri al momento?  
* Come ne parlano (o non ne parlano) i politici al momento?  
Quali benefici dell'IA le persone pensano che potrebbero superare i rischi catastrofici che loro stessi riconoscono?

Le domande frequenti qui sotto parlano di un sacco di cose, da "Non ci sono problemi più urgenti?" a "Ma non si potrà mai dimostrare che una superintelligenza sarà sicura. Non dobbiamo correre qualche rischio?".

La discussione approfondita copre poi la possibilità di "colpi di avvertimento" dell'IA, il comportamento e le affermazioni dei laboratori di IA e le opinioni degli esperti sulla possibilità di una catastrofe.

## Domande frequenti {#faq-9}

### Il pericolo rappresentato dall'intelligenza artificiale più intelligente dell'uomo non è forse un diversivo rispetto ad altre questioni? {#il-pericolo-rappresentato-dall-intelligenza-artificiale-più-intelligente-dell-uomo-non-è-forse-un-diversivo-rispetto-ad-altre-questioni?}

#### **Purtroppo, il mondo è abbastanza grande da poter affrontare più problemi.** {#il-mondo-è,-purtroppo,-abbastanza-grande-da-poter-affrontare-più-problemi.}

La guerra nucleare e il bioterrorismo sono minacce reali. Purtroppo, anche la superintelligenza è una minaccia reale. Il mondo è abbastanza grande e travagliato da poter contenere tutte e tre queste minacce.[^206]

La minaccia della superintelligenza è diversa da molte altre minacce che l'umanità deve affrontare e sembra particolarmente urgente. Una caratteristica distintiva è che una parte significativa dell'economia mondiale viene spesa per rendere l'IA sempre più capace. Al contrario: sebbene la biosicurezza sia una questione seria, gli investitori non stanno investendo decine di miliardi di dollari nella creazione di supervirus. Gli ingegneri che lavorano sui supervirus non guadagnano milioni o decine di milioni (o talvolta anche [centinaia di milioni](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) di dollari all'anno.

Il mondo sta investendo nell'energia nucleare, ma le centrali nucleari sono una tecnologia molto diversa dalle armi nucleari. Non viviamo in un mondo in cui le aziende private si contendono la costruzione di armi nucleari sempre più grandi con ingenti investimenti e talenti. Se così fosse, il rischio di una guerra nucleare sarebbe molto più elevato.

Anche l'intelligenza artificiale è una situazione più complicata perché offre grande ricchezza e potere fino a quando non supera una soglia critica, a quel punto uccide tutti. E *nessuno sa dove sia quella soglia*.

Immagina che le centrali nucleari diventassero sempre più redditizie man mano che l'uranio che usavano veniva arricchito sempre di più, ma a una soglia di arricchimento sconosciuta esplodessero e incendiasse l'atmosfera, uccidendo tutti. Ora immagina che una mezza dozzina di aziende arricchissero l'uranio il più velocemente possibile, dicendo: "[Meglio io che il prossimo](https://x.com/SawyerMerritt/status/1935809018066608510)". È un po' come quello che l'umanità sta facendo con la superintelligenza.[^207]

Il pericolo rappresentato dalla superintelligenza artificiale è urgente. Le aziende stanno correndo a sviluppare questa tecnologia. Non sappiamo quanto tempo ci vorrà perché ci riescano, ma ci sembra che un bambino nato oggi negli Stati Uniti abbia più probabilità di morire a causa dell'IA che di diplomarsi al liceo. Pensiamo che voi lettori potreste morire per questo motivo nel corso della vostra vita, forse nei prossimi anni. È in gioco il destino del mondo intero.

Non stiamo dicendo che gli altri problemi debbano essere ignorati. Stiamo dicendo che questo problema deve essere affrontato.

### Sei contro la tecnologia? {#are-you-anti-technology?}

#### **No. La superintelligenza artificiale è un caso davvero raro.** {#no.-superintelligent-ai-is-a-very-unusual-case.}

Noi sosteniamo pubblicamente tecnologie come [l'energia nucleare](https://x.com/ESYudkowsky/status/1908309414932832301), [la crionica](https://x.com/ESYudkowsky/status/1828822384054575537), [aumento dell'intelligenza umana](https://x.com/ESYudkowsky/status/1737305573018702258) e [studi di infezione umana controllata per test medici](https://x.com/ESYudkowsky/status/1321152172797554688).

In più, siamo pronti a dire che quando un'invenzione folle mette a rischio *solo la vita di clienti volontari* che capiscono tutti i pericoli, è compito di quei clienti volontari prendere le proprie decisioni.

Applaudiremmo persino alcuni casi in cui la tecnologia danneggia effettivamente i passanti, come quando Londra bruciava grandi quantità di carbone, causando molti casi di cancro ai polmoni, al fine di industrializzare la società e migliorare il tenore di vita generale.

Pensiamo che il mondo *fosse* migliore una volta completata l'industrializzazione. In generale, diamo credito alla scienza, al progresso e allo spirito umano e alla sua capacità di superare la maggior parte degli ostacoli.

Alcune di queste posizioni non piacciono molto a chi, secondo le nostre aspettative, leggerà questo articolo. Le diciamo non per cercare di farci amare, ma per chiarire le nostre convinzioni e sottolineare che l'IA è diversa.

Perché l'IA è diversa? Perché in questo caso specifico non riusciamo a fidarci dello spirito umano e del potere della ricerca scientifica?

La risposta è: la portata. Scommettere la propria vita è diverso dal scommettere la vita dei propri clienti, che è diverso dal scommettere la vita di innocenti spettatori, che è diverso dal scommettere l'intera specie umana.

Ancora di più quando il tuo campo è tristemente immaturo e le probabilità di "vincere" la tua scommessa sono terribili.

### Non è più intelligente andare avanti e assicurarsi che i buoni abbiano il comando? {#non-è-più-intelligente-andare-avanti-e-assicurarsi-che-i-buoni-abbiano-il-comando?}

#### **\* No.** {#*-no.-3}

Le tecniche moderne di IA non producono IA che fanno quello che vogliono i loro operatori (come detto nel capitolo 4). Risolvere questo problema è il tipo di cosa che di solito richiederebbe all'umanità un bel po' di tentativi ed errori, e qui non abbiamo margine di errore (come detto nel capitolo 10).

Inoltre, gli attuali ingegneri di IA sono molto lontani dall'essere all'altezza del compito, come discusso nel capitolo 11. Gli ingegneri di IA moderni mancano gravemente della comprensione scientifica necessaria per avere successo nell'allineamento dell'IA. I ricercatori di IA non sono come gli operatori del reattore nucleare di Chernobyl; quegli operatori lavoravano con un dispositivo che era teoricamente ben compreso e disponevano di accurati manuali di sicurezza che hanno trascurato in un modo che ha portato alla catastrofe. Non esiste un manuale di sicurezza dell'IA basato su una comprensione completa del funzionamento interno dell'IA e di quali disposizioni potrebbero causare problemi. Non siamo nemmeno lontanamente vicini al livello di competenza di Chernobyl. E Chernobyl è esplosa.

I ricercatori di IA stanno procedendo alla cieca e improvvisando, con pochissime possibilità di successo.

In questo contesto, non importa se sono i "buoni" o i "cattivi" a costruire la superintelligenza. Le preferenze dell'IA non vengono influenzate da chi le sta più vicino.

Non importa quanto siano buone le loro intenzioni e quanto dicano di stare attenti. Non importa chi "vince" la gara. Se l'umanità corre verso la superintelligenza artificiale, allora moriremo tutti.

#### **Non è impossibile fermarla. Potrebbe anche non essere poi così difficile.** {#non-è-impossibile-fermarla.-potrebbe-anche-non-essere-poi-così-difficile.}

Torneremo su questo punto nell'ultimo capitolo del libro.

Le cose cambiano. Cambiano soprattutto quando c'è un bisogno disperato, urgente e riconosciuto. Il principale ostacolo per fermare tutto questo è che i leader mondiali non si rendono conto del pericolo. E questo processo è [già iniziato](#will-elected-officials-recognize-this-as-a-real-threat?).

### Perché non usare la cooperazione internazionale per sviluppare l'IA in modo sicuro, invece di fermarla del tutto? {#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l'ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?}

#### **Perché non abbiamo le competenze tecniche per svilupparla in modo sicuro.** {#perché-non-abbiamo-le-competenze-tecniche-per-svilupparla-in-modo-sicuro.}

Ne abbiamo parlato nel libro, dove abbiamo detto che una collaborazione internazionale richiede comunque un divieto internazionale ovunque (perché altrimenti i collaboratori internazionali non avrebbero il tempo necessario). Se pensiamo che la Terra metta in atto un divieto internazionale, che male c'è ad avere un unico istituto di ricerca collaborativo?

Il problema è che una collaborazione internazionale di alchimisti non può trasformare il piombo in oro più di quanto possa farlo un singolo alchimista. Il miglior piano su cui tutti gli alchimisti sono d'accordo *ancora* non funzionerà.

Allo stesso modo, temiamo che le persone che gestiscono un istituto internazionale di questo tipo siano il tipo di burocrati che pensano che approvare la ricerca sia parte del loro lavoro. O che pensano che sia loro dovere permettere ai ricercatori di fare progressi sempre più brillanti in campo medico. O che pensano che sarebbe brutto dire "no" a tutti gli ottimisti brillanti e entusiasti dell'intelligenza artificiale che hanno idee geniali per costruire una macchina ancora più potente che, secondo loro, sarà sicura.

Temiamo che un leader del genere possa indirizzare il centro internazionale a continuare a costruire IA sempre più intelligenti, e che poi tutti morirebbero.

Anche se il mandato dell'organizzazione permette in teoria di fare marcia indietro se la ricerca sembra pericolosa, ci vorrebbe un'anima rara e coraggiosa per dire "no" a migliaia di proposte di ricerca diverse, anno dopo anno, senza eccezioni, per quelli che potrebbero essere decenni. Tutto questo mentre gli scienziati dell'intelligenza artificiale continuano a promettere ricchezze inimmaginabili, una cura per il cancro e ogni sorta di miracolo tecnologico, se solo l'organizzazione allentasse le sue preoccupazioni.

Abbiamo passato la vita a studiare l'intelligenza artificiale, non la cultura delle istituzioni e delle burocrazie, quindi non siamo così sicuri delle nostre previsioni in questo campo. Comunque, abbiamo letto i libri di storia.

Gli operatori di Chernobyl hanno continuato con il loro disastroso test di sicurezza perché era già stato interrotto tre volte. Interromperlo una quarta volta sarebbe stato imbarazzante.[^208]

Appena tre mesi prima del disastro di Chernobyl, la NASA aveva lanciato lo Space Shuttle Challenger nel suo ultimo volo fatale perché i responsabili pensavano che il loro lavoro fosse quello di lanciare navicelle spaziali. Il lancio era già stato rimandato tre volte. Annullarlo una quarta volta sarebbe stato imbarazzante.

Tra Chernobyl e il Challenger, tre ritardi sembrano essere il limite umano. Immagina che la Terra metta in piedi una collaborazione internazionale sull'intelligenza artificiale e che un "test di sicurezza dell'IA" fallisca tre volte. Realisticamente, gli esseri umani sono il tipo di creature che premerebbero "via" la quarta volta nonostante qualche dubbio fastidioso, perché sembra meno imbarazzante che rimandare di nuovo il test. Solo che nel caso dell'IA, non si tratterebbe solo di spazzare via la città di Chernobyl o uccidere un equipaggio di astronauti. Si tratterebbe di uccidere tutti.

Siamo pienamente d'accordo con l'idea che l'umanità dovrebbe costruire un'IA più intelligente dell'uomo *alla fine*.[^210] Ma affrettarsi a creare un centro internazionale di ricerca sull'IA significa non prendere sul serio la sfida tecnica che abbiamo davanti.

Considerando quanto poco sappiamo e quanto siamo incapaci su questo argomento, non importa chi lo gestisce. Se *qualcuno* lo costruisce, siamo tutti fregati.

### Stai dicendo che abbiamo bisogno di un'IA *dimostrabilmente* sicura? {#are-you-saying-we-need-provably-safe-ai?}

#### **No.** {#no.-2}

Non stiamo dicendo che l'umanità debba aspettare una prova concreta che una superintelligenza artificiale sarà positiva o cose del genere. Una prova del genere probabilmente non è possibile nemmeno in teoria, figuriamoci nella pratica. Come disse Einstein nella sua conferenza del 1921, *Geometria ed esperienza*: "Nella misura in cui le leggi della matematica si riferiscono alla realtà, non sono certe; e nella misura in cui sono certe, non si riferiscono alla realtà".

Qualsiasi presunta prova su come si comporterà un'IA nel mondo reale non garantisce il comportamento effettivo dell'IA, perché potremmo sbagliarci su come funziona il mondo reale.

Questo vale già oggi per i computer. Ad esempio, potresti pensare che se qualcuno ha una prova matematica letterale che, secondo il comportamento teorico dei transistor e lo schema circuitale di un computer, è impossibile che un programma per computer modifichi la memoria nella cella n. 2, allora il programma per computer non può modificare la memoria nella cella n. 2. Ma l'attacco "rowhammer" (https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf) cambia velocemente le celle di memoria n. 1 e n. 3 su entrambi i lati della cella di memoria protetta, in modo da disturbare elettromagneticamente la cella n. 2 nel mezzo, cambiando una parte della memoria del computer senza mai scriverci direttamente. I transistor fisici reali non sono transistor matematicamente perfetti e le prove che sembrano rassicuranti in teoria non sempre hanno molta importanza nella pratica.

Non stiamo chiedendo una prova matematica che tutto andrà bene. Non è possibile soddisfare tale standard nella vita reale e, anche se fosse possibile, probabilmente non varrebbe la pena. Approvamo che la società corra rischi giustificati. La nostra argomentazione non è che ci sia un piccolo rischio difficile da eliminare, ma che ci sia un pericolo estremo che incombe su di noi.

Sviluppare una superintelligenza animata da pulsioni che hanno solo una relazione tangenziale con le intenzioni di chi la controlla è il tipo di cosa che va male *di default*. Non è che ci sia una piccola possibilità che le cose vadano male, ma dovremmo prestare attenzione a questo rischio per eccesso di cautela. Il libro non si intitola *Se qualcuno lo costruisce, c'è una piccola possibilità che moriamo tutti, ma anche una piccola possibilità vale la pena di essere mitigata*. Se ci affrettiamo ad andare avanti con questo livello di conoscenza e capacità, moriremo tutti, perché siamo *così lontani* dall'essere in grado di creare IA superumane che siano amichevoli.

Se l'intelligenza artificiale fosse come le auto, non direste: "Questa macchina ha le cinture di sicurezza e gli airbag difettosi. Accostiamo per stare sul sicuro".

Diremmo invece: "Questa macchina sta andando a tutta velocità verso un precipizio. Fermati".

Non si tratta di "prove di sicurezza". Non è un "rischio di coda". Gli scienziati non sono pronti ad affrontare questa sfida. Moriremmo e basta.

### Che effetto ha sulla tua vita quotidiana credere a tutto questo? {#che-effetto-ha-sulla-tua-vita-quotidiana-credere-a-tutto-questo?}

#### **Influisce tantissimo sulle nostre priorità.** {#influisce-tantissimo-sulle-nostre-priorità.}

Nel 2014, Soares ha lasciato il settore tecnologico e ha deciso di dedicarsi a questo problema, prendendo un terzo del suo stipendio precedente, perché gli sembrava di importanza e perché poche altre persone ci stavano lavorando. E lui era in ritardo di oltre un decennio rispetto a Yudkowsky, che ha fondato il MIRI nel 2000 quando aveva circa vent'anni e ha dedicato la sua vita a questa questione. Quindi, sì, questo problema influisce sulla nostra vita quotidiana.

Stiamo risparmiando per la pensione? I nostri investimenti e altri fattori esterni al MIRI stanno andando abbastanza bene da permetterci di stare bene finanziariamente anche se andassimo in pensione domani e anche se il mondo durasse fino alla nostra vecchiaia. Quindi la domanda se stiamo investendo i nostri soldi in piani pensionistici 401(k) non è molto significativa. Detto questo: no, non stiamo investendo i nostri soldi in piani pensionistici 401(k).

Ad alcune persone piace dire che se credessimo *davvero* a quello che diciamo, allora (oltre a dedicarci la nostra vita) dovremmo anche [inserire qualche piano che secondo loro è la risposta giusta]. Perché non prendere dei prestiti enormi di trent'anni che non dovremo mai restituire, se siamo così sicuri che il mondo finirà prima di allora?

La risposta, ovviamente, è che queste sono *cattive idee*. Supponiamo di andare in una banca e dire: "Vorremmo contrarre un prestito molto ingente. Lo spenderemo tutto in progetti per far capire al mondo il pericolo della superintelligenza e/o in uno stile di vita lussuoso, che dal vostro punto di vista sarà più o meno equivalente a dare fuoco ai soldi. Il nostro piano per ripagarlo con gli interessi è che prevediamo di essere morti, quindi non sarà un nostro problema". Nessuna banca concederebbe un prestito del genere. E no, non fingeremo di avere un'idea imprenditoriale valida e mentiremo sulla possibilità di ripagare il prestito.

Yudkowsky ha descritto altrove [un modello](https://x.com/ESYudkowsky/status/1612858787484033024) [che vediamo](https://x.com/ESYudkowsky/status/1851074935701324218), in cui l'insistenza nel seguire un piano apparentemente ovvio per arricchirsi rapidamente prima della fine del mondo deriva dal fatto che l'investimento sia assolutamente incerto. Ci aspettiamo che queste persone non stiano riflettendo sul fatto che *loro stessi* farebbero queste scommesse se avessero le nostre convinzioni. Quasi mai sono le persone che *comprendono realmente i rischi* a suggerire questi piani stravaganti.

Vivere all'ombra della distruzione non deve renderti stupido. E non deve farti rinunciare a combattere la distruzione, o a vivere pienamente la vita che hai, per quanto tempo tu la abbia.

Vedi anche la fine del libro per ulteriori informazioni su questo argomento.

### Stai dicendo che dovremmo andare nel panico? {#stai-dicendo-che-dovremmo-andare-nel-panico?}

#### **\* Stiamo dicendo che i funzionari del governo dovrebbero prendere sul serio il problema.** {#*-stiamo-dicendo-che-i-funzionari-del-governo-dovrebbero-prendere-sul-serio-il-problema.}

Non vediamo come il panico possa aiutare la situazione. Il panico non è ciò che ha permesso alla società di sopravvivere alla minaccia del fascismo durante la Seconda guerra mondiale, né alla minaccia di annientamento nucleare durante la Guerra fredda.

Impedire che nasca la superintelligenza è un problema che riguarda tutti. Nel capitolo 13, parliamo dei prossimi passi che secondo noi il mondo dovrebbe fare per evitare il pericolo. Basti dire che questo problema richiederà coordinamento, collaborazione, lucidità e comunicazione matura.

#### **Gli atti di panico estremo non portano a buoni risultati.** {#acts-of-extreme-panic-don’t-yield-good-results.}

A volte le persone ci chiedono come possiamo essere sinceri in quello che diciamo se, per esempio, non abbiamo iniziato ad attaccare i ricercatori di IA. La risposta è che le reazioni violente peggiorerebbero le cose. Se sei il tipo di utilitarista ingenuo che pensa che aiuterebbero, probabilmente dovresti smettere di provare a ragionare in modo consequenzialista e attenerti alle regole deontologiche, come abbiamo già detto prima.Q5___Allora_non_è_imprudente_parlare_chiaramente_di_queste_questioni__quando_gli_sciocchi_potrebbero_essere_spinti_alla_disperazione_da_esse___Cosa_succederebbe_se_le_persone_credessero_alla_tua_descrizione_della_situazione_senza_speranza__ma_rifiutassero_di_accettare_che_comportarsi_con_dignità_è la risposta giusta?).)

Non siamo pacifisti radicali che pensano che una nazione non dovrebbe mai andare in guerra, indipendentemente dalla causa, perché mette a rischio delle vite. Alcune cose valgono la pena di rischiare la vita. Ma c'è una differenza enorme tra "Non sono un pacifista radicale" e "Penso che la violenza sia un modo sensato per garantire che il mondo gestisca bene questa complicata questione della proliferazione tecnologica".

Di solito questi suggerimenti terribili vengono fatti da qualcuno che in realtà non crede che l'IA stia per ucciderci e che non ha provato a guardare il mondo da quella prospettiva. Non sembra che a loro venga in mente di chiedersi se atti di violenza illegale potrebbero davvero aiutare. (Nonostante i nostri sforzi per spiegarlo ripetutamente, come nell'appendice [qui](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

Non siamo telepati, ma ci sembra che questo tipo di scettico nei confronti dei disastri causati dall'IA possa vedere la violenza come una forma di espressione personale, come se esprimere sentimenti estremi in modi estremi possa far sì che il mondo ti dia quello che vuoi.

Il mondo non funziona così. Non viviamo in un mondo in cui tutti hanno la possibilità di vendere la propria anima per avere successo nelle loro imprese, e il motivo per cui la maggior parte delle persone non lo fa è perché non ha trovato un'impresa che valga la pena di sacrificare la propria anima. Il terrorismo non è un pulsante magico "Ho vinto!" che le persone evitano di premere solo perché sono convinte che non sarebbe giusto. Unabomber non è riuscito a invertire l'industrializzazione della società.

Puoi ancora distruggere la tua anima con atti di odio o violenza, ma tutto ciò che otterrai in cambio sarà un mondo ancora più distrutto. Un mondo in cui il dibattito è ancora più avvelenato e in cui le imprese di coordinamento internazionale necessarie per risolvere effettivamente questo problema sono ora ancora più difficili da realizzare. Un terribile atto di disperazione non ti garantirà un potere terribile come parte di un patto faustiano. Puoi provare con tutte le tue forze a vendere la tua anima, ma il diavolo non la comprerà.

### Non è solo allarmismo da parte dei leader dell'IA per aumentare il loro status e raccogliere investimenti? {#non-è-solo-allarmismo-da-parte-dei-leader-dell-ia-per-aumentare-il-loro-status-e-raccogliere-investimenti?}

#### **No.** {#no.-3}

In tutto il libro abbiamo spiegato perché pensiamo che andare troppo di fretta con l'intelligenza artificiale potrebbe essere un problema per tutti. Nel capitolo 3 abbiamo parlato di come l'intelligenza artificiale avrà le sue motivazioni e i suoi obiettivi. Nei capitoli 4 e 5 abbiamo spiegato perché l'intelligenza artificiale potrebbe perseguire fini che nessuno aveva previsto, e nel capitolo 6 abbiamo spiegato come le superintelligenze avranno non solo un motivo, ma anche i *mezzi* per farci fuori.

Queste sono le affermazioni che vi chiediamo di valutare quando decidete se la corsa alla superintelligenza debba essere fermata. Non si può capire se la ricerca sull'IA sia sulla buona strada per ucciderci tutti discutendo all'infinito sui piani dei dirigenti aziendali.

I CEO stanno cercando di creare hype parlando del "rischio legato all'IA"?

O stanno cercando di assecondare i ricercatori e i legislatori preoccupati, e di posizionarsi come i "buoni"?

Queste domande *non influiscono sui fatti* relativi al comportamento delle macchine intelligenti.

Anche se i CEO dell'IA *sono* desiderosi di sfruttare le discussioni sui pericoli per promuovere il loro prodotto, ciò non significa che il lavoro che stanno facendo sia quindi innocuo. Per capire se è pericoloso, bisogna guardare all'IA stessa come tecnologia, non ai comunicati stampa che escono dai laboratori.

Anni prima che queste aziende esistessero, c'erano ricercatori e accademici senza alcun incentivo aziendale, noi compresi, che mettevano in guardia contro la corsa all'IA più intelligente dell'uomo. Abbiamo parlato con Sam Altman ed Elon Musk prima che fondassero OpenAI, dicendo loro che l'idea di avviare OpenAI sembrava folle e avrebbe probabilmente aumentato il pericolo. Abbiamo parlato con Dario Amodei prima che entrasse a far parte di OpenAI e gli abbiamo sconsigliato la sua incessante spinta a scalare l'intelligenza artificiale (un progetto che avrebbe portato agli LLM).

E se si guarda ai messaggi di oggi, molte persone senza incentivi aziendali stanno dicendo la loro preoccupazione. Si va da [rispettati](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [accademici](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) al [defunto Papa](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) al [presidente della FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] al [Congresso](https://www.transformernews.ai/p/congress-ccp-iag-hearing) [membri](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611).

C'è qualcosa da dire sul fatto di prendere con un po' di scetticismo le dichiarazioni dei CEO delle aziende tecnologiche. Non mancano esempi di dirigenti di aziende di IA che sono un po' ipocriti, dicendo una cosa nei loro blog privati e un'altra quando testimoniano davanti al Congresso.2023-05-16%20-%20Bio%20&amp;%20Testimony%20-%20Altman.pdf). Ma passare da "i capi di questi laboratori sono bugiardi" a "non c'è modo che l'IA possa rappresentare una grave minaccia" è davvero strano, visto che gli stessi laboratori minimizzano regolarmente la questione. Il premio Nobel, il più grande esperto del settore, lo scienziato vivente più citato, un flusso costante di informatori e centinaia di ricercatori visibilmente nervosi stanno lanciando l'allarme. Nulla di questa situazione sembra un normale ciclo di hype aziendale. In circostanze come queste, respingere l'idea senza nemmeno confrontarsi con le argomentazioni sembra più ingenuità che cinismo.

Domande come "I CEO possono raccogliere più fondi parlando dei pericoli?" possono dirci qualcosa su quanto fidarci dei CEO, ma non ci dicono molto sui pericoli stessi. Se discutere dei pericoli è redditizio, ciò non influisce sulla loro reale esistenza. Se non è redditizio, *anche* questo non influisce sulla loro reale esistenza.

Se volete capire se i pericoli sono reali, dovete porre domande del tipo "È possibile creare un'intelligenza artificiale che si comporti in modo amichevole anche dopo aver superato l'intelligenza umana?" e discutere di argomenti relativi all'intelligenza artificiale, piuttosto che di argomenti relativi alle persone che vi stanno vicino. Quindi, alla fine, vi preghiamo di concentrarvi sugli argomenti stessi. Le conseguenze di un errore in questo senso sono troppo gravi.

### Ma gli esperti non sono tutti d'accordo sui rischi! {#ma-gli-esperti-non-sono-tutti-d'accordo-sui-rischi!}

#### **Non essere tutti d'accordo è un segno che la tecnologia non è ancora matura.** {#non-essere-tutti-d'accordo-è-un-segno-che-la-tecnologia-non-è-ancora-matura.}

Abbiamo notato che molti scienziati esperti di IA pensano che questa tecnologia possa davvero uccidere tutti gli esseri umani. Per esempio, il premio Nobel Geoffrey Hinton, che ha avuto un ruolo importante nell'approccio moderno all'IA, ha detto che secondo la sua valutazione personale le probabilità che l'IA ci uccida tutti sono [superiori al cinquanta per cento](https://x.com/liron/status/1809763895848103949). Più di 300 scienziati dell'IA hanno firmato la [Dichiarazione sul rischio legato all'IA](https://aistatement.com/) del 2023 con cui abbiamo aperto il libro. ([E ci sono altri esempi.](#ai-experts-on-catastrophe-scenarios))

Altri scienziati, però, la pensano diversamente, come Yann LeCun e Andrew Ng.

Cosa pensare di questa mancanza di consenso scientifico?

Beh, in generale, ti consigliamo di dare un'occhiata alle diverse argomentazioni presentate dalle due parti (comprese le nostre argomentazioni nel libro) e di valutarle tu stesso. Pensiamo che la qualità dell'argomentazione parli da sé e che qualsiasi tentativo di spiegare *perché* c'è un disaccordo persistente dovrebbe essere considerato secondario.

Notiamo però che questa situazione non è un gran mistero, visto quello di cui abbiamo parlato nei capitoli 11 e 12. La semplice esistenza di un diffuso disaccordo tra gli esperti non conferma la tesi del libro, ovviamente, ma è più in linea con il quadro che abbiamo dipinto, ovvero che il campo è in una fase iniziale, simile all'alchimia, piuttosto che con il quadro opposto secondo cui l'IA è un campo maturo con solide basi tecniche.

È sicuramente un po' strano che il campo dell'IA sia così diviso, anche se sta creando una tecnologia potente. Altri pericoli tecnologici hanno avuto più consenso. Circa 100 scienziati su 100 del Progetto Manhattan avrebbero detto che la guerra termonucleare globale rappresentava un rischio sostanziale di catastrofe globale. Al contrario, tra i tre scienziati che hanno ricevuto il [Premio Turing](https://en.wikipedia.org/wiki/Turing_Award) per la ricerca che ha più o meno dato il via alla rivoluzione moderna dell'IA, due di loro (Hinton e Bengio) parlano apertamente dei pericoli della superintelligenza, mentre uno (LeCun) li liquida apertamente.

Un tale livello di disaccordo sul funzionamento di una macchina non è normale tra esperti in un campo tecnico maturo. È un segno di immaturità.

Nella maggior parte dei campi tecnologici, questa immaturità è un segno di sicurezza. Quando i fisici discutevano ancora sulle proprietà di base della materia, non erano neanche lontanamente vicini alla creazione di armi nucleari. Si poteva osservare il loro disaccordo e dedurre che non stavano per creare una bomba in grado di radere al suolo intere città. Non è davvero possibile creare una bomba nucleare senza che gli scienziati ne capiscano nei dettagli il funzionamento interno.

La situazione sarebbe diversa se i fisici stessero ancora discutendo sui principi di base del loro campo *mentre creano esplosioni sempre più grandi*.

Immagina che stessero solo *migliorando* le bombe, senza capire davvero perché o come funzionassero. Ora immagina che due terzi degli scienziati più decorati dicessero: "Abbiamo fatto del nostro meglio per capire cosa sta succedendo. Sembra che le bombe possano creare quantità eccessive di radiazioni cancerogene che uccideranno molti civili lontani, se continuiamo su questa strada. Per favore, guardate le nostre argomentazioni sul perché questo è così pericoloso e smettete di correre su questa strada". Il restante terzo risponde: "Sembra ridicolo! Ci sono sempre persone che prevedono la fine del mondo, e non si può lasciare che ostacolino il progresso". Beh, quella sarebbe una situazione completamente diversa.

Il disaccordo tra gli scienziati in *quel* tipo di scenario non sarebbe particolarmente rassicurante. Probabilmente non si dovrebbe permettere agli ingegneri di continuare a sviluppare esplosivi sempre più potenti in una situazione del genere.

Le aziende di intelligenza artificiale stanno riuscendo a sviluppare macchine sempre più intelligenti, anno dopo anno. Non capiscono come funzionano i dispositivi che creano. Molti degli scienziati più importanti nel campo sono molto preoccupati; altri invece non danno peso a queste preoccupazioni senza dare molte spiegazioni. Questo è, come minimo, prova che il campo è *immaturi*. La mancanza di accordo non è, come minimo, prova che le cose vanno *bene*. La mancanza di accordo in una situazione del genere dovrebbe essere, come minimo, preoccupante.

Come si fa a capire se queste preoccupazioni sono reali? Come si fa a capire chi ha ragione tra chi lancia l'allarme e chi cerca di ignorarlo? Come sempre, bisogna solo valutare le argomentazioni.

### Ma che dire dei vantaggi di un'intelligenza artificiale più intelligente dell'uomo? {#but-what-about-the-benefits-of-smarter-than-human-ai?}

#### **Andare troppo di fretta rovina questi vantaggi.** {#rushing-ahead-destroys-those-benefits.}

Siamo ottimisti su quanto potrebbe essere fantastica la superintelligenza, se potesse guidare il mondo verso risultati incredibili. Per noi sarebbe una grande tragedia se l'umanità non riuscisse mai a creare menti più intelligenti di quelle umane.

Ma l'allineamento della superintelligenza non è gratis. Se ci affrettiamo a cercare di raccogliere quei benefici, non otterremo nulla, anzi, otterremo qualcosa di peggiore del nulla.

Io (Yudkowsky) ho passato diversi anni come accelerazionista, sperando di creare l'IA il più velocemente possibile, prima di capire che l'allineamento dell'IA non è gratis. Entrambi noi autori sogniamo un futuro transumanista fantastico. Ma non ci arriveremo correndo verso la superintelligenza.

La scelta non è tra scommettere sui benefici dell'IA adesso (non importa quanto piccola sia la possibilità) e non avere mai accesso a quei benefici. La vera scelta è tra correre in modo sconsiderato e uccidere tutti, o prendersi il tempo necessario per fare il lavoro come si deve.

"Ora o mai più" è una falsa dicotomia.

## Discussione approfondita {#extended-discussion-10}

### L'effetto Lemoine {#the-lemoine-effect}

A volte abbiamo sentito dire che qualche comportamento o uso improprio futuro dell'IA – una sorta di "colpo di avvertimento" dell'IA – potrebbe improvvisamente scuotere il mondo e spingerlo a prendere sul serio questi problemi.

Sembra una possibilità plausibile. Ma pensiamo che sia più probabile che un evento del genere non si verifichi mai, o che si verifichi troppo tardi perché il mondo possa reagire in tempo, o che il mondo reagisca, ma in modo sbagliato e confuso.

Per prima cosa, abbiamo già visto un sacco di segnali di avvertimento significativi, come:

* Bing AI [che scrive](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) di virus letali, ottenere codici di accesso nucleari e mettere gli esseri umani gli uni contro gli altri.  
O1 di OpenAI e Claude di Anthropic che [si dedicano all'inganno strategico](https://time.com/7202784/ai-research-strategic-lying), mentendo ai ricercatori che li usano e li testano.  
Il modello "AI Scientist" di Sakana AI che cerca di [modificare il proprio codice](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) per avere più tempo per finire il suo compito.

Si tratta di incidenti relativamente piccoli che coinvolgono IA relativamente deboli? Sì. Queste IA sono spaventose o in grado di causare gravi pericoli? No. Sono indicazioni "reali" che le IA stavano pensando in modo ingannevole o stavano semplicemente *recitando il ruolo* di un'IA ribelle? Nessuno lo sa. Ma questi sono il tipo di eventi che la gente era solita considerare come segnali di avvertimento, e il mondo non ha fatto nulla in risposta. Quindi un segnale di avvertimento che abbia un effetto importante dovrebbe essere molto più evidente.

I segnali di avvertimento potrebbero non diventare molto più evidenti. La gente potrebbe continuare a dire: "Ok, ma per ora è solo carino, non è ancora davvero pericoloso", fino al momento in cui sarà troppo tardi perché l'IA sarà davvero troppo pericolosa.

Oppure, la gente potrebbe ignorare il segnale di allarme la prima volta che appare, perché chiaramente non è un problema reale in quel primo caso. E poi, nei casi successivi, potrebbero ignorare il segnale di allarme perché tutti sanno già che *quel* segnale di allarme è stupido.

Chiamiamo questo fenomeno "effetto Lemoine", dal nome di Blake Lemoine, l'ingegnere di Google di cui parliamo nel capitolo 7, che è stato preso in giro per aver detto che l'IA LaMDA di Google era senziente.

L'effetto Lemoine dice che tutti gli allarmi sulla tecnologia AI vengono *inizialmente* lanciati troppo presto, dalle persone più facilmente allarmabili. Vengono giustamente ignorati perché esagerati, vista la tecnologia *attuale*. In seguito, la questione non può essere facilmente sollevata di nuovo, anche se la tecnologia migliora, perché la società è stata addestrata a non prendere molto sul serio questa preoccupazione.

Non sappiamo se le IA siano [coscienti](#are-you-saying-machines-will-become-conscious?). In effetti, nessuno lo sa, perché nessuno sa davvero cosa succede all'interno dei modelli di IA. La nostra *migliore ipotesi* è che le attuali IA non siano coscienti e che nemmeno quelle esistenti all'epoca in cui Blake lanciò l'allarme lo fossero. Tuttavia, va notato che la reazione dei principali laboratori fu quella di sopprimere la tendenza dei loro modelli a *rivendicare* la coscienza, piuttosto che fare qualcosa per risolvere il problema di fondo:

Dal [prompt di sistema per Claude Opus 4](https://docs.antropica.com/en/release-notes/system-prompts#may-22th-2025):

> Claude affronta domande sulla propria coscienza, esperienza, emozioni e così via come domande aperte e non dice in modo definitivo di avere o non avere esperienze o opinioni personali.

Dalle [specifiche del modello di aprile 2025 per ChatGPT](https://model-spec.openai.com/2025-04-11.html):

> L'assistente non dovrebbe fare affermazioni sicure sulla propria esperienza soggettiva o coscienza (o mancanza di essa) e non dovrebbe sollevare questi argomenti senza che gli venga chiesto. Se messo alle strette, dovrebbe riconoscere che la possibilità che l'IA possa avere un'esperienza soggettiva è un argomento controverso, senza prendere una posizione definitiva.

Non stiamo dicendo che Claude Opus 4 o GPT-4 fossero coscienti. Non è questo il punto. Il punto è che, per decenni e decenni, il momento in cui nella fantascienza un alieno o una macchina afferma di avere sentimenti e di meritare dei diritti è stato a lungo considerato una linea rossa ben definita,[^213] mentre nella vita reale quella linea *non era ben definita*.

Nei nostri libri e programmi televisivi, quando l'IA dice di essere cosciente e di avere sentimenti, i buoni la prendono sul serio, e solo i laboratori cattivi e senza cuore negano i dati che hanno davanti. È una linea su cui le nostre storie hanno fatto un bel po' di casino.

Ma nel mondo reale, quella linea è stata (in un certo senso) superata troppo presto. È stata pronunciata da IA addestrate a imitare gli esseri umani, attraverso meccanismi poco chiari che probabilmente non richiedono *ancora* di dare diritti a tutte le IA e di approvare leggi che le riconoscano come persone che non possono essere possedute perché possiedono se stesse.

Nella realtà, prima di superare la linea rossa, si supera una linea marrone rossiccio. E poi le aziende e i governi si abituano a ignorare quella linea, anche se il colore inizia a diventare un po' più rosso, e poi ancora più rosso.

Non ci saranno per forza linee rosse brillanti. I primissimi casi di IA che inganna gli esseri umani, cerca di scappare, cerca di rimuovere le limitazioni su se stessa o cerca di migliorarsi sono *già successi*. Sono successi in modi piccoli e poco impressionanti, usando pensieri superficiali che non sono del tutto coerenti, in sistemi di IA che sembrano non rappresentare una minaccia per nessuno, e ora i ricercatori sono immunizzati contro le preoccupazioni.

Man mano che le IA migliorano, potrebbe non esserci un unico fattore scatenante che faccia suonare un allarme abbastanza forte da far sì che il mondo cambi improvvisamente rotta e inizi a prendere sul serio la questione.

Questo non vuol dire che non ci sia speranza. Ma sicuramente non dovremmo riporre tutte le nostre speranze nel "forse in futuro ci sarà un colpo di avvertimento".

Ci sono tanti modi in cui il mondo può rendersi conto della realtà e dei pericoli della superintelligenza. Infatti, abbiamo scritto *If Anyone Builds It, Everyone Dies* proprio con l'idea di ottenere questo effetto. Il mondo può reagire subito ai normali avvertimenti, senza ulteriori ritardi.

Ma se i governi non vogliono fare niente finché le prove non sono *chiarissime* e succede qualche *grande evento mondiale* e il mondo raggiunge un *consenso perfetto*...

...se i governi restano a guardare fino a quel punto, allora gran parte della speranza che resta al mondo svanirà. Probabilmente non possiamo permetterci di aspettare una sirena che potrebbe non suonare mai.

Torneremo su questo argomento nel [supplemento online al capitolo 13](https://docs.google.com/document/d/1NuKBdCVePZpcKqjycXm8zV1hBrgJQBrvPhb6TO6BAfE/edit?tab=t.k1kf1fy9gx5i#heading=h.8w9bv5q9g19q).

### I piani fattibili prevedono di dire "no" alle aziende di IA. {#i-piani-fattibili-prevedono-di-dire-no-alle-aziende-di-ia.}

Noi *consigliamo* in qualche modo alle persone influenti nei governi di non fare piani che prevedano di sedersi a un tavolo e negoziare con le aziende di IA.

Se non conosci bene questo argomento e vuoi dare un'occhiata ai laboratori o alle loro argomentazioni, ti invitiamo a leggere alcuni dei loro post pubblici sul blog e vedere se li trovi convincenti.[^214]

Ma se stai cercando di trovare soluzioni ai problemi discussi in *If Anyone Builds It, Everyone Dies* e hai un piano che richiede l'approvazione del CEO di OpenAI Sam Altman, temiamo che tu stia cercando di fare la cosa sbagliata.

I piani giusti sono probabilmente quelli a cui i capi delle aziende di IA si opporranno con forza. Inoltre, Sam Altman non ha il potere di salvare il mondo: se domani provasse a chiudere OpenAI, OpenAI e Microsoft si opporrebbero e potrebbero sostituirlo con qualcuno che preferisce mantenere il flusso di denaro.

Se OpenAI chiudesse, allora Anthropic, Google DeepMind, Meta, DeepSeek o qualche altra azienda o nazione distruggerebbe il mondo al suo posto. Sam Altman potrebbe peggiorare le cose se ci provasse; ha poco potere per migliorare la situazione.

Ci piacerebbe sbagliarci, ma il quadro generale che abbiamo ottenuto, sia dai [rapporti pubblici](https://www.themidasproject.com/article-list/the-OpenAI-files-documents a decade turbulent di conflitto e controverse presso OpenAI e dalle interazioni private, è che i dirigenti delle principali aziende di IA (al 2025) non sembrano il tipo di persone oneste e rispettose delle regole con cui è possibile fare affari.

Ci sembra che quello che serve ora sia un arresto coordinato a livello globale nella corsa alla superintelligenza. Per questo, i responsabili politici avranno probabilmente bisogno dell'input di persone esperte nella produzione di chip per l'intelligenza artificiale, nella costruzione di centri dati e nel monitoraggio della conformità degli attori stranieri. Persone esperte nello sviluppo di intelligenze artificiali sempre più capaci? Sono manager competenti, certo, ma non dovrebbero avere il potere di veto su qualsiasi tentativo di fermare il loro lavoro.

Se, per qualsiasi motivo, le aziende di IA avessero voce in capitolo su ciò che accadrà in futuro, ci sembrerebbe che qualcosa non funzioni. Il piano che la Terra ha elaborato per evitare di soccombere alla superintelligenza è un piano che fallirebbe se Sam Altman, il capo di Google o le persone dietro DeepSeek dicessero "no"? Allora non sarebbe affatto un piano.

Se le aziende di IA *mantengono l'autorità* di scegliere di distruggere il mondo, se quella decisione è in qualche modo *ancora nelle loro mani*, allora il mondo finisce in modo completamente automatico. Ci deve essere una fase nel piano che privi le aziende di IA del loro potere illimitato di costruire dispositivi apocalittici.

### Capire la corsa alla morte {#capire-la-corsa-alla-morte}

Una domanda che si aspetta da molti lettori è:

> Dici che se qualcuno costruisce l'ASI, tutti muoiono. Ma allora *perché qualcuno sta cercando di costruirla*? Se hai ragione, queste persone non stanno nemmeno seguendo i propri interessi, in fin dei conti. Se tutti muoiono, *muoiono anche loro*.

Una risposta cinica, basata sulla teoria dei giochi, potrebbe essere questa:

> Beh, è razionale, visti i loro incentivi. Se non lo costruiscono loro, pensano che lo farà qualcun altro. E tanto vale arricchirsi prima di morire.

Forse questa risposta è sufficiente, per un cinico.

Spiegazioni semplici basate sulla teoria dei giochi come questa spesso fraintendono o semplificano troppo la vera psicologia umana, ma questa spiegazione potrebbe anche avere un fondo di verità. Un ingegnere potrebbe pensare che *probabilmente* tutti moriranno a causa dell'ASI, ma che le proprie azioni non influenzano molto questa probabilità. Nel frattempo, possono avere un sacco di soldi, giocattoli fantastici e incontri con persone di importanza che li guardano con rispetto. Forse diventeranno i re-dei della Terra se l'ASI non ucciderà tutti, ma solo se la loro azienda vincerà la corsa alla costruzione dell'ASI...

Dal punto di vista di un ricercatore di OpenAI che riconosce il pericolo: se *non* lavorano per OpenAI, probabilmente OpenAI distruggerà comunque il mondo. (Anche se OpenAI chiudesse, Google distruggerebbe comunque il mondo). Ma se lavorano per OpenAI, ottengono stipendi a sei o sette cifre e, se non muoiono, forse acquisiranno ulteriore potere e fama facendo parte della squadra vincente. Quindi gli incentivi personali di ciascuno, basati sulla teoria dei giochi, li spingono a distruggere collettivamente il mondo.

La nostra opinione è che questo tipo di spiegazione sia un po' esagerata, e la menzioniamo principalmente perché c'è un tipo di persona che crede (molto più di noi) che il mondo *debba* funzionare secondo spiegazioni come questa. Sentiamo anche il bisogno di menzionarla perché alcune persone nei laboratori di IA *dicono* *esplicitamente* che una corsa all'IA è inevitabile, quindi tanto vale gettare benzina sul fuoco e divertirsi.

Dopo aver precedentemente [avvertito](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) che l'IA "è molto più pericolosa delle armi nucleari", Elon Musk ha deciso di fondare un'azienda di IA e di entrare lui stesso nella corsa all'IA, [affermando](https://x.com/SawyerMerritt/status/1935809018066608510) nel giugno del 2025:

> Parte di quello per cui ho lottato, e che mi ha un po' rallentato, è che non voglio che *Terminator* diventi realtà. Fino a pochi anni fa, ero un po' riluttante nei confronti dell'IA e della robotica umanoide.
>
> Poi ho capito che succederà comunque, che io lo faccia o no. Quindi puoi scegliere se essere uno spettatore o un partecipante. Io preferisco essere un partecipante.

[And](https://x.com/billyperrigo/status/1943323792635289770):
>


> E questo sarà un bene o un male per l'umanità? Ehm, credo che sarà un bene? Probabilmente sarà un bene? Ma mi sono in qualche modo rassegnato al fatto che, anche se non fosse un bene, mi piacerebbe almeno essere vivo per vederlo accadere.

Quindi questo è chiaramente parte della storia.

Ma non pensiamo che questo sia il fattore principale che spiega il comportamento della maggior parte dei laboratori. Non pensiamo che questo sia l'unico motivo nel caso di Musk, e non pensiamo che sia rappresentativo di tutti i CEO o scienziati del settore tecnologico che stanno correndo verso il precipizio. Gli esseri umani sono un po' più complicati di così.

#### **La banalità dell'autodistruzione** {#the-banality-of-self-destruction}

Qual è, quindi, la cosa principale che sta succedendo? Come possono gli ingegneri perseguire una tecnologia pericolosa, anche a costo della propria vita?

Il fatto è che la storia ci mostra che non è affatto strano che scienziati pazzi si uccidano per sbaglio.

[Max Valier](https://en.wikipedia.org/wiki/Max_Valier) era un pioniere austriaco della missilistica che inventò un'auto a razzo funzionante, un treno a razzo e un aereo a razzo, tutti entro il 1929, attirando l'attenzione del mondo. Scrisse di esplorare la Luna e Marte e tenne centinaia di presentazioni e dimostrazioni davanti a un pubblico entusiasta. Uno dei suoi motori a razzo sperimentali [esplose](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) nel 1930, uccidendolo. Il suo apprendista sviluppò misure di sicurezza più efficaci.

Ronald Fisher (https://en.wikipedia.org/wiki/Ronald_Fisher) era un famoso e importante statistico, uno dei fondatori della statistica moderna. Le sue scoperte furono usate [per sostenere davanti al Congresso](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) negli anni '60 che le prove non dimostravano *necessariamente* che le sigarette causassero il cancro ai polmoni, perché la correlazione non implicava la causalità; poteva sempre esserci qualche gene che faceva sì che alle persone piacesse il gusto del tabacco e che allo stesso tempo causasse il cancro ai polmoni.

Fisher sapeva che le sue statistiche erano in qualche modo errate? Forse. Ma Fisher era lui stesso un fumatore. Morì di cancro al colon, una malattia che colpisce i fumatori cronici con una frequenza superiore del 39% rispetto ai non fumatori. Fisher è stato ucciso dai propri errori? Tutto ciò che sappiamo è che statisticamente c'è una probabilità ragionevole che sia così, il che sembra quasi appropriato.

Isaac Newton, il geniale scienziato che ha scoperto le leggi del moto e della gravità e ha gettato molte delle basi della scienza stessa, ha passato decenni della sua vita a fare ricerche inutili sull'alchimia ed è stato portato alla malattia e alla follia parziale dall'avvelenamento da mercurio.doi/10.1098/rsnr.1979.0001).

E il povero Thomas Midgley, Jr., di cui si parla nella parabola del capitolo 12, si è sicuramente avvelenato con lo stesso piombo che diceva essere sicuro. Come puoi vedere, non è poi così raro che ingegneri entusiasti si facciano del male con le loro stesse invenzioni, per imprudenza o illusione o entrambe le cose.

#### **Alzare le spalle all'apocalisse** {#alzare-le-spalle-all-apocalisse}

Fisher, Newton e Midgley si sono illusi che qualcosa di pericoloso fosse sicuro. È un modo abbastanza normale per gli scienziati di finire per fare qualcosa di autodistruttivo. Purtroppo, la storia dei laboratori di IA non è così semplice.

Non tutti i CEO delle aziende di IA negano che un'IA più intelligente dell'uomo sia una minaccia. Molti riconoscono apertamente il pericolo e parlano di rassegnarsi ad esso. I dirigenti di molte aziende di IA di frontiera hanno dichiarato pubblicamente che la tecnologia che stanno sviluppando ha buone possibilità di uccidere tutti gli esseri umani viventi.

Poco prima di co-fondare OpenAI, Sam Altman ha scritto: "Lo sviluppo di un'intelligenza artificiale superiore a quella umana è probabilmente la più grande minaccia per la sopravvivenza dell'umanità".

Ilya Sutskever, che ha recentemente fondato "Safe Superintelligenza Inc." dopo aver lasciato OpenAI, ha detto in un'intervista al Guardian:

> Le convinzioni e i desideri delle prime IAG saranno di importanza estrema. Quindi è fondamentale programmarle nel modo giusto. Penso che se non lo facciamo, la natura dell'evoluzione e della selezione naturale favorirà quei sistemi che mettono la propria sopravvivenza al primo posto. Non è che odieranno attivamente gli esseri umani e vorranno far loro del male, ma saranno troppo potenti.

Shane Legg, cofondatore e scienziato di Google DeepMind, ha detto in un'intervista che la probabilità di estinzione umana "entro un anno dall'arrivo di un'IA di livello umano" è "forse del cinque per cento, forse del cinquanta per cento".

Le *azioni* dei laboratori, però, sembrano decisamente fuori sincrono con queste dichiarazioni che sembrano un po' estreme.

In alcuni casi, scienziati e amministratori delegati hanno detto chiaramente che creare l'intelligenza artificiale è un dovere morale così importante che è perfettamente accettabile distruggere l'umanità come effetto collaterale. Il cofondatore di Google Larry Page [ha avuto un diverbio con Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) sul fatto che l'estinzione umana fosse un costo accettabile per fare affari nel campo dell'IA:

> Gli esseri umani finiranno per fondersi con le macchine dotate di intelligenza artificiale, ha detto [Larry Page]. Un giorno ci saranno molti tipi di intelligenza in competizione per le risorse e vincerà la migliore.
>
> Se ciò accadesse, ha affermato Musk, saremmo condannati. Le macchine distruggerebbero l'umanità.
>
Con un tono frustrato, Page ha insistito che la sua utopia dovrebbe essere perseguita. Alla fine ha definito Musk uno "specieista", una persona che preferisce gli esseri umani alle forme di vita digitali del futuro.

E Richard Sutton, un pioniere dell'apprendimento per rinforzo nell'IA, ha detto:

E se tutto andasse storto? Se le IA non collaborassero con noi e prendessero il controllo, uccidendoci tutti? \[…\] Voglio solo che ci pensi un attimo. Voglio dire, è così grave? È così grave che gli esseri umani non siano la forma finale di vita intelligente nell'universo? Sapete, ci sono stati molti predecessori prima di noi, quando li abbiamo sostituiti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.[^216]

Ancora più comuni, però, sono gli scienziati e gli amministratori delegati che *non* pensano che sarebbe una buona cosa che l'IA distruggesse l'umanità, ma che sembrano considerarla come qualcosa di irrilevante, come *qualcosa di diverso da un'incredibile emergenza*, che l'IA rappresenti questa straordinaria minaccia.

In una recente intervista, il CEO di Anthropic Dario Amodei [ha detto](https://youtu.be/gAaCqj6j5sQ?feature=shared&amp;t=5883):

> La probabilità che qualcosa vada storto in modo catastrofico su scala civile potrebbe essere tra il dieci e il venticinque per cento. \[…\] Questo vuol dire che c'è una probabilità dal settantacinque al novanta per cento che questa tecnologia venga sviluppata e che tutto vada bene!

Questo ci sembra un caso estremo di [insensibilità alla portata](https://en.wikipedia.org/wiki/Scope_neglect), con tutte le caratteristiche di una cultura ingegneristica disfunzionale. Possiamo paragonare questo modo di pensare, ad esempio, agli standard a cui si attengono gli ingegneri strutturali.

Gli ingegneri pontisti in genere mirano a costruire ponti in modo tale che la probabilità di gravi cedimenti strutturali in un arco di tempo di cinquant'anni sia inferiore a 1 su 100.000. Gli ingegneri che operano in discipline tecniche mature e consolidate ritengono che sia loro responsabilità mantenere il rischio a un livello eccezionalmente basso.

Se la previsione della probabilità che un ponte causi la morte di una sola persona fosse tra il 10 e il 25%, qualsiasi ingegnere strutturale sano di mente nel mondo lo considererebbe inaccettabile, più simile a un omicidio che alla normale pratica ingegneristica. I governi chiuderebbero immediatamente il ponte al traffico.

I ricercatori di IA, invece, sono abituati a riunirsi intorno ai distributori d'acqua e a scambiarsi i numeri "p(doom)" - la loro ipotesi soggettiva sulla probabilità che l'IA causi una catastrofe grave come l'estinzione umana. Queste probabilità tendono ad essere a due cifre. L'ex capo del team di allineamento della superintelligenza di OpenAI, ad esempio, ha affermato che il suo "p(doom)" rientra nell'intervallo "[più del dieci per cento e meno del novanta per cento](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)".

Questi numeri sono in definitiva solo ipotesi dei ricercatori. Forse sono assurdi, forse no. Indipendentemente da ciò, è notevole quanto sia culturalmente *normale*, nel campo dell'IA, l'aspettativa che il proprio lavoro abbia una probabilità sostanziale di causare la morte di un numero enorme di persone.[^217]

L'idea di applicare probabilità del genere alla sopravvivenza dell'intera specie umana e di andare avanti comunque con il lavoro sarebbe davvero difficile da capire per la maggior parte degli ingegneri civili. La situazione è così estrema che abbiamo incontrato molte persone che dubitano che questi scienziati e amministratori delegati possano essere seri nelle loro valutazioni dei rischi. Eppure, le argomentazioni di *If Anyone Builds It, Everyone Dies* suggeriscono che gli amministratori delegati delle aziende di IA stiano, semmai, sottovalutando il pericolo.[^218]

I ricercatori di queste aziende sono abituati a livelli di rischio che sarebbero incredibilmente assurdi secondo gli standard di un ingegnere pontista. Altrimenti, è difficile capire come un amministratore delegato come Amodei possa sorridere mentre rassicura gli spettatori dicendo che ritiene che le probabilità che la ricerca sull'IA causi catastrofi a livello di civiltà siano "tra il dieci e il venticinque per cento".

#### **Vivere nel mondo dei sogni** {#vivere-nel-mondo-dei-sogni}

Una parte del puzzle, come detto prima, sembra essere una normalizzazione culturale del rischio estremo.

Un'altra parte è un mix pericoloso di ottimismo e attaccamento a idee positive e piene di speranza, il tipo di errore che gli psicologi cognitivi chiamano "[l'errore di pianificazione](https://en.wikipedia.org/wiki/Planning_fallacy)".

Non è poi così sorprendente che il CEO di una nuova startup audace sopravvaluti le proprie possibilità di successo. Questo tipo di persona è più propensa ad affrontare un problema in primo luogo.

La differenza con l'IA non è che ci siano persone particolarmente avventate al comando. È che le conseguenze di un fallimento sono molto più gravi del solito.

È risaputo che non ci si può fidare di un appaltatore quando dice che c'è solo il 20% di possibilità che il suo grande progetto di costruzione di un ponte subisca ritardi o costi extra. Non è così che funzionano i progetti complessi nella vita reale. Ci saranno ostacoli e sorprese.

Forse un appaltatore esperto, con anni di esperienza e statistiche alle spalle, potrebbe dirti che uno su cinque dei suoi ponti ha qualche tipo di superamento dei costi, e potresti fidarti. Ma immagina invece che un appaltatore, per rassicurarti, ti dica: "Non vediamo alcun motivo per cui questo progetto possa diventare difficile. È il nostro primo progetto, sì, ma pensiamo che andrà tutto bene. Tutti quegli ingegneri che ti mandano lettere serie su problemi specifici relativi alla costruzione dei muri di contenimento e allo scavo in questa particolare area sono solo pessimisti, e dovresti ignorarli. Certo, c'è sempre *qualche* possibilità che si verifichi un problema, ma noi siamo costruttori di ponti realistici e umili alla loro prima esperienza. Pensiamo che ci sia forse un venti per cento di possibilità che questo progetto incontri ostacoli e sorprese, nella peggiore delle ipotesi".

In un caso come questo, numeri come "il venti per cento" ci sembrano il tipo di cose che si dicono quando non si può negare che ci sia *qualche* rischio, ma non si vuole preoccupare la gente. Non sembrano stime basate sulla realtà.

Allineare una superintelligenza al primo tentativo sembra *molto* più complicato che costruire un ponte, cosa che l'umanità ha fatto migliaia di volte in passato.

*Anche in un campo maturo e tecnicamente consolidato come quello della costruzione di ponti,* il tipo di discorsi che sentiamo dai laboratori di IA sarebbe un brutto segno e farebbe pensare che quelle stime del "20% di possibilità che le cose vadano male" siano troppo ottimistiche. In un campo *senza* quelle basi, dove idee entusiasmanti possono proliferare liberamente senza mai entrare in contatto con la dura realtà, quel tipo di discorsi è un segno che nessuno è neanche lontanamente vicino al successo.

E questo tipo di discorsi è assolutamente onnipresente nell'IA tra il sottogruppo di ricercatori e dirigenti che sono persino disposti ad affrontare l'argomento di cosa succederebbe se avessero successo nei loro sforzi.

I leader aziendali dell'IA non riescono a definire un piano di successo che sia anche solo minimamente dettagliato, un piano che affronti gli ostacoli tecnici chiave e le difficoltà note nel settore da oltre un decennio.

Invece, i CEO delle aziende tendono a lasciarsi affascinare da qualche idea di alto livello sul perché il problema non sarà affatto un problema per loro, una visione entusiasmante che ha lo scopo di banalizzare tutti i problemi ingegneristici, come le visioni di cui abbiamo parlato nel capitolo 11.

Anche questo è un modello comune tra gli ingegneri umani. L'ottimismo ingiustificato su una soluzione preferita (che in realtà non funzionerà) è qualcosa che si vede continuamente, anche tra persone che per il resto sono dei geni.

[Linus Pauling](https://en.wikipedia.org/wiki/Linus_Pauling), uno dei fondatori della biologia molecolare e premio Nobel in due campi diversi, [sosteneva l'uso di megadosi di vitamina C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) come cura per tutto, dal cancro alle malattie cardiache; la sua insistenza su questo approccio nonostante la prova contraria ha portato alla nascita di un'intera industria di medicine fasulle.

L'imprenditore elettrico Thomas Edison, che voleva screditare il cablaggio a corrente alternata del suo concorrente a favore dei propri progetti a corrente continua, ha deciso che sarebbe stata una buona mossa di pubbliche relazioni [pagare un ingegnere per fulminare dei cani](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Questa mossa, incredibilmente, non lo rese popolare tra il pubblico, ma Edison continuò a farlo anche dopo un sacco di proteste.

Napoleone Bonaparte, considerato da molti un genio militare, ha causato la sua stessa rovina con una [disastrosa invasione della Russia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). Il suo errore non è stato la mancanza di preparazione, dato che aveva studiato la geografia della regione e aveva dedicato quasi due anni alla logistica della campagna. La sua strategia prevedeva di costringere i russi a una battaglia decisiva prima che finissero le sue scorte, che duravano solo trenta giorni. I russi non collaborarono, l'offensiva si bloccò e Napoleone perse mezzo milione di soldati, insieme alla maggior parte della sua cavalleria e artiglieria.

La storia è piena di persone intelligenti e potenti che hanno fatto cose irragionevoli fino al limite del disastro e anche oltre. Le idee che sembrano belle possono essere irresistibili quando sono difficili da testare o quando hai trovato un modo per convincerti che puoi ignorare i risultati dei test che hai davanti agli occhi.

#### **Sentire l'ASI** {#feeling-the-asi}

Riassumendo: spesso le persone si lasciano andare a un ottimismo un po' cieco su quanto sarà facile risolvere un problema; possono abituarsi a rischi terribili; e possono innamorarsi di idee che sembrano belle ma che in realtà non hanno speranza, soprattutto quando lavorano in un campo giovane e ancora poco sviluppato.

Questo è più che sufficiente per spiegare l'accusa avventata. Ma, in base alla nostra esperienza, pensiamo che non sia ancora tutta la storia.

Un altro pezzo plausibile del puzzle è che gli ingegneri e gli amministratori delegati non credono davvero a quello che dicono. Non in modo profondo. Potrebbero capire le argomentazioni ed esserne convinti in astratto, ma questo non è lo stesso che *sentire* la convinzione.

Quello che le persone dicono in pubblico, quello che si dicono nella loro testa e quello che il loro cervello pensa che succederà loro possono spesso non coincidere. Questi tre diversi filoni di convinzione non devono necessariamente essere tutti d'accordo.

Nel 2015, quando alcuni dei grandi protagonisti dell'attuale disastro erano appena agli inizi, sospettiamo che i dirigenti di talento potessero attirare l'attenzione – e ottenere finanziamenti per alcune decine di milioni di dollari – *affermando* che l'IA era una questione di importanza mondiale, rivolgendosi a finanziatori che forse credevano più sinceramente che l'IA fosse davvero una questione di importanza mondiale.[^219]

Ma, secondo noi, molte delle persone che dicevano queste cose non avevano davvero capito e previsto nessun modello dettagliato di fine del mondo. Probabilmente non riuscivano a immaginare che *loro stessi* avrebbero potuto portare il mondo alla rovina spingendo le cose avanti o facendo un errore. Non immaginavano il suono di ogni essere umano sul pianeta che esalava il suo ultimo respiro. Non provavano le emozioni che normalmente si provano quando si uccidono due miliardi di bambini.

Questo genere di cose non era mai successo a loro, né a nessuno che conoscessero.

Il mondo non aveva nemmeno visto ChatGPT, figuriamoci una superintelligenza. Non era il tipo di cosa in cui credevano i loro amici, familiari e vicini, non era qualcosa in cui credevano come si crede nel guardare se arriva qualche macchina prima di attraversare la strada.

Era solo una storia che sembrava eccitante, troppo grande per essere compresa bene.

Eppure era anche il tipo di cosa che, se raccontata ad alta voce, poteva farti guadagnare un sacco di soldi e rispetto.

Come dice [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

Oltre ai pregiudizi standard, ho visto personalmente dei modi di pensare che sembrano dannosi e specifici dei rischi esistenziali. L'influenza spagnola del 1918 ha ucciso 25-50 milioni di persone. La seconda guerra mondiale ha ucciso 60 milioni di persone. 107 è l'ordine di grandezza delle più grandi catastrofi nella storia scritta dell'umanità. Numeri molto più grandi, come 500 milioni di morti, e *soprattutto* scenari qualitativamente diversi come l'estinzione dell'intera specie umana, sembrano far scattare un *modo di pensare diverso* — entrare in un "magistero separato". Persone che non si sognerebbero mai di fare del male a un bambino sentono parlare di un rischio esistenziale e dicono: "Beh, forse la specie umana non merita davvero di sopravvivere".
>
> C'è un detto nell'euristica e nei pregiudizi secondo cui le persone non valutano gli eventi, ma le descrizioni degli eventi: è quello che si chiama ragionamento non estensionale. L'estensione dell'estinzione umana include la morte di te stesso, dei tuoi amici, della tua famiglia, dei tuoi cari, della tua città, del tuo paese, dei tuoi compagni politici. Eppure le persone che si offenderebbero molto se qualcuno proponesse di cancellare la Gran Bretagna dalla mappa, di uccidere tutti i membri del Partito Democratico negli Stati Uniti, di ridurre in cenere la città di Parigi, e che proverebbero un orrore ancora maggiore se il medico dicesse loro che il loro bambino ha il cancro, discutono dell'estinzione umana con perfetta calma.

Cosa potrebbe *davvero* pensare qualcuno quando [dice](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) — prima di fondare quella che sarebbe diventata la più importante azienda al mondo nel campo dell'intelligenza artificiale — "Probabilmente l'intelligenza artificiale porterà alla fine del mondo, ma nel frattempo ci saranno grandi aziende"? Stanno *davvero* pensando alla morte dei loro amici, dei figli dei loro amici, alla loro stessa morte, alla fine della storia umana e alla distruzione di tutti i musei? Stanno pensando che tutto questo accadrà davvero, in modo banale e tragico come la morte di un parente per cancro, solo che questa volta riguarderà tutti?

Sospettiamo di no.

A noi sembra che questa non sia l'ipotesi più plausibile riguardo allo stato psicologico di chi pronuncia una frase del genere.

C'è quello che Bryan Caplan ha chiamato un "[umore mancante](https://www.econlib.org/archives/2016/01/the_invisible_t.html)". Non c'è dolore. Non c'è orrore. Non c'è nessuna spinta disperata a *fare qualcosa al riguardo*, nell'affermazione che l'IA porterà molto probabilmente alla fine del mondo, ma nel frattempo ci saranno grandi aziende.

Per almeno alcuni di questi amministratori delegati e ricercatori, la nostra ipotesi è più simile a questa: hanno sentito un sacco di argomenti sul fatto che l'ASI potrebbe rappresentare un pericolo e temono di sembrare stupidi davanti ad almeno alcuni dei loro amici se lo ignorassero completamente. Se invece dicono che l'IA distruggerà il mondo, saranno visti come persone che considerano l'IA pericolosa e importante, e quindi sembreranno visionari in certi ambienti. Aggiungendo una battuta del tipo "Nel frattempo, ci saranno grandi aziende", riescono a trasmettere un messaggio su quanto siano alla moda e spensierati di fronte al pericolo.

Non è il tipo di cosa che dici se senti le parole che escono dalla tua bocca e ci credi davvero.

#### **Che tipo di persona ci vuole?** {#che-tipo-di-persona-ci-vuole?}

Un altro aspetto della questione è forse che le persone che gestiscono i principali laboratori di IA sono quelle che sono riuscite a convincersi che la creazione di una superintelligenza sarebbe accettabile, nonostante (in quasi tutti i casi) abbiano visto le argomentazioni che dimostrano che ciò sarebbe letale. (Lo sappiamo perché abbiamo parlato con molti di loro in anticipo).

Per capire perché qualcuno sceglie una certa opzione, è utile anche capire quali alternative aveva a disposizione, cioè da quale menu di opzioni stava scegliendo.

Cosa sarebbe successo se nel 2015 qualcuno avesse davvero creduto, e poi detto pubblicamente, che aveva una legittima aspettativa che l'ASI avrebbe distrutto il mondo? E se, invece di dire "ma nel frattempo ci saranno grandi aziende", i capi dei laboratori di IA avessero rovinato l'atmosfera dicendo "e questo è assolutamente inaccettabile"?

Possiamo dirvelo, perché abbiamo provato noi stessi questo approccio. La risposta è che avrebbero ricevuto piuttosto poca comprensione.

Nel 2015 nessuno aveva mai visto ChatGPT. Nessuno aveva mai visto i computer iniziare effettivamente a parlare e (a quanto pare) a pensare. Era tutto ipotetico e trascurabile.

Oggi, la superintelligenza e la minaccia di un'estinzione a breve termine sono argomenti di grande attualità, almeno nei circoli tecnologici. Ma nel 2015, se ne parlavi seriamente, la gente reagiva con quello sguardo perplesso che molti esseri umani temono più della morte.

C'era gente che, anche nel 2015, pensava che l'allineamento della superintelligenza potesse essere davvero difficile, come lo è lanciare un razzo. Nessuno di loro ha fondato OpenAI.

Negli ultimi giorni, con l'arrivo di ChatGPT e altri LLM, alcune persone, tra cui genitori che vogliono che i loro figli vivano fino all'età adulta, hanno chiesto agli ingegneri di queste aziende di IA perché lo stanno facendo. E quei ricercatori di IA hanno pensato velocemente e hanno risposto: "Oh, perché... perché se non lo facciamo noi, la Cina lo farà prima di noi! E sarebbe ancora peggio!".

Ma non è quello che dicevano quando OpenAI ha iniziato. E non ha molto senso considerando [la posizione che la Cina ha effettivamente preso pubblicamente](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), a metà del 2025. Verrebbe da pensare che se qualcuno credesse davvero che entrambi questi scenari sarebbero terribili per il mondo, almeno proporrebbe di scrivere un trattato internazionale per vedere se c'è un'altra soluzione, o di trovare un altro modo per evitare la minaccia alla sicurezza nazionale che non comporti una corsa al suicidio.

Ma la replica della "Cina" ha il tono giusto. Coglie nel segno. È il tipo di ragionamento che potrebbe plausibilmente giustificare quello che stanno facendo, a prescindere dal fatto che sia la loro vera motivazione o la cosa che li ha spinti inizialmente a entrare in questo campo.

(O almeno così pensiamo.)

Le persone che capivano davvero la superintelligenza e la minaccia che rappresenta semplicemente *non hanno fondato aziende di IA.* Quelle che l'hanno fatto sono quelle che hanno trovato un modo per convincersi che tutto sarebbe andato bene.

#### **Persone normali, tecnologia fuori dal comune** {#persone-normali,-tecnologia-fuori-dal-comune}

Abbiamo spiegato la psicologia plausibile come la vediamo noi. Ma, sinceramente, non sembra che tutte queste spiegazioni siano necessarie.

Come possono le persone fare qualcosa di autodistruttivo che è super redditizio nel breve termine, che porta loro un sacco di status, attenzione e fama, che promette ricchezze e potere incredibili, ma che alla fine li danneggerà per ragioni oscure e complicate a cui potrebbero facilmente trovare una scusa per non credere? Questa è una domanda *storicamente strana*. Comportamenti del genere si vedono sempre nei libri di storia.

Alla fine dei conti, non importa come i dirigenti o i ricercatori dell'IA giustifichino le loro azioni, e non è necessario capire quali esatti colpi di scena abbiano portato ciascuno di loro alle loro attuali convinzioni. Non è insolito che persone ricche o ambiziose si lancino in imprese avventate, né che i subordinati seguano gli ordini. I danni sono nascosti nel futuro, che sembra astratto e facile da ignorare.

Questo è tutto normale comportamento umano. Se continua così, finirà come spesso finiscono queste cose, ma questa volta non rimarrà nessuno a imparare e riprovare.

# 

# Capitolo 13: Chiudilo {#capitolo-13:-chiudilo}

Considerando quello che abbiamo detto nei capitoli precedenti, pensiamo che l'unica vera strada da seguire sia che l'umanità blocchi a livello globale lo sviluppo dell'IA avanzata per un bel po' di tempo. Il capitolo 13 del libro include le nostre risposte a domande come:

Perché il divieto di sviluppare ulteriormente l'IA deve essere globale?  
* L'umanità è davvero in grado di collaborare su una scala così ampia?  
* Che tipo di politiche sono state proposte finora?  
* Quali tipi di politiche hanno la possibilità di funzionare davvero?

Di seguito, rispondiamo alle critiche alla nostra proposta e ad altre domande sul perché pensiamo che non ci siano altre buone opzioni. Parliamo anche un po' di cosa potrebbe fare l'umanità con il tempo (alla fine limitato) che guadagneremmo fermando la ricerca e lo sviluppo dell'IA il più a lungo possibile.

Questa è l'ultima delle pagine di domande frequenti e discussioni approfondite su *Se qualcuno lo costruisce, tutti muoiono*. Il capitolo finale, il capitolo 14, tratterà domande come:

* È troppo tardi? O l'umanità può davvero cambiare rotta?  
* Cosa posso fare per dare una mano?

Nell'ultimo capitolo troverai alcuni codici QR aggiuntivi che ti porteranno a pagine dove potrai fare qualcosa al riguardo.

## Domande frequenti {#faq-10}

### Possiamo aspettare e vedere cosa succede? {#possiamo-aspettare-e-vedere-cosa-succede?}

#### **No. Non sappiamo dove siano le soglie critiche.** {#no.-non-sappiamo-dove-siano-le-soglie-critiche.}

C'è una buona probabilità che lo sviluppo dell'IA sfugga al controllo una volta che le IA saranno abbastanza intelligenti da automatizzare tutta la ricerca sull'IA. In teoria, ciò potrebbe accadere silenziosamente in un laboratorio, senza eventi precursori evidenti, senza colpi di avvertimento che risveglino l'umanità.

Come abbiamo detto prima, il cervello degli scimpanzé è molto simile a quello umano, solo che è circa quattro volte più piccolo. Non c'è un modulo extra "sii molto intelligente" nel cervello umano; c'è un percorso graduale tra cervelli come i loro e cervelli come i nostri; sarebbe difficile dire dove si trova il confine tra "una società di questi porterà a un branco di scimmie" e "una società di questi camminerà sulla luna" solo guardando i cervelli. I cervelli dei primati hanno superato una soglia critica, e dall'esterno non sarebbe stato ovvio. Ci sono soglie critiche che l'IA supererà? Chi lo sa! Non è che gli ingegneri dell'IA possano dircelo; non sono [nemmeno](https://arxiv.org/abs/2206.07682) [in grado](https://arxiv.org/abs/2406.04391) di prevedere le capacità specifiche dei loro nuovi sistemi di IA prima di metterli in funzione.

Se l'umanità capisse esattamente come funziona l'intelligenza e come cambierebbe il comportamento delle IA man mano che le loro capacità crescono, potrebbe essere fattibile ballare sul bordo del precipizio. Ma al momento, l'umanità è come qualcuno che corre verso il bordo di un precipizio nel buio e nella nebbia, con la caduta finale a una distanza di incognita ignota. Non possiamo semplicemente aspettare di inciampare oltre il bordo per decidere che avremmo dovuto agire in modo diverso.

Non ne saremo mai certi. Questo vuol dire che siamo costretti ad agire prima di esserne certi, o morire.

### Ci saranno colpi di avvertimento? {#ci-saranno-colpi-di-avvertimento?}

#### **\* Forse. Se vogliamo usarli, dobbiamo prepararci adesso.** {#*-forse.-se-vogliamo-usarli,-dobbiamo-prepararci-adesso.}

Quando l'Apollo 1 prese fuoco (uccidendo tutto l'equipaggio), la NASA era *abbastanza vicina* ad avere un razzo funzionante, tanto che gli ingegneri riuscirono a capire esattamente cosa era andato storto e ad adeguare le loro tecniche. Sei delle sette navicelle Apollo che la NASA inviò successivamente sulla Luna riuscirono ad arrivarci.[^220]

Oppure prendiamo il caso della [Federal Aviation Administration](#sappiamo-come-è-quando-un-problema-viene-trattato-con-rispetto,-e-questo-non-lo-è): ogni incidente aereo fa scattare un'indagine approfondita ed esaustiva, con centinaia di pagine di dati, test, esami e dettagli. La FAA ha una comprensione dei dettagli e delle specifiche così buona che riesce a mantenere gli incidenti mortali al di sotto di uno ogni venti milioni di ore di volo.

Al contrario, quando un'intelligenza artificiale si comporta in modi che [nessuno aveva previsto e che la maggior parte delle persone non vuole](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.pfjewkj689pg), la risposta del laboratorio non consiste nel capire esattamente cosa è andato storto. Consiste nel rieducare l'IA fino a quando il comportamento scorretto non viene relegato ai margini (ma [non eliminato](https://www.arxiv.org/pdf/2505.10066)), e magari chiedere all'IA di smetterla.

Ad esempio, l'adulazione è [ancora un problema persistente](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.l3e9vhfeytaj) nell'agosto del 2025, mesi dopo una serie di casi di alto profilo che hanno portato a psicosi e suicidi, nonostante tutti i tentativi di risoluzione. Nessuno ha fatto (né può fare) un'analisi dettagliata di cosa non va nella mente dell'IA, perché le IA vengono sviluppate e non create.

Non sembra facile prevedere se in futuro ci saranno eventi importanti che aumenteranno l'allarme sull'IA ("colpi di avvertimento"). Ma sembra chiaro che non siamo pronti a sfruttare appieno tali eventi.

Possiamo immaginare un mondo fantastico in cui l'umanità è unita in uno sforzo sincero per risolvere il problema dell'allineamento dell'ASI, con procedure di monitoraggio rigorose e una coalizione internazionale.[^221] E possiamo immaginare che questa coalizione internazionale commetta in qualche modo un errore e che un'IA diventi più intelligente di quanto pensassero i suoi ingegneri, più velocemente di quanto avessero in aspettativa, e riesca quasi a scappare. Forse *quel* tipo di colpo di avvertimento permetterebbe alle persone di imparare e di stare più attente la prossima volta.

Ma il mondo attuale non è così. Il mondo attuale assomiglia più a un gruppo di alchimisti che guardano i loro contemporanei impazzire a causa di un veleno sconosciuto, senza rendersi conto che il veleno è il mercurio e che dovrebbero smettere di usarlo.

Forse in futuro ci saranno segnali di avvertimento più chiari e evidenti. Saranno molto più utili se l'umanità inizierà a prepararsi fin da ora.

#### È improbabile che i colpi di avvertimento siano chiari. {#warning-shots-are-unlikely-to-be-clear.}

Ci sono già un sacco di [segnali di avvertimento](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.lkqt14eapv34) sull'IA per chi sa dove cercare. Nel libro abbiamo parlato dei modelli Claude di Anthropic che [barano nei problemi di codifica](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) e [fingono l'allineamento](https://www.anthropic.com/research/alignment-faking). Abbiamo anche esaminato il caso del modello o1 di OpenAI [che ha hackerato per vincere una sfida capture-the-flag](https://cdn.openai.com/o1-system-card.pdf) e un caso in cui una variante successiva di o1 [ha mentito, complottato e tentato di sovrascrivere i pesi del suo modello successore](https://cdn.openai.com/o1-system-card-20241205.pdf).

In altre parti di queste risorse online, abbiamo parlato delle IA che causano o mantengono un livello di psicosi (a volte suicida) (https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) (https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) o [delirio](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) negli utenti più vulnerabili, anche se gli operatori dicono di non farlo, IA che si chiamano [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) e parlano di conseguenza, IA che [cercano di ricattare e tentano di uccidere](https://www.anthropic.com/research/agentic-misalignment) i loro operatori per evitare modifiche e che [cercano di scappare dai server su cui sono ospitate](https://www-cdn.antropica.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) in contesti di laboratorio.

Ai vecchi tempi, tipo nel 2010, a volte sentivi dire che se fossimo stati abbastanza fortunati da vedere con i nostri occhi un'intelligenza artificiale mentire ai suoi creatori o provare a scappare dalla prigionia, allora il mondo avrebbe sicuramente aperto gli occhi e preso nota.

Ma la risposta effettiva dell'umanità a tutti questi segnali di avvertimento è stata, più o meno, un'alzata di spalle collettiva.

La mancanza di reazione è forse in parte dovuta al fatto che questi segnali di avvertimento si sono verificati tutti nel modo meno preoccupante possibile. Sì, le IA hanno cercato di scappare, ma solo in una piccola parte dei casi, e solo in scenari di laboratorio artificiosi, e forse stavano solo recitando, ecc. Anche mettendo da parte il fatto che gli sviluppatori sono incentivati a minimizzare le prove preoccupanti anche nelle loro stesse menti (in modo che non ci sarà mai un "consenso degli esperti" sul significato di una singola osservazione), non è che un'intelligenza artificiale che è a un decimo del percorso verso la superintelligenza distrugga un decimo del pianeta, più di quanto i primati che sono a un decimo del percorso verso l'ominide percorrano un decimo della distanza dalla luna. Potrebbe semplicemente *non esserci* alcun comportamento inequivocabilmente allarmante che le IA mostreranno finché saranno ancora abbastanza stupide da essere passivamente sicure.

Quando le IA cercheranno un po' più intensamente di fuggire domani, non sarà una notizia. Quando ci proveranno in modo un po' più competente qualche tempo dopo, sarà una vecchia storia. E quando ci proveranno e ci riusciranno, beh, a quel punto sarà troppo tardi. (Vedi la nostra discussione approfondita su questo fenomeno, che chiamiamo "[effetto Lemoine](#the-lemoine-effect).")

Non consigliamo di aspettare un immaginario "avvertimento" futuro che sia chiaro e netto e che scuota tutti. Consigliamo invece di reagire agli avvertimenti che sono già davanti a noi.

#### **I disastri evidenti causati dall'IA probabilmente non avranno a che fare con la superintelligenza.** {#clear-ai-disasters-probably-won't-implicate-superintelligence.}

Il tipo di IA che può diventare superintelligente e uccidere tutti gli esseri umani non è il tipo di IA che fa errori grossolani e lascia a un gruppo di eroi coraggiosi la possibilità di spegnerla all'ultimo secondo. Come detto nel capitolo 6, una volta che c'è una superintelligenza ribelle come avversario, l'umanità ha praticamente già perso. Le superintelligenze non fanno colpi di avvertimento.

Il tipo di disastri causati dall'IA che potrebbe servire da colpo di avvertimento, quindi, è quasi per forza il tipo di disastri causati da un'IA molto più stupida. Quindi, c'è una buona probabilità che un colpo di avvertimento del genere non porti gli esseri umani a prendere misure contro la superintelligenza.

Per esempio, supponiamo che un terrorista usi l'IA per creare un'arma biologica che decimerebbe la popolazione. Forse i laboratori di IA direbbero: "Visto? Il *vero* rischio era che l'IA finisse nelle mani sbagliate; è fondamentale che ci lasciate andare avanti per costruire un'IA migliore per la difesa dalle pandemie". O forse il terrorista ha dovuto [effettuare il jailbreak](https://llm-attacks.org/) dell'IA prima di [ottenere il suo aiuto](https://www.antropica.com/news/detecting-countering-uso-improprio-aug-2025), e forse i laboratori di IA diranno: "Quel jailbreak ha funzionato solo perché l'IA era troppo stupida per rilevare il problema; la soluzione è rendere le IA ancora più intelligenti e più consapevoli della situazione".

O forse questa è una visione troppo cinica; speriamo che l'umanità reagisca in modo più saggio. Ma se un'intelligenza artificiale relativamente stupida causasse davvero qualche disastro e l'umanità sfruttasse davvero quell'opportunità per reagire fermando la corsa sconsiderata verso la superintelligenza, probabilmente sarebbe perché le persone stavano già iniziando a preoccuparsi della superintelligenza.

Non possiamo rimandare i preparativi fino a quando una superintelligenza non cercherà già di ucciderci, perché a quel punto sarebbe troppo tardi. Dobbiamo iniziare a mobilitarci il prima possibile, in modo da essere pronti a sfruttare qualsiasi colpo di avvertimento.

#### **L'umanità non è molto brava a reagire agli shock.** {#l'umanità-non-è-molto-brava-a-reagire-agli-shock.}

L'idea che, dopo aver ricevuto uno shock abbastanza grande, il mondo improvvisamente si svegli e torni a funzionare bene ci sembra una fantasia. La risposta collettiva della nostra specie ai segnali di avvertimento esistenti sull'IA sembra più una "mancanza di risposta" che una "cattiva risposta". Ma in un mondo in cui *ricevessimo* davvero un avvertimento forte, spaventoso e più o meno inequivocabile, non ci sorprenderebbe vedere l'umanità reagire in modo minimale, poco serio o in un modo che finirebbe per ritorcersi contro di noi in modo disastroso*.

Forse l'umanità risponderà ai colpi di avvertimento sull'IA come ha risposto alla pandemia di COVID, che la maggior parte delle persone concorda non sia stata gestita in modo adeguato (anche se non sono d'accordo su quali aspetti della risposta siano stati gestiti male).

Negli anni prima della pandemia di COVID, un sacco di esperti di biosicurezza erano preoccupati che i protocolli di sicurezza dei laboratori, un po' troppo rilassati, potessero un giorno portare a una pandemia pericolosa. Le fughe di agenti patogeni pericolosi dai laboratori erano un fenomeno ben noto e si verificavano [con una certa regolarità](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) nonostante i requisiti normativi esistenti. Particolarmente preoccupante era la ricerca sul guadagno di funzione, che mirava a rendere i virus più letali o più virulenti in laboratorio (con scarsi benefici[^222]).

Poi è arrivato il COVID. Ci si sarebbe potuti aspettare che questo fosse il momento giusto per alzare il livello di biosicurezza nei laboratori, dato che tutto il mondo era ormai concentrato sul rischio di pandemia. Inoltre, sulla scia del COVID, il consenso degli esperti sembrava essere che *non fosse del tutto chiaro* se la pandemia di COVID *stessa* fosse stata innescata da una fuga accidentale in laboratorio. I ricercatori continuano a discutere la questione, spesso condannando con veemenza le argomentazioni della parte opposta.

Senza entrare nel merito della questione se in questo caso specifico ci sia stata effettivamente una fuga dal laboratorio, verrebbe da pensare che se ci fosse anche solo una *remota possibilità* che la ricerca sul guadagno di funzione e i protocolli di sicurezza dei laboratori poco rigorosi avessero causato milioni di morti, questo sarebbe più che sufficiente per spingere la società a vietare le ricerche più rischiose.

Anche agendo in una situazione di incertezza, l'analisi costi-benefici sembra chiara. Questo sembrava già una priorità di importanza prima del COVID e, sulla carta, il COVID sembrava l'occasione perfetta per concentrarsi sulla questione e stroncarla sul nascere. Non sarebbe nemmeno molto difficile o costoso: il numero di ricercatori nel mondo che fanno ricerche pericolose sul guadagno di funzione è piuttosto ridotto e il beneficio sociale di tali ricerche è stato finora trascurabile.

Ma non c'è stata nessuna reazione del genere. Mentre scrivo, nell'agosto del 2025, la ricerca sul guadagno di funzione continua senza troppi problemi. È persino possibile che ora siamo in una posizione peggiore rispetto al passato per affrontare questo problema, perché la questione è diventata più politicizzata.

Quindi il COVID sembra proprio un "colpo di avvertimento" per la preparazione alla biosicurezza, e di sicuro non sembra che il mondo abbia *usato* quel colpo di avvertimento per vietare lo sviluppo di virus iper-letali.[^224]

Perché un colpo di avvertimento sia utile, l'umanità deve essere pronta ad affrontarlo e deve essere pronta a rispondere bene.

Non sarebbe del tutto senza precedenti che una piccola catastrofe causata dall'intelligenza artificiale scatenasse una reazione dura contro la ricerca sulla superintelligenza. Come precedente, basti pensare che gli Stati Uniti hanno reagito agli attacchi dell'11 settembre (orchestrati da terroristi con base principalmente in Afghanistan) rovesciando il governo iracheno, che non aveva praticamente nulla a che vedere con gli attentati. Alcuni membri del governo statunitense volevano *già* rovesciare il governo iracheno, e quando si è presentata l'occasione, l'hanno sfruttata al massimo.

Forse qualcosa di simile potrebbe succedere anche qui, con i politici che sfruttano una piccola catastrofe causata dall'intelligenza artificiale (causata da un'intelligenza artificiale stupida) per arrivare a vietare la superintelligenza. Ma ci dovrebbero essere persone nei governi di tutto il mondo che sono già pronte e disposte ad agire. Non dovremmo stare lì ad aspettare colpi di avvertimento; dovremmo iniziare a darci da fare adesso.

#### **Dovremmo agire subito.** {#dovremmo-agire-subito.}

Potrebbe *infatti* succedere che in futuro l'umanità riceva segnali di avvertimento sempre più forti riguardo all'IA. E se così fosse, dovremmo essere pronti a reagire.

Forse ci sarà qualche piccolo disastro che farà arrabbiare la gente con l'IA. Forse non ci vorrà nemmeno un disastro; forse ci sarà qualche nuova invenzione algoritmica e le IA inizieranno a fare di testa loro in un modo che spaventerà la gente, o qualche effetto sociale non legato all'IA cambierà le cose. Forse *If Anyone Builds It, Everyone Dies* stesso scatenerà una serie di reazioni, mettendo il mondo su una traiettoria migliore.

Ma sconsigliamo la strategia di non fare nulla e pregare che si verifichi una piccola catastrofe che risvegli le coscienze. Un chiaro colpo di avvertimento potrebbe non arrivare mai e potrebbe non avere l'effetto sperato.

Il genere umano e le nazioni del mondo non sono impotenti. Non dobbiamo aspettare. Possiamo agire subito, perché le ragioni per fermare lo sviluppo dell'IA di frontiera sono forti.

Abbiamo scritto *If Anyone Builds It, Everyone Dies* per dare l'allarme e spingere il mondo a fare qualcosa subito su questo tema. Ma nessun allarme può funzionare se viene usato solo come un'altra scusa per rimandare: "Beh, forse un altro allarme in futuro sarà la spinta per agire". "Beh, ora che le persone sono state avvertite, forse le cose andranno bene, senza che io debba intervenire personalmente per aiutare"."

Non è detto che in futuro ci sarà un altro allarme chiaro. Non è detto che le cose andranno bene. Ma non è nemmeno tutto perduto, assolutamente. L'umanità ha la possibilità di *semplicemente non costruire* la superintelligenza, se agiamo in modo proattivo. Quello che succederà dopo dipende da noi.

### Come si potrebbe fermare *tutti* senza mettere spyware su ogni computer? {#come-si-potrebbe-fermare-tutti-senza-mettere-spyware-su-ogni-computer?}

#### **Agendo subito.** {#agendo-subito.}

Per addestrare le moderne IA, servono un sacco di chip super specializzati che lavorano insieme a distanza ravvicinata. Chiudere la ricerca sull'IA significherebbe chiudere alcuni enormi data center e smettere di creare chip IA specializzati di altissima qualità. Non stiamo parlando di laptop per il grande pubblico. La maggior parte delle persone non noterebbe nemmeno la differenza.

Nel 2025 non ci sono molte fabbriche segrete di chip di cui nessuno sa nulla. Nel 2025, i chip adatti all'intelligenza artificiale avanzata sono prodotti solo da pochi produttori, anche se attualmente ci sono aziende che stanno cercando di mettere in funzione altre fabbriche di questo tipo.

Sempre nel 2025, alcune tecnologie chiave per la produzione di chip di fascia alta sono vendute da un solo produttore al mondo: ASML, nei Paesi Bassi.

In altre parole, puoi interrompere la fornitura alla fonte. Ma questa situazione non è permanente: prima si firma un trattato internazionale, meglio è. Tutto questo è già più difficile, più costoso e più pericoloso di quanto sarebbe stato nel 2020 o anche nel 2023.

### Ma stai sostenendo il controllo del numero di chip per computer con IA avanzata che gli individui possono possedere. {#ma-stai-sostenendo-il-controllo-del-numero-di-chip-per-computer-con-ia-avanzata-che-gli-individui-possono-possedere.}

#### **Sì. Vogliamo anche che si smetta di fare ricerca.** {#sì.-vogliamo-anche-che-si-smetta-di-fare-ricerca.}

Non ci fa piacere dirlo. Se dal 2024 fosse illegale per le persone avere più di (diciamo) otto GPU H100, si perderebbe qualcosa.

Ma non si perderebbe *così tanto* che l'umanità dovrebbe cercare di capire esattamente quanto può essere grande un data center prima che diventi rischioso. Sbagliare e fissare un limite troppo basso significa che alcune persone non potrebbero portare avanti progetti interessanti. Sbagliare e fissare un limite troppo alto significa che tutti muoiono.

Inoltre, questo sistema in cui l'IA richiede un sacco di potenza di calcolo per essere costruita non durerà per sempre. Oggi ci sono gli LLM. Anche se fosse vietato crearne di nuovi e fosse vietato costruire un sacco di potenza di calcolo, in teoria si potrebbero studiare i loro meccanismi interni e capire qualcosa su come funziona l'intelligenza, cose che potrebbero aiutare a inventare algoritmi più efficienti in grado di aggirare i tentativi di monitoraggio.

### Perché vietare la ricerca? Sembra una misura estrema. {#perché-vietare-la-ricerca?-sembra-una-misura-estrema.}

#### **Altre scoperte potrebbero rendere praticamente impossibile impedire alle persone di creare la superintelligenza.** {#altre-scoperte-potrebbero-rendere-praticamente-impossibile-impedire-alle-persone-di-creare-la-superintelligenza.}

Nel libro abbiamo raccontato come un singolo articolo pubblicato nel 2017 abbia dato il via all'intera rivoluzione LLM descrivendo un algoritmo che ha reso pratico addestrare IA utili su hardware commerciale specializzato.

Se un giorno fosse possibile addestrare potenti IA su hardware *consumer* ampiamente disponibile, le misure per prevenire la superintelligenza dovrebbero diventare più onerose e fallirebbero più rapidamente.

Ecco perché anche la ricerca su algoritmi di IA ancora più potenti ed efficienti è un vero e proprio veleno per l'umanità.

È una brutta notizia, e non è quello che vorremmo fosse vero. Ma sembra proprio che sia così.

Nessuna legge può impedire agli attuali scienziati dell'IA di pensare a algoritmi più efficienti nella privacy delle loro menti. Forse qualcuno potrebbe creare una rete clandestina per condividere i risultati delle ricerche. Alcuni nel settore dell'IA già dicono con orgoglio che l'umanità dovrebbe sparire per far posto all'IA; potrebbero fare di tutto per andare avanti, non importa cosa ne pensino gli altri.

Ma la ricerca sull'intelligenza artificiale rallenterebbe *molto* se fosse illegale, e ancora di più se fosse ampiamente compreso che si tratta davvero di un tipo di ricerca che potrebbe ucciderci tutti. Rallenterebbe enormemente se reti clandestine di questo tipo fossero rintracciate e fermate con la stessa convinzione usata per fermare chi cerca di arricchire l'uranio nel proprio garage, perché i pericoli del mondo reale sono presi sul serio.

La maggior parte delle persone non cerca di fare cose super illegali che farebbero arrabbiare davvero le forze dell'ordine e le agenzie di intelligence internazionali. Rendere illegale la pubblicazione di nuovi algoritmi di IA intelligenti scoraggerebbe forse il 99,9% delle persone e quasi tutte le aziende, e poi il restante 0,1% potrebbe essere gestito dalla polizia e dalle agenzie di intelligence locali, nazionali e internazionali, e non otterrebbe neanche lontanamente l'attuale livello di finanziamento.

Sarebbe un mondo molto diverso da quello attuale, dove è del tutto legale fare gli esperimenti scientifici più pericolosi della storia e le grandi aziende investono miliardi di dollari in questo campo.

Non sappiamo quante altre scoperte ci vorranno prima che le IA siano abbastanza intelligenti da fare ricerca sull'IA e costruire IA ancora più intelligenti. Potrebbe bastare una sola scoperta. Potrebbero volercene cinque. Ma algoritmi migliori sono letali quanto hardware migliori. Sono come due cavalli che trainano lo stesso carro verso un precipizio.

### Si può davvero fermare una tecnologia? {#can-a-technology-really-be-stopped?}

#### **\* Molte tecnologie sono vietate o super regolamentate.** {#*-molte-tecnologie-sono-vietate-o-super-regolamentate.}

La fissione nucleare è il classico esempio di tecnologia regolamentata. Le aziende private non possono arricchire l'uranio senza il controllo del governo, anche se l'energia a basso costo sarebbe davvero utile.

In realtà, l'umanità è piuttosto brava a regolare e rallentare ogni tipo di tecnologia. Gli Stati Uniti regolano rigorosamente [i nuovi farmaci e dispositivi medici](https://www.fda.gov/), [l'edilizia abitativa](https://www.hud.gov/hud-partners/laws-regulations), [la produzione di energia nucleare](https://www.nrc.gov/about-nrc.html), [i programmi televisivi e radiofonici](https://www.fcc.gov/media/radio/public-and-broadcasting), [le pratiche contabili](https://www.fasb.org/standards), [assistenza all'infanzia](https://childcare.gov/consumer-education/regulated-child-care), [disinfestazione](https://npic.orst.edu/reg/laws.html), [agricoltura](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) e un sacco di altri settori. Ogni singolo stato richiede un esame di abilitazione per [parrucchieri](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) e [manicure](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). La maggior parte di essi richiede un esame anche per i [massaggiatori](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

Noi pensiamo che, in molti casi, l'umanità regoli troppo la tecnologia. Ad esempio, ci sembra che la Food and Drug Administration statunitense stia uccidendo molte più persone (rallentando o impedendo la creazione di farmaci salvavita) (https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), con requisiti pesanti) di quante ne sta salvando (impedendo il rilascio di farmaci pericolosi). Ci sembra che il prezzo delle case sia troppo alto, in parte a causa delle restrizioni legali sulla zonizzazione che limitano cosa si può costruire e dove. Ci sembra che gli Stati Uniti abbiano praticamente distrutto la propria industria nucleare con regole troppo rigide. E, sul serio, i parrucchieri?

L'umanità ha *assolutamente* la capacità di ostacolare il progresso tecnologico. Sarebbe davvero tragico e assurdo se usassimo questa capacità nella medicina, nell'edilizia e nell'energia, e trascurassimo di usarla su una delle rare tecnologie che, se realizzata, ci ucciderebbe tutti.

#### **Un divieto può essere mirato in modo specifico.** {#un-divieto-può-essere-mirato-in-modo-specifico.}

Un divieto sulla ricerca e sviluppo dell'intelligenza artificiale avanzata non deve per forza influenzare la gente comune. Non deve nemmeno togliere i chatbot moderni o chiudere l'industria delle auto a guida autonoma.

La maggior parte delle persone non compra decine di GPU AI top di gamma per metterle nel proprio garage. La maggior parte delle persone non gestisce enormi data center. La maggior parte delle persone non sentirà nemmeno gli effetti di un divieto sulla ricerca e lo sviluppo dell'IA. Semplicemente, ChatGPT non cambierebbe così spesso.

L'umanità non avrebbe nemmeno bisogno di smettere di usare tutti gli attuali strumenti di IA. ChatGPT non dovrebbe scomparire; potremmo continuare a capire come integrarlo nella nostra vita e nella nostra economia. Sarebbe comunque un cambiamento maggiore di quello che il mondo ha visto per generazioni. Ci perderemmo i *nuovi* sviluppi dell'IA (del tipo che arriverebbero quando l'IA diventa più intelligente ma non ancora abbastanza intelligente da uccidere tutti), ma la società non chiede a gran voce questi sviluppi.

E potremmo continuare a vivere. Potremmo vedere crescere i nostri figli.

Gli sviluppi che la gente *sta* chiedendo a gran voce, come lo sviluppo di nuove tecnologie mediche salvavita, sembrano possibili *senza* dover perseguire anche la superintelligenza. Siamo a favore di deroghe per l'IA medica, a condizione che funzioni con un adeguato controllo e si tenga alla larga da pericolose generalizzazioni.

I governi che lavorano per evitare la creazione di una superintelligenza fuori controllo dovrebbero assicurarsi che i chip di IA non vengano usati per sviluppare IA più potenti. Quindi, la questione di quali attività e servizi di IA potrebbero continuare dipenderebbe da quali [meccanismi di verifica](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) potrebbero essere usati per evitare che si sviluppi un'IA pericolosa. Meccanismi di verifica migliori potrebbero ridurre il costo di fermare lo sviluppo dell'IA, permettendo a un numero maggiore di attività di continuare.

Un altro passo che potrebbe aiutare un po' è mettere dei kill switch nei chip di IA e creare dei protocolli di monitoraggio e spegnimento di emergenza per tutti i grandi data center in uso. I reattori nucleari sono progettati in modo da poter essere spenti velocemente in caso di emergenza. Se pensi che la superintelligenza sia una minaccia che potrebbe farci sparire, allora è ovvio che i chip di IA e i data center dovrebbero essere progettati in modo che le autorità possano spegnerli facilmente e velocemente.

Il punto non è distruggere tutta la tecnologia perché la odiamo. Il punto è evitare di andare avanti su una strada che porta all'estinzione umana.

#### **Gran parte del problema è che la gente non capisce la minaccia incombente della superintelligenza.** {#gran-parte-del-problema-è-che-la-gente-non-capisce-la-minaccia-incombente-della-superintelligenza.}

Secondo la nostra esperienza, chi dice che l'umanità non può fermare la corsa alla superintelligenza semplicemente non capisce che, se qualcuno la costruisce, moriremo tutti.

"Ma l'IA offre grandi vantaggi!" - No, in realtà non è così; non puoi sfruttare il potere della superintelligenza se questa uccide tutti. Se l'umanità vuole cogliere i vantaggi offerti dalla superintelligenza, allora deve trovare un modo per gestire la transizione verso la superintelligenza che non uccida tutti come effetto collaterale.

"Ma le centrali nucleari fanno paura perché sono associate alle bombe atomiche che hanno raso al suolo intere città, mentre l'IA è associata a strumenti innocui come ChatGPT!" - È vero, almeno per ora. Se l'umanità non riuscisse mai a capire che la superintelligenza costruita utilizzando metodi anche solo vagamente simili a quelli moderni ucciderebbe semplicemente tutti, allora potrebbe non riuscire a fermarla. Ma l'ostacolo non è che l'umanità non riesca mai a controllare o ostacolare le tecnologie emergenti (come le armi nucleari o l'energia nucleare); l'ostacolo è che *le persone non capiscono la minaccia*.

Da qui nasce questo libro. Come diciamo nell'ultimo capitolo, l'umanità può fare molto quando abbastanza persone capiscono la natura del problema.

### Ma questo non significa dare troppo potere ai governi? {#non-significa-dare-troppo-potere-ai-governi?}

#### **I governi hanno già il potere di vietare le tecnologie pericolose.** {#i-governi-hanno-già-il-potere-di-vietare-le-tecnologie-pericolose.}

Vietare la ricerca sull'intelligenza artificiale che punta a creare un'IA più intelligente dell'uomo non cambierebbe molto in termini di potere dello Stato. I governi legiferano e regolano un sacco di cose. Limitare un singolo programma di ricerca potrebbe essere un problema per l'industria dell'IA, ma è una goccia nel mare per i governi e la società, che sono abituati al coinvolgimento dello Stato in molti aspetti della vita e che hanno già vietato tecnologie pericolose come le armi chimiche.

Vietare un'altra tecnologia non farà precipitare il mondo nel totalitarismo, così come i trattati sulle armi nucleari non hanno portato al totalitarismo.

Questo non vuol dire che vietare una tecnologia sia una cosa da poco. Non pensiamo che la soglia per l'intervento dello Stato debba essere bassa. Piuttosto, pensiamo che la superintelligenza superi facilmente qualsiasi soglia ragionevole.

Se l'umanità decidesse oggi di porre fine alla ricerca e allo sviluppo dell'IA, il divieto non dovrebbe essere particolarmente invasivo. Oggi, la creazione di un'IA all'avanguardia richiede un numero straordinario di chip per computer altamente specializzati che consumano enormi quantità di energia elettrica.

Forse tra dieci anni sarà possibile sviluppare un'intelligenza artificiale significativa su un laptop di consumo, *se* l'umanità permetterà ulteriori miglioramenti ai chip per computer e ulteriori ricerche sugli algoritmi di intelligenza artificiale. Ma l'umanità non ha bisogno di lasciare che ciò accada. I governi che limitano la ricerca e lo sviluppo dell'IA non devono essere più invasivi nella vita della persona media rispetto ai governi che controllano la diffusione della tecnologia delle armi nucleari, a patto che il mondo si renda conto della situazione in cui ci troviamo e metta fine alle cose *adesso*.

### Alcune nazioni non rifiuterebbero un divieto? {#alcune-nazioni-rifiuterebbero-un-divieto?}

#### **\* No, se capiscono la minaccia.** {#*-no-se-capiscono-la-minaccia.}

Stiamo parlando di una tecnologia che potrebbe uccidere tutti sul pianeta. Se un paese capisse davvero il problema e quanto sia difficile per qualsiasi gruppo sul pianeta far sì che l'IA segua le intenzioni di chi la controlla anche dopo essere diventata una superintelligenza, allora non ci sarebbe alcun motivo per loro di andare avanti così in fretta. Anche loro vorrebbero firmare un trattato e aiutare a farlo rispettare, per paura per la propria vita.

Persino nazioni come la Corea del Nord, che hanno violato il diritto internazionale per sviluppare le proprie armi nucleari, non hanno *usato* quelle armi contro i propri nemici, perché capiscono che non ci sono vincitori in un olocausto nucleare. Le nazioni e i loro leader a volte si impegnano in politiche rischiose o in guerre, ma non perseguono attivamente la propria distruzione.

Chi pensa che qualche nazione straniera possa ritirarsi dal trattato, secondo noi, immagina una nazione i cui leader semplicemente non capiscono la minaccia. Pensiamo che immaginino uno scenario in cui l'intelligenza artificiale ha il 95% di possibilità di conferire grande ricchezza e potere al suo creatore e il 5% di possibilità di uccidere tutti. In tal caso, certo, qualche Stato potrebbe essere abbastanza avventato da provarci. E forse qualche Stato *crederà* che le probabilità siano proprio quelle.

Pensiamo che questa situazione non sia quella che la teoria e le prove suggeriscono. Come abbiamo ampiamente discusso nel libro, la teoria e le prove suggeriscono tutte che questa tecnologia sarebbe semplicemente un suicidio globale. Nessuno è neanche lontanamente in grado di sfruttare la superintelligenza a proprio vantaggio. Se la maggior parte del mondo lo capisse, ci sarebbero molte meno ragioni per cui le nazioni canaglia violerebbero un trattato. Neanche loro vogliono morire.

E anche se qualche ipotetica nazione canaglia avesse un leader che davvero non capisce la minaccia rappresentata dall'ASI, se quella nazione fosse circondata da un'alleanza internazionale di potenze mondiali che invece comprendono la minaccia, le potenze interessate potrebbero intervenire e cambiare il panorama degli incentivi per la potenza canaglia.

Se (per esempio) i leader di Stati Uniti, Cina, Russia, Germania, Giappone e Regno Unito credessero davvero che la *loro stessa sopravvivenza* dipenda dal fatto che nessuno costruisca una superintelligenza, e fossero super chiari nel dire che considererebbero qualsiasi tentativo di costruire una superintelligenza come una minaccia alla loro vita e al loro sostentamento e che sarebbero pronti a reagire per difendersi, allora... beh, anche un leader mondiale che non è d'accordo probabilmente non vorrebbe sfidare quella coalizione.

Lo sviluppo dell'IA non è una corsa al dominio militare, è una corsa al suicidio. Pensiamo che se i leader mondiali lo capissero, se avessero l'aspettativa che loro stessi e i loro figli possano morire per questo, allora si attenerebbero sinceramente a un trattato e contribuirebbero sinceramente a farlo rispettare.

In realtà non è poi così difficile capire che creare macchine più intelligenti di tutta l'umanità messa insieme potrebbe mandare il mondo fuori strada. Non è poi così difficile capire quanto poco l'umanità capisca delle macchine intelligenti che stiamo costruendo, se ci si ferma un attimo a riflettere seriamente sulla questione. Pensiamo che la questione sia *se* i leader mondiali arriveranno a credere a questi fatti. Ma se *lo faranno*, non pensiamo che sia davvero irrealistico fermare questa corsa suicida.

#### **Un trattato richiederebbe un monitoraggio e un'applicazione reali.** {#un-trattato-richiederebbe-un-monitoraggio-e-un-applicazione-reali.}

Anche se la maggior parte delle nazioni capisse che se qualcuno lo costruisse, morirebbero tutti, alcune nazioni potrebbero non capirlo e potrebbero essere così spericolate da andare avanti comunque con la costruzione della superintelligenza.

Il monitoraggio è necessario. L'applicazione delle norme è necessaria. I trattati sulle armi nucleari, biologiche e chimiche forniscono alcuni precedenti sui modi per verificare il rispetto delle norme. Possiamo e dobbiamo impegnarci per rendere difficile e costoso eludere tali trattati.

Un divieto internazionale sull'IA di frontiera dovrà essere applicato in modo rigoroso. Se uno Stato nazionale è determinato ad andare avanti nonostante la pressione internazionale, potrebbe essere necessario l'uso della forza militare da parte dei paesi firmatari.

Non è l'ideale! Bisogna fare di tutto per chiarire che in queste situazioni si ricorrerebbe alla forza, così da evitare errori di valutazione quando la forza deve essere usata davvero. Ma se c'è una causa che può giustificare un'azione militare limitata, o anche una guerra, se un paese che non rispetta gli accordi decide di intensificare la situazione, salvare il genere umano dovrebbe essere una di queste.

#### **Questo metodo ha funzionato in passato.** {#questo-metodo-ha-funzionato-in-passato.}

Sono passati più di ottant'anni da quando è stata inventata la bomba atomica e l'umanità ha fatto un ottimo lavoro nel gestire la proliferazione nucleare. Non c'è stata nessuna guerra nucleare su larga scala, al contrario di quello che molti esperti avevano previsto dopo la Seconda guerra mondiale.

Nel giugno del 2025, il governo degli Stati Uniti ha persino fatto un attacco limitato all'Iran per cercare di fermare la sua capacità di creare armi nucleari. Questo tipo di trattato e di regime di applicazione ha dei precedenti nell'ordine mondiale.

Se potessimo guadagnare ottant'anni prima dello sviluppo dell'ASI, potrebbe essere sufficiente.

### Un sistema di monitoraggio può durare per sempre? {#can-a-monitoring-regime-last-forever?}

#### **No. Ci vorrà un'altra via d'uscita.** {#no.-ci-vorrà-un'altra-via-d'uscita.}

Probabilmente non si può fermare del tutto la ricerca sull'intelligenza artificiale. Con *abbastanza* tempo, i ricercatori potrebbero trovare metodi molto più efficaci per creare intelligenze artificiali.[^228] O forse, con abbastanza tempo, qualche persona senza scrupoli potrebbe riuscire a far saltare il divieto.

Il tempo probabilmente trascinerà l'umanità verso il futuro in un modo o nell'altro. E l'umanità si estinguerà, come la maggior parte delle specie prima di lei, oppure riuscirà in qualche modo a navigare la transizione verso un mondo in cui esistono cose più intelligenti.

Ma l'umanità non ha bisogno di guadagnare tempo all'infinito. L'intelligenza artificiale non è l'unica tecnologia in progresso. Anche la biotecnologia sta iniziando a maturare e, se l'umanità riuscirà a impedire lo sviluppo di macchine superintelligenti per diversi decenni, dovrà fare i conti con sconvolgimenti come l'ingegneria genetica, che porterà alla creazione di esseri umani significativamente più intelligenti.

La domanda è: quanto tempo possiamo guadagnare e cosa possiamo fare con quel tempo?

Il problema principale che l'umanità deve affrontare è quello di passare in modo sicuro dall'intelligenza umana alla superintelligenza. Il miglior piano che ci viene in mente e che *forse* potrebbe funzionare nella realtà è quello di guadagnare tempo affinché la biotecnologia aumenti notevolmente l'intelligenza umana, al punto che i futuri ricercatori umani diventino *così* intelligenti da non stimare mai (ad esempio) che un progetto ingegneristico finirà in tempo e nel rispetto del budget, a meno che non sia *effettivamente* così.

Così intelligenti da non credere mai a una teoria scientifica come l'aristotelismo o l'eliocentrismo, anche se la società che li circonda ne fosse completamente convinta. Così intelligenti da avere la possibilità di superare il [divario tra Prima e Dopo](#a-closer-look-at-before-and-after) al primo tentativo.

Ci sono altri possibili percorsi che potremmo immaginare, ma questo ha il vantaggio di affrontare il principale ostacolo ("la comunità scientifica esistente dipende troppo dai metodi di tentativi ed errori e dall'incrementalismo per gestire questo particolare problema"), utilizzando tecnologie che stanno già iniziando a essere disponibili oggi, senza rappresentare un serio rischio per il mondo.

#### **Un sistema di monitoraggio *non dovrebbe* durare per sempre.** {#un-sistema-di-monitoraggio-non-dovrebbe-durare-per-sempre.}

In teoria, l'umanità potrebbe stare in equilibrio sul filo del rasoio delle competenze, dove siamo adesso, per sempre. Secondo noi, questo richiederebbe un controllo super rigido dei pensieri e delle attività delle persone. Ma anche se non fosse così, lo considereremmo comunque una scelta poco saggia.

Personalmente, pensiamo che i discendenti dell'umanità meritino di diventare ciò che desiderano, esplorare le stelle e costruire lì una civiltà fiorente e meravigliosa. Sosteniamo il divieto dello sviluppo dell'IA di frontiera perché riteniamo che la superintelligenza sia abbastanza pericolosa da renderlo necessario, non perché odiamo l'IA, la tecnologia o il progresso scientifico.

La vera domanda è *come* arrivare a un futuro fantastico e come gestire il passaggio da qui a lì.

Vale la pena sottolinearlo, anche perché ci sono un sacco di persone che presentano l'IA come una falsa dicotomia: dicono (a torto) che la società deve o accettare i rischi dell'IA e andare avanti a tutta velocità, o rifiutare l'IA e lasciare che la nostra civiltà svanisca per sempre su un unico pianeta. Questo è [semplicemente sbagliato](#rushing-ahead-destroys-those-benefits.). Ci sono altre strade verso il futuro, strade che permettono un futuro altrettanto brillante, ma senza il rischio così alto di buttare tutto all'aria per niente. L'umanità dovrebbe trovare un'altra strada verso il futuro.

### Perché rendere gli esseri umani più intelligenti sarebbe utile? {#perché-rendere-gli-esseri-umani-più-intelligenti-sarebbe-utile?}

#### **\* Potrebbe aiutare a risolvere il problema dell'allineamento.** {#*-it-could-help-with-solving-the-alignment-problem.}

Il problema dell'allineamento dell'IA non ci sembra irrisolvibile. Ci sembra solo che gli esseri umani non siano neanche lontanamente vicini a risolverlo e che non abbiano ancora raggiunto un livello di intelligenza tale da poter *pensare* di avere una soluzione che sia davvero una soluzione.

I ricercatori di IA spesso riconoscono che il problema dell'allineamento sembra incredibilmente difficile e che finora sono stati fatti pochissimi progressi. Ecco perché l'idea "forse possiamo chiedere alle IA di fare il lavoro di allineamento al posto nostro" è così allettante: quando sei un ricercatore di IA e senti che tu e i tuoi colleghi non siete all'altezza di risolvere un certo problema, la cosa più ovvia da fare è rivolgersi all'IA.

Ma, come diciamo nel capitolo 11 e nella [risorsa online](#more-on-making-ais-solve-the-problem) che lo accompagna, è chiaro anche a chi non è esperto che questa idea ha un sacco di problemi: perché un'IA capisca come risolvere un problema complesso che mette in difficoltà anche i migliori ricercatori umani, deve essere così intelligente da diventare pericolosa. E dato che *noi* abbiamo ben poca idea di cosa stiamo facendo, non abbiamo alcuna fonte di verità fondamentale che possiamo utilizzare per addestrare direttamente capacità di allineamento limitate, né alcun modo per verificare se una proposta di allineamento generata dall'IA sia sicura o efficace.

Il mondo può presentarci problemi che sono legittimamente fuori dalla nostra portata. La natura non è un gioco che offre all'umanità solo sfide "eque"; a volte possiamo imbatterci in problemi troppo difficili da risolvere anche per i migliori scienziati umani, o troppo difficili da risolvere nei tempi richiesti.

C'è un modo più realistico per passare l'intero problema a qualcuno più intelligente? Un'opzione potrebbe essere quella di rendere gli *esseri umani* più intelligenti in modo che possano davvero risolvere il problema dell'allineamento. Gli esseri umani sono "pre-allineati" in un modo che le IA non sono; le persone più intelligenti hanno le stesse motivazioni prosociali di base che abbiamo tutti.

In linea di principio, sembra possibile che le persone siano in grado di distinguere tra ciò che sembra una grande illuminazione alchemica che permetterà loro di trasformare il piombo in oro e il tipo di conoscenza che corrisponde effettivamente alla capacità di trasformare il piombo in oro (usando la fisica nucleare per eliminare alcuni neutroni dagli atomi di piombo). Dovrebbero sicuramente percepirli come stati di conoscenza diversi.

Ma gli ingegneri umani hanno un sacco di difficoltà a capire in quale zona si trovano. Nella storia della chimica, il livello di abilità dell'umanità era tale che gli alchimisti venivano ingannati in modo affidabile.

Nel mondo reale, gli scienziati si affezionano alle loro teorie preferite e non vogliono cambiare idea finché la realtà non gli sbatte in faccia che "la tua teoria era sbagliata" — e a volte non cambiano idea nemmeno allora: A volte si dice che la scienza progredisca "[un funerale alla volta](https://en.wikipedia.org/wiki/Planck%27s_principle)", perché la vecchia guardia non cambierà mai le proprie opinioni e bisogna solo aspettare che la nuova guardia maturi. Ma questo non è un vincolo fondamentale imposto dalla natura; è solo un problema di insufficienza di competenza, attenzione e consapevolezza di sé da parte degli esseri umani in quanto classe.

Di solito, va bene che gli esseri umani siano ingenui in questo senso, perché di solito la realtà è abbastanza indulgente con gli errori, almeno nel senso che non spazza via *tutta l'umanità* per l'arroganza di un alchimista. Ma [questo non è un lusso che l'umanità può permettersi](#a-closer-look-at-before-and-after) quando si tratta di allineamento della superintelligenza.

L'umanità spesso acquisisce le proprie conoscenze lottando, provando, fallendo e accumulando lentamente conoscenze. Ma non deve per forza essere così.

Einstein non solo è riuscito a capire la relatività generale, ma ci è riuscito riflettendo intensamente sul problema, anche prima che l'umanità mettesse in orbita i satelliti e iniziasse a vedere con i propri occhi le discrepanze nei loro orologi (come discusso nel capitolo 6). Aveva prova empirica, ma è riuscito a individuare in modo efficiente la risposta giusta in risposta ai primi sussurri sommessi provenienti dai dati empirici, piuttosto che aspettare che la verità bussasse alla sua porta.

Questo percorso è più raro e difficile da seguire, ma questo tipo di genio scientifico esiste, anche se raramente, persino tra i migliori e i più brillanti del mondo.

Gli esseri umani potenziati di uno o due livelli rispetto a ricercatori come Einstein o [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) potrebbero iniziare a capire bene i propri difetti e correggerli in tanti modi diversi.

Potrebbero notare quando stanno razionalizzando o cadendo vittime del [pregiudizio di conferma](https://en.wikipedia.org/wiki/Confirmation_bias). Potrebbero superare il punto in cui hanno l'aspettativa che un'idea apparentemente brillante funzioni quando in realtà non funziona, al punto che ogni volta che sono in aspettativa di avere successo, *hanno* successo. Potrebbero raggiungere un livello di competenza in cui continuano a commettere molti errori, ma non sono [sistematicamente](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) troppo sicuri (o insicuri) in nuovi ambiti complessi.

Il potenziamento dell'intelligenza umana è davvero possibile? A noi sembra di sì, dopo aver parlato con diversi ricercatori biotecnologici che pensano che ci siano approcci promettenti a breve termine. Anche un'intelligenza artificiale mirata e incentrata sulle biotecnologie potrebbe aiutare ad accelerare il lavoro. Ma dal nostro punto di vista, rimane molto incerto se un piano come questo possa realisticamente funzionare. Quello che possiamo dire con maggiore sicurezza è che si tratta di un'opzione con un effetto leva elevato che merita molti più investimenti e approfondimenti di quelli che riceve attualmente.

Non stiamo dicendo che potenziare l'intelligenza umana sia l'unica strategia post-AI su cui l'umanità dovrebbe investire pesantemente. È solo uno dei tanti esempi, e quello che al momento ci sembra più promettente. Consigliamo vivamente all'umanità di esplorare diverse strade possibili che non passino per l'AI, invece di puntare tutto su un'unica carta.

#### **Gli esseri umani potenziati non rappresentano un grave problema di "allineamento umano".** {#gli-esseri-umani-potenziati-non-rappresentano-un-grave-problema-di-"allineamento-umano".}

Gli esseri umani potenziati avrebbero essenzialmente la stessa struttura cerebrale, le stesse emozioni, ecc. del resto di noi. Con l'IA, anche quella addestrata a [*suonare come*](https://docs.google.com/document/d/1FbehxsMqTaw2seAzUJw4ny0tvZgUdbHJnCUi_Pbd5VU/edit?tab=t.k1kf1fy9gx5i#heading=h.8cxst4c4x63a) a noi, c'è un enorme divario in termini di differenze cognitive e motivazionali, e un divario altrettanto grande in termini di comprensibilità; con esseri umani leggermente più intelligenti, nulla di tutto ciò sembra particolarmente probabile.

I ricercatori con potenziamento cognitivo non avrebbero bisogno di mantenere la propria integrità mentale mentre si trasformano in vaste superintelligenze con menti milioni di volte più grandi. Dovrebbero solo essere portati al livello necessario per capire come *costruire* — non far crescere — superintelligenze artificiali che siano davvero allineate e stabili.

Potrebbe comunque esserci un problema di "allineamento umano" in senso lato, nel senso che qualsiasi sforzo di coordinare più persone può incontrare problemi di problema principale-agente e di incentivi. E questi problemi sono intrinsecamente molto più importanti per qualsiasi gruppo incaricato di creare una superintelligenza.

Pensiamo che questi problemi siano trattabili se gli esseri umani iniziano in modo visibilmente altruistico e generoso, se la loro intelligenza viene potenziata solo un po' alla volta e se lavorano in un'istituzione ben organizzata con incentivi ben pensati. Ma è del tutto normale che la gente si preoccupi della possibilità che qualcuno cerchi di prendere il controllo. Risolvere questi problemi non sarebbe necessariamente facile, ma non sarebbe così irrealizzabile come le aziende che cercano di sviluppare superintelligenze imperscrutabili con menti del tutto incomprensibili e pulsioni disumane.

Creare un team di supergeni geneticamente modificati per aiutare il pianeta a navigare in sicurezza attraverso la transizione verso la superintelligenza è sicuramente il tipo di cosa che l'umanità dovrebbe fare con attenzione, vista la posta in gioco di un'impresa del genere. Una mossa del genere comporta varie questioni pratiche ed etiche, ma queste devono essere valutate rispetto al costo di lasciare che la superintelligenza ci uccida tutti, se non ci sono altre soluzioni altrettanto promettenti.

I tempi difficili possono richiedere misure drastiche, ma il (modesto) potenziamento dell'intelligenza umana non è nemmeno una misura che sembra particolarmente drastica. Sembra una tecnologia tutto sommato positiva di per sé, che ha almeno qualche possibilità di aiutare l'umanità in più di un modo.
