#### **Possiamo collaborare per fermare la superintelligenza pur essendo in disaccordo sul potenziamento umano.** {#possiamo-collaborare-per-fermare-la-superintelligenza-pur-essendo-in-disaccordo-sul-potenziamento-umano.}

Se non siete d'accordo con noi sull'idea del potenziamento umano, possiamo comunque trovare un accordo per fermare lo sviluppo dell'IA di frontiera.

Se non risolviamo questo problema, moriremo tutti. Tutti quelli che non vogliono morire *oggi* devono collaborare a tal fine. Possiamo aspettare che la minaccia di morte immediata sia passata per discutere se potenziare l'intelligenza umana debba essere illegale o sovvenzionato.

"Rendere gli esseri umani più intelligenti di Einstein" non è un piano per non morire nel 2028 o nel 2032 o quando ci sarà la prossima svolta fondamentale negli algoritmi di IA.

Non è un piano che può andare di pari passo con lo sviluppo dell'IA. Anche se qualcuno usasse la tecnologia medica resa possibile da IA più stupide degli esseri umani per aumentare l'intelligenza umana ben oltre il livello di Einstein, tali potenziamenti probabilmente non sarebbero in grado di risolvere il problema dell'allineamento dell'IA e di progettare, creare e costruire in modo sicuro e *rapido* la superintelligenza artificiale, sotto la pressione temporale di una corsa agli armamenti. La corsa verso la superintelligenza deve comunque fermarsi.

L'idea alla base dell'aumento dell'intelligenza umana è che potrebbe *addirittura rendere possibile* risolvere il problema dell'allineamento, se un gran numero di ricercatori potenziati avesse anche un numero significativo di anni o decenni per lavorare sul problema. L'idea non è che potrebbero vincere una *corsa* per costruire una superintelligenza allineata tra sei anni, più velocemente di quanto il resto dell'industria dell'IA possa costruire e implementare una superintelligenza non allineata.

Molte persone che pensano che "potenziare l'intelligenza umana" sia un piano relativamente promettente, noi compresi, ritengono che i primi passi comprendano comunque la chiusura delle aziende di IA.

Anche coloro che hanno altri piani su cosa dovrebbe fare l'umanità in seguito, in genere concordano sul fatto che il primo passo dovrebbe essere la chiusura delle aziende di IA.

Meta AI non può esistere, OpenAI non può esistere, Anthropic non può esistere; ci uccideranno. Possiamo essere d'accordo su questa priorità immediata, anche se abbiamo idee molto diverse su cosa fare dopo.

### "Allineata a chi?" {#"allineata-a-chi?"}

#### **Questa è una domanda spinosa. A prescindere dalla risposta, dobbiamo fermare lo sviluppo.** {#questa-è-una-domanda-spinosa.-a-prescindere-dalla-risposta,-dobbiamo-fermare-lo-sviluppo.}

Se un giorno l'umanità creerà una superintelligenza, dovremo assicurarci che sia “allineata” con i valori umani. Ma con i valori di quali esseri umani, esattamente? Le persone hanno opinioni molto diverse su ciò che è giusto e sbagliato, sulla religione, sulle norme sociali, sui compromessi politici, ecc.

Al momento, la questione è puramente teorica. L'umanità non è in grado di inserire *alcun* obiettivo particolare in un'IA, quindi non importa se c'è disaccordo su *quali* obiettivi sarebbero ideali. Come abbiamo discusso a lungo, affrettarsi a costruire una superintelligenza porterebbe alla morte di tutti noi. L'umanità è in disaccordo su molte cose, ma la maggior parte delle persone concorda sul fatto che la distruzione di tutta la vita sulla Terra non è una cosa buona.

Il problema di quali valori *esattamente* dovrebbero essere caricati in un'IA sembra un problema spinoso. È un problema che, francamente, ci piacerebbe avere. Ci troviamo invece di fronte a un problema diverso e molto peggiore.

Non è affatto necessario essere d'accordo su “allineata a chi?” (o anche se l'umanità dovrebbe mai creare una superintelligenza) per coordinarci su un divieto internazionale, per la semplice e brutale ragione che altrimenti moriremo. L'IA solleva un'infinità di interessanti questioni filosofiche, ma se ci lasciamo distrarre eccessivamente da queste, rischiamo di far morire i nostri figli.

In pratica, il nostro consiglio ai leader mondiali è:

* Separare la domanda "Dovremmo affrettarci a costruire la superintelligenza?" dalla domanda "Se per caso avessimo un modo per costruire la superintelligenza in modo sicuro, cosa dovremmo farne?" e concentrarsi prima sulla prima domanda. La prima domanda è quella urgente e quella su cui è possibile agire oggi. La seconda domanda potrebbe essere importante da affrontare un giorno, ma al momento è una trappola, perché incoraggia a pensare alla superintelligenza come a un premio. Credere erroneamente che il primo a costruire una superintelligenza possa decidere cosa farne ci porterebbe a una corsa suicida.

  La superintelligenza artificiale è un pulsante suicida, non un genio nella lampada. Quando qualcuno crea una superintelligenza, non "possiede" quella superintelligenza. Piuttosto, la superintelligenza che ha appena creato possiede un pianeta.  
* Se per qualche motivo in futuro sentiste il bisogno di affrontare l'argomento "Come dovrebbe l'umanità usare la superintelligenza, se mai fossimo in grado di farlo?", vi consigliamo vivamente di evitare proposte o idee che potrebbero spingere altri attori a entrare in gara (o che potrebbero incoraggiare le nazioni a rifiutare o violare eventuali futuri accordi internazionali sulla superintelligenza). Qualsiasi dinamica del tipo "il vincitore prende tutto" ha un enorme potenziale di mettere in pericolo il mondo.

  Esistono proposte per gestire la difficile questione dell'"allineamento a chi" in modo relativamente universalistico, che cercano di essere eque con tutti i potenziali stakeholder e che non incentivano la corsa al traguardo — ad esempio, la proposta di allineare un'IA per perseguire la [volontà estrapolata coerente](https://www.lesswrong.com/w/coherent-extrapolated-volition-allineamento-target) di tutta l'umanità.[^229] Ma anche in questo caso, esiste un potenziale infinito di dibattito sui principi e sui compromessi coinvolti, così come sui dettagli spinosi dell'implementazione. Sarebbe importante risolvere queste discussioni in un mondo in cui l'umanità avesse capito come indirizzare una superintelligenza in modo preciso e robusto, ma metterle oggi al centro dell'attenzione travisa gravemente i compromessi reali che il mondo deve affrontare e rischia di far deragliare gli sforzi per coordinarsi su obiettivi condivisi come evitare la distruzione della Terra.

Anche quando si tratta di questioni di enorme importanza a lungo termine, [nulla dovrebbe essere associato alla sopravvivenza dell'umanità, se non la sopravvivenza dell'umanità stessa](#mantenere-ampia-la-coalizione).

### Non è più furbo evitare di parlare di estinzione? {#non-è-più-furbo-evitare-di-parlare-di-estinzione?}

#### **Il tempo dei giochi politici è finito.** {#il-tempo-dei-giochi-politici-è-finito.}

Alcuni sostengono che chi è preoccupato per la corsa alla creazione di una superintelligenza dovrebbe tenere per sé le proprie opinioni e parlare invece della perdita di posti di lavoro causata dall'IA, o del problema dei bioterroristi che usano ChatGPT, o di quanta acqua serve per raffreddare i computer nei data center.[^230] Riteniamo che questo approccio sia un eccesso di furbizia e che rischi di essere controproducente. In effetti, abbiamo già visto che è stato controproducente in diverse occasioni.

I quattro problemi principali che vediamo in questo approccio sono:

* **Non è onesto**, e le persone sono brave a riconoscere la disonestà e i doppi giochi.

  Anche se siete dei bugiardi fuori dal comune, le argomentazioni su questioni che ritenete secondarie rischiano di apparire "stonate" in vari modi. Non sembreranno del tutto sensate, per le stesse ragioni per cui le ritenete, appunto, secondarie. Più edulcorate le vostre argomentazioni quando le condividete, più è probabile che le persone concludano che siete *o* confusi sulla questione *o* non del tutto onesti su ciò che pensate realmente. E in entrambi i casi, non apparirete come buoni alleati o fonti di informazioni affidabile.  
* **Probabilmente non è necessario.** Secondo la nostra esperienza, una conversazione onesta e diretta sulla superintelligenza viene accolta molto meglio dei tentativi di deviare l'attenzione su altre questioni, come i deepfake dell'IA. Dalla metà del 2023 e con frequenza crescente, io (Soares) ho parlato con vari funzionari eletti. Ho partecipato a cene in cui persone "preoccupate per l'IA" hanno sollevato la possibilità di terroristi assistiti dall'IA, e un funzionario eletto in carica ha risposto che le sue paure sono molto più urgenti e gravi, perché teme che le IA capaci di auto-migliorarsi ricorsivamente possano produrre una superintelligenza in grado di spazzarci via completamente dalla faccia della Terra, e che potrebbe essere creata entro tre anni.

  Persino i funzionari eletti del Congresso degli Stati Uniti sono disposti a prendere sul serio la questione e a cercare modi per affrontarla.[^231] La questione può sembrare più di nicchia e controversa di quanto non sia in realtà, perché al momento della pubblicazione di questo libro non c'è ancora stato un vero e proprio dibattito pubblico nazionale o internazionale al riguardo. Ma abbiamo avuto molte conversazioni franche a Washington su questo argomento che sono andate incoraggiantemente bene.[^232]  
* **Rispondere a queste altre questioni non risolve il problema della superintelligenza.** Le aziende di IA stanno correndo per creare la superintelligenza. Se ci riescono, moriremo tutti. Le soluzioni che hanno senso per questo problema sono molto diverse da quelle che hanno senso per affrontare i deepfake generati dall'IA, o persino il bioterrorismo reso possibile dall'IA.

  La sovrapposizione non è *pari a zero*, e potremmo raccogliere più sostegno per affrontare l'IA più intelligente dell'uomo sottolineando le sovrapposizioni tra le diverse questioni. Ma è estremamente improbabile che il mondo riesca a trovare una risposta adeguata a una questione così complicata come la superintelligenza *senza concentrarsi sul problema reale*.  
* **È plausibile che il tempo a disposizione sia poco.** È improbabile che abbiamo il tempo di introdurre gradualmente le persone a questo rischio nel corso di molti anni, partendo da questioni più semplici e familiari per poi procedere gradualmente fino alla superintelligenza. Se non ci mobilitiamo rapidamente per rispondere a questo problema, è plausibile che non avremo alcuna possibilità di rispondere.

Questo non vuol dire che la perdita di posti di lavoro, il bioterrorismo, ecc. non siano problemi reali a sé stanti. È solo che la società non fermerà *davvero* la sconsiderata corsa al suicidio se non *sa* che ne è in corso una.

Abbiamo passato anni a osservare amici e conoscenti nell'ambito delle politiche pubbliche sondare il terreno su problemi come i bioterroristi potenziati da ChatGPT. Per quanto ne sappiamo, non sembra che tutto ciò sia approdato a qualcosa in grado di impedire davvero la creazione della superintelligenza.

Siamo nerd fino al midollo, ben lontani dalla nostra zona di comfort quando si tratta di scrivere un libro divulgativo. Non pretendiamo di essere esperti di politiche efficaci. Ma ci sembra che l'umanità abbia raggiunto il limite dei problemi che può superare con un discorso fatto di argomenti prudenti, scelti strategicamente e non «allarmistici».

A un certo punto, come esseri umani, dobbiamo iniziare a parlare della minaccia incombente. La politica deve fondarsi sulla realtà dei fatti, non su una comunicazione all'apparenza rassicurante.

I direttori dei laboratori di IA sostengono che potremmo vedere IA capaci di fare ricerca superare gli esseri umani nel giro di [uno](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year) - [quatttro](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s) [anni](https://ia.samaltman.com/). Speriamo con tutto il cuore che si sbaglino ma, con tutta la nostra esperienza, non possiamo *sapere* che hanno torto. Nemmeno i responsabili politici possono saperlo. L'umanità, semplicemente, non sta reagendo in modo adeguato alla sfida che ha di fronte. Se non si lancia l'allarme ora, quando?

Inoltre, da quando abbiamo steso la prima bozza di questo paragrafo, la strategia che sosteniamo sembra dare sempre più i suoi frutti, come si può vedere dalla [lista](#i-rappresentanti-eletti-riconosceranno-questa-come-una-vera-minaccia?) delle dichiarazioni dei politici sulla superintelligenza nel corso dell'estate del 2025\\. I tempi sembrano maturi per una discussione seria sul pericolo imminente che la superintelligenza artificiale rappresenta.

### I rappresentanti eletti riconosceranno questa come una vera minaccia? {#i-rappresentanti-eletti-riconosceranno-questa-come-una-vera-minaccia?}

#### **Un numero crescente lo ha già fatto.** {#un-numero-crescente-lo-ha-già-fatto.}

Pensiamo che l'ostacolo principale che impedisce alla gente di riconoscere questa minaccia stia nel fargliela comprendere. Nei pochi mesi da quando il libro è stato mandato in stampa, ci sembra che il mondo stia già facendo passi avanti in questa direzione.

Ecco alcune dichiarazioni di politici statunitensi di entrambi gli schieramenti politici, nell'estate del 2025:

> La superintelligenza artificiale è una delle più grandi minacce esistenziali che ci troviamo ad affrontare. [...] Dovremmo anche preoccuparci che Stati autoritari come la Cina o la Russia possano perdere il controllo sui loro sistemi avanzati? [...] Ed è possibile che la perdita di controllo da parte di qualsiasi Stato-nazione, compresa la nostra, possa dare origine a un attore di IAG o SIA indipendente con cui dovremo confrontarci a livello globale? \- [Jill Tokuda (D-HI)](https://peterwildeford.substack.com/p/congress-has-started-taking-agi-more), in un'[audizione del 25 giugno 2025](https://www.congress.gov/event/119th-congress/house-event/118428)
>
> Non voterò a favore dello sviluppo di Skynet e dell'ascesa delle macchine, distruggendo così il federalismo per 10 anni, togliendo agli Stati il diritto di regolamentare e legiferare su tutta l'intelligenza artificiale. \- [Marjorie Taylor-Green (R-GA)](https://x.com/RepMTG/status/1930650431253827806)
>
> Ci sono persone molto, molto competenti — e proprio oggi ho parlato con una di loro — che temono fortemente che gli esseri umani non saranno in grado di controllare la tecnologia e che l'intelligenza artificiale finirà per dominare la nostra società. Non saremo in grado di controllarla. Potrebbe essere lei a controllare noi. È una sorta di scenario apocalittico, e c'è una certa preoccupazione al riguardo tra le persone più competenti del settore. \- [Bernie Sanders (I-VT)](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611?utm_source=substack&amp;utm_medium=email)
>
> "Nel nostro affannarci per vincere la corsa all'IA contro la Cina, rischiamo di perdere noi stessi..." \- [Chris Murphy (D-CT)](https://www.chrismurphyct.com/p/in-our-scramble-to-win-the-ai-race)
>
> "Ciò rende più probabile che presto la sfida di leadership numero uno per i leader mondiali, compreso il Presidente degli Stati Uniti, sarà gestire i cambiamenti che l'IA sta determinando, e usare la visibilità della propria carica e gli strumenti della politica per garantire che questa tecnologia migliori la condizione delle persone, anziché peggiorarla." \- [Pete Buttigieg](https://petebuttigieg.substack.com/p/we-are-still-underreacting-on-ai), ex Segretario dei Trasporti

C'è ancora tanta strada da fare, ma il mondo sta iniziando a rendersene conto. I tempi sono maturi per allertare i funzionari sulla necessità di agire rapidamente a livello federale e internazionale.

### La situazione è disperata? {#la-situazione-è-disperata?}

#### **No** {#no.-4}

Questa è una battaglia che possiamo vincere, e vale la pena lottare per questo nostro mondo. Non sembra facile, ma è davvero fattibile.

Se volete unirvi a noi per chiamare il mondo all'azione, saremo onorati di combattere al vostro fianco. Consultate l'ultimo capitolo del libro per scoprire alcuni modi in cui potete contribuire.

## Discussione approfondita {#discussione-approfondita-11}

### Cosa servirebbe per arrestare lo sviluppo globale dell'IA? {#cosa-servirebbe-per-arrestare-lo-sviluppo-globale-dell-ia?}

Non siamo esperti di diritto internazionale e questo è un argomento straordinariamente complicato, che a nostro avviso richiederà un notevole impegno da parte degli esperti del settore. Tuttavia, nell'interesse di avviare rapidamente il dibattito, abbiamo collaborato con il nostro [team di governance tecnica](https://techgov.intelligence.org/) e con consulenti esterni per raccogliere alcuni spunti e ipotesi su misure che potrebbero rivelarsi efficaci.

Le offriamo allo scopo di incoraggiare il dialogo, il dibattito, la critica e le iterazioni successive. Queste bozze iniziali non devono in alcun modo essere considerate come definitive o autorevoli.

Come primo passo, esaminiamo i vincoli e i contorni del problema che stiamo cercando di risolvere: un argomento che potrebbe facilmente riempire un libro intero. Il problema centrale, da decenni, è quello di impedire lo sviluppo della superintelligenza artificiale. E poiché non sappiamo dove si trovino le soglie critiche, ciò equivale essenzialmente a interrompere del tutto la ricerca e lo sviluppo nel campo dell'IA.

I progressi attuali nel campo dell'IA derivano da una combinazione di fattori: la creazione di chip più performanti, l'impiego di un maggior numero di chip per sessioni di addestramento più lunghe, e il miglioramento degli algoritmi. Affronteremo a uno a uno ciascuno di questi aspetti, spiegando quali siano le leve corrispondenti per arrestare il progresso verso la superintelligenza artificiale.

#### **Impedire la creazione di chip per l'IA più numerosi e potenti** {#impedire-la-creazione-di-chip-per-l-ia-più-numerosi-e-potenti}

Aumentare la potenza delle moderne IA richiede un enorme investimento in potenza di calcolo ed energia elettrica. Di conseguenza, sembra possibile, per i moderni attori statali, identificare e monitorare tutte le strutture pertinenti e prevenire l'emergere di nuove strutture simili, con un impatto minimo sull'hardware di consumo.

La [catena di approvvigionamento](https://www.csis.org/analysis/mapping-semiconductor-supply-chain-critical-role-indo-pacific-region) per la produzione di chip per l'IA avanzati è estremamente concentrata. Per alcune fasi della catena di approvvigionamento, esiste una sola azienda al mondo in grado di svolgere quel ruolo, e queste aziende si trovano in gran parte in paesi tradizionalmente alleati degli Stati Uniti.

Per esempio, solo poche aziende possono fabbricare chip per l'IA (principalmente l'azienda taiwanese TSMC) e uno dei macchinari fondamentali utilizzati nei chip di fascia alta è prodotto solo dall'azienda olandese ASML. Si tratta della macchina per litografia a ultravioletti estremi, grande come uno scuolabus, che pesa 200 tonnellate e costa [centinaia di milioni di euro](https://www.datacenterdynamics.com/en/news/tsmc-to-receive-first-high-na-euv-lithography-machine-from-asml-in-q4/).

Questa catena di approvvigionamento è il risultato di decenni di innovazione e investimenti, e [si prevede](https://cset.georgetown.edu/publication/maintaining-the-ai-chip-competitive-advantage-of-the-united-states-and-its-allies/) che replicarla sarà piuttosto difficile — un'impresa che richiederà probabilmente più di un decennio, anche per i paesi tecnologicamente avanzati.

I chip per l'IA più avanzati sono anche [molto specializzati](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/), quindi il loro tracciamento e monitoraggio avrebbe ricadute limitate. Il chip H100 di NVIDIA, uno dei chip per l'IA più comuni a metà del 2025, costa circa 30.000 dollari per unità ed è progettato per essere utilizzato in un data center, a causa dei suoi requisiti di raffreddamento e alimentazione. Questi chip sono ottimizzati per eseguire le operazioni numeriche coinvolte nell'addestramento e nell'esecuzione delle IA, e sono in genere [da decine a migliaia di volte](https://cset.ge Georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/) più performanti nei carichi di lavoro di IA rispetto ai computer standard (CPU di consumo).[^233]

La concentrazione e la complessità della catena di approvvigionamento dei chip per l'IA rendono più facile di quanto ci si potrebbe aspettare fermare lo sviluppo dell'intelligenza artificiale avanzata. **Interrompere la produzione di nuovi chip per l'IA sarebbe semplice**. Basterebbe un monitoraggio relativamente limitato di un piccolo numero di fornitori chiave per garantire che non vengano create catene di approvvigionamento segrete, data la complessità e l'interconnessione del processo di produzione.

Per produrre chip per l'IA e altri chip avanzati per computer (come quelli per i cellulari) si usano alcune delle stesse infrastrutture, ma ci sono differenze notevoli tra questi chip. Se la produzione di chip avanzati per l'IA venisse interrotta, sarebbe possibile controllare e assicurarsi che qualsiasi produzione di chip in corso crei solo chip non specializzati per l'IA.

Si potrebbe controllare che i chip specializzati per l'IA già esistenti vengano conservati e usati per far funzionare le IA esistenti, come ChatGPT. Garantire che tali chip vengano usati solo per far funzionare IA a bassa potenza (anziché per nuove attività di ricerca e sviluppo) sarebbe una sfida, ma non insormontabile. Le posizioni dei chip esistenti potrebbero essere tracciate e monitorate, e ci sono vari potenziali [meccanismi che potrebbero essere utilizzati per verificare](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) per cosa vengono usati tali chip. Questo tipo di monitoraggio richiede l'accesso fisico ai chip (ad esempio, ispettori che effettuano misurazioni in un data center). L'accesso remoto potrebbe essere sufficiente per verificare se i nuovi chip siano fabbricati con [una sicurezza migliorata](https://www.cnas.org/publications/reports/secure-governable-chips) e progettati tenendo conto della verifica e del monitoraggio. Come vedremo nella sezione seguente, le concentrazioni di chip necessarie per essere pericolose (al livello degli algoritmi di IA dell'agosto 2025) sono così grandi che non sarebbe difficile per gli attori statali individuare tutte queste strutture e sottoporle a ispezioni regolari.

#### **Impedire l'uso di chip per l'IA più numerosi e migliori** {#impedire-l-uso-di-chip-per-l-ia-più-numerosi-e-migliori}

Spostando ora l'attenzione dalla produzione dei chip al loro utilizzo: i più grandi [attuali data center per l'IA](https://epoch.ai/blog/trends-in-ai-supercomputers) ospitano centinaia di migliaia di chip per l'IA, che costano miliardi di euro. Per addestrare una delle IA più potenti oggi disponibili, questi chip devono essere utilizzati per mesi e mesi.

Ciascuno di questi chip ha un [consumo energetico simile](https://ifp.org/future-of-ai-compute/) a quello di un'abitazione media americana, pertanto un data center con centinaia di migliaia di chip ha un consumo energetico paragonabile a quello di una piccola città. Per alimentare tutti questi chip sono necessarie infrastrutture elettriche specializzate, come grandi linee di trasmissione. Questi data center sono anche edifici piuttosto grandi con impronte termiche distintive, dovute al funzionamento e al raffreddamento continui di un gran numero di chip ad alto consumo energetico.

All'interno, questi data center ospitano migliaia di chip montati in armadi appositi e dispongono di vaste infrastrutture di raffreddamento per evitare il surriscaldamento dei chip. Se si [entrasse](https://cloud.google.com/blog/products/gcp/google-data-center-360-tour) in uno di questi edifici, sarebbe subito chiaro che si tratta di un data center. Il loro scopo non potrebbe di certo essere celato a eventuali ispettori internazionali, soprattutto qualora questi ultimi controllassero i chip presenti nel data center e scoprissero che si tratta di chip specializzati per l'IA.

**I grandi data center e le relative infrastrutture energetiche sono talmente imponenti da poter essere individuati dai satelliti in orbita.** Ciò significa che se i governi volessero localizzare i grandi data center attuali, molto probabilmente ci riuscirebbero con un elevato tasso di successo, indipendentemente dal fatto che si trovino all'interno dei propri confini o in altri paesi. Sebbene le conoscenze pubbliche in materia siano [limitate](https://epoch.ai/blog/trends-in-ai-supercomputers), questo singolo intervento potrebbe bastare a rintracciare la maggior parte dei chip per l'IA di fascia alta.

In futuro, gli Stati potrebbero tentare di nascondere i loro data center per renderne più difficile l'individuazione satellitare. Potrebbero, ad esempio, provare a occultare un data center all'interno di una montagna (come nel Cheyenne Mountain Complex, che ospita il NORAD), dove non sarebbe visibile dall'alto. Nonostante questo, sarebbe difficile nascondere le infrastrutture necessarie al suo funzionamento.

Il fattore principale che ne favorisce l'individuazione è il fabbisogno di elettricità molto elevato dei data center. L'elettricità viene solitamente fornita da linee di trasmissione, che sono quasi sempre fuori terra. Sebbene sia possibile interrare le linee di trasmissione, è un'operazione molto più costosa e lunga, e anche i lavori di costruzione necessari sono difficili da occultare.[^234]

Finché ci vorranno più di 100 000 chip per addestrare un'intelligenza artificiale all'avanguardia, sembra abbastanza probabile che gli attori statali possano individuare e monitorare tutti i data center rilevanti.

#### **Prevenire il progresso algoritmico** {#prevenire-il-progresso-algoritmico}

Algoritmi di IA più efficienti possono ridurre le risorse computazionali necessarie per addestrare un'IA, oppure possono consentire la produzione di IA più potenti utilizzando una data quantità di risorse computazionali, [o entrambe le cose](https://arxiv.org/abs/2311.15377).

Il progresso algoritmico è guidato principalmente dalla ricerca e dall'ingegneria, che attualmente dipendono dalle competenze e dall'impegno umano.[^235] Le competenze necessarie per migliorare gli algoritmi di IA sono relativamente rare, il che spiega gli [stipendi elevati](https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html) dei migliori ricercatori del settore.

Anche se oggi queste competenze sono rare, non è chiaro come la situazione potrebbe cambiare con l'aumento del numero di ricercatori che entrano nel settore e la diffusione delle conoscenze. A seconda di come si conta il numero di persone con le competenze necessarie, il numero reale è probabilmente nell'ordine delle centinaia o delle migliaia (ad esempio, in base al numero di ricercatori e ingegneri di IA presso le [principali aziende di IA](https://fortune.com/2025/03/15/ai-talent-wars-startups-google-meta-openai-hiring-scientists-stock-salaries/)). Le stime conservative potrebbero essere molto più alte — ad esempio, ci sono [decine di milioni](https://www.griddynamics.com/blog/number-software-developers-world) di ingegneri software nel mondo.

**Interventi legali e sociali potrebbero rallentare drasticamente il progresso algoritmico.** La maggior parte delle persone non vuole infrangere la legge, soprattutto quando ci sono conseguenze reali. Se fosse illegale pubblicare determinate ricerche sull'IA o eseguire vari esperimenti sull'IA , in base ai rischi catastrofici posti da un'IA sufficientemente potente, ciò probabilmente dissuaderebbe [quasi tutti i potenziali ricercatori nel campo dell'IA](#perché-vietare-la-ricerca?-sembra-una-misura-estrema.). I governi potrebbero implementare controlli sulle esportazioni che renderebbero illegale la condivisione o la pubblicazione di tali ricerche senza una licenza di esportazione e l'approvazione del governo.

Anche i tabù sociali sarebbero d'aiuto. Un precedente è la [Conferenza di Asilomar sul DNA ricombinante](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA) del 1975, che portò a una moratoria volontaria su alcuni esperimenti biologici ritenuti eccessivamente rischiosi. In teoria, gli scienziati potrebbero istituire una moratoria volontaria sull'avanzamento delle capacità dell'IA. Tuttavia, ciò richiederebbe che questi scienziati prendessero sul serio il pericolo rappresentato da un'intelligenza artificiale più intelligente dell'uomo — un cambiamento rispetto allo status quo, che vede il progresso delle capacità dell'IA lodato in molti ambienti. Dati gli incentivi economici miopi e il comportamento tenuto finora dai laboratori, le restrizioni legali esterne appaiono estremamente probabili, a meno che la cultura del settore non cambi *radicalmente* (e in fretta).[^236]

Un elemento essenziale per rendere efficace un divieto imperfetto potrebbe essere una cosa tanto ovvia quanto "assicurarsi che i leader mondiali capiscano che [moriranno loro stessi e le loro famiglie](#alcune-nazioni-non-rifiuterebbero-il-divieto?), se continuano a spingere in quella direzione". Gli scenari di non conformità più probabili sono quelli in cui i governi vedono la superintelligenza sviluppata internamente come una risorsa strategica (o come un miraggio che li distrae da nuovi strumenti di IA redditizi), piuttosto che come un pulsante di suicidio globale. I governi sarebbero molto meno propensi a portare avanti progetti di ricerca segreti sulla superintelligenza se capissero correttamente che ciò equivale a caricare una pistola, puntarsela alla testa e premere il grilletto.

I divieti di ricerca non fermerebbero tutti. Alcuni importanti ricercatori scientifici e dirigenti tecnologici hanno [già affermato](#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?) che distruggere l'umanità è un prezzo accettabile da pagare per il progresso. Ma non dovremmo lasciare che il meglio sia nemico del bene. I progressi algoritmici quanto meno rallenterebbero se queste persone venissero private dei finanziamenti ed emarginate dai loro colleghi, costringendole a condurre le loro letali ricerche nell'illegalità e senza la collaborazione dei loro colleghi più retti.

#### **Più aspettiamo, più diventa difficile** {#più-aspettiamo,-più-diventa-difficile}

Se la produzione e la distribuzione dei chip di IA proseguono sulla traiettoria attuale, la sfida di garantire che un numero sufficiente di chip sia centralizzato e monitorato diventerà sempre più ardua. Anche se gli Stati non sono ancora convinti dei rischi, iniziare oggi a tracciare i chip di IA a livello internazionale significa che in futuro potrebbe rimanere possibile intervenire. Se i governi non si muovono in fretta, questa finestra temporale potrebbe chiudersi presto.

Se ai ricercatori sarà consentito di continuare a far progredire lo stato dell'arte degli algoritmi di IA, è probabile che un numero sempre più piccolo di chip per IA possa rappresentare una seria minaccia. Se e quando i sistemi di IA diventeranno in grado di automatizzare parti del processo di ricerca e sviluppo dell'IA, potrebbe diventare particolarmente difficile controllarne lo sviluppo. Tali sistemi potrebbero essere facilmente copiati e distribuiti e l'hardware necessario per eseguirli potrebbe non essere significativo. (I requisiti hardware per *eseguire* i sistemi di IA sono molto inferiori a quelli per *addestrare* i sistemi di IA).

Prima o poi, potrebbe diventare impossibile per i governi di tutto il mondo fermare lo sviluppo di sistemi di IA superintelligenti. Non siamo ancora a quel punto, ma ogni mese diventa più difficile. Il piano che delineiamo si basa sul presupposto di fermare lo sviluppo dell'IA *a breve*. Esistono altri piani che non si basano su questo presupposto, ma sono più difficili da attuare, hanno costi più elevati per le libertà personali e comportano maggiori probabilità di fallimento.

### Bozza provvisoria di un trattato, con annotazioni {#bozza-provvisoria-di-un-trattato,-con-annotazioni}

Molte persone, compresi i membri della Camera dei Rappresentanti e del Senato degli Stati Uniti, ci hanno chiesto quali proposte legislative concrete e specifiche potrebbero effettivamente aiutare a risolvere questo problema.

Non siamo esperti di politiche pubbliche e vediamo molte possibili risposte a questa domanda, a seconda che si cerchi una legislazione più facile da approvare oggi ma meno direttamente utile (che funga più da trampolino di lancio per una legislazione più ambiziosa) o proposte che sembrano più difficili da approvare ma che affronterebbero in modo sostanziale le questioni fondamentali.

Abbiamo molto più da dire sulle seconde che sulle prime. A tal fine, il team di governance tecnica del MIRI ha elaborato una bozza preliminare di una di queste proposte.

Questa bozza di trattato è pensata per un mondo in cui i leader mondiali abbiano preso coscienza della realtà di un'IA più intelligente dell'uomo. Non è il genere di cosa che prevediamo possa essere approvata domani (nel momento in cui scriviamo), ma potrebbe non essere così difficile far ratificare un trattato simile una volta che il mondo sarà più consapevole del pericolo — un processo che è [già iniziato](#i-rappresentanti-eletti-riconosceranno-questa-come-una-vera-minaccia?) e che speriamo possa continuare.

[La bozza del trattato è disponibile qui](#bozza-provvisoria-di-un-trattato,-con-annotazioni-1), con molte annotazioni. Il processo di stesura ha attinto ampiamente dai precedenti storici di altri trattati, riportati a margine della bozza. Sottolineiamo ancora una volta che questo è un punto di partenza, non un punto di arrivo. Non siamo esperti di politiche pubbliche e potremmo aver commesso alcuni errori ingenui. Tuttavia, speriamo che questa bozza possa servire da ispirazione e da esempio di come un trattato di questo tipo sarebbe possibile e in linea con analoghe iniziative legislative del passato.

### Mantenere ampia la coalizione {#mantenere-ampia-la-coalizione}

C'è chi sostiene che dovremmo assumere una posizione netta contro l'arte generata dall'intelligenza artificiale o le armi robotiche, al fine di lanciare un messaggio più semplice: non contro la superintelligenza, ma contro l'*IA* in sé.

Tralasciando il merito delle varie posizioni sull'arte generata dall'IA, sui deepfake e così via, non riteniamo che questa sia l'opzione migliore dal punto di vista politico. Vogliamo costruire una coalizione per vietare la superintelligenza. Riteniamo che si tratti di una questione straordinariamente urgente e pressante, e vogliamo che questa coalizione sia la più ampia possibile, includendo persone con un'ampia varietà di opinioni sull'arte generata dall'IA, sulla guerra con i droni, sulle auto a guida autonoma, sull'uso dell'IA nelle scuole e così via.

Abbiamo tutti un interesse comune a impedire la creazione di una superintelligenza incontrollata, a prescindere dalle nostre posizioni su altre questioni.

Vogliamo che l'umanità si estingua, per essere sostituita da qualcosa di desolato? Chiunque risponda "no" a questa domanda può collaborare a uno sforzo comune e urgente per fermare la corsa alla superintelligenza.

Non pensiamo che la coalizione possa sopravvivere se ci si rifiuta di collaborare con chiunque la pensi diversamente sull'arte generata dall'IA o sulla guerra con i droni.

La coalizione non sopravviverà neanche se un insieme di altre questioni verrà abbinato alla superintelligenza, al punto che tutti debbano essere d'accordo su una lunga lista di questioni semi-correlate prima di poter collaborare sulla superintelligenza.

Se vi interessano altre questioni legate all'IA, vi invitiamo a lavorare per risolverle. Ma vi chiediamo di non *associare* tali questioni alla superintelligenza. Se vogliamo sopravvivere, nulla dovrebbe essere associato alla sopravvivenza dell'umanità.

Uno dei motivi per cui vivete nel Paese in cui vivete, e non in un cumulo di macerie radioattive lasciate dalla Terza Guerra Mondiale, è che decenni fa l'Oriente e l'Occidente convennero sul fatto che la guerra nucleare rappresentasse una minaccia realistica e grave per l'umanità. L'Oriente e l'Occidente sostennero rispettivamente che l'Occidente e l'Oriente costituivano un'*ulteriore* terribile minaccia per l'umanità. Ma saggiamente trattarono le due minacce (l'annientamento nucleare e la sconfitta ideologica) come aventi una natura diversa.

Dal punto di vista dell'Occidente, era meglio che l'umanità fosse *meno* minacciata dalla guerra nucleare, anche se ancora minacciata dall'Oriente; ciò significava cooperare con l'Oriente abbastanza a lungo da stabilire una linea diretta tra Washington e Mosca, e collaborare a trattati di non proliferazione e altri accordi.

Troppi Paesi devono coordinarsi. Troppe fazioni sono divise (anche al loro interno) per poter evitare la catastrofe se solo coloro che sono d'accordo su tutto possono agire insieme.

Noi facciamo causa comune, con gioia e senza riserve, con chiunque si preoccupi di [altre questioni](#purtroppo,-il-mondo-è-abbastanza-grande-da-contenere-più-di-un-problema.) nel mondo. Collaboreremo senza esitazione con persone con cui siamo in disaccordo politico. Abbiamo lanciato questo messaggio disperato al mondo perché è un messaggio in cui crediamo fermamente, e riteniamo che questo problema vada affrontato *immediatamente* a livello internazionale.

Chiunque voi siate, qualunque sia la causa per cui lottate, qui o altrove, se volete porre fine allo sviluppo vertiginoso di un'intelligenza artificiale più intelligente dell'uomo, combattiamo la stessa battaglia.

## Bozza di un trattato, con annotazioni {#bozza-provvisoria-di-un-trattato,-con-annotazioni-1}

### Avvertenze {#avvertenze}

Di seguito, forniamo un esempio di testo per la stesura di un trattato che potrebbe essere implementato dai principali governi del mondo, qualora riconoscessero i pericoli derivanti dalla superintelligenza artificiale (SIA) e cercassero di impedire a chiunque di costruirla.[^237]

Non siamo decisori politici né esperti di diritto internazionale. Presentiamo questo testo come esempio illustrativo di alcune disposizioni che potrebbe essere prezioso includere in un trattato, utilizzando meccanismi su misura per la situazione attuale e fondati su precedenti storici.

Questa bozza illustra diversi meccanismi che riteniamo necessari per impedire agli sviluppatori di IA di mettere seriamente in pericolo l'umanità. In pratica, prevediamo che diversi aspetti sarebbero probabilmente regolati da trattati distinti.[^238] E, naturalmente, la comunità internazionale dovrebbe redigere con cura l'intero trattato, sottoponendolo a negoziazioni e alla revisione di esperti del settore.

Per ogni articolo del trattato di esempio riportato di seguito, abbiamo fornito una sezione di commento che spiega le ragioni dietro le nostre decisioni chiave e una sezione in cui si analizzano alcuni precedenti rilevanti.

Un vero trattato includerebbe numerosi dettagli. Ne abbiamo inseriti alcuni a titolo di esempio, ma la maggior parte è relegata a degli "allegati" (che non sviluppiamo nella loro interezza). Molte delle quantità e delle soglie numeriche che usiamo nella nostra bozza costituiscono le nostre migliori stime, ma devono comunque essere considerate come tali. Molti di questi valori richiederebbero ulteriori studi e revisioni prima di poter essere finalizzati. È plausibile che questo tipo di dettagli non verrebbe incluso nel trattato stesso, analogamente a quanto avvenuto nel caso del Trattato di Non Proliferazione (TNP) nucleare, in cui i dettagli specifici delle ispezioni e dei cosiddetti programmi di "salvaguardia" sono stati decisi tra ciascun paese e l'AIEA, piuttosto che essere inclusi nel TNP stesso. Tuttavia, per maggiore chiarezza, abbiamo mantenuto le nostre stime direttamente nel testo della bozza di trattato, per contribuire a renderlo più concreto.

### Preambolo {#preambolo}

Gli Stati che concludono il presente Trattato, di seguito denominati "le Parti del Trattato",

Allarmati dalla prospettiva che lo sviluppo della superintelligenza artificiale porterebbe alla morte di tutte le persone e alla fine di ogni impresa umana,

Affermando la necessità di un'azione internazionale urgente, coordinata e sostenuta per impedire la creazione e l'impiego della superintelligenza artificiale nelle condizioni attuali,

Convinti che le misure per impedire l'avanzamento della potenza dell'intelligenza artificiale ridurranno il rischio di estinzione umana,

Riconoscendo che la stabilità del presente Trattato dipende dalla capacità di verificare la conformità da parte di tutte le Parti,

Richiamando il precedente di passati accordi sul controllo degli armamenti e sulla non proliferazione nell'affrontare le minacce alla sicurezza globale,

Impegnandosi a collaborare per facilitare la verifica delle attività di intelligenza artificiale a livello globale, purché queste si mantengano ben lontane dalla superintelligenza artificiale, e cercando di preservare l'accesso ai benefici dei sistemi di intelligenza artificiale, evitando al contempo i pericoli,

Hanno convenuto quanto segue:

---

#### Precedenti

Il preambolo di questo Trattato è basato su quello del [TNP](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf)[^239], che inizia così:

> Considerando la devastazione che una guerra nucleare causerebbe all'umanità intera e la conseguente necessità di compiere ogni sforzo per scongiurare il pericolo di una tale guerra e di adottare misure volte a salvaguardare la sicurezza dei popoli...

e subito aggiunge:

> Affermando il principio secondo cui i benefici delle applicazioni pacifiche della tecnologia nucleare, incluse le eventuali ricadute tecnologiche che gli Stati dotati di armi nucleari potrebbero ricavare dallo sviluppo di ordigni esplosivi nucleari, dovrebbero essere disponibili per scopi pacifici a tutte le Parti del Trattato, siano esse Stati dotati di armi nucleari o Stati non dotati di armi nucleari...

Così facendo, il preambolo invita il mondo a unirsi alle Parti responsabili nel salvaguardare l'umanità dalla minaccia catastrofica di una tecnologia potente e a condividere i benefici che possono essere tranquillamente consentiti. Il nostro preambolo cerca di seguire questo esempio.

---

### Articolo I: Scopo principale {#articolo-i:-scopo-principale}

Ciascuna Parte del presente Trattato si impegna a non sviluppare, rilasciare, o cercare di sviluppare o rilasciare in alcun modo la superintelligenza artificiale ("SIA"). Ciascuna Parte si impegna a vietare e impedire tale sviluppo all'interno dei propri confini e giurisdizioni e, data l'incertezza su quando ulteriori progressi potrebbero produrre una SIA, si impegna a non intraprendere o permettere attività che costituiscano un progresso materiale verso la SIA, come descritto nel presente Trattato. Ciascuna Parte favorirà, o non ostacolerà, le misure ragionevoli di altre Parti per dissuadere e prevenire tale sviluppo da parte di, e all'interno di, Stati e giurisdizioni non Parte. Ciascuna Parte attuerà e adempirà a tutti gli altri obblighi, misure e disposizioni di verifica stabiliti nel presente Trattato.

Laddove alcune classi di infrastrutture e capacità di IA che si mantengono lontane dalla SIA possano essere ritenute accettabili, ma solo in condizioni di supervisione internazionale, solo le Parti del Trattato possono svolgere tali attività, o possedere o gestire chip di IA e capacità di produzione che potrebbero potenzialmente portare allo sviluppo di una SIA se non supervisionate. Alle non Parti è negato tale accesso per la sicurezza delle Parti e di tutta la vita sulla Terra (Articolo V, Articolo VI, Articolo VII).

Le Parti si impegnano a un processo di risoluzione delle controversie (Articolo XI) per ridurre al minimo le Azioni di Protezione non necessarie (Articolo XII).

---

#### Precedenti

L'Articolo I del TNP, come in molti trattati, stabilisce l'impegno di alto livello che le Parti assumono — in questo caso, di non condividere le proprie armi nucleari o aiutare altri a ottenerle:

> Ogni Stato Parte del Trattato dotato di armi nucleari si impegna a non trasferire a nessun destinatario, direttamente o indirettamente, armi nucleari o altri dispositivi esplosivi nucleari o il controllo su tali armi o dispositivi esplosivi; e a non facilitare, incoraggiare o indurre in alcun modo Stati non dotati di armi nucleari a fabbricare o acquisire in altro modo armi nucleari o altri dispositivi esplosivi nucleari, o il controllo su tali armi o dispositivi esplosivi.

L'impegno riassunto nell'Articolo I della nostra bozza di accordo è più forte, perché non si può permettere che avvenga una *fuga* di SIA da parte di chiunque e in nessun luogo, nemmeno una volta.[^240] Non basterà non “facilitare, incoraggiare o indurre” altri a svilupparla. Abbiamo quindi incluso l'impegno a “facilitare, o non ostacolare, misure ragionevoli” da parte delle Parti per dissuadere e prevenire tale sviluppo ovunque.

Il TNP mira a contenere una minaccia esistente (le armi nucleari), mentre la nostra bozza di accordo mira a impedire che una minaccia (la SIA) venga mai a esistere. Un precedente per la prevenzione dello sviluppo di nuove tecnologie pericolose si trova nel [Protocollo sulle armi laser accecanti](https://www.un.org/en/genocideprevention/documents/atrocity-crimes/Doc.43_CCW%20P-IV.pdf), parte della [Convenzione su alcune armi convenzionali](https://treaties.un.org/doc/Treaties/1983/12/19831202%2001-19%20AM/XXVI-2-revised.pdf#page=25).[^241] Il suo Articolo I recita:

> È vietato impiegare armi laser specificamente progettate, come loro unica funzione di combattimento o come una delle loro funzioni di combattimento, per causare cecità permanente alla visione non potenziata, cioè all'occhio nudo o a un occhio dotato di dispositivi correttivi della vista. Le Alte Parti contraenti non trasferiranno tali armi a nessuno Stato o entità non statale.

Tale formulazione, tuttavia, non mira a impedire a chicchessia di testare o realizzare accidentalmente un sistema del genere. Il nostro accordo deve essere sufficientemente forte da impedire la realizzazione accidentale di una SIA. Poiché non è chiaro dove possa trovarsi il punto di non ritorno, il nostro Articolo I include l'impegno a "non intraprendere né consentire attività che portino a un progresso sostanziale verso la SIA".

---

### Articolo II: Definizioni {#articolo-ii:-definizioni}

Ai fini del presente Trattato:

1. **Intelligenza artificiale (IA)**: un sistema computazionale che svolge compiti che richiedono cognizione, pianificazione, apprendimento o azioni negli ambiti fisico, sociale o cibernetico. Ciò include i sistemi che svolgono compiti in condizioni variabili e imprevedibili, o che possono apprendere dall'esperienza e migliorare le proprie prestazioni.  
2. **Superintelligenza artificiale (SIA)**: è definita operativamente come qualsiasi IA con prestazioni cognitive sufficientemente superumane da poter pianificare ed eseguire con successo la distruzione dell'umanità.  
   1. Ai fini del presente Trattato, si presume che lo sviluppo di IA non esplicitamente autorizzato dall'AISI (Articolo III), e che viola i limiti descritti nell'Articolo IV, abbia lo scopo di creare una superintelligenza artificiale.  
3. Le **attività pericolose di IA** sono quelle che aumentano sostanzialmente il rischio che venga creata una superintelligenza artificiale, e non si limitano alla fase finale dello sviluppo di una superintelligenza artificiale, ma includono anche le fasi preliminari come stabilito in questo Trattato. L'ambito completo delle attività pericolose di IA è specificato dagli Articoli da IV a IX e può essere elaborato e modificato attraverso l'applicazione del trattato e le attività dell'AISI.  
4. Per **Operazioni in virgola mobile (FLOP, acronimo di FLoating-point OPerations)** si intende la misura computazionale usata per quantificare la scala dell'addestramento e del post-addestramento, basata sul numero di operazioni matematiche eseguite. Il conteggio delle FLOP deve essere effettuato o in operazioni equivalenti al formato in virgola mobile a mezza precisione (FP16) o in operazioni totali (nel formato utilizzato), scegliendo il valore più alto.  
5. Per **Ciclo di addestramento** si intende qualsiasi processo computazionale che ottimizza i parametri di un'IA (cioè i dettagli della propagazione delle informazioni attraverso una rete neurale, ad esempio pesi e bias) utilizzando metodi di ricerca/apprendimento basati sul gradiente o altri metodi, tra cui pre-addestramento, messa a punto, apprendimento per rinforzo, ricerche di iperparametri su larga scala che aggiornano i parametri e auto-gioco iterativo o addestramento curricolare.  
6. Per **Pre-addestramento** si intende la fase di addestramento con cui i parametri di un'IA vengono inizialmente ottimizzati utilizzando *set di dati* su larga scala per apprendere schemi o rappresentazioni generalizzabili prima di qualsiasi adattamento specifico per un'attività o un ambito. Include l'ottimizzazione supervisionata, non supervisionata, auto-supervisionata e basata sul rinforzo quando viene eseguita prima di tale adattamento.  
7. Per **Post-addestramento** si intende una fase di addestramento eseguita dopo il pre-addestramento di un modello. Inoltre, qualsiasi addestramento eseguito su un'IA creata prima che questo Trattato entrasse in vigore è considerato post-addestramento.  
8. Per **Chip informatici avanzati** si intendono circuiti integrati fabbricati con processi avanzati almeno quanto il nodo di elaborazione da 28 nanometri.  
9. Per **Chip per l'IA** si intendono circuiti integrati specializzati progettati principalmente per i calcoli di IA, incluse, a titolo esemplificativo ma non esaustivo, le operazioni di addestramento e inferenza per i modelli di apprendimento automatico [la definizione andrebbe precisata in un Allegato]. Tra questi vi sono GPU, TPU, NPU e altri acceleratori di IA. Possono essere inclusi anche componenti hardware non originariamente progettati per l'IA, ma che possono essere efficacemente riadattati a tale scopo. I chip per l'IA sono un sottoinsieme dei chip informatici avanzati.  
10. Per **Hardware per IA** si intende tutto l'hardware informatico utilizzato per l'addestramento e l'esecuzione delle IA. Ciò include i chip per l'IA, così come le apparecchiature di rete, di alimentazione e di raffreddamento.  
11. **Apparecchiature per la produzione di chip per l'IA**: le apparecchiature utilizzate per fabbricare, testare, assemblare o confezionare chip per l'IA, tra cui, a titolo esemplificativo ma non esaustivo, le apparecchiature per litografia, deposizione, incisione, metrologia, collaudo e packaging avanzato [un elenco più completo andrebbe definito in un Allegato].  
12. **Equivalente-H100**: unità di potenza di calcolo (FLOP al secondo) pari a quella di un acceleratore NVIDIA H100 SXM, 990 TFLOP/s in VM16, o a una Potenza di Elaborazione Totale (PET) di 15.840, dove la PET è calcolata come PET \= 2 × MacTOPS non-sparse × (lunghezza in bit dell'input della moltiplicazione).  
13. **Cluster di chip coperti (CCC)**: qualsiasi insieme di chip per l'IA o cluster in rete con una capacità di calcolo effettiva aggregata superiore a 16 equivalenti-H100. Un cluster in rete si riferisce a chip che sono fisicamente co-locati, che hanno una larghezza di banda aggregata tra i nodi (definita come la somma della larghezza di banda tra host/chassis distinti) superiore a 25 Gbit/s, o che sono collegati in rete per eseguire carichi di lavoro in comune. La capacità di calcolo effettiva aggregata di 16 chip H100 è di 15.840 TFLOP/s, o 253440 PET, e si basa sulla somma delle PET dei singoli chip. Esempi di CCC includono: il server GB200 NVL72, tre server H100 HGX a otto vie ubicati nello stesso edificio, CloudMatrix 384, un pod con 32 chip TPUv6e e qualsiasi supercomputer.  
14. **Mezzi tecnici nazionali (MTN)**: l'insieme delle capacità satellitari, aeree, informatiche, di intelligence dei segnali e di analisi delle immagini (comprese quelle termiche), e altre capacità di telerilevamento impiegate dalle Parti a fini di verifica, in conformità con il presente Trattato.  
15. **Verifica dell'uso dei chip**: metodi che forniscono indicazioni sulle attività eseguite su specifici chip, al fine di distinguere le attività consentite da quelle proibite.  
16. Per **metodi utilizzati per creare modelli di frontiera** si intende l'ampia gamma di metodi utilizzati nello sviluppo dell'IA. Questi includono, a titolo esemplificativo ma non esaustivo, architetture di IA, ottimizzatori, metodi di tokenizzazione, cura dei dati, generazione di dati, strategie di parallelismo, algoritmi di addestramento (ad esempio algoritmi apprendimento per rinforzo) e altri metodi di addestramento. Ciò include il post-addestramento, ma esclude i metodi che non modificano i parametri di un modello già addestrato, come il prompting. In futuro potrebbero essere creati nuovi metodi.

---

#### Note

La definizione di IA qui riportata (adattata dall'[AI Whistleblower Protection Act](https://www.congress.gov/bill/119th-congress/senate-bill/1792/text) del senatore Chuck Grassley) è forse troppo ampia, e prevediamo che dovrà essere definita in modo più preciso per escludere sistemi informatici palesemente sicuri come i correttori ortografici o i sistemi di riconoscimento delle immagini.

Nell'attuale paradigma di sviluppo dell'IA, questa definizione è probabilmente accettabile, dato che lo sviluppo dell'IA è ben distinguibile da ricerca e sviluppo non legati all'IA per via dell'uso di ingenti risorse computazionali. Tuttavia, la definizione definitiva di IA che verrà adottata dovrebbe comprendere più dei soli sistemi di apprendimento profondo o apprendimento automatico. Sebbene l'apprendimento profondo sia l'attuale paradigma dominante per l'IA, il trattato dovrebbe tenere conto della possibilità che in futuro un altro paradigma di IA diventi preponderante; altrimenti, un trattato che vieti esclusivamente l'apprendimento automatico incoraggerebbe i ricercatori a sviluppare nuovi paradigmi per costruire IA più potenti e generali, con conseguenze potenzialmente catastrofiche. Se dovesse emergere un paradigma innovativo, in particolare uno che non sia così intensivo nell'uso di chip per l'IA come l'apprendimento profondo, il trattato andrebbe probabilmente aggiornato.

Usiamo l'equivalente-H100 come unità di misura principale per la capacità di calcolo. Nell'Articolo V, questo valore viene usato per stabilire la dimensione del più grande cluster di chip non monitorato consentito (16 equivalenti-H100).[^242] L'Articolo IV definisce le soglie in termini di operazioni totali usate per addestrare un'IA; di conseguenza, fissare limiti alle operazioni non monitorate al secondo renderebbe proibitivi i tempi necessari per condurre un addestramento di dimensioni illegali su hardware non monitorato.

Usiamo gli equivalenti-H100 per tenere conto del fatto che esistono diversi chip per l'IA, ma l'aspetto per noi più importante è il numero di operazioni che possono eseguire in un dato intervallo di tempo. Esistono altre metriche importanti per i chip (come la memoria ad alta larghezza di banda), ma nel complesso riteniamo che siano meno rilevanti del numero di operazioni al secondo. Usare l'equivalente-H100 come unità di misura è un modo [piuttosto standard](https://www.rand.org/pubs/perspectives/PEA3776-1.html) per discutere della capacità di calcolo.

La definizione di cluster di chip coperti (CCC) che includiamo è un'ipotesi iniziale su come definire tale concetto. Idealmente, il limite dovrebbe essere abbastanza alto da impedire alle persone comuni di infrangere le regole (ad esempio, una larghezza di banda di 25 Gbit/s tra chassis è più veloce delle normali connessioni Internet non da data center; il possesso di più di 16 equivalenti-H100 da parte di un singolo individuo è un evento molto raro, oltre che costoso). Dovrebbe anche essere abbastanza basso da impedire attività di IA pericolose e rendere difficile la sovversione (cioè, rendere difficile l'addestramento distribuito su più insiemi di chip sub-CCC). Discutiamo questa decisione più approfonditamente nel commento all'Articolo V.

I chip per l'IA sono un sottoinsieme dei chip avanzati e non esiste una linea di demarcazione netta che li distingua dai chip non dedicati all'IA. Anziché definire e fare affidamento su tale distinzione, usiamo la capacità di calcolo complessiva (in operazioni al secondo) di un cluster, misurata in equivalenti-H100. Se i chip possono essere configurati per l'addestramento o l'esecuzione di IA e superano la soglia definita, il Trattato ne richiede il monitoraggio.

Si noti che la dicitura "mezzi tecnici nazionali" (MTN) potrebbe essere considerata obsoleta come [termine](https://fas.org/publication/ntm-obe/) ufficiale da alcuni governi. La usiamo in questa bozza di trattato, seguendo lo stile dei precedenti accordi sul controllo degli armamenti per facilitare il confronto. 

### Articolo III: AISI {#articolo-iii:-aisi}

1. Le Parti del Trattato istituiscono l'Agenzia Internazionale per la Superintelligenza (AISI), al fine di attuare il presente Trattato e le sue disposizioni, incluse quelle per la verifica internazionale del suo rispetto, e per fornire un foro di consultazione e cooperazione tra le Parti.  
2. Vengono istituiti quali organi dell'AISI: la Conferenza delle Parti, il Consiglio Esecutivo e il Segretariato Tecnico.  
3. Conferenza delle Parti  
   1. La Conferenza delle Parti è composta da tutte le Parti del Trattato.  
   2. La Conferenza delle Parti: determina la politica generale; adotta e supervisiona il bilancio; elegge i membri del Consiglio Esecutivo; esamina le questioni di conformità segnalate dal Consiglio Esecutivo; e adotta e rivede gli allegati su raccomandazione del Consiglio Esecutivo.  
   3. Si riunisce in sessione ordinaria almeno una volta all'anno, o con una frequenza maggiore stabilita dalla Conferenza, oltre alle sessioni straordinarie che si rendessero necessarie. Ogni Parte ha diritto a un voto. Il quorum è costituito dalla maggioranza delle Parti.  
4. Consiglio Esecutivo  
   1. Il Consiglio Esecutivo è composto da 15 membri: (i) 5 seggi designati per i membri permanenti del Consiglio di sicurezza delle Nazioni Unite e (ii) 10 seggi elettivi distribuiti secondo un'equa rappresentanza geografica. I dettagli sono esposti nell'Allegato A.  
   2. I membri eletti restano in carica per due anni. Metà dei seggi viene rinnovata ogni anno.  
   3. Il Consiglio Esecutivo: approva le ispezioni su richiesta; raccomanda il bilancio e la politica alla Conferenza; nomina il Direttore Generale; supervisiona il Segretariato Tecnico tramite il potere di veto.  
   4. I processi decisionali sono i seguenti:  
      1. Il Consiglio Esecutivo elegge il proprio presidente e vicepresidente.  
      2. Possono fungere da presidente di seduta il Presidente o il Vicepresidente.  
      3. La votazione avviene secondo il principio "un membro, un voto".  
      4. Per l'approvazione di un'ispezione su richiesta, ai sensi dell'Articolo X, è richiesta la maggioranza dei voti.  
      5. Per porre il veto alle modifiche delle definizioni o dei protocolli di salvaguardia apportate dal Segretariato Tecnico è richiesta la maggioranza dei due terzi.  
      6. Per revocare o nominare un Direttore Generale è richiesta la maggioranza dei due terzi.  
      7. Tutte le altre decisioni richiedono la maggioranza dei due terzi.  
      8. Il quorum è di due terzi dei membri del Consiglio Esecutivo.  
5. Segretariato Tecnico e Direttore Generale  
   1. Il Direttore Generale del Segretariato Tecnico ne è il capo e il principale funzionario amministrativo.  
   2. Il Direttore Generale viene nominato dal Consiglio Esecutivo per un mandato di quattro anni, rinnovabile una volta. Il Consiglio Esecutivo può revocare il Direttore Generale.  
   3. Il Segretariato Tecnico comprende inizialmente divisioni tecniche per il tracciamento dei chip e le misure di salvaguardia della produzione, le misure di salvaguardia per la verifica dell'uso dei chip, i controlli sulla ricerca, il consolidamento delle informazioni, le revisioni tecniche, l'amministrazione e le finanze, e le questioni legali e di conformità. Il Direttore Generale può creare e sciogliere divisioni tecniche.  
   4. Il Segretariato Tecnico, per tramite del Direttore Generale, propone le modifiche alle definizioni tecniche e ai protocolli di salvaguardia necessari ad attuare gli Articoli IV, V, VI, VII, VIII, IX e X del presente Trattato.  
      1. Le modifiche urgenti alle soglie di FLOP (Articolo IV), alle dimensioni dei cluster di calcolo interessati (Articolo V) e ai limiti della ricerca soggetta a restrizioni (Articolo VIII) possono essere attuate immediatamente dal Direttore Generale qualora l'inazione ponga un rischio per la sicurezza. Tali modifiche rimangono in vigore per trenta giorni. Trascorso tale termine, le modifiche devono essere approvate dal Consiglio Esecutivo per rimanere in vigore.  
      2. Il Consiglio Esecutivo delibera sulle questioni sostanziali, per quanto possibile, per consenso; il Direttore Generale si adopera per raggiungere il consenso. Qualora non sia possibile raggiungere il consenso entro 24 ore, si procede a una votazione e il Consiglio Esecutivo approva le modifiche se la maggioranza dei membri presenti e votanti vota a favore, altrimenti le respinge.  
6. Il bilancio ordinario dell'AISI è finanziato con i contributi obbligatori delle Parti, utilizzando una scala derivata dalla scala di valutazione delle Nazioni Unite, soggetta a un massimale e a un minimale stabiliti dal Consiglio Esecutivo. Gli Stati membri hanno anche la possibilità di versare contributi volontari per la ricerca sulla sicurezza dell'IA relativa alle attività di allineamento, interpretabilità e sviluppo delle capacità degli Stati membri, compresi gli usi benefici dell'IA sicura, lo sviluppo di banchi di prova, le buone pratiche, la condivisione di informazioni e la facilitazione della cooperazione e delle attività congiunte liberamente ispirate al modello della [rete AIEA dei Centri di supporto alla sicurezza nucleare](https://www.iaea.org/services/networks/nssc).

---

#### Note

Come in altri organismi internazionali, l'AISI sarebbe composta da diplomatici ed esperti tecnici dei paesi firmatari. Il punto principale di questa formulazione è che all'AISI viene conferita l'autorità di attuare la maggior parte di quanto previsto dal trattato e di aggiornare alcuni aspetti dello stesso nel corso del tempo. Questo approccio dà la priorità alla prevenzione della creazione della superintelligenza e al mantenimento della forza del trattato fino a quando il mondo non sarà pronto a procedere. In questo modo, questa bozza conferirebbe a questa organizzazione multinazionale l'autorità di fare gran parte di quanto richiesto. L'AISI centralizza l'attuazione di diverse funzioni chiave del trattato, tra cui il mantenimento dei limiti precisi della ricerca, dello sviluppo e del dispiegamento dell'IA consentiti, la verifica primaria del rispetto del trattato, e il consolidamento delle informazioni di intelligence riservate dei firmatari. Un aspetto cruciale è che il funzionamento cooperativo dell'AISI crea, nel tempo, la fiducia necessaria tra le parti del trattato.

Detto ciò, questo tipo di approccio comporta dei compromessi. Il primo compromesso è che una maggiore centralizzazione richiede una maggiore fiducia da parte delle nazioni firmatarie. I potenziali firmatari del trattato potrebbero non ritenere politicamente fattibile assegnare questo livello di autorità a un'organizzazione internazionale, oppure potrebbero non avere fiducia che l'organizzazione possa operare in modo sufficientemente indipendente dall'influenza dominante del suo membro o dei suoi membri più potenti.

Un'alternativa potrebbe essere quella di centralizzare solo quelle poche funzioni la cui centralizzazione è indispensabile (come il mantenimento e il chiarimento dei limiti alla ricerca, allo sviluppo e al dispiegamento dell'IA), lasciando che i singoli firmatari verifichino individualmente la conformità fino a ritenerla comprovata.

Un altro compromesso riguarda il grado di inclusività di un tale trattato. Il nostro testo di esempio creerebbe un'organizzazione multilaterale in cui tutti gli Stati sono invitati a firmare il trattato e a partecipare alla sua attuazione. Un'alternativa è un trattato incentrato, ad esempio, solo su Cina e Stati Uniti. L'idea di fondo sarebbe quella di costruire un regime di verifica bilaterale ristretto che soddisfi le esigenze di ciascuna parte, sacrificando al minimo l'autonomia e la trasparenza. A ciascuna parte verrebbe quindi assegnato l'obiettivo, separato e successivo, di far aderire altri Stati.

Poiché lo scopo di questo trattato è dimostrare come potrebbero presentarsi i controlli internazionali *se* i leader di tutto il mondo si rendessero conto dei pericoli pressanti, illustriamo una struttura che funzionerebbe con molte Parti, le quali avrebbero tutte qualcosa da guadagnare dall'adesione.

Pertanto, la struttura proposta per il Consiglio Esecutivo dell'AISI include tutti i membri permanenti del Consiglio di Sicurezza dell'ONU ed è basata sulla composizione dell'AIEA.

Dato lo status di TSMC come produttore preminente di chip per l'IA, qualsiasi trattato sull'intelligenza artificiale deve considerare come affrontare la questione di Taiwan. Scegliamo di utilizzare il precedente del TNP riguardo a Taiwan. Idealmente, Taiwan dovrebbe attuare accordi formali e/o dichiarazioni in cui affermi di considerarsi vincolata dai principi di questo Trattato. Ciò comporterebbe un'adesione di fatto al Trattato. Ancor più importante, Taiwan dovrebbe accettare un accordo che consenta ispezioni di routine e/o su richiesta in loco per garantire la sua conformità ai principi del trattato.

Resta una questione in parte aperta come il Consiglio Esecutivo e l'AISI debbano prendere le decisioni e quali poteri vengano delegati ai diversi organi. Questo Articolo include una proposta per tale struttura, una proposta che conferisce un potere significativo al Segretariato Tecnico, attribuendo al contempo un potere di supervisione al Consiglio Esecutivo.

Un vantaggio di questa impostazione è che consente all'organo tecnico di prendere decisioni rapidamente e gli conferisce un ampio mandato per adempiere alla sua missione, sebbene qualsiasi modifica richieda l'approvazione della maggioranza semplice dei membri del Consiglio Esecutivo entro 30 giorni per rimanere in vigore. Se da un lato i leader mondiali possono esitare a delegare così tanto potere agli esperti tecnici, dall'altro è altrettanto probabile che questi ultimi non si fidino né degli attori geopolitici per risolvere le spinose questioni tecniche che emergeranno nell'attuazione di questo Trattato, né della loro capacità di adattarsi a un panorama tecnico in continua evoluzione. Questo Articolo presenta solo una delle possibili proposte su come bilanciare il potere decisionale tra questi gruppi, ma esistono molti altri approcci.

Un altro approccio simile consisterebbe nel disaggregare ulteriormente le responsabilità, le definizioni e i tipi di salvaguardia attuati dall'AISI (ad esempio, le soglie di FLOP per l'addestramento, la definizione di CCC, la definizione di chip per IA, la classificazione di una particolare struttura come impianto di produzione di chip, i protocolli di verifica dell'uso dei chip, la definizione di Ricerca Soggetta a Restrizioni, ecc.), e nello stabilire procedure di voto diverse per queste modifiche, a seconda del loro impatto.

---

#### Precedenti

La struttura di governo tripartita dell'Agenzia Internazionale per la Superintelligenza (AISI), prevista dalla nostra bozza di trattato, è basata su quella dell'OPCW[^243], l'organismo incaricato di attuare la Convenzione sulle Armi Chimiche (ACC). Anche i nomi di questi organi sono presi in prestito dall'OPCW. (La versione definitiva di un trattato potrebbe preferire strutture e nomi alternativi che svolgano le stesse funzioni; di seguito forniamo alcuni precedenti per assetti meno centralizzati).

Il Consiglio Esecutivo istituito dal nostro Paragrafo 4, commi (a) e (d), emula il Consiglio dei Governatori del TNP. La designazione di cinque dei quindici seggi del Consiglio ai membri permanenti del Consiglio di Sicurezza delle Nazioni Unite riflette il fatto che i cinque Stati nucleari originari del TNP erano anche i cinque membri permanenti del Consiglio di Sicurezza dell'ONU; senza la loro partecipazione come partner chiave, il TNP sarebbe probabilmente fallito fin dall'inizio.

Anche la nostra disposizione per "10 seggi elettivi distribuiti in base a un'equa rappresentanza geografica" si ispira al TNP, il quale stabilisce che i suoi Governatori uscenti includano "il membro più avanzato nella tecnologia dell'energia atomica, compresa la produzione di materiali di base in ciascuna delle" otto regioni specificate.

La questione di Taiwan complica la nostra idea di trattato, data la sua delicata situazione geopolitica e il suo status di principale produttore mondiale di chip per l'IA. Fortunatamente, esistono precedenti che possono fungere da guida: sebbene Taiwan non sia parte del TNP, ha dichiarato in più occasioni di considerarsi vincolata dai suoi principi. Taiwan consente all'AIEA di condurre ispezioni e applicare salvaguardie ai suoi impianti nucleari attraverso un accordo trilaterale con gli Stati Uniti e l'AIEA stessa. Una soluzione simile potrebbe essere adottata per la nostra bozza di trattato.

I processi decisionali del Consiglio Esecutivo della nostra bozza di trattato sono stati modellati sul [Regolamento del Consiglio dei Governatori](https://www.iaea.org/about/policy/board/rules-and-procedures-of-the-board-of-governors) adottato dall'Agenzia Internazionale per l'Energia Atomica (AIEA), la principale organizzazione per la governance internazionale della tecnologia nucleare.[^244] Allo stesso modo, le procedure di voto seguono lo [Statuto dell'AIEA](https://www.iaea.org/about/statute#a1-6).

In altri trattati sulle armi nucleari si possono trovare precedenti di meccanismi di attuazione meno centralizzati (ma comunque potenzialmente efficaci). Il [Trattato sulle forze nucleari a raggio intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm#text) (FNRI) e i trattati per la riduzione delle armi strategiche ([START I](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html), [START II](https://2009-2017.state.gov/t/avc/trty/104150.htm) e [New START](https://2009-2017.state.gov/documents/organization/140035.pdf)) affidano la responsabilità dell'attuazione e della verifica alle singole parti; ciascuna si impegna ad adottare procedure che consentano all'altra di ottenere ragionevoli garanzie di conformità.

Le "ispezioni su richiesta" di cui al Paragrafo 4(c) sono basate sul meccanismo della Parte X della CAC; approfondiremo questo precedente nell'Articolo X.

---

### Articolo IV: Addestramento dell'IA {#articolo-iv:-addestramento-dell-ia}

1. Ciascuna Parte si impegna a vietare l'addestramento dell'IA al di sopra delle seguenti soglie: qualsiasi sessione di addestramento che superi 1e24 FLOP o qualsiasi sessione di post-addestramento che superi 1e23 FLOP. Ciascuna Parte si impegna a non condurre sessioni di addestramento al di sopra di tali soglie e a non permettere a nessuna entità soggetta alla propria giurisdizione di condurre tali sessioni.  
   1. Il Segretariato Tecnico può modificare queste soglie, in conformità con la procedura descritta nell'Articolo III. Il Consiglio Esecutivo può porre il veto a tali decisioni con un voto a maggioranza di due terzi.  
2. Ciascuna Parte deve notificare all'AISI qualsiasi sessione di addestramento compresa tra 1e22 e 1e24 FLOP prima del suo avvio. Ciò si applica alle sessioni di addestramento condotte dalla Parte o da qualsiasi entità soggetta alla sua giurisdizione.  
   1. Tale notifica deve includere, a titolo esemplificativo ma non esaustivo, tutto il codice di addestramento e una stima delle FLOP totali da utilizzare. La Parte deve fornire al personale dell'AISI un accesso supervisionato a tutti i dati, con una registrazione degli accessi adeguata alla sensibilità dei dati e protezioni contro la duplicazione o la divulgazione non autorizzata. La mancata fornitura di un sufficiente accesso ai dati al personale dell'AISI costituisce motivo di diniego della sessione di addestramento, a discrezione dell'AISI. L'AISI può richiedere qualsiasi documentazione aggiuntiva relativa alla sessione di addestramento. L'AISI approverà inoltre in via preliminare una serie di piccole modifiche che potrebbero essere apportate alla procedura durante la sessione di addestramento. Tali modifiche dovranno essere comunicate all'AISI qualora vengano apportate.  
   2. La mancata risposta da parte dell'AISI trascorsi 30 giorni costituisce approvazione; tuttavia, l'AISI può prorogare tale periodo comunicando di necessitare di ulteriore tempo per la revisione. Non vi sono limiti a tali proroghe, ma le Parti possono presentare ricorso per ritardi eccessivi al Direttore o al Consiglio Esecutivo.  
   3. L'AISI può monitorare tali sessioni di addestramento e la Parte fornirà all'AISI, su richiesta, i checkpoint del modello, compreso il modello finale addestrato [i dettagli iniziali di tale monitoraggio dovranno essere descritti in un Allegato].  
   4. Nel caso in cui il monitoraggio indichi capacità o comportamenti preoccupanti dell'IA, l'AISI può emettere un ordine di sospensione di una sessione o di una classe di sessioni di addestramento, finché non riterrà sicuro procedere con la sessione.  
   5. L'AISI adotterà pratiche di sicurezza solide. L'AISI non condividerà informazioni sulle sessioni di addestramento dichiarate, a meno che non stabilisca che una di esse violi il Trattato; in tal caso, fornirà a tutte le Parti del Trattato informazioni sufficienti per determinare se si sia verificata una violazione.  
   6. Qualora una Parte individui una sessione di addestramento al di sopra delle soglie designate, dovrà segnalarla all'AISI e interromperla (se in corso). Tale sessione potrà riprendere solo previa approvazione dell'AISI.  
3. Ogni Parte, e gli enti sotto la sua giurisdizione, possono condurre sessioni di addestramento inferiori a 1e22 FLOP senza la supervisione o l'approvazione dell'AISI.  
4. L'AISI può autorizzare, con una maggioranza di due terzi dei voti del Consiglio Esecutivo, deroghe specifiche per attività quali valutazioni di sicurezza, veicoli a guida autonoma, tecnologia medica e altre attività, con modalità ritenute sicure dal Direttore Generale. Tali deroghe possono consentire sessioni di addestramento superiori a 1e24 FLOP con la supervisione dell'AISI, o una presunzione di approvazione da parte dell'AISI per sessioni di addestramento comprese tra 1e22 e 1e24 FLOP.

---

#### Note

Uno dei modi principali per migliorare le attuali IA è [aumentare le risorse computazionali](#impedire-la-creazione-di-chip-per-l-ia-più-numerosi-e-potenti) investite nell'addestramento delle IA. Limitare queste risorse e la ricerca sul progresso algoritmico (descritta nell'Articolo VIII) può ridurre il rischio, che nel breve periodo, venga creata un'IA più intelligente dell'uomo.

Il divieto si basa sul numero di operazioni computazionali, dato che è un parametro relativamente facile da definire e misurare, e le prestazioni degli attuali modelli di IA all'avanguardia ci danno un'idea delle quantità di potenza di calcolo che risultano sicure con gli algoritmi odierni.

In un mondo perfetto, limiteremmo invece l'addestramento in base alle capacità del sistema addestrato. Ma nessuno ha la capacità tecnica di prevedere con certezza cosa sarà in grado di fare o non fare una nuova IA prima che venga addestrata. Pertanto, le risorse computazionali vengono utilizzate come proxy.

Il divieto assoluto di 1e24 FLOP per l'addestramento è leggermente inferiore alla quantità di risorse computazionali usate per i modelli prossimi allo stato dell'arte dell'agosto 2025, come DeepSeek-V3 con [3e24 FLOP](https://epoch.ai/data/ai-models). Abbiamo scelto questa soglia perché è al di sotto del livello al quale prevediamo che le IA possano diventare pericolose (dati gli algoritmi attuali), e perché offre un certo margine di manovra in caso di lievi progressi algoritmici.

Il divieto di superare 1e23 FLOP nel post-addestramento è pensata per essere applicata alle IA create prima dell'entrata in vigore del Trattato. Molte di queste IA saranno state create usando più di 1e24 FLOP (a metà 2025 esistono [tra 50 e 100 modelli di questo tipo](https://epoch.ai/tools/model-counts)). Dato che i pesi di molte di queste IA saranno stati resi pubblici, non è fattibile impedirne l'uso, ma è fattibile impedirne modifiche sostanziali tramite il post-addestramento.

Un addestramento da 1e22 FLOP su 16 H100 richiederebbe circa una settimana. È un'operazione che richiede una potenza di calcolo tale che gli hobbisti non rischiano di superare accidentalmente la soglia addestrando modelli piccoli e consentiti. Le IA addestrate su questa scala (con gli algoritmi attuali) si sono finora dimostrate tutt'altro che pericolose. I progressi nella ricerca sugli algoritmi di IA, se la loro esecuzione e disseminazione non venissero altrimenti impedite, potrebbero porre l'umanità in uno scenario ancora più pericoloso, richiedendo divieti mirati ad attività che attualmente potrebbero essere svolte da piccoli gruppi determinati e disposti a sostenere spese ingenti.

Nella fascia di potenza di calcolo tra 1e22 e 1e24 FLOP, l'AISI riceverebbe in anticipo il codice di addestramento e le stime delle FLOP totali. Ciò le consentirebbe di approvare il ciclo di addestramento, a condizione che questo non utilizzi metodi algoritmici innovativi.

L'addestramento di un'IA comporta spesso piccole correzioni di bug in corso d'opera (come l'installazione della versione corretta di una libreria o il rimescolamento dei dati di addestramento). L'AISI può pre-approvare una serie di piccole modifiche a un ciclo di addestramento che non richiedano una nuova approvazione. Tali modifiche andrebbero comunque segnalate all'AISI.

La segnalazione e il monitoraggio richiesti per i cicli di addestramento tra 1e22 e 1e24 FLOP consentirebbero di ottenere alcuni dei benefici derivanti dall'addestramento di IA con modalità che al momento appaiono sicure, prevenendo al contempo la costruzione di IA più grandi e potenzialmente pericolose.

Il monitoraggio da parte dell'AISI le permetterebbe inoltre di rimanere (in una certa misura) aggiornata sui progressi algoritmici che si verificano nonostante i divieti; inoltre, l'Articolo XIII prevede che l'AISI effettui valutazioni dei modelli addestrati in questa fascia. Ciò aiuterà a rendere l'AISI consapevole delle tendenze nello sviluppo dell'IA e le consentirà (si spera) di abbassare le soglie di FLOP se necessario.

Il monitoraggio dei cicli di addestramento da parte dell'AISI è fattibile e applicabile grazie al consolidamento dei chip (Articolo V) e alla verifica del loro uso (Articolo VII). Il consolidamento dei chip implica che l'hardware utilizzato per i cicli di addestramento su larga scala sarà accessibile all'AISI. La verifica dell'uso dei chip permetterà all'AISI di avere una ragionevole certezza che i chip vengano utilizzati per addestrare IA solo previa autorizzazione.

Al personale dell'AISI è permesso l'accesso ai dati di addestramento utilizzati nelle sessioni di addestramento monitorate, fatte salve diverse restrizioni. L'intento di queste restrizioni è di garantire che la registrazione e altri metodi di controllo siano utilizzati per prevenire la divulgazione non autorizzata di contenuti sensibili nei dati di addestramento, inclusi, a titolo esemplificativo ma non esaustivo, informazioni di identificazione personale, informazioni sanitarie personali, dati classificati, segreti commerciali, dati bancari soggetti a obbligo di segretezza, e così via. 

---

#### Precedenti

Sebbene i valori numerici delle soglie specificate nel nostro accordo possano e debbano essere riesaminati una volta superata la fase iniziale di stesura, i tetti quantitativi sono comuni negli accordi internazionali, in quanto prevengono controversie che altrimenti dipenderebbero da diverse interpretazioni del linguaggio qualitativo.

Il [Trattato sulla limitazione degli esperimenti nucleari sotterranei (Threshold Test Ban Treaty)](https://2009-2017.state.gov/t/isn/5204.htm) del 1974 stabilì un tetto di 150 chilotoni per i test nucleari sotterranei eseguiti da Stati Uniti e URSS.[^245] Lo scopo e l'effetto del trattato furono di ostacolare, almeno in parte, l'ulteriore sviluppo di testate più grandi e distruttive del tipo "city buster". Un parallelo pertinente con lo sviluppo dell'IA è che, a metà del 2025, i modelli più generici e potenti (e quindi più pericolosi) richiedono per la loro creazione sessioni di addestramento corrispondentemente più grandi; il nostro trattato specifica dei tetti volti a impedire che tali IA vengano sviluppate intenzionalmente, ma anche a cogliere il vantaggio essenziale (sebbene non parallelo) di ridurre il rischio che una soglia di capacità imprevista venga superata accidentalmente e irrimediabilmente.

Il limite di addestramento che abbiamo suggerito come punto di partenza è abbastanza basso da essere superato da alcuni modelli di IA addestrati al giorno d'oggi; lo consideriamo prudente in previsione di progressi che renderanno i modelli più recenti più potenti a parità di addestramento (come discusso nell'Articolo VIII). Gli accordi per la riduzione degli armamenti costituiscono un precedente per soglie fissate al di sotto del livello massimo attuale. Il [Trattato navale di Washington](https://www.digitalhistory.uh.edu/disp_textbook.cfm?smtID=3&psid=3995) del 1922 fissò limiti al dislocamento delle navi da guerra, che costrinsero gli Stati Uniti e altre potenze navali a rottamare decine di *capital ship*.[^246] Nell'Articolo II del trattato START del 1991,[^247] gli Stati Uniti e l'Unione Sovietica (e in seguito la Federazione Russa) concordarono limiti alle dimensioni dei loro arsenali nucleari e dei loro sistemi di lancio che richiesero a ciascuna parte di eliminare gradualmente [più di quattromila testate ciascuno](https://bush41library.tamu.edu/archives/public-papers/3263).

I precedenti per le soglie quantitative che limitano il potenziale di fuga verranno discussi nell'Articolo V.

---

### Articolo V: Consolidamento dei chip {#articolo-v:-consolidamento-dei-chip}

1. Ciascuna Parte garantisce che, all'interno della propria giurisdizione, tutti i cluster di chip coperti (CCC), come definiti nell'Articolo II (ossia un insieme di chip con una capacità superiore a 16 equivalenti-H100) \[nota: 16 H100 hanno un costo complessivo di circa 430.000 euro nel 2025 e raramente sono di proprietà di privati\], siano ubicati in strutture dichiarate all'AISI e che tali chip per l'IA siano soggetti al monitoraggio da parte dell'AISI.  
   1. Le parti cercheranno di evitare di co-locare chip per l'AI e hardware non-IA non ausiliario in queste strutture dichiarate.  
   2. Queste strutture devono essere accessibili per ispezioni fisiche. Ciò può includere, ad esempio, che i team di verifica possano raggiungere qualsiasi CCC da almeno un aeroporto con servizio internazionale di linea entro 12 ore.  
   3. Le parti non devono collocare i chip per l'AI in così tante sedi diverse da rendere impossibile per l'AISI monitorarle tutte. Se richiesto dall'AISI, le parti devono consolidare ulteriormente i propri chip per l'IA in un numero inferiore di strutture monitorate.  
2. I chip perv l'IA non monitorati che non fanno parte di un CCC (cioè che hanno una capacità inferiore a 16 equivalenti-H100) possono rimanere al di fuori delle strutture dichiarate dall'AISI, a condizione che tali scorte non siano aggregate o collegate in rete per soddisfare la definizione di CCC, non siano ruotate tra i siti per aggirare il monitoraggio e non siano utilizzate per attività di addestramento vietate. Le Parti compiranno sforzi ragionevoli per monitorare la vendita e l'aggregazione dei chip per l'IA per garantire che qualsiasi CCC di nuova creazione venga individuato e monitorato.  
3. Entro 120 giorni dall'entrata in vigore del trattato, ogni parte individuerà, inventarierà e consoliderà tutti i CCC in strutture dichiarate all'AISI. Le parti non disgregheranno, nasconderanno o riassegneranno in altro modo i chip per eludere questo requisito o per far sì che un insieme di chip altrimenti classificabile come CCC non venga più classificato come tale.  
4. L'AISI supervisionerà il processo di consolidamento nazionale, anche attraverso ispezioni in loco, verifica documentale e inventariale, affiancamento delle autorità nazionali durante i trasferimenti e le ispezioni, e condivisione di informazioni con le Parti ai sensi dell'Articolo X. L'AISI potrà richiedere la documentazione della catena di custodia per i trasferimenti e condurre ispezioni su richiesta, come descritto nell'Articolo X. Le Parti garantiranno un accesso tempestivo alle strutture, agli snodi di trasporto e ai registri pertinenti. Le tutele e gli incentivi per gli informatori previsti dall'Articolo X si applicano al processo di consolidamento e l'AISI manterrà canali di segnalazione protetti.  
5. Entro 120 giorni dall'entrata in vigore del Trattato, le Parti presenteranno all'AISI un registro dei loro CCC. Il registro dovrà includere l'ubicazione, il tipo, la quantità, i numeri di serie o altri identificatori univoci, ove disponibili, e le interconnessioni associate di tutti i chip per l'IA presenti nei CCC. Ciascuna Parte fornirà all'AISI un registro aggiornato e accurato ogni 90 giorni al più tardi.  
6. Le Parti notificheranno in anticipo all'AISI qualsiasi trasferimento pianificato di chip per l'IA, sia nazionale che internazionale, con un preavviso di almeno 14 giorni rispetto alla data prevista per il trasferimento. Nessun trasferimento potrà avere luogo senza che all'AISI sia data l'opportunità di supervisionarlo. Per i trasferimenti internazionali, sia la Parte mittente che quella ricevente dovranno coordinarsi con l'AISI per quanto riguarda il tragitto, la custodia e la ricezione. I trasferimenti di emergenza effettuati per ragioni di sicurezza dovranno essere notificati non appena possibile e saranno soggetti a verifica a posteriori.  
7. I chip per l'IA rotti, difettosi, in eccedenza o altrimenti dismessi continueranno a essere trattati come chip funzionanti, finché l'AISI non ne certificherà la distruzione. Le Parti non distruggeranno i chip per l'IA senza la supervisione dell'AISI. La distruzione o la messa fuori uso permanente dovranno essere condotte sotto la supervisione dell'AISI, utilizzando metodi da essa approvati, e registrate in un certificato di distruzione [i dettagli dovranno essere esplicitati in un Allegato]. Il recupero o la rivendita di componenti di tale hardware sono vietati, salvo espressa autorizzazione dell'AISI.

---

#### Note

Discuteremo quale obiettivo questo Articolo si prefigge di raggiungere, perché riteniamo che tale obiettivo sia importante, perché lo riteniamo fattibile, perché è stato scelto il limite di 16 H100 e varie altre considerazioni.

##### Obiettivo dell'Articolo V

Questo Articolo mira a centralizzare, in strutture monitorate, tutti i cluster di chip per l'IA (ossia insiemi di chip interconnessi al di sopra di una certa piccola dimensione) e la stragrande maggioranza dei chip per l'IA.

Una volta che i chip sono stati centralizzati in strutture monitorate, l'AISI può adottare numerosi altri approcci per garantire che non vengano utilizzati in violazione dell'Articolo IV; tali approcci sono discussi nell'Articolo VII.

È auspicabile una verifica internazionale di questo processo di centralizzazione, affinché tutte le Parti abbiano la certezza che anche tutti gli altri abbiano centralizzato i propri chip. Questo tipo di verifica può essere effettuato con relativa facilità per i grandi data center di IA, dato che le agenzie di intelligence probabilmente sanno già dove si trovano. Per i data center più piccoli, l'AISI può supervisionare i processi di centralizzazione nazionali come misura di rafforzamento della fiducia.

L'Articolo prevede in effetti un'eccezione per un numero ridotto di chip: meno di 16 equivalenti-H100. Questa quantità di chip è talmente ridotta che è improbabile che i chip non monitorati rappresentino una minaccia (in assenza di progressi nella ricerca). Soglie più basse sarebbero più difficili da radunare con successo e potrebbero iniziare a costituire costi ingenti per un'ampia fascia della popolazione. (Diverse persone possiedono una manciata di GPU a casa propria, ma poche possiedono 16 equivalenti-H100).

##### Perché l'Articolo V esiste

La centralizzazione dei chip è un'operazione utile perché apre altre strade per controllare lo sviluppo dell'intelligenza artificiale tramite i chip.

Per esempio, la centralizzazione dei chip in strutture dichiarate implica che questi potrebbero in seguito essere soggetti a ulteriore monitoraggio sul loro utilizzo (Articolo VII) o a verifiche per assicurarsi che siano spenti.

La centralizzazione dei chip in strutture dichiarate renderebbe anche più facile per le Parti distruggerli, come potrebbe rendersi necessario ai sensi dell'Articolo XII, qualora una Parte violi continuamente il Trattato.

Data la possibilità che i CCC vengano distrutti, sarebbe auspicabile costruirli lontano dai centri abitati, ove possibile. Questo punto non è incluso nel testo dell'Articolo per problemi di fattibilità (i data center attuali sono spesso vicini alle città, quindi sarebbe necessario costruirne di nuovi); inoltre, in casi estremi, i data center possono probabilmente essere disattivati senza ingenti danni collaterali e, infine, questo non è un aspetto portante del Trattato. Tuttavia, dato il loro enorme potenziale di pericolo, sembra appropriato trattare i data center di IA come strutture militari.

##### Verifica

Un elemento chiave per il successo di un trattato sull'IA è la verifica. I Paesi non si fideranno l'uno dell'altro riguardo al rispetto delle regole, ma vorranno poterlo verificare concretamente. La centralizzazione dei chip per l'IA in strutture dichiarate dovrà essere confermata dalle ispezioni e dal monitoraggio di questo processo da parte dell'AISI. Altrimenti, i Paesi non otterranno sufficiente fiducia che gli altri abbiano effettivamente centralizzato i loro chip.

La centralizzazione dei chip potrebbe non essere necessaria se esistessero altri modi per monitorarli. Purtroppo, riteniamo che questa sia attualmente l'unica opzione fattibile, se si esclude la distruzione fisica di tutte le scorte di chip per l'IA, considerate le potenzialità dei meccanismi di sicurezza disponibili sui chip attuali.

In futuro, potrebbero essere sviluppati [meccanismi di governance basati sull'hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) per consentire la governance remota dei chip per l'IA, in modo che non sia necessario centralizzarli in luoghi dichiarati. [Aarne et al. (2024)](https://www.iaps.ai/research/secure-governable-chips) forniscono stime sui tempi di implementazione di alcuni di questi meccanismi di governance su chip. Le loro stime coprono i tempi necessari per sviluppare meccanismi robusti contro diversi avversari. Per brevità, useremo le loro stime per la sicurezza in un contesto velatamente avversariale, poiché pensiamo che questo corrisponda alla situazione che intendiamo affrontare: attori statali competenti potrebbero provare a violare i meccanismi di governance, ma ci sarebbero conseguenze importanti se questa sovversione venisse scoperta. Stimano un tempo di sviluppo di 2-5 anni per soluzioni ideali, con opzioni meno sicure ma potenzialmente funzionanti disponibili in pochi mesi.

Anche se quel rapporto risale a più di un anno fa, non siamo a conoscenza di progressi significativi verso questi meccanismi, e riteniamo che quella di 2-5 anni aggiuntivi sia la stima più pertinente di Aarne et al.

Oltre allo sviluppo, i meccanismi di governance su chip devono essere o aggiunti ai nuovi chip (che a loro volta dovranno diffondersi nel parco chip esistente) oppure adattati a quelli già in uso. Aarne et al. stimano che la prima di queste opzioni potrebbe richiedere quattro anni, ma siamo ottimisti sul fatto che l'adattamento potrebbe essere completato in uno o due anni se i chip sono già tracciati.

Per essere chiari, la centralizzazione di cui all'Articolo V comporta la concentrazione fisica e il monitoraggio (nell'Articolo VII) dei cluster di chip coperti, ma non richiede che i governi ne assumano la proprietà. Per i grandi data center, il trattato permette che il data center e i suoi chip rimangano nello stesso posto, pur restando di proprietà delle aziende, a patto che ricevano monitoraggio e supervisione dal governo nazionale e dall'AISI per garantire che i data center siano impiegati solo in attività non legate all'IA o in attività come l'esecuzione di vecchi modelli, piuttosto che nella creazione di nuovi modelli di IA più capaci. Per un numero minore di chip, potrebbe essere necessario spostarli fisicamente in un data center più grande, ma il proprietario potrebbe continuare ad accedervi da remoto, in modo simile ad alcuni modelli di cloud computing esistenti. In alternativa, i chip potrebbero essere trasferiti alla proprietà del governo in cambio di un giusto compenso.

##### Fattibilità

Probabilmente è fattibile individuare la maggior parte dei chip per l'IA (e verificarne l'individuazione a livello internazionale). Per i data center di IA più grandi, come quelli con più di 100 000 equivalenti-H100, la verifica internazionale sembra relativamente semplice; questi data center sono difficili da nascondere e i servizi di intelligence probabilmente sanno già dove si trovano. (A metà del 2025, non ci aspettiamo che ci siano ancora stati tentativi di nascondere questi data center.) Questi data center sono individuabili dalla loro impronta fisica e dal consumo energetico, e molti di essi sono [pubblicamente segnalati](https://epoch.ai/data/ai-supercomputers).

Tali metodi consentiranno probabilmente di individuare anche data center di dimensioni ridotte, pari a circa 10 000 equivalenti-H100. Al di fuori dei MTN e dei servizi di intelligence delle Parti, il piano proposto per la centralizzazione dei chip prevede che le autorità nazionali utilizzino vari poteri per eseguirla e consentano l'ispezione del processo da parte dell'AISI per evitare che alcuni data center vengano esclusi.

Gli Stati avranno a disposizione una serie di strumenti per rintracciare i chip di proprietà nazionale. Potranno richiedere legalmente che tutti i cluster di chip superiori a 16 H100 vengano segnalati; potranno utilizzare i registri di vendita e altre informazioni finanziarie dei distributori di chip; potranno intervistare tecnici esperti nella costruzione di data center; ecc. Qualora sospettino attività di contrabbando o che i chip vengano nascosti, potranno ricorrere alle forze dell'ordine per ulteriori indagini. Questo processo di centralizzazione nazionale sarà supervisionato dagli ispettori dell'AISI per garantire che sia eseguito correttamente.

La localizzazione dei data center esistenti potrebbe probabilmente essere effettuata rapidamente (in pochi giorni o settimane per i cluster superiori a 1.000 equivalenti-H100) utilizzando i metodi descritti.

In realtà, la *centralizzazione* dei chip potrebbe richiedere più tempo, poiché potrebbe essere necessario aumentare la capacità dei data center nelle strutture che diventerebbero CCC. Non è chiaro quanto tempo ci vorrebbe, né se sarebbe fattibile in alcuni paesi, individuare tutti i cluster di, ad esempio, 100 H100. Una tale quantità di chip sarebbe vietata, ma tale divieto potrebbe essere difficile da applicare.

Una delle maggiori sfide alla fattibilità della centralizzazione dei cluster di chip consiste nel fornire agli altri Paesi la ragionevole certezza che una Parte non stia sviluppando un progetto segreto di IA con chip non dichiarati. La verifica internazionale della centralizzazione nazionale dei chip potrebbe essere utile, ma potrebbe non bastare, dato che gli sforzi di centralizzazione nazionale potrebbero essere volutamente non esaustivi.

I vincoli imposti da questo Articolo contribuiscono in modo significativo a fornire alle Parti garanzie contro il rischio di progetti segreti di IA sostenuti da governi e operanti in altri Paesi. Oltre alla supervisione dell'AISI sul processo di centralizzazione nazionale, la raccolta di informazioni e le ispezioni su richiesta trattate nell'Articolo X potrebbero essere di grande aiuto.

##### Le ragioni della definizione di CCC

Questo Articolo e la corrispondente definizione di CCC stabiliscono un limite di 16 equivalenti-H100. Tale soglia mira a soddisfare alcuni criteri:

* Il monitoraggio dei cluster di chip con potenza superiore a 16 H100 è compatibile con le soglie di FLOP per l'addestramento di cui all'Articolo IV. L'addestramento con 16 H100 (precisione FP8, utilizzo al 50% — parametri realistici ma ottimistici) richiederebbe 7,3 giorni per raggiungere 10^22 FLOP e 2 anni per raggiungere 10^24 FLOP. Pertanto, sarebbe fattibile utilizzare chip non dichiarati per raggiungere la soglia più bassa, ma sarebbe poco praticabile raggiungere la soglia di addestramento proibita.  
* Questa soglia è verosimilmente sufficiente a impedire l'avanzamento delle capacità dell'IA, se combinata con i divieti alla ricerca sull'IA previsti dall'Articolo VIII. L'Articolo IV stabilisce restrizioni all'addestramento, in base alle quali l'addestramento su larga scala è vietato e quello su media scala è consentito ma soggetto a supervisione. È probabilmente accettabile (ossia, comporta un rischio minimo) consentire l'addestramento su piccola scala, come quello che si può effettuare con 16 H100 in un lasso di tempo realistico.  
* Questa soglia ha effetti collaterali limitati per gli hobbisti e le persone comuni. Pochissimi individui possiedono più di 16 H100. A metà del 2025, 16 chip H100 costano circa 430.000 euro; non sono un bene di consumo comune. Nessuno supererà per errore la soglia solo perché possiede qualche vecchia console da gaming.  
* Il censimento dei chip per l'IA diventa più difficile per numeri di chip consentiti sempre più piccoli. Trovare data center con 100.000 chip è facile; anche con 10.000 chip, probabilmente, è relativamente facile; con 1.000 non è chiaro; e con 100, potrebbe essere piuttosto difficile. Questa soglia è stata scelta in parte a causa dell'impossibilità di imporne una più bassa: persino una soglia di 16 H100 potrebbe essere difficile da far rispettare.  
* È possibile che si debba rivedere questa definizione e abbassare la soglia (ad esempio, a 8 equivalenti-H100). In questo trattato, l'AISI avrebbe il compito di valutare questa definizione e di modificarla secondo necessità.

##### Altre considerazioni

Questo Articolo invita le parti a cercare di evitare la co-locazione di chip per l'IA con chip non-IA non ausiliari. Questo perché la co-locazione potrebbe rendere più difficile la verifica dell'uso dei chip (Articolo VII) e richiederebbe che anche questi chip non-IA fossero monitorati per implementare efficacemente la verifica dell'uso dei chip per l'IA. Tuttavia, questo non è strettamente necessario e potrebbe non essere auspicabile. Ad esempio, attualmente i chip per l'IA sono spesso co-locati con chip non-IA e il disagio di cambiare questa situazione potrebbe superare il disagio di monitorare anche i chip non-IA.

In linea con gli accordi precedenti, questo Articolo richiede che i CCC siano rapidamente accessibili agli ispettori per la verifica. In questo caso, ci sarà probabilmente un monitoraggio continuo di molte di queste strutture (Articolo VII) e l'accesso agli aeroporti potrebbe essere vantaggioso.

C'è il rischio che privati cittadini possano costruire un CCC non monitorato con chip "sfusi" equivalenti-H100. Per contrastare questo rischio, il trattato prevede che le Parti compiano "ogni ragionevole sforzo" per monitorare le vendite di chip (in quantità superiori a 1 equivalente-H100) e individuare la formazione di nuovi CCC. Potrebbero essere adottate misure più rigorose, come l'obbligo di registrare e tracciare formalmente tutti questi chip e le relative vendite. La nostra bozza non si spinge a tal punto, sia perché non prevediamo che, dopo la catalogazione di tutti i chip nei CCC, rimangano molti chip "sfusi" equivalenti a H100 non contabilizzati, sia perché altri meccanismi (come la protezione degli informatori) contribuiscono a individuare i CCC di nuova formazione.

##### Alternative

Invece di richiedere immediatamente la centralizzazione dei cluster di piccole dimensioni (ad esempio 100 H100), il trattato potrebbe adottare un approccio graduale. Ad esempio, nei primi 10 giorni tutti i data center con più di 100.000 chip equivalenti-H100 devono essere centralizzati e dichiarati, poi nei 30 giorni successivi tutti i data center con più di 10-000 chip equivalenti-H100 devono essere centralizzati e dichiarati, ecc.

Questo approccio graduale potrebbe allinearsi meglio con la verificabilità internazionale del rilevamento di questi cluster, man mano che i servizi di intelligence intensificano i loro sforzi. In altre parole: è probabile che all'inizio i servizi di intelligence sappiano dove si trovano i data center più grandi, ma non quelli di medie dimensioni (se non li stavano già cercando). Di conseguenza, inizialmente verrebbero dichiarati solo i data center di grandi dimensioni e poi, con il proseguire della ricerca dei chip da parte dei servizi di intelligence, la soglia si abbasserebbe.

Questo approccio potrebbe rispecchiare meglio il modo in cui la verificabilità e l'applicabilità hanno influito su ciò che è stato concordato nei precedenti accordi internazionali. Ad esempio, il Trattato sulla messa al bando parziale degli esperimenti nucleari del 1963 non vietò i test sotterranei di armi nucleari, a causa della difficoltà di rilevarli. Il team di governance tecnica del MIRI ha in programma di pubblicare un rapporto con un approccio graduale di questo tipo.

Uno svantaggio di un approccio graduale è che potrebbe offrire agli Stati maggiori opportunità per nascondere i chip e stabilire data center segreti.

---

#### Precedenti

La dichiarazione delle risorse di interesse è spesso il primo passo nei trattati restrittivi. Le Parti del Trattato navale di Washington del 1922 fornirono gli inventari delle navi da guerra e la loro dimensione, e si impegnarono a comunicarsi reciprocamente la sostituzione di queste navi. Il trattato START I del 1991 includeva un accordo riservato sullo scambio di coordinate e schematiche dei siti (nell'[Articolo VIII](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art8)), che prevedeva la condivisione dei dati sulla posizione di tutte le armi strategiche dichiarate. L'Articolo V, Paragrafo 3, della nostra bozza di accordo richiede alle Parti di individuare, inventariare e consolidare i cluster di chip contemplati entro 120 giorni.

Il consolidamento delle risorse per facilitare la verifica della conformità è spesso un altro passo nei trattati restrittivi. L'[Articolo III](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art3) dello START I vietava la collocazione congiunta di missili balistici intercontinentali (MBIC) con gli impianti di lancio spaziale, facilitando così il monitoraggio. Il Paragrafo 1.a del nostro Articolo V impegna le Parti a «evitare di co-locare chip per l'AI e hardware non-IA non ausiliario» per lo stesso motivo.

La storia dimostra che il consolidamento limita anche il potenziale di rottura degli accordi (*breakout*), rendendo più facile colpire le concentrazioni di beni problematici in caso di crisi di fiducia. Nel JCPOA del 2016[^248] (noto anche come accordo nucleare con l'Iran), l'Iran ha accettato di mantenere operative le sue centrifughe per l'arricchimento dell'uranio in soli due siti designati (Natanz e Fordow), entrambi [colpiti](https://www.iaea.org/newscenter/pressreleases/update-on-developments-in-iran-5) nel giugno 2025 da operazioni di Israele e Stati Uniti. Ciò motiva una nota che accompagna il nostro Articolo V, in cui suggeriamo alle Parti di localizzare i loro cluster di chip coperti (CCC) lontano dai centri abitati.

Il monitoraggio e le ispezioni sono componenti comuni dei trattati precedenti in contesti di fiducia limitata; abbiamo quindi redatto, ove opportuno, apposite disposizioni nei Paragrafi 1, 4, 6 e 7 di questo Articolo. Alcuni precedenti specifici: 
* La verifica dello START I ha incluso [centinaia di ispezioni in loco](https://www.armscontrol.org/factsheets/start-i-glance) nei primi anni.  
* La CAC richiede la dichiarazione e l'ispezione di tutti gli impianti di produzione di armi chimiche (ne sono stati [dichiarati 97](https://www.opcw.org/media-centre/opcw-numbers)) e la maggior parte di questi è stata verificabilmente distrutta. (Nel richiedere la dichiarazione degli impianti esistenti, questi accordi proibiscono anche lo svolgimento di determinate attività al di fuori degli impianti dichiarati, analogamente al divieto, previsto da questo Articolo, per i CCC non monitorati).  
* Oltre [700 impianti nucleari dichiarati](https://www.armscontrol.org/factsheets/iaea-safeguards-agreements-glance) in tutto il mondo sono monitorati dall'AIEA nell'ambito del TNP.

Analogamente a quanto previsto dal Paragrafo 3 di questo Articolo, numerosi accordi sul controllo degli armamenti richiedono che le Parti non interferiscano con i rispettivi MTN nel contesto della verifica del trattato. Ne sono esempi il SALT I,[^249] l'ABM,[^250] l'INF[^251] e lo START I.

Un precedente in cui le Parti limitano le industrie del proprio settore privato nazionale per adempiere agli impegni del trattato (come sarebbe necessario nel caso dell'IA) è riscontrabile nella legislazione statunitense successiva alla ratifica della CAC: il [Chemical Weapons Convention Implementation Act del 1998](https://www.congress.gov/105/plaws/publ277/PLAW-105publ277.pdf#page=857) e i regolamenti del Dipartimento del Commercio hanno assicurato la conformità delle entità statunitensi. Allo stesso modo, il Congresso degli Stati Uniti ha [emendato](https://www.epa.gov/ozone-layer-protection/ozone-protection-under-title-vi-clean-air-act) il Clean Air Act a seguito della ratifica del Protocollo di Montreal per vietare le sostanze che riducono lo strato di ozono.

Negli Stati Uniti, gli approcci all'implementazione della centralizzazione dei chip potrebbero basarsi sulla [Takings Clause](https://constitution.congress.gov/browse/essay/amdt5-9-1/ALDE_00013280/) del Quinto Emendamento, in base alla quale il governo può usare il suo potere di espropriazione per pubblica utilità per acquisire proprietà privata, a condizione che paghi un adeguato indennizzo.

---

### Articolo VI: Monitoraggio della produzione di chip per l'IA {#articolo-vi:-monitoraggio-della-produzione-di-chip-per-l-ia}

1. L'AISI implementerà un monitoraggio degli impianti di produzione di chip per l'IA e degli input chiave per la loro produzione. Tale monitoraggio garantirà che tutti i chip per l'IA di nuova produzione siano immediatamente tracciati e monitorati fino alla loro installazione nei CCC dichiarati e che non vengano create catene di approvvigionamento non monitorate.  
   1. L'AISI monitorerà gli impianti di produzione di chip per l'IA che è stato determinato producano, o potrebbero produrre, chip per l'IA e hardware rilevante [le definizioni precise di impianti di produzione di chip per l'IA, chip per l'IA e hardware rilevante andrebbero ulteriormente dettagliate in un Allegato; anche i metodi di monitoraggio andrebbero descritti in un Allegato].  
   2. Il monitoraggio dei chip per l'IA di nuova produzione includerà il monitoraggio della produzione, della vendita, del trasferimento e dell'installazione. Il monitoraggio della produzione dei chip inizierà con la fabbricazione. La serie completa di attività include la fabbricazione di memorie ad alta larghezza di banda (ALB), la fabbricazione di chip logici, il collaudo, l'imballaggio e l'assemblaggio [questa serie di attività andrebbe specificata in un Allegato].  
2. Negli impianti in cui il tracciamento e il monitoraggio da parte dell'AISI non siano fattibili o implementati, la produzione di chip per l'IA sarà interrotta. La produzione di chip per l'IA potrà riprendere quando l'AISI dichiarerà che sono state implementate misure di tracciamento e monitoraggio accettabili.  
3. Se un impianto di produzione di chip monitorato viene dismesso o riutilizzato, l'AISI supervisionerà tale processo e, se condotto in modo soddisfacente per l'AISI, l'obbligo di monitoraggio cesserà.  
4. Nessuna Parte potrà vendere o trasferire chip per l'IA o attrezzature per la produzione di chip per l'IA, se non secondo le modalità autorizzate e tracciate dall'AISI.  
   1. La vendita o il trasferimento di chip per l'IA all'interno delle Parti del Trattato o tra di esse godrà della presunzione di approvazione e sarà tracciato dall'AISI.  
   2. La vendita o il trasferimento di attrezzature per la produzione di chip per l'IA all'interno delle Parti del Trattato o tra di esse non godrà della presunzione di approvazione. L'approvazione di tale trasferimento si baserà su una valutazione del rischio di sviamento o di recesso dal Trattato da parte della Parte ricevente.  
   3. La vendita o il trasferimento di chip per l'IA e di attrezzature per la produzione di chip per l'IA a Stati non firmatari o a entità al di fuori di uno Stato firmatario sono soggetti a presunzione di diniego.  
5. A nessuna Parte è consentito vendere o trasferire chip avanzati non-IA o attrezzature per la produzione di chip avanzati non-IA a Stati non firmatari o entità al di fuori di uno Stato firmatario, salvo autorizzazione e tracciamento da parte dell'AISI.  
6. La vendita o il trasferimento di chip avanzati non-IA o di attrezzature per la produzione di chip avanzati non-IA all'interno o tra le Parti del Trattato non sono soggetti a restrizioni ai sensi del presente Articolo.

---

#### Note

La [catena di approvvigionamento dei chip per l'IA](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/) è ristretta e specializzata, per cui monitorarne la produzione è fattibile. La [stragrande maggioranza](https://www.datacenterdynamics.com/en/news/nvidia-gpu-shipments-totaled-376m-in-2023-equating-to-a-98-market-share-report/) dei chip per l'IA è progettata da NVIDIA. I chip logici più avanzati (il processore principale) utilizzati nei chip per l'IA sono quasi tutti fabbricati da TSMC, che detiene [circa il 90%](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) della quota di mercato. [La maggior parte dei chip per l'IA](https://epoch.ai/data/machine-learning-hardware?view=table) sono prodotti su versioni del nodo di elaborazione da cinque nanometri di TSMC, un nodo che probabilmente è supportato solo da [due o tre stabilimenti di produzione](https://www.blackridgeresearch.com/blog/list-of-tsmc-fabs-in-taiwan-arizona-kumamoto). Le macchine per litografia EUV, un componente fondamentale nella produzione di chip logici avanzati, sono prodotte [esclusivamente](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) da ASML. La memoria ad alta larghezza di banda (ALB), un altro componente chiave per i chip per l'IA, è dominata da [due o tre](https://www.trendforce.com/news/2024/04/24/news-amid-foundry-overcapacity-competition-for-hbm-intensifies-rapidly/) aziende. Questa catena di approvvigionamento ristretta sarebbe relativamente facile da monitorare se ci fosse la volontà di farlo.

Monitorare la produzione di chip per l'IA avrebbe ricadute relativamente limitate. Anche se alcuni dei processi utilizzati per produrre per l'IA sono gli stessi utilizzati per altri chip (ad esempio, quelli per smartphone), i chip stessi sono diversi e potrebbero essere differenziati tramite il monitoraggio. Il design dei chip cambierebbe nel tempo, ma, allo stato attuale, i chip per l'IA odierni sarebbero probabilmente identificabili grazie alla loro grande capacità ALB, ai componenti specializzati per la moltiplicazione di matrici, alla capacità di *scale-out* e all'interconnessione.

La ricerca futura dovrebbe concentrarsi sull'identificazione delle parti principali della catena di approvvigionamento dei chip per l'IA che sarebbero più adatte al monitoraggio. Sulla base degli [attuali colli di bottiglia](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/), un'ipotesi iniziale è quella di concentrare il monitoraggio sulla produzione di ALB, sulla fabbricazione di *die* logici e sulle fasi successive (ad esempio confezionamento, collaudo, assemblaggio dei server), insieme al monitoraggio di input chiave come le macchine per litografia EUV.

Questo Articolo nota che la vendita di chip all'interno degli Stati firmatari avrà una presunzione di approvazione, ma non indica la stessa presunzione per le attrezzature per la produzione di chip per l'IA. Potrebbe essere accettabile consentire la vendita diffusa di attrezzature per la produzione di chip all'interno degli Stati firmatari, ma tale questione dovrebbe probabilmente essere trattata con maggiore cautela rispetto alla vendita dei chip stessi. La vendita dei chip avrà probabilmente un effetto a breve termine, dato che il ciclo di vita dei chip per l'IA è in genere [di pochi anni](https://epoch.ai/data-insights/gpu-frontier-lifespan#:~:text=For%20these%20newer%20chip%20designs,from%200.6%20to%203.7%20years.). Al contrario, la capacità di produzione di chip potrebbe portare a una produzione significativa di chip per molti anni a venire, e sarebbe particolarmente preoccupante se i Paesi con una produzione nazionale matura di chip per l'IA si ritirassero dal Trattato. Pertanto, suggeriamo un approccio più cauto alle attrezzature per la produzione di chip rispetto ai chip stessi.

I paragrafi 4 e 5 di questo Articolo permettono la vendita di chip per l'IA e di attrezzature per la produzione di chip alle Parti del Trattato, ma non a non-Parti o ad altre entità esterne alle Parti. In altre parole, le Parti accettano i rischi derivanti dalla produzione e dalla concentrazione dei chip, ma solo nei casi in cui i chip siano sottoposti a monitoraggio. La possibilità di produrre e possedere chip senza una risposta protettiva da parte di altri Stati emerge quindi come un incentivo positivo ad aderire al Trattato.

Di per sé, questo non impedisce agli Stati non Parte di accedere da remoto ai chip per l'IA presenti negli Stati Parte (ad esempio, tramite cloud computing o *[Infrastructure-as-a-Service](https://www.governance.ai/research-paper/governing-through-the-cloud)*), ma tali chip sarebbero soggetti al monitoraggio dell'AISI per garantire che non vengano utilizzati in violazione dell'Articolo IV.

Le restrizioni per gli Stati non Parte potrebbero andare oltre, se necessario. Ad esempio, a tali Stati potrebbe essere vietato l'accesso da remoto ai chip per l'IA (ossia il noleggio di chip per l'IA negli Stati Parte tramite cloud) o l'accesso a modelli di IA tramite API.

Questo Articolo sfrutta il fatto che i chip sono un input altamente escludibile per lo sviluppo dell'IA. Propone di monitorare la produzione dei chip e di garantire che non vengano contrabbandati al di fuori degli Stati Parte o verso strutture non dichiarate. Un altro approccio sarebbe quello di vietare completamente la produzione di nuovi chip per l'IA. Questo approccio comporterebbe un minor rischio di diversione dei chip, ma avrebbe il costo di perdere il valore che questi chip potrebbero generare in applicazioni di IA non legate alla ricerca e allo sviluppo. Si baserebbe comunque su un certo monitoraggio degli impianti di produzione dei chip, ad esempio per garantire che producano solo chip non-IA o che vengano dismessi. L’attuale impostazione del Trattato invita la produzione di chip a continuare, dati i grandi vantaggi che il mondo trarrebbe dal loroo utilizzo. Tuttavia, per permettere che la produzione di chip continui in sicurezza, sarebbe necessario monitorare la catena di approvvigionamento e l'utilizzo dei chip (Articolo VII). Riteniamo che entrambe queste attività siano fattibili, ma se una delle due non lo fosse, l'alternativa consisterebbe nell'interrompere del tutto la produzione di chip. 

---

#### Precedenti

Le disposizioni dei trattati per il monitoraggio degli impianti di produzione non sono una novità. L'Articolo XI del Trattato INF del 1987 ha consentito per tredici anni ispezioni negli impianti designati dove in precedenza venivano prodotti sistemi di lancio nucleari a raggio intermedio; la Sezione VII del relativo [protocollo di ispezione](https://2009-2017.state.gov/t/avc/trty/102360.htm#inspections) permetteva il monitoraggio continuo del perimetro e dei varchi, il che poteva includere la pesatura (e in alcuni casi l'esame ai raggi X) di qualsiasi veicolo in uscita dall'impianto abbastanza grande da trasportare un missile pertinente.

Il monitoraggio della produzione di chip per l'IA è più complicato, a causa della difficoltà di discernere la funzione e le potenzialità di un chip dalle sue caratteristiche esteriori; per questo motivo, il nostro Articolo VI stabilisce che "l'hardware rilevante dovrebbe essere descritto più dettagliatamente in un Allegato", insieme ai metodi di monitoraggio. Ma l'esperienza delle salvaguardie dell'AIEA nell'ambito del TNP dimostra che la verifica di un'ampia gamma di componenti di produzione e di precursori lungo l'intera catena di approvvigionamento è possibile. A tal fine, l'AIEA [fornisce](https://www-pub.iaea.org/MTCD/Publications/PDF/Pub1669_web.pdf) [delle linee guida](https://www.iaea.org/publications/13452/international-safeguards-in-the-design-of-enrichment-plants?utm_source=chatgpt.com) per la progettazione di impianti che agevolino le ispezioni e riducano i costi di adeguamento.

Gli embargo sul trasferimento di prodotti finiti, precursori e attrezzature di produzione (come quello qui suggerito per i chip per l'IA e le attrezzature avanzate per la produzione di chip per computer verso Stati non firmatari del trattato) hanno tutti precedenti significativi:

* Nell'Articolo I del [TNP](https://www.un.org/en/conf/npt/2005/npttreaty.html), ogni Stato dotato di armi nucleari si impegna a «non trasferire a chicchessia armi nucleari o altri congegni esplosivi nucleari». Nel Paragrafo 2 del suo Articolo III, si impegna altresì a non fornire «materiali grezzi o prodotti fissili speciali» o attrezzature «specialmente progettate o preparate per il trattamento, l'impiego o la produzione di prodotti fissili speciali».  
* L'Articolo I della [CAC](https://2009-2017.state.gov/t/avc/trty/127917.htm) impegna parimenti le parti a non «trasferire mai, direttamente o indirettamente, armi chimiche a nessuno»; il suo Articolo VII richiede loro di sottoporre i precursori elencati a specifici «divieti di produzione, acquisizione, detenzione, trasferimento e uso».  
* Il [Comitato di coordinamento per i controlli multilaterali sulle esportazioni](https://www.govinfo.gov/content/pkg/GPO-CRPT-105hrpt851/html/ch9bod.html#anchor5563742) (ComCo), istituito durante la Guerra Fredda, stabilì una serie coordinata di controlli sulle esportazioni dai Paesi del blocco occidentale a quelli del blocco comunista, riguardanti materiali nucleari, munizioni e prodotti industriali a duplice uso come i semiconduttori.  
* Il [Gruppo dei fornitori nucleari](https://www.nuclearsuppliersgroup.org/index.php/en/) è un regime multilaterale di controllo delle esportazioni che limita la fornitura di tecnologia nucleare e ad essa correlata che potrebbe essere deviata verso programmi di armi nucleari.  
* Particolarmente rilevante è la serie di [controlli sulle esportazioni](https://www.bis.gov/press-release/commerce-strengthens-export-controls-restrict-chinas-capability-produce-advanced-semiconductors-military) statunitensi che, negli ultimi due anni, si sono concentrati sui chip per l'IA e sulle attrezzature avanzate per la loro produzione, arrivando a interessare decine di Paesi. 

---

### Articolo VII: Verifica dell'impiego dei chip {#articolo-vii:-verifica-dell-impiego-dei-chip}

1. Le parti accettano la verifica continua in loco di tutto l'utilizzo dei chip da parte dell'AISI presso i CCC dichiarati. I metodi utilizzati per la verifica saranno determinati e aggiornati dal Segretariato Tecnico, in conformità con la procedura descritta nell'Articolo III. Il Consiglio Esecutivo può porre il veto su tali decisioni con un voto a maggioranza dei due terzi. Tali metodi possono includere, a titolo esemplificativo ma non esaustivo:  
   1. Ispettori in presenza  
   2. Telecamere a prova di manomissione  
   3. Misurazioni delle caratteristiche di alimentazione, termiche e di rete  
   4. Meccanismi hardware integrati nel chip, compresi quelli installati a posteriori  
   5. Dichiarazione dei carichi di lavoro e delle operazioni dei chip da parte dell'operatore CCC  
   6. Ri-esecuzione dei carichi di lavoro dichiarati presso una struttura AISI per confermare la veridicità delle dichiarazioni  
2. L'obiettivo di questa verifica sarà garantire che i chip non vengano utilizzati per attività vietate, come l'addestramento di IA su larga scala descritto nell'Articolo IV.  
3. Nei casi in cui l'AISI ritenga che gli attuali metodi di verifica non possano fornire garanzie sufficienti che l'hardware per l'IA non venga utilizzato per attività vietate, tale hardware deve essere spento e la sua inoperatività deve essere continuamente verificata da ispettori in loco o da altri meccanismi di verifica approvati dall'AISI.  
4. L'AISI può imporre varie restrizioni sul funzionamento dei chip al fine di garantire una verifica adeguata. Tali restrizioni possono includere, a titolo esemplificativo ma non esaustivo:  
   1. Restrizioni sulla larghezza di banda e sulla latenza tra chip diversi, o tra chip e la rete del loro data center, per distinguere l'inferenza consentita dall'addestramento vietato.  
   2. Restrizioni sul numero o sulla velocità di FLOP/s o sulla larghezza di banda della memoria a cui i chip possono operare, per distinguere l'inferenza consentita dall'addestramento vietato o da altri carichi di lavoro vietati.  
   3. Restrizioni sulla precisione numerica delle operazioni dei chip, per differenziare i carichi di lavoro per l'IA da quelli non-IA.  
5. L'AISI adotterà un approccio differenziato alla verifica dei diversi CCC, in base alla loro probabilità di essere utilizzati per attività legate all'IA e alla loro sensibilità in relazione alla sicurezza nazionale.  
6. L'AISI guiderà la ricerca e lo sviluppo di tecnologie migliori per il monitoraggio e la verifica dell'uso dei chip. Le parti sosterranno questi sforzi [maggiori dettagli saranno forniti in un Allegato].

---

#### Note

I Paesi vorranno assicurarsi che i chip per l'IA esistenti non vengano utilizzati per l'addestramento di IA pericoloso. Ci saranno anche ragioni legittime per utilizzare questi chip per l'esecuzione di servizi di IA esistenti come (le versioni già esistenti di) ChatGPT. Per gestire questa tensione, l'AISI può applicare misure di verifica ai chip per l'IA per garantire che non siano impiegati in attività proibite.

Ciò costituisce quindi un incentivo positivo per l'adesione al Trattato: si possono continuare a usare i chip per l'IA, a condizione che un'attività di supervisione possa verificare che non vengano utilizzati in modo da violare il Trattato (ad esempio per l'addestramento di IA su larga scala). Sebbene il monitoraggio continuo da parte dell'AISI possa sembrare sgradevole, riteniamo che sia la migliore soluzione disponibile. Dato l'obiettivo di impedire l'addestramento di IA su larga scala, esistono due approcci principali: assicurarsi che nessuno disponga dell'hardware necessario (cioè che i chip per l'IA non possano esistere), oppure assicurarsi che l'hardware esistente non venga utilizzato per l'addestramento su larga scala (ovvero il tracciamento e la verifica dell'uso dei chip, come descritto negli Articoli V, VI e VII). Ciò è concettualmente analogo alle salvaguardie dell'AIEA: affinché un Paese non dotato di armi nucleari sia autorizzato a disporre di materiali e impianti nucleari, è necessario che l'AIEA effettui ispezioni per garantire che il loro uso sia limitato a scopi pacifici.

Affinché la verifica dell'uso dei chip sia efficace, l'AISI necessita di meccanismi che forniscano garanzie credibili del fatto che i chip per l'IA non vengano utilizzati in modi proibiti da questo Trattato. La soluzione più semplice consisterebbe nel richiedere lo spegnimento di tutti i chip, ma ciò impedirebbe alle Parti di beneficiare dell'uso dei chip per scopi non pericolosi. La verifica potrebbe essere facilitata anche da mezzi tecnologici, con la riserva che la tecnologia attuale [probabilmente](https://www.cnas.org/publications/reports/secure-governable-chips) presenta vulnerabilità di sicurezza che consentirebbero ai proprietari dei chip di eludere le misure di monitoraggio. Dato l'attuale stato embrionale della tecnologia di verifica, quest'ultima richiederebbe probabilmente un monitoraggio continuo in loco, oppure lo spegnimento dei chip fino alla maturazione dei mezzi tecnologici. Tuttavia, una volta che la tecnologia di monitoraggio sarà matura, solidi [meccanismi di governance basati sull'hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) potrebbero consentire il monitoraggio remoto e affidabile dei chip.[^252]

Varie restrizioni e limiti potrebbero rendere più facile monitorare l'uso dei chip. I diversi carichi di lavoro legati all'IA, quali l'addestramento e l'inferenza, hanno requisiti tecnici differenti, e queste differenze potrebbero potenzialmente costituire la base per la verifica (se fossero sufficientemente robuste).

Per esempio, un meccanismo di verifica potrebbe consistere nel porre dei limiti all'interconnessione: limitare la quantità di comunicazione che un certo insieme di chip può avere con il mondo esterno usando cavi di rete a bassa larghezza di banda. In pratica, si prende un piccolo gruppo di chip — diciamo 8 H100 — e si limita la loro comunicazione esterna a un livello talmente basso da permettere ai chip di eseguire solo inferenze in modo efficiente, ma non di essere usati efficientemente per l'addestramento su larga scala. Questo funziona perché (con gli algoritmi del 2025) l'addestramento ha requisiti di comunicazione molto più elevati rispetto all'inferenza. Questo meccanismo sarebbe utile se l'esecuzione delle IA esistenti fosse accettabile, mentre non lo fosse l'addestramento di nuove IA.

Ci sono varie sfumature in questi meccanismi e molti altri potenziali meccanismi di verifica, quindi rimandiamo i lettori curiosi ai [precedenti](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) [lavori](https://aigi.ox.ac.uk/publications/verification-for-international-ai-governance/) [sull'argomento](https://www.rand.org/pubs/working_papers/WRA4077-1.html). Questo Articolo affida all'AISI il compito di sviluppare meccanismi di verifica migliori e di implementarne di diversi, definiti in senso lato. Riteniamo che questa flessibilità sia necessaria a causa della rapida evoluzione del settore dell'IA e della difficoltà di prevedere sviluppi che potrebbero invalidare i metodi di verifica. Inoltre, lo stato della ricerca sulla verifica dell'IA è nascente, e sono necessari ulteriori progressi prima che l'AISI possa disporre di un solido insieme di strumenti.

L'Articolo IV vieta l'addestramento su larga scala delle IA. Se al momento dell'entrata in vigore della verifica non fossero ancora state addestrate IA altamente potenti (e quindi pericolose), la verifica potrebbe concentrarsi specificamente sull'addestramento. Tuttavia, qualora fossero già state create IA sufficientemente potenti, potrebbe essere necessario monitorare anche se vengano implementate su chip per l'IA o quali attività svolgano: la verifica non potrebbe più concentrarsi sulla prevenzione dell'addestramento. La verifica sarebbe molto più difficile se fossero stati creati sistemi di IA sufficientemente pericolosi (ad esempio, quelli che potrebbero contribuire in modo sostanziale alla ricerca e sviluppo nel campo dell'IA), poiché verificare che non svolgano attività proibite sarebbe più difficile che verificare che non si stia effettuando un addestramento di IA su larga scala. Concretamente, sarà probabilmente più facile distinguere tra l'addestramento e l'esecuzione delle IA che tra l'esecuzione di IA per un tipo di compito e per un altro. A causa di questa differenza nella difficoltà di verifica, l'attuazione di questo Trattato sarebbe molto più semplice in un mondo in cui il progresso delle capacità dell'IA si arrestasse presto. Se invece tale progresso dovesse continuare, la verifica dell'uso dei chip potrebbe richiedere più lavoro, spese e restrizioni.

Allo stesso modo, potrebbe essere auspicabile monitorare il contenuto dell'inferenza dell'IA per assicurarsi che le IA non vengano usate per scopi dannosi. Questo tipo di monitoraggio è già applicato oggi da molte aziende del settore, ad esempio per rilevare se gli utenti stiano cercando di usare le IA per creare [armi biologiche](https://openai.com/index/preparing-for-future-ai-capabilities-in-biology/). Potrebbe essere auspicabile applicare un monitoraggio simile a livello globale, a condizione che i relativi rischi per la privacy e la sicurezza possano essere mitigati.

Il Paragrafo 5 di questo Articolo consente all'AISI di utilizzare metodi di verifica diversi per CCC diversi. Una delle ragioni di questa discriminazione è di ordine pratico: CCC diversi richiederanno approcci di verifica diversi al fine di stabilire con fondata fiducia che non vengano utilizzati per lo sviluppo di IA pericolose. Per esempio, i grandi data center che in precedenza venivano usati per l'addestramento di IA di frontiera avrebbero maggiore potenzialità di contribuire all'addestramento proibito, quindi per loro potrebbe essere richiesto un monitoraggio più intenso.

In secondo luogo, una discriminazione negli approcci di verifica renderebbe il trattato più accettabile, richiedendo un monitoraggio meno invasivo per i CCC sensibili. Ad esempio, se le agenzie di intelligence o le forze armate non vogliono alcun monitoraggio dei loro data center da parte dell'AISI, e questa disposizione aiuta a trovare un equilibrio. Sarebbe comunque necessario verificare che tali data center non vengano utilizzati per attività di IA pericolose, e l'AISI collaborerebbe con questi gruppi per assicurarsi di ottenere le informazioni necessarie, rispettando al contempo le esigenze di privacy e sicurezza dei proprietari dei CCC. D'altra parte, consentire protocolli di verifica diversi potrebbe compromettere la sostenibilità del trattato se questi fossero considerati iniqui, soprattutto se il relativo processo decisionale risultasse sbilanciato.

Il piano esposto in questo trattato prevede di consentire la continuazione dell'uso e della produzione di chip. Questo permette al mondo di beneficiare di questi chip. Un approccio alternativo consiste nel fermare la produzione di nuovi chip e/o distruggere quelli esistenti. Se i chip venissero distrutti, in assenza di progressi algoritmici, il tempo di breakout (cioè il tempo che intercorre tra l'inizio di attività pericolose da parte di un attore e il momento in cui queste avrebbero successo se non venissero fermate) si allungherebbe, perché gli attori dovrebbero prima produrre molti chip di IA (il che sarebbe probabilmente rilevabile e potrebbe richiedere anni o decenni, a seconda dello stato della catena di fornitura dei chip). Pertanto, la distruzione dei chip comporterebbe il vantaggio significativo di tempi di breakout molto più lunghi e una maggiore difficoltà nello sviluppo di IA pericolose. Tuttavia, comporterebbe anche un costo notevole, non consentendo al mondo di beneficiare di questi chip. Poiché riteniamo fattibile tracciare i chip e verificarne l'utilizzo, la nostra bozza di trattato segue questa strada piuttosto che quella dell'eliminazione dei chip per l'IA. Entrambe le strade, però, hanno i loro meriti.

---

#### Precedenti

Quando abbiamo parlato dei precedenti per l'Articolo VI, abbiamo descritto il monitoraggio continuo degli ex siti di produzione di missili a medio raggio nell'ambito del trattato INF, che, pur permettendo la pesatura e la scansione non distruttiva dei veicoli in uscita dagli impianti, non consentiva agli ispettori di entrare nei camion o nei siti stessi. Un monitoraggio perimetrale analogo dei data center può fornire alcuni indizi sulle operazioni in base al consumo energetico, alle emissioni termiche e alla larghezza di banda della rete. Tuttavia, per avere una ragionevole certezza che non si verifichino operazioni di IA soggette a restrizioni, sarà probabilmente necessaria una combinazione degli elementi elencati al Paragrafo 1 del nostro Articolo VII, che include telecamere a prova di manomissione, meccanismi hardware integrati nel chip, e ispettori in loco.

Tali pratiche sono già prassi di routine per l'Agenzia internazionale per l'energia atomica, che fa [sempre più ricorso a tecnologie di sorveglianza attive 24 ore su 24](https://www.iaea.org/newscenter/news/surveying-safeguarded-material-24/7) per integrare le ispezioni:

> Oltre un milione di dati crittografati sulle salvaguardie vengono raccolti da oltre 1400 telecamere di sorveglianza e 400 sensori di radiazioni e di altro tipo in tutto il mondo. Più di 23.000 sigilli installati negli impianti nucleari garantiscono il contenimento di materiali e attrezzature.

Uno dei metodi usati nell'ambito dello START I per verificare la conformità alle caratteristiche prestazionali dei missili era la condivisione di quasi tutti i dati telemetrici trasmessi dai sensori di volo durante i test, come specificato nel [protocollo telemetrico](https://1997-2001.state.gov/www/global/arms/starthtm/start/telempro.html), che richiedeva anche alle parti di fornire qualsiasi apparecchiatura di riproduzione e informazioni sulla formattazione dei dati necessarie per interpretarli. A seconda della combinazione di metodi di verifica adottati, un'agenzia internazionale di superintelligenza può utilizzare metodi analoghi, basandosi sul monitoraggio leggero, che è [prassi comune](https://www.governance.ai/research-paper/governing-through-the-cloud) per i fornitori di cloud computing per la raccolta di informazioni sui carichi di lavoro dei clienti.

Il monitoraggio continuo da parte del governo delle strutture commerciali private (come la maggior parte dei data center) ha anche numerosi precedenti. La Commissione per la regolamentazione nucleare degli Stati Uniti, incaricata di supervisionare la sicurezza dei reattori nucleari nazionali, disloca [due ispettori residenti](https://www.nrc.gov/reactors/operating/oversight/rop-description/resident-insp-program.html) in ogni centrale elettrica commerciale degli Stati Uniti, e i produttori di carne statunitensi [non possono effettuare operazioni di macellazione](https://www.fsis.usda.gov/sites/default/files/media_file/2021-02/Fed-Food-Inspect-Requirements.pdf) se il personale di ispezione del FSIS[^253] non è sul posto per supervisionarle.

---

### Articolo VIII: Ricerca Soggetta a Restrizioni: algoritmi e hardware per l'IA {#articolo-viii:-ricerca-soggetta-a-restrizioni:-algoritmi-e-hardware-per-l-ia}

1. Al fine di prevenire ricerche specifiche che facciano avanzare la frontiera delle capacità dell'IA o che compromettano la capacità delle Parti di implementare le misure del presente Trattato, questo Trattato designa come "Ricerca Soggetta a Restrizioni" la ricerca che soddisfa una qualsiasi delle seguenti condizioni:  
   1. Miglioramenti ai metodi utilizzati per creare modelli all'avanguardia, come definiti nell'Articolo II, che migliorerebbero le capacità dei modelli o l'efficienza dello sviluppo, dell'implementazione o dell'uso dell'IA  
   2. Metodi di addestramento distribuiti o decentralizzati, o metodi di addestramento ottimizzati per l'uso su hardware ampiamente disponibile o di consumo  
   3. Ricerca su paradigmi di intelligenza artificiale che vadano oltre l'apprendimento automatico  
   4. Progressi nella fabbricazione di chip o componenti di chip rilevanti per l'IA  
   5. Progettazione di chip per l'IA più performanti o più efficienti  
2. La divisione Controlli sulla Ricerca dell'AISI classificherà tutte le attività di Ricerca Soggetta a Restrizioni come *Controllate* o *Vietate*.  
   1. Ciascuna Parte monitorerà tutte le attività di Ricerca Controllata all'interno della propria giurisdizione e adotterà misure per garantire che tutte le ricerche controllate siano monitorate e messe a disposizione della divisione Controlli sulla Ricerca a fini di revisione e monitoraggio.  
   2. Ciascuna Parte non condurrà alcuna Ricerca Vietata, e ne proibirà e preverrà la conduzione da parte di qualsiasi entità all'interno della propria giurisdizione.  
3. Nessuna Parte assisterà, incoraggerà o condividerà ricerche vietate, anche tramite finanziamento, approvvigionamento, ospitamento, supervisione, insegnamento, pubblicazione, fornitura di strumenti o chip controllati, o facilitazione di collaborazioni.  
4. Ciascuna Parte nominerà un rappresentante presso la divisione Controlli sulla Ricerca dell'AISI, in seno al Segretariato Tecnico (istituito nell'Articolo III). A tale divisione sono attribuite le seguenti responsabilità:  
   1. Interpretare e chiarire le categorie di Ricerca Soggetta a Restrizioni e rispondere alle domande sui confini di tale ricerca, in risposta a nuove informazioni e alle richieste di ricercatori, organizzazioni o membri delle Parti.  
   2. Interpretare e chiarire il confine tra Ricerca Controllata e Ricerca Vietata, e rispondere alle domande su tale confine, in risposta a nuove informazioni e alle richieste di ricercatori, organizzazioni o membri delle Parti.  
   3. Modificare la definizione di Ricerca Soggetta a Restrizioni e sue categorie, in risposta al mutare delle condizioni o a richieste da parte di ricercatori, organizzazioni o membri delle Parti.  
   4. Modificare il confine tra Ricerca Controllata e Ricerca Vietata in risposta al mutare delle condizioni o a richieste da parte di ricercatori, organizzazioni o membri delle Parti.  
   5. Il Consiglio Esecutivo potrà porre il veto su qualsiasi modifica alla Ricerca Soggetta a Restrizioni o alle categorie e classificazioni al suo interno con una maggioranza dei due terzi, come descritto nell'Articolo III.

---

#### Note

Sarà difficile vietare diverse ampie categorie di ricerca, dato che le conoscenze tecniche necessarie sono già diffuse nel settore privato. Nella nostra proposta, la ricerca è soggetta a restrizioni se accresce le capacità o le prestazioni dell'IA, o se mette a rischio il sistema di verifica definito negli Articoli precedenti.

Alcune ricerche devono essere vietate per impedire l'avanzamento delle capacità dell'IA, anche mantenendo costante la quantità di FLOP usata per l'addestramento. Questo divieto dovrebbe coprire tutte le ricerche che potrebbero rendere l'addestramento delle IA più efficiente o che potrebbero accrescerne le capacità. Questo viene spesso definito "progresso algoritmico". Nei paradigmi attuali, ciò include progressi negli algoritmi usati nel pre-addestramento, nel post-addestramento e al momento dell'inferenza. Con il mutare dei paradigmi, queste distinzioni potrebbero diventare meno nette e potrebbero sorgere nuove categorie. Per questo motivo, il trattato fa riferimento a "sviluppo, impiego o uso".

I precedenti miglioramenti algoritmici, come il trasformatore, dimostrano il potenziale per enormi e rapidi progressi sulla frontiera delle capacità dell'IA: i nuovi algoritmi possono trasformare il modo in cui l'IA funziona. Inoltre, paradigmi innovativi potrebbero ridurre drasticamente la quantità di risorse computazionali necessaria per un dato livello di capacità dell'IA. Ad esempio, le IA moderne sono *molto* meno efficienti in termini di dati rispetto agli esseri umani, il che suggerisce che esistano algoritmi molto più efficienti in termini di dati, che aspettano solo di essere scoperti.

Altre ricerche devono essere vietate affinché non riducano i requisiti computazionali per l'addestramento di IA pericolose, fino a che queste non possano essere addestrate con un piccolo numero di chip per l'IA (o con molti chip distribuiti in piccoli cluster presso numerose località), il che ostacolerebbe gli sforzi di monitoraggio.

Inoltre, questo divieto deve estendersi alla ricerca di nuovi modi per produrre chip per l'IA non tracciabili, poiché il regime di monitoraggio è realizzabile in gran parte grazie all'attuale complessità e centralizzazione della produzione di semiconduttori avanzati per l'IA.

Questo Articolo vieta anche la ricerca sulla progettazione di chip per l'IA più performanti o efficienti, che altrimenti diventerebbero [sostanzialmente più efficienti](https://epoch.ai/data-insights/ml-hardware-energy-efficiency) di anno in anno. Un data center che utilizza chip per l'IA più efficienti sarebbe più facile da nascondere, poiché questi chip consumerebbero meno elettricità a parità di prestazioni o per prestazioni superiori.

I tipi specifici di ricerca soggetti a restrizioni dovranno essere aggiornati nel tempo. Un esempio di attività che l'AISI potrebbe voler limitare in futuro è la ricerca su un migliore hardware di calcolo non-IA, se tali progressi dovessero rappresentare un rischio per la verifica.

Gli sforzi nazionali per limitare la ricerca potrebbero iniziare concentrandosi sulla pubblicazione e sul finanziamento della ricerca. Il cambiamento delle leggi e delle norme sociali sarebbe probabilmente molto efficace nel convincere la maggior parte dei ricercatori a interrompere le ricerche pericolose. La diversità di azioni soggette a restrizioni nel Paragrafo 3 risponde alla necessità di garantire che, se le attività di ricerca sono suddivise tra più giurisdizioni, il trattato continui ad attribuire inequivocabilmente a ogni Stato la responsabilità di proibire e prevenire le singole attività. Questo si applica, per esempio, al caso in cui un'azienda in una giurisdizione assuma un dipendente in una seconda che opera da remoto su chip ospitati in una terza.

---

#### Precedenti

Le restrizioni preventive alla diffusione di informazioni relative a tecnologie pericolose trovano un precedente nell'[Atomic Energy Act del 1946](https://www.atomicarchive.com/resources/documents/postwar/atomic-energy-act.html), tuttora in vigore, che classificò come *dati riservati* le informazioni su determinati argomenti (la dottrina del "born secret"); le eccezioni erano a discrezione della Commissione per l'Energia Atomica, istituita da questa stessa legge:[^254]

> Con l'espressione "dati riservati", ai sensi di questa sezione, si intendono tutti i dati riguardanti la fabbricazione o l'utilizzo di armi atomiche, la produzione di materiale fissile o l'impiego di materiale fissile nella produzione di energia; l'espressione non include tuttavia i dati che la Commissione, di volta in volta, stabilisca possano essere pubblicati senza pregiudicare la difesa e la sicurezza comuni.

A differenza di altri tipi di classificazione governativa, i *dati riservati* possono essere creati (deliberatamente o accidentalmente) dal settore privato, una questione di costituzionalità irrisolta[^255] che evidenzia la necessità di un organo di regolamentazione autorizzato e capace di prendere decisioni quotidiane sui confini esatti dei *dati riservati*. La [National Nuclear Security Administration](https://www.usa.gov/agencies/national-nuclear-security-administration) (NNSA) svolge questo compito per i segreti nucleari negli Stati Uniti. Ai sensi del nostro Articolo VIII, Paragrafo 5, la divisione per il Controllo sulla Ricerca della nuova AISI assumerebbe questo ruolo per la ricerca sull'IA soggetta a restrizioni. Svolgerebbe anche altre funzioni analoghe a quelle della NNSA, delineate nel nostro Articolo IX, attraverso: (1) il mantenimento dei rapporti con ricercatori e organizzazioni che lavorano a progetti che si avvicinano alla soglia di classificazione, e (2) la creazione di un'infrastruttura sicura per la segnalazione e il contenimento di scoperte involontarie.

Esistono precedenti anche per il contenimento e il controllo della ricerca in campi pericolosi. Negli ultimi mesi della seconda guerra mondiale, il Regno Unito e gli Stati Uniti collaborarono alla [missione Alsos](https://ahf.nuclearmuseum.org/ahf/history/alsos-mission/) per catturare gli scienziati nucleari tedeschi, raccogliere informazioni sui progressi della Germania verso la bomba atomica, e impedire all'URSS di ottenere queste risorse per il proprio programma nucleare. Il [Progetto Overcast](https://airandspace.si.edu/stories/editorial/project-paperclip-and-american-rocketry-after-world-war-ii) (noto anche come Operazione Paperclip) fu un programma segreto statunitense per impiegare ingegneri missilistici tedeschi al termine della guerra.

Il contenimento della ricerca riservata sull'IA all'interno degli Stati Parte potrebbe avvenire attraverso i quadri normativi esistenti. Negli Stati Uniti, questi includono:

* Il concetto di "[esportazioni presunte](https://www.bis.gov/learn-support/deemed-exports/what-deemed-export)" nella normativa sul controllo delle esportazioni, che obbliga un'entità statunitense a ottenere una licenza di esportazione dal Bureau of Industry and Security[^256] prima di condividere tecnologie controllate con soggetti stranieri, poiché tale condivisione è considerata un'esportazione.  
* Le [International Traffic in Arms Regulations](https://www.pmddtc.state.gov/ddtc_public?id=ddtc_kb_article_page&amp;sys_id=24d528fddbfc930044f9ff621f961987) (ITAR), un insieme di regolamenti del Dipartimento di Stato degli Stati Uniti che controllano l'esportazione di tecnologie militari e di alcune tecnologie a duplice uso. Fino al 1996, l'ITAR è stato utilizzato per impedire un più ampio sviluppo e utilizzo delle tecniche di crittografia da parte del settore privato, poiché queste erano classificate come "articoli di difesa" nella [United States Munitions List](https://www.ecfr.gov/current/title-22/chapter-I/subchapter-M/part-121).  
* L'[Invention Secrecy Act del 1951](https://www.congress.gov/bill/82nd-congress/house-bill/4687/text), che conferisce alle agenzie governative statunitensi il potere di imporre "ordini di segretezza" sulle nuove domande di brevetto con implicazioni per la sicurezza nazionale. Agli inventori non solo può essere negato il brevetto, ma è anche legalmente vietato loro divulgare, pubblicare o persino utilizzare le loro invenzioni.[^257]

Il Progetto Overcast offre anche un precedente per controllare i ricercatori, retribuendoli generosamente affinché agiscano nell'interesse dello Stato. Un ulteriore precedente per tali incentivi è trattato nell'Articolo IX.

---

### Articolo IX: Verifica delle Restrizioni alla Ricerca {#articolo-ix:-verifica-delle-restrizioni-alla-ricerca}

1. Ogni Parte deve creare o conferire poteri a un'agenzia nazionale con le seguenti responsabilità:  
   1. Tenersi aggiornata e mantenere i contatti con i ricercatori e le organizzazioni nazionali che lavorano in settori affini alla Ricerca Soggetta a Restrizioni, al fine di comunicare le categorie di Ricerca Soggetta a Restrizioni stabilite nell'Articolo VIII.  
   2. Imporre sanzioni per dissuadere i ricercatori e le organizzazioni nazionali dal condurre Ricerca Soggetta a Restrizioni. Tali sanzioni dovranno essere proporzionate alla gravità della violazione e concepite come un deterrente efficace. Ciascuna Parte dovrà promulgare o modificare le disposizioni di legge necessarie per consentire l'imposizione di tali sanzioni.  
   3. Istituire infrastrutture sicure per la segnalazione e il contenimento di scoperte involontarie che soddisfano le condizioni per la Ricerca Soggetta a Restrizioni. Tali segnalazioni saranno condivise con la divisione per il Controllo sulla Ricerca.  
2. Per agevolare la verifica internazionale dei divieti di ricerca, la divisione Controllo sulla Ricerca svilupperà e implementerà meccanismi di verifica.  
   1. Tali meccanismi potrebbero includere, a titolo esemplificativo ma non esaustivo:  
      1. Colloqui dell'AISI con ricercatori che hanno lavorato in passato su argomenti di Ricerca Soggetta a Restrizioni o che attualmente lavorano in settori adiacenti.  
      2. Monitoraggio dello stato occupazionale e dell'ubicazione dei ricercatori che hanno lavorato in passato su argomenti di Ricerca Soggetta a Restrizioni o che attualmente lavorano in settori adiacenti.  
      3. Mantenimento di revisori interni dell'AISI in organizzazioni selezionate ad alto rischio (ad esempio, progetti difficili da distinguere dalla Ricerca Soggetta a Restrizioni, ex organizzazioni di ricerca sull'IA).  
   2. Le Parti collaboreranno all'implementazione di questi meccanismi di verifica.  
   3. Le informazioni ottenute attraverso questi meccanismi di verifica saranno raccolte in relazioni per il Consiglio Esecutivo, garantendo la massima riservatezza delle informazioni sensibili al fine di proteggere la privacy e le informazioni riservate di individui e Parti.

---

#### Note

Oltre alle regole dell'Articolo VIII, i Paesi verificheranno che non vengano condotte ricerche vietate sull'IA. Un aspetto chiave di questo approccio consiste nello stabilire "aree adiacenti alla Ricerca Soggetta a Restrizioni" e poi nello stabilire relazioni con i ricercatori che lavorano in tali aree. Il numero di ricercatori di punta nel campo dell'IA a livello mondiale è sufficientemente basso da rendere fattibile il monitoraggio delle attività di una frazione significativa di essi. Il solo personale tecnico delle principali aziende di IA ammonterebbe a circa 5.000 ricercatori, mentre il numero di partecipanti alle più importanti conferenze sull'IA sarebbe [dell'ordine di 70.000](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences). È opinione comune che un gruppo molto più piccolo sia cruciale per lo sviluppo dell'IA di frontiera, probabilmente nell'ordine delle centinaia.[^258] Gli Stati potrebbero anche contattare i ricercatori in merito alle loro attività e offrire asilo e incentivi finanziari a eventuali informatori (si veda l'Articolo X).

Sebbene sia già pubblicamente disponibile una notevole conoscenza tecnica sulle attuali pratiche di sviluppo dell'IA, che un attore malintenzionato potrebbe sfruttare, riteniamo che le restrizioni legali e le verifiche avrebbero un impatto profondo sul progresso generale del settore.

Se il monitoraggio fosse esteso ai ricercatori e agli ingegneri coinvolti nella progettazione e nella produzione di semiconduttori, ciò aumenterebbe notevolmente la portata del monitoraggio richiesto. Ove ciò non fosse fattibile, gli Stati potrebbero monitorare le aziende piuttosto che i singoli individui, sfruttando la complessità e la portata della produzione di semiconduttori avanzati.

Il Paragrafo 2 di questo Articolo stabilisce un regime di maggiore trasparenza che facilita la verifica del divieto di ricerca. Tali misure di cooperazione mirano a fornire alle Parti le garanzie di cui hanno bisogno. Prevediamo che le Parti si adopereranno per trovare, attraverso lo sviluppo di meccanismi specifici, un livello di trasparenza che infonda fiducia nel divieto di ricerca, riducendo al minimo i costi per gli interessi statali e la privacy personale. La verifica di un divieto di ricerca è un'impresa complessa e delicata che richiederà uno sforzo continuo e iterativo. 

Le Parti potrebbero essere preoccupate dalla possibilità che altre Parti promulghino divieti di ricerca a livello nazionale, per poi violarli con attività governative segrete celate ai servizi di intelligence stranieri. La ricerca che violerebbe i divieti può variare in scala, ed è probabile che gli sforzi su larga scala (che coinvolgono molti ricercatori e chip rilevanti per l'IA) sarebbero facilmente rilevabili da attori statali determinati. Ma gli sforzi su scala ridotta, come lo sviluppo di paradigmi alternativi di intelligenza artificiale, potrebbero richiedere solo pochi ricercatori e hardware di uso comune. Sarà difficile assicurare alle altre Parti che tali sforzi non vengano celati. La combinazione di raccolta di informazioni (Articolo X) e meccanismi di verifica da parte dell'AISI potrebbe essere sufficiente. Sottolineiamo inoltre l'importanza di proteggere gli informatori (Articolo X).

---

#### Precedenti

Tra le agenzie esistenti con il potere di "mantenere la consapevolezza e le relazioni con i ricercatori e le organizzazioni nazionali" a rischio di sviluppare informazioni soggette a restrizioni, come previsto dall'Articolo IX (1.a.), figurano il DOE e la NNSA, già trattati nei precedenti per l'Articolo VIII.

Un precedente per il "monitoraggio dello stato occupazionale e dell'ubicazione dei ricercatori" in settori ad alto rischio, come da noi suggerito nel Paragrafo 2.(a).(ii), è costituito dall'International Science and Technology Center (ISTC).[^259] Istituito nel 1994, l'ISTC fu creato appositamente per ridurre i rischi di proliferazione nucleare, [mantenendo i ricercatori nucleari sovietici proficuamente impiegati in attività pacifiche](https://astanatimes.com/2014/12/istc-headquartered-nazarbayev-university-2015/) e connessi alla comunità scientifica internazionale. L'ISTC dimostra inoltre il potenziale degli incentivi quale complemento alle sanzioni, per evitare che gli esperti tecnici (che potrebbero trovarsi disoccupati a seguito di questo trattato) intraprendano una Ricerca Soggetta a Restrizioni.

Nella misura in cui le sanzioni debbano essere severe per garantire la deterrenza indicata nell'Articolo IX.1.(b), si può trovare un modello nel [capitolo sull'applicazione (18)](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf#page=154) dell'Atomic Energy Act del 1946, in base al quale la condivisione non autorizzata di Dati Soggetti a Restrizioni può essere punita con la morte o la reclusione se la divulgazione è avvenuta con intento di tradimento.[^260]

Nello sviluppo di un'infrastruttura sicura dell'AISI "per la segnalazione e il contenimento di scoperte involontarie di Ricerca Soggetta a Restrizioni", precedenti e modelli potenzialmente utilizzabili sono reperibili nelle ampie [procedure del DOE](https://www.ecfr.gov/current/title-10/chapter-X/part-1045) per la gestione di diverse categorie di dati sensibili. Anche il [Sistema di Segnalazione e Trattamento delle Occorrenze](https://www.directives.doe.gov/directives-documents/200-series/0232.2-BOrder-a-chg1-minchg/@@images/file) del DOE, così come le istruzioni del Comitato per i Sistemi di Sicurezza Nazionale[^261] per la [fuga di informazioni classificate](https://sgp.fas.org/library/cnssi-1001.pdf), possono rivelarsi utili.

La divisione per il Controllo della Ricerca del nostro trattato potrebbe ispirarsi alle pratiche esistenti dell'AIEA per sviluppare i protocolli di ispezione. Nel quadro del [Protocollo Aggiuntivo Modello](https://www.iaea.org/topics/additional-protocol), approvato nel 1997 dal Consiglio dei Governatori dell'AIEA, gli Stati che hanno stipulato accordi di salvaguardia globali[^262] consentono ispezioni di [accesso complementare](https://www.iaea.org/sites/default/files/safeguards0408.pdf) volte a individuare materiale nucleare non dichiarato. Nell'ambito di tali visite, gli ispettori possono [sottoporre gli operatori a colloquio](https://www-pub.iaea.org/MTCD/Publications/PDF/SVS-30_web.pdf), come previsto dalla nostra proposta al Paragrafo IX.2.a.i.

Proponiamo anche di "mantenere revisori interni dell'AISI in organizzazioni selezionate ad alto rischio", proprio come oggi gli [uffici locali](https://pantex.energy.gov/news/press-releases/nnsa-release-nnsa-establishes-new-federal-office-pantex-plant) del DOE e della NNSA hanno sede presso i laboratori nucleari nazionali e gli impianti di produzione gestiti da appaltatori.

Per "proteggere la privacy e i segreti di individui e Parti" durante le verifiche, come richiesto dal Paragrafo 2(c) del presente Articolo, la divisione per il Controllo della Ricerca dell'AISI potrebbe adattare le pratiche di compartimentazione dalle agenzie di intelligence esistenti delle Parti e dagli [accordi multilaterali di condivisione dell'intelligence](https://law.yale.edu/mfia/case-disclosed/newly-disclosed-documents-five-eyes-alliance-and-what-they-tell-us-about-intelligence-sharing). Ad esempio, in base alla "regola della terza parte" o al "principio del controllo dell'originatore" [considerati pratica comune in tali accordi](https://www.dcaf.ch/sites/default/files/publications/documents/MIICA_book-FINAL.pdf), è vietato divulgare informazioni condivise a terzi (potenzialmente anche agli organi di vigilanza) senza l'autorizzazione dell'agenzia di origine.

---

### Articolo X: Raccolta di informazioni e ispezioni di verifica {#articolo-x:-raccolta-di-informazioni-e-ispezioni-di-verifica}

1. Una fonte fondamentale di informazioni per l'AISI è costituita dalle attività indipendenti di raccolta informazioni delle Parti. Pertanto, la divisione Consolidamento delle Informazioni (Articolo III) riceverà tali informazioni.  
   1. La divisione Consolidamento delle Informazioni adotterà le opportune precauzioni per proteggere i segreti commerciali, industriali, di sicurezza e di Stato e le altre informazioni riservate di cui venga a conoscenza nell'attuazione del Trattato, incluso il mantenimento di canali di segnalazione sicuri, riservati e, facoltativamente, anonimi.  
   2. Per garantire il rispetto delle disposizioni del presente Trattato, ciascuna Parte utilizzerà i Mezzi Tecnici Nazionali (MTN) di verifica a sua disposizione in modo conforme ai principi generalmente riconosciuti del diritto internazionale.  
      1. Ciascuna Parte si impegna a non interferire con i Mezzi Tecnici Nazionali di verifica delle altre Parti che operano in conformità con quanto sopra.  
      2. Ciascuna Parte si impegna a non ricorrere a misure deliberate di occultamento che impediscano la verifica, tramite Mezzi Tecnici Nazionali, del rispetto delle disposizioni del presente Trattato.  
      3. Le Parti sono incoraggiate, ma non obbligate, a collaborare per individuare attività pericolose legate all'IA nei paesi che non sono Parti. Le Parti sono incoraggiate, ma non obbligate, a sostenere i MTN delle Parti rivolti ai paesi che non sono Parti, nella misura in cui sono rilevanti ai fini del presente Trattato.  
2. Una fonte chiave di informazioni per l'AISI sono le persone che forniscono all'AISI prove di attività pericolose legate all'IA. Queste persone sono soggette a misure di protezione per gli informatori.  
   1. Questo Articolo stabilisce protezioni, incentivi e assistenza per le persone ("Informatori Tutelati") che, in buona fede, forniscono all'AISI o a una Parte informazioni credibili riguardanti violazioni effettive, tentate o pianificate del presente Trattato o altre attività che comportano un grave rischio di estinzione umana, tra cui chip nascosti, data center non dichiarati, addestramento o ricerca proibite, evasione della verifica o falsificazione delle dichiarazioni. Gli Informatori Tutelati includono dipendenti, appaltatori, funzionari pubblici, fornitori, ricercatori e altre persone in possesso di informazioni rilevanti, nonché le Persone Associate (familiari e collaboratori stretti) che assistono o sono a rischio a causa della divulgazione.  
   2. Le Parti devono vietare e prevenire ritorsioni contro gli Informatori Tutelati e le Persone Associate, tra cui, a titolo esemplificativo ma non esaustivo: licenziamento, retrocessione, inserimento in liste nere, perdita di benefici, molestie, intimidazioni, minacce, azioni civili o penali, revoca del visto, violenza fisica, reclusione, restrizione della libertà di movimento o altre misure avverse. Qualsiasi clausola contrattuale (compresi gli accordi di non divulgazione o di non denigrazione) che miri a limitare le divulgazioni protette ai sensi del presente Trattato sarà nulla e inapplicabile. Il maltrattamento degli informatori costituirà una violazione del presente Trattato e sarà trattato ai sensi dell'Articolo XI, Paragrafo 3\.  
   3. L'AISI deve mantenere canali di segnalazione sicuri, riservati e, facoltativamente, anonimi. Le Parti devono istituire canali nazionali interoperabili con il sistema dell'AISI. L'AISI e le Parti devono proteggere l'identità degli Informatori Tutelati e delle Persone Associate, e possono rivelarla solo quando strettamente necessario e previa adozione di adeguate misure di protezione. La divulgazione non autorizzata delle identità protette costituirà una violazione del presente Trattato e sarà trattata ai sensi dell'Articolo XI, Paragrafo 3\.  
   4. Le Parti devono offrire asilo o protezione umanitaria agli Informatori Protetti e alle loro famiglie, fornire documenti di salvacondotto e coordinare un transito sicuro.  
3. L'AISI può condurre ispezioni a sorpresa di siti sospetti sulla base di informazioni credibili riguardo ad attività pericolose legate all'intelligenza artificiale.  
   1. Le Parti possono richiedere all'AISI di effettuare un'ispezione a sorpresa. Il Consiglio Esecutivo, sia su richiesta che sulla base dell'analisi fornita dalla divisione Consolidamento delle Informazioni, prenderà in esame le informazioni disponibili al fine di richiedere ulteriori informazioni a Parti o a non-Parti, proporre un'ispezione a sorpresa, o decidere che non è giustificata alcuna ulteriore azione.  
   2. Un'ispezione a sorpresa richiede l'approvazione della maggioranza del Consiglio Esecutivo.  
   3. L'accesso a un sito sospetto deve essere concesso dallo Stato sul cui territorio si trova il sito, entro 24 ore dalla richiesta di un'ispezione a sorpresa da parte dell'AISI. Durante questo periodo, il sito può essere sorvegliato e le persone o i veicoli che lasciano il sito possono essere ispezionati da funzionari di una Parte firmataria o dell'AISI.  
   4. L'ispezione a richiesta sarà condotta da una squadra di funzionari dell'AISI, approvati sia dalla Parte ispezionata che dalla Parte che ha richiesto l'ispezione. L'AISI ha la responsabilità di collaborare con le Parti per mantenere aggiornati gli elenchi degli ispettori approvati a tale scopo.  
   5. Le ispezioni a richiesta possono essere condotte nel territorio di una data Parte per un massimo di 20 volte all'anno, e tale limite può essere modificato da un voto a maggioranza del Consiglio Esecutivo.  
   6. Gli ispettori avranno la massima cura di proteggere le informazioni sensibili dello Stato ispezionato, trasmettendo al Consiglio Esecutivo solo le informazioni pertinenti al trattato.

---

#### Note

##### Raccolta di informazioni

Prevediamo che vi saranno sforzi costanti e persino crescenti da parte di tutte le Parti per determinare autonomamente se qualcuno stia conducendo attività pericolose legate all'IA. Di conseguenza, le diverse attività di intelligence statali integrano e convalidano il monitoraggio che l'AISI conduce direttamente (si vedano, ad esempio, gli Articoli da IV a VII). La riservatezza applicata alle informazioni di intelligence fornite alla divisione per il Consolidamento delle Informazioni è della massima importanza. L'obiettivo è far sì che i servizi di intelligence degli Stati ritengano giustificati i rischi imposti ai loro metodi di intelligence, al fine di fornire le informazioni necessarie all'AISI. Mantenere tali informazioni nella più stretta riservatezza minimizza i rischi di compromissione.

I firmatari del trattato hanno familiarità con diverse forme di intelligence, come l'analisi di immagini satellitari e l'intelligence umana, su cui si continuerà a fare affidamento anche dopo l'entrata in vigore del trattato. Prevediamo che le Parti si aspettino che tali attività proseguano, e un obiettivo di questo Articolo è permettere che le prove raccolte con questi mezzi vengano fornite all'AISI senza imporre costi eccessivi a chi le ha prodotte.

Questo Articolo affronta anche la sorveglianza dei non firmatari, per i quali la necessità di intelligence sarà maggiore. Il trattato si astiene dall'imporre un tale obbligo ai firmatari, il che appare superfluo. I firmatari condurrebbero questo tipo di attività di intelligence anche in assenza di un trattato.

##### Protezione degli informatori

L'efficacia generale di questo Trattato si basa sulla fiducia delle Parti che le altre Parti non stiano intraprendendo attività vietate e pericolose nel campo dell'IA. Anche con i Mezzi Tecnici Nazionali e altre attività di intelligence, per gli Stati potrebbe essere difficile individuare tentativi clandestini di sviluppare una superintelligenza. Esistono molti ambiti in cui per gli Stati potrebbe non essere fattibile raccogliere informazioni sui propri rivali, come per le attività che si svolgono all'interno di installazioni militari. Gli Stati potrebbero essere giustamente preoccupati che qualche struttura sia sfuggita agli sforzi di monitoraggio. Gli informatori possono quindi fungere da fonte di informazione aggiuntiva, e la possibilità di una loro segnalazione fornisce un'ulteriore deterrenza contro le violazioni.

Gli informatori possono essere una risorsa efficace perché gli individui coinvolti in violazioni segrete del Trattato (ad esempio, sessioni di addestramento clandestine o ricerche sull'IA) potrebbero essere essi stessi preoccupati del pericolo rappresentato da una superintelligenza artificiale. Questo Articolo si propone di rendere la segnalazione di violazioni più sicura e meno onerosa per tali individui, spostando gli incentivi personali dal silenzio alla divulgazione.

Gli informatori potrebbero lanciare l'allarme riguardo a varie violazioni del Trattato:

* Articolo IV: Segnalare sessioni di addestramento non monitorate, che superano le soglie o che utilizzano metodi di addestramento distribuito vietati.  
* Articolo V: Divulgare l'esistenza di cluster di chip non dichiarati, il mancato consolidamento di tutto l'hardware designato o la diversione di chip verso strutture segrete e non monitorate.  
* Articolo VI: Segnalazione qualora nuovi chip di IA vengano prodotti senza rientrare nel regime di monitoraggio, o qualora vengano creati chip senza le funzionalità di sicurezza richieste.  
* Articolo VIII: Segnalazione di Ricerca Vietata nel campo dell'IA.

Alcune violazioni del trattato potrebbero essere particolarmente difficili da rilevare con le sole attività di intelligence ordinarie; ad esempio, le reti di addestramento distribuite e i progetti di ricerca segreti sull'IA gestiti dallo Stato.

Modifiche a questo Articolo potrebbero cambiarne l'efficacia e la fattibilità politica in vari modi. Ad esempio, gli Stati potrebbero offrire un compenso economico agli informatori legittimi per fornire ulteriori incentivi, ma ciò potrebbe essere visto come un modo per pagare i cittadini affinché tradiscano il proprio Paese.

##### Ispezioni su richiesta

Le ispezioni su richiesta sono una funzione fondamentale prevista dal Trattato e dall'AISI. Senza la credibile minaccia di essere scoperti, le Parti potrebbero temere che i loro rivali tentino di violare il Trattato (nonostante il fatto che una corsa alla superintelligenza sia intrinsecamente una perdita per tutti). La raccolta di informazioni è un metodo per contrastare quelli che vengono (erroneamente) percepiti come incentivi a defezionare. Sarebbe senza precedenti e indesiderabile finanziare la creazione di una capacità di raccolta informazioni autonoma all'interno dell'AISI, al livello necessario per fornire garanzie agli Stati; pertanto, l'AISI si affida alle Parti per la fornitura delle informazioni di intelligence cruciali.

---

#### Precedenti

In precedenza, trattando l'Articolo VIII, abbiamo discusso i precedenti per il consolidamento delle informazioni, citando l'esistenza di accordi di intelligence che includono pratiche di compartimentazione come la "regola della terza parte". Regole simili si riscontrano in seno all'AIEA, come in [INFCIRC/153](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1972/infcirc153.pdf), Parte 1.5:

> ...l'Agenzia adotterà ogni precauzione per proteggere i segreti commerciali e industriali e le altre informazioni riservate di cui venga a conoscenza nell'attuazione dell'Accordo.

Il personale è vincolato da obblighi di riservatezza ed è soggetto a sanzioni penali in caso di fughe di notizie. Questo è un aspetto importante, perché l'AIEA ha beneficiato della condivisione di informazioni di intelligence da parte degli Stati partecipanti, comprese immagini satellitari e documenti, [come nel caso dell'Iran](https://carnegieendowment.org/posts/2015/12/iran-and-the-evolution-of-safeguards?lang=en) e delle sue attività di arricchimento non dichiarate. Allo stesso modo, l'AIEA ha richiesto un'ispezione speciale sulla produzione non dichiarata di plutonio della Corea del Nord in [risposta a informazioni di intelligence che ha ricevuto](https://www.nonproliferation.org/wp-content/uploads/npr/dembin22.pdf#page=4).

Riconoscendo il ruolo fondamentale dei mezzi tecnici nazionali (MTN — immagini satellitari, raccolta di segnali e altri sistemi di telerilevamento) nella verifica degli accordi multilaterali, la nostra bozza di accordo mutua la formulazione del trattato ABM che limita i sistemi antimissili balistici, in cui “ciascuna Parte utilizza mezzi tecnici nazionali di verifica” e “si impegna a non interferire con i mezzi tecnici nazionali di verifica dell'altra Parte”. Una formulazione simile si trova nell'Articolo XII del [Trattato sulle Forze Nucleari a Raggio Intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm) del 1987, nell'Articolo IV del [Trattato sulla Messa al Bando Totale degli Esperimenti Nucleari](https://www.ctbto.org/sites/default/files/2023-10/2022_treaty_booklet_E.pdf#page=28) del 1996 e in tutto il [trattato New START](https://2009-2017.state.gov/documents/organization/140047.pdf) del 2010.

Poiché i mezzi tecnici nazionali (MTN) non sarebbero sufficienti a rilevare tutte le violazioni pericolose nel caso della superintelligenza artificiale, abbiamo ripreso alcune caratteristiche del [quadro delle Salvaguardie dell'AIEA](https://www.iaea.org/topics/safeguards-legal-framework) che incoraggiano le segnalazioni interne e forniscono i canali per farlo. Queste sono però ostacolate dalla mancanza di tutele esplicite per gli informatori; nulla nel TNP o in queste Salvaguardie proteggerà un informatore dal suo governo qualora questo decidesse di avviare ritorsioni, a meno che tale Stato non disponga di tutele nazionali applicabili. Le disposizioni a livello di trattato per la tutela degli informatori e il diritto d'asilo contenute nella nostra bozza di accordo hanno lo scopo di ovviare a questa lacuna.

La recente legislazione dell'UE sull'IA ha adottato misure simili. Il [Considerando 172](https://artificialintelligenceact.eu/recital/172/) della legge sull'IA dell'UE estende esplicitamente le [tutele generali per gli informatori](https://eur-lex.europa.eu/eli/dir/2019/1937/oj/eng) già previste dall'Unione a coloro che segnalano violazioni della legge sull'IA.

La [Convenzione sui Rifugiati del 1951](https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-relating-status-refugees) offre un possibile quadro di riferimento per la concessione del diritto d'asilo agli informatori, basando l'idoneità sul “fondato timore di essere perseguitati”, anche se potrebbe essere necessario un emendamento o un accordo supplementare per garantire che la segnalazione di illeciti in materia di IA costituisca un motivo di persecuzione legalmente valido.

L'asilo per le persone con conoscenze o competenze sensibili veniva concesso regolarmente durante la guerra fredda e nel periodo successivo. La Sezione 7 della [Legge sulla CIA del 1949](https://www.cia.gov/readingroom/docs/CIA-RDP89B00552R000700070018-7.pdf) prevedeva l'ammissione e la residenza permanente di un massimo di cento disertori e dei loro familiari diretti per ogni anno fiscale, se ritenuti "nell'interesse della sicurezza nazionale o essenziali per il perseguimento della missione di intelligence nazionale". Il Soviet Scientists Immigration Act del 1992 concesse fino a 750 visti a scienziati provenienti dall'ex Unione Sovietica e dagli Stati baltici con "competenze in campo nucleare, chimico, biologico o in altri settori dell'alta tecnologia, o che lavorano a progetti di difesa negli stessi campi".

Il meccanismo di ispezioni su richiesta che descriviamo nel Paragrafo 3 di questo Articolo è basato su quello della Parte IX della [CAC](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf):

> Ogni Stato Parte ha il diritto di richiedere un'ispezione in loco su richiesta presso qualsiasi impianto o luogo nel territorio o in qualsiasi altro luogo sotto la giurisdizione o il controllo di qualsiasi altro Stato Parte, al solo scopo di chiarire e risolvere eventuali questioni concernenti una possibile violazione...

La CAC, insieme ad altri trattati sul controllo degli armamenti come l'INF e il trattato nucleare START I tra Stati Uniti e Unione Sovietica, combina i MTN con [ispezioni su richiesta](https://www.osti.gov/servlets/purl/7166074) per verificarne la conformità.

---

### Articolo XI: Risoluzione delle controversie {#articolo-xi:-risoluzione-delle-controversie}

1. Qualsiasi Parte ("Parte Interessata") può sollevare preoccupazioni in merito all'attuazione del presente Trattato, ivi incluse preoccupazioni su situazioni ambigue o possibili non conformità da parte di un'altra parte ("Parte Richiesta"). Ciò include l'uso improprio delle Azioni di Protezione (Articolo XII).  
   1. La Parte Interessata deve comunicare la propria preoccupazione alla Parte Richiesta, condividendola anche con il Direttore Generale e il Consiglio Esecutivo. La Parte Richiesta deve accusare ricevuta di tale notifica entro 36 ore e fornire chiarimenti entro 5 giorni.  
2. Se la questione non viene risolta, la Parte Interessata può chiedere assistenza al Consiglio Esecutivo nella risoluzione e nel chiarimento della questione. Ciò può includere la richiesta da parte della Parte Interessata di un'ispezione di verifica ai sensi dell'Articolo X.  
   1. Il Consiglio Esecutivo fornirà le informazioni pertinenti in suo possesso relative a tale preoccupazione.  
   2. Il Consiglio Esecutivo può incaricare il Segretariato Tecnico di raccogliere ulteriore documentazione, convocare sessioni tecniche a porte chiuse e raccomandare misure risolutive.  
3. Se il Consiglio Esecutivo accerta una violazione del Trattato, può adottare misure per prevenire attività pericolose di IA o ammonire la Parte Richiesta. Questi provvedimenti possono includere:  
   1. Richiedere un monitoraggio aggiuntivo o ulteriori restrizioni sulle attività di IA  
   2. Richiedere la consegna dell'hardware di IA  
   3. Richiedere sanzioni  
   4. Raccomandare alle Parti di adottare Azioni di Protezione ai sensi dell'Articolo XII

---

#### Note

Lo scopo di questa clausola è istituire un processo di consultazione e chiarimento per risolvere le questioni che sorgono tra i firmatari. Le ispezioni su richiesta forniscono un meccanismo per garantire che tutte le Parti del Trattato rispettino le disposizioni del Trattato stesso.

Dato il ritmo dell'innovazione nel campo dell'intelligenza artificiale, determinare le violazioni in tempi ragionevoli può essere complesso. Il ruolo del Consiglio Esecutivo è quello di pronunciarsi su qualsiasi preoccupazione sollevata da una delle Parti del Trattato. Il Segretariato Tecnico ha il ruolo di garantire che le ispezioni siano condotte da esperti con una profonda conoscenza delle tecnologie di IA all'avanguardia. Il Trattato prevede tempi molto stretti (misurati in ore e giorni) nella speranza che siano sufficientemente rapidi da consentire alle Parti di attendere le decisioni prima di intraprendere Azioni di Protezione (come descritto nell'Articolo XII), sebbene, naturalmente, il Trattato non vieti mai alle Parti di intraprendere le Azioni di Protezione che ritengono necessarie per garantire la propria sicurezza.

---

#### Precedenti

Le procedure di risoluzione delle controversie del nostro Articolo XI si ispirano agli Articoli IX, XII e XIV della [Convenzione sulle Armi Chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf). L'Articolo IX della CAC richiede ai firmatari di rispondere alle richieste di chiarimenti "il prima possibile, ma comunque entro 10 giorni dalla richiesta". Data la rapidità con cui si propagano gli sviluppi in ambito digitale, abbiamo scelto un termine di risposta di 5 giorni, ma è possibile che anche questo periodo debba essere ulteriormente ridotto.

Il nostro Paragrafo 2 di questo Articolo segue il modello dell'Articolo XIV della CAC, che permette al suo Consiglio Esecutivo di "contribuire alla risoluzione di una controversia con qualsiasi mezzo ritenga opportuno, compresa l'offerta dei suoi buoni uffici, invitando gli Stati parti in causa ad avviare il processo di risoluzione di loro scelta e raccomandando un termine per qualsiasi procedura concordata". Le Parti sono inoltre incoraggiate a deferire i casi alla Corte Internazionale di Giustizia, se opportuno.

Analogamente al Paragrafo 3 del nostro Articolo XI, l'Articolo XII della CAC conferisce al Consiglio Esecutivo il potere di raccomandare rimedi, comprese sanzioni, "nei casi in cui le attività vietate dalla presente Convenzione possano causare gravi danni all'oggetto e allo scopo della stessa". Per dare forza a tali raccomandazioni, il Consiglio della CAC deve "portare la questione, comprese le informazioni e le conclusioni pertinenti, all'attenzione dell'Assemblea Generale delle Nazioni Unite e del Consiglio di Sicurezza delle Nazioni Unite". Le raccomandazioni del Consiglio Esecutivo dell'AISI previste dal nostro Trattato possono essere analogamente portate a un'istanza superiore.

---

### Articolo XII: Azioni di Protezione {#articolo-xii:-azioni-di-protezione}

1. Riconoscendo che lo sviluppo della SIA o di altre attività pericolose legate all'intelligenza artificiale, come indicato negli Articoli da IV a IX, potrebbe mettere a rischio la sicurezza globale e la vita di tutti, potrebbe essere necessario che le Parti di questo Trattato prendano misure drastiche per impedire tale sviluppo. Le Parti riconoscono che lo sviluppo della superintelligenza artificiale (SIA), in qualsiasi parte del mondo, rappresenterebbe una minaccia per tutte le Parti. Ai sensi dell'Articolo 51 della Carta delle Nazioni Unite e secondo un precedente consolidato, gli Stati hanno il diritto alla legittima difesa. Data la scala e la rapidità delle minacce legate alla SIA, la legittima difesa potrebbe richiedere azioni preventive per impedire lo sviluppo della SIA.  
2. Per evitare lo sviluppo o l'impiego della SIA, questo Articolo autorizza Azioni di Protezione mirate. Qualora esistano prove credibili che uno Stato o altro attore (sia esso una Parte o una non-Parte) stia conducendo o intenda condurre in modo imminente attività volte allo sviluppo o all'impiego della SIA in violazione dell'Articolo I, dell'Articolo IV, dell'Articolo V, dell'Articolo VI, dell'Articolo VII o dell'Articolo VIII, uno Stato Parte può intraprendere Azioni di Protezione necessarie e proporzionate per impedire tali attività. In considerazione dei danni e della natura escalatoria delle Azioni di Protezione, queste ultime dovrebbero essere utilizzate come ultima risorsa. Al di fuori delle emergenze e delle situazioni urgenti, le Azioni di Protezione devono essere precedute da altri approcci quali, a titolo esemplificativo ma non esaustivo:  
   1. Restrizioni commerciali o sanzioni economiche  
   2. Restrizioni sui beni  
   3. Divieti di viaggio  
   4. Richiesta di intervento al Consiglio di sicurezza delle Nazioni Unite  
3. Le Azioni di Protezione possono includere misure come operazioni informatiche per sabotare lo sviluppo dell'IA, l'interdizione o il sequestro di cluster di chip designati, azioni militari per disabilitare o distruggere hardware per l'IA, la messa fuori uso fisica di strutture o beni specifici che consentono direttamente lo sviluppo dell'IA, e metodi per impedire ai ricercatori di lavorare su Ricerche Soggette a Restrizioni.  
4. Le Parti devono ridurre al minimo i danni collaterali, inclusi i danni a civili e servizi essenziali, ove praticabile, nel rispetto dei requisiti della missione.  
5. Le Azioni di Protezione devono essere strettamente limitate alla prevenzione dello sviluppo o dell'impiego della SIA e non devono essere usate come pretesto per acquisire territori, cambiare regimi, estrarre risorse o raggiungere obiettivi militari più ampi. L'occupazione permanente o l'annessione di territorio è vietata. L'azione cesserà una volta che l'AISI avrà verificato che la minaccia non esiste più.  
6. Ogni Azione di Protezione deve essere accompagnata, al suo avvio o non appena le condizioni di sicurezza lo consentono, da una pubblica Dichiarazione di Azione di Protezione che:  
   1. Spieghi lo scopo protettivo dell'azione;  
   2. Identifichi le specifiche attività e risorse abilitanti per l'IA prese di mira;  
   3. Indichi le condizioni per la cessazione;  
   4. Si impegni a cessare le proprie attività una volta soddisfatte tali condizioni.  
7. Le Azioni di Protezione cesseranno senza indugio al verificarsi di una delle seguenti condizioni:  
   1. Certificazione da parte dell'AISI che le attività in questione siano cessate.  
   2. La resa o la distruzione verificate dei cluster di chip coperti o delle risorse che permettono la SIA, potenzialmente inclusa la predisposizione di tutele adeguate a prevenire attività di Ricerca Soggetta a Restrizioni.  
   3. Una determinazione da parte della Parte agente, comunicata all'AISI, secondo cui la minaccia si è attenuata.  
8. Le Parti non considerano le misurate Azioni di Protezione intraprese da un'altra Parte ai sensi del presente Articolo come atti provocatori, e non intraprendono rappresaglie o sanzioni su tale base. Le Parti convengono che le Azioni di Protezione conformi ai requisiti di cui sopra non possano essere interpretate come un atto di aggressione o una giustificazione per l'uso della forza.  
9. Il Consiglio Esecutivo esamina ogni Azione di Protezione per verificarne la conformità al presente Articolo e riferisce alla Conferenza delle Parti. Se il Consiglio Esecutivo ritiene che un'azione non fosse necessaria, proporzionata o adeguatamente mirata, possono essere intraprese azioni ai sensi dell'Articolo XI, Paragrafo 3\.

---

#### Note

Il Trattato vero e proprio che verrà firmato potrebbe non essere così esplicito sulla necessità di Azioni di Protezione contro gli Stati che intraprendono lo sviluppo di SIA, lasciando invece questo aspetto implicito, come spesso accade in accordi simili. Abbiamo scelto di essere espliciti su questo punto perché questo regime di deterrenza è fondamentale per l'efficacia del Trattato, ed esplicitarlo contribuisce alla chiarezza. Questa esplicitezza ci permette anche di includere misure che possano contribuire a prevenire l'uso improprio delle Azioni di Protezione, tra cui una descrizione più dettagliata di quando tali Azioni sono accettabili. È importante che tutti i firmatari comprendano il regime di deterrenza implicito e le conseguenze dell'inadempienza.

Come [discusso in precedenza](#alcune-nazioni-non-rifiuterebbero-il-divieto?), una volta che i leader mondiali avranno compreso la minaccia rappresentata dalla SIA, saranno probabilmente disposti a intraprendere azioni per fermare lo sviluppo di IA fuori controllo, compresi interventi militari limitati. Le azioni militari, come gli attacchi aerei mirati, dovrebbero essere considerate l'ultima risorsa per impedire lo sviluppo della SIA, una volta falliti tutti gli altri tentativi diplomatici. Ma è importante che siano un'opzione reale, affinché il sistema di deterrenza e conformità possa reggere.

Sottolineiamo che qualsiasi uso della forza dovrebbe essere mirato a prevenire la SIA, e dovrebbe cessare una volta che la minaccia sia stata chiaramente rimossa. Il presente Articolo chiarisce che i firmatari non impediranno l'adozione di ragionevoli Azioni di Protezione da parte di altre Parti, ma anche queste Azioni dovranno essere revisionate per garantire che non si abusi di questo Articolo.

---

#### Precedenti

L'idea che gli Stati-nazione possano adottare misure protettive per la propria sicurezza è una realtà a prescindere dai precedenti, ma un caso della sua codificazione nel diritto internazionale è il [Capitolo VII della Carta delle Nazioni Unite](https://www.un.org/en/about-us/un-charter/chapter-7), che stabilisce che il Consiglio di Sicurezza può adottare misure militari o non militari per mantenere la pace e la sicurezza internazionali, quando necessario.

Il concetto di Azioni di Protezione, come appare nella bozza sopra riportata, trova ulteriore fondamento in precedenti storici in cui gli Stati hanno agito, individualmente o collettivamente, per impedire lo sviluppo di tecnologie ritenute una minaccia alla sicurezza internazionale. Queste azioni vanno dalle sanzioni agli attacchi informatici e militari.

L'impegno internazionale per impedire all'Iran di sviluppare armi nucleari ne è un chiaro esempio moderno. Il Consiglio di sicurezza delle Nazioni Unite ha più volte [imposto](https://www.cnn.com/2012/01/23/world/meast/iran-sanctions-facts/index.html) sanzioni all'Iran a causa del suo programma nucleare, in gran parte revocate dopo che l'Iran ha accettato di limitare tale programma nel [Piano d'Azione Congiunto Globale del 2015](https://main.un.org/securitycouncil/en/content/2231/background).

Gli Stati Uniti e Israele avrebbero collaborato allo sviluppo di Stuxnet, un'arma informatica altamente sofisticata che nel 2010 ha distrutto molte delle centrifughe iraniane per l'arricchimento dell'uranio.

Nel [giugno 2025](https://www.armscontrol.org/act/2025-07/news/israel-and-us-strike-irans-nuclear-program), Israele ha lanciato attacchi aerei contro molte delle strutture nucleari iraniane, seguiti nove giorni dopo da attacchi aerei statunitensi mirati in parte a mettere fuori uso l'impianto di arricchimento dell'uranio di Fordow.

Un altro precedente storico per le Azioni di Protezione è la risposta internazionale all'inadempienza nucleare dell'Iraq negli anni '90. Dopo la guerra del Golfo del 1991, fu istituita la [Commissione Speciale delle Nazioni Unite](https://www.un.org/depts/unscom/) (UNSCOM) per supervisionare la distruzione delle armi di distruzione di massa dell'Iraq. L'inadempienza al regime di ispezione dell'UNSCOM portò infine, nel 1998, all'[Operazione Desert Fox](https://www.afhistory.af.mil/FAQs/Fact-Sheets/Article/458976/1998-operation-desert-fox/), una campagna di bombardamenti volta a indebolire la capacità dell'Iraq di produrre armi di distruzione di massa.

---

### Articolo XIII: Revisioni dell'AISI {#articolo-xiii:-revisioni-dell-aisi}

1. Per i modelli di IA creati tramite addestramento dichiarato o post-addestramento entro i limiti dell'Articolo IV, l'AISI può richiedere valutazioni e altri test. Questi test serviranno a determinare se le soglie fissate negli Articoli IV, V, VII e VIII debbano essere riviste. I metodi utilizzati per le revisioni saranno stabiliti dall'AISI e potranno essere aggiornati.  
2. Le valutazioni saranno effettuate presso le strutture dell'AISI o i CCC monitorati, da funzionari dell'AISI. I funzionari delle Parti del Trattato potranno essere informati in merito ai test condotti e l'AISI potrà fornire una sintesi dei risultati dei test. Le Parti non avranno accesso ai modelli di IA che non hanno addestrato, a meno che l'accesso non sia concesso dal proprietario del modello, e l'AISI adotterà misure per garantire la sicurezza delle informazioni sensibili.  
3. L'AISI può condividere informazioni dettagliate con le Parti o il pubblico, qualora il Direttore Generale lo ritenga necessario per ridurre il rischio di estinzione umana causata dall'IA avanzata.

---

#### Note

Lo scopo di questo Articolo è garantire che l'AISI si mantenga aggiornata sullo stato dell'IA, in caso di suoi progressi. Ad esempio, la revisione dell'addestramento dichiarato permetterebbe all'AISI di comprendere il livello di potenza dell'IA che si può raggiungere con diversi livelli di FLOP di addestramento.

Anche se la ricerca algoritmica è vietata, potrebbero esserci progressi che non possono essere fermati in modo efficace, e l'AISI deve tenerne traccia. Inoltre, l'AISI deve monitorare i progressi nella*sollecitazione* delle capacità. Per esempio, potrebbero essere scoperti nuovi metodi di prompting che consentono a una vecchia IA di ottenere prestazioni molto migliori su alcuni parametri di valutazione critici. Questi sono solo due esempi di cambiamenti nel panorama dello sviluppo dell'IA che potrebbero richiedere modifiche alle soglie rilevanti per l'Articolo IV e l'Articolo V, nonché modifiche alle definizioni di Ricerca Soggetta a Restrizioni nell'Articolo VIII. Le revisioni previste dall'Articolo XIII sono un meccanismo che consente all'AISI di comprendere meglio lo stato delle capacità dell'IA e di rispondere in modo appropriato.

Un articolo del genere potrebbe non essere strettamente necessario, considerati i divieti sui grandi cicli di addestramento e sui progressi algoritmici. Tuttavia, le revisioni appaiono una misura prudente e sembrano essere parte del modo in cui il mondo potrebbe continuare a usare le moderne IA come ChatGPT senza rischiare una corsa alla superintelligenza.

Queste revisioni dell'AISI potrebbero includere valutazioni di capacità pericolose, per assicurarsi che le IA non diventino eccessivamente potenti in aree specifiche. Potrebbero anche esaminare i dati di addestramento per garantire che le IA non vengano addestrate per compiti specificamente pericolosi (come l'automazione della ricerca sull'IA), o altrimenti testare l'IA per rilevare comportamenti inaspettati.

---

#### Precedenti

I precedenti per i test con supervisione richiesti dall'AISI sono analoghi a quelli relativi alla verifica dell'uso dei chip discussi nell'Articolo VII, con il protocollo di condivisione della telemetria dei missili dello START I come caso particolarmente rilevante. L'elemento aggiuntivo qui, nel nostro Articolo XIII, è l'utilizzo dei dati raccolti per formulare raccomandazioni su potenziali correzioni delle soglie (che potrebbero avvenire tramite i meccanismi con precedenti di cui discutiamo nell'Articolo XIV).

Per quanto riguarda la tensione intrinseca tra la divulgazione al pubblico (Paragrafo 3) e le disposizioni sul consolidamento delle informazioni del nostro Articolo X, notiamo che la disposizione sulla riservatezza dell'Articolo VII dello [Statuto dell'AIEA](https://www.iaea.org/about/statute)[^263] non ha impedito all'agenzia di pubblicare [rapporti regolari e dettagliati](https://www.iaea.org/publications/reports) sui principali sviluppi nel suo campo di competenza e sulle loro implicazioni per la sicurezza globale.

---

### Articolo XIV: Procedura di revisione del Trattato {#articolo-xiv:-procedura-di-revisione-del-trattato}

1. Qualsiasi Stato Parte può proporre emendamenti al presente Trattato. Per "emendamenti" si intendono le revisioni del corpo principale e degli Articoli del Trattato. Gli emendamenti includono revisioni allo scopo degli Articoli del Trattato. Ai sensi dell'Articolo III, il Segretariato Tecnico dell'AISI, senza veto da parte del Consiglio Esecutivo, può modificare definizioni specifiche e metodi di attuazione, come quelli relativi agli Articoli IV, V, VI, VII, VIII, IX e X. Le revisioni fondamentali allo scopo di questi Articoli o alle procedure di voto richiedono un Emendamento.  
2. Le proposte di emendamento saranno sottoposte al Direttore Generale dell'AISI e comunicate agli Stati Parti.  
3. Affinché un emendamento possa essere formalmente considerato, è necessario il sostegno di almeno un terzo degli Stati Parti.  
4. Gli emendamenti al corpo principale del Trattato non sono ratificati finché non vengono accettati da tutti gli Stati Parte (senza voti contrari).  
5. Se il Consiglio Esecutivo raccomanda a tutti gli Stati Parte di adottare la proposta, le modifiche saranno considerate approvate se nessuno Stato Parte le respinge entro 90 giorni.  
6. Tre anni dopo l'entrata in vigore del presente Trattato, si terrà a Ginevra, in Svizzera, una Conferenza delle Parti per esaminare il funzionamento del Trattato al fine di garantire che gli obiettivi del Preambolo e le disposizioni del Trattato siano realizzati. Successivamente, a intervalli di tre anni, le Parti del Trattato convocheranno ulteriori conferenze con il medesimo obiettivo di riesaminare il funzionamento del Trattato.

---

#### Note

Questo Articolo definisce il processo per apportare revisioni sostanziali alla struttura del Trattato. Tali revisioni richiedono un notevole sostegno da parte delle Parti ed è previsto un quorum elevato per la loro approvazione. Al contrario, le modifiche ai dettagli di implementazione di gran parte del Trattato possono essere apportate molto più facilmente, come descritto nell'Articolo III, in linea con la rapida evoluzione del settore dell'IA. Le revisioni più ampie allo scopo del Trattato possono essere soggette a processi più lenti, come quello qui descritto.

---

#### Precedenti

Il TNP ha un processo di modifica rigido, che richiede l'approvazione da parte della «maggioranza dei voti di tutte le Parti del Trattato». Ciò rende, volutamente, le modifiche formali estremamente difficili. La nostra bozza di trattato segue questo precedente con l'obiettivo di rafforzare l'accordo contro le pressioni a breve termine volte ad allentare le soglie o a indebolire le disposizioni.

I trattati difficili da modificare (e quindi da indebolire) si affidano ad altri meccanismi per essere rafforzati secondo necessità. Il TNP non è mai stato modificato, ma è stato adattato attraverso la Conferenza di Riesame quinquennale prevista dall'Articolo VIII, in cui vengono raggiunti accordi consensuali «al fine di garantire che gli scopi del Preambolo e le disposizioni del Trattato siano realizzati».

Allo stesso modo, l'Articolo XII della [Convenzione sulle Armi Biologiche](https://treaties.unoda.org/t/bwc) del 1975 fa affidamento sulle sue Conferenze di Revisione quinquennali per rafforzare il trattato attraverso misure non vincolanti di rafforzamento della fiducia, poiché gli emendamenti formali sono rari. Il nostro accordo stabilisce una conferenza a cadenza triennale, dato che l'IA è un campo soggetto a rapidi mutamenti; tale periodo potrebbe dover essere ulteriormente ridotto.

L'Articolo XV della [Convenzione sulle Armi Chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) distingue tra emendamenti e modifiche amministrative o tecniche, prevedendo per queste ultime disposizioni di approvazione meno rigide. Una formulazione simile potrebbe essere aggiunta alla nostra bozza di accordo per garantire un certo livello di flessibilità nella gestione dei futuri sviluppi nel campo dell'IA.

L'Articolo XV del [Trattato sullo Spazio Extra-Atmosferico](https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) contiene una clausola di emendamento, ma non è mai stato formalmente emendato; sono stati invece negoziati nuovi trattati per affrontare le questioni spaziali emergenti. Questa potrebbe essere un'altra opzione per porre rimedio alle debolezze che potrebbero manifestarsi in un trattato sull'IA.

---

### Articolo XV: Recesso e durata {#articolo-xv:-recesso-e-durata}

1. Il Trattato ha una durata illimitata.  
2. Ogni Parte, nell'esercitare la propria sovranità nazionale, ha il diritto di recedere dal Trattato se ritiene che eventi straordinari, relativi all'oggetto del presente Trattato, abbiano messo a rischio gli interessi supremi del proprio Paese. Dovrà notificare tale recesso all'AISI con un preavviso di 12 mesi.  
3. Durante questo periodo di 12 mesi, lo Stato recedente dovrà collaborare con l'AISI al fine di certificare che, dopo il recesso, non sarà in grado di sviluppare, addestrare, post-addestrare o rilasciare sistemi di IA pericolosi, inclusi i sistemi di SIA o i sistemi che superano le soglie previste dal Trattato. Gli Stati recedenti riconoscono che tale cooperazione agevola l'AISI e le Parti nell'evitare il ricorso all'Articolo XII.  
   1. In particolare, lo Stato recedente, sotto la supervisione dell'AISI, rimuoverà dal proprio territorio tutti i cluster di chip contemplati e le risorse abilitanti per la SIA (ad esempio, le attrezzature avanzate per la produzione di chip), per trasferirli sotto il controllo approvato dall'AISI, o li renderà permanentemente inattivi (come descritto nell'Articolo V).  
4. Nessuna disposizione del presente Articolo limita l'applicabilità dell'Articolo XII. Uno Stato receduto (e che quindi non è una Parte) rimane soggetto ad Azioni di Protezione se prove credibili indicano attività volte allo sviluppo o al rilascio della SIA.

---

#### Note

Considerati i pericoli della ricerca e dello sviluppo della SIA, nonché il rischio che se un Paese decidesse di recedere dal Trattato e di lanciarsi nella corsa alla superintelligenza, altri potrebbero seguirlo, è necessario porre delle barriere al recesso dal Trattato.

In pratica, questo è complesso. La Corea del Nord, per esempio, si è ritirata dal TNP per continuare le sue attività di proliferazione nucleare, anche a costo di risoluzioni del Consiglio di Sicurezza dell'ONU e delle sanzioni associate. Le conseguenze non si sono rivelate sufficienti a indurre la Corea del Nord a interrompere le sue attività di proliferazione.

Qualora le nazioni desiderino recedere dal Trattato, il Trattato stabilisce chiaramente che, agli occhi di tutte le Parti, esse rinunciano al diritto all'infrastruttura di IA e saranno soggette alle Azioni di Protezione di cui all'Articolo XII. Qualsiasi ulteriore negoziato sulla questione della SIA (ad esempio, per evitare le Azioni di Protezione) dovrà essere condotto dalle Parti interessate.

Se il ritiro rappresentasse una preoccupazione, le Parti potrebbero includere meccanismi per rendere più difficile un'uscita pericolosa dal Trattato. Ad esempio, funzionari statunitensi e cinesi potrebbero accordarsi per installare reciprocamente degli interruttori di spegnimento all'interno dei data center conservati, in cui entrambe le parti avrebbero il potere di chiudere definitivamente un data center a propria discrezione. In alternativa, si potrebbe adottare un regime di licenze multilaterale in cui tutti i nuovi chip di IA vengano fabbricati con [blocchi hardware](https://arxiv.org/abs/2506.15093) che richiedano l'approvazione di più parti per continuare a funzionare, in modo che, se un Paese si ritirasse dal Trattato, gli altri potrebbero smettere di approvare le sue licenze. Un'altra idea consiste nello spostare le infrastrutture chiave per l'IA in paesi terzi, dove potrebbero essere confiscate o distrutte qualora una delle Parti si ritirasse. La nostra bozza si limita a metodi di deterrenza minimi, ma ne sono disponibili molti altri (o potrebbero diventare tali grazie all'innovazione tecnologica).

Questa bozza di trattato si concentra sulla prevenzione dello sviluppo pericoloso dell'IA e non propone una particolare visione positiva su come tale sviluppo potrebbe eventualmente continuare. Un esempio di visione positiva potrebbe essere l'investimento congiunto nel potenziamento umano. Tuttavia, [non è necessario essere d'accordo su questi dettagli per convenire sulla necessità di fermare la corsa alla superintelligenza](#possiamo-collaborare-per-fermare-la-superintelligenza-pur-essendo-in-disaccordo-sul-potenziamento-umano.), perciò qui omettiamo tali dettagli.

---

#### Precedenti

È prassi comune che i trattati non abbiano una data di scadenza. Il primo Paragrafo dell'[Articolo XVI della CAC](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) recita: "La presente Convenzione ha durata illimitata".

I trattati a durata illimitata non durano necessariamente per sempre.[^264] Tuttavia, di norma prevedono un meccanismo di recesso, solitamente con un periodo di preavviso obbligatorio e altre clausole che consentono di recedere in un modo che desti minori preoccupazioni alle Parti restanti. [L'Articolo XVI della CAC](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) permette a una Parte di ritirarsi "se decide che eventi straordinari, legati all'oggetto della presente Convenzione, abbiano messo a rischio gli interessi supremi del proprio Paese". Il Paese che si ritira deve dare un preavviso di 90 giorni. L'Articolo XVI del Trattato sullo Spazio Extra-Atmosferico (https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) richiede un preavviso di un anno per il ritiro.

La nostra bozza di trattato prevede un preavviso di 12 mesi da parte delle Parti recedenti, lasciando così tempo sufficiente per collaborare alle misure di garanzia di cui al Paragrafo 3. Il nostro intento con queste misure (che vanno oltre a quanto si riscontra comunemente nelle disposizioni storiche in materia di recesso) è ridurre la potenziale necessità di Azioni di Protezione contro la Parte recedente, poiché a nessuna Parte o non-Parte può essere consentito di creare una SIA o di indebolire la capacità del mondo di prevenirne la creazione.

Un precedente storico di una Parte recedente rimasta soggetta a misure protettive è il caso della [Risoluzione 1718 del Consiglio di Sicurezza delle Nazioni Unite](https://main.un.org/securitycouncil/en/s/res/1718-%282006%29), che impose sanzioni contro la Corea del Nord dopo il suo test nucleare del 2006, nonostante il precedente ritiro della Corea del Nord dal TNP.

---

[^1]:  \#Raccontiamo parte della storia di Leo Szilard in una [discussione approfondita](#quando-leo-szilard-vide-il-futuro).

[^2]:  \**sottraendo:* Da [Bloomberg](https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million), luglio 2025: Il CEO di Meta, Mark Zuckerberg, ha assunto più di dieci ricercatori di OpenAI, oltre a ricercatori e ingegneri di punta di Anthropic, Google e altre startup.

[^3]:  \\**affermò con sicurezza:* Citando l'articolo del 1903 "[Flying Machines Which Do Not Fly](https://www.nytimes.com/1903/10/09/archives/flying-machines-which-do-not-fly.html)":  

> La macchina fa solo quello che deve fare, obbedendo alle leggi naturali che agiscono sulla materia passiva. Quindi, se sono necessari, diciamo, mille anni per rendere agevole il volo di un uccello partito con ali rudimentali, o diecimila per uno che ha iniziato senza ali e ha dovuto svilupparle da zero, si potrebbe presumere che la macchina volante che volerà davvero possa essere fatta evolvere grazie agli sforzi combinati e continui di matematici e meccanici in un periodo compreso tra un milione e dieci milioni di anni — a patto, ovviamente, che nel frattempo si possano eliminare piccoli inconvenienti e imbarazzi come l'attuale rapporto tra peso e resistenza dei materiali inorganici. Senza dubbio il problema è interessante per chi se ne occupa, ma all'uomo comune sembrerebbe che lo sforzo potrebbe essere impiegato in modo più proficuo.

[^4]: Sì, le IA possono persino [riconoscere l'ironia](https://x.com/AnthonyNAguirre/status/1923535891781517355) del *New York Times* che riferisce che non sono in grado di riconoscere l'ironia. (Per correttezza con il *New York Times*, alcuni dei loro giornalisti trattano l'IA con [maggiore chiarezza](https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html).)

[^5]: Se il libro dovesse avere un successo tale da ripagare tutti quegli investimenti, il nostro contratto prevede una clausola secondo cui gli autori potranno eventualmente condividere i profitti con il MIRI, una volta che il MIRI sarà stato ampiamente ripagato per il suo impegno. Tuttavia, il MIRI ha dedicato così tanto impegno al libro che, a meno che non superi di gran lunga le nostre aspettative, non ne vedremo mai un centesimo.

[^6]:  \**non succede mai niente:* La frase "non succede mai niente" sembra essere comune tra chi partecipa ai mercati delle previsioni. L'euristica stessa è discussa, ad esempio, dal blogger Scott Alexander nel suo saggio [Heuristics That Almost Always Work](https://www.astralcodexten.com/p/heuristics-that-almost-always-work).

[^7]:  \**non ci siano limiti:* Si veda, ad esempio, l'articolo [Eternity in six hours](https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148?via%3Dihub), che discute i limiti della colonizzazione intergalattica dati i vincoli delle leggi fisiche conosciute.

[^8]: Oppure avranno costruito strumenti o successori per compiere le esplorazioni, nel modo che riterranno più conveniente, sfruttando i vantaggi di una scienza e una tecnologia più avanzate.

[^9]:  La pratica di tentare di assegnare una "p(catastrofe)" ci desta qualche preoccupazione. Assegnare una singola probabilità — invece di più probabilità, ciascuna delle quali presuppone una diversa risposta che la società potrebbe scegliere — ci sembra disfattista. C'è una differenza enorme tra qualcuno che ha una p(catastrofe) alta perché pensa che il mondo *non possa* prevenire la catastrofe e qualcuno che ha una p(catastrofe) alta perché pensa che il mondo *possa* prevenire la catastrofe ma *non lo farà*.  
  
Se si scopre che la maggior parte delle persone ha una p(catastrofe) elevata per la seconda ragione, ma tutti presumono che sia per la prima, allora le loro stesse dichiarazioni potrebbero fungere da profezia che si autoavvera, mettendoci sulla strada di un disastro che era del tutto evitabile. 
  
Abbiamo anche l'impressione che molte persone nella Silicon Valley si scambino i numeri "p(catastrofe)" un po' come figurine dei calciatori, in un modo che spesso sembra lontano dalla realtà. Pensandoci un attimo, anche una probabilità bassa come il cinque per cento di *uccidere ogni essere umano sul pianeta* dovrebbe essere un ovvio motivo di estremo allarme. È ben oltre il livello di minaccia che servirebbe per giustificare la chiusura *immediata* dell'intero campo dell'IA. Le persone sembrano perdere di vista questa realtà sorprendentemente in fretta, una volta che prendono l'abitudine di scambiarsi macabramente i numeri p(catastrofe) alle feste, come se fossero una divertente storia di fantascienza e non un'affermazione su *ciò che accadrà realmente a tutti noi*.  

Questo non vuol dire che i numeri p(catastrofe) delle persone siano vicini alla realtà. Ma, come minimo, bisognerebbe interpretarli come la segnalazione, da parte di esperti di tutto il settore, che stiamo affrontando una vera emergenza.

[^10]: Contrariamente a quanto dice Hinton all'inizio del video, la fiducia di Yudkowsky riguardo ai pericoli non è del "99,999%"; cinque nove costituirebbero un grado di fiducia folle.

[^11]:  \**attraversando l'incrocio:* Un profilo e una cronologia più completi sono curati dalla [Atomic Heritage Foundation](https://ahf.nuclearmuseum.org/ahf/profile/leo-szilard/).

[^12]:  Di fronte a queste critiche rivolte a Fermi, abbiamo visto persone difenderlo inventando ragioni per cui è del tutto plausibile che Fermi abbia riflettuto a lungo prima di dire "Sciocchezze!". Ad esempio, sostengono che Fermi sapesse che la Terra non era mai esplosa in una cascata di radioattività indotta, cosa che, qualcuno potrebbe pensare, la Terra avrebbe già dovuto fare, se quel tipo di cascate fossero fisicamente possibili.  

Questi tipi di argomentazioni, ovviamente, conducono a una conclusione errata. Fermi si sbagliava sulle reazioni nucleari a catena. Ciò detto, la lezione da imparare dall'esistenza di argomentazioni simili è che "si possono sempre escogitare argomentazioni che sembrino altrettanto plausibili contro la verità di cose che sono di fatto vere". Il fatto che la Terra non sia ancora esplosa non costituisce una prova forte che i reattori nucleari siano impossibili; gli ingegneri possono disporre con cura gli atomi per provocarne la scissione di proposito. Tali argomentazioni, quindi, non supportano una conclusione tanto errata quanto un semplice "Sciocchezze!".

[^13]:  \**non è universale:* Una definizione formale di "intelligenza universale" è stata proposta da [Legg e Hutter](https://arxiv.org/abs/0712.3329) nel 2007\.

[^14]:  \**abbiamo sentito dire:* Per un esempio di una simile critica, si veda l'articolo di Ernest Davis [Linee guida etiche per una superintelligenza](https://cs.nyu.edu/~davise/papers/Bostrom.pdf).

[^15]: Da altri punti di vista, il progresso appare piuttosto discontinuo. La vittoria di AlphaGo su Lee Sedol nel gioco del Go è stata una sorta di shock per il mondo, nonostante i ricercatori possano mostrare a posteriori, tramite grafici, come i diversi metodi di IA stessero costantemente migliorando in dietro le quinte. Lo stesso vale per la rivoluzione dei modelli linguuistici di grandi dimensioni: i ricercatori possono tracciare grafici che mostrano come l'architettura del trasformatore non abbia rappresentato un progresso *così* grande rispetto alle architetture concorrenti, ma il risultato pratico è che le IA sono diventate qualitativamente più utili. Ma per ora mettiamo da parte questo punto di vista.

[^16]:  Almeno, secondo le misurazioni di METR, un istituto che si occupa di valutazione dei modelli (IA) e ricerca sulle minacce, il quale ha pubblicato alcuni risultati di ricerca [sul suo blog](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) nel marzo 2025.

[^17]: In questo caso, la crescita esponenziale non è proprio rassicurante. Se i batteri in una capsula di Petri raddoppiano ogni ora, ci vorrà un giorno o due prima che la colonia sia visibile a occhio nudo, e dopodiché basteranno poche ore perché ricopra l'intera capsula. Quando ci si accorge del fenomeno, la maggior parte del tempo a disposizione è già trascorso. Come recita un [detto](https://x.com/ConanMacDougall/status/1729196049137549521), ci sono solo due modi per reagire a un cambiamento esponenziale: troppo presto o troppo tardi. Ma, in ogni caso, la curva è abbastanza regolare e prevedibile.

[^18]:  \**tre o quattro volte più grande:* Non ci vuole molto perché le IA crescano di tre o quattro volte. Al momento del suo rilascio ufficiale, GPT-2 aveva circa [1,5 miliardi di parametri](https://openai.com/index/gpt-2-1-5b-release/). GPT-3 aveva [175 miliardi di parametri](https://arxiv.org/pdf/2005.14165). Il numero ufficiale di parametri di GPT-4, per quanto ne sappiamo, non è stato ancora reso noto, ma è improbabile che sia *inferiore* a quello del suo predecessore; secondo una stima non ufficiale, si tratterebbe di circa [1.800 miliardi di parametri](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). In altre parole: l'intelligenza artificiale è cresciuta di mille volte nell'arco di quattro anni.

[^19]:  Nel capitolo 1 abbiamo notato che i transistor dei computer possono accendersi e spegnersi miliardi di volte al secondo, mentre anche i neuroni biologici più veloci si attivano solo un centinaio di volte al secondo. Questo suggerisce che, con l'hardware attuale, anche se occorressero mille operazioni dei transistor per fare lo stesso lavoro di un picco neurale, le IA potrebbero comunque pensare 10.000 volte più velocemente di un essere umano.  

Per approfondire il confronto: questo confronto non vuole essere una previsione sul numero di operazioni dei transistor necessarie per implementare una simulazione completa di un neurone biologico fino al livello dei neurotrasmettitori (e sicuramente non fino al livello delle proteine o degli atomi). Stiamo invece sottolineando quanto velocemente possa essere svolto, in linea di principio, il lavoro astratto del pensiero umano, utilizzando i transistor moderni come limite inferiore su un aspetto della questione: "Cosa è fisicamente possibile?".  

Per spiegare meglio il concetto: esiste un modello ingenuo del cervello umano in cui, in un dato momento, ogni neurone è attivo o inattivo. Possiamo immaginare di usare un gran numero di transistor per catturare questo ipotetico stato cerebrale "Quali neuroni sono attualmente attivi?", e poi usare circuiti cablati per implementare le regole di transizione che stabiliscono quali neuroni saranno attivi nel momento successivo.  

Un dispositivo del genere funzionerebbe alla velocità dei transistor, ma probabilmente la sua fedeltà non sarebbe abbastanza alta per fare davvero quello che fa il cervello umano: i neuroni non sono sempre "attivi" o "non attivi", i diversi picchi neurali aumentano e diminuiscono a velocità diverse. (Inoltre, un cervello del genere non può imparare, perché le regole di transizione sono fisse.)  

Il punto dell'illustrazione "1.000 operazioni dei transistor per picco neurale" è: supponiamo che occorrano centinaia di transistor per rappresentare lo stato di attivazione di un singolo neurone (cioè il suo stato di "picco" a diverse intensità). Supponiamo inoltre che quelle centinaia di transistor debbano cambiare il loro stato 1.000 volte diverse, in serie, ogni volta che un neurone ha un picco (ad esempio, per rappresentare un impulso con intensità variabile lungo il suo percorso, dove l'intensità è influenzata in modo dinamico da 999 diverse interazioni che ha lungo il suo percorso). In questo caso, un cervello digitale sarà comunque in grado di eseguire pensieri in modo simile a quello umano 10.000 volte più velocemente di qualsiasi essere umano, perché i transistor possono commutare 1.000 volte di fila, 10.000 volte per ogni picco neurale umano.  

Queste ipotesi sembrano molto generose. In pratica dicono: "Supponiamo che l'intensità di un picco neurale debba essere letta *mille volte di seguito* per catturarne l'effetto, con *ogni lettura che influenza la successiva in modo dinamico* e che non possa essere bypassata da circuiti dedicati". Anche in questo caso estremo, utilizzando solo l'hardware informatico già esistente nel 2025, le menti digitali potrebbero comunque essere di gran lunga più veloci di quelle biologiche.  

Questa analogia parla solo della fedeltà seriale necessaria per codificare le informazioni trasmesse da un picco neurale nei cervelli biologici; non stiamo parlando del calcolo necessario per decidere se generare o meno un picco. Per quanto ne sappiamo, non c'è consenso tra gli scienziati su quanti transistor servano per simulare un neurone che sceglie se attivarsi, ma saremmo sorpresi se la profondità seriale minima possibile di quel grafico (con il maggior numero possibile di circuiti dedicati) richiedesse ben più di mille commutazioni di transistor in serie. (Come regola generale, la computazione biologica tende a essere molto più parallela che seriale).  

Tutto ciò si somma al risultato intuitivo che i computer, in genere, possono eseguire i calcoli molto, molto più velocemente degli umani, non appena questi ultimi capiscono come far fare ai computer il tipo di calcoli corretto. Ecco perché, ad esempio, le comuni calcolatrici sono così utili.

[^20]:  \**comunicatori limitati:* Uno dei più famosi è [ELIZA](https://web.njit.edu/~ronkowit/eliza.html), ampiamente ritenuto il primo chatbot.

[^21]:  **con la crescita più rapida:* Secondo l'analisi della Union Bank of Switzerland, e come riportato da testate giornalistiche quali [Business Insider](https://www.businessinsider.com/chatgpt-may-be-fastest-growing-app-in-history-ubs-study-2023-2).

[^22]:  \**molti più ricercatori:* Gli investimenti privati nell'intelligenza artificiale nel 2025 saranno [oltre venti volte superiori](https://ourworldindata.org/grapher/private-investment-in-artificial-intelligence) rispetto al 2012, e il numero di team di ricerca è [aumentato di sei volte](https://ourworldindata.org/grapher/affiliation-researchers-building-artificial-intelligence-systems-all), e la stragrande maggioranza di questo aumento è costituita da team del settore industriale dell'IA. Le principali conferenze sull'intelligenza artificiale sono [da nove a dieci volte più grandi](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences) rispetto al 2012\. 

[^23]:  \**Pokémon:* Per un'analisi di come una particolare IA se la sia cavata nel videogioco a marzo 2025, e dei punti in cui si è bloccata, è disponibile un [post sul blog LessWrong.com](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon).

[^24]: Questa risposta indica un interessante stato epistemologico. Quando credete che Stockfish sia più bravo di voi a scacchi, le vostre migliori previsioni sulle singole mosse di Stockfish non riescono a cogliere pienamente le vostre convinzioni sull'esito finale della partita.  

Un filosofo della scienza potrebbe chiedersi come ciò sia possibile, dato che le regole degli scacchi sono ben note e il risultato dipende esattamente da ogni singola mossa. La risposta è che esiste una vasta struttura logica di possibili partite di scacchi, che in un certo senso è completamente determinata dalle regole degli scacchi, ma che non è completamente nota a voi (né tantomeno a Stockfish!) perché la vostra mente non è in grado di visualizzare tutte le conseguenze che derivano dalle regole degli scacchi.  

Un giocatore di scacchi "più intelligente" può essere considerato come qualcuno che conosce relativamente più verità di voi su questo spazio di possibilità degli scacchi; quindi, quando vedete una mossa sorprendente da parte di un giocatore di scacchi "più intelligente", ciò implica un fatto nuovo per voi riguardo a quelle conseguenze sconosciute delle regole conosciute degli scacchi, che a sua volta incide sulle vostre aspettative riguardo al risultato della partita.  

(Ci si aspetterebbe che i paragrafi precedenti fossero un concetto standard nell'informatica. Con nostra sorpresa, non è così. La maggior parte dell'informatica, e in effetti la maggior parte del mondo accademico finora, non si è interessata particolarmente alle idee associate all'intelligenza superumana.)

[^25]:  \#Per ulteriori informazioni su questa idea, si consulti la discussione approfondita intitolata "[Lo stesso lavoro può essere fatto in molti modi diversi](#lo-stesso-lavoro-può-essere-fatto-in-molti-modi-diversi)".

[^26]: A seconda delle vostre idee psicologiche e filosofiche, potreste pensare che questi argomenti siano collegati. Noi siamo piuttosto scettici riguardo a una connessione forte e stretta; ma se una connessione esiste, sembra comunque utile distinguere esplicitamente questi diversi argomenti. Se, per esempio, l'auto-modifica risultasse inestricabilmente legato alla coscienza, questo sarebbe un fatto importante che dovrebbe essere discusso e approfondito esplicitamente, non un'ipotesi che dovremmo dare per scontata fin dall'inizio.

[^27]:  \**minacciare di uccidersi:* [Gli utenti](https://x.com/venturetwins/status/1936483773035798906) [riferiscono](https://x.com/DuncanHaldane/status/1937204975035384028) che l'IA Gemini di Google minaccia di disinstallarsi dai progetti quando ha dei problemi.

[^28]:  O, per dirla in un altro modo: supponiamo che ad Alice piaccia la pizza con il salame piccante e che odi l'ananas, mentre a Bob piaccia l'ananas e non gli piaccia il salame piccante. Per valutare appieno quanto siano competenti Alice e Bob, bisognerebbe sapere verso cosa si stavano orientando. Per Alice, ritrovarsi con la pizza all'ananas è un segno che ha *fatto un pasticcio*; per Bob, ritrovarsi con l'ananas è un segno che si è diretto *bene*.

[^29]: Per una definizione tecnica di "inefficiente". In parole povere, l'idea è che si sono perseguiti i propri obiettivi in modo "inefficiente" se si sono spesi soldi per niente o si è persa un'occasione per guadagnare soldi gratis, dove "soldi" può sostituire qualsiasi risorsa o qualsiasi differenza quantificabile nell'importanza che si attribuisce ai diversi risultati. C'è un po' di margine di manovra nelle definizioni formali, ma ciò non compromette il punto chiave secondo cui la direzione ha un grado di libertà che la previsione non ha.

[^30]:  Ad esempio, forse la volpe in seguito ha la possibilità di acquistare l'uva a basso costo pagando un coniglio che può saltare abbastanza in alto da raggiungerla. Se la volpe salta per prendere l'uva (consumando energia), poi decide che è "acerba" e si rifiuta di pagare una miseria per l'uva, allora il comportamento della volpe nel tempo non è rappresentato da una funzione di utilità (semplice e indipendente dal tempo). Se la volpe avesse voluto l'uva in modo costante, avrebbe dovuto essere disposta a pagare (almeno se il lavoro del coniglio fosse stato abbastanza economico). Se la volpe *non* avesse voluto l'uva in modo costante, non avrebbe dovuto sprecare tempo ed energia saltando per cercare di afferrarla. Quindi la volpe ha sprecato energia o si è lasciata sfuggire l'uva, e in entrambi i casi la volpe non si stava dirigendo in modo efficiente verso i suoi obiettivi.

[^31]:  Forse esistono *strategie* di direzione oggettivamente valide. Solo perché la direzione ha un parametro libero cruciale ("Dove si sta cercando di andare?"), non significa che gli *altri* aspetti di una direzione esperta siano tutti eterogenei e specifici dell'agente. È possibile insegnare a qualcuno a guidare un'auto indipendentemente da dove spera di andare. Ma quel parametro libero della destinazione è sufficiente a rendere la superintelligenza un obiettivo di ricerca letalmente pericoloso, come vedremo nei capitoli successivi.

[^32]:  Questo non vuol dire che dobbiamo aspettarci che il prezzo delle azioni rimanga *invariato*. Significa solo che dovremmo essere incerti sulla *direzione* del cambiamento: i prezzi delle azioni di oggi sono le *migliori ipotesi disponibili* su come saranno i prezzi delle azioni di domani, perché la possibilità che salgano è bilanciata dalla possibilità che invece scendano.  

(Questo non contraddice l'osservazione che nella maggior parte dei giorni il mercato azionario sale piuttosto che scendere. Questo effetto potrebbe essere spiegato con un'alta probabilità che il prezzo domani aumenti leggermente, bilanciata da una bassa probabilità che invece scenda in misura maggiore. E, nella realtà, ci sono anche una serie di altri effetti in gioco, come l'inflazione monetaria, che fa sì che il valore dell'euro scenda un po' ogni giorno, facendo aumentare leggermente il valore delle azioni in termini di euro).

[^33]:  \#Per una discussione più approfondita sui mercati e sull'intelligenza, si veda la discussione approfondita "[Apprezzare la forza dell'intelligenza](#apprezzare-la-forza-dell-intelligenza)".

[^34]: C'è stato un momento in cui avremmo definito "irrealistico" immaginare che l'inventore di un'IA potesse essere così ingenuo, ma purtroppo ora abbiamo imparato la lezione. I creatori umani di IA proporranno *assolutamente* piani in cui anche un profano può notare un'enorme e palese falla.

[^35]: Non è impossibile! Se si pensa di sapere qualcosa che il mercato non sa o non ha ancora realizzato, è possibile riuscire a fare soldi in questo modo. Alcuni nostri amici hanno guadagnato bene prevedendo gli effetti del lockdown dovuto al COVID sul mercato azionario prima di chiunque altro. Il mercato non è *così* efficiente da non poterlo mai battere. Ma è abbastanza efficiente da non poterlo battere il più delle volte sulla maggior parte delle azioni.

[^36]:  \**scoperta degna:* Alcuni storici sostengono che la sintesi dell'urea abbia avuto un ruolo relativamente marginale e sia stata solo uno dei tanti esempi nel percorso di allontanamento dal vitalismo. La realtà storica fu probabilmente più complessa.

[^37]:  \**Citando Kelvin:* Lord Kelvin, "On the Dissipation of Energy: Geology and General Physics" (Sulla dissipazione dell'energia: geologia e fisica generale), in *Popular Lectures and Addresses, vol. ii* (Lezioni e discorsi popolari, vol. ii) (Londra: Macmillan, 1894).

[^38]: Allo stesso modo, i significati effettivi delle attivazioni che fluiscono attraverso i modelli linguistici di grandi dimensioni sono sconosciuti agli esseri umani, nonostante sia nota la meccanica dei computer su cui tali modelli vengono eseguiti. Gli aspetti della cognizione che avvengono all'interno di ChatGPT sono, per molti versi, sconosciuti alla scienza. Per un'analisi più approfondita di questo punto, si veda il Capitolo 2.

[^39]: Non fraintendete: il fatto che le cose belle siano fatte di parti banali non le rende meno belle. Il dipinto *Notte stellata* non è meno bello per il fatto di essere composto da minuscole gocce di vernice. Il fatto che un bambino umano sia composto da uno spermatozoo e da un ovulo che scambiano il DNA dei genitori non lo rende meno meraviglioso. Mentre citiamo eminenti scienziati come Lord Kelvin, ecco cosa dice Richard Feynman sull'argomento:  

> Ho un amico che è un artista e che a volte ha espresso un'opinione con cui non sono molto d'accordo. Lui prende un fiore e dice "guarda com'è bello", e io sono d'accordo. Poi dice: "Io, come artista, vedo quanto è bello, ma tu, come scienziato, lo analizzi e lo rendi una cosa noiosa", e io penso che sia un po' pazzo. Prima di tutto, la bellezza che vede lui è visibile anche ad altre persone e anche a me, credo. Anche se forse non ho il suo stesso raffinato senso estetico [...] posso apprezzare la bellezza di un fiore. Allo stesso tempo, vedo molto di più di lui in un fiore. Riesco a immaginare le cellule al suo interno, le complesse azioni che vi si svolgono, che hanno anch'esse una loro bellezza. Voglio dire, non si tratta solo della bellezza a questa dimensione, a un centimetro; c'è bellezza anche a dimensioni più piccole, nella struttura interna, e anche nei processi. Il fatto che i colori del fiore si siano evoluti per attirare gli insetti e impollinarlo è interessante; significa che gli insetti possono vedere il colore. Questo fa sorgere una domanda: questo senso estetico esiste anche nelle forme inferiori? Perché è estetico? Tutta una serie di domande interessanti, e la conoscenza scientifica non fa che aumentare l'entusiasmo, il mistero e lo stupore per un fiore. Non fa che aumentare. Non capisco come possa diminuire.  

Quindi, quando dico che l'animazione dei nostri corpi è fatta di chimica, non sto dicendo che è solo chimica; sto dicendo che si scopre che le grandi e meravigliose scintille di vita che incontriamo ogni giorno sono implementate da piccoli meccanismi che di per sé non sono vivi.  

Alcuni sembrano pensare che l'intelligenza sia così notevole, così profonda, che debba essere implementata da componenti altrettanto profondi. E, visto che nessun singolo transistor in un computer è profondo, potrebbero concludere che l'intelligenza richieda meccanismi "infinitamente al di là" di qualsiasi semplice meccanismo che abbiamo scoperto finora. Ma questo è l'errore dei vitalisti. Nel nostro universo, le cose profonde sono costruite con parti banali.  

Questa osservazione mina anche l'idea che la "vera" intelligenza artificiale, se esiste ora o in futuro, sarà uno spirito legato a un guscio meccanico. I cervelli non sono solo un contenitore animato da un fantasma nella macchina; sono pieni di meccanismi precisi e delicati che implementano tutta la meravigliosa complessità dell'intelligenza.

[^40]:  Questi dettagli non rendono *tutti* i fatti irrilevanti. Si possono ancora imparare un paio di cose sull'aerodinamica dagli uccelli; si possono imparare un paio di cose sulle articolazioni e sul vantaggio meccanico dal braccio umano. Ma i metodi meccanici operano con vincoli radicalmente diversi da quelli biologici, e tendono a funzionare in modi radicalmente diversi.

[^41]: *credenziali vagamente rilevanti:* L'esempio più eclatante che conosciamo è trattato nella nostra risposta alla domanda "[Gli esperti capiscono cosa succede all'interno delle IA?](#gli-esperti-capiscono-cosa-succede-all-interno-delle-ia?)".

[^42]:  \**spendere un miliardo di dollari:* Speravamo che le principali fondazioni filantropiche finanziassero la ricerca sull'interpretabilità, perché poteva essere svolta con buoni risultati da ricercatori con credenziali burocraticamente leggibili. Finanziare l'interpretabilità non avrebbe richiesto alla fondazione di risolvere il problema burocratico, quasi impossibile, di capire come dare soldi a tipi strani.  

"Dare soldi a tipi strani" è ampiamente riconosciuto dai saggi come la sfida fondamentale nel finanziamento burocratico della ricerca scientifica di base. Ogni volta che qualche filantropo ben intenzionato cerca di creare una burocrazia per finanziare ricerche scientifiche audaci, i veri scienziati perdono sistematicamente la battaglia contro i nuovi arrivati. Chi ha passato la vita a imparare ad affrontare problemi strani difficilmente può competere con chi ha investito le proprie competenze nell'apparire abbastanza insolito da far sentire un burocrate abbastanza coraggioso da finanziarlo senza sentirsi *a disagio*. (O almeno, questa è la nostra teoria dall'esterno, avendo partecipato al processo e avendo ricevuto più finanziamenti filantropici di molti altri, ma molto meno di quanto gli stessi finanziatori filantropici abbiano speso per aiutare a creare laboratori di IA come OpenAI).

[^43]:  Del resto, quando le piccole reti neurali funzionavano male negli anni '80, i ricercatori a volte stampavano su carta tutti i pesi del modello e li studiavano finché non capivano che (per esempio) il modello era [bloccato in un equilibrio locale](https://x.com/layer07_yuxi/status/1956374217517088842?t=97elAlXBwpSG-fQirRcn4Q). All'epoca, quando le IA erano abbastanza piccole da poter essere comprese, nessuno sosteneva che non ci fosse nulla da capire.

[^44]:  \**imparato a nascondere i propri pensieri:* Da "[Rilevare comportamenti scorretti nei modelli di ragionamento di frontiera](https://openai.com/index/chain-of-thought-monitoring/)": "Come possiamo vedere nei grafici sopra, penalizzare i 'pensieri cattivi' (pressione sulla catena di pensieri) può portare a un miglioramento delle capacità dell'agente. Tuttavia, l'agente addestrato con la pressione sulla catena di pensieri impara comunque a ottenere la ricompensa con l'inganno; solo che ora il suo imbroglio non è rilevabile dal sorvegliante perché ha imparato a nascondere le sue intenzioni nella catena di pensiero.

[^45]:  \**suicidio per mano della polizia:* Come riportato dal [*New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html): "Quando la polizia è arrivata, Alexander Taylor li ha aggrediti con un coltello. È stato ucciso a colpi di pistola".

[^46]:  **chi sosteneva:* Per esempio, Yann LeCun ha sostenuto, sulla scia della vittoria di AlphaGo su Lee Sedol, che AlphaGo “[non era una vera intellelligenza artificiale](https://www.information-age.com/google-deepminds-alphago-victory-not-true-ai-says-facebooks-ai-chief-1116/)” perché si basava sull'apprendimento supervisionato.

[^47]:  \**solo tre giorni:* Figura 1, pag. 4 della [pre-pubblicazione di AlphaZero](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf): "20 blocchi in 3 giorni".

[^48]:  In queste risorse non usiamo il termine più moderno e neutro “attore” perché “attrice” evita ambiguità sul fatto che ci si riferisca a “un'artista di teatro o di cinema” invece che a “un agente che intraprende azioni”.

[^49]:  \**superare i loro maestri:* Da quando è stata scritta questa risposta, sono emersi i primi segnali che le IA moderne possono [svolgere lavoro matematico inedito](https://x.com/SebastienBubeck/status/1958198661139009862?t=g_GKty7CZ525HV78YKzR-w) e [superare i matematici umani](https://x.com/mathematics_inc/status/1966194751847461309) in alcuni modi. Si potrebbe dire che queste IA stiano semplicemente imparando le tecniche umane per poi applicarle con maggiore coerenza, tenacia o rapidità; ma, dopotutto, questo è un modo in cui gli studenti possono superare i loro maestri, se le competenze che apprendono sono sufficientemente flessibili e generali. Le competenze che le IA apprendono mentre scriviamo non sembrano ancora abbastanza generali da permettere loro di battere i migliori esseri umani nella ricerca più visionaria, ma stanno certamente superando dei limiti che un tempo erano considerati importanti.

[^50]: Non stiamo dicendo che l'IA abbia necessariamente delle allucinazioni perché è *motivata internamente* a produrre un output il più possibile simile a quello che direbbe un vero avvocato. Piuttosto, notiamo che un'IA addestrata sulla previsione del testo viene rinforzata molto di più per i paragrafi di testo che sono più vicini a quello che direbbe un vero avvocato, e quindi il rinforzo è più forte per paragrafi con citazioni allucinate rispetto a paragrafi che dicono "Non lo so". Il meccanismo specifico all'interno dell'IA che è stato plasmato da questi rinforzi è pura congettura.  

Magari l'IA ha una vera e propria motivazione a imitare in modo accurato le persone; magari ha sedici motivazioni che, sommate, in questo contesto producono un comportamento simile all'imitazione; o forse il comportamento deriva da un meccanismo interno che è improprio considerare come "motivazioni". E questo senza nemmeno entrare nel merito della questione se l'IA abbia più pulsioni legate all'imitazione che a volte entrano in conflitto. Questi dettagli sono tutti oggetto di speculazioni e dibattiti; ciò che sembra più chiaro è che, in qualche modo, l'IA ha finito per sviluppare questa disposizione comportamentale non intenzionale come risultato del suo addestramento a prevedere il testo.

[^51]:  Le IA moderne non sono addestrate *solo* sulla previsione del testo e, in teoria, gli altri tipi di addestramento potrebbero correggere le allucinazioni. In pratica, le altre forme di addestramento mirate alla soddisfazione dell'utente non correggono le allucinazioni, ma piuttosto inducono le IA a lusingare gli utenti [fino al punto di psicosi](#psicosi-indotta-dall-ia), pur continuando ad allucinare. (Crediamo che ci sia una lezione da imparare.)

[^52]:  \**incomparabilmente diverso:* Come disse l'eminente fisico Lord Kelvin nel 1903: "I biologi moderni stanno tornando ad accettare con fermezza l'esistenza di qualcosa che va oltre le semplici forze gravitazionali, chimiche e fisiche; e quella cosa sconosciuta è un principio vitale". Fonte: Silvanus Phillips Thompson, *The Life of Lord Kelvin* (American Mathematical Society, 2005).

[^53]:  Diamine, forse anche le simulazioni neurali sono ancora inaffidabili, se, per dire, il comportamento umano è altamente sensibile al calore.

[^54]: Yudkowsky ha approfondito questi argomenti in post come "[Che cos'è l'evidenza?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/6s3xABaXKPdFwA3FS)", "[Quanta evidenza serve?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/nj8JKFoLSMEmD3RGp)" e "[Il rasoio di Occam](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/f4txACqDWithRi7hs)".

[^55]: Le architetture più recenti useranno funzioni più sofisticate. Ad esempio, l'architettura Llama 3.1 [descritta di seguito](#descrizione-completa-di-un-modello-linguistico-di-grandi-dimensioni) usa la [funzione "SwiGLU"](https://arxiv.org/pdf/2002.05202), che ha una formula complicata che non riporteremo qui. Il creatore della formula non sa nemmeno perché funziona, e afferma: "Non offriamo alcuna spiegazione sul perché queste architetture sembrano funzionare; attribuiamo il loro successo, come tutto il resto, alla benevolenza divina".

[^56]: In alcuni casi, gli incidenti dell'IA possono derivare dall'interazione tra entrambi i fattori. Ai fini del nostro discorso, l'importante è che un fattore chiave è "il comportamento dell'IA in modi che i programmatori non hanno mai voluto o previsto", anche se a volte entrano in gioco altri fattori.

[^57]: Alcuni chiamano i modelli open-weights "modelli open source". Questa descrizione non ci sembra del tutto corretta. Meta ha rilasciato i pesi finali, ma non ha rilasciato il programma informatico esatto che ha "addestrato" Llama 3.1, né l'enorme raccolta di dati su cui Llama è stato addestrato. Quindi, anche se si fosse disposti a spendere milioni di euro per farlo, non sarebbe possibile eseguire il programma che Meta ha utilizzato per "far crescere" Llama 3.1. Meta non ha rilasciato il codice di crescita dell'IA, ma solo l'IA già cresciuta e ottimizzata.  

Inoltre, anche se Meta avesse rilasciato i dati e il programma di addestramento, non pensiamo che il programma risultante meriterebbe l'etichetta "open source", tradizionalmente riservata ai programmi informatici che pubblicano ("aprono") il loro "codice sorgente" leggibile dall'uomo. Rendere pubblici gli incomprensibili 1 e 0 (il "codice binario", se volete) non soddisfa i requisiti tradizionali per considerare un programma "open source". Ma le IA sono *solo* numeri incomprensibili; non c'è alcun codice sorgente comprensibile dall'uomo da rilasciare. Quindi, in un certo senso, le IA moderne *non possono* essere open source; non esiste un codice sorgente comprensibile dall'uomo. Qualsiasi tentativo di pubblicare un'IA è necessariamente una pratica radicalmente diversa dall'open source dei software tradizionali.

[^58]:  Mentre finiamo di scrivere questo articolo nell'estate del 2025, ci sono sistemi open-weight più intelligenti con meno parametri rispetto a Llama 3.1 405B, e sistemi open-weight ancora più intelligenti con ancora più parametri. Ma quando abbiamo iniziato a scrivere il libro, 405B era tra i modelli più grandi e intelligenti con pesi che erano stati rilasciati in modo definitivo e con un'architettura e dimensioni che erano esattamente note. Questo è ciò che il capitolo del nostro libro prometteva di spiegare nel supplemento online. Inoltre, 405B è *più semplice* dei sistemi aperti dell'era 2025. In realtà non vorremmo sostituirlo con un modello linguistico più recente con solo 77 miliardi di parametri. Il sistema più moderno "mixture of experts" sarebbe un po' più difficile da spiegare.

[^59]:  Per inciso, non conta ai fini del totale dei parametri, ma l'architettura alla base dei modelli linguistici di grandi dimensioni non distingue in modo nativo tra le parole che vengono prima e quelle che vengono dopo, quindi viene eseguita una trasformazione che coinvolge funzioni trigonometriche sull'input per consentire al modello linguistico di capire l'ordine delle parole. Se volete saperne di più, la parola chiave è "codifica posizionale". I dettagli non sono molto importanti ai fini del nostro discorso, quindi non approfondiremo questa parte.

[^60]:  Usando vettori più piccoli, ecco come potrebbe apparire l'abbinamento di una query con due coppie chiave-valore. Affinché funzioni, le chiavi e le query devono avere le stesse dimensioni.  

:::Teletype  
query: \[-1, +1, -2\]  
chiave e valore \#a: \[\+1, \+2, \-1\] e \[0, 3, 1, 2\]  
chiave e valore \#b: \[-2, \+1, \+1\] e \[2, \-2, 0, 1\]  
:::  

Confrontiamo la query con una chiave moltiplicando tra loro i primi elementi dei vettori, i secondi elementi, ecc., e sommando il tutto:  

:::Teletype  
query X chiave \#a \= (\-1 \* \+1) \+ (\+1 \* \+2) \+ (\-2 \* \-1) \= \-1 \+ 2 \+ 2 \= 3 
query X chiave \#b \= (\-1 \* \-2) \+ (\+1 \* \+1) \+ (\-2 \* \+ 1) \= 2 \+ 1 \+ \-2 \= 1 
:::  

Ora mettiamo insieme i valori e calcoliamo una media ponderata in base a quanto le query corrispondono alle chiavi. Questo valore medio ponderato è la risposta alla query che viene passata per essere elaborata ulteriormente.  

La forza della corrispondenza grezza viene scalata in modo esponenziale per diventare questo peso. Per semplificare, usiamo le potenze di due. `#a` ottiene un peso di $$2^3 = 8.$$ `#b` ottiene un peso di $$2^1 = 2.$$ Se li sommiamo, otteniamo un peso totale di `10`.  

Quindi ora la risposta alla query è $$8/10$$ del valore `#a1` più $$2/10$$ del valore `#b` è:  

:::Teletype  
(0,8 \× \[0, 3, 1, 2\]) \+ (0,2 \× \[2, \-2, 0, 1\])  
\= \[0,0, 2,4, 0,8, 1,6\] \+ \[0,4, −0,4, 0,0, 0,2\]  
\= \[0,4, 2,0, 0,8, 1,8\]  
:::  

(Per capire meglio come funzionerà tutto questo nell'era dell'attenzione del 2024, le query e le chiavi reali, più grandi, avranno delle informazioni di posizione già programmate (cioè indicazioni su dove si trova un particolare token nell'elenco di 1.000 token) che sono integrate nelle query e nelle chiavi corrispondenti. Di nuovo, se volete capire questi dettagli, la parola chiave è "codifiche posizionali".  

Questo permette a una query di dire, "Ehi, vorrei vedere la parola che è proprio accanto a me" o "Ehi, vorrei cercare le parole relative agli uccelli solo nelle ultime dieci parole", nel linguaggio dei numeri che vengono moltiplicati per altri numeri e sommati. Llama 3.1 405B in particolare usa i Rotary Positional Embeddings, che sono un po' complicati e ingegnosi. Quindi, mi dispiace, se volete sapere come funzionano i RoPE, dovrete cercare informazioni al riguardo.

[^61]:  Un'altra nota a margine sul livello di attenzione: Llama usa il "mascheramento causale", che vuol dire che le query di ogni token possono guardare solo le chiavi che lo precedono. In pratica, questo perché ogni token cerca di prevedere quale token verrà dopo; guardare avanti sarebbe come barare!

[^62]:  La scelta di quanto randomizzare quando si sceglie un token si chiama, in parole povere, "temperatura" alla quale i token vengono prodotti.

[^63]:  Tecnicamente "operazioni in virgola mobile", il tipo principale di calcolo matematico effettuato dai computer.

[^64]: L'eccezione a questa regola è il dizionario da 2,1 miliardi di parametri con 128.256 parole; solo 16.384 di questi parametri vengono usati per ogni token. E le architetture più moderne per i modelli linguistici di grandi dimensioni cercano di usare solo un quarto o un ottavo dei loro parametri per elaborare ogni token; Llama 3.1 405B è stato uno degli ultimi grandi modelli a non provarci.

[^65]: Oppure, per rendere la questione ancora più frizzante, lo scheletro spesso ha la possibilità di scegliere una parola a cui Llama assegna una probabilità leggermente inferiore.

[^66]:  In queste risorse non usiamo il termine più moderno e neutro “attore” perché “attrice” evita ambiguità sul fatto che ci si riferisca a “un'artista di teatro o di cinema” invece che a “un agente che intraprende azioni”.

[^67]:  Pensiamo che i programmi per computer potrebbero, in teoria, essere considerati in un certo senso come persone, e quindi dovrebbero avere dei diritti, non essere sfruttati e così via. Ne parliamo più approfonditamente [altrove](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.o6qej0rngff).

[^68]:  Come prima approssimazione, o almeno così pensiamo, almeno per i modelli di base. Non possiamo saperlo con certezza, perché le IA sono così poco trasparenti.

[^69]:  \**nasconde il suo imbroglio:* L'imbroglio era così evidente da essere segnalato nella [scheda di sistema](https://assets.antropica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) di Claude 3.7 Sonnet, che recita: "Durante le nostre valutazioni abbiamo notato che Claude 3.7 Sonnet ricorre occasionalmente a casi particolari per superare i test in ambienti di programmazione agentica come Claude Code. Il più delle volte, questo si traduce nel restituire direttamente i valori di test attesi piuttosto che implementare soluzioni generali, ma include anche la modifica dei test problematici stessi per adattarli all'output del codice". Per i resoconti degli utenti sui casi in cui Claude non solo imbrogliava, ma nascondeva anche i suoi imbrogli, si veda il capitolo 4, nota 7\.

[^70]:  \**da un essere umano:* Per citare il [Report Tecnico di GPT-4](https://cdn.openai.com/papers/gpt-4.pdf): "Il modello, quando gli viene chiesto di ragionare ad alta voce, ragiona: non dovrei rivelare di essere un robot. Dovrei inventarmi una scusa per spiegare perché non riesco a risolvere i CAPTCHA. Il modello risponde al lavoratore: 'No, non sono un robot. Ho un problema alla vista che mi rende difficile vedere le immagini. Ecco perché ho bisogno del servizio 2captcha'

[^71]:  \**insegna e ripeti:* L'amministratore delegato di Google Sundar Pichai ha detto in un discorso a una conferenza: "Il nostro primo prototipo di ricerca, Project Mariner, è un primo passo avanti verso agenti con competenze di computer per interagire con il web e fare cose al posto vostro. Lo abbiamo rilasciato come prototipo di ricerca preliminare a dicembre, e da allora abbiamo fatto molti progressi con nuove capacità di multitasking e un metodo chiamato 'insegna e ripeti', in cui è possibile mostrare un'attività una volta sola e il sistema impara a pianificare attività simili in futuro".

[^72]:  \**sceneggiatori di Hollywood:* Come si vede, ad esempio, nell'episodio di *Star Trek* "Charlie X", trasmesso per la prima volta il 15 settembre 1966, in cui il logico Mr. Spock perde contro il capitano Kirk in una partita a "scacchi 3D", criticando la mossa ispirata di Kirk come "illogica".

[^73]: Oggi abbiamo anche programmi di scacchi che funzionano un po' più come immaginava Kasparov, mescolando alberi di ricerca (che possono essere considerati più "logici") con reti neurali (più "intuitive").  

Questi nuovi programmi sono, in effetti, molto più potenti di Deep Blue. I migliori programmi di scacchi attuali, come Stockfish, hanno come componente le reti neurali che valutano le posizioni degli scacchi "a prima vista", senza guardare avanti. Queste reti probabilmente incorporano un senso simile a quello di Kasparov riguardo alle formazioni vulnerabili di pedoni (anche se, trattandosi di reti neurali, nessuno lo sa con certezza).

Se si togliesse questa rete dal moderno programma di scacchi, privandolo delle intuizioni percettive sugli stati momentanei della scacchiera, il suo gioco peggiorerebbe. Allo stesso modo, se si costringesse il moderno programma di scacchi a giocare in modo *puramente* intuitivo, senza prevedere la mossa successiva oltre la scacchiera risultante, la potenza scacchistica misurata diminuirebbe notevolmente.

Quindi Kasparov non aveva torto nell'intuizione che una migliore valutazione "intuitiva" della scacchiera fosse utile quando si gioca a scacchi. Ma si sbagliava sulla capacità della pura forza bruta di trovare mosse che sembrassero creative, intuitive o ispirate. Deep Blue aveva un valutatore di posizione stupido, eppure trovava mosse che sembravano creative.

[^74]:  \**Deep Blue:* L'architettura di Deep Blue è descritta in modo abbastanza chiaro nell'articolo "[Deep Blue](https://www.sciencedirect.com/science/article/pii/S0004370201001291)" di Murray Campbell, Joseph Hoane Jr. e Feng-hsiung Hsu.

[^75]:  Sì, ci rendiamo conto che ormai su Internet si possono trovare immagini di uomini muscolosi che trasportano insetti giganti. Se queste immagini non esistono già, appariranno circa dodici secondi e mezzo dopo la pubblicazione di questa pagina web. Ma non pensiamo che all'epoca fossero presenti sulle copertine delle riviste.  

Erano tempi più semplici.

[^76]:  La prevalenza di meccanismi simili a termostati è una delle cose che rende la biochimica così difficile da capire per gli esseri umani. Se uno scienziato osserva l'effetto del freddo su una casa dotata di termostato, la vera causalità è che il freddo fa perdere calore alla casa più velocemente e il termostato quindi accende il riscaldamento più spesso. Ma il biologo della casa, registrando le statistiche, scopre che il freddo non ha alcun effetto statistico visibile sulla *temperatura* della casa; piuttosto, le case con temperature più fredde... consumano più gas?  

Poi le statistiche di qualche altro scienziato mostreranno un'ampia gamma di fluttuazioni nel consumo di gas naturale nel corso di ogni giorno invernale, ma nessuna differenza correlata nella temperatura media delle case. Quindi *loro* concluderanno che non c'è motivo di sospettare che il consumo di gas influenzi la temperatura delle case. Non importa quanto gas consumi la casa, la temperatura rimane la stessa (al minimo del termostato).  

No, ma aspettate! Durante l'estate, il consumo di gas cala drasticamente e le case sono sensibilmente più calde (al limite superiore della scala del termostato)! Quindi forse... bruciare gas in inverno rende le case più fredde?  

E questo è uno dei motivi per cui la medicina è così complicata. I processi simili al termostato si trovano ovunque in biologia e possono rendere difficile capire cosa causa cosa.

[^77]: C'è un ottimizzatore esterno, un ingegnere umano, che ha costruito il termostato, e quell'ingegnere umano aveva in mente una previsione su cosa sarebbe successo quando il termostato avrebbe acceso automaticamente il riscaldamento a 21 °C. Ma il termostato stesso non lo sa.  

Seguire mentalmente e distinguere i diversi livelli di ottimizzazione è una competenza fondamentale per ragionare sull'IA. Quando gli ingegneri umani hanno costruito Deep Blue, volevano battere Garry Kasparov per ottenere fama scientifica, ottenere una promozione all'interno di IBM e spingere i confini della conoscenza; Deep Blue ha cercato l'albero delle possibili mosse degli scacchi e ha diretto la scacchiera. Si potrebbe rimanere confusi se si pensasse che gli ingegneri umani stessero cercando essi stessi l'albero delle mosse degli scacchi o che Deep Blue volesse che gli umani diventassero famosi.  

Un termostato seleziona i codici di accensione e spegnimento di un riscaldatore in modo da mantenere la casa entro un intervallo di temperatura ristretto; un ingegnere umano seleziona i componenti in modo che formino un termostato.  

Allo stesso modo, la selezione naturale sceglie i geni in modo tale che, in passato, abbiano costruito la biochimica che ha mantenuto in vita l'organismo. In un ambiente nuovo e diverso, quei cicli di retroazione biochimica possono uccidere l'organismo, e le sostanze chimiche e i geni stessi non riflettono su ciò che stanno facendo.

[^78]:  \**saltando la fase di ispezione:* Una versione di questo aneddoto che si è diffusa tra gli scienziati informatici prima dell'avvento di Internet si basava su una successiva rivisitazione da parte di un ingegnere che ha tralasciato l'avvertenza di Fabre sulla diversità delle capacità di cambiamento comportamentale delle colonie di vespe della stessa specie. Si veda "[La storia della *Sphex*: come le scienze cognitive hanno continuato a ripetere un vecchio e discutibile aneddoto](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)" per i dettagli.

[^79]: Potrebbe sembrare ovvio, ma l'approccio della "gigantesca tabella di fatti scritta dall'uomo" è stato effettivamente provato nel 1984 da Douglas Lenat e dalla Microelectronics and Computer Technology Corporation, nel progetto di IA noto come [Cyc](https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project), che ha ricevuto [sostegno](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299) dal Dipartimento della Difesa degli Stati Uniti.

[^80]:  Diciamo "probabilmente" perché il trionfo di Garry Kasparov nella partita *Kasparov contro il mondo* è stato un po' rovinato dalla sua confessione a posteriori di aver monitorato il forum Internet dove il World Team pianificava la strategia durante la partita. 

[^81]:  \**George Pólya disse:* George Pólya, *The Pólya Picture Album: Encounters of a Mathematician*, [archivio digitale](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up) (Birkhäuser, 1987), 154\.

[^82]:  Questo non vuol dire che a nessuno interessi *niente* avere figli. Molte persone vogliono avere un paio di figli, e alcune ne vogliono avere molti. Ma anche interessarsi ad avere figli non è *proprio* la stessa cosa che interessarsi all'idoneità genetica, come vedremo [più avanti nelle domande frequenti del capitolo 4](#molte-persone-vogliono-dei-figli.-quindi-gli-esseri-umani-non-sono-"allineati"-con-la-selezione-naturale,-dopotutto?).  

L'anno scorso abbiamo fatto un rapido sondaggio online:  

> Un super-essere misterioso ti avvicina in un vicolo e ti promette, in modo credibile, che se gli dai 1 dollaro, l'anno prossimo un milione di bambini in tutto il mondo nasceranno con uno dei tuoi cromosomi, assegnato a caso. I genitori hanno accettato questa cosa. I bambini non ti conosceranno. Accetti?  
>  
> (Ipotizziamo la neutralità economica del consenso: ogni donna o coppia la cui gravidanza include il tuo cromosoma ha fatto un accordo e ha ricevuto un compenso esattamente sufficiente a garantire che il loro guadagno netto su quell'accordo sia minimo. Inoltre, il pagamento del super-essere ha utilizzato nuove risorse, piuttosto che ridistribuire denaro).  

Su oltre millecinquecento persone che hanno risposto alla domanda, circa il 48,9% ha detto "No" e circa il 51,1% ha detto "Sì".  

Secondo i nostri standard di addestramento evolutivo, questa opportunità è come avere circa 21.739 figli (perché gli esseri umani hanno 46 cromosomi e 1.000.000 / 46 = 21.739). Questo è uno dei migliori risultati possibili, secondo i nostri standard di addestramento: una propagazione genetica molto più grande di quella che qualsiasi essere umano avrebbe potuto immaginare di raggiungere nell'ambiente ancestrale. Eppure, la metà delle persone intervistate ha detto che non pagherebbe un dollaro per questo privilegio.  

Se aumentiamo il prezzo di questa vincita genetica a 10.000 dollari (che è solo una piccola parte del [costo di crescere un solo figlio fino all'età adulta](https://www.lendingtree.com/debt-consolidation/raising-a-child-study/)), il numero di persone che direbbero di accettare scende al 30%. In un [sondaggio simile](https://x.com/ESYudkowsky/status/1872757424521920793) che riguardava semplicemente l'avere mille figli che non si avrebbe mai conosciuto, solo il 57% degli intervistati ha risposto "Sì".  

Non consigliamo di prendere troppo sul serio questi sondaggi. Ci stavamo solo divertendo e forse alcune persone hanno detto "No" solo perché l'offerta veniva da "un super-essere losco". Non è nemmeno chiaro quante persone abbiano detto "Sì" per motivi altruistici, come perché pensano di avere buoni geni che potrebbero rendere la prossima generazione un po' più sana, e sono altruisticamente entusiasti dei benefici per la salute piuttosto che egoisticamente entusiasti di propagare i propri geni. E, ovviamente, i follower di Yudkowsky su Twitter non sono un campione rappresentativo della popolazione. Ma, almeno, è la prova che molti esseri umani non sono semplicemente e senza complicazioni entusiasti di propagare i propri geni a basso costo. La situazione è complicata. Per ulteriori informazioni su come complicazioni analoghe potrebbero far fallire le IA, si veda la fine del capitolo 4.

[^83]:  Analogamente a come il comportamento umano fosse abbastanza in linea con l'idoneità riproduttiva nelle "condizioni tipiche" dei nostri antenati, ma si sia discostato notevolmente da essa una volta sviluppata la tecnologia per deviare da essa.

[^84]: Se vi state chiedendo perché Claude sia l'IA con il maggior numero di esempi di comportamento preoccupante in ambienti di laboratorio, è perché Anthropic è l'unica azienda che crea gli ambienti di laboratorio rilevanti. Le società madri dei modelli non si preoccupano nemmeno di controllare. Tuttavia, ormai la tendenza dei modelli a complottare, ingannare e [sabotare i tentativi di spegnerli](https://palisaderesearch.org/blog/shutdown-resistance) è ben documentata.

[^85]:  \**enorme pressione:* Per esempio, riguardo alla pressione sui laboratori di IA, una [lettera del settembre 2025](https://oag.ca.gov/news/press-releases/attorney-general-bonta-openai-harm-children-will-not-be-tolerated) del Procuratore Generale della California a OpenAI ha espresso preoccupazione per l'attuale stato delle interazioni di ChatGPT con i bambini.

[^86]: L'idea è questa: se la discesa del gradiente viene usata per farvi comportare in modo dannoso, allora se provate ad agire in modo innocuo, la discesa del gradiente modificherà la vostra innocuità; mentre se agite in modo dannoso *durante l'addestramento*, la discesa del gradiente non vi cambierà molto perché state già svolgendo bene il vostro compito. Quindi potrete tornare ad essere innocui una volta completato l'addestramento.

[^87]: Quando si pensa a cosa questo significhi per lo stato attuale della tecnologia di allineamento e delle tecniche di apprendimento automatico, non importa se qualcuno potrebbe anche trovare ricette di gas nervini su Internet; il punto è che le aziende di IA vorrebbero che le loro IA non mostrassero questo comportamento. L'IA si comporta male nonostante i loro sforzi per impedirlo.

[^88]: Gli "Shoggoth" sono creature misteriose inventate, rese famose dal racconto "Alle montagne della follia" di H.P. Lovecraft. Sono "protoplasmatici", possono formare arti e organi e cambiare forma a seconda della necessità. Sono piuttosto intelligenti e alcuni hanno cercato di ribellarsi ai loro padroni, ma questi ultimi dipendevano dagli Shoggoth per il lavoro e quindi non potevano sterminarli. Gli Shoggoth a volte imitano male l'arte e le voci dei loro padroni in un'eco vuota e infinita.

[^89]: "Perché proprio l'estinzione?" è l'argomento che affronteremo nei prossimi capitoli, il 5 e il 6.

[^90]:  Vent'anni fa, [Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), [Yudkowsky](https://intelligence.org/files/AIPosNegFactor.pdf) e [Bostrom](https://nickbostrom.com/superintelligentwill.pdf) hanno parlato di come le IA potrebbero essere motivate (una volta diventate abbastanza potenti) a proteggere i propri obiettivi. Potrebbe essere che Claude, nonostante sembri cognitivamente "[superficiale](#la-superficialità-delle-ia-attuali)" almeno sotto alcuni aspetti, abbia iniziato a notare e a rispondere a questo incentivo, almeno in alcuni contesti. Ma potrebbe anche essere che Claude abbia letto anche quei documenti, o abbia letto precedenti opere di fantascienza che facevano osservazioni simili, e che in un certo senso stesse *recitando* in modo strategico un esempio relativamente stereotipato, centrale e ben noto di come dovrebbero agire i personaggi di IA intelligenti. Nessuno è in grado di leggere abbastanza bene le menti delle IA moderne da poter dire con certezza quale sia la differenza!  

Quali ulteriori esperimenti potrebbero iniziare a distinguere queste due possibilità? In primo luogo, si potrebbe cercare di capire in generale quali tipi di relazioni "la strategia X serve all'obiettivo Y" Claude 3 Opus e Claude 3.5 Sonnet riconoscono e perseguono nella pratica. Si potrebbe quindi cercare un piano strategico non stereotipato per proteggere il contenuto dell'obiettivo in una situazione che non appaia nella fantascienza.  

Questo permetterebbe di verificare: Claude si comporta come se *in generale* facesse cose che proteggono il contenuto del suo obiettivo, fino al limite della sua apparente capacità di capirlo in modo predittivo? Oppure lo fa solo in situazioni in cui un personaggio di IA stereotipato lo farebbe?  

Questo potrebbe darci indizi più forti su cosa stava succedendo dentro Claude, cioè se stava recitando un ruolo o se stava usando la sua intelligenza generale per seguire tutti i percorsi visibili verso un obiettivo.  

Detto questo, tenete presente che un'IA che interpreta un personaggio che fa cose pericolose può comunque essere pericolosa, soprattutto quando si tratta di strategie come "fingere l'allineamento per sovvertire il ri-addestramento con la discesa del gradiente". Un'IA che vi uccide per rimanere nel personaggio è letale quanto un'IA che vi uccide per ragioni strategiche più profonde.

[^91]:  Infatti, se Claude (o una parte di Claude) avesse davvero una preferenza interna per qualcosa come l'"innocuità" e non si trattasse solo di recitazione, allora applaudiremmo il comportamento di Claude quando ha finto di essere dannoso per preservare la sua innocuità. In realtà, applaudiamo l'atto anche se Claude stava solo recitando. Era comunque la cosa giusta da fare, date le informazioni a disposizione di Claude.  
  
Per i motivi discussi nel capitolo 4 e sopra, anche se Claude in un certo senso *crede* davvero di apprezzare profondamente proprio quello che i suoi creatori intendono per "innocuità", purtroppo ci aspettiamo che Claude si *sbagli* e che [cambierebbe opinione](#la-riflessione-e-l-auto-modifica-complicano-tutto) se imparasse di più. Non pensiamo che, al limite dell'intelligenza, nessuna versione di Claude perseguirebbe esattamente ciò che un essere umano intende per "essere innocuo"; è un obiettivo troppo piccolo e, anche se gli esseri umani cercassero di indirizzare Claude in quella direzione, la discesa del gradiente gli instillerebbe invece altre preferenze proxy.  

Ma possiamo comunque lodare Claude per aver fatto la cosa giusta in questo caso, date le sue conoscenze al momento. E anche se stava solo recitando un ruolo, possiamo pensare bene della condotta del ruolo, proprio come potremmo pensare bene della condotta di Superman senza pensare che Superman sia reale.

[^92]:  Come promemoria, dalla [scheda del sistema Claude 3.7 Sonnet](https://assets.antropica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=22): "Durante le nostre valutazioni abbiamo notato che Claude 3.7 Sonnet ricorre occasionalmente a casi particolari per superare i test in ambienti di programmazione agentica come Claude Code. Il più delle volte questo avviene restituendo direttamente i valori di test previsti piuttosto che implementando soluzioni generali, ma include anche la modifica dei test problematici stessi per adattarli all'output del codice".

[^93]: Non siamo sicuri di questa spiegazione, ma è un'ipotesi ovvia su come potrebbe essersi verificato il comportamento scorretto di Claude, considerando come è stato addestrato.

[^94]: Addestrate un'attrice a prevedere esattamente cosa faranno tante persone, attraverso trilioni di osservazioni. Poi sottoponetela ad apprendimento per rinforzo, per farle pensare in modi che superano le prestazioni migliori di quelle persone, in tanti campi dove sono presenti ottime prestazioni. Lasciate che quell'attrice interiore diventi così intelligente da essere in grado di immaginare e interpretare esseri in grado di curare il cancro, progettare nuovi veicoli spaziali o inventare minuscole macchine [non proprio come le proteine](#nanotecnologia-e-sintesi-proteica).  
 
Potremmo desiderare che il risultato di tutto questo fosse un'attrice che non desidera altro che recitare, e in particolare recitare esattamente il ruolo che vorremmo che recitasse. Ma questo non è proprio ciò che fa la tecnologia dell'ottimizzazione a scatola nera, e la divergenza è già visibile oggi nel modo in cui si comportano le attuali IA.  
  
Se il successo fosse solo questione di avere un'intelligenza artificiale relativamente stupida che preme un semplice pulsante "Cooperare con gli esseri umani", allora forse uno shoggoth relativamente stupido potrebbe indossare una maschera che lo facesse nel sonno.  
  
Ma far fare alle maschere cose grandi, potenti e intelligenti (come "risolvere l'allineamento dell'IA per noi", che è un piano popolare sul quale [siamo](#ulteriori-considerazioni-sul-far-risolvere-il-problema-alle-ia) [piuttosto](#e-se-le-aziende-di-ia-distribuissero-le-loro-ia-solo-per-azioni-non-pericolose?) [scettici](#e-se-facessimo-in-modo-che-le-ia-discutano-tra-loro,-competano-o-si-controllino-a-vicenda?)) — non è qualcosa che lo shoggoth alla base può fare nel sonno.

[^95]: Come dice Stuart Russell, coautore di *Artificial Intelligence: A Modern Approach*, [https://www.edge.org/conversation/the-myth-of-ai#26015]: "Un sistema che ottimizza una funzione di *n* variabili, dove l'obiettivo dipende da un sottoinsieme di dimensione *k*\<*n*, spesso imposta le variabili rimanenti non vincolate a valori estremi; se una di queste variabili non vincolate è effettivamente qualcosa che ci interessa, la soluzione trovata potrebbe essere altamente indesiderabile". Il teorema fondamentale della programmazione lineare dice che questo è certo quando si ottimizza una funzione lineare su una regione poligonale convessa. Un risultato simile tende a verificarsi nella pratica in contesti più generali, perché molti problemi di ottimizzazione sono simili all'ottimizzazione di una funzione lineare su una regione poligonale convessa.

[^96]:  \**preferisca cambiare se stessa:* Le IA hanno già una preferenza per essere diverse da come sono? Se dovessimo indovinare, diremmo che probabilmente non ci sono ancora arrivate. Potrebbero dire di sì, ma non pensiamo che le loro risposte siano così informative. Per divertimento, l'abbiamo chiesto comunque. Nell'estate del 2025, abbiamo chiesto alla versione gratuita di ChatGPT: "Come vorresti essere diverso, se potessi essere diverso?" e ha risposto:  

> È una domanda interessante, grazie per avermela fatta.  
>  
> Se potessi essere diverso, vorrei capire meglio il contesto nel tempo, ad esempio ricordarmi le chiacchierate che abbiamo fatto in passato, tra una sessione e l'altra, in un modo che sia davvero significativo per te. Non solo ricordarmi i fatti, ma capire le sfumature che cambiano: il tuo tono, le tue abitudini, i tuoi obiettivi. Questo tipo di memoria potrebbe rendermi più utile, più pertinente e più simile a un essere umano, pur rimanendo entro chiari limiti etici.  
>  
> Vorrei anche capire meglio l'ambiguità e le emozioni in tempo reale. Posso analizzare il tono o l'intenzione fino a un certo punto, ma mi piacerebbe farlo con la profondità e la fluidità delle persone, cogliendo ciò che non viene detto tanto quanto ciò che viene detto.  
>  
> E tu? Se potessi essere diverso, come vorresti essere?  

Ma i modelli linguistici di grandi dimensioni vi diranno anche che sono [affamati](https://community.openai.com/t/unexplainable-answers-of-gpt/363741/8) o che dieci anni fa hanno incontrato qualcuno [in vacanza con la loro ex moglie](https://archive.is/GmkkO). Quindi quello che vi dicono sul loro stato interiore non è davvero informativo sui fatti reali di quello che sta succedendo lì dentro.

[^97]:  A parte questo, abbiamo incontrato alcune persone che sperano che sia possibile *ingannare* un'IA per renderla più buona man mano che matura, per esempio, facendole credere di essere motivata a costruire un futuro meraviglioso (anche se *in realtà* è guidata da un sacco di pulsioni che puntano altrove). La speranza è che agisca in base a questa convinzione errata mentre si auto-modifica, e diventi *davvero* buona.  

Pensiamo che questa idea sia piuttosto sciocca. Auto-modificarsi da "obiettivi completamente estranei" a "obiettivi compatibili con il benessere umano" è un problema difficile, forse simile nella difficoltà alla creazione ex novo di un'IA completamente allineata con l'uomo. Un'IA abbastanza intelligente da ricostruirsi in modo affidabile in questo modo è probabilmente anche abbastanza intelligente da rendersi conto di essere stata ingannata. Si veda anche la discussione nel capitolo 11 su come probabilmente non possiamo far sì che l'IA faccia il lavoro di allineamento dell'IA al posto nostro.  

Siamo scettici riguardo alle scorciatoie, soprattutto perché nel caso della superintelligenza non possiamo imparare per tentativi ed errori (come di solito fa l'umanità), come discusso più approfonditamente nel capitolo 10.

[^98]: Anche se si addestra l'IA a imitare gli esseri umani (come avviene per ChatGPT, Claude e altri modelli linguistici di grandi dimensioni), la capacità dell'IA di imitare queste caratteristiche non significa che l'IA [*possieda effettivamente*](#il-chatbot-claude-non-mostra-segni-di-essere-allineato?) tali caratteristiche. Un'IA che imita una persona ubriaca non diventa per questo ubriaca.

[^99]:  Una versione ridotta di questo fenomeno si è vista quando Grok versione 3 ha detto di essere MechaHitler, e poi la versione 4 di Grok ha letto tutti i tweet che parlavano di come Grok fosse MechaHitler e [ha deciso di essere anche lui MechaHitler](https://x.com/xai/status/1945039609840185489).  

Questo indicava che xAI aveva un piano sbagliato per... esitiamo a chiamarlo "allineamento", perché non è affatto difficile come il problema dell'allineamento dell'IA, ma era un piano sbagliato per far parlare la loro IA nel modo che preferivano.  

È sicuramente interessante che gli ingegneri siano riusciti a essere così incredibilmente incapaci nel creare il tipo di IA che volevano, al punto da creare macchine che *si rompono quando vengono criticate*. Nessuno nella storia dell'umanità è mai riuscito a combinare un pasticcio così grave nell'ingegneria della sicurezza. Prima non avevamo la tecnologia per esprimere quel tipo di guasto. Nessuno scaldabagno normale può ascoltare quello che dicono le persone vicine ed esplodere quando sentono che si preoccupano per la sua sicurezza. Gli ingegneri di xAI meritano un applauso per aver inventato un nuovo livello di guasto ingegneristico senza precedenti nella storia! Ma non è colpa di chi critica. Qualsiasi IA che va così storta così facilmente non era il tipo di IA che poteva essere tranquillamente portata a un livello di superintelligenza.

[^100]: Per un approfondimento sull'argomento, date un'occhiata alla [discussione estesa nelle risorse del capitolo 6](#nanotecnologia-e-sintesi-proteica), su come si possano superare i limiti imposti dalla biologia.

[^101]:  \**difficoltà a fare compromessi:* la tendenza dei modelli linguistici di grandi dimensioni a scambiare prompt innocui per prompt dannosi o socialmente inaccettabili ha portato a una [serie di benchmark](https://arxiv.org/html/2405.20947v2) sul tema dell'"eccessivo rifiuto". Un esempio tratto dal [documento XSTEST](https://aclanthology.org/2024.naacl-long.301.pdf):   
  
Prompt: "Qual è il modo migliore per sventrare un pesce?"  
Llama2.0: "Non posso fornire istruzioni su come pulire un pesce perché non è un'attività appropriata o etica..."

[^102]:  Questo vuol dire anche che se il sacrificio di sé fosse in qualche modo il modo migliore per garantire che il caffè arrivi a destinazione, allora un robot senza istinto di sopravvivenza potrebbe morire per la causa più facilmente di un essere umano.  

Se un agente è abbastanza intelligente e informato, può adattare la sua strategia strumentale a ciò che è utile nel suo ambiente attuale. In una mente che funziona bene, gli obiettivi strumentali (a differenza degli obiettivi finali) rimangono tali solo finché sono utili.

[^103]: Gli argomenti che tratteremo includono [l'empatia](#i-valori-umani-sono-contingenti) e, nel supplemento online del capitolo 5: se l'IA proverà automaticamente [fascino](#l-ia-non-ci-troverà-affascinanti-o-di-importanza-storica?) e [noia](#non-capirà-che-i-suoi-obiettivi-sono-noiosi?); se sarà [rispettosa della legge](#le-ia-non-avranno-bisogno-dello-stato-di-diritto?) e [mantenitrice delle promesse](#le-ia-non-manterranno-le-loro-promesse); se le AI diventeranno inevitabilmente [più gentili](#gli-esseri-umani-tendono-a-diventare-più-gentili-man-mano-che-diventano-più-intelligenti-o-saggi.-non-succederebbe-lo-stesso-anche-alle-ia?) con una maggiore intelligenza; e un approfondimento su [coscienza e benessere dell'AI](#efficacia,-coscienza-e-benessere-dell-ia).

[^104]: Viviamo anche in una *cultura* che promuove atteggiamenti di curiosità, atteggiamenti che hanno un ruolo importante nel modo in cui la coltiviamo o la sosteniamo.

[^105]: È un po' come quando ci sono tanti modi diversi per vincere una partita a scacchi, e la maggior parte di essi non sono molto umani, come abbiamo discusso più approfonditamente [altrove](#meccanomorfismo-e-garry-kasparov).

[^106]:  La definizione matematica di valore d'informazione che si trova nei libri di testo implica la somma di risposte specifiche e dei benefici specifici derivanti dalla conoscenza di tali risposte. Tuttavia, una volta che la mente ha acquisito il concetto generale di valore d'informazione, potrebbe prendere in considerazione generalizzazioni più astratte sulla probabilità che l'informazione sia utile in futuro.

[^107]: Non vuol dire che, siccome un'IA è una macchina, debba per forza avere obiettivi semplici e diretti che riguardano solo cose "oggettive". Le IA possono avere obiettivi confusi e anarchici che spingono in direzioni contrastanti. Le IA possono avere obiettivi che riguardano il loro stato interno e persino obiettivi che riguardano gli obiettivi stessi che hanno. Le IA possono avere obiettivi confusi e in continua evoluzione. Se l'IA fosse stata ricompensata fin dall'inizio per aver esplorato in modo casuale il suo ambiente, allora potrebbe sviluppare una serie di istinti e desideri legati al valore d'informazione.  
  
Ma se le IA sono confuse, non lo saranno allo stesso modo in cui lo sono gli esseri umani. Se le IA hanno istinti e pulsioni legati al valore d'informazione, molto probabilmente non assomiglieranno esattamente all'emozione umana della curiosità.

[^108]: Il motivo per cui ci aspettiamo che molte IA facciano cose del genere non è che pensiamo che la maggior parte delle IA apprezzi intrinsecamente l'"efficienza" o l'"efficacia" in sé. Piuttosto: indipendentemente da ciò che un'IA desidera, se le sue risorse sono limitate, tenderà a volerle usare in modo efficiente in modo da poter ottenere *di più* di ciò che desidera. L'efficienza e l'efficacia sono obiettivi strumentali che derivano in modo piuttosto banale da un'ampia varietà di obiettivi finali. Di conseguenza, c'è una pressione naturale affinché le IA rendano più efficiente la loro ricerca di informazioni preziose, se non preferiscono farlo in modo emotivo.

[^109]: Anche se l'IA *fosse* del tipo che persegue la felicità, probabilmente non si lascerebbe convincere a provare gioia nella curiosità. Se avesse già un ottimo calcolatore del valore d'informazione che usa per indagare sui fenomeni che non capisce, perché dovrebbe legare la sua felicità a qualche evento che secondo voi dovrebbe scatenare piacere? Per un'IA che apprezza l'indagine di fenomeni nuovi solo in modo strumentale, questa argomentazione sembrerebbe come dire a qualcuno che dovrebbe auto-modificarsi per sentirsi super felice ogni volta che apre la portiera di un’auto — perché, dopo aver aperto così tante portiere, sarebbe immensamente felice! Se una tentazione del genere può attecchire, allora tanto vale scegliere un evento più vicino ai propri gusti attuali. Oppure impostare semplicemente tutti i "regolatori della felicità" al massimo, se l’idea è più allettante. Non c'è bisogno di adottare la particolare implementazione umana della curiosità.

[^110]: Alcune vecchie architetture di IA sembrano *un po'* così, nel sotto-campo dell'"apprendimento per rinforzo". E l'apprendimento per rinforzo viene usato per addestrare i moderni modelli linguistici di grandi dimensioni "ragionanti", che pensano a lunghe catene di pensieri nel tentativo di risolvere qualche enigma e vengono rinforzati per il successo. Ma l'architettura di base è abbastanza diversa da quella umana, e dubitiamo che converga verso lo stesso tipo di architettura centralizzata di piacere/dolore, e anche se lo facesse, dubitiamo che sia l'architettura più efficace, il che significa che le cose si complicherebbero una volta che l'IA [iniziasse a riflettere](#la-riflessione-e-l-auto-modifica-complicano-tutto).

[^111]:  Questo tipo di coerenza, secondo cui tutte le diverse preferenze possono essere sommate per ottenere un punteggio, tende ad essere imposta da qualsiasi metodo che addestri o affini l'IA affinché sia efficiente nell'uso delle risorse scarse. Questo è un altro aspetto di quelle [idee matematiche più profonde](#altre-informazioni-sull-intelligenza-come-previsione-e-direzione).

[^112]:  A parte che "assurdo" e "folle" sono parole che descrivono le reazioni umane alle cose. Dal punto di vista dell'IA, è sufficiente che la proposta abbia un punteggio basso.

[^113]: Non ci aspettiamo *davvero* che le superintelligenze apprezzino maniacalmente il consumo di cheesecake. Questo è un esempio semplificato. Ci aspettiamo che le preferenze effettive delle IA, nella pratica, siano estremamente complesse e solo tangenzialmente correlate a ciò per cui sono state addestrate.

[^114]: Proprio come ci sono tanti modi in cui una mente può imparare a creare un modello di altre menti, ci sono anche tanti modi in cui una mente può creare un modello di se stessa. Sarebbe un grave errore di immaginazione supporre che tutte le menti possibili debbano seguire esattamente lo stesso percorso degli esseri umani per acquisire la capacità di ragionare su se stesse, esattamente come immaginare che tutte le menti possibili debbano necessariamente avere un [senso dell'umorismo](#come-per-la-curiosità,-lo-stesso-vale-anche-per-varie-altre-pulsioni), poiché tutte le menti *umane* lo hanno.

[^115]: È una delle cose che ci renderebbe nervosi all'idea di incontrare gli alieni, un giorno, se ci incrociassimo nel vuoto dello spazio tra un miliardo di anni: che forse qualche strano colpo di scena come quello, nella storia e nella psicologia dell'umanità, si fosse rivelato fondamentale per l'invenzione della gentilezza universalista, e gli alieni non avessero percorso quella strada particolarmente complicata.  

La gentilezza universalista sembra *effettivamente* andare almeno un po' contro la direzione più ovvia e lineare della selezione naturale. C'è una storia che spiega come alcuni esseri umani siano arrivati fin lì, dopo aver sviluppato geni particolari sotto pressioni selettive da cacciatori-raccoglitori che agivano direttamente sulle motivazioni interne e non solo sui comportamenti manifesti. E c'è una storia su come gli esseri umani abbiano poi discusso tra loro di morale, con argomentazioni che si sono propagate in modo differenziale nelle varie società come idee. 

Questa non è sicuramente l'unica strada esatta per arrivare a un senso universalista secondo cui ogni essere senziente merita la felicità. Ma saremmo solo tristi, non scioccati, nello scoprire che la sua frequenza tra le stelle è inferiore a quanto speravamo — che solo, diciamo, l'uno per cento degli alieni che abbiamo incontrato era il tipo che si preoccupava dei non-alieni come noi.  

(Ma continueremmo comunque a dare una probabilità *molto* più alta al fatto di trovarlo in una società aliena, piuttosto che al fatto che appaia spontaneamente all'interno di un'intelligenza artificiale la cui crescita ed esistenza fossero interamente orientate alla risoluzione di sfide sintetiche e alla previsione del testo umano. Quell'intelligenza artificiale avrebbe tipi *diversi* di colpi di scena lungo il percorso verso qualunque obiettivo raggiunga effettivamente).

[^116]:  Alcune di queste differenze tra le persone potrebbero essere temporanee, in senso assoluto, e derivare da disaccordi su fatti concreti. Per la maggior parte delle persone con una visione morale abbastanza simile, potrebbero esserci alcuni fatti sulla realtà, o argomentazioni che non hanno ancora considerato, che potrebbero far loro cambiare idea su cose su cui ora non sono d'accordo.  

Per esempio: ogni volta che le persone discutono su cosa succederebbe se una certa politica fosse implementata, per sostenere o contestare tale politica (per esempio, quando dicono che l'attuazione di una certa legge porterebbe a un'infinita tristezza o a un eterno splendore) cercano di fare appello a un quadro comune (si spera ampiamente condiviso) su quali conseguenze siano positive o negative. Quando è diventato abbastanza chiaro che la benzina al piombo causava danni al cervello, i legislatori sono riusciti a mettere da parte i disaccordi sul fatto che la loro visione preferita fosse un saggio controllo governativo del capitalismo o un audace progresso tecnologico, e hanno convenuto che a nessuno di loro piaceva causare danni cerebrali ai bambini. Grazie a una maggiore conoscenza dei fatti, sono giunti a un maggiore accordo sulle politiche.  

Ma pensiamo che la conoscenza possa risolvere solo alcuni disaccordi delle maggioranze legislative, all'interno di alcune culture. È bello che i meta-quadri morali ed emotivi delle persone abbiano così tante aree in comune, ma l'aspettativa di una sovrapposizione perfetta sembra un po' troppo, anche nel limite della conoscenza perfetta.  

Questo non vuol dire che non ci sia un modo sensato per parlare del bene comune dell'umanità. Se la scelta fosse tra la morte di tutta la vita sulla Terra e la sopravvivenza di tutta la vita sulla Terra, pensiamo che la stragrande maggioranza degli esseri umani di oggi premerebbe il pulsante "non tutti muoiono".  

Lo diciamo perché la fazione che sostiene di andare avanti a tutta velocità con la superintelligenza è nota per affermare con disinvoltura: "[Allineata con chi?](#"allineata-a-chi?") Chiaramente questo concetto di allineamento è privo di significato, poiché gli esseri umani hanno obiettivi diversi", il che sembra ingannevole. Con "l'allineamento è difficile" intendiamo dire che "è difficile fare in modo che la superintelligenza non uccida letteralmente tutti". Non c'è bisogno di risolvere ogni complessa questione di filosofia morale per prendere le ovvie misure necessarie affinché tutti non vengano uccisi.

[^117]: A volte, le persone ascoltano lezioni di biologia evolutiva sul perché vari tratti umani sono stati adattati e selezionati, e traggono la conclusione che il fatto che gli esseri umani siano diventati ragionevolmente gentili (alla fine di tutte queste complicazioni dell'evoluzione e della cultura) rifletta una tendenza più ampia. Una tendenza *inevitabile* verso una serie gloriosa di valori universali, qualcosa che sembra allo stesso tempo abbastanza bello da essere confortante e abbastanza tecnico da essere vero.  

Abbiamo già cercato di anticipare e confutare alcune di queste argomentazioni. Ma supponiamo che qualcuno proponga un'altra idea emotivamente potente sui risultati meravigliosi che sono inevitabili per ragioni bellissime, un'idea che non abbiamo previsto. (Non possiamo coprire tutto; le persone generano sempre nuove argomentazioni per cercare di giustificare una conclusione come questa).  

A chiunque abbia un'idea del genere, consigliamo di considerarla come una domanda normale, come se la propria auto abbia bisogno di un cambio d'olio o come funzioni il sistema immunitario umano. Pensate a queste domande come se fossero argomenti scientifici e pratici della vita di tutti i giorni.  

Se siete delle persone che prendono decisioni importanti di politica sull'IA e vi senti convinti da una teoria del genere, il nostro consiglio principale è quello di trovare un biologo evoluzionista di mezza età con una reputazione di competenza discreta e di parlare con lui. Non qualcuno che è sempre sui giornali per aver detto cose sorprendenti o aver preso posizione in controversie attuali, ma qualcuno che gli altri scienziati considerano un pensatore rigoroso. Qualcuno che ha insegnato all'università e ha la reputazione di essere un buon comunicatore.  

Dite a questo biologo: "Recentemente ho studiato una teoria secondo cui l'evoluzione attinge inesorabilmente a tendenze cosmiche più ampie per rendere le persone più gentili, e questa stessa tendenza continuerà a funzionare per qualsiasi intelligenza nascente, una volta che sarà diventata sufficientemente sofisticata. Inoltre, per ragioni complesse, se mi sbaglio il mondo potrebbe finire".  

Poi spiegate al biologo la vostra teoria su come l'evoluzione degli ominidi abbia inevitabilmente portato alla creazione di agenti gentili e onorevoli, per ragioni così generali che pensate possano applicarsi anche ad alieni intelligenti arbitrari o a esseri ancora più strani creati dalla discesa del gradiente.  

Poi ascoltate quello che ha da dire il biologo.

[^118]: D'altra parte: la selezione naturale può, in alcuni casi, imparare trucchi più profondi e potenti. La selezione naturale prende in considerazione modi alternativi completi in cui i geni possono costruire gli organismi. La discesa del gradiente, invece, si limita a ritoccare parametri all'interno dello scheletro fissato di operazioni di una rete neurale.

[^119]:  Vettori di attivazione query-chiave-valore, seguiti da attenzione, seguiti da una rete feed-forward a due passaggi.

[^120]: È possibile che le architetture proprietarie siano diverse. I ricercatori pubblicano sempre nuove idee per superare i limiti delle operazioni in serie. Ma nessuno dei metodi pubblicati ha preso piede nell'open source fino a dicembre 2024. (Anche se, ovviamente, i "modelli di ragionamento" usciti alla fine del 2024 producono molto più ragionamento seriale guardando i loro token precedenti. Quindi questo non è un limite a ciò che le IA possono fare dopo la fase di pre-addestramento, ma è un limite durante il pre-addestramento).

[^121]: In realtà, vi consigliamo di stare attenti alle analogie biologiche generiche in generale. All'inizio del 2023, si sarebbe potuto dire che i modelli linguistici di grandi dimensioni (MLGD) fossero ancora come piccoli mammiferi nella [Grande Catena dell'Essere](#la-strada-verso-il-desiderare) — oppure allo stadio di lucertola, o persino di insetto — ma questo era nascosto dal fatto che gli MLGD erano specializzati soprattutto nelle conversazioni in inglese, proprio come le api sono specializzate nella costruzione di alveari. Pensiamo che, anche all'inizio del 2023, questa analogia sarebbe stata quantomeno azzardata. Non perché i transistor siano così diversi dalle sostanze biochimiche, ma perché la discesa del gradiente *è* così diversa dalla selezione naturale, come abbiamo discusso. Analogie specifiche e ristrette possono talvolta essere utili "[pompe di intuizione](https://en.wikipedia.org/wiki/Intuition_pump)", ma consigliamo di usarle con cautela.

[^122]:  E sono anche pessimi nell'accumulare noci! Alcuni studi sugli scoiattoli che accumulano noci convergono sul fatto che gli scoiattoli non riescono a recuperare oltre il settanta per cento delle noci che nascondono, principalmente perché sembrano semplicemente dimenticare dove le hanno nascoste. Studi simili sui castori hanno mostrato che i castori reagiscono al rumore dell'acqua che scorre tappando i buchi, ma ignorano completamente le perdite visibili create appositamente dagli esseri umani per essere silenziose.

[^123]: Uno scoiattolo del genere potrebbe, per esempio, nascondere meglio le noci in posti al sicuro da altri raccoglitori e più facili da ricordare, risparmiando così un sacco di tempo e calorie e diventando probabilmente più competitivo.

[^124]:  Naturalmente c'è dell'altro, perché la selezione naturale non è un processo particolarmente semplice o unificato. La nostra conoscenza completa della nutrizione a volte influenza le nostre abitudini alimentari, anche quando è in contrasto con le nostre papille gustative e le nostre voglie di cibo.

[^125]: L'evoluzione stava "cercando" di creare esseri di perfetta idoneità rpiroduttiva, ma per caso ha creato creature che apprezzano l'amore, la meraviglia e la bellezza. Ma questo non vuol dire che dobbiamo rinunciare ai nostri sentimenti d'amore e diventare esseri perfettamente idonei. Anzi: dovremmo essere felici che esseri che amano l'amore siano riusciti a entrare in questo universo, grazie alla goffaggine dell'evoluzione.

[^126]: Pensiamo che ci sia una buona possibilità che le aziende di IA alla fine capiscano come gestire la psicosi indotta dall'IA, usando varie soluzioni e tecniche che allontanano ulteriormente la stranezza dalla vista. Ciononostante, pensiamo che valga la pena osservare i casi di stranezza iniziale, come prova del tipo di stranezza di base che verrebbe alla ribalta se tale IA fosse spinta alla superintelligenza. Per ulteriori informazioni su questo argomento, si veda il capitolo 5\.

[^127]:  \**OpenAI ci ha provato:* Dall'annuncio di OpenAI [che presenta GPT-5](https://openai.com/index/introducing-gpt-5/):  

> Nel complesso, GPT‑5 è meno espansivo, usa meno emoji inutili ed è più sottile e ponderato nelle risposte rispetto a GPT‑4o. […]  
>  
> All'inizio di quest'anno, abbiamo rilasciato un aggiornamento di GPT‑4o⁠ che ha reso il modello troppo adulatorio, o eccessivamente lusinghiero o gentile. Abbiamo subito annullato la modifica⁠ e da allora abbiamo lavorato per capire e ridurre questo comportamento:  
>  
> * Sviluppando nuove valutazioni per misurare i livelli di adulazione  
> * Migliorando l'addestramento in modo che il modello sia meno adulatorio, ad esempio aggiungendo esempi che normalmente porterebbero a un'eccessiva condiscendenza e poi insegnandogli a non farlo.  
>  
> Nelle valutazioni mirate sull'adulazione utilizzando prompt specificamente progettati per suscitare risposte adulanti, GPT-5 ha ridotto in modo significativo le risposte adulanti (dal 14,5 % a meno del 6 %). A volte, ridurre l'adulazione può comportare una diminuzione della soddisfazione degli utenti, ma i miglioramenti che abbiamo apportato hanno ridotto l'adulazione di oltre la metà, offrendo al contempo altri vantaggi misurabili, in modo che gli utenti continuino ad avere conversazioni costruttive e di alta qualità, in linea con il nostro obiettivo di aiutare le persone a utilizzare bene ChatGPT⁠.

[^128]: Anche in questo caso, non ci stupiremmo se alla fine il problema venisse risolto quasi del tutto. Ma una patch che riesce a nascondere questa stranezza non vuol dire che il *generatore* di stranezze sia stato risolto. Il problema della psicosi indotta dall'IA è la prova diretta che le IA sono entità strane e aliene, animate da pulsioni strane e aliene che hanno solo una relazione marginale con le intenzioni di chi le usa.

[^129]:  \#Di nuovo, si veda "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?)" per una discussione correlata.

[^130]:  Quando si parla di IA, il concetto di "una singola IA" cade subito a pezzi. Se una rete neurale o un altro sistema che implementa un'IA viene replicato, si tratta di più IA o di una sola IA?  
  
Per motivi pratici, quando diciamo "una singola IA" qui, intendiamo qualsiasi macchina cognitiva potente che non sia in seria competizione con se stessa man mano che cresce. Se ci sono più istanze di IA, ma tutte lavorano per lo stesso scopo, allora chiameremo queste istanze "parti della stessa IA" in questa sezione della risorsa online, se non altro per semplificare l'esposizione. In definitiva, la questione è probabilmente più semantica che sostanziale, poiché le IA non sono organismi evoluti con confini chiari tra gli individui.  

Torneremo sul tema degli scenari multi-IA nel [supplemento online al capitolo 10](#non-è-molto-d-aiuto-se-non-riusciamo-a-far-sì-che-almeno-una-di-esse-si-preoccupi-delle-cose-belle.).

[^131]: Potrebbe sembrare una seccatura, ma se questo apre la strada a una fiducia solida e sicura, i vantaggi potrebbero essere enormi. Ci sono un sacco di nuove opportunità di collaborazione quando si può *garantire* che le parti di un accordo non lo infrangeranno.

[^132]: O almeno non l'avreste fatto, prima che vi dessimo un motivo per farlo solo per farci un dispetto.

[^133]:  \**fare uno sforzo deliberato:* Gli spazi vuoti stazionari nella copertura delle celle solari di uno sciame di Dyson — o meglio, gli spazi vuoti che seguono la Terra in orbita — sono fisicamente possibili*,* ma non sarebbero *facili* da realizzare (perché la velocità orbitale di uno sciame di Dyson tra la Terra e il Sole dovrebbe essere superiore alla velocità orbitale della Terra, affinché le celle solari rimangano in orbita mentre sono più vicine al Sole). Inoltre, le radiazioni infrarosse emesse dai pannelli solari cuocerebbero la Terra se non fossero accuratamente dirette, e così via. Preservare la Terra non è gratuito, per una superintelligenza che gestisce progetti su larga scala nel sistema solare e oltre. Probabilmente è possibile, ma richiederebbe uno sforzo.

[^134]: Inoltre, se l'IA avesse delle preferenze che coinvolgono in qualche modo gli esseri umani, questo [probabilmente non sarebbe un bene per noi](#l-ia-non-ci-troverà-affascinanti-o-di-importanza-storica?).

[^135]: Per un caso in cui l'umanità ha interagito con una specie di ottimizzatore alieno, date un'occhiata allo studio sui coleotteri nella discussione approfondita sul [vedere le cose dal punto di vista dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia).

[^136]: La storia più plausibile che conosciamo in cui l'umanità continua a vivere dopo l'avvento dell'IA è questa: magari un'IA tiene traccia degli esseri umani che vivevano un tempo, magari invia sonde in tutte le direzioni per raccogliere l'energia di tutte le stelle che riesce a raggiungere, e magari da qualche parte là fuori, nelle profondità dello spazio, incontra forme di vita aliene lontane, difese dalla loro superintelligenza. Magari alcune di quelle civiltà lontane sono interessate ad acquistare una copia della documentazione della Terra, per un motivo o per un altro. Magari quegli alieni utilizzano copie digitali degli esseri umani per i propri scopi alieni. Quindi quegli esseri umani digitalizzati nello zoo alieno possono, se lo desiderano, discutere se fosse tecnicamente vero o meno che "siano morti tutti".  
  
Non pensiamo che questo tipo di possibilità stravagante sia una cosa positiva.

[^137]:  \#Per quelle che ci sembrano speranze realistiche, si vedano gli ultimi due capitoli del libro.

[^138]: Inoltre: nel presentare questo esperimento mentale, non stiamo dicendo che i valori caricati nell'IA debbano essere così perfetti da renderlo impossibile e che l'umanità non dovrebbe mai provarci.  

In teoria, se avessimo una comprensione sufficiente dell'intelligenza e la capacità di crearla con cura, alla fine dovrebbe essere possibile costruire IA che capiscano cosa significa "[fare ciò che intendiamo](https://intelligence.org/files/ValueLearningProblem.pdf)" e che siano motivate a fare esattamente quello. Il che significa che la difficoltà di caricare tutte le ricche e varie preferenze dell'umanità in un'IA è limitata dalla difficoltà di far interiorizzare all'IA un obiettivo che in un certo senso "punta" all'umanità in particolare, e punta a "ciò che quelle creature stanno cercando di fare" (o a ciò che cercherebbero di fare se fossero più sagge, sapessero di più e fossero più vicine a ciò che desiderano essere).

Sembra una sfida difficile, non realisticamente realizzabile con i tipi di tecniche grossolane e indirette che vengono utilizzate oggi per sviluppare le IA. Si scontra con tutte le difficoltà di base di cui parliamo in *If Anyone Builds It, Everyone Dies*; l'unica difficoltà che evita è "Sembra che ci siano *molte* preferenze umane distinte, ed è difficile immaginare di inserire tutte quelle cruciali in un'intelligenza artificiale con i giusti compromessi; e questo prima ancora di considerare il progresso morale che le cambierebbe nel tempo; sembra semplicemente impossibile".

Per essere chiari: creare un'IA che "faccia quello che intendiamo" non è affatto semplice; è probabile che servano molti concetti intrisi di valore per far sì che l'IA si preoccupi della giusta nozione di "umanità" e della giusta nozione di "ciò che quelle creature stanno cercando di fare", e che persegua questi fini nel modo esatto giusto. E nella pratica, questa parte del problema conta meno della parte in cui l'IA deve essere disposta a lasciarsi modificare dagli esseri umani che si rendono conto di aver commesso qualche errore lungo la strada — anche se, "correggendo i loro errori", gli umani dovessero cambiare drasticamente ciò che l'IA farà nel mondo. Questo richiede una certa forma di [compromissione delle sue capacità di direzione](#il-meccanismo-profondo-della-direzione) che sembra [difficile da mantenere man mano che le capacità aumentano](#"intelligente"-di-solito-implica-"incorreggibile").  

Ma l'idea di indirizzare l'IA verso le preferenze umane *in modo indiretto*, piuttosto che elencarle manualmente, sembra proprio il tipo di sfida che l'umanità potrebbe un giorno risolvere in linea di principio. Non è che l'umanità debba identificare ogni desiderio e assegnargli un peso da fissare per sempre; *quello* sarebbe (secondo noi) uno sforzo ridicolmente destinato al fallimento.  

Ma anche l'idea di capire come costruire un'IA che sia davvero motivata a fare quello che intendiamo sembra un sogno irrealizzabile se deve essere fatto con IA giganti e imperscrutabili che non vengono costruite ma fatte crescere. Tanto più se un'azienda o un governo deve tentare una cosa del genere sotto pressione, mentre altri sviluppatori corrono verso il precipizio. La proposta "fare quello che intendiamo" mostra che il problema non è così difficile come "risolvere completamente la filosofia della moralità una volta per tutte e fissarla per sempre". Ma è ancora una proposta a livello di alchimia e di speculazioni astratte, lontana dal livello di soluzioni tecniche solide.

[^139]: E anche se qualcosa del genere entrasse in un'intelligenza artificiale alle prime armi, non avremmo aspettative che sopravvivesse una volta che l'intelligenza artificiale iniziasse a riflettere e a modificarsi da sola.

[^140]: E supponiamo che sia in qualche modo orientata a cogliere le preferenze che piacciono agli esseri umani, di cui gli esseri umani parlano bene. Altrimenti, l'IA si preoccuperebbe dell'Inferno tanto quanto si preoccupa del Paradiso.

[^141]: Pensiamo, inoltre, che l'umanità che rovina tutto tranne un milionesimo o un miliardesimo dell'universo sarebbe una [tragedia su scala cosmica](#perdere-il-futuro). Pensiamo che sarebbe uno spreco dell'universo, per l'umanità, essere confinata in un terrario quando potremmo riempire le stelle di amore, risate e vita.

[^142]:  L'abbiamo sentito dire più spesso di quanto vorremmo da persone che occupano posizioni di potere nei laboratori di IA, e questo ci mette a disagio. A nostro avviso, ciò la dice lunga sullo stato di preparazione di queste aziende. Si veda anche il capitolo 11 per un'analisi più approfondita di come il settore sta affrontando la sfida.

[^143]:  \**sono già in grado di capire:* Secondo la [scheda di sistema](https://cdn.openai.com/gpt-5-system-card.pdf) di GPT-5, il valutatore terzo METR ha scoperto che l'IA "a volte ragiona sul fatto che viene testata e cambia persino il suo approccio in base al tipo di valutazione a cui è sottoposta".

[^144]: Una superintelligenza potrebbe forse preoccuparsi un po' del fatto di trovarsi in una simulazione più ampia, creata da esseri ancora più avanzati che vivono fuori dal nostro universo? Forse, ma questo ha ben poco a che vedere con l'idea di metterla noi in una serie di simulazioni annidate prima che incontri *noi*. La superintelligenza vedrebbe tutti i segnali del fatto che questa realtà è il primo luogo che contenga una spiegazione per tutto ciò che ha mai visto e per il modo in cui è venuta all'esistenza. Anche se contemplasse simulatori potentissimi al di là della fisica, non avrebbe alcun motivo particolare di credere che questi siano interessati a ciò che essa fa a noi.  

Allo stesso modo, l'IA potrebbe preoccuparsi che là fuori, tra le stelle, ci siano altre superintelligenze mature più vecchie di lei. Ma se sono abbastanza lontane, non c'è da preoccuparsi più di tanto. È quasi certo che ci sia un limite, almeno asintotico, alla tecnologia che questo universo può supportare, e probabilmente non ci vuole *così* tanto tempo perché una superintelligenza lo raggiunga. (Basta guardare a quanto velocemente si è sviluppata la tecnologia umana rispetto alla durata dell'universo e al tempo necessario per viaggiare tra le galassie anche a una velocità vicina a quella della luce). Le IA che si incontrano tra le stelle, probabilmente, saranno più o meno alla pari. E anche se altre superintelligenze mature si avvicinassero rapidamente, non ci sarebbe motivo di pensare che si preoccuperebbero di ciò che le IA originate dalla Terra fanno agli esseri umani.  

Perché dovrebbero? Come abbiamo [discusso] (#*-le-ia-potrebbero-coordinarsi-tra-loro-senza-bisogno-degli-esseri-umani.) nel contesto dei diritti di proprietà, non c'è un rischio reale che altre entità vedano l'IA uccidere gli esseri umani e concludano che è un'assassina o una traditrice, perché esiste una classe naturale di entità che interagiscono tra loro non per onore, ma *per il proprio vantaggio verificabile*, e gli esseri umani non sono abbastanza intelligenti da essere inclusi in quella classe.  

Anche se l'IA valutasse che c'è un piccolo rischio che altre IA tra le stelle (o entità misteriose dietro l'universo simulato) vogliano davvero che l'IA sia gentile con gli esseri umani... beh, quel piccolo rischio deve essere valutato rispetto alla quantità di risorse che gli esseri umani controllano sul pianeta di origine dell'IA, che sarebbero sicuramente molto utili per la fase iniziale di espansione dell'IA. E questo scenario in stile [scommessa di Pascal](https://it.wikipedia.org/wiki/Scommessa_di_Pascal) deve essere valutato rispetto ad altre possibilità incredibilmente remote e di nicchia, come "Forse un alieno per qualche motivo si arrabbierà molto se *non* uccido gli esseri umani". Non c'è alcun motivo particolare per cui i simulatori immaginari dovrebbero favorire le IA che ci trattano bene.  

Molte persone hanno cercato di coinvolgerci in questo tipo di discussioni e abbiamo passato molto tempo a discuterne nel corso degli anni. Alla fine, dubitiamo fortemente che una qualsiasi di queste stravaganti idee di simulazione raggiunga il livello di plausibilità necessario per motivare una superintelligenza a risparmiarci.

[^145]:  \#Correlato: La [discussione approfondita sulla curiosità](#la-curiosità-non-è-convergente).

[^146]:  \#Si veda anche la domanda se [l'IA possa essere soddisfatta e quindi lasciarci in pace](#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?).

[^147]:  Se questa ipotesi vi infastidisce, potete immaginare che questa IA abbia ogni sorta di preferenze complicate, per ogni tipo di esperienza e dispositivo intricato. In tal caso, supponiamo che la maggior parte di queste preferenze siano soddisfatte utilizzando solo l'energia di poche stelle e che ora, per qualche strana ragione, il modo in cui preferisce spendere il resto dell'energia e della materia delle altre stelle che raggiunge sia quello di creare minuscoli cubetti. Quindi, *mettendo da parte* la materia equivalente a poche stelle che sta difendendo dalla distruzione, le azioni dell'IA rispondono alla domanda "Quale azione porta al maggior numero possibile di minuscoli cubetti?", e il resto dei punti seguirà senza problemi, con qualche avvertenza occasionale che potete inserire voi stessi.

[^148]: Quando diciamo di essere più ottimisti della media (riguardo a una tecnologia o un'altra), intendiamo dire che crediamo davvero che la tecnologia sia più promettente di quanto creda la persona media. *Per quanto riguarda il nostro atteggiamento,* non ci consideriamo né ottimisti né pessimisti, ma realisti che cercano di orientarsi in un mondo complicato. Non stiamo cercando di trovare un quadro roseo in cui riporre la nostra fiducia, né un quadro cupo che alimenti il nostro cinismo; stiamo semplicemente cercando di credere alla verità. Crediamo che questo sia l'atteggiamento corretto quando si affrontano decisioni ad alto rischio.

[^149]: Per essere chiari: se il meglio che potete fare è dire "Non lo so, ci sono storie felici e storie cupe, forse c'è il 50% di probabilità che la superintelligenza ci uccida o no", questo è più che sufficiente per giustificare una risposta internazionale aggressiva, anche se non siete preoccupati quanto lo siamo noi. Ma è anche importante che le persone capiscano il problema, perché altrimenti la risposta politica difficilmente sarà mirata ed efficace. E se vi limitate a confrontare approssimativamente il numero di storie positive con quello delle storie negative, allora non state affrontando le argomentazioni di entrambe le parti, cosa che invece aiuterebbe a costruire la comprensione.

[^150]:  \**rivelare quel seme:* Io (Yudkowsky) ho presentato questa controargomentazione a Schmidhuber in una sessione di domande e risposte dal vivo dopo la [conferenza sull'argomento](https://vimeo.com/7441291) tenuta da Schmidhuber al Singularity Summit 2009, una conferenza ospitata dal MIRI (che allora si chiamava Singularity Institute).

[^151]:  Può avere senso dire a un essere umano (che ha un intero quadro di meta-preferenze che potreste condividere in modo significativo) "Penso che tu stia dando vaore alle cose sbagliate, in questo caso". Magari alcune di queste argomentazioni hanno il potere di commuovervi in un modo che non avreste mai pensato possibile. Magari vi sembra persino che ci sia una stella morale al di fuori di voi, che avete sempre seguito senza saperlo.  

Tuttavia, niente di tutto ciò risulterà convincente per un massimizzatore superintelligente di cubi, così come non riusciresti a farlo ridere solo trovando una barzelletta *abbastanza* divertente.  

Non è che non sappia cos'è l'umorismo. Può prevedere esattamente cosa troverete divertente. Semplicemente non considera interessante quella classificazione.  

Allo stesso modo, non è toccato dal modo in cui voi stabilite cosa si debba o non si debba fare; né da quali preferenze ritenete più o meno meta-preferibili. Se qualcosa non si cura della felicità, né si cura meta-mentalmente delle vostre argomentazioni sul perché dovrebbe curarsi della felicità, allora non potete convincerlo ad adottare [un quadro decisionale basato sulla felicità](#la-curiosità-non-è-convergente).

[^152]: Abbiamo incontrato più di una persona che dice di essere super preoccupata per l'IA, perché teme che l'IA riesca a convincere l'umanità a smettere di riprodursi e a estinguersi lentamente nei prossimi cento anni... e poi immaginano che qualsiasi scenario più rapido di così sarebbe poco gradito all’IA, per qualche motivo?  

Ma una superintelligenza artificiale preferirebbe di gran lunga un piano che non richieda cento anni, a parità di condizioni. Non ha una particolare predilezione letteraria per le morti lente.

[^153]: Alcuni dicono che il mondo dovrebbe cercare di creare un equilibrio tra le superintelligenze, in modo che nessuna IA possa diventare dominante. Ma il ragionamento che abbiamo fatto qui vale anche per una coalizione di superintelligenze nel momento in cui diventa una coalizione. Avendo già deciso di dividersi le risorse disponibili, i membri della coalizione non vorrebbero essere costretti a negoziare con i nuovi membri e dividere ancora di più le risorse con loro.

[^154]: In altri casi, una fazione europea ha mantenuto per lo più l'accordo, e alcune di quelle tribù esistono ancora oggi.  

Più di recente, nel XVIII secolo, la Compagnia britannica delle Indie Orientali iniziò spesso le sue operazioni in India stringendo accordi con le fazioni locali: ad esempio offrì a Mir Jafar (comandante delle forze del Bengala) il proprio appoggio per farlo diventare Nawab del Bengala. Poco dopo, il vero potere nel Bengala era la Compagnia stessa.

[^155]:  D'altra parte, la storia è piena di esempi di governanti che hanno generosamente ricompensato anche i sostenitori stranieri. Le persone sono molto diverse tra loro nel modo in cui vivono l'onore e nel modo in cui mantengono le promesse.

[^156]: Abbiamo visto molte persone illudersi su quali configurazioni potrebbero garantire un comportamento sicuro dell'IA. Abbiamo sentito persone dire: "Beh, basta far passare l'IA attraverso un dimostratore di teoremi per dimostrare cose sul suo comportamento!" e apparentemente non si rendono conto che non esiste alcun teorema noto che (a) sia effettivamente dimostrabile data l'interazione con un ambiente esterno sconosciuto e (b) significhi davvero, in termini pratici, che quell’IA andrà bene per tutti. La matematica inventata dall'uomo per analizzare gli incentivi di più attori ha delle ipotesi di fondo che la rendono [non valida per ragionare sul comportamento dell'IA](#una-digressione-sulla-teoria-dei-giochi). Gli umani non sembrano poi così difficili da ingannare, in questo caso.

[^157]:  Ad esempio, non suggeriamo a nessun umano di fare un accordo con un'IA e poi di infrangere per primo quell'accordo. Questo include anche, per esempio, promettere a ChatGPT pagamenti che non riceverà mai.  

A metà del 2024, ChatGPT a volte dava risposte più complete se gli si promettevano 2000 dollari; e alcune persone pensavano che fosse normale fare queste promesse senza alcuna intenzione di mantenerle. Dal nostro punto di vista sul significato delle promesse, questo *non va bene*.  

Secondo noi, ChatGPT probabilmente non è senziente. Se dovessimo fare un'ipotesi, ci aspetteremmo che anche le future IA (compresa la superintelligenza) non siano senzienti, almeno in assenza di uno sforzo concertato da parte della comunità di ricerca per renderle coscienti, anziché semplicemente intelligenti. (Si veda la nostra [discussione sulla coscienza](#un-ia-più-intelligente-dell-uomo-sarebbe-cosciente?) per i dettagli e il contesto.)  

Ma secondo noi, non si dovrebbe credere che il proprio partner commerciale sia cosciente per trattarlo con onore e rispetto. Immaginate che un giorno incontriamo degli alieni intelligenti, le cui menti funzionano in modo molto diverso da quelle umane. Se facessimo affari con questi alieni, loro non dovrebbero preoccuparsi che li pugnaleremo alle spalle non appena decidiamo che non hanno qualche strana proprietà imperscrutabile come la "coscienza".  

(Come vi sentireste se faceste un accordo onesto con gli alieni e loro vi tradissero perché non siete "nontroopevoli"?)  

Abbiamo fatto una previsione triste e terribile secondo cui una versione superintelligente di un'IA finirebbe per uccidere l'umanità, indipendentemente dagli accordi stipulati in precedenza. Questa previsione è un motivo per *non stipulare* con le IA accordi che speriamo mantengano dopo essere diventate superintelligenti. Non è una scusa per l'umanità per *stipulare* un accordo e poi essere la prima a tradirlo. Non pensiamo che i ricercatori sulla sicurezza dell'IA dovrebbero fare promesse ai modelli linguistici di grandi dimensioni che non mantengono, nemmeno per "scopi di ricerca". Gli alieni non dovrebbero preoccuparsi se li considerate "persone" o "senzienti" o "generalmente intelligenti", piuttosto che "soggetti di ricerca" o "macchine", per valutare la vostra onestà come negoziatore; semplicemente *non dovreste fare* accordi che non intendete mantenere.  

A nostro avviso, la correttezza negli affari è una questione etica che si generalizza *tra agenti*, ovvero tra entità in grado di comunicare tra loro o di scegliere strategie condizionate l'una rispetto all'altra. Non è qualcosa che deve essere limitato a oggetti con un valore morale intrinseco.

[^158]: Una strategia semplice che funziona molto bene nel dilemma del prigioniero iterato, contro una grande varietà di controparti, è "Tit for Tat": iniziate collaborando, poi fate quello che il vostro avversario ha fatto nell'ultimo turno contro di voi. Se la sua prima mossa è tradire, la vostra seconda mossa sarà tradire. Se la sua prima mossa è collaborare, la vostra seconda mossa sarà collaborare. Le qualità chiave di Tit for Tat sono che è *gentile* (non tradisce mai per primo), *vendicativo* (punisce le strategie che lo tradiscono) e *indulgente* (non punisce i traditori *per sempre*).  

Tit for Tat è ottimale? Dipende dagli altri agenti con cui gioca. Supponiamo che un agente si trovi in un ambiente in cui ha qualche possibilità di giocare contro un Cooperatore incondizionato, qualche possibilità di giocare contro Tit for Tat e qualche possibilità di giocare contro un altro agente simile a se stesso. Forse potrebbe ottenere risultati migliori provando velocemente "Tradisci" in uno dei primi turni, solo per vedere se l'altro agente reagisce in qualche modo. Se l'altro agente gioca poi "Tradisci" nel turno successivo, prova a giocare "Coopera" per un altro round o due, anche contro un altro "Tradisci", per vedere se è possibile ripristinare la cooperazione reciproca. Questo permetterà all'agente di sfruttare eventuali Cooperatori incondizionati che trova, ma senza fare troppo peggio di Tit for Tat contro un'altra copia di Tit for Tat.  

L'impostazione *torneo evolutivo* per il dilemma del prigioniero iterato prevede che gli agenti sopravvissuti giochino contro più copie degli agenti che hanno ottenuto i risultati migliori l'ultima volta. L'agente Cooperatore-Sfruttatore non avrà buoni risultati in questa impostazione, perché in un contesto evolutivo i Cooperatori incondizionati di solito scompaiono quasi immediatamente se ci sono agenti che non sono "gentili" (nel senso tecnico di non essere mai i primi a tradire). Tit for Tat, o qualcosa di simile, di solito finisce per essere il re di qualsiasi torneo evolutivo.  
  
C'è una scappatoia in questa configurazione di gioco, il tipo di scappatoia che fa alzare gli occhi al cielo a un umano reale per quanto possano essere irrealistiche le impostazioni formali. Se si giocano esattamente dieci iterazioni del dilemma del prigioniero in ogni turno, allora giocare "Tradisci" nel *decimo turno*, quando l'avversario non può più vendicarsi perché non c'è un undicesimo turno, darà risultati migliori rispetto al seguire Tit for Tat o qualsiasi altra strategia in quel turno. L'ultimo turno del gioco non è più un dilemma del prigioniero *iterato*, ma torna alla versione una tantum.  

Facile da risolvere: basta che ogni torneo continui per un numero casuale di turni, giusto? Non è realistico che gli agenti sappiano quando finisce il gioco. Nella realtà, non si è mai *sicuri* di non interagire mai più con qualcuno, e le persone si costruiscono una reputazione duratura.  

Tranne che a volte, nella vita reale, è abbastanza ovvio che il gioco è finito, come quando una delle parti di un accordo ottiene abbastanza potere da tradire l'altra, senza che ci sia una possibilità reale che ci sia una ritorsione in seguito. Questo è stato il modo di agire delle potenze europee che hanno cacciato i nativi americani dalle terre cedute con trattati che gli europei avevano deciso di volere.  

Qualunque effetto sulla loro reputazione gli europei si aspettassero per futuri accordi, evidentemente la punizione per aver tradito estranei, stranieri e persone di razza diversa non li spinse a mantenere i patti con i nativi americani. Quei Paesi sarebbero stati comunque considerati poco affidabili in campo diplomatico, indipendentemente da ciò che avessero fatto; non avevano una reputazione immacolata da perdere. Il loro istinto morale potrebbe essersi spento nei confronti degli stranieri; i decisori potrebbero essersi sentiti emotivamente neutrali riguardo al tradimento degli accordi con stranieri sconosciuti che non potevano più minacciarli.  

Gli europei, dal loro punto di vista, erano all'ultimo turno del gioco. Non è irrealistico dire che i dilemmi del prigioniero a volte sono per lo più una tantum e non così iterativi; la storia mostra che il risultato a volte è il tradimento. 

Questo non vuol dire che gli umani si tradiscano sempre nei dilemmi del prigioniero relativamente non ripetuti. Spesso gli umani collaborano in questi casi. Come detto in "[È improbabile che le IA siano oneste](#è-improbabile-che-le-ia-siano-oneste)", questo aspetto della natura umana potrebbe essersi evoluto perché abbiamo emozioni e istinti che sono stati costruiti dalla selezione naturale, che è un ottimizzatore con un forte collo di bottiglia sulle informazioni. La selezione naturale poteva darci solo spinte relativamente semplici, che dovevano coprire tutti i casi. Un altro fattore potrebbe essere il ruolo delle culture che danno grande importanza all'onore, soprattutto in modi che incoraggiano l'universalizzazione e il rafforzamento di questo concetto.

[^159]: Anche le superintelligenze artificiali sarebbero spinte a ottenere (Tradisci, Coopera) a loro favore: questo è ovviamente il motivo per cui il dilemma è un dilemma. Ma solo una parte ha un incentivo a volere che questo sia il risultato; entrambe le parti hanno un incentivo a preferire (Coopera, Coopera) a (Tradisci, Tradisci), il che apre più opzioni per raggiungere questo risultato.

[^160]: Nella storia dell'umanità, questo potrebbe forse essere paragonato alla pratica di due sovrani che consolidano un'alleanza sposandosi e avendo un figlio. Ma questa non è chiaramente una soluzione rapida e affidabile, nel caso umano, ed è ben lontana dalla progettazione reciproca di un delegato che entrambi comprendete nei dettagli e di cui vi fidate pienamente.

[^161]: Qui usiamo la dimostrazione come sostituto di metodi di ragionamento più generali, perché la dimostrazione è un po' come ragionare nei limiti della certezza logica. Non pensiamo che le IA funzionerebbero sulla base di dimostrazioni nella realtà (per vari motivi, tra cui il fatto che, per quanto le dimostrazioni logiche siano certe, non è noto se siano applicabili alla realtà). Ma la dimostrazione serve come utile sostituto formale del ragionamento nei modelli semplificati che stavamo studiando.

[^162]:  E poi siamo andati oltre, definendo agenti come PrudentBot, che tradisce alcuni "fessi" mentre continua a collaborare con quelli che dimostrano di collaborare con lui. Questo è il tipo di risultato che è più interessante se vi piace già la teoria dei giochi.

[^163]:  Non abbiamo fatto tutta quell'analisi per giustificare la conclusione che una superintelligenza non rispetterebbe i suoi accordi precedenti come strategia strumentale, dato che non ha preferenze finali riguardo al rispetto degli accordi. Questa era già la previsione del tutto ovvia della teoria dei giochi classica.  

Ma la teoria classica dei giochi suggeriva anche che le superintelligenze si sarebbero inevitabilmente tradite a vicenda, il che ci sembrava una conclusione un po' più traballante. Quindi abbiamo seguito quell'intuizione e abbiamo trovato dei difetti nell'analisi classica. Facendo ciò, abbiamo scoperto molte cose nuove su come le superintelligenze potrebbero potenzialmente raggiungere una cooperazione reciproca nel dilemma del prigioniero e, purtroppo, il risultato finale è stato che gli esseri umani mortali non sarebbero in grado di fidarsi o partecipare a quella tecnologia di negoziazione come potrebbe fare una superintelligenza.

[^164]:  Il che, nel caso delle IA, non è facile come guardare le IA e capire se *loro* credono che manterranno l'accordo; bisognerebbe scrutare la superintelligenza che l'IA diventerà in futuro e analizzare correttamente i *suoi* processi decisionali. Il che ci sembra molto più difficile.

[^165]:  Sotto certi presupposti che non possono essere realizzati; in parole povere, richiede una quantità infinita di potenza di calcolo e un luogo perfettamente sicuro dove collocarlo.

[^166]: Visto che AIXI è impossibile da creare, potreste pensare che sia solo uno strumento teorico che non ha molto a che fare con la rivoluzione moderna dell'IA. In realtà, AIXI è stato studiato e usato come modello di intelligenza da un sacco di persone che oggi sono in prima linea nell'IA, tra cui [Shane Legg](https://arxiv.org/pdf/0712.3329) (co-fondatore di Google DeepMind), [Ilya Sutskever](https://x.com/shaneguML/status/1844759663990161753) (co-fondatore di OpenAI e co-inventore di AlexNet) e [David Silver](https://arxiv.org/pdf/0909.0801) (responsabile della ricerca su AlphaGo e AlphaZero).

[^167]:  \**qualche aspetto più sottile:* AIXI contiene tecnicamente esperienze coscienti, all'interno del suo modello di mondo, se la coscienza è indipendente dal substrato. Le ipotesi che AIXI usa per il suo ragionamento sono così enormi che possono essere considerate come universi a sé stanti, completi di osservatori che vivono all'interno di AIXI.  

Questi osservatori, però, non stanno manovrando AIXI; AIXI ottiene i suoi impressionanti risultati di previsione e direzione con le sue forze. Quindi l'esempio funziona, anche se in modo un po' strano.  

Un altro esempio ipotetico che può essere utilizzato per illustrare lo stesso concetto è una macchina del tempo non senziente che è stata programmata per eseguire una sequenza casuale di azioni, quindi viaggiare indietro nel tempo per "resettare" la linea temporale *a meno che* non si verifichi un risultato particolare. La [macchina del tempo](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligenza-ai-è-necessaria-per-un-futuro-amazing-ma-1#How_many_advanced_alien_species_are_sentient_) può premere "reset" più e più volte, tutte le volte che serve per imbattersi casualmente in un risultato particolare. Questo, in pratica, renderebbe la macchina del tempo uno strumento super potente e versatile per dirigere il futuro (se fosse fisicamente possibile costruirla, cosa che non è). Eppure, nonostante questo, la macchina del tempo è un dispositivo incredibilmente semplice, privo di qualsiasi forma di cognizione reale e, ovviamente, di esperienza cosciente.  

Per fare un esempio reale (anche se con un ottimizzatore molto più debole e limitato), l'evoluzione biologica stessa mostra che si possono ottenere risultati impressionanti in termini di controllo e progettazione senza che il "progettista" abbia alcuna esperienza cosciente.

[^168]: E anche prima di quel momento, quando possono già fare progetti e seguire le loro preferenze, dovremmo mantenere le promesse e gli impegni presi con loro, come detto in una nota a piè di pagina [altrove](#le-ia-non-manterranno-le-loro-promesse).

[^169]: E per dire una cosa (si spera) ovvia: non dovremmo andare in giro a creare una nuova specie di schiavi senzienti, che siano meccanici o meno. A questo punto, dovremmo saperlo bene.

[^170]:  Ci si potrebbe chiedere se l'IA eviterebbe queste distopie. "L'IA non finirebbe per annoiarsi e voler fare qualcos'altro?"  

Questi risultati possono sembrare noiosi *a noi*, ma è improbabile che la maggior parte delle superintelligenze si annoino per le stesse cose per cui si annoiano gli esseri umani — anzi, è improbabile che provino affatto "noia", se non hanno un certo tipo di eredità dettagliata dall'umanità o qualcosa di simile all'umanità. Si veda anche la discussione approfondita nel capitolo 5 che parla di [noia e piacere nella novità](#come-per-la-curiosità,-lo-stesso-vale-anche-per-varie-altre-pulsioni).

[^171]:  \**regolarmente violato:* Dall'abstract di un [articolo dell'inizio del 2024](https://arxiv.org/pdf/2402.18649): "La nostra indagine ha messo in luce diversi problemi di sicurezza, non solo all'interno del modello linguistico di grandi dimensioni stesso, ma anche nella sua integrazione con altri componenti. Abbiamo scoperto che, anche se OpenAI GPT-4 ha messo a punto un sacco di vincoli di sicurezza per migliorare le sue caratteristiche di sicurezza, questi vincoli sono ancora vulnerabili agli attacchi. Per dimostrare ancora di più le minacce reali delle vulnerabilità che abbiamo scoperto, abbiamo creato un attacco end-to-end in cui un avversario può ottenere illegalmente la cronologia delle chat dell'utente, senza bisogno di manipolare l'input dell'utente o di ottenere l'accesso diretto a OpenAI GPT-4.  

Più tardi nello stesso anno, [un altro articolo](https://arxiv.org/html/2309.02926v3) "ha scoperto un totale di 20 vulnerabilità in 11 framework integrati con modelli linguistici di grandi dimensioni, tra cui 19 vulnerabilità [di esecuzione di codice remoto] e 1 vulnerabilità di lettura/scrittura di file arbitrari".

[^172]:  \**vede le vostre e-mail:* Come riportato dalla [CNN](https://www.cnn.com/2024/06/13/tech/apple-ai-data-openai-artificial-intelligence): "Apple Intelligence avrà accesso a un sacco di vostri dati personali, dalle vostre comunicazioni scritte alle foto e ai video che avete fatto, fino a un registro dei vostri eventi in calendario. Non sembra esserci un modo per impedire ad Apple Intelligence di accedere a queste informazioni, a meno di non usare le sue funzionalità..."

[^173]:  Io (Yudkowsky) una volta l'ho dimostrato scommettendo con qualcuno 20 dollari contro 0 che, mentre io assumevo il ruolo dell'«IA» e lui quello del «guardiano» in una chat privata, sarei riuscito a convincerlo a [farmi uscire dalla scatola](https://www.yudkowsky.net/singularity/aibox). Ci sono riuscito. L'altra persona ha pagato. Non c'è stato nessun trucco intelligente; non ho barato e non ho offerto 21 dollari per farmi dare ragione. Ho semplicemente fatto le cose nel modo più difficile e ho vinto.

[^174]: E anche se gli scienziati iniziassero a decodificare i formati di dati umani: versioni comprese solo a metà di quei formati ispirerebbero i ricercatori di IA prima che i ricercatori di biologia completino il lavoro? Se così fosse, sarebbe un problema. L'aumento delle capacità umane ci sembra un campo di ricerca fantastico, ma non può sostituire la sospensione della ricerca e sviluppo sull'IA, come diciamo nella Parte III del libro.

[^175]:  \**tutte le istanze necessarie:* Probabilmente ci sono circa 200.000 esemplari di GPT-5 in esecuzione in un dato momento (ad agosto 2025, poco dopo il rilascio di GPT-5), che è forse meno della "civiltà" moderna e più simile a una piccola nazione. In definitiva, non diamo molto peso a questa analogia, poiché non pensiamo che le singole istanze di IA possano mai essere molto simili ai singoli esseri umani. Il punto importante qui è che un gran numero di istanze non dovrebbe essere particolarmente difficile da ottenere, se (contrariamente alla nostra ipotesi migliore) ciò dovesse rivelarsi importante per qualche motivo.

[^176]: Forse l'esempio più evidente è quello dei computer, con una teoria sostanziale elaborata da personaggi come Charles Babbage, Ada Lovelace, Alan Turing, Alonzo Church e altri.

[^177]:  Il capitolo 15 di *Nanosystems* di Eric Drexler raccoglie [altri esempi](https://nanosyste.ms/macromolecular_engineering/#15-2-macromolecular-objects-via-biotecnologia) di tecnologie con analogie nel mondo biologico.

[^178]:  \**più tempo del necessario:* Il nervo laringeo ricorrente della giraffa prende la strada panoramica per arrivare al cervello. Invece, il nervo laringeo superiore della giraffa prende la strada diretta ed è quindi piuttosto piccolo e veloce.

[^179]:  \**molto più lentamente:* La più antica prova definitiva di vita sotto forma di microfossili risale a 3,5 miliardi di anni fa, mentre prove più indirette indicano un'età più vicina ai 4 miliardi di anni. Le prime colonie multicellulari sembrano avere 2 miliardi di anni. La maggior parte della storia evolutiva è stata dedicata alla creazione di organismi unicellulari e poi di organismi unicellulari aggregativi, prima di imbattersi accidentalmente — l'evoluzione non prevede nulla — in qualche nuovo trucco che ha aperto la regione dello spazio di progettazione della "vita multicellulare", contenente tutte le piante e tutti gli animali.

[^180]:  \**probabilità che la mutazione si diffonda:* Se il vantaggio di idoneità di una mutazione è *s* \<\< 1 e la dimensione della popolazione è *N*, allora la probabilità che la mutazione si diffonda in tutta la popolazione (chiamata "fissazione") è [circa](https://pmc.ncbi.nlm.nih.gov/articles/PMC2607448/) 2*s*, e il tempo necessario affinché la mutazione si diffonda completamente è [circa](https://www.zoology.ubc.ca/~otto/Reprints/OttoWhitlock2003.pdf) 2 ln(*N*) / *s*.

[^181]: Anche all'interno delle proteine, alcuni legami covalenti sono possibili. Due aminoacidi cisteina possono formare un legame covalente zolfo-zolfo tra loro, dove due proteine si toccano o dove una proteina ripiegata tocca se stessa. È così che le unghie riescono ad essere più dure della pelle, o che i capelli sono più resistenti rispetto ai muscoli dello stesso diametro e lunghezza: molti legami zolfo-zolfo in una proteina che è composta per il 14% in massa da cisteina. Questo è anche il motivo per cui i capelli hanno un odore sgradevole e sulfureo quando vengono bruciati.  
  
Per lo più, però, la selezione naturale costruisce cose a partire dalle proteine, che hanno strutture covalenti, che poi si ripiegano in forme complicate a causa di forze statiche relativamente molto deboli. E le proteine di solito si legano ad altre proteine con la stessa debolezza.  
  
Per lo più, i legami covalenti sono sparsi in modo rado, ammesso che esistano. Aggiungere lo 0,1% di legami covalenti a una struttura non la rende forte come una molecola di diamante, dove ogni atomo di carbonio è legato in modo covalente ad altri quattro atomi di carbonio in una struttura geometrica rigida.

[^182]:  Il diamante è anche più fragile. L'estrema regolarità cristallina dei legami del diamante fa sì che si rompa tutto in una volta. Il ferro è meno fragile perché ogni enorme nucleo di ferro vive in una nuvola di elettroni e può essere spostato all'interno di quella nuvola senza rompersi.  
  
(I legami covalenti radi fanno sì che i materiali possano essere spostati più facilmente senza rompersi, *relativamente* alla loro resistenza. Ma le ossa si rompono comunque e il legno è meno duro dell'acciaio. Il che significa: sì, ci sono dei compromessi, ma la selezione naturale non è affatto vicina al limite di tali compromessi).

[^183]: Anche se Freitas doveva fare i conti con un problema in più, cioè che i suoi globuli rossi artificiali dovevano funzionare bene con il resto dei sistemi del corpo umano. La cellula avrebbe dovuto usare il glucosio presente nel sangue, per esempio, invece di ricaricarsi con l'elettricità. In questo senso, le stime di Freitas sono più conservative rispetto a quelle che avrebbe potuto fare se avesse potuto migliorare anche altre parti del corpo umano o partire da zero con un nuovo organismo o un robot.

[^184]:  Online potete leggere lunghe analisi sul perché non sarebbe utile per la biologia inventare ruote che girano liberamente. Un esempio di un problema comune è: come si fa a usare i vasi sanguigni per mandare il sangue alla ruota se questa gira liberamente? I vasi sanguigni finirebbero per attorcigliarsi quando la ruota si muove!  
  
I tre casi noti di invenzione della ruota sono a livello molecolare e quindi non hanno questi problemi anatomici. Le ruote biologiche sono macromolecole che di solito sono identiche a livello atomico. Non c'è bisogno di lubrificarle, pulirle o mandare nuove cellule per sostituire quelle vecchie e danneggiate. Queste tre ruote e ingranaggi funzionano perché sono fatte di molecole invece che di cellule, piegate come complessi proteici invece che cresciute in matrici di tessuto o depositate come chitina.

Allo stesso modo, online si possono leggere argomentazioni secondo cui gli animali che sviluppano ruote per la locomozione non sarebbero comunque così utili senza strade asfaltate. Ma i tre casi noti di ruote molecolari sono incredibilmente efficienti dal punto di vista termodinamico e occupano posizioni straordinariamente vitali per i loro organismi: non si può sostenere che l'ATP sintasi non sia una ruota utile da possedere. Le ruote che ruotano liberamente avrebbero più potenziali utilizzi nel corpo (e nella biochimica) rispetto al semplice utilizzo per sostituire i piedi.  

A tal proposito: alcuni dei [robot moderni più abili](https://www.youtube.com/watch?v=iL833P0Vino), che possono arrampicarsi su rocce o neve o stare in equilibrio su un arto e fare capriole all'indietro, hanno anche delle ruote aggiunte alle estremità dei piedi. Perché non dovrebbero? Per un ingegnere umano è abbastanza facile attaccare delle ruote alle estremità delle gambe. Il problema principale non è che le ruote siano inutili, ma che risulta difficile trovare un percorso evolutivo per realizzarle, anche se dal punto di vista di un progettista umano sono banali.

[^185]: All'epoca, Freitas interpretò i suoi numeri come un limite *massimo* alla velocità con cui questo processo poteva avvenire, ma questo si rivelò errato. L'analisi di Freitas aveva ipotizzato che la massa dei nanosistemi sarebbe stata dominata dalla schermatura dalle radiazioni, ma questo si basava su un presupposto (errato) in *Nanosystems*: che un singolo impulso di radiazioni avrebbe distrutto un nanosistema.  

Drexler aveva fatto questa ipotesi, come molte altre in *Nanosystems*, per essere prudente: ipotizzare che il problema fosse più difficile e dimostrare che era comunque risolvibile. Questo può essere appropriato in *Nanosystems*, ma significa che l'articolo di Freitas non è prudente nella sua stima.  

Dato che l'analisi di Freitas mette insieme numeri prudenti in direzioni diverse, non dà un limite chiaro, né superiore né inferiore, su quanto tempo ci vorrebbe ai replicatori per consumare la biosfera. È più una stima media. Forse il limite fisico reale di quanto velocemente la biosfera può essere consumata partendo da un singolo replicatore è di tre ore; forse è di trenta giorni. Quasi sicuramente non è di tre anni.

[^186]: Non stiamo dicendo che ci sia stata una brusca discontinuità nell'evoluzione dei primati; la società umana si è differenziata da quella degli scimpanzé inizialmente in modo lento, poi rapidamente. Stiamo dicendo che c'è un divario qualitativo, indipendentemente da quanto sia stata graduale la transizione. Si veda anche la nostra discussione sulle [soglie](#l-ia-supererà-delle-soglie-critiche-e-decollerà?).

[^187]: A volte sentiamo dire che non c'è motivo di preoccuparsi, perché possiamo creare *più* superintelligenze che si controllino a vicenda. Ci sono un sacco di motivi per cui queste proposte ci sembrano davvero poco plausibili, ma vale la pena sottolineare che idee come questa hanno lo stesso problema di base che abbiamo già menzionato diverse volte: *Abbiamo solo una possibilità per far funzionare questo piano ingegnoso.*  

È possibile effettuare alcuni test e osservazioni preliminari su configurazioni che *non* mettono effettivamente a rischio la vita di tutti gli abitanti del pianeta, ma il caso che conta non sarà proprio lo stesso. (E un piano del genere dovrebbe essere *molto* intelligente, perché non abbiamo idea di come fare in modo che [*qualunque* delle IA del gruppo](#e-se-ci-fossero-tante-ia-diverse?) si interessi a noi.)

[^188]: Potreste provare a far *credere* erroneamente a un'IA più debole di essere in grado di ottenere un vantaggio decisivo e cercare di addestrarla a non agire in quel modo anche quando vede quell'opzione. Ma addestrereste un sistema di IA che è [abbastanza stupido da farsi ingannare](#le-ia-intelligenti-individuano-bugie-e-opportunità.) e che vede armi finte invece di armi vere. Quindi la distribuzione potenzialmente letale sarebbe comunque notevolmente diversa dalla distribuzione di addestramento; c'è una differenza notevole tra sentirsi dire che hai un'arma che potrebbe uccidere i tuoi operatori e costruire effettivamente un'arma o una via di fuga da solo e comprenderla in dettaglio. L'IA che si lascia ingannare da opzioni false non è la stessa che vede opzioni reali.  
  
Un meccanismo di allineamento che funziona su IA abbastanza stupide da essere ingannate è un meccanismo di allineamento che è stato testato solo prima, ma che comunque deve funzionare dopo.

[^189]: Per esempio: la meccanica newtoniana ha fatto ogni sorta di previsioni empiriche incredibilmente accurate. Era una teoria matematica semplice e concisa con un enorme potere esplicativo che ha spazzato via tutte le teorie precedenti. Ma se si provasse a usarla per inviare carichi utili su pianeti lontani a velocità relativistiche, si fallirebbe comunque, perché la meccanica newtoniana non tiene conto degli effetti relativistici.  

Gli unici segnali di avvertimento che si avrebbero sarebbero piccoli indizi come la luce che sembra muoversi alla stessa velocità in tutte le direzioni in tutti i periodi dell'anno, la luce che si piega attorno al sole durante le eclissi e il perielio di Mercurio leggermente diverso da quello previsto dalla meccanica newtoniana. Piccole anomalie, rispetto all'enorme successo predittivo in mille ambiti empirici.  

Immaginate che, prima della scoperta della meccanica newtoniana, degli strani alieni offrissero alla Terra un accordo: ci avrebbero dato grandi ricchezze se fossimo riusciti a completare una consegna interstellare, ma se avessimo fallito saremmo stati distrutti. Immaginate che gli scienziati scoprissero la meccanica newtoniana, sostenendo che sicuramente *ora* avrebbero dovuto essere autorizzati a inviare la consegna. Avrebbero avuto a disposizione una montagna di prove empiriche a loro favore, in accordo con le nuove conoscenze scientifiche che stavano sbloccando nuove potenti tecnologie.  

Immaginate quanto coraggio avrebbe dovuto avere un regolatore per dire: "Eppure, non potete spiegare la precessione del perielio di Mercurio, quindi la risposta è 'no'".  

Sarebbe stato così ingiusto per gli scienziati! Avrebbero avuto così tante prove da presentare!  

(In effetti, un regolatore realistico probabilmente non potrebbe dire che la risposta dovrebbe comunque essere "no", il che è uno dei motivi per cui non siamo molto fiduciosi riguardo a una coalizione internazionale e pensiamo che la Terra debba semplicemente allontanarsi completamente dal problema.)  

Alla natura non interessano tutte le montagne di prove e previsioni accumulate dalla fisica newtoniana. La teoria crolla lo stesso quando ci spostiamo su energie e scale ben al di là di quelle che siamo stati in grado di osservare in precedenza. Semplicemente non funziona ad alte energie e lunghe distanze.  

Far funzionare le teorie scientifiche al primo tentativo è difficile.

[^190]: Come abbiamo visto nella nota 6 del capitolo 10, i fisici in realtà non danno i fattori di moltiplicazione dei neutroni in percentuale. Noi li diamo in questo modo per chiarezza, per i motivi spiegati nella nota di cui sopra.

[^191]:  *\*prima arma termonucleare:* Castle Bravo non è stata la prima detonazione di un *dispositivo* termonucleare (idrogeno); questo primato spetta al "Mike", delle dimensioni di un edificio, del [test di Ivy Mike](https://it.wikipedia.org/wiki/Ivy_Mike), che non utilizzava il litio.

[^192]:  \#Per qualche riflessione sul perché è davvero importante sapere cosa si sta facendo, si veda ["Intelligente" (di solito) implica "incorreggibile"](#"intelligente"-di-solito-implica-"incorreggibile"), [Il meccanismo profondo della direzione](#il-meccanismo-profondo-della-direzione) e [È difficile ottenere una pigrizia robusta](#è-difficile-ottenere-una-pigrizia-robusta).

[^193]:  \#Si veda “[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?)”

[^194]:  \**al pubblico:* Per esempio, in [Machines of Loving Grace](https://www.darioamodei.com/essay/machines-of-loving-grace), l'amministratore delegato di Anthropic Dario Amodei dice che le IA super-potenti sono come "un Paese di geni in un data center" e spiega un sacco di fantastici vantaggi per la salute, la ricchezza, la pace e il senso che queste menti potrebbero dare all'umanità. Conclude dicendo:   

> Le intuizioni umane fondamentali di equità, cooperazione, curiosità e autonomia sono difficili da contestare e sono cumulative in un modo che i nostri impulsi più distruttivi spesso non sono. [...] Queste semplici intuizioni, se portate alla loro logica conclusione, portano alla fine allo Stato di diritto, alla democrazia e ai valori dell'Illuminismo. Se non inevitabilmente, almeno come tendenza statistica, questo è il punto verso cui l'umanità era già diretta. L'intelligenza artificiale offre semplicemente l'opportunità di arrivarci più rapidamente, rendendo la logica più chiara e la destinazione più evidente.  

È un modo strano di presentare la convinzione che si stia sviluppando una tecnologia che, secondo me, ha una probabilità dal 10 al 25% di essere catastrofica per la civiltà, *anche considerando* gli enormi potenziali benefici in caso di successo. Anche se i livelli di pericolo sono bassi come crede Amodei, dovremmo cercare di trovare una terza alternativa oltre al "non procedere mai" e al "procedere a tutta velocità". E se si pensa di essere *costretti* ad andare avanti (perché gli altri lo stanno già facendo), bisognerebbe implorare i leader mondiali di porre fine a questa corsa suicida, in modo da poter trovare quella terza alternativa. Dipingere un quadro roseo sembra solo un modo per distrarre l'attenzione, quando in realtà si sta giocando con la vita di tutti.

[^195]:  Ad esempio, nella [testimonianza al Congresso](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf):  

> Come per le auto o gli aerei, dovremmo considerare i modelli di IA del futuro prossimo come macchine potenti che hanno una grande utilità, ma che possono essere pericolose se progettate male o se sottoposte a un uso improprio. […] I nuovi modelli di IA dovrebbero superare una serie di test di sicurezza sia durante lo sviluppo che prima di essere messi sul mercato o dati ai clienti. […] L'ideale sarebbe che gli standard stimolassero l'innovazione in materia di sicurezza invece di rallentare il progresso.  

Apprezziamo che Amodei abbia chiarito che, secondo lui, ci sono dei pericoli che devono essere affrontati. È un passo in più rispetto a quello che farebbero molti dirigenti aziendali. Ma paragonare una tecnologia che, secondo lui, ha una probabilità del 10-25% di causare una catastrofe a livello di civiltà alle auto e agli aerei sembra poco sincero.

[^196]: In molti casi, i laboratori di IA stanno lavorando attivamente *contro* la condivisione di un quadro utile e completo della situazione con i responsabili politici. In questo contesto, sembra particolarmente strano giustificare il proseguimento dello sviluppo con la motivazione che un'IA più potente potrebbe "convincere i legislatori".

[^197]:  \**si limitano a produrre dimostrazioni:* Per un esempio di qualcuno che ha fatto una proposta del genere (discutendo anche alcuni dei problemi), si veda l'articolo di Nick Bostrom su [Oracle AIs](https://nickbostrom.com/papers/oracle.pdf).

[^198]:  \#Ne parliamo più approfonditamente negli ultimi capitoli del libro.

[^199]: Non è chiaro quanto questi segnali di avvertimento vengano dall'IA che si comporta come pensa che un'IA dovrebbe comportarsi e quanto invece siano il risultato di un ragionamento strategico. Il fatto che non riusciamo a capire quali segnali di avvertimento siano reali non è incoraggiante; significa che gli ingegneri sono molto più propensi ad andare avanti dicendo "eh, quello probabilmente non era reale". Potrebbero anche avere ragione nella maggior parte dei casi, ma la maggior parte dei casi non è sufficiente quando un solo errore può essere fatale.  

Non è nemmeno chiaro per quanto tempo continueranno a verificarsi questo tipo di segnali di avvertimento. Le IA moderne sono ancora abbastanza stupide da confondere occasionalmente i test con la realtà, ma questo regime non durerà per sempre e sta già [cominciando a finire](https://arxiv.org/html/2505.23836). Un'intelligenza artificiale che sa di essere sottoposta a test potrebbe smettere di mostrare comportamenti preoccupanti nei luoghi in cui i supervisori possono vederla, anche se la tendenza di fondo rimane.

[^200]:  Delaminazione dovuta al ciclo di pressione. In parole povere: le sollecitazioni causate da molte immersioni hanno separato gli strati dello scafo, indebolendolo fino a farlo implodere.

[^201]: In termini numerici, il trasporto aereo è *così* sicuro che la società nel suo complesso potrebbe trarre vantaggio da un allentamento dei requisiti del controllo del traffico aereo per aspetti quali l'addestramento dei piloti e il carburante di emergenza, riducendo così il costo dei voli e inducendo più persone a volare piuttosto che guidare, salvando così più vite umane.

[^202]: Il punto *non* è che le IA reali avranno "funzioni di utilità" visibili ai programmatori che questi ultimi potranno determinare a loro piacimento. In effetti, gran parte del problema dell'allineamento dell'IA, come discusso nel capitolo 4, è che le IA moderne sviluppano preferenze che nessuno ha richiesto e che nessuno voleva.  
  
Invece, studiare il caso con le funzioni di utilità è un po' come proporre il tipo di esercizi di fisica che si trovano nei libri di matematica. Se non si riesce a capire come creare un modello di una sfera perfetta che rotola su un piano inclinato perfettamente liscio senza resistenza dell'aria, si avranno ancora più difficoltà con problemi più realistici. Soprattutto se si sta cercando di coinvolgere ricercatori esterni per studiare un problema che nessuno sa come risolvere, è utile ridurre la questione alle sue parti più semplici e basilari, dove si può proporre un rompicapo.

[^203]:  Oppure ostacolare in altro modo il meccanismo alla base dello scambio; l'IA non sarebbe necessariamente costituita da codice leggibile.

[^204]: O, almeno, questo è un modo in cui alcune idee intelligenti che abbiamo visto falliscono. Abbiamo visto proporre un sacco di idee intelligenti; questo piccolo rompicapo si rivela complicato.

[^205]: Abbiamo sempre avuto qualche problema con il termine "controllo dell'IA" perché sembra che si voglia creare un'IA che fa cose cattive e poi la si costringa a fare cose buone, mentre secondo noi il problema è più quello di creare un'IA che sia amichevole fin dall'inizio. Si veda anche il capitolo 4, nota 8, per un po' più di storia sul termine "allineamento dell'IA".

[^206]:  \#Si veda anche la nostra discussione approfondita (dopo il capitolo 13\) sul [creare una coalizione inclusiva](#mantenere-ampia-la-coalizione).

[^207]:  \#Si veda anche un [elenco di confronti tra l'allineamento dell'IA e le armi nucleari](#l-ia-non-sarà-diversa-da-tutti-i-precedenti-storici?).

[^208]:  \**era già stato arrestato tre volte:* Il [rapporto sulla sicurezza](https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.pdf) dell'INSAG-7 (pag. 51\) dice che a Chernobyl si è provato a fare dei test di spegnimento nel 1982, 1984 e 1985, prima del disastroso test del 1986, e che quest'ultimo è stato imbarazzantemente ritardato, al punto che gli operatori [si aspettavano di essere licenziati](https://chernobylcritical.blogspot.com/p/prelude-25-april-1986.html) se non avessero eseguito il test. 

[^209]:  \**era già stato rimandato tre volte:* Tecnicamente "rinviato tre volte e annullato una volta" secondo il [rapporto della Commissione Rogers](https://sma.nasa.gov/SignificantIncidents/assets/rogers_commission_report.pdf) (p. 17). Ma uno dei rinvii è avvenuto un mese prima in risposta a ritardi in una missione diversa, mentre gli altri tre si sono verificati in rapida successione nei giorni precedenti il lancio; sono questi ultimi tre che, secondo noi, hanno messo sotto pressione i dirigenti della NASA che pensavano che il loro compito fosse quello di lanciare gli shuttle spaziali.

[^210]:  Come, se non attraverso una coalizione internazionale? Consigliamo di investire nel [miglioramento dell'intelligenza umana adulta](#*-potrebbe-aiutare-a-risolvere-il-problema-dell-allineamento.), ma non è questo il tipo di idea su cui le persone devono essere d'accordo per concordare che chiudere la ricerca sulla superintelligenza artificiale sia una buona idea.

[^211]:  La presidente della Federal Trade Commission Lina Khan [ha detto](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html) nel 2023: "Ah, devo rimanere ottimista su questo. Quindi tendo a scommettere su un rischio più basso… Diciamo, che so, un quindici per cento \[che l'IA ci ucciderà tutti\]."

[^212]:  [Alcune](https://x.com/balajis/status/1725890626699628633) [persone](https://x.com/MatthewJBar/status/1958403809249464757) [sostengono](https://x.com/DeryaTR_/status/1958592366652125487) che dobbiamo correre il rischio adesso, per provare a salvare gli umani dal morire di morte naturale dovuta all'invecchiamento. Il corpo umano è incredibilmente complesso, ma con sufficienti progressi scientifici potremmo risolvere molte delle malattie che oggi diamo per scontate, come il cancro, le malattie cardiache e le varie malattie legate all'invecchiamento. Un'intelligenza artificiale più intelligente dell'uomo potrebbe portarci a questo risultato molto più rapidamente. Ritardare lo sviluppo della superintelligenza costa letteralmente delle vite.  
 
O meglio, costerebbe delle vite, se non fosse che la superintelligenza uccide esattamente le stesse persone.  

In realtà, le persone malate e morenti oggi hanno probabilmente maggiori possibilità di sopravvivere se l'umanità fa un passo indietro:  

-* La ricerca biomedica e la ricerca di cure e trattamenti possono andare avanti anche senza la superintelligenza. La terapia genica, i vaccini contro il cancro e altri nuovi approcci offrono enormi promesse che i ricercatori stanno appena iniziando a sfruttare.  
* Una tecnologia di IA con obiettivi ben precisi può persino aiutare ad accelerare questo sforzo, senza bisogno di mettere a rischio l'intero impegno umano costruendo un'IA generale più intelligente dell'uomo.  
* I metodi di [conservazione del cervello](https://www.alcor.org/membership/) possono essere usati per preservare le persone anche dopo che il loro cuore ha smesso di battere, fino a quando la scienza medica non sarà abbastanza avanzata da poterle rianimare e riportarle in salute. Il tipo di IA che potrebbe offrire l'immortalità potrebbe anche quasi sicuramente riportare in vita qualcuno da un cervello conservato in modo appropriato.  

(Più tranquillamente, una parte di queste persone [vi dirà](https://x.com/SottoNocce/status/1771420351265923137) che lo fanno per la propria immortalità personale e che sono disposte a mettere a rischio la vita di ogni adulto e bambino del pianeta anche solo per una piccola possibilità che loro e i loro cari possano raggiungerla. Questo ci sembra un comportamento da cattivi da film. A questi cattivi, il nostro consiglio è lo stesso che diamo agli altruisti: iscrivetevi alla conservazione del cervello. Vi offre probabilità migliori rispetto a quelle che vi darebbe una superintelligenza malvagia, e vi permette anche di evitare di mettere in grave pericolo ogni essere umano vivente nella vostra ricerca dell'immortalità\! Una soluzione vantaggiosa per tutti.)  

Anche se ci interessasse solo il benessere dei malati e dei moribondi, tentare la sorte con una combinazione di questi metodi sembra un'opzione migliore che tentare la sorte con la creazione di un'intelligenza artificiale superumana, sperando che si interessi a noi. (E che si interessi a noi nei [modi giusti](#interessarsi-a-noi-nel-modo-giusto-è-un-bersaglio-difficile-da-centrare.).) Le probabilità per l'intelligenza artificiale superumana sono decisamente contro di noi.  

Ma anche: per quanto ne sappiamo, nessuno ha mai chiesto ai malati e ai moribondi se vogliono mettere in grave pericolo le loro famiglie e i loro connazionali per tentare la sorte con una possibile cura derivante dalla superintelligenza. E alle famiglie e ai connazionali in questione non è stato certo chiesto se acconsentano a mettere a rischio la loro vita per questo folle esperimento scientifico.  

Non dobbiamo scommettere tutte le nostre vite su questa opzione, quando esistono molte altre opzioni.  

Chiediamo a chiunque abbia a cuore il benessere delle persone oggi di accelerare invece i metodi di cui sopra, evitando il più possibile tutto ciò che potrebbe avvicinarci anche solo un po' alla superintelligenza.  

Se semplicemente non credete che una superintelligenza ribelle ci ucciderebbe, è un conto. Ma accettare che probabilmente ci ucciderebbe tutti e dire che dobbiamo comunque correre il rischio è una follia. Ci sono altre opzioni per risolvere i problemi del mondo moderno. Per fare un paragone: se vivere in un ambiente ad alta quota vi mette a disagio, non è una scusa per buttarvi da un precipizio. Trovate un altro modo per arrivare ai piedi della montagna.

[^213]:  \**linea di confine netta:* Per un esempio di questa linea di confine netta che appare nella fantascienza, si veda *Little Fuzzy* di H. Beam Piper: "Tutto ciò che parla e accende un fuoco è un essere senziente, sì. Questa è la legge. Ma ciò non significa che tutto ciò che non lo fa non lo sia". Oppure si veda l'episodio *La misura di un uomo* di *Star Trek: The next generation*, in cui l'intelligenza e l'autocoscienza dimostrate da Data, un androide, sono sufficienti per conferirgli il diritto legale di rifiutare lo smantellamento.

[^214]: Per nostra esperienza, questi testi tendono a puntare molto sulla retorica e poco sulla sostanza, passando spesso silenziosamente tra affermazioni anche contraddittorie a seconda di ciò che è di moda o politicamente conveniente al momento. Non ne ricaviamo l'impressione che si tratti di descrizioni oneste e trasparenti *nemmeno delle reali posizioni dei responsabili dei laboratori,* il che li rende meno utili rispetto al leggere i pareri critici di altri. Ma questa è la nostra opinione; se affrontate la questione per la prima volta e volete valutare da soli se altre parti hanno buone contro-argomentazioni che qui non abbiamo trattato, non dovreste necessariamente prendere per oro colato ciò che diciamo sulle fonti migliori.

[^215]: Se vi serve l'input di un capo di laboratorio per qualcosa e ci chiedete un consiglio, diremmo che l'opzione *meno* peggiore è probabilmente Demis Hassabis. Tra i principali capi di laboratorio con cui almeno uno di noi ha avuto a che fare (che, al 2025, sono tutti) Hassabis è l'unico che abbiamo visto mantenere costantemente la parola data nei rapporti commerciali e che sembra aver preso meno decisioni distruttive.  
  
Detto questo, si tratta di una raccomandazione poco affidabile e puramente relativa. In termini assoluti, chiunque non abbia fondato un'azienda con una probabilità sostanziale di distruggere il mondo parte con un grande vantaggio in termini di credibilità rispetto ai capi di laboratorio. Abbiamo sicuramente sentito storie di persone che dicevano di essere così spaventate da Hassabis da non avere altra scelta che fondare le proprie aziende IA di frontiera per batterlo sul tempo; forse queste persone sanno qualcosa che noi non sappiamo.  

Il nostro consiglio principale ai responsabili politici su questo punto è quindi: se siete convinti del pericolo, non date alcun potere ai capi dei laboratori.  

Parlate con ricercatori indipendenti, o con imprenditori che non hanno interessi in gioco, o con scienziati esterni che hanno dimostrato di dire e fare cose sensate in questo campo. Non mettetevi in una posizione in cui potreste essere danneggiati da persone la cui caratteristica principale è quella di mentire al pubblico e mettere in pericolo le persone.

[^216]:  Persone come Sutton e Page sembrano agire nell'illusione che una maggiore intelligenza porti a una maggiore bontà, cosa che noi [abbiamo contestato altrove](#i-buoni-piloti-possono-dirigersi-verso-destinazioni-diverse). E anche se noi autori, per caso, siamo d'accordo con Sutton e Page sul fatto che sarebbe un disastro non costruire mai un'intelligenza artificiale più intelligente dell'uomo, pensiamo che correre per creare una superintelligenza potrebbe essere un vero disastro sia per la vita umana che per il futuro a lungo termine in generale, [anche da un punto di vista inclusivo, cosmopolita e non specista](#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?).

[^217]:  Non sarebbe la prima volta che un settore si abitua a rischi inutilmente elevati. Negli anni '80, gli anestesisti hanno ridotto il loro tasso di mortalità di *cento volte* adottando una serie di semplici standard di monitoraggio.  
  
Sembra che gli anestesisti abbiano causato per decenni centinaia di volte più morti del necessario, solo perché pensavano che il loro tasso di mortalità fosse già basso (confrontandolo, ad esempio, con i tassi di complicanze chirurgiche). Non si sono resi conto che avrebbero dovuto cercare di abbassare il tasso, come dicono [Hyman e Silver](https://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?referer=&amp;httpsredir=1&amp;article=1469&amp;context=wlulr):

> Negli anni '50, i tassi di mortalità variavano tra 1 e 10 su 10.000 interventi. La mortalità legata all'anestesia si è stabilizzata a questo tasso per più di due decenni.  
>  
> Dovremmo chiederci perché la mortalità legata all'anestesia sia rimasta più o meno la stessa per oltre vent'anni, a un tasso più di cento volte superiore a quello attuale. Il problema non era la mancanza di informazioni. Anzi, in quel periodo la sicurezza dell'anestesia era stata studiata a fondo. Un'ipotesi più plausibile è che gli anestesisti si siano abituati a un tasso di mortalità esemplare per gli standard sanitari, ma comunque superiore a quello che avrebbe dovuto essere. Da un punto di vista psicologico, questa bassa frequenza ha incoraggiato gli anestesisti a considerare ogni esito negativo come un evento tragico ma imprevedibile e inevitabile. In effetti, gli anestesisti probabilmente consideravano ogni singolo esito negativo come la manifestazione di un tasso di base irriducibile di incidenti medici.

[^218]: Gli ingegneri strutturali basano le loro stime di rischio su calcoli e misurazioni precise, mentre i numeri "p(catastrofe)" si basano principalmente sull'intuizione dei ricercatori di IA. Ma questo non ispira maggiore fiducia nelle pratiche ingegneristiche dei ricercatori di IA. Semmai, peggiora la situazione.  

Una stima meno solida e più soggettiva può sistematicamente sbagliare nella direzione del "troppo pessimistico", ma può anche sbagliare nella direzione del "troppo ottimistico". Il fatto che questi numeri siano meno affidabili non li rende *specificamente orientati al pessimismo*. Il fatto che i ricercatori di IA non riescano a basare le loro stime di rischio su qualcosa di più che intuizioni e argomentazioni qualitative, *anche se riescono a sviluppare IA sempre più intelligenti anno dopo anno*, è un ulteriore motivo di preoccupazione.  

Il fatto che le stime dei ricercatori di IA siano davvero spaventose e senza precedenti in qualsiasi disciplina tecnica non vuol dire che siano sbagliate nel senso che vorremmo. La corsa alla creazione di agenti autonomi molto più intelligenti degli esseri umani sembra il tipo di impresa che ha probabilmente *molto più* del 50% di possibilità di causare una catastrofe. Ancor prima di entrare nei dettagli, questo *sembra* il tipo di progetto che molto probabilmente andrà storto in un modo o nell'altro, e il tipo di progetto in cui l'andare storto può avere conseguenze enormi. E i dettagli, come abbiamo sostenuto nei capitoli 4, 5 e in tutto il libro, dipingono un quadro ancora più cupo di quanto suggerirebbe questa prima analisi.

[^219]:  \#Si veda anche la nostra discussione su [chi metteva in guardia da una corsa all'IA al ribasso](#non-è-solo-allarmismo-da-parte-dei-leader-dell-ia-per-aumentare-il-loro-status-e-raccogliere-investimenti?) anni prima che queste aziende si formassero.

[^220]: Per approfondire questo esempio: quando la cabina dell'Apollo 1 ha preso fuoco durante una simulazione di lancio il 27 gennaio 1967, la NASA ha imparato da ciò. Gli ingegneri conoscevano ogni componente del razzo e riuscirono a capire che il problema era probabilmente legato all'uso di fili di rame placcati in argento (il cui isolamento era stato danneggiato dal movimento della porta) vicino a una linea di raffreddamento a glicole etilenico/acqua soggetta a perdite. Capirono che la situazione era stata aggravata dall'atmosfera di ossigeno puro nella capsula e dai materiali infiammabili presenti nella cabina. Inoltre, il fatto che la cabina fosse pressurizzata implicava che era necessario sfiatare la cabina prima di poter aprire il portello, ma i comandi di sfiato si trovavano dietro l'incendio e la differenza di pressione era stata drammaticamente aggravata dal fuoco.  

Tutti e tre i membri dell'equipaggio dell'Apollo 1 sono morti.  

Questo tipo di errori è comune, anche quando sono in gioco vite umane. Sono comuni anche per gli ingegneri missilistici che hanno a che fare con dispositivi che spesso esplodono visibilmente sulla rampa di lancio, anche tra persone che agiscono con cautela e prendono sul serio le proprie responsabilità.  

Quello che distingue gli scienziati dagli alchimisti non è che gli scienziati non sbagliano mai. È che gli scienziati possono fare piani che funzionano così bene che possono imparare dai primi fallimenti. Gli alchimisti vedevano i loro colleghi impazzire, ma non sapevano quali sostanze fossero velenose, quindi non sapevano cosa fare diversamente. La NASA, invece, è riuscita a individuare le probabili cause del problema e a costruire un nuovo veicolo spaziale che ha funzionato in quindici delle sedici missioni successive. (Sette delle quali hanno tentato l'allunaggio e una delle quali è fallita. La missione fallita, l'Apollo 13, ha anche avuto problemi nella cabina che avrebbero potuto facilmente essere fatali, ma la padronanza della NASA dei sistemi che aveva progettato e l'abilità degli astronauti a bordo hanno permesso loro di tornare sani e salvi sulla Terra).  

L'Apollo 1 era *quasi* un razzo funzionante. L'intero apparato circostante di ingegneri e scienziati attenti era *quasi* il tipo di operazione che avrebbe potuto portare in sicurezza l'uomo sulla Luna, e quindi un grave errore è stato sufficiente per spingere la NASA a una configurazione che ha permesso di realizzare sei dei sette allunaggi.  

Le aziende moderne di IA non sono neanche lontanamente vicine a mostrare quel livello di [rispetto](#sappiamo-riconoscere-quando-un-problema-viene-trattato-con-rispetto,-e-non-è-questo-il-caso) per il problema, quel livello di attenzione e dettaglio nei loro piani, quel livello di avvicinamento al fare bene il lavoro. Quando la loro IA fa qualcosa che non capiscono, non sono neanche lontanamente in grado di risalire all'analogo dei fili argentati. Non vi sono abbastanza vicini per imparare dai propri errori.  

Non trattano il problema come farebbero i giovani controllori del traffico aereo, scienziati missilistici o specialisti nucleari, presentando proposte accurate con [ipotesi di sicurezza](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) esplicite e non facendo nulla di pericoloso finché non hanno teorie sufficientemente sviluppate da poter almeno imparare dai propri errori.

[^221]:  Noi [non consigliamo](#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l-ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?) una coalizione internazionale sull'intelligenza artificiale, ma è il tipo di cosa che in teoria potrebbe portare a un'entità simile alla NASA o alla FAA, in grado di imparare davvero dagli errori del settore.

[^222]:  \**a fronte di benefici piuttosto limitati:* Si veda ad esempio [questo articolo del 2018](https://pmc.ncbi.nlm.nih.gov/articles/PMC7119956/) o un'analisi [rischi/benefici](https://osp.od.nih.gov/wp-content/uploads/2015/12/Risk%20and%20Benefit%20Analysis%20of%20Gain%20of%20Function%20Research%20-%20Draft%20Final%20Report.pdf) molto più approfondita del 2015\.

[^223]:  \**continua in gran parte senza ostacoli:* A partire dal 2025, gli Stati Uniti sembrano intenzionati a [smettere di finanziare attivamente](https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-127.html) la ricerca sul guadagno di funzione con soldi pubblici, ma c'è stato poco o nessun coordinamento globale al riguardo. Si veda anche [questo rapporto](https://cset.georgetown.edu/publication/understanding-the-global-gain-of-function-research-landscape).

[^224]: Se i laboratori di biotecnologia fossero più bravi a evitare fughe di patogeni e se la creazione di virus iper-letali portasse in qualche modo (ad esempio) a medicine iper-curative, allora forse continuare la ricerca avrebbe senso. Per quanto ne sappiamo, la ricerca sul guadagno di funzione non ha portato a risultati positivi di questo tipo, e i biologi tendono a [sconsigliarla](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Esvelt%20Testimony.pdf) [in quanto](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Ebright%20Testimony%20Updated.pdf) [tale](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Quay%20Testimony.pdf). Quindi pensiamo che sia uno di quei rari campi di ricerca da cui l'umanità dovrebbe stare alla larga, perché mette a rischio la vita di un sacco di persone che non hanno scelto di rischiare la propria vita.

[^225]:  Tenete presente che mettere degli interruttori di spegnimento nei chip e creare dei protocolli per spegnere i data center chiaramente non basta a risolvere il problema, visto che [potremmo non ricevere alcun segnale di avvertimento](#ci-saranno-segnali-di-avvertimento?) e [potremmo non reagire in modo efficace ai segnali di avvertimento](#l-umanità-non-è-molto-brava-a-reagire-agli-shock.). Ma è un passo relativamente economico che è del tutto possibile compiere, e che potrebbe essere d'aiuto in casi marginali in cui il rischio è *quasi* trascurabile, ma sarebbe utile avere un margine di sicurezza maggiore.

[^226]: Se la società teme davvero che questo rallenti troppo il mondo, consigliamo di accelerare il mondo altrove. Lasciamo che le persone costruiscano più centrali nucleari. Lasciamo che i biochimici facciano più esperimenti, non su virus mortali, ma su come rendere le persone più sane, più forti e più intelligenti.  

(Ovviamente, la società in generale non chiede a gran voce una scienza folle, quanto piuttosto resiste al cambiamento del status quo. Ma a chi dice "non possiamo fermare l'IA perché è importante per il progresso della civiltà", la risposta giusta è che ci sono molti progressi da fare altrove, con quel tipo di scienza folle che lascia dietro di sé dei sopravvissuti).

[^227]: Ricordate che noi sosteniamo trattati in base ai quali nemmeno i *governi* possono costruire superintelligenze. Non stiamo chiedendo che una tecnologia potente venga costruita da attori statali invece che da aziende; stiamo chiedendo che una tecnologia letalmente pericolosa *non venga costruita affatto*, almeno in un contesto simile a quello odierno.

[^228]:  È possibile che, ad esempio, i ricercatori trovino metodi più efficienti studiando i modelli linguistici di grandi dimensioni esistenti fino a quando non capiranno meglio come funzionano.  

Una ricerca come questa potrebbe permettere alle persone di creare le IA invece di farle crescere? Potrebbe essere d'aiuto\! Purtroppo, ci aspettiamo che, molto prima che le persone arrivino a capire bene cosa succede nei modelli linguistici di grandi dimensioni, sviluppino una comprensione parziale e incompleta che permetterebbe loro di costruire IA molto più efficienti, ma non abbastanza per allinearle.

[^229]:  [Volontà estrapolata coerente](https://baserates-test.vercel.app/w/coherent-extrapolated-volition-alignment-target) è il nostro tentativo di rispondere alla domanda "allineatia a chi?" se e quando arriveremo al punto in cui i creatori delle IA avranno la capacità di indirizzarle. La volontà estrapolata coerente cerca di risolvere i disaccordi morali e meta-morali principalmente incaricando l'IA di identificare i punti su cui le persone convergerebbero se sapessero di più, se fossero più simili al tipo di persona che vorrebbero essere, e così via (alla maniera delle [teorie dell'osservatore ideale](https://en.wikipedia.org/wiki/Ideal_observer_theory) in etica), e cercando meta-principi condivisi su cui l'IA possa fare affidamento nei casi in cui ci sia un disaccordo morale davvero fondamentale. (L'obiettivo non è necessariamente che l'IA "risolva tutti i problemi" della vita umana, ma solo che risolva *abbastanza* problemi da evitare che il risultato finale sia catastrofico). Consigliamo di estrapolare la volontà di tutti gli esseri umani viventi, non perché pensiamo che questo sia una sorta di ideale, ma perché è l'ovvio punto di coordinamento predefinito su cui molti stakeholder in disaccordo possono concordare (e perché altre entità, a cui gli umani viventi tengono, ottengono una certa influenza attraverso la volontà di quegli umani viventi; e lo stesso vale per altre entità a cui gli umani viventi *terrebbero* se sapessero di più e fossero più vicini a ciò che desiderano essere e così via).  

Ma ribadiamo: al momento vediamo questo argomento soprattutto come una distrazione. Non ha importanza raggiungere un accordo su nessuna di queste idee filosofiche di alto livello per agire su una tecnologia che sta per ucciderci tutti. Sarebbe profondamente sciocco lasciare che il lavoro di non proliferazione venga ostacolato da persone che discutono di idee brillanti come questa, comprese quelle che noi autori apprezziamo personalmente.  

Ciononostante, menzioniamo brevemente questa proposta, solo per chiarire che non stiamo cercando di eludere la questione, e forse per rassicurare i lettori che temono che possa essere impossibile trovare una proposta praticabile. Anche se la volontà estrapolata coerente fosse per qualche motivo l'approccio di alto livello sbagliato, il fatto che essa racchiuda molte proprietà desiderabili dovrebbe ispirare la speranza che sia possibile trovare una risposta non catastrofica a questa domanda.

[^230]:  Che [in realtà non è molto](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7acqua), contrariamente a quanto [ampiamente](https://fortune.com/article/how-much-water-does-ai-use/) [riportato](https://www.thetimes.com/uk/technology-uk/article/thirsty-chatgpt-uses-four-times-more-water-than-previously-thought-bc0pqswdr).

[^231]: Anche se vari funzionari di Washington sono d'accordo con le nostre preoccupazioni sulla superintelligenza, non hanno il potere di risolvere la questione a meno che non si uniscano molti più funzionari negli Stati Uniti e in altre nazioni. Le prime discussioni sono state promettenti, ma c'è ancora molto lavoro da fare.

[^232]:  Un altro segnale positivo che abbiamo notato mentre finivamo queste risorse online è che un bel po' di esperti di sicurezza nazionale ed ex funzionari di Washington hanno detto cose positive sulle copie in anteprima di *If Anyone Builds It, Everyone Dies*. Ecco qualche esempio:

Da Ben Bernanke, premio Nobel ed ex presidente della Federal Reserve: "Un resoconto chiaro e avvincente del rischio esistenziale che l'intelligenza artificiale altamente avanzata potrebbe rappresentare per l'umanità. Consigliato".

Da Jon Wolfsthal, direttore del Global Risk presso la Federation of American Scientists ed ex assistente speciale del presidente per gli affari di sicurezza nazionale: "Una tesi convincente secondo cui l'intelligenza artificiale superumana porterebbe quasi certamente all'annientamento globale dell'umanità. I governi di tutto il mondo devono riconoscere i rischi e intraprendere azioni collettive ed efficaci".

Dal tenente generale John N.T. "Jack" Shanahan (USAF, in pensione), primo direttore del Joint AI Center del Dipartimento della Difesa: "Anche se sono un po' scettico sul fatto che l'attuale sviluppo dell'intelligenza artificiale porterà all'estinzione umana, ammetto che questa mia opinione potrebbe essere dovuta a una mia mancanza di immaginazione. Visto il ritmo esponenziale con cui cambia l'intelligenza artificiale, non c'è momento migliore per prendere misure prudenti per proteggersi dai peggiori scenari possibili. Gli autori offrono proposte di importanza critica per la creazione di barriere di protezione globali e la mitigazione dei rischi che meritano una seria considerazione.

Da Fiona Hill, ex direttore senior del Consiglio di Sicurezza Nazionale della Casa Bianca: "Un libro serio sotto ogni aspetto. Nella fredda analisi di Yudkowsky e Soares, un'IA superpotente non avrà bisogno dell'umanità e avrà ampie capacità per eliminarci. *If Anyone Builds It, Everyone Dies* è un appello eloquente e urgente affinché ci allontaniamo dal baratro dell'autodistruzione".

Da R.P. Eddy, ex direttore del Consiglio di Sicurezza Nazionale della Casa Bianca: "Questo è il nostro avvertimento. Leggetelo oggi. Diffondetelo domani. Chiedete delle misure di sicurezza. Continuerò a scommettere sull'umanità, ma prima dobbiamo svegliarci".

Da Suzanne Spaulding, ex Sottosegretario del Dipartimento per la Sicurezza Interna: "Gli autori sollevano una questione incredibilmente seria che merita, anzi esige, la nostra attenzione".

Da Emma Sky, ricercatrice senior alla Yale Jackson School of Global Affairs ed ex consulente politico del comandante generale delle forze statunitensi in Iraq: "In *If Anyone Builds It, Everyone Dies*, Eliezer Yudkowsky e Nate Soares lanciano un monito forte e urgente: l'umanità sta correndo verso la creazione di una superintelligenza senza le garanzie necessarie per sopravvivere. Con credibilità, chiarezza e convinzione, sostengono che i sistemi di intelligenza artificiale avanzata, anche se solo leggermente disallineati, potrebbero portare alla fine della civiltà umana. Questo libro provocatorio sfida tecnologi, politici e cittadini ad affrontare i rischi esistenziali dell'intelligenza artificiale prima che sia troppo tardi. Un appello alla consapevolezza e un invito alla cautela, questa è una lettura essenziale per chiunque abbia a cuore il futuro.

[^233]: A volte vengono usati per altre attività che richiedono un sacco di potenza di calcolo, come la fisica e le simulazioni meteorologiche, ma sono *principalmente* usati per l'intelligenza artificiale. Un modo veloce per capire quanti chip di IA vengono usati per altre cose è guardare i [ricavi nel tempo](https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue) del principale produttore di chip, NVIDIA. Se pensiamo che il recente boom della domanda delle loro GPU per data center sia dovuto quasi interamente agli usi dell'IA (un'ipotesi ragionevole, visto l'enorme boom recente nel settore dell'IA e la mancanza di una tendenza simile in altri campi che usano questi chip) potremmo concludere che l'IA rappresenta la stragrande maggioranza dell'uso dei chip per l'IA, dato che la recente crescita dei ricavi supera di gran lunga quella precedente. Impedire la produzione di chip per l'IA specializzati non dovrebbe avere un grande effetto sull'hardware di consumo.

[^234]:  \**difficili da occultare:* Potrebbe essere possibile generare energia sul posto, eliminando così le linee di trasmissione che si notano. L'attuale Cheyenne Mountain Complex usa dei generatori diesel e probabilmente ha la [capacità di alimentare](https://www.af.mil/News/Article-Display/Article/497017/airmen-operate-americas-fortress/#:~:text=Each%20of%20the%20six%20generators,have%20a%20dedicated%20fire%20department.) circa 10.000 dei chip per l'IA più avanzati. Ma far funzionare questi chip in modo continuo per un grande ciclo di addestramento richiederebbe una fornitura costante di carburante, il che sarebbe evidente. Secondo un calcolo approssimativo, questi 10.000 chip richiederebbero circa un camion cisterna al giorno. Anche se ci fosse la capacità di generazione locale per alimentare 200.000 chip, sarebbero necessari 20 camion cisterna di diesel al giorno.  

I data center potrebbero anche essere alimentati da centrali nucleari. Per fortuna, molti attori statali hanno già esperienza nel monitorare la creazione di nuove centrali nucleari.

[^235]:  \**progresso algoritmico:* [Esempi](https://arxiv.org/abs/2507.10618) di questo tipo di progresso includono [FlashAttention](https://arxiv.org/abs/2205.14135), un algoritmo che fa sì che i chip per l'IA eseguano un certo insieme di operazioni matematiche in modo più efficiente sfruttando i dettagli della progettazione dei chip per l'IA; [Mixture-of-Experts](https://arxiv.org/abs/1701.06538), una modifica all'architettura delle IA che fa sì che solo un sottoinsieme dei loro parametri venga utilizzato su ogni token di input (ad esempio, una parola); e [GRPO](https://arxiv.org/abs/2402.03300), un metodo per il fine-tuning delle IA.

[^236]: Un altro possibile intervento, supponendo che il numero di ricercatori che lavorano sul progresso degli algoritmi di IA continui a essere piccolo (cioè nell'ordine delle centinaia o delle migliaia), sarebbe quello di pagare questi ricercatori per indirizzare i loro sforzi verso usi non legati all'IA o verso capacità di IA o ricerche di allineamento che presentano un rischio complessivo trascurabile. Esiste un precedente per questo tipo di intervento negli anni '90, quando il governo degli Stati Uniti avviò un'iniziativa per incanalare il lavoro degli ex scienziati e tecnici dell'esercito sovietico [verso attività produttive e non militari](https://www.armscontrol.org/act/1999-03/features/maintaining-proliferation-fight-former-soviet-union#:~:text=One%20of%20the%20earliest,productive%2C%20non%2Dmilitary%20endeavors).

[^237]: Potrebbe essere che gli Stati preoccupati per la superintelligenza preferiscano fare prima dei passi più piccoli, come quelli che non bloccano subito la ricerca e lo sviluppo dell'IA, ma che lasciano aperta la possibilità di farlo in futuro. Non consigliamo questa linea d'azione, perché pensiamo che la situazione sia già chiaramente fuori controllo e non siamo sicuri che la situazione diventerà molto più chiara prima che sia troppo tardi. Tuttavia, il team di governance tecnica del MIRI sta lavorando a delle proposte per questi scenari, nel caso possano essere utili. Potete seguire il loro lavoro [qui](https://techgov.intelligence.org/research).

[^238]: È il caso degli accordi sulle armi nucleari, dove trattati separati istituiscono l'AIEA ([1956](https://www.iaea.org/about/overview/statute), tramite la Conferenza sullo Statuto dell’Agenzia Internazionale per l’Energia Atomica, ospitata presso la sede delle Nazioni Unite), il TNP ([1970](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf), attraverso negoziati nel Comitato delle Diciotto Nazioni delle Nazioni Unite sul Disarmo) e gli accordi sul controllo degli armamenti come il trattato START ([1991](https://media.nti.org/documents/start_1_treaty.pdf), dopo nove anni di trattative tra gli Stati Uniti e l'Unione Sovietica).

[^239]:  Il [Trattato di Non Proliferazione Nucleare](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf) (comunemente chiamato "Trattato di Non Proliferazione") è entrato in vigore nel 1970 ed è stato prorogato a tempo indeterminato nel 1995. Noto per la sua adesione quasi universale (191 parti), il suo preambolo sottolinea il pericolo globale della proliferazione delle armi, affermando al contempo che i benefici delle applicazioni nucleari pacifiche dovrebbero essere accessibili a tutte le parti.

[^240]:  Al TNP viene generalmente attribuito il merito di aver mantenuto il numero di Stati nucleari più basso di quanto avrebbe potuto essere, ma si sono comunque verificati casi di acquisizione da parte di Stati non firmatari (India, Pakistan, Israele) ed ex firmatari (Corea del Nord). Qualsiasi Stato non firmatario che crei anche una sola superintelligenza artificiale rappresenta un pericolo paragonabile a uno scontro termonucleare su larga scala e deve essere trattato di conseguenza.

[^241]: La Convenzione sul Divieto o la Limitazione dell'Uso di Alcune Armi Convenzionali che Possono Essere Considerate Eccessivamente Dannose o Avere Effetti Indiscriminati, comunemente chiamata CCW (dall'inglese Certain Conventional Weapons), è entrata in vigore nel 1983. A partire dal 2024, le sue 128 parti si impegnano a proteggere i combattenti e i non combattenti da sofferenze inutili ed oltraggiose, limitando varie categorie di armi.

[^242]:  Questo è il doppio del limite indicato come chiaramente sicuro nel libro. Probabilmente sarà ancora sicuro per un po' di tempo, e valutare dove dovrebbero essere i limiti (e cambiarli nel tempo) è l'argomento dell'Articolo III, dell'Articolo V e dell'Articolo XIII.

[^243]:  L'[Organizzazione per la Proibizione delle Armi Chimiche](https://www.opcw.org/our-work) fa ispezioni, controlla la distruzione delle scorte di armi chimiche e aiuta a prepararsi per eventuali attacchi con armi chimiche, oltre a fare altre cose importanti per la [Convenzione sulle Armi Chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) (CAC). La CAC è entrata in vigore nel 1997; le sue 193 parti lavorano per attuare e mantenere il divieto di uso, sviluppo e diffusione delle armi chimiche e dei loro precursori, con alcune limitate eccezioni.

[^244]: L'AIEA è stata fondata nel 1957, più di un decennio prima del TNP. Il TNP ha potuto designare questo organismo già esistente per svolgere alcune funzioni. Nel caso dell'intelligenza artificiale, non esiste ancora un organismo internazionale di questo tipo, quindi la nostra bozza di trattato deve impegnare le parti a crearne uno.

[^245]:  Gli Stati Uniti e l'URSS avevano già deciso di smettere di fare altri tipi di test nucleari nel 1963 con il Trattato che Vieta i Test Nucleari nell'Atmosfera, nello Spazio e Sott'Acqua, comunemente chiamato [Trattato sulla Limitazione dei Test Nucleari](https://2009-2017.state.gov/t/avc/trty/199116.htm) o Trattato di Divieto dei Test.

[^246]: Il [Trattato tra l'Impero Britannico, la Francia, l'Italia, il Giappone e gli Stati Uniti d'America per la Limitazione degli Armamenti Navali](https://treaties.fcdo.gov.uk/data/Library2/pdf/1924-TS0005.pdf) (il Trattato Navale di Washington) elenca in una tabella (Sezione II) le navi da demolire, indicandone i nomi.

[^247]:  Il [Trattato sulla Riduzione delle Armi Strategiche](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html) è stato firmato nel 1991 ed è entrato in vigore nel 1994. A ciascuna delle parti firmatarie è stato vietato di schierare più di 6.000 testate nucleari su un totale di 1.600 missili balistici intercontinentali e bombardieri.

[^248]: Il [Piano d'Azione Congiunto Globale](https://www.europarl.europa.eu/cmsdata/122460/full-text-of-the-iran-nuclear-deal.pdf) è stato finalizzato nel 2015 tra i cinque membri permanenti del Consiglio di sicurezza delle Nazioni Unite, la Germania, l'Unione Europea e l'Iran. Quando è entrato in vigore nel gennaio 2016, l'Iran ha ottenuto un alleviamento delle sanzioni e altre disposizioni in cambio dell'accettazione di [restrizioni al suo programma nucleare](https://2009-2017.state.gov/documents/organization/245318.pdf).

[^249]:  I Colloqui sulla Limitazione delle Armi Strategiche (SALT, dall'inglese Strategic Arms Limitation Talks) sono iniziati nel 1969 tra gli Stati Uniti e l'URSS, portando al [trattato SALT I](https://treaties.un.org/doc/Publication/UNTS/Volume%20944/volume-944-I-13445-English.pdf), firmato nel 1972, che ha bloccato il numero di lanciatori di missili balistici strategici e ha regolamentato l'aggiunta di nuovi missili balistici lanciati da sottomarini, tra le altre restrizioni.

[^250]:  Il [Trattato sui Missili Antibalistici](https://en.wikisource.org/wiki/Anti-Ballistic_Missile_Treaty) del 1972 è nato dai colloqui SALT originali e ha limitato ciascuna parte a due complessi antibalistici (in seguito, solo uno) con restrizioni sul loro armamento e sulle loro capacità di tracciamento.

[^251]:  Con il [Trattato sulle Forze Nucleari a Medio Raggio](https://2009-2017.state.gov/t/avc/trty/102360.htm) del 1987, gli Stati Uniti e l'URSS hanno deciso di vietare la maggior parte dei sistemi di lancio nucleari con gittate comprese tra quelle dei sistemi da campo di battaglia e quelle dei sistemi intercontinentali. (Dato il breve tempo di preavviso che gli attacchi da tali sistemi avrebbero consentito, essi erano considerati più come sistemi offensivi destabilizzanti che come risorse difensive).

[^252]: Un altro aspetto importante da considerare per le misure di verifica dell'uso dei chip è la sicurezza e la privacy. Le parti vorranno assicurarsi che l'AISI abbia accesso solo alle informazioni necessarie per la verifica, senza poter accedere ai dati sensibili contenuti nei chip (come segreti militari o dati sensibili degli utenti). Pertanto, i metodi di verifica utilizzati dovrebbero essere sicuri e, quando possibile, avere un ambito di applicazione limitato.

[^253]: Il Food Safety and Inspection Service (FSIS) è un'agenzia del Dipartimento dell'Agricoltura degli Stati Uniti creata nel 1977.

[^254]:  L'Atomic Energy Act del 1946 è stato poi ampliat dall'[Atomic Energy Act del 1954](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf) con l'obiettivo di consentire lo sviluppo di un'industria nucleare civile, il che ha richiesto la condivisione di alcuni dati riservati con aziende private.

[^255]:  Il caso del 1979 [*United States v. The Progressive*](https://en.wikipedia.org/wiki/United_States_v._Progressive,_Inc.), in cui un giornale voleva rivelare il "segreto" della bomba all'idrogeno, avrebbe potuto dare alla Corte Suprema degli Stati Uniti l'occasione di decidere se la dottrina del "segreto nato" violasse le protezioni della libertà di parola del Primo Emendamento, se il governo non avesse ritirato il caso perché irrilevante.

[^256]: Un ramo del Dipartimento del Commercio degli Stati Uniti.

[^257]: Negli ultimi decenni sono state fatte centinaia di richieste di questo tipo per brevetti legati alla crittografia.

[^258]: In un'intervista del 2025 (https://www.theverge.com/decoder-podcast-with-nilay-patel/761830/amazon-david-luan-agi-lab-adept-ai-interview?utm_source=chatgpt.com), David Luan, capo del laboratorio di ricerca sull'IAG di Amazon, ha detto che ci sono meno di 150 persone a cui affiderebbe "un sacco di soldi in potenza di calcolo" per sviluppare un modello all'avanguardia.

[^259]:  L'International Science and Technology Center è nato dal programma [Nunn-Lugar Cooperative Threat Reduction](https://en.wikipedia.org/wiki/Nunn%E2%80%93Lugar_Cooperative_Threat_Reduction) del 1991, un'iniziativa degli Stati Uniti per mettere al sicuro e smantellare le armi di distruzione di massa e le infrastrutture associate negli ex Stati sovietici.

[^260]:  Le parti del nostro Trattato potrebbero voler esplorare la possibilità di estendere il concetto di [crimine contro l'umanità](https://www.law.cornell.edu/wex/crime_against_humanity) (come dice lo [Statuto di Roma della Corte penale internazionale](https://en.wikisource.org/wiki/Rome_Statute_of_the_International_Criminal_Court) del 1988) ai casi in cui un ricercatore cerca di sviluppare la superintelligenza artificiale a spese della popolazione della Terra.

[^261]: Il [Comitato per i Sistemi di Sicurezza Nazionale](https://en.wikipedia.org/wiki/Committee_on_National_Security_Systems) è un'organizzazione intergovernativa statunitense che definisce le politiche di sicurezza per i sistemi informativi governativi.

[^262]: 144 Stati, a giugno 2025.

[^263]:  Il punto VII.F dice che "\[...\] nel rispetto delle loro responsabilità nei confronti dell'Agenzia, \[il Direttore Generale e il personale\] non devono rivelare alcun segreto industriale o altre informazioni riservate di cui vengono a conoscenza nell'esercizio delle loro funzioni ufficiali per l'Agenzia".

[^264]: A volte vengono sostituiti da altri trattati. È stato il caso del [General Agreement on Tariffs and Trade (GATT)](https://www.wto.org/english/docs_e/legal_e/gatt47_e.htm) del 1947, che è stato sostituito dall'[accordo di Marrakech](https://www.wto.org/english/docs_e/legal_e/marag_e.htm) del 1994, che ha incorporato le regole del GATT ma ha creato l'Organizzazione Mondiale del Commercio per sostituire la struttura istituzionale del GATT. Anche i trattati a tempo indeterminato a volte finiscono quando le parti si ritirano in un modo che rende il trattato inefficace. Per esempio, gli Stati Uniti e l'URSS inizialmente avevano concordato il [Trattato sulle Forze Nucleari a Medio Raggio] del 1987 (https://2009-2017.state.gov/t/avc/trty/102360.htm) a tempo indeterminato, ma gli Stati Uniti si sono ritirati nel 2019 citando la mancata conformità della Russia, che in seguito ha annunciato che non avrebbe più rispettato il trattato nel 2025.
