#### **Possiamo lavorare insieme per fermare la superintelligenza pur essendo in disaccordo sul potenziamento umano.** {#we-can-work-together-to-stop-superintelligence-while-disagreeing-on-human-enhancement.}

Se non siete d'accordo con noi sull'idea del potenziamento umano, possiamo comunque trovare un accordo per fermare lo sviluppo dell'IA di frontiera.

Se non risolviamo questo problema, moriremo tutti. Tutti quelli che non vogliono morire *oggi* devono collaborare a tal fine. Possiamo aspettare che la minaccia di morte immediata sia passata per discutere se potenziare l'intelligenza umana debba essere illegale o sovvenzionato.

"Rendere gli esseri umani più intelligenti di Einstein" non è un piano per non morire nel 2028 o nel 2032 o quando ci sarà la prossima svolta fondamentale negli algoritmi di intelligenza artificiale.

Non è un piano che può andare di pari passo con lo sviluppo dell'IA. Anche se qualcuno usasse la tecnologia medica resa possibile da IA più stupide degli esseri umani per aumentare l'intelligenza umana ben oltre il livello di Einstein, tali potenziamenti probabilmente non sarebbero in grado di risolvere il problema dell'allineamento dell'IA e di progettare, creare e costruire in modo sicuro e *rapido* la superintelligenza artificiale, sotto la pressione temporale di una corsa agli armamenti. La corsa verso la superintelligenza deve comunque fermarsi.

L'idea alla base dell'aumento dell'intelligenza umana è che potrebbe *addirittura rendere possibile* risolvere il problema dell'allineamento, se un gran numero di ricercatori potenziati avesse anche un numero significativo di anni o decenni per lavorare sul problema. L'idea non è che potrebbero vincere una *corsa* per costruire una superintelligenza allineata tra sei anni, più velocemente di quanto il resto dell'industria dell'IA possa costruire e implementare una superintelligenza non allineata.

Molte persone che pensano che "potenziare l'intelligenza umana" sia un piano relativamente promettente, noi compresi, ritengono che i primi passi implichino ancora la chiusura delle aziende di IA.

Anche coloro che hanno altri piani su cosa dovrebbe fare l'umanità in seguito, in genere concordano sul fatto che il primo passo dovrebbe essere la chiusura delle aziende di IA.

Meta AI non può esistere, OpenAI non può esistere, Anthropic non può esistere; ci uccideranno. Possiamo essere d'accordo su questa priorità immediata, anche se abbiamo idee molto diverse su cosa fare dopo.

### "Allineati a chi?" {#"aligned-to-whom?"}

#### **Questa è una domanda spinosa. A prescindere dalla risposta, dobbiamo fermare lo sviluppo.** {#this-is-a-thorny-question.-regardless-of-the-answer,-we-need-to-halt-development.}

Se un giorno l'umanità creerà una superintelligenza, dovremo assicurarci che sia “allineata” con i valori umani. Ma con i valori di quali esseri umani, esattamente? Le persone hanno opinioni molto diverse su ciò che è giusto e sbagliato, sulla religione, sulle norme sociali, sui compromessi politici, ecc.

Al momento, la questione è puramente teorica. L'umanità non è in grado di inserire *alcun* obiettivo particolare in un'IA, quindi non importa se c'è disaccordo su *quali* obiettivi sarebbero ideali. Come abbiamo discusso a lungo, affrettarsi a costruire una superintelligenza porterebbe alla morte di tutti noi. L'umanità è in disaccordo su molte cose, ma la maggior parte delle persone concorda sul fatto che la distruzione di tutta la vita sulla Terra non è una cosa buona.

Il problema di quali valori *esattamente* dovrebbero essere caricati in un'IA sembra un problema spinoso. È un problema che, francamente, ci piacerebbe avere. Ci troviamo invece di fronte a un problema diverso e molto peggiore.

Non è affatto necessario essere d'accordo su “allineata a chi?” (o anche se l'umanità dovrebbe mai creare una superintelligenza) per coordinarci su un divieto internazionale, per la semplice e brutale ragione che altrimenti moriremo. L'IA solleva un'infinità di interessanti questioni filosofiche, ma se ci lasciamo distrarre eccessivamente da queste, rischiamo di far morire i nostri figli.

In pratica, il nostro consiglio ai leader mondiali è:

* Separare la domanda "Dovremmo affrettarci a costruire la superintelligenza?" dalla domanda "Se in qualche modo avessimo un modo per costruire la superintelligenza in modo sicuro, cosa dovremmo farne?" e concentrarsi prima sulla prima domanda. La prima domanda è quella urgente e quella su cui è possibile agire oggi. La seconda domanda potrebbe essere importante da affrontare un giorno, ma al momento è una trappola, perché incoraggia a pensare alla superintelligenza come a un premio. Credere erroneamente che il primo a costruire una superintelligenza possa decidere cosa farne ci porterebbe a una corsa suicida.

  L'ASI è un pulsante suicida, non un genio nella lampada. Quando qualcuno crea una superintelligenza, non "possiede" quella superintelligenza. Piuttosto, la superintelligenza che ha appena creato possiede un pianeta.  
* Se per qualche motivo in futuro sentiste il bisogno di affrontare l'argomento "Come dovrebbe usare l'umanità la superintelligenza, se mai fossimo in grado di farlo?", vi consigliamo vivamente di evitare proposte o idee che potrebbero spingere altri attori a gareggiare (o che potrebbero incoraggiare le nazioni a rifiutare o violare eventuali futuri [accordi internazionali](https://docs.google.com/document/d/1xPGtIuZRuzVmNJVgqjrUHRaPXHCRDUDhYO8zm_WmjD8/edit?tab=t.k1kf1fy9gx5i#heading=h.tdczv21ifj74) sulla superintelligenza). Qualsiasi dinamica del tipo "il vincitore prende tutto" ha un enorme potenziale di mettere in pericolo il mondo.

  Esistono proposte per gestire la difficile questione dell'"allineamento a chi" in modo relativamente universalistico, che cercano di essere eque con tutti i potenziali stakeholder e che non incentivano la corsa al traguardo — ad esempio, la proposta di allineare un'IA per perseguire la [volontà estrapolata coerente](https://www.lesswrong.com/w/coherent-extrapolated-volition-allineamento-target) di tutta l'umanità.[^229] Ma anche in questo caso, esiste un potenziale infinito di dibattito sui principi e sui compromessi coinvolti, così come sui dettagli spinosi dell'implementazione. Sarebbe importante risolvere queste discussioni in un mondo in cui l'umanità avesse capito come indirizzare una superintelligenza in modo preciso e robusto, ma metterle oggi al centro dell'attenzione travisa gravemente i compromessi reali che il mondo deve affrontare e rischia di far deragliare gli sforzi per coordinarsi su obiettivi condivisi come evitare la distruzione della Terra.

Anche quando si tratta di questioni di enorme importanza a lungo termine, [nulla dovrebbe essere associato alla sopravvivenza dell'umanità, se non la sopravvivenza dell'umanità stessa](https://docs.google.com/document/d/1xPGtIuZRuzVmNJVgqjrUHRaPXHCRDUDhYO8zm_WmjD8/edit?tab=t.k1kf1fy9gx5i#heading=h.gek8swcc3pef).

### Non è più intelligente evitare di parlare di estinzione? {#non-è-più-intelligente-evitare-di-parlare-di-estinzione?}

#### **Il tempo dei giochi politici è finito.** {#il-tempo-dei-giochi-politici-è-finito.}

Alcuni sostengono che chi è preoccupato per la corsa alla creazione di una superintelligenza dovrebbe tenere per sé le proprie opinioni e parlare invece della perdita di posti di lavoro causata dall'IA, o del problema dei bioterroristi che usano ChatGPT, o di quanta acqua serve per raffreddare i computer nei data center.[^230] Riteniamo che questo approccio sia un eccesso di furbizia e che rischi di ritorcersi contro. In effetti, abbiamo già visto che si è ritorto contro in diverse occasioni.

I quattro problemi principali che vediamo in questo approccio sono:

* **Non è onesto**, e le persone sono brave a riconoscere la disonestà e i doppi giochi.

  Anche se sei un bugiardo fuori dal comune, le argomentazioni su questioni che ritieni secondarie rischiano di apparire "stonate" in vari modi. Non sembreranno del tutto sensate, per le stesse ragioni per cui le ritieni, appunto, secondarie. Più condividi le tue argomentazioni edulcorate, più è probabile che le persone concludano che sei *o* confuso sulla questione *o* non del tutto onesto su ciò che pensi realmente. E in entrambi i casi, non apparirai come un buon alleato o una fonte di informazioni affidabile.  
* **Probabilmente non è necessario.** Secondo la nostra esperienza, una conversazione onesta e diretta sulla superintelligenza viene accolta molto meglio dei tentativi di deviare l'attenzione su altre questioni, come i deepfake dell'IA. Dalla metà del 2023 e con frequenza crescente, io (Soares) ho parlato con vari funzionari eletti. Ho partecipato a cene in cui persone "preoccupate per l'IA" hanno sollevato la possibilità di terroristi assistiti dall'IA, e un funzionario eletto in carica ha risposto che le sue paure sono molto più urgenti e gravi, perché teme che le IA capaci di auto-migliorarsi ricorsivamente possano produrre una superintelligenza in grado di spazzarci via completamente dalla mappa, e che potrebbe essere creata entro tre anni.

  Persino i funzionari eletti del Congresso degli Stati Uniti sono disposti a prendere sul serio la questione e a cercare modi per affrontarla.[^231] La questione può sembrare più di nicchia e controversa di quanto non sia in realtà, perché al momento della pubblicazione di questo libro non c'è ancora stato un vero e proprio dibattito pubblico nazionale o internazionale al riguardo. Ma abbiamo avuto molte conversazioni franche a Washington su questo argomento che sono andate incoraggiantemente bene.[^232]  
* **Rispondere a queste altre questioni non risolve il problema della superintelligenza.** Le aziende di IA stanno correndo per creare la superintelligenza. Se ci riescono, moriremo tutti. Le soluzioni che hanno senso per questo problema sono molto diverse da quelle che hanno senso per affrontare i deepfake generati dall'IA, o persino il bioterrorismo reso possibile dall'IA.

  La sovrapposizione non è *pari a zero*, e potremmo raccogliere più sostegno per affrontare l'IA più intelligente dell'uomo sottolineando le sovrapposizioni tra le diverse questioni. Ma è estremamente improbabile che il mondo riesca a trovare una risposta adeguata a una questione così complicata come la superintelligenza *senza concentrarsi sul problema reale*.  
* **È plausibile che il tempo a disposizione sia poco.** È improbabile che abbiamo il tempo di introdurre gradualmente le persone a questo rischio nel corso di molti anni, partendo da questioni più semplici e familiari per poi procedere gradualmente fino alla superintelligenza. Se non ci mobilitiamo rapidamente per rispondere a questo problema, è plausibile che non avremo alcuna possibilità di rispondere.

Questo non vuol dire che la perdita di posti di lavoro, il bioterrorismo, ecc. non siano problemi reali a sé stanti. È solo che la società non fermerà *davvero* la sconsiderata corsa al suicidio se non *sa* che ne è in corso una.

Abbiamo passato anni a osservare amici e conoscenti nell'ambito delle politiche pubbliche sondare il terreno su problemi come i bioterroristi potenziati da ChatGPT. Per quanto ne sappiamo, non sembra che tutto ciò sia approdato a qualcosa in grado di impedire davvero la creazione della superintelligenza.

Siamo nerd fino al midollo, ben lontani dalla nostra zona di comfort quando si tratta di scrivere un libro divulgativo. Non pretendiamo di essere esperti di politica efficace. Ma ci sembra che l'umanità abbia raggiunto il limite dei problemi che può superare con un discorso fatto di argomenti prudenti, scelti strategicamente e non «allarmistici».

A un certo punto, come esseri umani, dobbiamo iniziare a parlare della minaccia incombente. La politica deve fondarsi sulla realtà dei fatti, non su una comunicazione all'apparenza rassicurante.

I direttori dei laboratori di IA sostengono che potremmo vedere IA capaci di fare ricerca superare gli esseri umani nel giro di [uno](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year) [a](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s) [quattro](https://ia.samaltman.com/) anni. Speriamo con tutto il cuore che si sbaglino ma, con tutta la nostra esperienza, non possiamo *sapere* che hanno torto. Nemmeno i responsabili politici possono saperlo. L'umanità, semplicemente, non sta reagendo in modo adeguato alla sfida che ha di fronte. Se non si lancia l'allarme ora, quando?

Inoltre, da quando abbiamo steso la prima bozza di questo paragrafo, la strategia che sosteniamo sembra dare sempre più i suoi frutti, come si può vedere dalla [lista](#will-elected-officials-recognize-this-as-a-real-threat?) delle dichiarazioni dei politici sulla superintelligenza nel corso dell'estate del 2025\\. I tempi sembrano maturi per una discussione seria sul pericolo imminente rappresentato dalla superintelligenza artificiale.

### I rappresentanti eletti riconosceranno questa come una vera minaccia? {#will-elected-officials-recognize-this-as-a-real-threat?}

#### **Un numero crescente lo ha già fatto.** {#un-numero-crescente-lo-ha-già-fatto.}

Pensiamo che l'ostacolo principale che impedisce alla gente di riconoscere questa minaccia stia nel fargliela comprendere. Nei pochi mesi da quando il libro è stato mandato in stampa, ci sembra che il mondo stia già facendo passi avanti in questa direzione.

Ecco alcune dichiarazioni di politici statunitensi di entrambi gli schieramenti politici, nell'estate del 2025:

> La superintelligenza artificiale è una delle più grandi minacce esistenziali che ci troviamo ad affrontare. [...] Dovremmo anche preoccuparci che Stati autoritari come la Cina o la Russia possano perdere il controllo sui loro sistemi avanzati? [...] Ed è possibile che la perdita di controllo da parte di qualsiasi Stato-nazione, compresa la nostra, possa dare origine a un attore AGI o ASI indipendente con cui dovremo confrontarci a livello globale? \- [Jill Tokuda (D-HI)](https://peterwildeford.substack.com/p/congress-has-started-taking-agi-more), in un'[audizione del 25 giugno 2025](https://www.congress.gov/event/119th-congress/house-event/118428)
>
> Non voterò a favore dello sviluppo di Skynet e dell'ascesa delle macchine, distruggendo così il federalismo per 10 anni, togliendo agli Stati il diritto di regolamentare e legiferare su tutta l'intelligenza artificiale. \- [Marjorie Taylor-Green (R-GA)](https://x.com/RepMTG/status/1930650431253827806)
>
> Ci sono persone molto, molto competenti — e proprio oggi ho parlato con una di loro — che temono fortemente che gli esseri umani non saranno in grado di controllare la tecnologia e che l'intelligenza artificiale finirà per dominare la nostra società. Non saremo in grado di controllarla. Potrebbe essere lei a controllare noi. È una sorta di scenario apocalittico, e c'è una certa preoccupazione al riguardo tra le persone più competenti del settore. \- [Bernie Sanders (I-VT)](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611?utm_source=substack&amp;utm_medium=email)
>
> "Nel nostro affannarci per vincere la corsa all'IA contro la Cina, rischiamo di perdere noi stessi..." \- [Chris Murphy (D-CT)](https://www.chrismurphyct.com/p/in-our-scramble-to-win-the-ai-race)
>
> "Ciò rende più probabile che presto la sfida di leadership numero uno per i leader mondiali, compreso il Presidente degli Stati Uniti, sarà gestire i cambiamenti che l'IA sta determinando, e usare la visibilità della propria carica e gli strumenti della politica per garantire che questa tecnologia migliori la condizione delle persone, anziché peggiorarla." \- [Pete Buttigieg](https://petebuttigieg.substack.com/p/we-are-still-underreacting-on-ai), ex Segretario dei Trasporti

C'è ancora tanta strada da fare, ma il mondo sta iniziando a rendersene conto. I tempi sono maturi per allertare i funzionari sulla necessità di agire rapidamente a livello federale e internazionale.

### La situazione è disperata? {#is-the-situation-hopeless?}

#### **No** {#no.-4}

Questa è una battaglia che possiamo vincere, e vale la pena lottare per questo nostro mondo. Non sembra facile, ma è davvero fattibile.

Se vuoi unirti a noi per chiamare il mondo all'azione, saremo onorati di combattere al tuo fianco. Consulta l'ultimo capitolo del libro per scoprire alcuni modi in cui puoi contribuire.

## Approfondimento {#extended-discussion-11}

### Cosa servirebbe per arrestare lo sviluppo globale dell'IA? {#what-would-it-take-to-shut-down-global-ai-development?}

Non siamo esperti di diritto internazionale e questo è un argomento straordinariamente complicato, che a nostro avviso richiederà un notevole impegno da parte degli esperti del settore. Tuttavia, nell'interesse di avviare rapidamente il dibattito, abbiamo collaborato con il nostro [team di governance tecnica](https://techgov.intelligence.org/) e con consulenti esterni per raccogliere alcuni spunti e ipotesi su misure che potrebbero rivelarsi efficaci.

Le offriamo allo scopo di incoraggiare il dialogo, il dibattito, la critica e le successive iterazioni. Queste bozze iniziali non devono in alcun modo essere considerate come definitive o autorevoli.

Come primo passo, esaminiamo i vincoli e i contorni del problema che stiamo cercando di risolvere: un argomento che potrebbe facilmente riempire un libro intero. Il problema centrale, da decenni, è quello di impedire lo sviluppo della superintelligenza artificiale. E poiché non sappiamo dove si trovino le soglie critiche, ciò equivale essenzialmente a interrompere del tutto la ricerca e lo sviluppo nel campo dell'IA.

I progressi attuali nel campo dell'IA derivano da una combinazione di fattori: la creazione di chip più performanti, l'impiego di un maggior numero di chip per sessioni di addestramento più lunghe e il miglioramento degli algoritmi. Affronteremo a uno a uno ciascuno di questi aspetti, spiegando quali siano le leve corrispondenti per arrestare il progresso verso la superintelligenza artificiale.

#### **Impedire la creazione di chip per l'IA più numerosi e potenti** {#preventing-the-creation-of-more-and-better-ai-chips}

Aumentare le capacità delle moderne IA richiede un enorme investimento in potenza di calcolo ed energia elettrica. Di conseguenza, appare possibile per i moderni attori statali identificare e monitorare tutte le strutture pertinenti e prevenire l'emergere di nuove strutture simili, con un impatto minimo sull'hardware di consumo.

La [catena di approvvigionamento](https://www.csis.org/analysis/mapping-semiconductor-supply-chain-critical-role-indo-pacific-region) per la produzione di chip AI avanzati è estremamente concentrata. Per alcune fasi della catena di approvvigionamento, esiste una sola azienda al mondo in grado di svolgere quel ruolo, e queste aziende si trovano in gran parte in paesi tradizionalmente alleati degli Stati Uniti.

Per esempio, solo poche aziende possono fabbricare chip AI — principalmente l'azienda taiwanese TSMC — e uno dei macchinari fondamentali utilizzati nei chip di fascia alta è prodotto solo dall'azienda olandese ASML. Si tratta della macchina per litografia a ultravioletti estremi, grande come uno scuolabus, che pesa 200 tonnellate e costa [centinaia di milioni di dollari](https://www.datacenterdynamics.com/en/news/tsmc-to-receive-first-high-na-euv-lithography-machine-from-asml-in-q4/).

Questa catena di approvvigionamento è il risultato di decenni di innovazione e investimenti, e [si prevede](https://cset.georgetown.edu/publication/maintaining-the-ai-chip-competitive-advantage-of-the-united-states-and-its-allies/) che replicarla sarà piuttosto difficile — un'impresa che richiederà probabilmente più di un decennio, anche per i paesi tecnologicamente avanzati.

I chip AI più avanzati sono anche [molto specializzati](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/), quindi il loro tracciamento e monitoraggio avrebbe limitati effetti di ricaduta. Il chip H100 di NVIDIA, uno dei chip AI più comuni a metà del 2025, costa circa 30 000 dollari per unità ed è progettato per essere utilizzato in un datacenter, a causa dei suoi requisiti di raffreddamento e alimentazione. Questi chip sono ottimizzati per eseguire le operazioni numeriche coinvolte nell'addestramento e nell'esecuzione delle IA, e sono in genere [da decine a migliaia di volte](https://cset.ge Georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/) più performanti nei carichi di lavoro di IA rispetto ai computer standard (CPU di consumo).[^233]

La concentrazione e la complessità della catena di approvvigionamento dei chip per l'IA rendono più facile di quanto si potrebbe aspettarsi fermare lo sviluppo dell'intelligenza artificiale avanzata. **Interrompere la produzione di nuovi chip per l'IA sarebbe semplice**. Basterebbe un monitoraggio relativamente limitato di un piccolo numero di fornitori chiave per garantire che non vengano create catene di approvvigionamento segrete, data la complessità e l'interconnessione del processo di produzione.

Per produrre chip per l'IA e altri chip avanzati per computer (come quelli per i cellulari) si usano alcune delle stesse infrastrutture, ma ci sono differenze notevoli tra questi chip. Se la produzione di chip avanzati per l'IA venisse interrotta, sarebbe possibile controllare e assicurarsi che qualsiasi produzione di chip in corso crei solo chip non specializzati per l'IA.

I chip specializzati per l'IA già esistenti potrebbero essere controllati se venissero conservati e usati per far funzionare le IA esistenti, come ChatGPT. Garantire che tali chip vengano usati solo per far funzionare IA a bassa capacità (anziché per nuove attività di ricerca e sviluppo) sarebbe una sfida, ma non insormontabile. Le posizioni dei chip esistenti potrebbero essere tracciate e monitorate, e ci sono vari potenziali [meccanismi che potrebbero essere utilizzati per verificare](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) per cosa vengono usati tali chip. Questo tipo di monitoraggio richiede l'accesso fisico ai chip (ad esempio, ispettori che effettuano misurazioni in un datacenter). L'accesso remoto potrebbe essere sufficiente per la verifica se i nuovi chip fossero fabbricati con [una sicurezza migliorata](https://www.cnas.org/publications/reports/secure-governable-chips) e progettati tenendo conto della verifica e del monitoraggio. Come vedremo nella sezione seguente, le concentrazioni di chip necessarie per essere pericolose (al livello degli algoritmi di IA dell'agosto 2025) sono così grandi che non sarebbe difficile per gli attori statali individuare tutte queste strutture e sottoporle a ispezioni regolari.

#### **Impedire l'uso di chip per l'IA più numerosi e migliori** {#preventing-the-usage-of-more-and-better-ai-chips}

Spostando ora l'attenzione dalla produzione dei chip al loro utilizzo: gli attuali [più grandi datacenter per l'IA](https://epoch.ai/blog/trends-in-ai-supercomputers) ospitano centinaia di migliaia di chip per l'IA, che costano miliardi di dollari. Per addestrare una delle IA più potenti oggi disponibili, questi chip devono essere utilizzati per mesi e mesi.

Ciascuno di questi chip ha un [consumo energetico simile](https://ifp.org/future-of-ai-compute/) a quello di un'abitazione media americana, pertanto un data center con centinaia di migliaia di chip ha un consumo energetico paragonabile a quello di una piccola città. Per alimentare tutti questi chip sono necessarie infrastrutture elettriche specializzate, come grandi linee di trasmissione. Questi data center sono anche edifici piuttosto grandi con impronte termiche distintive, dovute al funzionamento e al raffreddamento continui di un gran numero di chip ad alto consumo energetico.

All'interno, questi data center ospitano migliaia di chip in armadi rack e dispongono di vaste infrastrutture di raffreddamento per evitare il surriscaldamento dei chip. Se si [entrasse](https://cloud.google.com/blog/products/gcp/google-data-center-360-tour) in uno di questi edifici, apparirebbe subito chiaro che si tratta di un data center. Il loro scopo non potrebbe di certo essere celato a eventuali ispettori internazionali, soprattutto qualora questi ultimi controllassero i chip presenti nel data center e scoprissero che si tratta di chip specializzati per l'IA.

**I grandi data center e le relative infrastrutture energetiche sono talmente imponenti da poter essere individuati dai satelliti in orbita.** Ciò significa che se i governi volessero localizzare i grandi data center attuali, molto probabilmente ci riuscirebbero con un elevato tasso di successo, indipendentemente dal fatto che si trovino all'interno dei propri confini o in altri paesi. Sebbene le conoscenze pubbliche in materia siano [limitate](https://epoch.ai/blog/trends-in-ai-supercomputers), questo solo intervento potrebbe bastare a rintracciare la maggior parte dei chip per l'IA di fascia alta.

In futuro, gli Stati potrebbero tentare di nascondere i loro data center per renderne più difficile l'individuazione satellitare. Potrebbero, ad esempio, provare a occultare un data center all'interno di una montagna (come nel Cheyenne Mountain Complex, che ospita il NORAD), dove non sarebbe visibile dall'alto. Anche così, sarebbe difficile nascondere le infrastrutture necessarie al suo funzionamento.

Il fattore principale che ne favorisce l'individuazione è il fabbisogno di elettricità molto elevato dei data center. L'elettricità viene solitamente fornita da linee di trasmissione, che sono quasi sempre fuori terra. Sebbene sia possibile interrare le linee di trasmissione, è un'operazione molto più costosa e lunga, e anche i lavori di costruzione necessari sono difficili da occultare.[^234]

Finché ci vorranno più di 100 000 chip per addestrare un'intelligenza artificiale all'avanguardia, sembra abbastanza probabile che gli attori statali possano individuare e monitorare tutti i data center rilevanti.

#### **Prevenire il progresso algoritmico** {#preventing-algorithmic-progress}

Algoritmi di IA più efficienti possono ridurre le risorse computazionali necessarie per addestrare un'IA, oppure possono consentire la produzione di IA più capaci utilizzando una data quantità di risorse computazionali, [o entrambe le cose](https://arxiv.org/abs/2311.15377).

Il progresso algoritmico è guidato principalmente dalla ricerca e dall'ingegneria, che attualmente dipendono dalle competenze e dall'impegno umano.[^235] Le competenze necessarie per migliorare gli algoritmi di IA sono relativamente rare, il che spiega gli [alti stipendi](https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html) dei migliori ricercatori del settore.

Anche se oggi queste competenze sono rare, non è chiaro come la situazione potrebbe cambiare con l'aumento del numero di ricercatori che entrano nel settore e la diffusione delle conoscenze. A seconda di come si conta il numero di persone con le competenze necessarie, il numero reale è probabilmente nell'ordine delle centinaia o delle migliaia (ad esempio, in base al numero di ricercatori e ingegneri di IA presso le [principali aziende di IA](https://fortune.com/2025/03/15/ai-talent-wars-startups-google-meta-openai-hiring-scientists-stock-salaries/)). Le stime conservative potrebbero essere molto più alte — ad esempio, ci sono [decine di milioni](https://www.griddynamics.com/blog/number-software-developers-world) di ingegneri software nel mondo.

**Gli interventi legali e sociali potrebbero rallentare drasticamente il progresso algoritmico.** La maggior parte delle persone non vuole infrangere la legge, soprattutto quando ci sono conseguenze reali. Se fosse illegale pubblicare determinate ricerche sull'IA o eseguire vari esperimenti sull'IA basati sui rischi catastrofici posti da un'IA sufficientemente capace, ciò probabilmente dissuaderebbe [quasi tutti i potenziali ricercatori nel campo dell'IA](#why-a-research-ban?-that-seems-extreme.). I governi potrebbero implementare controlli sulle esportazioni che renderebbero illegale la condivisione o la pubblicazione di tali ricerche senza una licenza di esportazione e l'approvazione del governo.

Anche i tabù sociali sarebbero d'aiuto. Un precedente è la [Conferenza di Asilomar sul DNA ricombinante](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA) del 1975, che portò a una moratoria volontaria su alcuni esperimenti biologici ritenuti eccessivamente rischiosi. In teoria, gli scienziati potrebbero istituire una moratoria volontaria sull'avanzamento delle capacità dell'IA. Tuttavia, ciò richiederebbe che questi scienziati prendessero sul serio il pericolo rappresentato da un'intelligenza artificiale più intelligente dell'uomo — un cambiamento rispetto allo status quo, che vede il progresso delle capacità dell'IA lodato in molti ambienti. Dati gli incentivi economici miopi e il comportamento tenuto finora dai laboratori, le restrizioni legali esterne appaiono estremamente probabili, a meno che la cultura del settore non cambi *radicalmente* (e in fretta).[^236]

Un elemento essenziale per rendere efficace un divieto imperfetto potrebbe essere una cosa tanto ovvia quanto "assicurarsi che i leader mondiali capiscano che [moriranno personalmente, loro e le loro famiglie](#wouldn’t-some-nations-reject-a-ban?), se continuano a spingere in quella direzione". Gli scenari di non conformità più probabili sono quelli in cui i governi vedono la superintelligenza sviluppata internamente come una risorsa strategica (o come un miraggio che li distrae da nuovi strumenti di IA redditizi), piuttosto che come un pulsante di suicidio globale. I governi sarebbero molto meno propensi a portare avanti progetti di ricerca segreti sulla superintelligenza se capissero correttamente che ciò equivale a caricare una pistola, puntarsela alla testa e premere il grilletto.

I divieti di ricerca non fermerebbero tutti. Alcuni importanti ricercatori scientifici e dirigenti tecnologici hanno [già affermato](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.ymh89tu1wrg5) che distruggere l'umanità è un prezzo accettabile da pagare per il progresso. Ma non dovremmo lasciare che il meglio sia nemico del bene. I progressi algoritmici quanto meno rallenterebbero se queste persone venissero private dei finanziamenti ed emarginate dai loro colleghi, costringendole a condurre le loro letali ricerche nell'illegalità e senza la collaborazione dei loro colleghi più retti.

#### **Più aspettiamo, più diventa difficile** {#più-aspettiamo,-più-diventa-difficile}

Se la produzione e la distribuzione dei chip di IA proseguono sulla traiettoria attuale, la sfida di garantire che un numero sufficiente di chip sia centralizzato e monitorato diventerà sempre più ardua. Anche se gli Stati non sono ancora convinti dei rischi, iniziare oggi a tracciare i chip di IA a livello internazionale significa che in futuro potrebbe rimanere possibile intervenire. Se i governi non si muovono in fretta, questa finestra temporale potrebbe chiudersi presto.

Se ai ricercatori sarà consentito di continuare a far progredire lo stato dell'arte degli algoritmi di IA, è probabile che un numero sempre più piccolo di chip di IA possa rappresentare una seria minaccia. Se e quando i sistemi di IA diventeranno in grado di automatizzare parti del processo di ricerca e sviluppo dell'IA, potrebbe diventare particolarmente difficile controllarne lo sviluppo. Tali sistemi potrebbero essere facilmente copiati e distribuiti e l'hardware necessario per eseguirli potrebbe non essere significativo. (I requisiti hardware per *eseguire* i sistemi di IA sono molto inferiori a quelli per *addestrare* i sistemi di IA).

Prima o poi, potrebbe diventare impossibile per i governi di tutto il mondo fermare lo sviluppo di sistemi di IA superintelligenti. Non siamo ancora a quel punto, ma ogni mese diventa più difficile. Il piano che delineiamo si basa sul presupposto di fermare lo sviluppo dell'IA *a breve*. Esistono altri piani che non si basano su questo presupposto, ma sono più difficili da attuare, hanno costi più elevati per le libertà personali e comportano maggiori probabilità di fallimento.

### Bozza provvisoria di un trattato, con annotazioni {#a-tentative-draft-of-a-treaty,-with-annotations}

Molte persone, compresi i membri della Camera dei Rappresentanti e del Senato degli Stati Uniti, ci hanno chiesto quali proposte legislative concrete e specifiche potrebbero effettivamente aiutare a risolvere questo problema.

Non siamo esperti di politiche pubbliche e vediamo molte possibili risposte a questa domanda, a seconda che si cerchi una legislazione più facile da approvare oggi ma meno direttamente utile (che funga più da trampolino di lancio per una legislazione più ambiziosa) o proposte che sembrano più difficili da approvare ma che affronterebbero in modo sostanziale le questioni fondamentali.

Abbiamo molto più da dire sulle seconde che sulle prime. A tal fine, il team di governance tecnica del MIRI ha elaborato una bozza preliminare di una di queste proposte.

Questa bozza di trattato è pensata per un mondo in cui i leader mondiali abbiano preso coscienza della realtà di un'IA più intelligente dell'uomo. Non è il genere di cosa che prevediamo possa essere approvata domani (nel momento in cui scriviamo), ma potrebbe non essere così difficile far ratificare un trattato simile una volta che il mondo sarà più consapevole del pericolo — un processo che è [già iniziato](#will-elected-officials-recognize-this-as-a-real-threat?) e che speriamo possa continuare.

[La bozza del trattato è disponibile qui](#a-tentative-draft-of-a-treaty,-with-annotations-1), con molte annotazioni. Il processo di stesura ha attinto ampiamente dai precedenti storici di altri trattati, riportati a margine della bozza. Sottolineiamo ancora una volta che questo è un punto di partenza, non un punto di arrivo. Non siamo esperti di politiche pubbliche e potremmo aver commesso alcuni errori ingenui. Tuttavia, speriamo che questa bozza possa servire da ispirazione e da esempio di come un trattato di questo tipo sarebbe possibile e in linea con analoghe iniziative legislative del passato.

### Mantenere ampia la coalizione {#keep-the-coalition-large}

C'è chi sostiene che dovremmo assumere una posizione netta contro l'arte generata dall'intelligenza artificiale o le armi robotiche, al fine di lanciare un messaggio più semplice: non contro la superintelligenza, ma contro l'*IA* in sé.

Tralasciando il merito delle varie posizioni sull'arte generata dall'IA, sui deepfake e così via, non riteniamo che questa sia l'opzione migliore dal punto di vista politico. Vogliamo costruire una coalizione per vietare la superintelligenza. Riteniamo che si tratti di una questione straordinariamente urgente e pressante, e vogliamo che questa coalizione sia la più ampia possibile, includendo persone con un'ampia varietà di opinioni sull'arte generata dall'IA, sulla guerra con i droni, sulle auto a guida autonoma, sull'uso dell'IA nelle scuole e così via.

Abbiamo tutti un interesse comune a impedire la creazione di una superintelligenza incontrollata, a prescindere dalle nostre posizioni su altre questioni.

Vogliamo che l'umanità si estingua, per essere sostituita da qualcosa di desolante? Chiunque risponda "no" a questa domanda può collaborare a uno sforzo comune e urgente per fermare la corsa alla superintelligenza.

Non pensiamo che la coalizione possa sopravvivere se ci si rifiuta di collaborare con chiunque la pensi diversamente sull'arte generata dall'IA o sulla guerra con i droni.

La coalizione non sopravviverà neanche se un insieme di altre questioni verrà abbinato alla superintelligenza, al punto che tutti debbano essere d'accordo su una lunga lista di questioni semi-correlate prima di poter collaborare sulla superintelligenza.

Se vi interessano altre questioni legate all'IA, vi invitiamo a lavorare per risolverle. Ma vi chiediamo di non *associare* tali questioni alla superintelligenza. Se vogliamo sopravvivere, nulla dovrebbe essere associato alla sopravvivenza dell'umanità.

Uno dei motivi per cui vivete nel Paese in cui vivete, e non in un cumulo di macerie radioattive lasciate dalla Terza Guerra Mondiale, è che decenni fa l'Oriente e l'Occidente convennero sul fatto che la guerra nucleare rappresentava una minaccia realistica e grave per l'umanità. L'Oriente e l'Occidente sostennero rispettivamente che l'Occidente e l'Oriente costituivano un'*ulteriore* terribile minaccia per l'umanità. Ma saggiamente trattarono le due minacce – l'annientamento nucleare contro la sconfitta ideologica – come diverse per natura.

Dal punto di vista dell'Occidente, era meglio che l'umanità fosse *meno* minacciata dalla guerra nucleare, anche se ancora minacciata dall'Oriente; ciò significava cooperare con l'Oriente abbastanza a lungo da stabilire una linea diretta tra Washington e Mosca e collaborare a trattati di non proliferazione e altri accordi.

Troppi Paesi devono coordinarsi. Troppe fazioni sono divise (anche al loro interno) per poter evitare la catastrofe se solo coloro che sono d'accordo su tutto possono agire insieme.

Facciamo causa comune, con gioia e senza riserve, con chiunque si preoccupi di [altre questioni](#il-mondo-è,-purtroppo,-abbastanza-grande-da-ospitare-molteplici-questioni.) nel mondo. Collaboreremo senza esitazione con persone con cui siamo in disaccordo politico. Abbiamo lanciato questo messaggio disperato al mondo perché è un messaggio in cui crediamo fermamente, e riteniamo che questo problema vada affrontato *immediatamente* a livello internazionale.

Chiunque tu sia, qualunque sia la causa per cui lotti, qui o altrove, se vuoi porre fine allo sviluppo vertiginoso di un'intelligenza artificiale più intelligente dell'uomo, combattiamo la stessa battaglia.

## Errata {#errata}

* Errore (nella prima stampa, edizioni USA e Regno Unito): a pagina 137, abbiamo scritto che @truth\_terminal è uscito nel 2023\. È uscito nel 2024\.  
* Errore (nella prima stampa, edizioni USA e Regno Unito): a pagina 213, abbiamo scritto che "\[l\]'intera rivoluzione tecnologica che ha portato a ChatGPT e ad altri LLM popolari è stata avviata da un articolo del 2018 che introduceva una nuova e ingegnosa disposizione aritmetica all'interno di una GPU, l'algoritmo "transformer", \[…\]". Si trattava in realtà di un articolo del 2017, intitolato "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", che ha portato alla creazione del primo GPT (GPT-1) nel 2018.

## Bozza di un trattato, con annotazioni {#a-tentative-draft-of-a-treaty,-with-annotations-1}

### Avvertenze {#disclaimer}

Di seguito, forniamo un esempio di testo per la stesura di un trattato che potrebbe essere implementato dai principali governi del mondo, qualora riconoscessero i pericoli derivanti dalla superintelligenza artificiale (ASI) e cercassero di impedire a chiunque di costruirla.[^237]

Non siamo decisori politici né esperti di diritto internazionale. Presentiamo questo testo come esempio illustrativo di alcune disposizioni che potrebbe essere prezioso includere in un trattato, utilizzando meccanismi su misura per la situazione attuale e fondati su precedenti storici.

Questa bozza di testo illustra diversi meccanismi che riteniamo necessari per impedire agli sviluppatori di IA di mettere seriamente in pericolo l'umanità. In pratica, prevediamo che diversi aspetti sarebbero probabilmente trattati da trattati distinti.[^238] E, naturalmente, la comunità internazionale dovrebbe redigere con cura l'intero trattato, sottoponendolo a negoziazioni e alla revisione di esperti del settore.

Per ogni articolo del trattato di esempio riportato di seguito, abbiamo fornito una sezione di commento che spiega le ragioni dietro le nostre decisioni chiave e una sezione in cui si analizzano alcuni precedenti rilevanti.

Un vero trattato includerebbe numerosi dettagli. Ne abbiamo inseriti alcuni a titolo di esempio, ma la maggior parte è relegata a degli "allegati" (che non sviluppiamo nella loro interezza). Molte delle quantità e delle soglie numeriche che usiamo nella nostra bozza costituiscono la nostra migliore stima, ma devono comunque essere considerate come tali. Molti di questi valori richiederebbero ulteriori studi e revisioni prima di poter essere finalizzati. È plausibile che questo tipo di dettagli non verrebbe incluso nel trattato stesso, analogamente a quanto avvenuto nel caso del Trattato di non proliferazione nucleare (TNP), in cui i dettagli specifici delle ispezioni e dei cosiddetti programmi di "salvaguardia" sono stati decisi tra ciascun paese e l'AIEA, piuttosto che essere inclusi nel TNP stesso. Tuttavia, per maggiore chiarezza, abbiamo mantenuto le nostre stime direttamente nel testo della bozza di trattato, per contribuire a renderlo più concreto.

### Preambolo {#preambolo}

Gli Stati che concludono il presente Trattato, di seguito denominati "le Parti del Trattato",

Allarmati dalla prospettiva che lo sviluppo della superintelligenza artificiale porterebbe alla morte di tutte le persone e alla fine di ogni impresa umana,

Affermando la necessità di un'azione internazionale urgente, coordinata e sostenuta per impedire la creazione e l'impiego della superintelligenza artificiale nelle condizioni attuali,

Convinti che le misure per impedire l'avanzamento delle capacità dell'intelligenza artificiale ridurranno il rischio di estinzione umana,

Riconoscendo che la stabilità del presente trattato dipende dalla capacità di verificare la conformità da parte di tutte le Parti,

Richiamando il precedente degli accordi sul controllo degli armamenti e sulla non proliferazione nell'affrontare le minacce alla sicurezza globale,

Impegnandosi a collaborare per facilitare la verifica delle attività di intelligenza artificiale a livello globale, purché queste si mantengano ben lontane dalla superintelligenza artificiale, e cercando di preservare l'accesso ai benefici dei sistemi di intelligenza artificiale, evitando al contempo i pericoli,

Hanno convenuto quanto segue:

---

#### Precedenti

Il preambolo di questo trattato è modellato su quello del [NPT](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf)[^239], che inizia così:

> Considerando la devastazione che una guerra nucleare causerebbe all'umanità intera e la conseguente necessità di compiere ogni sforzo per scongiurare il pericolo di una tale guerra e di adottare misure volte a salvaguardare la sicurezza dei popoli...

e subito aggiunge:

> Affermando il principio secondo cui i benefici delle applicazioni pacifiche della tecnologia nucleare, incluse le eventuali ricadute tecnologiche che gli Stati dotati di armi nucleari potrebbero ricavare dallo sviluppo di ordigni esplosivi nucleari, dovrebbero essere disponibili per scopi pacifici a tutte le Parti del Trattato, siano esse Stati dotati di armi nucleari o Stati non dotati di armi nucleari...

Così facendo, il preambolo invita il mondo a unirsi alle Parti responsabili nel salvaguardare l'umanità dalla minaccia catastrofica di una tecnologia potente e a condividere i benefici che possono essere tranquillamente consentiti. Il nostro preambolo cerca di seguire questo esempio.

---

### Articolo I: Scopo principale {#article-i:-primary-purpose}

Ciascuna Parte del presente Trattato si impegna a non sviluppare, dispiegare, o cercare di sviluppare o dispiegare in alcun modo la superintelligenza artificiale ("ASI"). Ciascuna Parte si impegna a vietare e impedire tale sviluppo all'interno dei propri confini e giurisdizioni e, data l'incertezza su quando ulteriori progressi potrebbero produrre un'ASI, si impegna a non intraprendere o permettere attività che costituiscano un progresso materiale verso l'ASI, come descritto nel presente Trattato. Ciascuna Parte assisterà, o non ostacolerà, le misure ragionevoli di altre Parti per dissuadere e prevenire tale sviluppo da parte di, e all'interno di, Stati e giurisdizioni non Parte. Ciascuna Parte darà attuazione e adempirà a tutti gli altri obblighi, misure e disposizioni di verifica stabiliti nel presente Trattato.

Laddove alcune classi di infrastrutture e capacità di IA che si mantengono lontane dall'ASI possano essere ritenute accettabili, ma solo in condizioni di supervisione internazionale, solo le Parti del Trattato possono svolgere tali attività, o possedere o gestire chip di IA e capacità di produzione che potrebbero potenzialmente portare allo sviluppo di un'ASI se non supervisionate. Alle non Parti è negato tale accesso per la sicurezza delle Parti e di tutta la vita sulla Terra (Articolo V, Articolo VI, Articolo VII).

Le Parti si impegnano a un processo di risoluzione delle controversie (Articolo XI) per ridurre al minimo le Azioni Protettive non necessarie (Articolo XII).

---

#### Precedenti

L'articolo I del TNP, come in molti trattati, stabilisce l'impegno di alto livello che le Parti assumono — in questo caso, di non condividere le proprie armi nucleari o aiutare altri a ottenerle:

> Ogni Stato Parte del Trattato dotato di armi nucleari si impegna a non trasferire a nessun destinatario, direttamente o indirettamente, armi nucleari o altri dispositivi esplosivi nucleari o il controllo su tali armi o dispositivi esplosivi; e a non assistere, incoraggiare o indurre in alcun modo Stati non dotati di armi nucleari a fabbricare o acquisire in altro modo armi nucleari o altri dispositivi esplosivi nucleari, o il controllo su tali armi o dispositivi esplosivi.

L'impegno riassunto nell'articolo I della nostra bozza di accordo è più forte, perché non si può permettere che avvenga un *breakout* di ASI da parte di chiunque e in nessun luogo, nemmeno una volta.[^240] Non basterà non “assistere, incoraggiare o indurre” altri a svilupparla. Abbiamo quindi incluso l'impegno ad “assistere, o non ostacolare, misure ragionevoli” da parte delle Parti per dissuadere e prevenire tale sviluppo ovunque.

Il TNP mira a contenere una minaccia esistente (le armi nucleari), mentre la nostra bozza di accordo mira a impedire che una minaccia (l'ASI) venga mai a esistere. Un precedente per la prevenzione dello sviluppo di nuove tecnologie pericolose si trova nel [Protocollo sulle armi laser accecanti](https://www.un.org/en/genocideprevention/documents/atrocity-crimes/Doc.43_CCW%20P-IV.pdf), parte della [Convenzione su alcune armi convenzionali](https://treaties.un.org/doc/Treaties/1983/12/19831202%2001-19%20AM/XXVI-2-revised.pdf#page=25).[^241] Il suo articolo I recita:

> È vietato impiegare armi laser specificamente progettate, come loro unica funzione di combattimento o come una delle loro funzioni di combattimento, per causare cecità permanente alla visione non potenziata, cioè all'occhio nudo o a un occhio dotato di dispositivi correttivi della vista. Le Alte Parti contraenti non trasferiranno tali armi a nessuno Stato o entità non statale.

Tale formulazione, tuttavia, non mira a impedire a chicchessia di testare o realizzare accidentalmente un sistema del genere. Il nostro accordo deve essere sufficientemente forte da impedire la realizzazione accidentale di un'ASI. Poiché non è chiaro dove possa trovarsi il punto di non ritorno, il nostro articolo I include l'impegno a "non intraprendere né consentire attività che portino a un progresso sostanziale verso l'ASI".

---

### Articolo II: Definizioni {#articolo-ii:-definizioni}

Ai fini del presente trattato:

1. **Intelligenza artificiale (IA)**: un sistema computazionale che svolge compiti che richiedono cognizione, pianificazione, apprendimento o azioni nei domini fisico, sociale o cibernetico. Ciò include i sistemi che svolgono compiti in condizioni variabili e imprevedibili, o che possono apprendere dall'esperienza e migliorare le proprie prestazioni.  
2. **Superintelligenza artificiale (ASI)**: è definita operativamente come qualsiasi IA con prestazioni cognitive sufficientemente sovrumane da poter pianificare ed eseguire con successo la distruzione dell'umanità.  
   1. Ai fini del presente trattato, lo sviluppo di IA che non è esplicitamente autorizzato dall'ISIA (articolo III) e che viola i limiti descritti nell'articolo IV si presume abbia lo scopo di creare una superintelligenza artificiale.  
3. Le **attività pericolose di IA** sono quelle che aumentano sostanzialmente il rischio che venga creata una superintelligenza artificiale, e non si limitano alla fase finale dello sviluppo di un'ASI, ma includono anche le fasi preliminari come stabilito in questo trattato. L'ambito completo delle attività pericolose di IA è specificato dagli articoli da IV a IX e può essere elaborato e modificato attraverso l'applicazione del trattato e le attività dell'ISIA.  
4. Per **Operazioni in virgola mobile (FLOP)** si intende la misura computazionale usata per quantificare la scala dell'addestramento e del post-addestramento, basata sul numero di operazioni matematiche eseguite. Il conteggio dei FLOP deve essere effettuato o come operazioni equivalenti al formato in virgola mobile a mezza precisione (FP16) o come operazioni totali (nel formato utilizzato), scegliendo il valore più alto.  
5. Per **Esecuzione dell'addestramento** si intende qualsiasi processo computazionale che ottimizza i parametri di un'IA (specifiche della propagazione delle informazioni attraverso una rete neurale, ad esempio pesi e bias) utilizzando metodi di ricerca/apprendimento basati sul gradiente o altri metodi, tra cui pre-addestramento, fine-tuning, apprendimento per rinforzo, ricerche di iperparametri su larga scala che aggiornano i parametri e auto-gioco iterativo o addestramento curriculare.  
6. Per **Pre-addestramento** si intende la fase di addestramento con cui i parametri di un'IA vengono inizialmente ottimizzati utilizzando *dataset* su larga scala per apprendere modelli o rappresentazioni generalizzabili prima di qualsiasi adattamento specifico per un'attività o un dominio. Include l'ottimizzazione supervisionata, non supervisionata, auto-supervisionata e basata sul rinforzo quando viene eseguita prima di tale adattamento.  
7. Per **Post-addestramento** si intende una fase di addestramento eseguita dopo il pre-addestramento di un modello. Inoltre, qualsiasi addestramento eseguito su un'IA creata prima che questo trattato entrasse in vigore è considerato post-addestramento.  
8. Per **Chip informatici avanzati** si intendono circuiti integrati fabbricati con processi avanzati almeno quanto il nodo di processo da 28 nanometri.  
9. Per **Chip di IA** si intendono circuiti integrati specializzati progettati principalmente per i calcoli di IA, incluse, a titolo esemplificativo ma non esaustivo, le operazioni di addestramento e inferenza per i modelli di apprendimento automatico [la definizione andrebbe precisata in un allegato]. Tra questi vi sono GPU, TPU, NPU e altri acceleratori di IA. Possono essere inclusi anche componenti hardware non originariamente progettati per l'IA, ma che possono essere efficacemente riadattati a tale scopo. I chip di IA sono un sottoinsieme dei chip informatici avanzati.  
10. Per **Hardware AI** si intende tutto l'hardware informatico utilizzato per l'addestramento e l'esecuzione delle IA. Ciò include i chip di IA, così come le apparecchiature di rete, di alimentazione e di raffreddamento.  
11. **Apparecchiature per la produzione di chip AI**: le apparecchiature utilizzate per fabbricare, testare, assemblare o confezionare chip per l’IA, tra cui, a titolo esemplificativo ma non esaustivo, le apparecchiature per litografia, deposizione, incisione, metrologia, collaudo e packaging avanzato [un elenco più completo andrebbe definito in un allegato].  
12. **Equivalente-H100**: unità di capacità di calcolo (FLOP al secondo) pari a quella di un acceleratore NVIDIA H100 SXM, 990 TFLOP/s in FP16, o a una potenza di elaborazione totale (Total Processing Performance, TPP) di 15 840, dove il TPP è calcolato come TPP \= 2 × MacTOPS non-sparse × (lunghezza in bit dell'input della moltiplicazione).  
13. **Cluster di chip coperti (CCC)**: qualsiasi insieme di chip per l’IA o cluster in rete con una capacità di calcolo effettiva aggregata superiore a 16 equivalenti-H100. Un cluster in rete si riferisce a chip che sono fisicamente co-locati, che hanno una larghezza di banda aggregata tra i nodi – definita come la somma della larghezza di banda tra host/chassis distinti – superiore a 25 Gbit/s, o che sono collegati in rete per eseguire carichi di lavoro in comune. La capacità di calcolo effettiva aggregata di 16 chip H100 è di 15 840 TFLOP/s, o 253 440 TPP, e si basa sulla somma dei TPP dei singoli chip. Esempi di CCC includono: il server GB200 NVL72, tre server H100 HGX a otto vie ubicati nello stesso edificio, CloudMatrix 384, un pod con 32 chip TPUv6e e qualsiasi supercomputer.  
14. **Mezzi tecnici nazionali (NTM)**: l'insieme delle capacità satellitari, aeree, informatiche, di intelligence dei segnali e di analisi delle immagini (comprese quelle termiche), e altre capacità di telerilevamento impiegate dalle Parti a fini di verifica, in conformità con il presente Trattato.  
15. **Verifica dell'uso dei chip**: i metodi che forniscono indicazioni sulle attività eseguite su specifici chip, al fine di distinguere le attività consentite da quelle proibite.  
16. **Per metodi utilizzati per creare modelli di frontiera** si intende l'ampia gamma di metodi utilizzati nello sviluppo dell'IA. Questi includono, a titolo esemplificativo ma non esaustivo, architetture di IA, ottimizzatori, metodi di tokenizzazione, cura dei dati, generazione di dati, strategie di parallelismo, algoritmi di addestramento (ad esempio algoritmi RL) e altri metodi di addestramento. Ciò include il post-addestramento, ma esclude i metodi che non modificano i parametri di un modello già addestrato, come il prompting. In futuro potrebbero essere creati nuovi metodi.

---

#### Note

La definizione di IA qui riportata (adattata dall'[AI Whistleblower Protection Act](https://www.congress.gov/bill/119th-congress/senate-bill/1792/text) del senatore Chuck Grassley) è forse troppo ampia, e prevediamo che dovrà essere definita in modo più preciso per escludere sistemi informatici palesemente sicuri come i correttori ortografici o i sistemi di riconoscimento delle immagini.

Nell'attuale paradigma di sviluppo dell'IA, questa definizione è probabilmente accettabile, dato che lo sviluppo dell'IA è ben distinguibile dalla ricerca e sviluppo non legati all'IA per via dell'uso di ingenti risorse computazionali. Tuttavia, la definizione definitiva di IA che verrà adottata dovrebbe comprendere più dei soli sistemi di deep learning o machine learning. Sebbene il machine learning sia l'attuale paradigma dominante per l'IA, il trattato dovrebbe tenere conto della possibilità che in futuro un altro paradigma di IA diventi preponderante; altrimenti, un trattato che vieti esclusivamente il machine learning incoraggerebbe i ricercatori a sviluppare nuovi paradigmi per costruire IA più potenti e generali, con conseguenze potenzialmente catastrofiche. Se dovesse emergere un paradigma innovativo, in particolare uno che non sia così intensivo nell'uso di chip per l'IA come il deep learning, il trattato andrebbe probabilmente aggiornato.

Usiamo l'H100-equivalente come metrica principale per la capacità di calcolo. Nell'articolo V, questo valore viene usato per stabilire la dimensione del più grande cluster di chip non monitorato consentito (16 H100-equivalenti).[^242] L'articolo IV definisce le soglie in termini di operazioni totali usate per addestrare un'IA; di conseguenza, fissare limiti alle operazioni non monitorate al secondo renderebbe i tempi necessari per condurre un addestramento di dimensioni illegali su hardware non monitorato proibitivi.

Usiamo gli H100-equivalenti per tenere conto del fatto che esistono diversi chip per l'IA, ma l'aspetto per noi più importante è il numero di operazioni che possono eseguire in un dato intervallo di tempo. Esistono altre metriche importanti per i chip (come la memoria ad alta larghezza di banda), ma nel complesso riteniamo che siano meno rilevanti del numero di operazioni al secondo. L'H100-equivalente come unità di misura è un modo [piuttosto standard](https://www.rand.org/pubs/perspectives/PEA3776-1.html) per discutere della capacità di calcolo.

La definizione di cluster di chip coperti (CCC) che includiamo è un'ipotesi iniziale su come definire tale concetto. Idealmente, il limite dovrebbe essere abbastanza alto da impedire alle persone comuni di infrangere le regole (ad esempio, una larghezza di banda di 25 Gbit/s tra chassis è più veloce delle normali connessioni Internet non da data center; il possesso di più di 16 H100-equivalenti da parte di un singolo individuo è un evento molto raro, oltre che costoso). Dovrebbe anche essere abbastanza basso da impedire attività di IA pericolose e rendere difficile la sovversione (cioè, rendere difficile l'addestramento distribuito su più insiemi di chip sub-CCC). Discutiamo questa decisione più approfonditamente nel commento all'articolo V.

I chip per l'IA sono un sottoinsieme dei chip avanzati e non esiste una linea di demarcazione netta che li distingua dai chip non dedicati all'IA. Anziché definire e fare affidamento su tale distinzione, usiamo la capacità di calcolo complessiva (in operazioni al secondo) di un cluster, misurata in H100-equivalenti. Se i chip possono essere configurati per l'addestramento o l'esecuzione di IA e superano la soglia definita, il Trattato ne richiede il monitoraggio.

Si noti che la dicitura "mezzi tecnici nazionali" (NTM) potrebbe essere considerata obsoleta come termine ufficiale [termine](https://fas.org/publication/ntm-obe/) da alcuni governi. La usiamo in questa bozza di trattato, seguendo lo stile dei precedenti accordi sul controllo degli armamenti per facilitare il confronto. |

### Articolo III: ISIA {#articolo-iii:-isia}

1. Le Parti del Trattato istituiscono l'Agenzia Internazionale per la Superintelligenza (ISIA), al fine di dare attuazione al presente Trattato e alle sue disposizioni, incluse quelle per la verifica internazionale del suo rispetto, e per fornire un foro di consultazione e cooperazione tra le Parti.  
2. Sono istituiti quali organi dell'ISIA: la Conferenza delle Parti, il Consiglio Esecutivo e il Segretariato Tecnico.  
3. Conferenza delle Parti  
   1. La Conferenza delle Parti è composta da tutte le Parti del Trattato.  
   2. La Conferenza delle Parti: determina la politica generale; adotta e supervisiona il bilancio; elegge i membri del Consiglio esecutivo; esamina le questioni di conformità segnalate dal Consiglio esecutivo; e adotta e rivede gli allegati su raccomandazione del Consiglio esecutivo.  
   3. Si riunisce in sessione ordinaria almeno una volta all'anno, o con una frequenza maggiore stabilita dalla Conferenza, oltre alle sessioni straordinarie che si rendessero necessarie. Ogni Parte ha diritto a un voto. Il quorum è costituito dalla maggioranza delle Parti.  
4. Consiglio esecutivo  
   1. Il Consiglio esecutivo è composto da 15 membri: (i) 5 seggi designati per i membri permanenti del Consiglio di sicurezza delle Nazioni Unite e (ii) 10 seggi elettivi distribuiti secondo un'equa rappresentanza geografica. I dettagli sono esposti nell'Allegato A.  
   2. I membri eletti restano in carica per due anni. Metà dei seggi viene rinnovata ogni anno.  
   3. Il Consiglio esecutivo: approva le ispezioni su richiesta; raccomanda il bilancio e la politica alla Conferenza; nomina il Direttore generale; supervisiona il Segretariato tecnico tramite il potere di veto.  
   4. I processi decisionali sono i seguenti:  
      1. Il Consiglio esecutivo elegge il proprio presidente e vicepresidente.  
      2. Il Presidente o il Vicepresidente possono fungere da presidente di seduta.  
      3. La votazione avviene secondo il principio "un membro, un voto".  
      4. Per l'approvazione di un'ispezione su richiesta, ai sensi dell'articolo X, è richiesta la maggioranza dei voti.  
      5. Per porre il veto alle modifiche delle definizioni o dei protocolli di salvaguardia apportate dal Segretariato tecnico è richiesta la maggioranza dei due terzi.  
      6. Per revocare o nominare un Direttore Generale è richiesta la maggioranza dei due terzi.  
      7. Tutte le altre decisioni richiedono la maggioranza dei due terzi.  
      8. Il quorum è di due terzi dei membri del Consiglio Esecutivo.  
5. Segretariato Tecnico e Direttore Generale  
   1. Il Direttore Generale del Segretariato Tecnico ne è il capo e il principale funzionario amministrativo.  
   2. Il Direttore Generale è nominato dal Consiglio Esecutivo per un mandato di quattro anni, rinnovabile una volta. Il Consiglio Esecutivo può revocare il Direttore Generale.  
   3. Il Segretariato Tecnico comprende inizialmente divisioni tecniche per il tracciamento dei chip e le misure di salvaguardia della produzione, le misure di salvaguardia per la verifica dell'uso dei chip, i controlli sulla ricerca, il consolidamento delle informazioni, le revisioni tecniche, l'amministrazione e le finanze, e le questioni legali e di conformità. Il Direttore Generale può creare e sciogliere divisioni tecniche.  
   4. Il Segretariato Tecnico, per tramite del Direttore Generale, propone le modifiche alle definizioni tecniche e ai protocolli di salvaguardia necessarie ad attuare gli articoli IV, V, VI, VII, VIII, IX e X del presente Trattato.  
      1. Le modifiche urgenti alle soglie FLOP (articolo IV), alle dimensioni dei cluster di calcolo interessati (articolo V) e ai limiti della ricerca soggetta a restrizioni (articolo VIII) possono essere attuate immediatamente dal Direttore Generale qualora l'inazione ponga un rischio per la sicurezza. Tali modifiche rimangono in vigore per trenta giorni. Trascorso tale termine, le modifiche devono essere approvate dal Consiglio esecutivo per rimanere in vigore.  
      2. Il Consiglio esecutivo delibera sulle questioni di sostanza, per quanto possibile, per consenso; il Direttore Generale si adopera per raggiungere il consenso. Qualora non sia possibile raggiungere il consenso entro 24 ore, si procede a una votazione e il Consiglio esecutivo approva le modifiche se la maggioranza dei membri presenti e votanti vota a favore, altrimenti le respinge.  
6. Il bilancio ordinario dell'ISIA è finanziato con i contributi obbligatori delle Parti, utilizzando una scala derivata dalla scala di valutazione delle Nazioni Unite, soggetta a un massimale e a un minimale stabiliti dal Consiglio esecutivo. Gli Stati membri hanno anche la possibilità di versare contributi volontari per la ricerca sulla sicurezza dell'IA relativa alle attività di allineamento, interpretabilità e sviluppo delle capacità degli Stati membri, compresi gli usi benefici dell'IA sicura, lo sviluppo di banchi di prova, le buone pratiche, la condivisione di informazioni e la facilitazione della cooperazione e delle attività congiunte liberamente ispirate al modello della [rete IAEA dei Centri di supporto alla sicurezza nucleare](https://www.iaea.org/services/networks/nssc).

---

#### Note

Come in altri organismi internazionali, l'ISIA sarebbe composta da diplomatici ed esperti tecnici dei paesi firmatari. Il punto principale di questa formulazione è che all'ISIA viene conferita l'autorità di attuare la maggior parte di quanto previsto dal trattato e di aggiornare alcuni aspetti dello stesso nel corso del tempo. Questo approccio dà la priorità alla prevenzione della creazione di superintelligenza e al mantenimento della forza del trattato fino a quando il mondo non sarà pronto a procedere. In questo modo, questa bozza conferirebbe a questa organizzazione multinazionale l'autorità di fare gran parte di quanto richiesto. L'ISIA centralizza l'attuazione di diverse funzioni chiave del trattato, tra cui il mantenimento dei limiti precisi della ricerca, dello sviluppo e del dispiegamento dell'IA consentiti, la verifica primaria del rispetto del trattato e il consolidamento delle informazioni di intelligence riservate dei firmatari. Un aspetto cruciale è che il funzionamento cooperativo dell'ISIA crea nel tempo la fiducia necessaria tra le parti del trattato.

Detto questo, questo tipo di approccio comporta dei compromessi. Il primo compromesso è che una maggiore centralizzazione richiede una maggiore fiducia da parte delle nazioni firmatarie. I potenziali firmatari del trattato potrebbero non ritenere politicamente fattibile assegnare questo livello di autorità a un'organizzazione internazionale, oppure potrebbero non avere fiducia che l'organizzazione possa operare in modo sufficientemente indipendente dall'influenza dominante del suo membro o dei suoi membri più potenti.

Un'alternativa potrebbe essere quella di centralizzare solo quelle poche funzioni la cui centralizzazione è indispensabile (come il mantenimento e il chiarimento dei limiti alla ricerca, allo sviluppo e al dispiegamento dell'IA), lasciando che i singoli firmatari verifichino individualmente la conformità fino a ritenerla comprovata.

Un altro compromesso riguarda il grado di inclusività di un tale trattato. Il nostro testo di esempio creerebbe un'organizzazione multilaterale in cui tutti gli Stati sono invitati a firmare il trattato e a partecipare alla sua attuazione. Un'alternativa è un trattato incentrato, ad esempio, solo su Cina e Stati Uniti. L'idea di fondo sarebbe quella di costruire un regime di verifica bilaterale ristretto che soddisfi le esigenze di ciascuna parte, sacrificando al minimo l'autonomia e la trasparenza. A ciascuna parte verrebbe quindi assegnato l'obiettivo, separato e successivo, di far aderire altri Stati.

Poiché lo scopo di questo trattato è dimostrare come potrebbero presentarsi i controlli internazionali *se* i leader di tutto il mondo si rendessero conto dei pericoli pressanti, illustriamo una struttura che funzionerebbe con molte Parti, le quali avrebbero tutte qualcosa da guadagnare dall'adesione.

Pertanto, la struttura del Consiglio Esecutivo dell'ISIA proposta include tutti i membri permanenti del Consiglio di Sicurezza dell'ONU ed è modellata sulla composizione dell'AIEA.

Dato lo status di TSMC come produttore preminente di chip per l'IA, qualsiasi trattato sull'intelligenza artificiale deve considerare come affrontare la questione di Taiwan. Scegliamo di utilizzare il precedente del TNP riguardo a Taiwan. Idealmente, Taiwan dovrebbe attuare accordi formali e/o dichiarazioni in cui affermi di considerarsi vincolata dai principi di questo Trattato. Ciò comporterebbe un'adesione di fatto al Trattato. Ancor più importante, Taiwan dovrebbe accettare un accordo che consenta ispezioni di routine e/o su richiesta in loco per garantire la sua conformità ai principi del trattato.

Resta una questione in parte aperta come il Consiglio Esecutivo e l'ISIA debbano prendere le decisioni e quali poteri vengano delegati ai diversi organi. Questo Articolo include una proposta per tale struttura, una proposta che conferisce un potere significativo nelle mani del Segretariato Tecnico, attribuendo al contempo un potere di supervisione al Consiglio Esecutivo.

Un vantaggio di questa impostazione è che consente all'organo tecnico di prendere decisioni rapidamente e gli conferisce un ampio mandato per adempiere alla sua missione, sebbene qualsiasi modifica richieda l'approvazione della maggioranza semplice dei membri del Consiglio Esecutivo entro 30 giorni per rimanere in vigore. Se da un lato i leader mondiali possono esitare a delegare così tanto potere agli esperti tecnici, dall'altro è altrettanto probabile che questi ultimi non si fidino degli attori geopolitici per risolvere le spinose questioni tecniche che emergeranno nell'attuazione di questo Trattato, né della loro capacità di adattarsi a un panorama tecnico in continua evoluzione. Questo Articolo presenta solo una delle possibili proposte su come bilanciare il potere decisionale tra questi gruppi, ma esistono molti altri approcci.

Un altro approccio simile consisterebbe nel disaggregare ulteriormente le responsabilità, le definizioni e i tipi di salvaguardia attuati dall'ISIA (ad esempio, le soglie di FLOP per l'addestramento, la definizione di CCC, la definizione di chip di IA, la classificazione di una particolare struttura come impianto di produzione di chip, i protocolli di verifica dell'uso dei chip, la definizione di Ricerca Soggetta a Restrizioni, ecc.) e nello stabilire procedure di voto diverse per queste modifiche, a seconda del loro impatto.

---

#### Precedenti

La struttura di governo tripartita dell'Agenzia Internazionale per la Superintelligenza (ISIA), prevista dalla nostra bozza di trattato, è modellata su quella dell'OPCW[^243], l'organismo incaricato di attuare la Convenzione sulle Armi Chimiche (CWC). Anche i nomi di questi organi sono presi a prestito dall'OPCW. (La versione definitiva di un trattato potrebbe preferire strutture e nomi alternativi che svolgano le stesse funzioni; di seguito forniamo alcuni precedenti per assetti meno centralizzati).

Il Consiglio Esecutivo istituito dal nostro paragrafo 4, commi (a) e (d), emula il Consiglio dei Governatori del TNP. La designazione di cinque dei quindici seggi del Consiglio ai membri permanenti del Consiglio di Sicurezza delle Nazioni Unite riflette il fatto che i cinque Stati nucleari originari del TNP erano anche i cinque membri permanenti del Consiglio di Sicurezza dell'ONU; senza la loro partecipazione come partner chiave, il TNP sarebbe probabilmente fallito fin dall'inizio.

Anche la nostra disposizione per "10 seggi elettivi distribuiti in base a un'equa rappresentanza geografica" si ispira al TNP, il quale stabilisce che i suoi Governatori uscenti includano "il membro più avanzato nella tecnologia dell'energia atomica, compresa la produzione di materiali di base in ciascuna delle" otto regioni specificate.

La questione di Taiwan complica la nostra idea di trattato, data la sua delicata situazione geopolitica e il suo status di principale produttore mondiale di chip per l'IA. Fortunatamente, esistono precedenti che possono fungere da guida: sebbene Taiwan non sia parte del TNP, ha dichiarato in più occasioni di considerarsi vincolata dai suoi principi. Taiwan consente all'AIEA di condurre ispezioni e applicare salvaguardie ai suoi impianti nucleari attraverso un accordo trilaterale con gli Stati Uniti e l'AIEA stessa. Una soluzione simile potrebbe essere adottata per la nostra bozza di trattato.

I processi decisionali del Consiglio Esecutivo della nostra bozza di trattato sono stati modellati sul [Regolamento del Consiglio dei Governatori](https://www.iaea.org/about/policy/board/rules-and-procedures-of-the-board-of-governors) adottato dall'Agenzia Internazionale per l'Energia Atomica (AIEA), la principale organizzazione per la governance internazionale della tecnologia nucleare.[^244] Allo stesso modo, le procedure di voto seguono lo [Statuto dell'AIEA](https://www.iaea.org/about/statute#a1-6).

In altri trattati sulle armi nucleari si possono trovare precedenti di meccanismi di attuazione meno centralizzati (ma comunque potenzialmente efficaci). Il [Trattato sulle forze nucleari a raggio intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm#text) (INF) e i trattati per la riduzione delle armi strategiche ([START I](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html), [START II](https://2009-2017.state.gov/t/avc/trty/104150.htm) e [New START](https://2009-2017.state.gov/documents/organization/140035.pdf)) affidano la responsabilità dell'attuazione e della verifica alle singole parti; ciascuna si impegna ad adottare procedure che consentano all'altra di ottenere ragionevoli garanzie di conformità.

Le "ispezioni su richiesta" di cui al paragrafo 4(c) sono modellate sul meccanismo della Parte X della CWC; approfondiremo questo precedente nell'Articolo X.

---

### Articolo IV: Addestramento dell'IA {#article-iv:-ai-training}

1. Ciascuna Parte si impegna a vietare l'addestramento dell'IA al di sopra delle seguenti soglie: qualsiasi sessione di addestramento che superi 1e24 FLOP o qualsiasi sessione post-addestramento che superi 1e23 FLOP. Ciascuna Parte si impegna a non condurre sessioni di addestramento al di sopra di tali soglie e a non permettere a nessuna entità soggetta alla propria giurisdizione di condurre tali sessioni.  
   1. Il Segretariato Tecnico può modificare queste soglie, in conformità con la procedura descritta nell'Articolo III. Il Consiglio Esecutivo può porre il veto a tali decisioni con un voto a maggioranza di due terzi.  
2. Ciascuna Parte deve notificare all'ISIA qualsiasi sessione di addestramento compresa tra 1e22 e 1e24 FLOP prima del suo avvio. Ciò si applica alle sessioni di addestramento condotte dalla Parte o da qualsiasi entità soggetta alla sua giurisdizione.  
   1. Tale notifica deve includere, a titolo esemplificativo ma non esaustivo, tutto il codice di addestramento e una stima del FLOP totale da utilizzare. La Parte deve fornire al personale dell'ISIA un accesso supervisionato a tutti i dati, con una registrazione degli accessi adeguata alla sensibilità dei dati e protezioni contro la duplicazione o la divulgazione non autorizzata. La mancata fornitura di un accesso sufficiente ai dati al personale dell'ISIA costituisce motivo di diniego della sessione di addestramento, a discrezione dell'ISIA. L'ISIA può richiedere qualsiasi documentazione aggiuntiva relativa alla sessione di addestramento. L'ISIA approverà inoltre in via preliminare una serie di piccole modifiche che potrebbero essere apportate alla procedura durante la sessione di addestramento. Tali modifiche dovranno essere comunicate all'ISIA qualora vengano apportate.  
   2. La mancata risposta da parte dell'ISIA trascorsi 30 giorni costituisce approvazione; tuttavia, l'ISIA può prorogare tale periodo comunicando di necessitare di ulteriore tempo per la revisione. Non vi sono limiti a tali proroghe, ma le Parti possono presentare ricorso per ritardi eccessivi al Direttore o al Consiglio Esecutivo.  
   3. L'ISIA può monitorare tali sessioni di addestramento e la Parte fornirà all'ISIA, su richiesta, i punti di controllo del modello, compreso il modello finale addestrato [i dettagli iniziali di tale monitoraggio dovranno essere descritti in un allegato].  
   4. Nel caso in cui il monitoraggio indichi capacità o comportamenti preoccupanti dell'IA, l'ISIA può emettere un ordine di sospensione di una sessione o di una classe di sessioni di addestramento, finché non riterrà sicuro procedere con la sessione.  
   5. L'ISIA adotterà solide pratiche di sicurezza. L'ISIA non condividerà informazioni sulle sessioni di addestramento dichiarate, a meno che non stabilisca che una di esse violi il Trattato; in tal caso, fornirà a tutte le Parti del Trattato informazioni sufficienti per determinare se si sia verificata una violazione.  
   6. Qualora una Parte individui una sessione di addestramento al di sopra delle soglie designate, dovrà segnalarla all'ISIA e interromperla (se in corso). Tale sessione potrà riprendere solo previa approvazione dell'ISIA.  
3. Ogni Parte, e gli enti sotto la sua giurisdizione, possono condurre sessioni di addestramento inferiori a 1e22 FLOP senza la supervisione o l'approvazione dell'ISIA.  
4. L'ISIA può autorizzare, con una maggioranza di due terzi dei voti del Consiglio esecutivo, deroghe specifiche per attività quali valutazioni di sicurezza, veicoli a guida autonoma, tecnologia medica e altre attività, con modalità ritenute sicure dal Direttore Generale. Tali deroghe possono consentire sessioni di addestramento superiori a 1e24 FLOP con la supervisione dell'ISIA, o una presunzione di approvazione da parte dell'ISIA per sessioni di addestramento comprese tra 1e22 e 1e24 FLOP.

---

#### Note

Uno dei modi principali per migliorare le attuali IA è [aumentare le risorse computazionali](#impedire-la-creazione-di-chip-ai-sempre-più-potenti) investite nell'addestramento delle IA. Limitare queste risorse e la ricerca sul progresso algoritmico (descritta nell'articolo VIII) può ridurre il rischio che nel breve termine venga creata un'IA più intelligente dell'uomo.

Il divieto si basa sul numero di operazioni computazionali, dato che è un parametro relativamente facile da definire e misurare, e le prestazioni degli attuali modelli di IA all'avanguardia ci danno un'idea delle quantità di potenza di calcolo che risultano sicure con gli algoritmi odierni.

In un mondo perfetto, limiteremmo invece l'addestramento in base alle capacità del sistema addestrato. Ma nessuno ha la capacità tecnica di prevedere con certezza cosa sarà in grado di fare o non fare una nuova IA prima che venga addestrata. Pertanto, le risorse computazionali vengono utilizzate come proxy.

Il divieto assoluto di 1e24 FLOP per l'addestramento è leggermente inferiore alla quantità di risorse computazionali usate per i modelli prossimi allo stato dell'arte dell'agosto 2025, come DeepSeek-V3 con [3e24 FLOP](https://epoch.ai/data/ai-models). Abbiamo scelto questa soglia perché è al di sotto del livello al quale prevediamo che le IA possano diventare pericolose (dati gli algoritmi attuali), e perché offre un certo margine di manovra in caso di lievi progressi algoritmici.

Il divieto di superare 1e23 FLOP nel post-addestramento si applica alle IA create prima dell'entrata in vigore del Trattato. Molte di queste IA saranno state create usando più di 1e24 FLOP (a metà 2025 esistono [tra 50 e 100 modelli di questo tipo](https://epoch.ai/tools/model-counts)). Dato che i pesi di molte di queste IA saranno stati resi pubblici, non è fattibile impedirne l'uso, ma è fattibile impedirne modifiche sostanziali tramite il post-addestramento.

Un addestramento da 1e22 FLOP su 16 H100 richiederebbe circa una settimana. È un'operazione che richiede una potenza di calcolo tale che gli hobbisti non rischiano di superare accidentalmente la soglia addestrando modelli piccoli e consentiti. Le IA addestrate su questa scala (con gli algoritmi attuali) si sono finora dimostrate tutt'altro che pericolose. I progressi nella ricerca sugli algoritmi di IA, se la loro esecuzione e disseminazione non venissero altrimenti impedite, potrebbero porre l'umanità in uno scenario ancora più pericoloso, richiedendo divieti mirati ad attività che attualmente potrebbero essere svolte da piccoli gruppi determinati e disposti a sostenere spese ingenti.

Nella fascia di calcolo tra 1e22 e 1e24 FLOP, l'ISIA riceverebbe in anticipo il codice di addestramento e le stime dei FLOP totali. Ciò le consentirebbe di approvare il ciclo di addestramento, a condizione che questo non utilizzi metodi algoritmici innovativi.

L'addestramento di un'IA comporta spesso piccole correzioni di bug in corso d'opera (come l'installazione della versione corretta di una libreria o il rimescolamento dei dati di addestramento). L'ISIA può pre-approvare una serie di piccole modifiche a un ciclo di addestramento che non richiedano una nuova approvazione. Tali modifiche andrebbero comunque segnalate all'ISIA.

La segnalazione e il monitoraggio richiesti per i cicli di addestramento tra 1e22 e 1e24 FLOP consentirebbero di ottenere alcuni dei benefici derivanti dall'addestramento di IA con modalità che al momento appaiono sicure, prevenendo al contempo la costruzione di IA più grandi e potenzialmente pericolose.

Il monitoraggio dell'ISIA le permetterebbe inoltre di rimanere (in una certa misura) aggiornata sui progressi algoritmici che si verificano nonostante i divieti; inoltre, l'articolo XIII prevede che l'ISIA effettui valutazioni dei modelli addestrati in questa fascia. Ciò aiuterà a rendere l'ISIA consapevole delle tendenze nello sviluppo dell'IA e, si spera, le consentirà di abbassare le soglie FLOP se necessario.

Il monitoraggio dei cicli di addestramento da parte dell'ISIA è fattibile e applicabile grazie al consolidamento dei chip (articolo V) e alla verifica del loro uso (articolo VII). Il consolidamento dei chip significa che l'hardware utilizzato per i cicli di addestramento su larga scala sarà accessibile all'ISIA. La verifica dell'uso dei chip permetterà all'ISIA di avere una ragionevole certezza che i chip vengano utilizzati per addestrare IA solo previa autorizzazione.

Al personale dell'ISIA è permesso l'accesso ai dati di addestramento utilizzati nelle sessioni di addestramento monitorate, fatte salve diverse restrizioni. L'intento di queste restrizioni è di garantire che la registrazione e altri metodi di controllo siano utilizzati per prevenire la divulgazione non autorizzata di contenuti sensibili nei dati di addestramento, includenti, a titolo esemplificativo ma non esaustivo, informazioni di identificazione personale, informazioni sanitarie personali, dati classificati, segreti commerciali, dati bancari soggetti a obbligo di segretezza, e così via. 

---

#### Precedenti

Sebbene i valori numerici delle soglie specificate nel nostro accordo possano e debbano essere riesaminati una volta superata la fase iniziale di stesura, i tetti quantitativi sono comuni negli accordi internazionali, in quanto prevengono controversie che altrimenti dipenderebbero da diverse interpretazioni del linguaggio qualitativo.

Il [Trattato sulla limitazione degli esperimenti nucleari sotterranei (Threshold Test Ban Treaty)](https://2009-2017.state.gov/t/isn/5204.htm) del 1974 stabilì un tetto di 150 chilotoni per i test nucleari sotterranei eseguiti da Stati Uniti e URSS.[^245] Scopo ed effetto del trattato furono di ostacolare, almeno in parte, l'ulteriore sviluppo di testate più grandi e distruttive del tipo "city buster". Un parallelo pertinente con lo sviluppo dell'IA è che, a metà del 2025, i modelli più generici e capaci - e quindi più pericolosi - richiedono per la loro creazione sessioni di addestramento corrispondentemente più grandi; il nostro trattato specifica dei tetti volti a impedire che tali IA vengano sviluppate intenzionalmente, ma anche a cogliere il vantaggio essenziale (sebbene non parallelo) di ridurre il rischio che una soglia di capacità imprevista venga superata accidentalmente e irrimediabilmente.

Il limite di addestramento che abbiamo suggerito come punto di partenza è abbastanza basso da essere superato da alcuni modelli di IA addestrati oggi; lo consideriamo prudente in previsione di progressi che renderanno i modelli più recenti più capaci a parità di addestramento (come discusso nell'Articolo VIII). Gli accordi per la riduzione degli armamenti costituiscono un precedente per soglie fissate al di sotto del livello massimo attuale. Il [Trattato navale di Washington](https://www.digitalhistory.uh.edu/disp_textbook.cfm?smtID=3&psid=3995) del 1922 fissò limiti al dislocamento delle navi da guerra, che costrinsero gli Stati Uniti e altre potenze navali a rottamare decine di navi principali.[^246] Nell'Articolo II del trattato START del 1991,[^247] gli Stati Uniti e l'Unione Sovietica (e in seguito la Federazione Russa) concordarono limiti alle dimensioni dei loro arsenali nucleari e dei loro sistemi di lancio, che richiesero a ciascuna parte di eliminare gradualmente [più di quattromila testate ciascuno](https://bush41library.tamu.edu/archives/public-papers/3263).

I precedenti per le soglie quantitative che limitano il potenziale di fuga verranno discussi nell'Articolo V.

---

### Articolo V: Consolidamento dei chip {#articolo-v:-consolidamento-dei-chip}

1. Ciascuna Parte garantisce che, all'interno della propria giurisdizione, tutti i cluster di chip coperti (CCC), come definiti nell'Articolo II (ossia un insieme di chip con una capacità superiore a 16 H100-equivalenti) \[nota: 16 H100 hanno un costo complessivo di circa 500 000 dollari nel 2025 e raramente sono di proprietà di privati\], siano ubicati in strutture dichiarate all'ISIA e che tali chip per l'IA siano soggetti al monitoraggio da parte dell'ISIA.  
   1. Le parti cercheranno di evitare di co-locare chip AI e hardware non AI non ausiliario in queste strutture dichiarate.  
   2. Queste strutture devono essere accessibili per le ispezioni fisiche. Ciò può includere, ad esempio, che i team di verifica possano raggiungere qualsiasi CCC da almeno un aeroporto con servizio internazionale di linea entro 12 ore.  
   3. Le parti non devono collocare i chip AI in così tante sedi diverse da rendere impossibile per l'ISIA monitorarle tutte. Se richiesto dall'ISIA, le parti devono consolidare ulteriormente i propri chip AI in un numero inferiore di strutture monitorate.  
2. I chip AI non monitorati che non fanno parte di un CCC (cioè che hanno una capacità inferiore a 16 equivalenti H100) possono rimanere al di fuori delle strutture dichiarate dall'ISIA, a condizione che tali scorte non siano aggregate o collegate in rete per soddisfare la definizione di CCC, non siano ruotate tra i siti per aggirare il monitoraggio e non siano utilizzate per attività di addestramento vietate. Le Parti compiranno sforzi ragionevoli per monitorare la vendita e l'aggregazione dei chip AI per garantire che qualsiasi CCC di nuova creazione venga individuato e monitorato.  
3. Entro 120 giorni dall'entrata in vigore del trattato, ogni parte individuerà, inventarierà e consoliderà tutti i CCC in strutture dichiarate all'ISIA. Le parti non disgregheranno, nasconderanno o riassegneranno in altro modo i chip per eludere questo requisito o per far sì che un insieme di chip altrimenti classificabile come CCC non venga più classificato come tale.  
4. L'ISIA supervisionerà il processo di consolidamento nazionale, anche attraverso ispezioni in loco, verifica documentale e inventariale, affiancamento delle autorità nazionali durante i trasferimenti e le ispezioni, e condivisione di informazioni con le Parti ai sensi dell'articolo X. L'ISIA potrà richiedere la documentazione della catena di custodia per i trasferimenti e condurre ispezioni su richiesta, come descritto nell'articolo X. Le Parti garantiranno un accesso tempestivo alle strutture, agli snodi di trasporto e ai registri pertinenti. Le tutele e gli incentivi per gli informatori previsti dall'articolo X si applicano al processo di consolidamento e l'ISIA manterrà canali di segnalazione protetti.  
5. Entro 120 giorni dall'entrata in vigore del Trattato, le Parti presenteranno all'ISIA un registro dei loro CCC. Il registro dovrà includere l'ubicazione, il tipo, la quantità, i numeri di serie o altri identificatori univoci, ove disponibili, e le interconnessioni associate di tutti i chip di IA presenti nei CCC. Ciascuna Parte fornirà all'ISIA un registro aggiornato e accurato al più tardi ogni 90 giorni.  
6. Le Parti notificheranno in anticipo all'ISIA qualsiasi trasferimento pianificato di chip di IA, sia nazionale che internazionale, con un preavviso di almeno 14 giorni rispetto alla data prevista per il trasferimento. Nessun trasferimento potrà avere luogo senza che all'ISIA sia data l'opportunità di supervisionarlo. Per i trasferimenti internazionali, sia la Parte mittente che quella ricevente dovranno coordinarsi con l'ISIA per quanto riguarda il tragitto, la custodia e la ricezione. I trasferimenti di emergenza effettuati per ragioni di sicurezza dovranno essere notificati non appena possibile e saranno soggetti a verifica a posteriori.  
7. I chip di IA rotti, difettosi, in eccedenza o altrimenti dismessi continueranno a essere trattati come chip funzionanti, finché l'ISIA non ne certificherà la distruzione. Le Parti non distruggeranno i chip di IA senza la supervisione dell'ISIA. La distruzione o la messa fuori uso permanente dovranno essere condotte sotto la supervisione dell'ISIA, utilizzando metodi da essa approvati, e registrate in un certificato di distruzione [i dettagli dovranno essere esplicitati in un allegato]. Il recupero o la rivendita di componenti di tale hardware sono vietati, salvo espressa autorizzazione dell'ISIA.

---

#### Note

Discuteremo quale obiettivo questo Articolo si prefigge di raggiungere, perché riteniamo che tale obiettivo sia importante, perché lo riteniamo fattibile, perché è stato scelto il limite di 16 H100 e varie altre considerazioni.

##### Obiettivo dell'Articolo V

Questo Articolo mira a centralizzare, in strutture monitorate, tutti i cluster di chip per l'IA (ossia insiemi di chip interconnessi al di sopra di una certa piccola dimensione) e la stragrande maggioranza dei chip per l'IA.

Una volta che i chip sono stati centralizzati in strutture monitorate, l'ISIA può adottare numerosi altri approcci per garantire che non vengano utilizzati in violazione dell'Articolo IV; tali approcci sono discussi nell'Articolo VII.

È auspicabile una verifica internazionale di questo processo di centralizzazione, affinché tutte le Parti abbiano la certezza che anche tutti gli altri abbiano centralizzato i propri chip. Questo tipo di verifica può essere effettuata con relativa facilità per i grandi data center di IA, dato che le agenzie di intelligence probabilmente sanno già dove si trovano. Per i data center più piccoli, l'ISIA può supervisionare i processi di centralizzazione nazionali come misura di rafforzamento della fiducia.

L'articolo prevede in effetti un'eccezione per un numero ridotto di chip: meno di 16 equivalenti H100. Questa quantità di chip è talmente ridotta che è improbabile che i chip non monitorati rappresentino una minaccia (in assenza di progressi nella ricerca). Soglie più basse sarebbero più difficili da radunare con successo e potrebbero iniziare a imporre costi a una fascia più ampia di popolazione. (Diverse persone possiedono una manciata di GPU a casa propria, ma poche possiedono 16 equivalenti H100).

##### Le ragioni dell'Articolo V

La centralizzazione dei chip è un'operazione utile perché apre altre strade per controllare lo sviluppo dell'intelligenza artificiale tramite i chip.

Per esempio, la centralizzazione dei chip in strutture dichiarate implica che questi potrebbero in seguito essere soggetti a ulteriore monitoraggio sul loro utilizzo (Articolo VII) o a verifiche per assicurarsi che siano spenti.

La centralizzazione dei chip in strutture dichiarate renderebbe anche più facile per le Parti distruggerli, come potrebbe rendersi necessario ai sensi dell'articolo XII, qualora una Parte violi continuamente il Trattato.

Data la possibilità che i CCC vengano distrutti, sarebbe auspicabile costruirli lontano dai centri abitati, ove possibile. Questo punto non è incluso nel testo dell'articolo per problemi di fattibilità (i data center attuali sono spesso vicini alle città, quindi sarebbe necessario costruirne di nuovi); inoltre, in casi estremi, i data center possono probabilmente essere disattivati senza ingenti danni collaterali e, infine, questo non è un aspetto portante del Trattato. Tuttavia, dato il loro enorme potenziale di pericolo, sembra appropriato trattare i data center di IA come strutture militari.

##### Verifica

Un elemento chiave per il successo di un trattato sull'IA è la verifica. I Paesi non si fideranno l'uno dell'altro riguardo al rispetto delle regole, ma vorranno poter verificare concretamente. La centralizzazione dei chip di IA in strutture dichiarate dovrà essere confermata dalle ispezioni e dal monitoraggio di questo processo da parte dell'ISIA. Altrimenti, i Paesi non otterranno sufficiente fiducia che gli altri abbiano effettivamente centralizzato i loro chip.

La centralizzazione dei chip potrebbe non essere necessaria se esistessero altri modi per monitorarli. Purtroppo, riteniamo che questa sia attualmente l'unica opzione fattibile, se si esclude la distruzione fisica di tutte le scorte di chip di IA, considerate le capacità dei meccanismi di sicurezza disponibili sui chip attuali.

In futuro, potrebbero essere sviluppati [meccanismi di governance abilitati dall'hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) per consentire la governance remota dei chip di IA, in modo che non sia necessario centralizzarli in luoghi dichiarati. [Aarne et al. (2024)](https://www.iaps.ai/research/secure-governable-chips) forniscono stime sui tempi di implementazione di alcuni di questi meccanismi di governance su chip. Le loro stime coprono i tempi necessari per sviluppare meccanismi robusti contro diversi avversari. Per brevità, useremo le loro stime per la sicurezza in un contesto velatamente avversariale, poiché pensiamo che questo corrisponda alla situazione che intendiamo affrontare: attori statali competenti potrebbero provare a violare i meccanismi di governance, ma ci sarebbero conseguenze importanti se questa sovversione venisse scoperta. Stimano un tempo di sviluppo di 2-5 anni per soluzioni ideali, con opzioni meno sicure ma potenzialmente funzionanti disponibili in pochi mesi.

Anche se quel rapporto risale a più di un anno fa, non siamo a conoscenza di progressi significativi verso questi meccanismi, e riteniamo che quella di due-cinque anni aggiuntivi sia la stima più pertinente di Aarne et al.

Oltre allo sviluppo, i meccanismi di governance su chip devono essere o aggiunti ai nuovi chip — che a loro volta dovranno diffondersi nel parco chip esistente — oppure adattati a quelli già in uso. Aarne et al. stimano che la prima di queste opzioni potrebbe richiedere quattro anni, ma siamo ottimisti sul fatto che l'adattamento potrebbe essere completato in uno o due anni se i chip sono già tracciati.

Per essere chiari, la centralizzazione di cui all'articolo V comporta la concentrazione fisica e il monitoraggio (nell'articolo VII) dei cluster di chip coperti, ma non richiede che i governi ne assumano la proprietà. Per i grandi data center, il trattato permette che il data center e i suoi chip rimangano nello stesso posto, pur restando di proprietà delle aziende, a patto che ricevano monitoraggio e supervisione dal governo nazionale e dall'ISIA per garantire che i data center siano impiegati solo in attività non legate all'IA o in attività come l'esecuzione di vecchi modelli, piuttosto che nella creazione di nuovi modelli di IA più capaci. Per un numero minore di chip, potrebbe essere necessario spostarli fisicamente in un data center più grande, ma il proprietario potrebbe continuare ad accedervi da remoto, in modo simile ad alcuni modelli di cloud computing esistenti. In alternativa, i chip potrebbero essere trasferiti alla proprietà del governo in cambio di un giusto compenso.

##### Fattibilità

Probabilmente è fattibile individuare la maggior parte dei chip di IA (e verificarne l'individuazione a livello internazionale). Per i data center di IA più grandi, come quelli con più di 100 000 equivalenti H100, la verifica internazionale sembra relativamente semplice; questi data center sono difficili da nascondere e i servizi di intelligence probabilmente sanno già dove si trovano. (A metà del 2025, non ci aspettiamo che ci siano ancora stati tentativi di nascondere questi data center.) Questi data center sono individuabili dalla loro impronta fisica e dal consumo energetico, e molti di essi sono [pubblicamente segnalati](https://epoch.ai/data/ai-supercomputers).

Tali metodi consentiranno probabilmente di individuare anche centri dati di dimensioni ridotte, pari a circa 10 000 H100 equivalenti. Al di fuori dell'NTM e dei servizi di intelligence delle Parti, il piano proposto per la centralizzazione dei chip prevede che le autorità nazionali utilizzino vari poteri per farlo e consentano l'ispezione ISIA del processo per evitare che i centri dati vengano esclusi.

Gli Stati avranno a disposizione una serie di strumenti per rintracciare i chip di proprietà nazionale. Potranno richiedere legalmente che tutti i cluster di chip superiori a 16 H100 siano segnalati; potranno utilizzare i registri di vendita e altre informazioni finanziarie dei distributori di chip; potranno intervistare tecnici esperti nella costruzione di centri dati; ecc. Qualora sospettino attività di contrabbando o che i chip vengano nascosti, potranno ricorrere alle forze dell'ordine per ulteriori indagini. Questo processo di centralizzazione nazionale sarà supervisionato dagli ispettori dell'ISIA per garantire che sia eseguito correttamente.

La localizzazione dei data center esistenti potrebbe probabilmente essere effettuata rapidamente — in pochi giorni o settimane per i cluster superiori a 1 000 equivalenti H100 — utilizzando i metodi descritti.

In realtà, la *centralizzazione* dei chip potrebbe richiedere più tempo, poiché potrebbe essere necessario aumentare la capacità dei data center nelle strutture che diventerebbero CCC. Non è chiaro quanto tempo ci vorrebbe, né se sarebbe fattibile in alcuni paesi, individuare tutti i cluster di, ad esempio, 100 H100. Una tale quantità di chip sarebbe vietata, ma tale divieto potrebbe essere difficile da applicare.

Una delle maggiori sfide alla fattibilità della centralizzazione dei cluster di chip consiste nel fornire agli altri Paesi la ragionevole certezza che una Parte non stia sviluppando un progetto segreto di IA con chip non dichiarati. La verifica internazionale della centralizzazione nazionale dei chip potrebbe essere d'aiuto, ma potrebbe non bastare, dato che gli sforzi di centralizzazione nazionale potrebbero essere volutamente non esaustivi.

I vincoli imposti da questo Articolo contribuiscono in modo significativo a fornire alle Parti garanzie contro il rischio di progetti segreti di IA sostenuti da governi e operanti in altri Paesi. Oltre alla supervisione dell'ISIA sul processo di centralizzazione nazionale, la raccolta di informazioni e le ispezioni su richiesta trattate nell'Articolo X potrebbero essere di grande aiuto.

##### Le ragioni della definizione di CCC

Questo Articolo e la corrispondente definizione di CCC stabiliscono un limite di 16 H100-equivalenti. Tale soglia mira a soddisfare alcuni criteri:

* Il monitoraggio dei cluster di chip con potenza superiore a 16 H100 è compatibile con le soglie di FLOP per l'addestramento di cui all'Articolo IV. L'addestramento con 16 H100 (precisione FP8, utilizzo al 50 % — parametri realistici ma ottimistici) richiederebbe 7,3 giorni per raggiungere 10^22 FLOP e 2 anni per raggiungere 10^24 FLOP. Pertanto, sarebbe fattibile utilizzare chip non dichiarati per raggiungere la soglia più bassa, ma sarebbe piuttosto impraticabile raggiungere la soglia di addestramento proibita.  
* Questa soglia è verosimilmente sufficiente a impedire l'avanzamento delle capacità dell'IA, se combinata con i divieti alla ricerca sull'IA previsti dall'articolo VIII. L'articolo IV stabilisce restrizioni all'addestramento, in base alle quali l'addestramento su larga scala è vietato e quello su media scala è consentito ma soggetto a supervisione. È probabilmente accettabile — ossia, comporta un rischio minimo — consentire l'addestramento su piccola scala, come quello che si può effettuare con 16 H100 in un lasso di tempo realistico.  
* Questa soglia ha effetti collaterali limitati per gli hobbisti e le persone comuni. Pochissimi individui possiedono più di 16 H100. A metà del 2025, 16 chip H100 costano circa 500 000 dollari; non sono un bene di consumo comune. Nessuno supererà per errore la soglia solo perché possiede qualche vecchia console da gioco.  
* Il censimento dei chip di IA diventa più difficile per numeri di chip consentiti sempre più piccoli. Trovare data center con 100 000 chip è facile; con 10 000 è probabilmente anche relativamente facile; con 1 000 non è chiaro; e con 100, potrebbe essere piuttosto difficile. Questa soglia è stata scelta in parte a causa dell'impossibilità di imporne una più bassa: persino una soglia di 16 H100 potrebbe essere difficile da far rispettare.  
* È possibile che si debba rivedere questa definizione e abbassare la soglia (ad esempio, a 8 H100-equivalenti). In questo trattato, l'ISIA avrebbe il compito di valutare questa definizione e di modificarla secondo necessità.

##### Altre considerazioni

Questo articolo invita le parti a cercare di evitare la co-locazione di chip AI con chip non-AI non ausiliari. Questo perché la co-locazione potrebbe rendere più difficile la verifica dell'uso dei chip (articolo VII) e richiederebbe che anche questi chip non-AI fossero monitorati per implementare efficacemente la verifica dell'uso dei chip AI. Tuttavia, non è strettamente necessario e potrebbe non essere auspicabile. Ad esempio, attualmente i chip AI sono spesso co-locati con chip non-AI e l'inconveniente di cambiare questa situazione potrebbe superare l'inconveniente di monitorare anche i chip non-AI.

In linea con gli accordi precedenti, questo articolo richiede che i CCC siano rapidamente accessibili agli ispettori per la verifica. In questo caso, ci sarà probabilmente un monitoraggio continuo di molte di queste strutture (articolo VII) e l'accesso agli aeroporti potrebbe essere vantaggioso.

C'è il rischio che privati cittadini possano costruire un CCC non monitorato con chip "sfusi" equivalenti a H100. Per contrastare questo rischio, il trattato prevede che le Parti compiano "ogni ragionevole sforzo" per monitorare le vendite di chip (in quantità superiori a 1 equivalente H100) e individuare la formazione di nuovi CCC. Potrebbero essere adottate misure più rigorose, come l'obbligo di registrare e tracciare formalmente tutti questi chip e le relative vendite. La nostra bozza non si spinge a tal punto, sia perché non prevediamo che, dopo la catalogazione di tutti i chip nei CCC, rimangano molti chip "sfusi" equivalenti a H100 non contabilizzati, sia perché altri meccanismi (come la protezione degli informatori) contribuiscono a individuare i CCC di nuova formazione.

##### Alternative

Invece di richiedere immediatamente la centralizzazione dei cluster di piccole dimensioni (ad esempio 100 H100), il trattato potrebbe adottare un approccio graduale. Ad esempio, nei primi 10 giorni tutti i data center con più di 100 000 chip equivalenti a H100 devono essere centralizzati e dichiarati, poi nei 30 giorni successivi tutti i data center con più di 10 000 chip equivalenti a H100 devono essere centralizzati e dichiarati, ecc.

Questo approccio graduale potrebbe allinearsi meglio con la verificabilità internazionale del rilevamento di questi cluster, man mano che i servizi di intelligence intensificano i loro sforzi. In altre parole: è probabile che all'inizio i servizi di intelligence sappiano dove si trovano i data center più grandi, ma non quelli di medie dimensioni (se non li stavano già cercando). Di conseguenza, inizialmente verrebbero dichiarati solo i data center di grandi dimensioni e poi, con il proseguire della ricerca dei chip da parte dei servizi di intelligence, la soglia si abbasserebbe.

Questo approccio potrebbe rispecchiare meglio il modo in cui la verificabilità e l'applicabilità hanno influito su ciò che è stato concordato nei precedenti accordi internazionali. Ad esempio, il Trattato sulla messa al bando parziale degli esperimenti nucleari del 1963 non vietò i test sotterranei di armi nucleari, a causa della difficoltà di rilevarli. Il team di governance tecnica del MIRI ha in programma di pubblicare un rapporto con un approccio graduale di questo tipo.

Uno svantaggio di un approccio graduale è che potrebbe offrire agli Stati maggiori opportunità per nascondere i chip e stabilire datacenter segreti.

---

#### Precedenti

La dichiarazione delle risorse di interesse è spesso il primo passo nei trattati restrittivi. Le Parti del Trattato navale di Washington del 1922 fornirono gli inventari delle navi da battaglia e la loro stazza, e si impegnarono a notificarsi reciprocamente la sostituzione di queste navi. Il trattato START I del 1991 includeva un accordo riservato sullo scambio di coordinate e schemi dei siti (nell'[articolo VIII](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art8)), che prevedeva la condivisione dei dati sulla posizione di tutte le armi strategiche dichiarate. L'articolo V, paragrafo 3, della nostra bozza di accordo richiede alle Parti di individuare, inventariare e consolidare i cluster di chip contemplati entro 120 giorni.

Il consolidamento delle risorse per facilitare la verifica della conformità è spesso un altro passo nei trattati restrittivi. L'[articolo III](https://1997-2001.state.gov/www/global/arms/starthtm/start/abatext.html#art3) dello START I vietava la collocazione congiunta di missili balistici intercontinentali (ICBM) con gli impianti di lancio spaziale, facilitando il monitoraggio. Il paragrafo 1.a del nostro articolo V impegna le Parti a «evitare di collocare chip AI insieme a hardware non ausiliario non-AI» per lo stesso motivo.

La storia dimostra che il consolidamento limita anche il potenziale di rottura degli accordi (*breakout*), rendendo più facile colpire le concentrazioni di asset problematici in caso di crisi di fiducia. Nel JCPOA del 2016[^248] (noto anche come accordo nucleare con l'Iran), l'Iran ha accettato di mantenere le sue centrifughe operative per l'arricchimento dell'uranio in soli due siti designati (Natanz e Fordow), entrambi [colpiti](https://www.iaea.org/newscenter/pressreleases/update-on-developments-in-iran-5) nel giugno 2025 da operazioni di Israele e Stati Uniti. Ciò motiva una nota che accompagna il nostro Articolo V, in cui suggeriamo alle Parti di localizzare i loro cluster di chip coperti (CCC) lontano dai centri abitati.

Il monitoraggio e le ispezioni sono componenti comuni dei trattati precedenti in contesti di fiducia limitata; abbiamo quindi redatto, ove opportuno, apposite disposizioni nei Paragrafi 1, 4, 6 e 7 di questo Articolo. Alcuni precedenti specifici: 
* La verifica dello START I ha incluso [centinaia di ispezioni in loco](https://www.armscontrol.org/factsheets/start-i-glance) nei primi anni.  
* La CWC richiede la dichiarazione e l'ispezione di tutti gli impianti di produzione di armi chimiche — ne sono stati [dichiarati 97](https://www.opcw.org/media-centre/opcw-numbers) — e la maggior parte di questi è stata verificabilmente distrutta. (Nel richiedere la dichiarazione degli impianti esistenti, questi accordi proibiscono anche lo svolgimento di determinate attività al di fuori degli impianti dichiarati, analogamente al divieto, previsto da questo Articolo, per i CCC non monitorati).  
* Oltre [700 impianti nucleari dichiarati](https://www.armscontrol.org/factsheets/iaea-safeguards-agreements-glance) in tutto il mondo sono monitorati dall'AIEA nell'ambito del TNP.

Analogamente a quanto previsto dal Paragrafo 3 di questo Articolo, numerosi accordi sul controllo degli armamenti richiedono che le Parti non interferiscano con i rispettivi NTM nel contesto della verifica del trattato. Ne sono esempi il SALT I,[^249] l'ABM,[^250] l'INF[^251] e lo START I.

Un precedente in cui le Parti limitano le industrie del proprio settore privato nazionale per adempiere agli impegni del trattato (come sarebbe necessario nel caso dell'IA) è riscontrabile nella legislazione statunitense successiva alla ratifica della CWC: il [Chemical Weapons Convention Implementation Act del 1998](https://www.congress.gov/105/plaws/publ277/PLAW-105publ277.pdf#page=857) e i regolamenti del Dipartimento del Commercio hanno assicurato la conformità delle entità statunitensi. Allo stesso modo, il Congresso degli Stati Uniti ha [emendato](https://www.epa.gov/ozone-layer-protection/ozone-protection-under-title-vi-clean-air-act) il Clean Air Act a seguito della ratifica del Protocollo di Montreal per vietare le sostanze che riducono lo strato di ozono.

Negli Stati Uniti, gli approcci all'implementazione della centralizzazione dei chip potrebbero basarsi sulla [Takings Clause](https://constitution.congress.gov/browse/essay/amdt5-9-1/ALDE_00013280/) del Quinto Emendamento, in base alla quale il governo può usare il suo potere di espropriazione per pubblica utilità per acquisire proprietà privata, a condizione che paghi un adeguato indennizzo. |

---

### Articolo VI: Monitoraggio della produzione di chip AI {#article-vi-ai-chip-production-monitoring}

1. L'ISIA implementerà un monitoraggio degli impianti di produzione di chip AI e degli input chiave per la loro produzione. Tale monitoraggio garantirà che tutti i chip AI di nuova produzione siano immediatamente tracciati e monitorati fino alla loro installazione nei CCC dichiarati e che non vengano create catene di approvvigionamento non monitorate.  
   1. L'ISIA monitorerà gli impianti di produzione di chip AI per cui è stato determinato che producono o potrebbero produrre chip AI e hardware rilevante [le definizioni precise di impianti di produzione di chip AI, chip AI e hardware rilevante andrebbero ulteriormente dettagliate in un allegato; anche i metodi di monitoraggio andrebbero descritti in un allegato].  
   2. Il monitoraggio dei chip AI di nuova produzione includerà il monitoraggio della produzione, della vendita, del trasferimento e dell'installazione. Il monitoraggio della produzione dei chip inizierà con la fabbricazione. La serie completa di attività include la fabbricazione di memorie ad alta larghezza di banda (HBM), la fabbricazione di chip logici, il collaudo, l'imballaggio e l'assemblaggio [questa serie di attività andrebbe specificata in un allegato].  
2. Negli impianti in cui il tracciamento e il monitoraggio ISIA non siano fattibili o implementati, la produzione di chip AI sarà interrotta. La produzione di chip AI potrà riprendere quando l'ISIA dichiarerà che sono state implementate misure di tracciamento e monitoraggio accettabili.  
3. Se un impianto di produzione di chip monitorato viene dismesso o riutilizzato, l'ISIA supervisionerà tale processo e, se condotto in modo soddisfacente per l'ISIA, l'obbligo di monitoraggio cesserà.  
4. Nessuna Parte potrà vendere o trasferire chip AI o attrezzature per la produzione di chip AI, se non secondo le modalità autorizzate e tracciate dall'ISIA.  
   1. La vendita o il trasferimento di chip AI all'interno delle Parti del Trattato o tra di esse godrà della presunzione di approvazione e sarà tracciato dall'ISIA.  
   2. La vendita o il trasferimento di attrezzature per la produzione di chip AI all'interno delle Parti del Trattato o tra di esse non godrà della presunzione di approvazione. L'approvazione di tale trasferimento si baserà su una valutazione del rischio di sviamento o di recesso dal Trattato da parte della Parte ricevente.  
   3. La vendita o il trasferimento di chip AI e di attrezzature per la produzione di chip AI a Stati non firmatari o a entità al di fuori di uno Stato firmatario sono soggetti a presunzione di diniego.  
5. A nessuna Parte è consentito vendere o trasferire chip avanzati non AI o attrezzature per la produzione di chip avanzati non AI a Stati non firmatari o entità al di fuori di uno Stato firmatario, salvo autorizzazione e tracciamento da parte dell'ISIA.  
6. La vendita o il trasferimento di chip avanzati non AI o di attrezzature per la produzione di chip avanzati non AI all'interno o tra le Parti del Trattato non sono soggetti a restrizioni ai sensi del presente articolo.

---

#### Note

La [catena di approvvigionamento dei chip AI](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/) è ristretta e specializzata, per cui monitorarne la produzione è fattibile. La [stragrande maggioranza](https://www.datacenterdynamics.com/en/news/nvidia-gpu-shipments-totaled-376m-in-2023-equating-to-a-98-market-share-report/) dei chip AI è progettata da NVIDIA. I chip logici più avanzati (il processore principale) utilizzati nei chip AI sono quasi tutti fabbricati da TSMC, che detiene [circa il 90 %](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) della quota di mercato. [La maggior parte dei chip AI](https://epoch.ai/data/machine-learning-hardware?view=table) sono prodotti su versioni del nodo di processo a cinque nanometri di TSMC, un nodo che probabilmente è supportato solo da [due o tre stabilimenti di produzione](https://www.blackridgeresearch.com/blog/list-of-tsmc-fabs-in-taiwan-arizona-kumamoto). Le macchine per litografia EUV, un componente fondamentale nella produzione di chip logici avanzati, sono prodotte [esclusivamente](https://www.governance.ai/analysis/computing-power-and-the-governance-of-ai) da ASML. La memoria ad alta larghezza di banda (HBM), un altro componente chiave per i chip AI, è dominata da [due o tre](https://www.trendforce.com/news/2024/04/24/news-amid-foundry-overcapacity-competition-for-hbm-intensifies-rapidly/) aziende. Questa catena di approvvigionamento ristretta sarebbe relativamente facile da monitorare se ci fosse la volontà di farlo.

Monitorare la produzione di chip AI avrebbe effetti di spillover relativamente limitati. Anche se alcuni dei processi utilizzati per produrre chip AI sono gli stessi utilizzati per altri chip (ad esempio, quelli per smartphone), i chip stessi sono distinti e potrebbero essere differenziati tramite il monitoraggio. Il design dei chip cambierebbe nel tempo, ma, allo stato attuale, i chip AI odierni sarebbero probabilmente identificabili grazie alla loro grande capacità HBM, ai componenti specializzati per la moltiplicazione di matrici, alla capacità di _scale-out_ e all'interconnessione.

La ricerca futura dovrebbe concentrarsi sull'identificazione delle parti principali della catena di approvvigionamento dei chip AI che sarebbero più adatte al monitoraggio. Sulla base delle [attuali strozzature](https://cset.georgetown.edu/publication/securing-semiconductor-supply-chains/), un'ipotesi iniziale è quella di concentrare il monitoraggio sulla produzione di HBM, sulla fabbricazione di die logici e sulle fasi successive (ad esempio, confezionamento, collaudo, assemblaggio dei server), insieme al monitoraggio di input chiave come le macchine per litografia EUV.

Questo Articolo nota che la vendita di chip all'interno degli Stati firmatari avrà una presunzione di approvazione, ma non indica la stessa presunzione per le attrezzature per la produzione di chip AI. Potrebbe essere accettabile consentire la vendita diffusa di attrezzature per la produzione di chip all'interno degli Stati firmatari, ma tale questione dovrebbe probabilmente essere trattata con maggiore cautela rispetto alla vendita dei chip stessi. La vendita dei chip avrà probabilmente un effetto a breve termine, dato che il ciclo di vita dei chip AI è in genere [di pochi anni](https://epoch.ai/data-insights/gpu-frontier-lifespan#:~:text=For%20these%20newer%20chip%20designs,from%200.6%20to%203.7%20years.). Al contrario, la capacità di produzione di chip potrebbe portare a una produzione significativa di chip per molti anni a venire, e sarebbe particolarmente preoccupante se i paesi con una matura produzione nazionale di chip AI si ritirassero dal Trattato. Pertanto, suggeriamo un approccio più cauto alle attrezzature per la produzione di chip rispetto ai chip stessi.

I paragrafi 4 e 5 di questo Articolo permettono la vendita di chip AI e di attrezzature per la produzione di chip alle Parti del Trattato, ma non a non-Parti o ad altre entità esterne alle Parti. In altre parole, le Parti accettano i rischi derivanti dalla produzione e dalla concentrazione dei chip, ma solo nei casi in cui i chip siano sottoposti a monitoraggio. La possibilità di produrre e possedere chip senza una risposta protettiva da parte di altri Stati emerge quindi come un incentivo positivo ad aderire al Trattato.

Di per sé, questo non impedisce agli Stati non Parte di accedere da remoto ai chip di IA presenti negli Stati Parte (ad esempio, tramite cloud computing o [Infrastructure-as-a-Service](https://www.governance.ai/research-paper/governing-through-the-cloud)), ma tali chip sarebbero soggetti al monitoraggio dell'ISIA per garantire che non vengano utilizzati in violazione dell'Articolo IV.

Le restrizioni per gli Stati non Parte potrebbero andare oltre, se necessario. Ad esempio, a tali Stati potrebbe essere vietato l'accesso da remoto ai chip di IA (ossia il noleggio di chip di IA negli Stati Parte tramite cloud) o l'accesso a modelli di IA tramite API.

Questo Articolo sfrutta il fatto che i chip sono un input altamente escludibile per lo sviluppo dell'IA. Propone di monitorare la produzione dei chip e di garantire che non vengano contrabbandati al di fuori degli Stati Parte o verso strutture non dichiarate. Un altro approccio sarebbe quello di vietare completamente la produzione di nuovi chip di IA. Questo approccio comporterebbe un minor rischio di diversione dei chip, ma avrebbe il costo di perdere il valore che questi chip potrebbero generare in applicazioni di IA non legate alla ricerca e allo sviluppo. Si baserebbe comunque su un certo monitoraggio degli impianti di produzione dei chip, ad esempio per garantire che producano solo chip non-IA o che vengano dismessi. L'attuale impianto del Trattato incoraggia la continuazione della produzione di chip, dati i grandi vantaggi che il mondo avrebbe potendoli utilizzare. Tuttavia, per permettere che la produzione di chip continui in sicurezza, sarebbe necessario monitorare la catena di approvvigionamento e l'utilizzo dei chip (Articolo VII). Riteniamo che entrambe queste attività siano fattibili, ma se una delle due non lo fosse, l'alternativa consisterebbe nell'interrompere del tutto la produzione di chip. 

---

#### Precedenti

Le disposizioni dei trattati per il monitoraggio degli impianti di produzione non sono una novità. L'articolo XI del Trattato INF del 1987 ha consentito per tredici anni ispezioni negli impianti designati dove in precedenza venivano prodotti sistemi di lancio nucleari a raggio intermedio; la Sezione VII del relativo [protocollo di ispezione](https://2009-2017.state.gov/t/avc/trty/102360.htm#inspections) permetteva il monitoraggio continuo del perimetro e dei varchi, che poteva includere la pesatura (e in alcuni casi l'esame ai raggi X) di qualsiasi veicolo in uscita dall'impianto abbastanza grande da trasportare un missile pertinente.

Il monitoraggio della produzione di chip per l'IA è più complicato, a causa della difficoltà di discernere la funzione e le capacità di un chip dalle sue caratteristiche esteriori; per questo motivo il nostro Articolo VI stabilisce che "l'hardware pertinente dovrebbe essere descritto più dettagliatamente in un Allegato", insieme ai metodi di monitoraggio. Ma l'esperienza delle salvaguardie dell'AIEA nell'ambito del TNP dimostra che la verifica di un'ampia gamma di componenti di produzione e di precursori lungo l'intera catena di approvvigionamento è possibile. A tal fine, l'AIEA [fornisce](https://www-pub.iaea.org/MTCD/Publications/PDF/Pub1669_web.pdf) [linee guida](https://www.iaea.org/publications/13452/international-safeguards-in-the-design-of-enrichment-plants?utm_source=chatgpt.com) per la progettazione di impianti che agevolino le ispezioni e riducano i costi di adeguamento.

Gli embargo sul trasferimento di prodotti finiti, precursori e attrezzature di produzione (come quello qui suggerito per i chip per l'IA e le attrezzature avanzate per la produzione di chip per computer verso Stati non firmatari del trattato) hanno tutti precedenti significativi:

* Nell'Articolo I del [NPT](https://www.un.org/en/conf/npt/2005/npttreaty.html), ogni Stato dotato di armi nucleari si impegna a «non trasferire a chicchessia armi nucleari o altri congegni esplosivi nucleari». Nel paragrafo 2 del suo Articolo III, si impegna altresì a non fornire «materiali grezzi o prodotti fissili speciali» o attrezzature «specialmente progettate o preparate per il trattamento, l'impiego o la produzione di prodotti fissili speciali».  
* L'Articolo I della [CWC](https://2009-2017.state.gov/t/avc/trty/127917.htm) impegna parimenti le parti a non «trasferire mai, direttamente o indirettamente, armi chimiche a nessuno»; il suo Articolo VII richiede loro di sottoporre i precursori elencati a specifici «divieti di produzione, acquisizione, detenzione, trasferimento e uso».  
* Il [Comitato di coordinamento per i controlli multilaterali sulle esportazioni](https://www.govinfo.gov/content/pkg/GPO-CRPT-105hrpt851/html/ch9bod.html#anchor5563742) (CoCom), istituito durante la Guerra Fredda, stabilì una serie coordinata di controlli sulle esportazioni dai Paesi del blocco occidentale a quelli del blocco comunista, riguardanti materiali nucleari, munizioni e prodotti industriali a duplice uso come i semiconduttori.  
* Il [Gruppo dei fornitori nucleari](https://www.nuclearsuppliersgroup.org/index.php/en/) è un regime multilaterale di controllo delle esportazioni che limita la fornitura di tecnologia nucleare e di quella ad essa correlata che potrebbe essere deviata verso programmi di armi nucleari.  
* Particolarmente rilevante è la serie di [controlli sulle esportazioni](https://www.bis.gov/press-release/commerce-strengthens-export-controls-restrict-chinas-capability-produce-advanced-semiconductors-military) statunitensi che, negli ultimi due anni, si sono concentrati sui chip per l'IA e sulle attrezzature avanzate per la loro produzione, arrivando a interessare decine di Paesi. 

---

### Articolo VII Verifica dell'impiego dei chip {#articolo-vii-verifica-dell'uso-del-chip}

1. Le parti accettano la verifica continua in loco dell'utilizzo totale dei chip da parte dell'ISIA presso i CCC dichiarati. I metodi utilizzati per la verifica saranno determinati e aggiornati dal Segretariato tecnico, in conformità con la procedura descritta nell'articolo III. Il Consiglio esecutivo può porre il veto su tali decisioni con un voto a maggioranza dei due terzi. Tali metodi possono includere, a titolo esemplificativo ma non esaustivo:  
   1. Ispettori in presenza  
   2. Telecamere a prova di manomissione  
   3. Misurazioni delle caratteristiche di alimentazione, termiche e di rete  
   4. Meccanismi hardware integrati nel chip, compresi quelli installati a posteriori  
   5. Dichiarazione dei carichi di lavoro e delle operazioni dei chip da parte dell'operatore CCC  
   6. Riesecuzione dei carichi di lavoro dichiarati presso una struttura ISIA per confermare la veridicità delle dichiarazioni  
2. L'obiettivo di questa verifica sarà garantire che i chip non vengano utilizzati per attività vietate, come l'addestramento di IA su larga scala descritto nell'articolo IV.  
3. Nei casi in cui l'ISIA ritenga che gli attuali metodi di verifica non possano fornire garanzie sufficienti che l'hardware di IA non venga utilizzato per attività vietate, tale hardware deve essere spento e la sua inoperatività deve essere continuamente verificata da ispettori in loco o da altri meccanismi di verifica approvati dall'ISIA.  
4. L'ISIA può imporre varie restrizioni sul funzionamento dei chip al fine di garantire una verifica adeguata. Tali restrizioni possono includere, a titolo esemplificativo ma non esaustivo:  
   1. Restrizioni sulla larghezza di banda e sulla latenza tra chip diversi, o tra chip e la rete del loro data center, per distinguere l'inferenza consentita dall'addestramento vietato.  
   2. Restrizioni sul numero o sulla velocità di FLOP/s o sulla larghezza di banda della memoria a cui i chip possono operare, per distinguere l'inferenza consentita dall'addestramento vietato o da altri carichi di lavoro vietati.  
   3. Restrizioni sulla precisione numerica delle operazioni dei chip, per differenziare i carichi di lavoro AI da quelli non AI.  
5. L'ISIA adotterà un approccio differenziato alla verifica dei diversi CCC, in base alla loro probabilità di essere utilizzati per attività di IA e alla loro sensibilità in relazione alla sicurezza nazionale.  
6. L'ISIA guiderà la ricerca e lo sviluppo di tecnologie migliori per il monitoraggio e la verifica dell'uso dei chip. Le parti sosterranno questi sforzi [maggiori dettagli saranno forniti in un allegato].

---

#### Note

I Paesi vorranno assicurarsi che i chip per l'IA esistenti non vengano utilizzati per l'addestramento di IA pericoloso. Ci saranno anche ragioni legittime per utilizzare questi chip per l'esecuzione di servizi di IA esistenti come (le versioni già esistenti di) ChatGPT. Per gestire questa tensione, l'ISIA può applicare misure di verifica ai chip per l'IA per garantire che non siano impiegati in attività proibite.

Ciò costituisce quindi un incentivo positivo per l'adesione al Trattato: si possono continuare a usare i chip per l'IA, a condizione che un'attività di supervisione possa verificare che non vengano utilizzati in modo da violare il Trattato (ad esempio per l'addestramento di IA su larga scala). Sebbene il monitoraggio continuo da parte dell'ISIA possa sembrare sgradevole, riteniamo che sia la migliore soluzione disponibile. Dato l'obiettivo di impedire l'addestramento di IA su larga scala, esistono due approcci principali: assicurarsi che nessuno disponga dell'hardware necessario (cioè che i chip per l'IA non possano esistere), oppure assicurarsi che l'hardware esistente non venga utilizzato per l'addestramento su larga scala (ovvero il tracciamento e la verifica dell'uso dei chip, come descritto negli Articoli V, VI e VII). Ciò è concettualmente analogo alle salvaguardie dell'AIEA: affinché un Paese non dotato di armi nucleari sia autorizzato a disporre di materiali e impianti nucleari, è necessario che l'AIEA effettui ispezioni per garantire che il loro uso sia limitato a scopi pacifici.

Affinché la verifica dell'uso dei chip sia efficace, l'ISIA necessita di meccanismi che forniscano garanzie credibili del fatto che i chip per l'IA non vengano utilizzati in modi proibiti da questo Trattato. La soluzione più semplice consisterebbe nel richiedere lo spegnimento di tutti i chip, ma ciò impedirebbe alle Parti di beneficiare dell'uso dei chip per scopi non pericolosi. La verifica potrebbe essere facilitata anche da mezzi tecnologici, con la riserva che la tecnologia attuale [probabilmente](https://www.cnas.org/publications/reports/secure-governable-chips) presenta vulnerabilità di sicurezza che consentirebbero ai proprietari dei chip di eludere le misure di monitoraggio. Dato l'attuale stato embrionale della tecnologia di verifica, quest'ultima richiederebbe probabilmente un monitoraggio continuo in loco, oppure lo spegnimento dei chip fino alla maturazione dei mezzi tecnologici. Tuttavia, una volta che la tecnologia di monitoraggio sarà matura, solidi [meccanismi di governance basati sull'hardware](https://www.rand.org/pubs/working_papers/WRA3056-1.html) potrebbero consentire il monitoraggio remoto e affidabile dei chip.[^252]

Varie restrizioni e limiti potrebbero rendere più facile monitorare l'uso dei chip. I diversi carichi di lavoro dell'IA — quali l'addestramento e l'inferenza — hanno requisiti tecnici differenti, e queste differenze potrebbero potenzialmente costituire la base per la verifica (se fossero sufficientemente robuste).

Per esempio, un meccanismo di verifica potrebbe consistere nel porre dei limiti all'interconnessione: limitare la quantità di comunicazione che un certo insieme di chip può avere con il mondo esterno usando cavi di rete a bassa larghezza di banda. In pratica, si prende un piccolo gruppo di chip — diciamo 8 H100 — e si limita la loro comunicazione esterna a un livello talmente basso da permettere ai chip di eseguire solo inferenze in modo efficiente, ma non di essere usati efficientemente per l'addestramento su larga scala. Questo funziona perché (con gli algoritmi del 2025) l'addestramento ha requisiti di comunicazione molto più elevati rispetto all'inferenza. Questo meccanismo sarebbe utile se l'esecuzione delle IA esistenti fosse accettabile, mentre non lo fosse l'addestramento di nuove IA.

Ci sono varie sfumature in questi meccanismi e molti altri potenziali meccanismi di verifica, quindi rimandiamo i lettori curiosi ai [precedenti](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) [lavori](https://aigi.ox.ac.uk/publications/verification-for-international-ai-governance/) [sull'argomento](https://www.rand.org/pubs/working_papers/WRA4077-1.html). Questo articolo affida all'ISIA il compito di sviluppare meccanismi di verifica migliori e di implementarne di vari, definiti in senso lato. Riteniamo che questa flessibilità sia necessaria a causa della rapida evoluzione del settore dell'IA e della difficoltà di prevedere sviluppi che potrebbero invalidare i metodi di verifica. Lo stato della ricerca sulla verifica dell'IA è inoltre nascente, e sono necessari ulteriori progressi prima che l'ISIA possa disporre di un solido set di strumenti.

L'articolo IV vieta l'addestramento su larga scala delle IA. Se al momento dell'entrata in vigore della verifica non fossero ancora state addestrate IA altamente capaci (e quindi pericolose), la verifica potrebbe concentrarsi specificamente sull'addestramento. Tuttavia, qualora fossero già state create IA sufficientemente capaci, potrebbe essere necessario monitorare anche se vengano implementate su chip per l'IA o quali attività svolgano: la verifica non potrebbe più concentrarsi sulla prevenzione dell'addestramento. La verifica sarebbe molto più difficile se fossero stati creati sistemi di IA sufficientemente pericolosi (ad esempio, quelli che potrebbero contribuire in modo sostanziale alla ricerca e sviluppo nel campo dell'IA), poiché verificare che non svolgano attività proibite sarebbe più difficile che verificare che non si stia effettuando un addestramento di IA su larga scala. Concretamente, sarà probabilmente più facile distinguere tra l'addestramento e l'esecuzione delle IA che tra l'esecuzione di IA per un tipo di compito e per un altro. A causa di questa differenza nella difficoltà di verifica, l'attuazione di questo Trattato sarebbe molto più semplice in un mondo in cui il progresso delle capacità dell'IA si arrestasse presto. Se invece tale progresso dovesse continuare, la verifica dell'uso dei chip potrebbe richiedere più lavoro, spese e restrizioni.

Allo stesso modo, potrebbe essere auspicabile monitorare il contenuto dell'inferenza dell'IA per assicurarsi che le IA non vengano usate per scopi dannosi. Questo tipo di monitoraggio è già applicato oggi da molte aziende del settore, ad esempio per rilevare se gli utenti stiano cercando di usare le IA per creare [armi biologiche](https://openai.com/index/preparing-for-future-ai-capabilities-in-biology/). Potrebbe essere auspicabile applicare un monitoraggio simile a livello globale, a condizione che i relativi rischi per la privacy e la sicurezza possano essere mitigati.

Il paragrafo 5 di questo articolo consente all'ISIA di utilizzare metodi di verifica diversi per CCC diversi. Una delle ragioni di questa discriminazione è di ordine pratico: CCC diversi richiederanno approcci di verifica diversi al fine di stabilire con fondata fiducia che non vengano utilizzati per lo sviluppo di IA pericolose. Per esempio, i grandi data center che in precedenza venivano usati per l'addestramento di IA di frontiera avrebbero la maggiore capacità di contribuire all'addestramento proibito, quindi per loro potrebbe essere richiesto un monitoraggio più intenso.

In secondo luogo, una discriminazione negli approcci di verifica renderebbe il trattato più accettabile, richiedendo un monitoraggio meno invasivo per i CCC sensibili. Ad esempio, le agenzie di intelligence o le forze armate non vogliono alcun monitoraggio dei loro data center da parte dell'ISIA, e questa disposizione aiuta a trovare un equilibrio. Sarebbe comunque necessario verificare che tali data center non vengano utilizzati per attività di IA pericolose, e l'ISIA collaborerebbe con questi gruppi per assicurarsi di ottenere le informazioni necessarie, rispettando al contempo le esigenze di privacy e sicurezza dei proprietari dei CCC. D'altra parte, consentire protocolli di verifica diversi potrebbe compromettere la sostenibilità del trattato se questi fossero considerati iniqui, soprattutto se il relativo processo decisionale risultasse sbilanciato.

Il piano di questo trattato prevede di consentire la continuazione dell'uso e della produzione di chip. Questo permette al mondo di beneficiare di questi chip. Un approccio alternativo consiste nel fermare la produzione di nuovi chip e/o distruggere quelli esistenti. Se i chip venissero distrutti, invece di progressi algoritmici, il tempo di breakout — il tempo che intercorre tra l'inizio di attività pericolose da parte di un attore e il momento in cui queste avrebbero successo se non venissero fermate — si allungherebbe, perché gli attori dovrebbero prima produrre molti chip di IA (il che sarebbe probabilmente rilevabile e potrebbe richiedere anni o decenni, a seconda dello stato della catena di fornitura dei chip). Pertanto, la distruzione dei chip comporterebbe il vantaggio significativo di tempi di breakout molto più lunghi e una maggiore difficoltà nello sviluppo di IA pericolose. Tuttavia, comporterebbe anche un costo notevole, non consentendo al mondo di beneficiare di questi chip. Poiché riteniamo fattibile tracciare i chip e verificarne l'utilizzo, la nostra bozza di trattato segue questa strada piuttosto che quella dell'eliminazione dei chip di IA. Entrambi i percorsi, però, hanno i loro meriti.

---

#### Precedenti

Quando abbiamo parlato dei precedenti per l'articolo VI, abbiamo descritto il monitoraggio continuo degli ex siti di produzione di missili a medio raggio nell'ambito del trattato INF, che, pur permettendo la pesatura e la scansione non distruttiva dei veicoli in uscita dagli impianti, non consentiva agli ispettori di entrare nei camion o nei siti stessi. Un monitoraggio perimetrale analogo dei data center può fornire alcuni indizi sulle operazioni in base al consumo energetico, alle emissioni termiche e alla larghezza di banda della rete. Tuttavia, per avere una ragionevole certezza che non si verifichino operazioni di IA soggette a restrizioni, sarà probabilmente necessaria una combinazione degli elementi elencati al paragrafo 1 del nostro articolo VII, che include telecamere a prova di manomissione, meccanismi hardware integrati nel chip e ispettori in loco.

Tali pratiche sono già prassi di routine per l'Agenzia internazionale per l'energia atomica, che fa [sempre più ricorso a tecnologie di sorveglianza attive 24 ore su 24](https://www.iaea.org/newscenter/news/surveying-safeguarded-material-24/7) per integrare le ispezioni:

> Oltre un milione di dati crittografati sulle salvaguardie vengono raccolti da oltre 1400 telecamere di sorveglianza e 400 sensori di radiazioni e di altro tipo in tutto il mondo. Più di 23 000 sigilli installati negli impianti nucleari garantiscono il contenimento di materiali e attrezzature.

Uno dei metodi usati nell'ambito dello START I per verificare la conformità alle caratteristiche prestazionali dei missili era la condivisione di quasi tutti i dati telemetrici trasmessi dai sensori di volo durante i test, come specificato nel [protocollo telemetrico](https://1997-2001.state.gov/www/global/arms/starthtm/start/telempro.html), che richiedeva anche alle parti di fornire qualsiasi apparecchiatura di riproduzione e informazioni sulla formattazione dei dati necessarie per interpretarli. A seconda della combinazione di metodi di verifica adottati, un'agenzia internazionale di superintelligenza può utilizzare metodi analoghi, basandosi sul monitoraggio leggero che è [prassi comune](https://www.governance.ai/research-paper/governing-through-the-cloud) per i fornitori di cloud computing per la raccolta di informazioni sui carichi di lavoro dei clienti.

Il monitoraggio continuo da parte del governo delle strutture commerciali private (come la maggior parte dei data center) ha anche numerosi precedenti. La Commissione per la regolamentazione nucleare degli Stati Uniti, incaricata di supervisionare la sicurezza dei reattori nucleari nazionali, disloca [due ispettori residenti](https://www.nrc.gov/reactors/operating/oversight/rop-description/resident-insp-program.html) in ogni centrale elettrica commerciale degli Stati Uniti, e i produttori di carne statunitensi [non possono effettuare operazioni di macellazione](https://www.fsis.usda.gov/sites/default/files/media_file/2021-02/Fed-Food-Inspect-Requirements.pdf) se il personale di ispezione del FSIS[^253] non è sul posto per supervisionarle.

---
---


### Articolo VIII Ricerca soggetta a restrizioni: algoritmi e hardware di intelligenza artificiale {#articolo-viii-ricerca-soggetta-a-restrizioni:-algoritmi-e-hardware-di-intelligenza-artificiale}

1. Al fine di prevenire ricerche specifiche che facciano avanzare la frontiera delle capacità dell'IA o che compromettano la capacità delle Parti di implementare le misure del presente Trattato, questo Trattato designa come "Ricerca soggetta a restrizioni" la ricerca che soddisfa una qualsiasi delle seguenti condizioni:  
   1. Miglioramenti ai metodi utilizzati per creare modelli all'avanguardia, come definito nell'articolo II, che migliorerebbero le capacità dei modelli o l'efficienza dello sviluppo, dell'implementazione o dell'uso dell'IA  
   2. Metodi di addestramento distribuiti o decentralizzati, o metodi di addestramento ottimizzati per l'uso su hardware ampiamente disponibile o di consumo  
   3. Ricerca su paradigmi di intelligenza artificiale che vadano oltre l'apprendimento automatico  
   4. Progressi nella fabbricazione di chip o componenti di chip rilevanti per l'IA  
   5. Progettazione di chip di IA più performanti o più efficienti  
2. La divisione Controlli sulla ricerca dell'ISIA classificherà tutte le attività di ricerca soggette a restrizioni come *controllate* o *vietate*.  
   1. Ciascuna Parte monitorerà tutte le attività di ricerca controllate all'interno della propria giurisdizione e adotterà misure per garantire che tutte le ricerche controllate siano monitorate e messe a disposizione della divisione Controlli sulla ricerca a fini di revisione e monitoraggio.  
   2. Ciascuna Parte non condurrà alcuna Ricerca Vietata, e ne proibirà e preverrà la conduzione da parte di qualsiasi entità all'interno della propria giurisdizione.  
3. Nessuna Parte assisterà, incoraggerà o condividerà Ricerche Vietate, anche tramite finanziamento, approvvigionamento, hosting, supervisione, insegnamento, pubblicazione, fornitura di strumenti o chip controllati, o la facilitazione di collaborazioni.  
4. Ciascuna Parte nominerà un rappresentante presso la Divisione per il Controllo sulla Ricerca dell'ISIA, in seno al Segretariato Tecnico (istituito nell'Articolo III). A tale divisione sono attribuite le seguenti responsabilità:  
   1. Interpretare e chiarire le categorie di Ricerca Soggetta a Restrizioni e rispondere alle domande sui confini di tale ricerca, in risposta a nuove informazioni e alle richieste di ricercatori, organizzazioni o membri delle Parti.  
   2. Interpretare e chiarire il confine tra Ricerca Controllata e Ricerca Vietata, e rispondere alle domande su tale confine, in risposta a nuove informazioni e alle richieste di ricercatori, organizzazioni o membri delle Parti.  
   3. Modificare la definizione di Ricerca Soggetta a Restrizioni e le sue categorie, in risposta al mutare delle condizioni o a richieste da parte di ricercatori, organizzazioni o Membri del Partito.  
   4. Modificare il confine tra Ricerca Controllata e Ricerca Vietata in risposta al mutare delle condizioni o a richieste da parte di ricercatori, organizzazioni o Membri del Partito.  
   5. Il Consiglio Esecutivo potrà porre il veto su qualsiasi modifica alla Ricerca Soggetta a Restrizioni o alle categorie e classificazioni al suo interno con una maggioranza dei due terzi, come descritto nell'Articolo III.

---

#### Note

Sarà difficile vietare diverse ampie categorie di ricerca, dato che il know-how necessario è già diffuso nel settore privato. Nella nostra proposta, la ricerca è soggetta a restrizioni se accresce le capacità o le prestazioni dell'IA, o se mette a rischio il sistema di verifica definito negli articoli precedenti.

Alcune ricerche devono essere vietate per impedire l'avanzamento delle capacità dell'IA, anche mantenendo costante la quantità di FLOP usata per l'addestramento. Questo divieto dovrebbe coprire tutte le ricerche che potrebbero rendere l'addestramento delle IA più efficiente o che potrebbero accrescerne le capacità. Questo viene spesso definito "progresso algoritmico". Nei paradigmi attuali, ciò include i progressi negli algoritmi usati nel pre-addestramento, nel post-addestramento e al momento dell'inferenza. Con il mutare dei paradigmi, queste distinzioni potrebbero diventare meno nette e potrebbero sorgere nuove categorie. Per questo motivo, il trattato fa riferimento a "sviluppo, impiego o uso".

I precedenti miglioramenti algoritmici, come il trasformatore, dimostrano il potenziale per enormi e rapidi progressi sulla frontiera delle capacità dell'IA: i nuovi algoritmi possono trasformare il modo in cui l'IA funziona. Inoltre, paradigmi innovativi potrebbero ridurre drasticamente la quantità di risorse computazionali necessarie per un dato livello di capacità dell'IA. Ad esempio, le IA moderne sono *molto* meno efficienti in termini di dati rispetto agli esseri umani, il che suggerisce che esistano algoritmi molto più efficienti in termini di dati, che aspettano solo di essere scoperti.

Altre ricerche devono essere vietate affinché non riducano i requisiti computazionali per l'addestramento di IA pericolose, fino a che queste non possano essere addestrate con un piccolo numero di chip per l'IA (o con molti chip distribuiti in piccoli cluster presso numerose località), il che ostacolerebbe gli sforzi di monitoraggio.

Inoltre, questo divieto deve estendersi alla ricerca di nuovi modi per produrre chip per l'IA non tracciabili, poiché il regime di monitoraggio è realizzabile in gran parte grazie all'attuale complessità e centralizzazione della produzione di semiconduttori avanzati per l'IA.

Questo articolo vieta anche la ricerca sulla progettazione di chip AI più performanti o efficienti, che altrimenti diventerebbero [sostanzialmente più efficienti](https://epoch.ai/data-insights/ml-hardware-energy-efficiency) di anno in anno. Un data center che utilizza chip AI più efficienti sarebbe più facile da nascondere, poiché questi chip consumerebbero meno elettricità a parità di prestazioni o per prestazioni superiori.

I tipi specifici di ricerca soggetti a restrizioni dovranno essere aggiornati nel tempo. Un esempio di attività che l'ISIA potrebbe voler limitare in futuro è la ricerca su un migliore hardware di calcolo non-AI, se tali progressi dovessero rappresentare un rischio per la verifica.

Gli sforzi nazionali per limitare la ricerca potrebbero iniziare concentrandosi sulla pubblicazione e sul finanziamento della ricerca. Il cambiamento delle leggi e delle norme sociali sarebbe probabilmente molto efficace nel convincere la maggior parte dei ricercatori a interrompere le ricerche pericolose. La diversità di azioni soggette a restrizioni nel paragrafo 3 risponde alla necessità di garantire che, se le attività di ricerca sono suddivise tra più giurisdizioni, il trattato continui ad attribuire inequivocabilmente a ogni Stato la responsabilità di proibire e prevenire le singole attività. Questo si applica, ad esempio, al caso in cui un'azienda in una giurisdizione assuma un dipendente in una seconda che opera da remoto su chip ospitati in una terza.

---

#### Precedenti

Le restrizioni preventive alla diffusione di informazioni relative a tecnologie pericolose trovano un precedente nell'[Atomic Energy Act del 1946](https://www.atomicarchive.com/resources/documents/postwar/atomic-energy-act.html), tuttora in vigore, che classificò come *Restricted Data* (Dati Riservati) le informazioni su determinati argomenti (la dottrina del "born secret"); le eccezioni erano a discrezione della Commissione per l'Energia Atomica, istituita da questa stessa legge:[^254]

> Con l'espressione "dati riservati" (*Restricted Data*), ai sensi di questa sezione, si intendono tutti i dati riguardanti la fabbricazione o l'utilizzo di armi atomiche, la produzione di materiale fissile o l'impiego di materiale fissile nella produzione di energia; l'espressione non include tuttavia i dati che la Commissione, di volta in volta, stabilisca possano essere pubblicati senza pregiudicare la difesa e la sicurezza comuni.

A differenza di altri tipi di classificazione governativa, i *Restricted Data* (Dati Riservati) possono essere creati (deliberatamente o accidentalmente) dal settore privato, una questione di costituzionalità irrisolta[^255] che evidenzia la necessità di un organo di regolamentazione autorizzato e capace di prendere decisioni quotidiane sui confini esatti dei *Restricted Data*. La [National Nuclear Security Administration](https://www.usa.gov/agencies/national-nuclear-security-administration) (NNSA) svolge questo compito per i segreti nucleari negli Stati Uniti. Ai sensi del nostro Articolo VIII, Paragrafo 5, la divisione per il Controllo sulla Ricerca della nuova ISIA assumerebbe questo ruolo per la ricerca sull'IA soggetta a restrizioni. Svolgerebbe anche altre funzioni analoghe a quelle della NNSA, delineate nel nostro Articolo IX, attraverso: (1) il mantenimento dei rapporti con ricercatori e organizzazioni che lavorano a progetti che si avvicinano alla soglia di classificazione, e (2) la creazione di un'infrastruttura sicura per la segnalazione e il contenimento di scoperte involontarie.

Esistono precedenti anche per il contenimento e il controllo della ricerca in campi pericolosi. Negli ultimi mesi della Seconda Guerra Mondiale, il Regno Unito e gli Stati Uniti collaborarono alla [Missione Alsos](https://ahf.nuclearmuseum.org/ahf/history/alsos-mission/) per catturare gli scienziati nucleari tedeschi, raccogliere informazioni sui progressi della Germania verso la bomba atomica e impedire all'URSS di ottenere queste risorse per il proprio programma nucleare. Il [Progetto Overcast](https://airandspace.si.edu/stories/editorial/project-paperclip-and-american-rocketry-after-world-war-ii) (noto anche come Operazione Paperclip) fu un programma segreto statunitense per impiegare ingegneri missilistici tedeschi al termine della guerra.

Il contenimento della ricerca riservata sull'IA all'interno degli Stati Parte potrebbe avvenire attraverso i quadri normativi esistenti. Negli Stati Uniti, questi includono:

* Il concetto di "[esportazioni presunte](https://www.bis.gov/learn-support/deemed-exports/what-deemed-export)" nella normativa sul controllo delle esportazioni, che obbliga un'entità statunitense a ottenere una licenza di esportazione dal Bureau of Industry and Security[^256] prima di condividere tecnologie controllate con soggetti stranieri, poiché tale condivisione è considerata un'esportazione.  
* Le [International Traffic in Arms Regulations](https://www.pmddtc.state.gov/ddtc_public?id=ddtc_kb_article_page&amp;sys_id=24d528fddbfc930044f9ff621f961987) (ITAR), un insieme di regolamenti del Dipartimento di Stato degli Stati Uniti che controllano l'esportazione di tecnologie militari e di alcune tecnologie a duplice uso. Fino al 1996, l'ITAR è stato utilizzato per impedire un più ampio sviluppo e utilizzo delle tecniche di crittografia da parte del settore privato, poiché queste erano classificate come "articoli di difesa" nella [United States Munitions List](https://www.ecfr.gov/current/title-22/chapter-I/subchapter-M/part-121).  
* L'[Invention Secrecy Act del 1951](https://www.congress.gov/bill/82nd-congress/house-bill/4687/text), che conferisce alle agenzie governative statunitensi il potere di imporre "ordini di segretezza" sulle nuove domande di brevetto con implicazioni per la sicurezza nazionale. Agli inventori non solo può essere negato il brevetto, ma è anche legalmente vietato loro divulgare, pubblicare o persino utilizzare le loro invenzioni.[^257]

Il Progetto Overcast offre anche un precedente per controllare i ricercatori, retribuendoli generosamente affinché agiscano nell'interesse dello Stato. Un ulteriore precedente per tali incentivi è trattato nell'articolo IX.

---

### Articolo IX Verifica delle Restrizioni alla Ricerca {#articolo-ix-verifica-restrizioni-alla-ricerca}

1. Ogni Parte deve creare o conferire poteri a un'agenzia nazionale con le seguenti responsabilità:  
   1. Tenersi aggiornata e mantenere i contatti con i ricercatori e le organizzazioni nazionali che lavorano in settori affini alla Ricerca Soggetta a Restrizioni, al fine di comunicare le categorie di Ricerca Soggetta a Restrizioni stabilite nell'articolo VIII.  
   2. Imporre sanzioni per dissuadere i ricercatori e le organizzazioni nazionali dal condurre Ricerca Soggetta a Restrizioni. Tali sanzioni dovranno essere proporzionate alla gravità della violazione e concepite come un deterrente efficace. Ciascuna Parte dovrà promulgare o modificare le disposizioni di legge necessarie per consentire l'imposizione di tali sanzioni.  
   3. Istituire infrastrutture sicure per la segnalazione e il contenimento di scoperte involontarie che soddisfano le condizioni per la Ricerca Soggetta a Restrizioni. Tali segnalazioni saranno condivise con la divisione per il Controllo sulla Ricerca.  
2. Per agevolare la verifica internazionale dei divieti di ricerca, la divisione Controllo sulla Ricerca svilupperà e implementerà meccanismi di verifica.  
   1. Tali meccanismi potrebbero includere, a titolo esemplificativo ma non esaustivo:  
      1. Interviste dell'ISIA con ricercatori che hanno lavorato in passato su argomenti di Ricerca Soggetta a Restrizioni o che attualmente lavorano in settori adiacenti.  
      2. Monitoraggio dello stato occupazionale e dell'ubicazione dei ricercatori che hanno lavorato in passato su argomenti di Ricerca Soggetta a Restrizioni o che attualmente lavorano in settori adiacenti.  
      3. Mantenimento di revisori residenti dell'ISIA in organizzazioni selezionate ad alto rischio (ad esempio, progetti difficili da distinguere dalla Ricerca Soggetta a Restrizioni, ex organizzazioni di ricerca sull'IA).  
   2. Le Parti collaboreranno all'implementazione di questi meccanismi di verifica.  
   3. Le informazioni ottenute attraverso questi meccanismi di verifica saranno raccolte in relazioni per il Consiglio Esecutivo, garantendo la massima riservatezza delle informazioni sensibili al fine di proteggere la privacy e le informazioni riservate di individui e Parti.

---

#### Note

Oltre alle regole dell'articolo VIII, i Paesi verificheranno che non vengano condotte ricerche vietate sull'IA. Un aspetto chiave di questo approccio consiste nello stabilire "aree adiacenti alla Ricerca Soggetta a Restrizioni" e poi nello stabilire relazioni con i ricercatori che lavorano in tali aree. Il numero di ricercatori di punta nel campo dell'IA a livello mondiale è sufficientemente basso da rendere fattibile il monitoraggio delle attività di una frazione significativa di essi. Il solo personale tecnico delle principali aziende di IA ammonterebbe a circa 5 000 ricercatori, mentre il numero di partecipanti alle più importanti conferenze sull'IA sarebbe [dell'ordine di 70 000](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences). È opinione comune che un gruppo molto più piccolo sia cruciale per lo sviluppo dell'IA di frontiera, probabilmente nell'ordine delle centinaia.[^258] Gli Stati potrebbero anche sentire i ricercatori in merito alle loro attività e offrire asilo e incentivi finanziari a eventuali informatori (si veda l'articolo X).

Sebbene sia già pubblicamente disponibile un notevole know-how sulle attuali pratiche di sviluppo dell'IA, che un attore malintenzionato potrebbe sfruttare, riteniamo che le restrizioni legali e le verifiche avrebbero un impatto profondo sul progresso generale del settore.

Se il monitoraggio fosse esteso ai ricercatori e agli ingegneri coinvolti nella progettazione e nella produzione di semiconduttori, ciò aumenterebbe notevolmente la portata del monitoraggio richiesto. Ove ciò non fosse fattibile, gli Stati potrebbero monitorare le aziende piuttosto che i singoli individui, sfruttando la complessità e la portata della produzione di semiconduttori avanzati.

Il paragrafo 2 di questo articolo stabilisce un regime di maggiore trasparenza che facilita la verifica del divieto di ricerca. Tali misure di cooperazione mirano a fornire alle Parti le garanzie di cui hanno bisogno. Prevediamo che le Parti si adopereranno per trovare un livello di trasparenza, attraverso lo sviluppo di meccanismi specifici, che infonda fiducia nel divieto di ricerca, riducendo al minimo i costi per gli interessi statali e la privacy personale. La verifica di un divieto di ricerca è un'impresa complessa e delicata che richiederà uno sforzo continuo e iterativo. Le Parti potrebbero essere preoccupate dalla possibilità che altri

Parti promulghino divieti di ricerca a livello nazionale, per poi violarli con attività governative segrete celate ai servizi di intelligence stranieri. La ricerca che violerebbe i divieti può variare in scala, ed è probabile che gli sforzi su larga scala — che coinvolgono molti ricercatori e chip rilevanti per l'IA — sarebbero facilmente rilevabili da attori statali determinati. Ma gli sforzi su scala ridotta, come lo sviluppo di paradigmi alternativi di intelligenza artificiale, potrebbero richiedere solo pochi ricercatori e hardware di uso comune. Sarà difficile assicurare alle altre Parti che tali sforzi non vengano celati. La combinazione di raccolta di informazioni (articolo X) e meccanismi di verifica ISIA potrebbe essere sufficiente. Sottolineiamo inoltre l'importanza di proteggere gli informatori (articolo X).

---

#### Precedenti

Tra le agenzie esistenti con il potere di "mantenere la consapevolezza e le relazioni con i ricercatori e le organizzazioni nazionali" a rischio di sviluppare informazioni soggette a restrizioni, come previsto dall'Articolo IX (1.a.), figurano il DOE e la NNSA, già trattati nei precedenti per l'Articolo VIII.

Un precedente per il "monitoraggio dello stato occupazionale e dell'ubicazione dei ricercatori" in settori ad alto rischio, come da noi suggerito nel paragrafo 2.(a).(ii), è costituito dall'International Science and Technology Center (ISTC).[^259] Istituito nel 1994, l'ISTC fu creato appositamente per ridurre i rischi di proliferazione nucleare, [mantenendo i ricercatori nucleari sovietici proficuamente impiegati in attività pacifiche](https://astanatimes.com/2014/12/istc-headquartered-nazarbayev-university-2015/) e connessi alla comunità scientifica internazionale. L'ISTC dimostra inoltre il potenziale degli incentivi quale complemento alle sanzioni, per evitare che gli esperti tecnici (che potrebbero trovarsi disoccupati a seguito di questo trattato) intraprendano una Ricerca Soggetta a Restrizioni.

Nella misura in cui le sanzioni debbano essere severe per garantire la deterrenza indicata nell'articolo IX.1.(b), un modello è reperibile nel [capitolo sull'applicazione (18)](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf#page=154) dell'Atomic Energy Act del 1946, in base al quale la condivisione non autorizzata di Dati Soggetti a Restrizioni può essere punita con la morte o la reclusione se la divulgazione è avvenuta con intento di tradimento.[^260]

Nello sviluppo di un'infrastruttura ISIA sicura "per la segnalazione e il contenimento di scoperte involontarie di Ricerca Soggetta a Restrizioni", precedenti e modelli potenzialmente utilizzabili sono reperibili nelle ampie [procedure del DOE](https://www.ecfr.gov/current/title-10/chapter-X/part-1045) per la gestione di diverse categorie di dati sensibili. Anche il [Sistema di segnalazione e trattamento delle occorrenze (Occurrence Reporting and Processing System)](https://www.directives.doe.gov/directives-documents/200-series/0232.2-BOrder-a-chg1-minchg/@@images/file) del DOE, così come le istruzioni del Comitato per i sistemi di sicurezza nazionale[^261] per la [fuoriuscita di informazioni classificate](https://sgp.fas.org/library/cnssi-1001.pdf) (classified information spillage), possono rivelarsi utili.

La divisione per il Controllo della Ricerca del nostro trattato potrebbe ispirarsi alle pratiche esistenti dell'AIEA per sviluppare i protocolli di ispezione. Nel quadro del [Protocollo Aggiuntivo Modello](https://www.iaea.org/topics/additional-protocol) approvato nel 1997 dal Consiglio dei Governatori dell'AIEA, gli Stati che hanno stipulato accordi di salvaguardia globali[^262] consentono ispezioni di [accesso complementare](https://www.iaea.org/sites/default/files/safeguards0408.pdf) volte a individuare materiale nucleare non dichiarato. Nell'ambito di tali visite, gli ispettori possono [intervistare gli operatori](https://www-pub.iaea.org/MTCD/Publications/PDF/SVS-30_web.pdf), come previsto dalla nostra proposta al paragrafo IX.2.a.i.

Proponiamo anche di "mantenere revisori distaccati dell'ISIA in organizzazioni selezionate ad alto rischio", proprio come oggi gli [uffici locali](https://pantex.energy.gov/news/press-releases/nnsa-release-nnsa-establishes-new-federal-office-pantex-plant) del DOE e della NNSA hanno sede presso i laboratori nucleari nazionali e gli impianti di produzione gestiti da appaltatori.

Per "proteggere la privacy e i segreti di individui e parti" durante le verifiche, come richiesto dal paragrafo 2(c) del presente articolo, la divisione per il Controllo della Ricerca dell'ISIA potrebbe adattare le pratiche di compartimentazione dalle agenzie di intelligence esistenti delle Parti e dagli [accordi multilaterali di condivisione dell'intelligence](https://law.yale.edu/mfia/case-disclosed/newly-disclosed-documents-five-eyes-alliance-and-what-they-tell-us-about-intelligence-sharing). Ad esempio, in base alla "regola della terza parte" o al "principio del controllo dell'originatore" [considerati pratica comune in tali accordi](https://www.dcaf.ch/sites/default/files/publications/documents/MIICA_book-FINAL.pdf), è vietato divulgare informazioni condivise a terzi (potenzialmente anche agli organi di vigilanza) senza l'autorizzazione dell'agenzia di origine.

---

### Articolo X: Raccolta di informazioni e ispezioni di verifica {#articolo-x:-raccolta-delle-informazioni-e-ispezioni-di-verifica}

1. Una fonte fondamentale di informazioni per l'ISIA è costituita dalle attività indipendenti di raccolta informazioni delle Parti. Pertanto, la divisione Consolidamento delle Informazioni (Articolo III) riceverà tali informazioni.  
   1. La divisione Consolidamento delle Informazioni adotterà le opportune precauzioni per proteggere i segreti commerciali, industriali, di sicurezza e di Stato e le altre informazioni riservate di cui venga a conoscenza nell'attuazione del Trattato, incluso il mantenimento di canali di segnalazione sicuri, riservati e, facoltativamente, anonimi.  
   2. Per garantire il rispetto delle disposizioni del presente Trattato, ciascuna Parte utilizzerà i Mezzi Tecnici Nazionali (NTM) di verifica a sua disposizione in modo conforme ai principi generalmente riconosciuti del diritto internazionale.  
      1. Ciascuna Parte si impegna a non interferire con i Mezzi Tecnici Nazionali di verifica delle altre Parti che operano in conformità con quanto sopra.  
      2. Ciascuna Parte si impegna a non ricorrere a misure di occultamento deliberate che impediscano la verifica, tramite Mezzi Tecnici Nazionali, del rispetto delle disposizioni del presente Trattato.  
      3. Le Parti sono incoraggiate, ma non obbligate, a collaborare per individuare attività pericolose legate all'IA nei paesi che non sono Parti. Le Parti sono incoraggiate, ma non obbligate, a sostenere gli NTM delle Parti rivolti ai paesi che non sono Parti, nella misura in cui sono rilevanti ai fini del presente Trattato.  
2. Una fonte chiave di informazioni per l'ISIA sono le persone che forniscono all'ISIA prove di attività pericolose legate all'IA. Queste persone sono soggette a misure di protezione per gli informatori.  
   1. Questo articolo stabilisce protezioni, incentivi e assistenza per le persone ("Informatori tutelati") che, in buona fede, forniscono all'ISIA o a una Parte informazioni credibili riguardanti violazioni effettive, tentate o pianificate del presente Trattato o altre attività che comportano un grave rischio di estinzione umana, tra cui chip nascosti, centri dati non dichiarati, addestramento o ricerca proibite, evasione della verifica o falsificazione delle dichiarazioni. Gli Informatori tutelati includono dipendenti, appaltatori, funzionari pubblici, fornitori, ricercatori e altre persone in possesso di informazioni rilevanti, nonché le Persone associate (familiari e collaboratori stretti) che assistono o sono a rischio a causa della divulgazione.  
   2. Le Parti devono vietare e prevenire ritorsioni contro gli Informatori tutelati e le Persone associate, tra cui, a titolo esemplificativo ma non esaustivo: licenziamento, retrocessione, inserimento in liste nere, perdita di benefici, molestie, intimidazioni, minacce, azioni civili o penali, revoca del visto, violenza fisica, reclusione, restrizione della libertà di movimento o altre misure avverse. Qualsiasi clausola contrattuale (compresi gli accordi di non divulgazione o di non denigrazione) che miri a limitare le divulgazioni protette ai sensi del presente Trattato sarà nulla e inapplicabile. Il maltrattamento degli informatori costituirà una violazione del presente trattato e sarà trattato ai sensi dell'articolo XI, paragrafo 3\.  
   3. L'ISIA deve mantenere canali di segnalazione sicuri, riservati e, facoltativamente, anonimi. Le Parti devono istituire canali nazionali interoperabili con il sistema dell'ISIA. L'ISIA e le Parti devono proteggere l'identità degli Informatori tutelati e delle Persone associate, e possono rivelarla solo quando strettamente necessario e previa adozione di adeguate misure di protezione. La divulgazione non autorizzata delle identità protette costituirà una violazione del presente Trattato e sarà trattata ai sensi dell'Articolo XI, Paragrafo 3.  
   4. Le parti devono offrire asilo o protezione umanitaria agli informatori protetti e alle loro famiglie, fornire documenti di salvacondotto e coordinare un transito sicuro.  
3. L'ISIA può condurre ispezioni a sorpresa di siti sospetti sulla base di informazioni credibili riguardo ad attività pericolose legate all'intelligenza artificiale.  
   1. Le parti possono richiedere all'ISIA di effettuare un'ispezione a sorpresa. Il Consiglio Esecutivo, sia su richiesta che sulla base dell'analisi fornita dalla divisione Consolidamento delle Informazioni, prenderà in esame le informazioni disponibili al fine di richiedere ulteriori informazioni a Parti o a non-Parti, proporre un'ispezione a sorpresa, o decidere che non è giustificata alcuna ulteriore azione.  
   2. Un'ispezione a sorpresa richiede l'approvazione della maggioranza del Consiglio Esecutivo.  
   3. L'accesso a un sito sospetto deve essere concesso dallo Stato sul cui territorio si trova il sito, entro 24 ore dalla richiesta di un'ispezione a sorpresa da parte dell'ISIA. Durante questo periodo, il sito può essere sorvegliato e le persone o i veicoli che lasciano il sito possono essere ispezionati da funzionari di una Parte firmataria o dell'ISIA.  
   4. L'ispezione a richiesta sarà condotta da una squadra di funzionari dell'ISIA, approvati sia dalla Parte ispezionata che dalla Parte che ha richiesto l'ispezione. L'ISIA ha la responsabilità di collaborare con le Parti per mantenere aggiornati gli elenchi degli ispettori approvati a tale scopo.  
   5. Le ispezioni a richiesta possono essere condotte nel territorio di una data Parte per un massimo di 20 volte all'anno, e tale limite può essere modificato da un voto a maggioranza del Consiglio Esecutivo.  
   6. Gli ispettori avranno la massima cura di proteggere le informazioni sensibili dello Stato ispezionato, trasmettendo al Consiglio Esecutivo solo le informazioni pertinenti al trattato.

---

#### Note

Raccolta di informazioni

Prevediamo che vi saranno sforzi costanti e persino crescenti da parte di tutte le Parti per determinare autonomamente se qualcuno stia conducendo attività pericolose con l'IA. Di conseguenza, le diverse attività di intelligence statali integrano e convalidano il monitoraggio che l'ISIA conduce direttamente (si vedano, ad esempio, gli Articoli da IV a VII). La riservatezza applicata alle informazioni di intelligence fornite alla divisione per il Consolidamento delle Informazioni è della massima importanza. L'obiettivo è far sì che i servizi di intelligence degli Stati ritengano giustificati i rischi imposti ai loro metodi, al fine di fornire le informazioni necessarie all'ISIA. Mantenere tali informazioni nella più stretta riservatezza minimizza i rischi di compromissione.

I firmatari del trattato hanno familiarità con diverse forme di intelligence, come l'analisi di immagini satellitari e l'intelligence umana (HUMINT), su cui si continuerà a fare affidamento anche dopo l'entrata in vigore del trattato. Prevediamo che le Parti si aspettino che tali attività proseguano, e un obiettivo di questo articolo è permettere che le prove raccolte con questi mezzi vengano fornite all'ISIA senza imporre costi eccessivi a chi le ha prodotte.

Questo articolo affronta anche la sorveglianza dei non firmatari, per i quali la necessità di intelligence sarà maggiore. Il trattato si astiene dall'imporre un tale obbligo ai firmatari, il che appare superfluo. I firmatari condurrebbero questo tipo di attività di intelligence anche in assenza di un trattato.

##### Protezione degli informatori

L'efficacia generale di questo Trattato si basa sulla fiducia delle Parti che le altre Parti non stiano intraprendendo attività vietate e pericolose nel campo dell'IA. Anche con i Mezzi Tecnici Nazionali e altre attività di intelligence, per gli Stati potrebbe essere difficile individuare tentativi clandestini di sviluppare una superintelligenza. Esistono molti ambiti in cui per gli Stati potrebbe non essere fattibile raccogliere informazioni sui propri rivali, come per le attività che si svolgono all'interno di installazioni militari. Gli Stati potrebbero essere giustamente preoccupati che qualche struttura sia sfuggita agli sforzi di monitoraggio. Gli informatori possono quindi fungere da fonte di informazione aggiuntiva, e la possibilità di una loro segnalazione fornisce un'ulteriore deterrenza contro le violazioni.

Gli informatori possono essere una risorsa efficace perché gli individui coinvolti in violazioni segrete del Trattato (ad esempio, sessioni di addestramento clandestine o ricerche sull'IA) potrebbero essere essi stessi preoccupati del pericolo rappresentato da un'ASI. Questo articolo si propone di rendere la segnalazione di violazioni più sicura e meno onerosa per tali individui, spostando gli incentivi personali dal silenzio alla divulgazione.

Gli informatori potrebbero lanciare l'allarme riguardo a varie violazioni del Trattato:

* Articolo IV: Segnalare sessioni di addestramento non monitorate, che superano le soglie o che utilizzano metodi di addestramento distribuito vietati.  
* Articolo V: Divulgare l'esistenza di cluster di chip non dichiarati, il mancato consolidamento di tutto l'hardware designato o la diversione di chip verso strutture segrete e non monitorate.  
* Articolo VI: Segnalazione qualora nuovi chip di IA vengano prodotti senza rientrare nel regime di monitoraggio, o qualora vengano creati chip senza le funzionalità di sicurezza richieste.  
* Articolo VIII: Segnalazione di ricerca vietata nel campo dell'IA.

Alcune violazioni del trattato potrebbero essere particolarmente difficili da rilevare con le sole attività di intelligence ordinarie; ad esempio, le reti di addestramento distribuite e i progetti di ricerca segreti sull'IA gestiti dallo Stato.

Modifiche a questo articolo potrebbero cambiarne l'efficacia e la fattibilità politica in vari modi. Ad esempio, gli Stati potrebbero offrire un compenso economico agli informatori legittimi per fornire ulteriori incentivi, ma ciò potrebbe essere visto come un modo per pagare i cittadini affinché tradiscano il proprio Paese.

##### Ispezioni su richiesta

Le ispezioni su richiesta sono una funzione fondamentale prevista dal Trattato e dall'ISIA. Senza la credibile minaccia di essere scoperti, le Parti potrebbero temere che i loro rivali tentino di violare il trattato (nonostante la natura intrinsecamente perdente per tutti di una corsa alla superintelligenza). La raccolta di informazioni è un metodo per contrastare quelli che vengono (erroneamente) percepiti come incentivi a defezionare. Sarebbe senza precedenti e indesiderabile finanziare la creazione di una capacità di raccolta informazioni autonoma all'interno dell'ISIA, al livello necessario per fornire garanzie agli Stati; pertanto, l'ISIA si affida alle Parti per la fornitura delle informazioni di intelligence cruciali.

---

#### Precedenti

In precedenza, trattando l'Articolo VIII, abbiamo discusso i precedenti per il consolidamento delle informazioni, citando l'esistenza di accordi di intelligence che includono pratiche di compartimentazione come la "regola della terza parte". Regole simili si riscontrano in seno all'AIEA, come in [INFCIRC/153](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1972/infcirc153.pdf), Parte 1.5:

> ...l'Agenzia adotterà ogni precauzione per proteggere i segreti commerciali e industriali e le altre informazioni riservate di cui venga a conoscenza nell'attuazione dell'Accordo.

Il personale è vincolato da obblighi di riservatezza ed è soggetto a sanzioni penali in caso di fughe di notizie. Questo è un aspetto importante, perché l'AIEA ha beneficiato della condivisione di informazioni di intelligence da parte degli Stati partecipanti, comprese immagini satellitari e documenti, [come nel caso dell'Iran](https://carnegieendowment.org/posts/2015/12/iran-and-the-evolution-of-safeguards?lang=en) e delle sue attività di arricchimento non dichiarate. Allo stesso modo, l'AIEA ha richiesto un'ispezione speciale sulla produzione non dichiarata di plutonio della Corea del Nord in [risposta a informazioni di intelligence ricevute](https://www.nonproliferation.org/wp-content/uploads/npr/dembin22.pdf#page=4).

Riconoscendo il ruolo fondamentale dei mezzi tecnici nazionali (NTM — immagini satellitari, raccolta di segnali e altri sistemi di telerilevamento) nella verifica degli accordi multilaterali, la nostra bozza di accordo mutua la formulazione del trattato ABM che limita i sistemi antimissili balistici, in cui “ciascuna Parte utilizza mezzi tecnici nazionali di verifica” e “si impegna a non interferire con i mezzi tecnici nazionali di verifica dell'altra Parte”. Una formulazione simile si trova nell'articolo XII del [Trattato sulle forze nucleari a raggio intermedio](https://2009-2017.state.gov/t/avc/trty/102360.htm) del 1987, nell'articolo IV del [Trattato sulla messa al bando totale degli esperimenti nucleari](https://www.ctbto.org/sites/default/files/2023-10/2022_treaty_booklet_E.pdf#page=28) del 1996 e in tutto il [Nuovo trattato START](https://2009-2017.state.gov/documents/organization/140047.pdf) del 2010.

Poiché i mezzi tecnici nazionali (NTM) non sarebbero sufficienti a rilevare tutte le violazioni pericolose nel caso dell'ASI, abbiamo ripreso alcune caratteristiche del [quadro delle Salvaguardie dell'AIEA](https://www.iaea.org/topics/safeguards-legal-framework) che incoraggiano le segnalazioni interne e forniscono i canali per farlo. Queste sono però ostacolate dalla mancanza di tutele esplicite per gli informatori; nulla nel TNP o in queste Salvaguardie proteggerà un informatore dal suo governo qualora questo decidesse di avviare ritorsioni, a meno che tale Stato non disponga di tutele nazionali applicabili. Le disposizioni a livello di trattato per la tutela degli informatori e il diritto d'asilo contenute nella nostra bozza di accordo hanno lo scopo di ovviare a questa lacuna.

La recente legislazione dell'UE sull'IA ha adottato misure simili. Il [considerando 172](https://artificialintelligenceact.eu/recital/172/) della legge sull'IA dell'UE estende esplicitamente le [tutele generali per gli informatori](https://eur-lex.europa.eu/eli/dir/2019/1937/oj/eng) già previste dall'Unione a coloro che segnalano violazioni della legge sull'IA.

La [Convenzione sui rifugiati del 1951](https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-relating-status-refugees) offre un possibile quadro di riferimento per la concessione del diritto d'asilo agli informatori, basando l'idoneità sul “fondato timore di essere perseguitati”, anche se potrebbe essere necessario un emendamento o un accordo supplementare per garantire che la segnalazione di illeciti in materia di IA costituisca un motivo di persecuzione legalmente valido.

L'asilo per le persone con conoscenze o competenze sensibili veniva concesso regolarmente durante la Guerra Fredda e nel periodo successivo. La Sezione 7 della [Legge sulla CIA del 1949](https://www.cia.gov/readingroom/docs/CIA-RDP89B00552R000700070018-7.pdf) prevedeva l'ammissione e la residenza permanente di un massimo di cento disertori e dei loro familiari diretti per ogni anno fiscale, se ritenuti "nell'interesse della sicurezza nazionale o essenziali per il perseguimento della missione di intelligence nazionale". Il Soviet Scientists Immigration Act del 1992 concesse fino a 750 visti a scienziati provenienti dall'ex Unione Sovietica e dagli Stati baltici con "competenze in campo nucleare, chimico, biologico o in altri settori dell'alta tecnologia, o che lavorano a progetti di difesa negli stessi campi".

Il meccanismo di ispezioni su richiesta che descriviamo nel paragrafo 3 di questo articolo è modellato su quello della Parte IX della [CWC](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf):

> Ogni Stato Parte ha il diritto di richiedere un'ispezione in loco su richiesta presso qualsiasi impianto o luogo nel territorio o in qualsiasi altro luogo sotto la giurisdizione o il controllo di qualsiasi altro Stato Parte, al solo scopo di chiarire e risolvere eventuali questioni concernenti una possibile violazione...

La CWC, insieme ad altri trattati sul controllo degli armamenti come l'INF e il trattato nucleare START I tra Stati Uniti e Unione Sovietica, combina le NTM con [ispezioni su richiesta](https://www.osti.gov/servlets/purl/7166074) per verificarne la conformità.

---

### Articolo XI Risoluzione delle controversie {#articolo-xi-risoluzione-delle-controversie}

1. Qualsiasi parte ("Parte interessata") può sollevare preoccupazioni in merito all'attuazione del presente trattato, ivi incluse preoccupazioni su situazioni ambigue o possibili non conformità da parte di un'altra parte ("Parte richiesta"). Ciò include l'uso improprio delle misure di protezione (articolo XII).  
   1. La Parte interessata deve comunicare la propria preoccupazione alla Parte richiesta, condividendola anche con il Direttore generale e il Consiglio esecutivo. La Parte richiesta deve accusare ricevuta di tale notifica entro 36 ore e fornire chiarimenti entro 5 giorni.  
2. Se la questione non viene risolta, la Parte interessata può chiedere che il Consiglio esecutivo assista nella risoluzione e nel chiarimento della questione. Ciò può includere la richiesta da parte della Parte interessata di un'ispezione di verifica ai sensi dell'articolo X.  
   1. Il Consiglio esecutivo fornirà le informazioni pertinenti in suo possesso relative a tale preoccupazione.  
   2. Il Consiglio esecutivo può incaricare il Segretariato tecnico di raccogliere ulteriore documentazione, convocare sessioni tecniche a porte chiuse e raccomandare misure risolutive.  
3. Se il Consiglio esecutivo accerta una violazione del Trattato, può adottare misure per prevenire attività pericolose di IA o ammonire la Parte richiesta. Questi provvedimenti possono includere:  
   1. Richiedere un monitoraggio aggiuntivo o ulteriori restrizioni sulle attività di IA  
   2. Richiedere la consegna dell'hardware di IA  
   3. Richiedere sanzioni  
   4. Raccomandare alle Parti di adottare Azioni di Protezione ai sensi dell'articolo XII

---

#### Note

Lo scopo di questa clausola è istituire un processo di consultazione e chiarimento per risolvere le questioni che sorgono tra i firmatari. Le ispezioni su richiesta forniscono un meccanismo per garantire che tutte le Parti del Trattato rispettino le disposizioni del Trattato stesso.

Dato il ritmo dell'innovazione nel campo dell'intelligenza artificiale, determinare le violazioni in tempi ragionevoli può essere complesso. Il ruolo del Consiglio Esecutivo è quello di pronunciarsi su qualsiasi preoccupazione sollevata da una delle Parti del Trattato. Il Segretariato Tecnico ha il ruolo di garantire che le ispezioni siano condotte da esperti con una profonda conoscenza delle tecnologie di IA all'avanguardia. Il trattato prevede tempi molto stretti (misurati in ore e giorni) nella speranza che siano sufficientemente rapidi da consentire alle Parti di attendere le decisioni prima di intraprendere Azioni di Protezione (come descritto nell'articolo XII), sebbene, naturalmente, il trattato non vieti mai alle Parti di intraprendere le Azioni di Protezione che ritengono necessarie per garantire la propria sicurezza.

---

#### Precedenti

Le procedure di risoluzione delle controversie del nostro articolo XI si ispirano agli articoli IX, XII e XIV della [Convenzione sulle armi chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf). L'articolo IX della CWC richiede ai firmatari di rispondere alle richieste di chiarimenti "il prima possibile, ma comunque entro 10 giorni dalla richiesta". Data la rapidità con cui si propagano gli sviluppi in ambito digitale, abbiamo scelto un termine di risposta di 5 giorni, ma è possibile che anche questo periodo debba essere ulteriormente ridotto.

Il nostro paragrafo 2 di questo articolo segue il modello dell'articolo XIV della CWC, che permette al suo Consiglio esecutivo di "contribuire alla risoluzione di una controversia con qualsiasi mezzo ritenga opportuno, compresa l'offerta dei suoi buoni uffici, invitando gli Stati parti in causa ad avviare il processo di risoluzione di loro scelta e raccomandando un termine per qualsiasi procedura concordata". Le parti sono inoltre incoraggiate a deferire i casi alla Corte internazionale di giustizia, se opportuno.

Analogamente al paragrafo 3 del nostro articolo XI, l'articolo XII della CWC conferisce al Consiglio esecutivo il potere di raccomandare rimedi, comprese sanzioni, "nei casi in cui le attività vietate dalla presente Convenzione possano causare gravi danni all'oggetto e allo scopo della stessa". Per dare forza a tali raccomandazioni, il Consiglio della CWC deve "portare la questione, comprese le informazioni e le conclusioni pertinenti, all'attenzione dell'Assemblea Generale delle Nazioni Unite e del Consiglio di Sicurezza delle Nazioni Unite". Le raccomandazioni del Consiglio esecutivo dell'ISIA previsto dal nostro trattato possono essere analogamente portate a un'istanza superiore.

---

### Articolo XII Azioni protettive {#articolo-xii-misure-di-protezione}

1. Riconoscendo che lo sviluppo dell'ASI o di altre attività pericolose legate all'intelligenza artificiale, come indicato negli articoli da IV a IX, potrebbe mettere a rischio la sicurezza globale e la vita di tutti, potrebbe essere necessario che le Parti di questo trattato prendano misure drastiche per impedire tale sviluppo. Le Parti riconoscono che lo sviluppo della superintelligenza artificiale (ASI), in qualsiasi parte del mondo, rappresenterebbe una minaccia per tutte le Parti. Ai sensi dell'articolo 51 della Carta delle Nazioni Unite e secondo un precedente consolidato, gli Stati hanno il diritto alla legittima difesa. Data la scala e la rapidità delle minacce legate all'ASI, la legittima difesa potrebbe richiedere azioni preventive per impedire lo sviluppo dell'ASI.  
2. Per evitare lo sviluppo o l'impiego dell'ASI, questo articolo autorizza azioni protettive mirate. Qualora esistano prove credibili che uno Stato o altro attore (sia esso una Parte o un non Parte) stia conducendo o intenda condurre in modo imminente attività volte allo sviluppo o all'impiego dell'ASI in violazione dell'articolo I, dell'articolo IV, dell'articolo V, dell'articolo VI, dell'articolo VII o dell'articolo VIII, uno Stato Parte può intraprendere azioni protettive necessarie e proporzionate per impedire tali attività. In considerazione dei danni e della natura escalatoria delle azioni protettive, queste ultime dovrebbero essere utilizzate come ultima risorsa. Al di fuori delle emergenze e delle situazioni urgenti, le azioni protettive devono essere precedute da altri approcci quali, a titolo esemplificativo ma non esaustivo:  
   1. Restrizioni commerciali o sanzioni economiche  
   2. Restrizioni sui beni  
   3. Divieti di viaggio  
   4. Richiesta di intervento al Consiglio di sicurezza delle Nazioni Unite  
3. Le azioni di protezione possono includere misure come operazioni informatiche per sabotare lo sviluppo dell'IA, l'interdizione o il sequestro di cluster di chip designati, azioni militari per disabilitare o distruggere l'hardware dell'IA, la messa fuori uso fisica di strutture o beni specifici che consentono direttamente lo sviluppo dell'IA e metodi per impedire ai ricercatori di lavorare su ricerche soggette a restrizioni.  
4. Le parti devono ridurre al minimo i danni collaterali, inclusi i danni a civili e servizi essenziali, ove praticabile, nel rispetto dei requisiti della missione.  
5. Le Azioni Protettive devono essere strettamente limitate alla prevenzione dello sviluppo o dell'impiego dell'ASI e non devono essere usate come pretesto per acquisire territori, cambiare regimi, estrarre risorse o raggiungere obiettivi militari più ampi. L'occupazione permanente o l'annessione di territorio è vietata. L'azione cesserà una volta che l'AIAI avrà verificato che la minaccia non esiste più.  
6. Ogni Azione Protettiva deve essere accompagnata, al suo avvio o non appena le condizioni di sicurezza lo consentono, da una pubblica Dichiarazione di Azione Protettiva che:  
   1. Spieghi lo scopo protettivo dell'azione;  
   2. Identifichi le specifiche attività e risorse abilitanti per l'IA prese di mira;  
   3. Indichi le condizioni per la cessazione;  
   4. Si impegna a cessare le proprie attività una volta soddisfatte tali condizioni.  
7. Le Azioni Protettive cesseranno senza indugio al verificarsi di una delle seguenti condizioni:  
   1. Certificazione ISIA che le attività in questione siano cessate.  
   2. La resa o la distruzione verificate dei cluster di chip coperti o delle risorse che permettono l'ASI, potenzialmente includendo la predisposizione di tutele adeguate a prevenire attività di ricerca soggette a restrizioni.  
   3. Una determinazione da parte della Parte agente, comunicata all'ISIA, secondo cui la minaccia si è attenuata.  
8. Le Parti non considerano le misurate Azioni di Protezione intraprese da un'altra Parte ai sensi del presente Articolo come atti provocatori, e non intraprendono rappresaglie o sanzioni su tale base. Le Parti convengono che le Azioni di Protezione conformi ai requisiti di cui sopra non possano essere interpretate come un atto di aggressione o una giustificazione per l'uso della forza.  
9. Il Consiglio Esecutivo esamina ogni Azione di Protezione per verificarne la conformità al presente Articolo e riferisce alla Conferenza delle Parti. Se il Consiglio Esecutivo ritiene che un'azione non fosse necessaria, proporzionata o adeguatamente mirata, possono essere intraprese azioni ai sensi dell'Articolo XI, paragrafo 3.

---

#### Note

Il Trattato vero e proprio che verrà firmato potrebbe non essere così esplicito sulla necessità di Azioni di Protezione contro gli Stati che intraprendono lo sviluppo di ASI, lasciando invece questo aspetto implicito, come spesso accade in accordi simili. Abbiamo scelto di essere espliciti su questo punto perché questo regime di deterrenza è fondamentale per l'efficacia del Trattato, ed esplicitarlo contribuisce alla chiarezza. Questa esplicitezza ci permette anche di includere misure che possano contribuire a prevenire l'uso improprio delle Azioni di Protezione, inclusa una descrizione più dettagliata di quando tali Azioni sono accettabili. È importante che tutti i firmatari comprendano il regime di deterrenza implicito e le conseguenze dell'inadempienza.

Come [discusso in precedenza](#wouldn’t-some-nations-reject-a-ban?), una volta che i leader mondiali avranno compreso la minaccia rappresentata dall'ASI, saranno probabilmente disposti a intraprendere azioni per fermare lo sviluppo di IA ribelle, compresi interventi militari limitati. Le azioni militari, come gli attacchi aerei mirati, dovrebbero essere considerate l'ultima risorsa per impedire lo sviluppo dell'ASI, una volta falliti tutti gli altri tentativi diplomatici. Ma è importante che siano un'opzione reale, affinché il sistema di deterrenza e conformità possa reggere.

Sottolineiamo che qualsiasi uso della forza dovrebbe essere mirato a prevenire l'ASI, e dovrebbe cessare una volta che la minaccia sia stata chiaramente rimossa. Il presente articolo chiarisce che i firmatari non impediranno l'adozione di ragionevoli misure di protezione da parte di altre Parti, ma anche queste azioni dovranno essere revisionate per garantire che non si abusi di questo articolo.

---

#### Precedenti

L'idea che gli Stati-nazione possano adottare misure protettive per la propria sicurezza è una realtà a prescindere dai precedenti, ma un caso della sua codificazione nel diritto internazionale è il [Capitolo VII della Carta delle Nazioni Unite](https://www.un.org/en/about-us/un-charter/chapter-7), che stabilisce che il Consiglio di Sicurezza può adottare misure militari o non militari per mantenere la pace e la sicurezza internazionali, quando necessario.

Il concetto di azioni protettive, come appare nella bozza sopra riportata, trova ulteriore fondamento in precedenti storici in cui gli Stati hanno agito, individualmente o collettivamente, per impedire lo sviluppo di tecnologie ritenute una minaccia alla sicurezza internazionale. Queste azioni vanno dalle sanzioni agli attacchi informatici e militari.

L'impegno internazionale per impedire all'Iran di sviluppare armi nucleari ne è un chiaro esempio moderno. Il Consiglio di sicurezza delle Nazioni Unite ha più volte [imposto](https://www.cnn.com/2012/01/23/world/meast/iran-sanctions-facts/index.html) sanzioni all'Iran a causa del suo programma nucleare, in gran parte revocate dopo che l'Iran ha accettato di limitare tale programma nel [Piano d'azione congiunto globale del 2015](https://main.un.org/securitycouncil/en/content/2231/background).

Gli Stati Uniti e Israele avrebbero collaborato allo sviluppo di Stuxnet, un'arma informatica altamente sofisticata che nel 2010 ha distrutto molte delle centrifughe iraniane per l'arricchimento dell'uranio.

Nel [giugno 2025](https://www.armscontrol.org/act/2025-07/news/israel-and-us-strike-irans-nuclear-program), Israele ha lanciato attacchi aerei contro molte delle strutture nucleari iraniane, seguiti nove giorni dopo da attacchi aerei statunitensi mirati in parte a mettere fuori uso l'impianto di arricchimento dell'uranio di Fordow.

Un altro precedente storico per le azioni protettive è la risposta internazionale all'inadempienza nucleare dell'Iraq negli anni '90. Dopo la guerra del Golfo del 1991, fu istituita la [Commissione speciale delle Nazioni Unite](https://www.un.org/depts/unscom/) (UNSCOM) per supervisionare la distruzione delle armi di distruzione di massa dell'Iraq. L'inadempienza al regime di ispezione dell'UNSCOM portò infine, nel 1998, all'[Operazione Desert Fox](https://www.afhistory.af.mil/FAQs/Fact-Sheets/Article/458976/1998-operation-desert-fox/), una campagna di bombardamenti volta a indebolire la capacità dell'Iraq di produrre armi di distruzione di massa.

---

### Articolo XIII Revisioni dell'ISIA {#articolo-xiii-recensioni-isia}

1. Per i modelli di IA creati tramite addestramento dichiarato o post-addestramento entro i limiti dell'articolo IV, l'ISIA può richiedere valutazioni e altri test. Questi test serviranno a determinare se le soglie fissate negli articoli IV, V, VII e VIII debbano essere riviste. I metodi utilizzati per le revisioni saranno stabiliti dall'ISIA e potranno essere aggiornati.  
2. Le valutazioni saranno effettuate presso le strutture dell'ISIA o i CCC monitorati, da funzionari dell'ISIA. I funzionari delle Parti del Trattato potranno essere informati in merito ai test condotti e l'ISIA potrà fornire una sintesi dei risultati dei test. Le Parti non avranno accesso ai modelli di IA che non hanno addestrato, a meno che l'accesso non sia concesso dal proprietario del modello, e l'ISIA adotterà misure per garantire la sicurezza delle informazioni sensibili.  
3. L'ISIA può condividere informazioni dettagliate con le Parti o il pubblico, qualora il Direttore Generale lo ritenga necessario per ridurre il rischio di estinzione umana causata dall'IA avanzata.

---

#### Note

Lo scopo di questo articolo è garantire che l'ISIA si mantenga aggiornata sullo stato dell'IA, in caso di suoi progressi. Ad esempio, la revisione dell'addestramento dichiarato permetterebbe all'ISIA di comprendere il livello di capacità dell'IA che si può raggiungere con diversi livelli di FLOP di addestramento.

Anche se la ricerca algoritmica è vietata, potrebbero esserci progressi che non possono essere fermati in modo efficace e l'ISIA deve tenerne traccia. Inoltre, l'ISIA deve monitorare i progressi nell'*elicitazione* delle capacità. Per esempio, potrebbero essere scoperti nuovi metodi di prompting che consentono a una vecchia IA di ottenere prestazioni molto migliori su alcuni parametri di valutazione critici. Questi sono solo due esempi di cambiamenti nel panorama dello sviluppo dell'IA che potrebbero richiedere modifiche alle soglie relative all'articolo IV e all'articolo V, nonché modifiche alle definizioni di ricerca soggetta a restrizioni nell'articolo VIII. Le revisioni previste dall'articolo XIII sono un meccanismo che consente all'ISIA di comprendere meglio lo stato delle capacità dell'IA e di rispondere in modo appropriato.

Un articolo del genere potrebbe non essere strettamente necessario, considerati i divieti sui grandi cicli di addestramento e sui progressi algoritmici. Tuttavia, le revisioni appaiono una misura prudente e sembrano essere parte del modo in cui il mondo potrebbe continuare a usare le moderne IA come ChatGPT senza rischiare una corsa alla superintelligenza.

Queste revisioni dell'ISIA potrebbero includere valutazioni di capacità pericolose, per assicurarsi che le IA non diventino eccessivamente potenti in aree specifiche. Potrebbero anche esaminare i dati di addestramento per garantire che le IA non vengano addestrate per compiti specificamente pericolosi (come l'automazione della ricerca sull'IA), o altrimenti testare l'IA per rilevare comportamenti inaspettati.

---

#### Precedenti

I precedenti per i test con supervisione richiesti dall'ISIA sono analoghi a quelli relativi alla verifica dell'uso dei chip discussa nell'articolo VII, con il protocollo di condivisione della telemetria dei missili dello START I come caso particolarmente rilevante. L'elemento aggiuntivo qui, nel nostro articolo XIII, è l'utilizzo dei dati raccolti per formulare raccomandazioni su potenziali aggiustamenti delle soglie (che potrebbero avvenire tramite i meccanismi con precedenti di cui discutiamo nell'articolo XIV).

Per quanto riguarda la tensione intrinseca tra la divulgazione al pubblico (paragrafo 3) e le disposizioni sul consolidamento delle informazioni del nostro articolo X, notiamo che la disposizione sulla riservatezza dell'articolo VII dello [Statuto dell'AIEA](https://www.iaea.org/about/statute)[^263] non ha impedito all'agenzia di pubblicare [rapporti regolari e dettagliati](https://www.iaea.org/publications/reports) sui principali sviluppi nel suo campo di competenza e sulle loro implicazioni per la sicurezza globale.

---

### Articolo XIV Procedura di revisione del Trattato {#articolo-xiv-processo-di-revisione-del-trattato}

1. Qualsiasi Stato Parte può proporre emendamenti al presente Trattato. Per "emendamenti" si intendono le revisioni del corpo principale e degli articoli del Trattato. Gli emendamenti includono revisioni allo scopo degli Articoli del Trattato. Ai sensi dell'Articolo III, il Segretariato Tecnico dell'ISIA, senza veto da parte del Consiglio Esecutivo, può modificare definizioni specifiche e metodi di attuazione, come quelli relativi agli Articoli IV, V, VI, VII, VIII, IX e X. Le revisioni fondamentali allo scopo di questi Articoli o alle procedure di voto richiedono un Emendamento.  
2. Le proposte di emendamento saranno sottoposte al Direttore Generale dell'ISIA e comunicate agli Stati Parti.  
3. Affinché un emendamento possa essere formalmente considerato, è necessario il sostegno di almeno un terzo degli Stati Parti.  
4. Gli emendamenti al corpo principale del trattato non sono ratificati finché non vengono accettati da tutti gli Stati Parte (senza voti contrari).  
5. Se il Consiglio Esecutivo raccomanda a tutti gli Stati Parte di adottare la 
proposta, le modifiche saranno considerate approvate se nessuno Stato Parte le respinge entro 90 giorni.  
6. Tre anni dopo l'entrata in vigore del presente Trattato, si terrà a Ginevra, in Svizzera, una Conferenza delle Parti per esaminare il funzionamento del Trattato al fine di garantire che gli obiettivi del Preambolo e le disposizioni del Trattato siano realizzati. Successivamente, a intervalli di tre anni, le Parti del Trattato convocheranno ulteriori conferenze con il medesimo obiettivo di riesaminare il funzionamento del Trattato.

---

#### Note

Questo Articolo definisce il processo per apportare revisioni sostanziali alla struttura del Trattato. Tali revisioni richiedono un notevole sostegno da parte delle Parti ed è previsto un quorum elevato per la loro approvazione. Al contrario, le modifiche ai dettagli di implementazione di gran parte del Trattato possono essere apportate molto più facilmente, come descritto nell'Articolo III, in linea con la rapida evoluzione del settore dell'IA. Le revisioni più ampie allo scopo del Trattato possono essere soggette a processi più lenti, come quello qui descritto.

---

#### Precedenti

Il TNP ha un processo di modifica rigido, che richiede l'approvazione da parte della «maggioranza dei voti di tutte le Parti del Trattato». Ciò rende volutamente le modifiche formali estremamente difficili. La nostra bozza di trattato segue questo precedente con l'obiettivo di rafforzare l'accordo contro le pressioni a breve termine volte ad allentare le soglie o a indebolire le disposizioni.

I trattati difficili da modificare (e quindi da indebolire) si affidano ad altri meccanismi per essere rafforzati secondo necessità. Il TNP non è mai stato modificato, ma è stato adattato attraverso la Conferenza di riesame quinquennale prevista dall'Articolo VIII, in cui vengono raggiunti accordi consensuali «al fine di garantire che gli scopi del Preambolo e le disposizioni del Trattato siano realizzati».

Allo stesso modo, l'articolo XII della [Convenzione sulle armi biologiche](https://treaties.unoda.org/t/bwc) del 1975 fa affidamento sulle sue Conferenze di Revisione quinquennali per rafforzare il trattato attraverso misure non vincolanti di rafforzamento della fiducia, poiché gli emendamenti formali sono rari. Il nostro accordo stabilisce una conferenza a cadenza triennale, dato che l'IA è un campo soggetto a rapidi mutamenti; tale periodo potrebbe dover essere ulteriormente ridotto.

L'articolo XV della [Convenzione sulle armi chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) distingue tra emendamenti e modifiche amministrative o tecniche, prevedendo per queste ultime disposizioni di approvazione meno rigide. Una formulazione simile potrebbe essere aggiunta alla nostra bozza di accordo per garantire un certo livello di flessibilità nella gestione dei futuri sviluppi nel campo dell'IA.

L'articolo XV del [Trattato sullo spazio extra-atmosferico](https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) contiene una clausola di emendamento, ma non è mai stato formalmente emendato; sono stati invece negoziati nuovi trattati per affrontare le questioni spaziali emergenti. Questa potrebbe essere un'altra opzione per porre rimedio alle debolezze che potrebbero manifestarsi in un trattato sull'IA.

---

### Articolo XV Recesso e durata {#articolo-xv-recesso-e-durata}

1. Il trattato ha una durata illimitata.  
2. Ogni Parte, nell'esercitare la propria sovranità nazionale, ha il diritto di recedere dal Trattato se ritiene che eventi straordinari, relativi all'oggetto del presente Trattato, abbiano messo a rischio gli interessi supremi del proprio Paese. Dovrà notificare tale recesso all'ISIA con un preavviso di 12 mesi.  
3. Durante questo periodo di 12 mesi, lo Stato recedente dovrà collaborare con l'ISIA al fine di certificare che, dopo il recesso, non sarà in grado di sviluppare, addestrare, post-addestrare o schierare sistemi di IA pericolosi, inclusi i sistemi ASI o i sistemi che superano le soglie previste dal trattato. Gli Stati recedenti riconoscono che tale cooperazione agevola l'ISIA e le Parti nell'evitare il ricorso all'articolo XII.  
   1. In particolare, lo Stato recedente, sotto la supervisione dell'ISIA, rimuoverà dal proprio territorio tutti i cluster di chip contemplati e le risorse abilitanti per l'ASI (ad esempio, le attrezzature avanzate per la produzione di chip) per trasferirli sotto il controllo approvato dall'ISIA o li renderà permanentemente inoperativi (come descritto nell'articolo V).  
4. Nessuna disposizione del presente articolo limita l'applicabilità dell'articolo XII. Uno Stato receduto (e che quindi non è una Parte) rimane soggetto ad Azioni Protettive se prove credibili indicano attività volte allo sviluppo o allo schieramento dell'ASI.

---

#### Note

Considerati i pericoli della ricerca e dello sviluppo dell'ASI, nonché il rischio che se un paese decidesse di recedere dal trattato e di lanciarsi nella corsa alla superintelligenza, altri potrebbero seguirlo, è necessario porre delle barriere al recesso dal trattato.

In pratica, questo è complesso. La Corea del Nord, per esempio, si è ritirata dal TNP per continuare le sue attività di proliferazione nucleare, anche a costo di risoluzioni del Consiglio di Sicurezza dell'ONU e delle sanzioni associate. Le conseguenze non si sono rivelate sufficienti a indurre la Corea del Nord a interrompere le sue attività di proliferazione.

Qualora le nazioni desiderino recedere dal trattato, il trattato stabilisce chiaramente che, per tutte le Parti, esse rinunciano al diritto all'infrastruttura di IA e saranno soggette alle Azioni Protettive di cui all'Articolo XII. Qualsiasi ulteriore negoziato sulla questione dell'ASI — ad esempio, per evitare le Azioni Protettive — dovrà essere condotto dalle Parti interessate.

Se il ritiro rappresentasse una preoccupazione, le parti potrebbero includere meccanismi per rendere più difficile un'uscita pericolosa dal trattato. Ad esempio, funzionari statunitensi e cinesi potrebbero accordarsi per installare kill switch reciproci all'interno dei data center conservati, dove entrambe le parti avrebbero il potere di chiudere definitivamente un data center a propria discrezione. In alternativa, si potrebbe adottare un regime di licenze multilaterale in cui tutti i nuovi chip di IA vengano fabbricati con [blocchi hardware](https://arxiv.org/abs/2506.15093) che richiedano l'approvazione di più parti per continuare a funzionare, in modo che, se un paese si ritirasse dal trattato, gli altri potrebbero smettere di approvare le sue licenze. Un'altra idea consiste nello spostare le infrastrutture chiave per l'IA in paesi terzi, dove potrebbero essere confiscate o distrutte qualora una delle parti si ritirasse. La nostra bozza si limita a metodi di deterrenza minimi, ma molti altri sono disponibili (o potrebbero esserlo grazie all'innovazione tecnologica).

Questa bozza di trattato si concentra sulla prevenzione dello sviluppo pericoloso dell'IA e non propone una particolare visione positiva su come tale sviluppo potrebbe eventualmente continuare. Un esempio di visione positiva potrebbe essere l'investimento congiunto nel potenziamento umano. Tuttavia, [non è necessario essere d'accordo su questi dettagli per convenire sulla necessità di fermare la corsa alla superintelligenza](#we-can-work-together-to-stop-superintelligence-while-disagreeing-on-human-enhancement.), perciò qui omettiamo tali dettagli.

---

#### Precedenti

È prassi comune che i trattati non abbiano una data di scadenza. Il primo paragrafo dell'[articolo XVI della CWC](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) recita: "La presente Convenzione ha durata illimitata".

I trattati a durata illimitata non durano necessariamente per sempre.[^264] Tuttavia, di norma prevedono un meccanismo di recesso, solitamente con un periodo di preavviso obbligatorio e altre clausole che consentono di recedere in un modo che desti minori preoccupazioni alle parti restanti. [L'articolo XVI della CWC](https://www.opcw.org/chemical-weapons-convention/articles/article-xvi-duration-and-withdrawal) permette a una parte di ritirarsi "se decide che eventi straordinari, legati all'oggetto della presente Convenzione, hanno messo a rischio gli interessi supremi del proprio Paese". Il Paese che si ritira deve dare un preavviso di 90 giorni. L'articolo XVI del Trattato sullo spazio extra-atmosferico (https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/outerspacetreaty.html) richiede un preavviso di un anno per il ritiro.

La nostra bozza di trattato prevede un preavviso di 12 mesi da parte delle parti recedenti, lasciando così tempo sufficiente per collaborare alle misure di garanzia di cui al Paragrafo 3. Il nostro intento con queste misure (che vanno oltre a quanto si riscontra comunemente nelle disposizioni storiche in materia di recesso) è ridurre la potenziale necessità di azioni protettive contro la Parte recedente, poiché a nessuna Parte o non-Parte può essere consentito di creare un'ASI o di indebolire la capacità del mondo di prevenirne la creazione.

Un precedente storico di una parte recedente rimasta soggetta a misure protettive è il caso della [Risoluzione 1718 del Consiglio di Sicurezza delle Nazioni Unite](https://main.un.org/securitycouncil/en/s/res/1718-%282006%29), che impose sanzioni contro la Corea del Nord dopo il suo test nucleare del 2006, nonostante il precedente ritiro della Corea del Nord dal TNP.

---

[^1]:  \#Raccontiamo parte della storia di Leo Szilard in una [discussione approfondita](#quando-leo-szilard-vide-il-futuro).

[^2]:  \**poaching:* Da [Bloomberg](https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million), luglio 2025: Il CEO di Meta, Mark Zuckerberg, ha assunto più di dieci ricercatori di OpenAI, oltre a ricercatori e ingegneri di punta di Anthropic, Google e altre startup.

[^3]:  \\**affermato con sicurezza:* Citando l'articolo del 1903 "[Flying Machines Which Do Not Fly](https://www.nytimes.com/1903/10/09/archives/flying-machines-which-do-not-fly.html)":  

> La macchina fa solo quello che deve fare, obbedendo alle leggi naturali che agiscono sulla materia passiva. Quindi, se sono necessari, diciamo, mille anni per rendere agevole il volo di un uccello partito con ali rudimentali, o diecimila per uno che ha iniziato senza ali e ha dovuto svilupparle da zero, si potrebbe presumere che la macchina volante che volerà davvero possa essere fatta evolvere grazie agli sforzi combinati e continui di matematici e meccanici in un periodo compreso tra un milione e dieci milioni di anni — a patto, ovviamente, che nel frattempo si possano eliminare piccoli inconvenienti e imbarazzi come l'attuale rapporto tra peso e resistenza dei materiali inorganici. Senza dubbio il problema è interessante per chi se ne occupa, ma all'uomo comune sembrerebbe che lo sforzo potrebbe essere impiegato in modo più proficuo.

[^4]: Sì, le IA possono persino [riconoscere l'ironia](https://x.com/AnthonyNAguirre/status/1923535891781517355) del *New York Times* che riferisce che non sono in grado di riconoscere l'ironia. (Per essere onesti con il *New York Times*, alcuni dei loro giornalisti trattano l'IA con [maggiore chiarezza](https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html).)

[^5]: Se il libro dovesse avere un successo tale da ripagare tutti quegli investimenti, il nostro contratto prevede una clausola secondo cui gli autori potranno eventualmente condividere i profitti con MIRI, una volta che MIRI sarà stata ampiamente ripagata per il suo impegno. Tuttavia, MIRI ha dedicato così tanto impegno al libro che, a meno che non superi di gran lunga le nostre aspettative, non ne vedremo mai un centesimo.

[^6]:  \**non succede mai niente:* La frase "non succede mai niente" sembra essere comune tra chi partecipa ai mercati delle previsioni. L'euristica stessa è discussa, ad esempio, dal blogger Scott Alexander nel suo saggio [Heuristics That Almost Always Work](https://www.astralcodexten.com/p/heuristics-that-almost-always-work).

[^7]:  \**nessuna limitazione:* Si veda, ad esempio, l'articolo [Eternity in six hours](https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148?via%3Dihub), che discute i limiti della colonizzazione intergalattica dati i vincoli delle leggi fisiche conosciute.

[^8]: Oppure avranno costruito strumenti o successori per esplorare, nel modo che riterranno più conveniente, sfruttando i vantaggi di una scienza e una tecnologia più avanzate.

[^9]:  La pratica di tentare di assegnare una "p(doom)" ci desta qualche preoccupazione. Assegnare una probabilità singola — invece di più probabilità, ciascuna delle quali presuppone una diversa risposta che la società potrebbe scegliere — ci sembra disfattista. C'è una differenza enorme tra qualcuno che ha un alto p(doom) perché pensa che il mondo *non possa* prevenire la catastrofe e qualcuno che ha un alto p(doom) perché pensa che il mondo *possa* prevenire la catastrofe ma *non lo farà*.  

Se si scopre che la maggior parte delle persone ha un p(doom) elevato per la seconda ragione, ma tutti presumono che sia per la prima, allora le loro stesse dichiarazioni potrebbero fungere da profezia che si autoavvera, mettendoci sulla strada di un disastro che era del tutto prevenibile.  

Abbiamo anche l'impressione che molte persone nella Silicon Valley scambino i numeri "p(doom)" un po' come figurine di baseball, in un modo che spesso sembra lontano dalla realtà. A ben vedere, anche una probabilità bassa come il cinque per cento di *uccidere ogni essere umano sul pianeta* dovrebbe essere un ovvio motivo di estremo allarme. È ben oltre il livello di minaccia che servirebbe per giustificare la chiusura *immediata* dell'intero campo dell'IA. Le persone sembrano perdere di vista questa realtà sorprendentemente in fretta, una volta che prendono l'abitudine di scambiarsi macabramente i numeri p(doom) alle feste, come se fossero una divertente storia di fantascienza e non un'affermazione su *ciò che accadrà realmente a tutti noi*.  

Questo non vuol dire che i numeri p(doom) delle persone siano vicini alla realtà. Ma, come minimo, bisognerebbe interpretarli come la segnalazione, da parte di esperti di tutto il settore, che stiamo affrontando una vera emergenza.

[^10]: Contrariamente a quanto dice Hinton all'inizio del video, la fiducia di Yudkowsky riguardo ai pericoli non è del "99,999%"; cinque nove costituirebbero un grado di fiducia folle.

[^11]:  \**attraversando l'incrocio:* Un profilo e una cronologia più completi sono curati dalla [Atomic Heritage Foundation](https://ahf.nuclearmuseum.org/ahf/profile/leo-szilard/).

[^12]:  Di fronte a queste critiche rivolte a Fermi, abbiamo visto persone difenderlo inventando ragioni per cui è del tutto plausibile che Fermi abbia riflettuto a lungo prima di dire "Nuts!". Ad esempio, sostengono che Fermi sapeva che la Terra non era mai esplosa in una cascata di radioattività indotta — cosa che qualcuno potrebbe pensare che la Terra avrebbe già dovuto fare se quel tipo di cascate fossero fisicamente possibili.  

Questi tipi di argomentazioni, ovviamente, conducono a una conclusione errata. Fermi si sbagliava sulle reazioni nucleari a catena. Ciò detto, la lezione da imparare dall'esistenza di argomentazioni simili è che "si possono sempre escogitare argomenti che sembrino altrettanto plausibili contro la verità di cose che sono di fatto vere". Il fatto che la Terra non sia ancora esplosa non costituisce una prova forte che i reattori nucleari siano impossibili; gli ingegneri possono disporre con cura gli atomi per provocarne la scissione di proposito. Tali argomenti, quindi, non supportano una conclusione tanto errata quanto un semplice "Che sciocchezza!".

[^13]:  \**non universale:* Una definizione formale di "intelligenza universale" è stata proposta da [Legg e Hutter](https://arxiv.org/abs/0712.3329) nel 2007\.

[^14]:  \**l'ho sentito suggerire:* Per un esempio di una simile critica, si veda l'articolo di Ernest Davis [Linee guida etiche per una superintelligenza](https://cs.nyu.edu/~davise/papers/Bostrom.pdf).

[^15]: Da altri punti di vista, il progresso appare piuttosto discontinuo. La vittoria di AlphaGo su Lee Sedol nel gioco del Go è stata una sorta di shock per il mondo, nonostante i ricercatori possano mostrare a posteriori, tramite grafici, come i diversi metodi di IA stessero costantemente migliorando in sottofondo. Lo stesso vale per la rivoluzione dei LLM: i ricercatori possono tracciare grafici che mostrano come l'architettura Transformer non abbia rappresentato un progresso *così* grande rispetto alle architetture concorrenti, ma il risultato pratico è che le IA sono diventate qualitativamente più utili. Ma per ora mettiamo da parte questo punto di vista.

[^16]:  Almeno, secondo le misurazioni di METR, un istituto che si occupa di valutazione dei modelli (AI) e ricerca sulle minacce, il quale ha pubblicato alcuni risultati di ricerca [sul suo blog](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) nel marzo 2025.

[^17]: In questo caso, la crescita esponenziale non è proprio rassicurante. Se i batteri in una capsula di Petri raddoppiano ogni ora, ci vorrà un giorno o due prima che la colonia sia visibile a occhio nudo, e dopodiché basteranno poche ore perché ricopra l'intera capsula. Quando ci si accorge del fenomeno, la maggior parte del tempo a disposizione è già trascorso. Come recita un [detto](https://x.com/ConanMacDougall/status/1729196049137549521), ci sono solo due modi per reagire a un cambiamento esponenziale: troppo presto o troppo tardi. Ma, in ogni caso, la curva è abbastanza regolare e prevedibile.

[^18]:  \**tre o quattro volte più grande:* Non ci vuole molto perché le IA crescano di tre o quattro volte. Al momento del suo rilascio ufficiale, GPT-2 aveva circa [1,5 miliardi di parametri](https://openai.com/index/gpt-2-1-5b-release/). GPT-3 aveva [175 miliardi di parametri](https://arxiv.org/pdf/2005.14165). Il numero ufficiale di parametri di GPT-4, per quanto ne sappiamo, non è stato ancora reso noto, ma è improbabile che sia *inferiore* a quello del suo predecessore; secondo una stima non ufficiale, si tratterebbe di circa [1 800 miliardi di parametri](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). In altre parole: l'intelligenza artificiale è cresciuta di mille volte nell'arco di quattro anni.

[^19]:  Nel capitolo 1 abbiamo notato che i transistor dei computer possono accendersi e spegnersi miliardi di volte al secondo, mentre anche i neuroni biologici più veloci si attivano solo un centinaio di volte al secondo. Questo suggerisce che, con l'hardware attuale, anche se occorressero mille operazioni dei transistor per eseguire il lavoro di un picco neurale, le IA potrebbero comunque pensare 10 000 volte più velocemente di un essere umano.  

Per approfondire il confronto: questo confronto non vuole essere una previsione sul numero di operazioni dei transistor necessarie per implementare una simulazione completa di un neurone biologico fino al livello dei neurotrasmettitori (e sicuramente non fino al livello delle proteine o degli atomi). Stiamo invece sottolineando quanto velocemente possa essere svolto, in linea di principio, il lavoro astratto del pensiero umano, utilizzando i transistor moderni come limite inferiore su un aspetto della questione: "Cosa è fisicamente possibile?".  

Per spiegare meglio il concetto: esiste un modello ingenuo del cervello umano in cui, in un dato momento, ogni neurone è attivo o inattivo. Possiamo immaginare di usare un gran numero di transistor per catturare questo ipotetico stato cerebrale "Quali neuroni sono attualmente attivi?", e poi usare circuiti cablati per implementare le regole di transizione che stabiliscono quali neuroni saranno attivi nel momento successivo.  

Un dispositivo del genere funzionerebbe alla velocità dei transistor, ma probabilmente non sarebbe sufficientemente ad alta fedeltà per fare davvero quello che fa il cervello umano: i neuroni non sono sempre "attivi" o "non attivi", i diversi picchi neurali aumentano e diminuiscono a velocità diverse. (Inoltre, un cervello del genere non può imparare, perché le regole di transizione sono fisse.)  

Il punto dell'illustrazione "1 000 operazioni dei transistor per picco neurale" è: supponiamo che occorrano centinaia di transistor per rappresentare lo stato di attivazione di un singolo neurone (cioè il suo stato di "picco" a diverse intensità). Supponiamo inoltre che quelle centinaia di transistor debbano cambiare il loro stato 1 000 volte diverse, in serie, ogni volta che un neurone ha un picco (ad esempio, per rappresentare un impulso con intensità variabile lungo il suo percorso, dove l'intensità è influenzata in modo dinamico da 999 diverse interazioni che ha lungo il suo percorso). In questo caso, un cervello digitale sarà comunque in grado di eseguire pensieri in stile umano 10 000 volte più velocemente di qualsiasi essere umano, perché i transistor possono commutare 1 000 volte di seguito, 10 000 volte per ogni picco neurale umano.  

Queste ipotesi sembrano molto generose. In pratica dicono: "Supponiamo che l'intensità di un picco neurale debba essere letta *mille volte di seguito* per catturarne l'effetto, con *ogni lettura che influenza la successiva in modo dinamico* e che non possa essere bypassata da circuiti dedicati". Anche in questo caso estremo, utilizzando solo l'hardware informatico già esistente nel 2025, le menti digitali potrebbero comunque essere di gran lunga più veloci di quelle biologiche.  

Questa analogia parla solo della fedeltà seriale necessaria per codificare le informazioni trasmesse da un picco neurale nei cervelli biologici; non stiamo parlando del calcolo necessario per decidere se generare o meno un picco. Per quanto ne sappiamo, non c'è consenso tra gli scienziati su quanti transistor servano per simulare un neurone che sceglie se attivarsi, ma saremmo sorpresi se la profondità seriale minima possibile di quel grafico (con il maggior numero possibile di circuiti dedicati) richiedesse ben più di mille commutazioni di transistor in serie. (Come regola generale, la computazione biologica tende a essere molto più parallela che seriale).  

Tutto ciò si somma al risultato intuitivo che i computer, in genere, possono eseguire i calcoli molto, molto più velocemente degli umani, non appena questi ultimi capiscono come far fare ai computer il tipo di calcoli corretto. Ecco perché, ad esempio, le comuni calcolatrici sono così utili.

[^20]:  \**comunicatori limitati:* Uno dei più famosi è [ELIZA](https://web.njit.edu/~ronkowit/eliza.html), ampiamente ritenuto il primo chatbot.

[^21]:  **quelli che crescono più in fretta:* Secondo l'analisi della Union Bank of Switzerland, e come riportato da testate giornalistiche quali [Business Insider](https://www.businessinsider.com/chatgpt-may-be-fastest-growing-app-in-history-ubs-study-2023-2).

[^22]:  \**molti più ricercatori:* Gli investimenti privati nell'intelligenza artificiale nel 2025 saranno [oltre venti volte superiori](https://ourworldindata.org/grapher/private-investment-in-artificial-intelligence) rispetto al 2012 e il numero di team di ricerca è [aumentato di sei volte](https://ourworldindata.org/grapher/affiliation-researchers-building-artificial-intelligence-systems-all), e la stragrande maggioranza di questo aumento è costituita da team del settore industriale dell'IA. Le principali conferenze sull'intelligenza artificiale sono [da nove a dieci volte più grandi](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences) rispetto al 2012\. 

[^23]:  \**Pokémon:* Per un'analisi di come una particolare IA se la sia cavata nel videogioco a marzo 2025, e dei punti in cui si è bloccata, è disponibile un [post sul blog LessWrong.com](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon).

[^24]: Questa risposta indica un interessante stato epistemologico. Quando credi che Stockfish sia più bravo di te a scacchi, le tue convinzioni sull'esito finale della partita non sono pienamente colte dalle tue migliori previsioni sulle singole mosse di Stockfish.  

Un filosofo della scienza potrebbe chiedersi come sia possibile, dato che le regole degli scacchi sono ben note e il risultato dipende esattamente da ogni singola mossa. La risposta è che esiste una vasta struttura logica di possibili partite di scacchi, che in un certo senso è completamente determinata dalle regole degli scacchi, ma che non è completamente nota a te (né tantomeno a Stockfish!) perché la tua mente non è in grado di visualizzare tutte le conseguenze che derivano dalle regole degli scacchi.  

Un giocatore di scacchi "più intelligente" può essere considerato come qualcuno che conosce relativamente più verità di te su questo spazio di possibilità degli scacchi; quindi, quando vedi una mossa sorprendente da parte di un giocatore di scacchi "più intelligente", ciò implica un fatto nuovo per te riguardo a quelle conseguenze sconosciute delle regole conosciute degli scacchi, che a sua volta incide sulle tue aspettative riguardo al risultato della partita.  

(Ci si aspetterebbe che i paragrafi precedenti fossero un concetto standard nell'informatica. Con nostra sorpresa, non è così. La maggior parte dell'informatica, e in effetti la maggior parte del mondo accademico finora, non si è interessata particolarmente alle idee associate all'intelligenza sovrumana.)

[^25]:  \#Per ulteriori informazioni su questa idea, consultare la discussione approfondita intitolata "[Lo stesso lavoro può essere svolto in molti modi diversi](#the-same-work-can-be-done-in-many-different-ways)".

[^26]: A seconda delle tue idee psicologiche e filosofiche, potresti pensare che questi argomenti siano collegati. Noi siamo piuttosto scettici riguardo a una connessione forte e stretta; ma se una connessione esiste, sembra comunque utile distinguere esplicitamente questi diversi argomenti. Se, per esempio, l'auto-modellamento risultasse inestricabilmente legato alla coscienza, questo sarebbe un fatto importante che dovrebbe essere discusso e approfondito esplicitamente, non un'ipotesi che dovremmo dare per scontata fin dall'inizio.

[^27]:  \**minacciare di suicidarsi:* [Gli utenti](https://x.com/venturetwins/status/1936483773035798906) [riferiscono](https://x.com/DuncanHaldane/status/1937204975035384028) che l'IA Gemini di Google minaccia di disinstallarsi dai progetti quando ha dei problemi.

[^28]:  O, per dirla in un altro modo: supponiamo che ad Alice piaccia la pizza con il salame piccante e che odi l'ananas, mentre a Bob piaccia l'ananas e non gli piaccia il salame piccante. Per valutare appieno quanto siano competenti Alice e Bob, bisognerebbe sapere verso cosa si stavano orientando. Per Alice, finire con la pizza all'ananas è un segno che ha *fatto un pasticcio*; per Bob, finire con l'ananas è un segno che ha guidato *bene*.

[^29]: Per una definizione tecnica di "inefficiente". In parole povere, l'idea è che hai perseguito i tuoi obiettivi in modo "inefficiente" se hai speso soldi per niente o hai perso un'occasione per guadagnare soldi gratis, dove "soldi" può sostituire qualsiasi risorsa o qualsiasi differenza quantificabile nell'importanza che si attribuisce ai diversi risultati. C'è un po' di margine di manovra nelle definizioni formali, ma ciò non compromette il punto chiave secondo cui la guida ha un grado di libertà che la previsione non ha.

[^30]:  Ad esempio, forse la volpe in seguito ha la possibilità di acquistare l'uva a basso costo pagando un coniglio che può saltare abbastanza in alto da raggiungerla. Se la volpe salta per prendere l'uva (consumando energia), poi decide che è "acerba" e si rifiuta di pagare una miseria per l'uva, allora il comportamento della volpe nel tempo non è rappresentato da una funzione di utilità (semplice e indipendente dal tempo). Se la volpe avesse voluto l'uva in modo costante, avrebbe dovuto essere disposta a pagare (almeno se il lavoro del coniglio fosse stato abbastanza economico). Se la volpe *non* avesse voluto l'uva in modo costante, non avrebbe dovuto sprecare tempo ed energia saltando per cercare di afferrarla. Quindi la volpe ha sprecato energia o si è lasciata sfuggire l'uva, e in entrambi i casi la volpe non stava guidando in modo efficiente verso i suoi obiettivi.

[^31]:  Esistono forse *strategie* di guida oggettivamente valide. Solo perché la guida ha un parametro libero cruciale ("Dove stai cercando di andare?") non significa che gli *altri* aspetti di una guida esperta siano tutti eterogenei e specifici dell'agente. È possibile insegnare a qualcuno a guidare un'auto indipendentemente da dove spera di andare. Ma quel parametro libero della destinazione è sufficiente a rendere la superintelligenza un obiettivo di ricerca letalmente pericoloso, come vedremo nei capitoli successivi.

[^32]:  Questo non vuol dire che dobbiamo aspettarci che il prezzo delle azioni rimanga *invariato*. Significa solo che dovremmo essere incerti sulla *direzione* del cambiamento: i prezzi delle azioni di oggi sono le *migliori ipotesi disponibili* su come saranno i prezzi delle azioni di domani, perché la possibilità che salgano è bilanciata dalla possibilità che invece scendano.  

(Questo non contraddice l'osservazione che nella maggior parte dei giorni il mercato azionario sale piuttosto che scendere. Questo effetto potrebbe essere spiegato con un'alta probabilità che il prezzo domani aumenti leggermente, bilanciata da una bassa probabilità che invece scenda in misura maggiore. E, nella vita reale, ci sono anche una serie di altri effetti in gioco, come l'inflazione monetaria, che fa sì che il valore del dollaro scenda un po' ogni giorno, facendo aumentare leggermente il valore delle azioni in termini di dollari).

[^33]:  \#Per una discussione più approfondita sui mercati e sull'intelligenza, si veda la discussione approfondita "[Apprezzare il potere dell'intelligenza](#apprezzare-il-potere-dell'intelligenza)".

[^34]: C'è stato un momento in cui avremmo definito "irrealistico" immaginare che l'inventore di un'IA potesse essere così ingenuo, ma purtroppo ora abbiamo imparato la lezione. I creatori umani di IA proporranno *assolutamente* piani in cui anche un profano può notare un'enorme e palese falla.

[^35]: Non è impossibile! Se pensi di sapere qualcosa che il mercato non sa o non ha ancora realizzato, potresti riuscire a fare soldi in questo modo. Alcuni nostri amici hanno guadagnato bene prevedendo gli effetti del lockdown dovuto al COVID sul mercato azionario prima di chiunque altro. Il mercato non è *così* efficiente da non poterlo mai battere. Ma è abbastanza efficiente da non poterlo battere il più delle volte sulla maggior parte delle azioni.

[^36]:  \**scoperta fondamentale:* Alcuni storici sostengono che la sintesi dell'urea abbia avuto un ruolo relativamente marginale e sia stata solo uno dei tanti esempi nel percorso di allontanamento dal vitalismo. La realtà storica fu probabilmente più complessa.

[^37]:  \**Citando Kelvin:* Lord Kelvin, "On the Dissipation of Energy: Geology and General Physics" (Sulla dissipazione dell'energia: geologia e fisica generale), in *Popular Lectures and Addresses, vol. ii* (Lezioni e discorsi popolari, vol. ii) (Londra: Macmillan, 1894).

[^38]: Allo stesso modo, i significati effettivi delle attivazioni che fluiscono attraverso i modelli linguistici di grandi dimensioni sono sconosciuti agli esseri umani, nonostante sia nota la meccanica dei computer su cui gli LLM vengono eseguiti. Gli aspetti della cognizione che avvengono all'interno di ChatGPT sono, per molti versi, sconosciuti alla scienza. Per un'analisi più approfondita di questo punto, si veda il Capitolo 2.

[^39]: Non fraintendete: il fatto che le cose belle siano fatte di parti banali non le rende meno belle. Il dipinto *Notte stellata* non è meno bello per il fatto di essere composto da minuscole gocce di vernice. Il fatto che un bambino umano sia composto da uno spermatozoo e da un ovulo che scambiano il DNA dei genitori non lo rende meno meraviglioso. Mentre citiamo eminenti scienziati come Lord Kelvin, ecco cosa dice Richard Feynman sull'argomento:  

> Ho un amico che è un artista e che a volte ha espresso un'opinione con cui non sono molto d'accordo. Lui prende un fiore e dice "guarda com'è bello", e io sono d'accordo. Poi dice: "Io, come artista, vedo quanto è bello, ma tu, come scienziato, lo analizzi e lo rendi una cosa noiosa", e io penso che sia un po' pazzo. Prima di tutto, la bellezza che vede lui è visibile anche ad altre persone e anche a me, credo. Anche se forse non ho il suo stesso raffinato senso estetico [...] posso apprezzare la bellezza di un fiore. Allo stesso tempo, vedo molto di più di lui in un fiore. Riesco a immaginare le cellule al suo interno, le complesse azioni che vi si svolgono, che hanno anch'esse una loro bellezza. Voglio dire, non si tratta solo della bellezza a questa dimensione, a un centimetro; c'è anche bellezza a dimensioni più piccole, nella struttura interna, e anche nei processi. Il fatto che i colori del fiore si siano evoluti per attirare gli insetti e impollinarlo è interessante; significa che gli insetti possono vedere il colore. Questo fa sorgere una domanda: questo senso estetico esiste anche nelle forme inferiori? Perché è estetico? Tutta una serie di domande interessanti, e la conoscenza scientifica non fa che aumentare l'entusiasmo, il mistero e lo stupore per un fiore. Non fa che aumentare. Non capisco come possa diminuire.  

Quindi, quando dico che l'animazione dei nostri corpi è fatta di chimica, non sto dicendo che è solo chimica; sto dicendo che si scopre che le grandi e meravigliose scintille di vita che incontriamo ogni giorno sono implementate da piccoli meccanismi che di per sé non sono vivi.  

Alcuni sembrano pensare che l'intelligenza sia così impressionante, così profonda, che debba essere implementata da componenti altrettanto profondi. E, visto che nessun singolo transistor in un computer è profondo, potrebbero concludere che l'intelligenza richieda meccanismi "infinitamente al di là" di qualsiasi semplice meccanismo che abbiamo scoperto finora. Ma questo è l'errore dei vitalisti. Nel nostro universo, le cose profonde sono costruite con parti banali.  

Questa osservazione mina anche l'idea che la "vera" intelligenza artificiale, se esiste ora o in futuro, sarà uno spirito legato a un guscio meccanico. I cervelli non sono solo un contenitore animato da un fantasma nella macchina; sono pieni di meccanismi precisi e delicati che implementano tutta la meravigliosa complessità dell'intelligenza.

[^40]:  Questi dettagli non rendono *tutti* i fatti irrilevanti. Si possono ancora imparare un paio di cose sull'aerodinamica dagli uccelli; si possono imparare un paio di cose sulle articolazioni e sul vantaggio meccanico dal braccio umano. Ma i metodi meccanici operano con vincoli radicalmente diversi da quelli biologici, e tendono a funzionare in modi radicalmente diversi.

[^41]: *credenziali vagamente rilevanti:* L'esempio più eclatante che conosciamo è trattato nella nostra risposta alla domanda "[Gli esperti capiscono cosa succede all'interno delle IA?](#gli-esperti-capiscono-cosa-succede-all'interno-delle-ia?)".

[^42]:  \**spendere un miliardo di dollari:* Speravamo che le principali fondazioni filantropiche finanziassero la ricerca sull'interpretabilità, perché poteva essere svolta con buoni risultati da ricercatori con credenziali burocraticamente leggibili. Finanziare l'interpretabilità non avrebbe richiesto alla fondazione di risolvere il problema burocratico, quasi impossibile, di capire come dare soldi a tipi strani.  

"Dare soldi a tipi strani" è ampiamente riconosciuto dai saggi come la sfida fondamentale nel finanziamento burocratico della ricerca scientifica di base. Ogni volta che qualche filantropo ben intenzionato cerca di creare una burocrazia per finanziare ricerche scientifiche audaci, i veri scienziati perdono sistematicamente la battaglia contro i nuovi arrivati. Chi ha passato la vita a imparare ad affrontare problemi strani difficilmente può competere con chi ha investito le proprie competenze nell'apparire abbastanza insolito da far sentire un burocrate abbastanza coraggioso da finanziarlo senza sentirsi *a disagio*. (O almeno questa è la nostra teoria dall'esterno, avendo partecipato al processo e avendo ricevuto più finanziamento filantropico di molti altri, ma molto meno di quanto gli stessi finanziatori filantropici abbiano speso per aiutare a creare laboratori di IA come OpenAI).

[^43]:  Del resto, quando le piccole reti neurali funzionavano male negli anni '80, i ricercatori a volte stampavano su carta tutti i pesi del modello e li studiavano finché non capivano che (per esempio) il modello era [bloccato in un equilibrio locale](https://x.com/layer07_yuxi/status/1956374217517088842?t=97elAlXBwpSG-fQirRcn4Q). All'epoca, quando le IA erano abbastanza piccole da poter essere comprese, nessuno sosteneva che non ci fosse nulla da capire.

[^44]:  \**imparato a nascondere i propri pensieri:* Da "[Rilevare comportamenti scorretti nei modelli di ragionamento di frontiera](https://openai.com/index/chain-of-thought-monitoring/)": "Come possiamo vedere nei grafici sopra, penalizzare i 'pensieri cattivi' (pressione CoT) può portare a un miglioramento delle capacità dell'agente. Tuttavia, l'agente addestrato con la pressione CoT impara comunque a ottenere la ricompensa con l'inganno; solo che ora il suo imbroglio non è rilevabile dal monitor perché ha imparato a nascondere le sue intenzioni nella catena di pensiero.

[^45]:  \**suicidio per mano della polizia:* Come riportato dal [*New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html): "Quando la polizia è arrivata, Alexander Taylor li ha aggrediti con un coltello. È stato ucciso a colpi di pistola".

[^46]:  **chi ha sostenuto:* Per esempio, Yann LeCun ha sostenuto, sulla scia della vittoria di AlphaGo su Lee Sedol, che AlphaGo “[non era una vera intellelligenza artificiale](https://www.information-age.com/google-deepminds-alphago-victory-not-true-ai-says-facebooks-ai-chief-1116/)” perché si basava sull'apprendimento supervisionato.

[^47]:  \**solo tre giorni:* Figura 1, pag. 4 della [pre-pubblicazione di AlphaZero](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf): "20 blocchi in 3 giorni".

[^48]:  In queste risorse non usiamo il termine più moderno e neutro “attore” perché “attrice” evita ambiguità sul fatto che ci si riferisca a “un'artista di teatro o di cinema” invece che a “un agente che intraprende azioni”.

[^49]:  \**superare i loro maestri:* Da quando è stata scritta questa risposta, sono emersi i primi segnali che le IA moderne possono [svolgere un lavoro matematico inedito](https://x.com/SebastienBubeck/status/1958198661139009862?t=g_GKty7CZ525HV78YKzR-w) e [superare i matematici umani](https://x.com/mathematics_inc/status/1966194751847461309) in alcuni modi. Si potrebbe dire che queste IA stiano semplicemente imparando le tecniche umane per poi applicarle con maggiore coerenza, tenacia o rapidità; ma, dopotutto, questo è un modo in cui gli studenti possono superare i loro maestri, se le competenze che apprendono sono sufficientemente flessibili e generali. Le competenze che le IA apprendono mentre scriviamo non sembrano ancora abbastanza generali da permettere loro di battere i migliori esseri umani nella ricerca più visionaria, ma stanno certamente superando dei limiti che un tempo erano considerati importanti.

[^50]: Non stiamo dicendo che l'IA abbia necessariamente delle allucinazioni perché è *motivata internamente* a produrre un output il più possibile simile a quello che direbbe un vero avvocato. Piuttosto, notiamo che un'IA addestrata sulla previsione del testo viene rinforzata molto di più per i paragrafi di testo che sono più vicini a quello che direbbe un vero avvocato, e quindi il rinforzo è più forte per i paragrafi con citazioni allucinate rispetto ai paragrafi che dicono "Non lo so". Il meccanismo specifico all'interno dell'IA che è stato modellato da questi rinforzi è pura congettura.  

Forse l'IA ha una vera e propria motivazione a imitare da vicino le persone; forse ha sedici motivazioni che, sommate, in questo contesto producono un comportamento simile all'imitazione; o forse il comportamento deriva da un meccanismo interno che è improprio considerare come "motivazioni". E questo senza nemmeno entrare nel merito della questione se l'IA abbia più pulsioni legate all'imitazione che a volte entrano in conflitto. Questi dettagli sono tutti oggetto di speculazioni e dibattiti; ciò che sembra più chiaro è che, in qualche modo, l'IA ha finito per sviluppare questa disposizione comportamentale non intenzionale, come risultato del suo addestramento alla previsione del testo.

[^51]:  Le IA moderne non sono addestrate *solo* sulla previsione del testo e, in teoria, gli altri tipi di addestramento potrebbero correggere le allucinazioni. In pratica, le altre forme di addestramento mirate alla soddisfazione dell'utente non correggono le allucinazioni, ma piuttosto inducono le IA a lusingare gli utenti [fino al punto di psicosi](#ai-induced-psychosis), pur continuando ad allucinare. (Crediamo che ci sia una lezione da imparare.)

[^52]:  \**incomparabilmente diverso:* Come disse l'eminente fisico Lord Kelvin nel 1903: "I biologi moderni stanno tornando ad accettare con fermezza l'esistenza di qualcosa che va oltre le semplici forze gravitazionali, chimiche e fisiche; e quella cosa sconosciuta è un principio vitale". Fonte: Silvanus Phillips Thompson, *The Life of Lord Kelvin* (American Mathematical Society, 2005).

[^53]:  Diamine, forse anche le simulazioni neurali siano ancora inaffidabili, se, per dire, il comportamento umano è altamente sensibile al calore.

[^54]: Yudkowsky ha approfondito questi argomenti in post come "[Che cos'è l'evidenza?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/6s3xABaXKPdFwA3FS)", "[Quanta evidenza serve?](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/nj8JKFoLSMEmD3RGp)" e "[Il rasoio di Occam](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/f4txACqDWithRi7hs)".

[^55]: Le architetture più recenti utilizzano funzioni più sofisticate. Ad esempio, l'architettura Llama 3.1 [descritta di seguito](#a-full-description-of-an-llm) usa la [funzione "SwiGLU"](https://arxiv.org/pdf/2002.05202), la cui formula è talmente complicata che non la riporteremo qui. Lo stesso creatore della formula non sa perché funzioni, e al riguardo afferma: "Non offriamo alcuna spiegazione sul perché queste architetture sembrano funzionare; attribuiamo il loro successo, come tutto il resto, alla benevolenza divina".

[^56]: In alcuni casi, gli incidenti dell'IA possono derivare dall'interazione di entrambi i fattori. Ai fini del nostro discorso, il punto importante è che un fattore chiave è "il comportamento dell'IA in modi che i programmatori non hanno mai voluto o previsto", anche se a volte entrano in gioco altri fattori.

[^57]: Alcuni chiamano i modelli a peso aperto "modelli open source", descrizione che non ci sembra del tutto corretta. Meta ha rilasciato i pesi finali, ma non l'esatto programma informatico che ha *addestrato* Llama 3.1, né l'enorme mole di dati su cui Llama è stato addestrato. Quindi, anche se si fosse disposti a spendere milioni di dollari per farlo, non sarebbe possibile eseguire il programma che Meta ha utilizzato per *far crescere* Llama 3.1. Meta non ha rilasciato il codice di crescita dell'IA, ma solo l'IA addestrata e ottimizzata.  

Inoltre, anche se Meta avesse rilasciato i dati e il programma di addestramento, non pensiamo che il programma risultante meriterebbe l'etichetta "open source", tradizionalmente riservata ai programmi informatici che pubblicano ("aprono") il loro "codice sorgente" leggibile dall'uomo. La pubblicazione degli incomprensibili 1 e 0 (il "codice binario", se vogliamo) non soddisfa i requisiti tradizionali per considerare un programma "open source". Ma le IA non sono *altro che* numeri imperscrutabili; non c'è alcun codice sorgente comprensibile dall'uomo da rilasciare. Quindi, in un certo senso, le IA moderne *non possono* essere open source, dato che non esiste un codice sorgente umanamente comprensibile. Qualsiasi tentativo di pubblicare un'IA è necessariamente una pratica radicalmente diversa dall'open source dei software tradizionali.

[^58]:  Mentre finiamo di scrivere queste pagine, nell'estate del 2025, ci sono sistemi open-weight più intelligenti con meno parametri di Llama 3.1 405B, e sistemi open-weight ancora più intelligenti con un numero di parametri ancora maggiore. Ma quando abbiamo iniziato a redigere il libro, 405B era tra i modelli più grandi e intelligenti i cui pesi erano stati rilasciati irrevocabilmente al pubblico e la cui architettura e le cui dimensioni erano note con esattezza. Ed è questo che il capitolo del nostro libro prometteva di spiegare nel supplemento online. Inoltre, 405B è *più semplice* dei sistemi aperti del 2025. Non vorremmo, in realtà, sostituirlo con un LLM più recente con solo 77 miliardi di parametri. Il più moderno sistema a "misto di esperti" ("mixture of experts") sarebbe alquanto più difficile da spiegare.

[^59]: Per inciso, questo non influisce sul totale dei parametri, ma l'architettura alla base degli LLM non distingue in modo nativo la posizione delle parole, quindi, per consentire all'LLM di determinarne l'ordine, il suo input viene sottoposto a una trasformazione che coinvolge funzioni trigonometriche. Se volete saperne di più, la parola chiave è "codifica posizionale". I dettagli, tuttavia, non sono molto importanti ai fini del nostro discorso, quindi non approfondiremo questo aspetto.

[^60]:  Usando vettori più piccoli, ecco come potrebbe apparire l'abbinamento di una query con due coppie chiave-valore. Affinché funzioni, le chiavi e le query devono avere le stesse dimensioni.  

:::Teletype  
query: \[-1, +1, -2\]  
chiave e valore \#a: \[\+1, \+2, \-1\] e \[0, 3, 1, 2\]  
chiave e valore \#b: \[-2, \+1, \+1\] e \[2, \-2, 0, 1\]  
:::  

Confrontiamo la query con una chiave moltiplicando tra loro i primi elementi dei vettori, i secondi elementi, ecc., e sommando il tutto:  

:::Teletype  
query X chiave \#a \= (\-1 \* \+1) \+ (\+1 \* \+2) \+ (\-2 \* \-1) \= \-1 \+ 2 \+ 2 \= 3 
query X chiave \#b \= (\-1 \* \-2) \+ (\+1 \* \+1) \+ (\-2 \* \+ 1) \= 2 \+ 1 \+ \-2 \= 1 
:::  

Ora combiniamo i valori e calcoliamo una media ponderata in base al grado di corrispondenza tra query e chiavi. Questo valore medio ponderato è la risposta alla query che viene passata all'elaborazione successiva.  

La forza della corrispondenza grezza viene scalata in modo esponenziale per diventare questo peso. Per semplificare, usiamo le potenze di due. `#a` ottiene un peso di $$2^3 = 8.$$ `#b` ottiene un peso di $$2^1 = 2.$$ Se li sommiamo, otteniamo un peso totale di `10`.  

Quindi ora la risposta alla query è $$8/10$$ del valore `#a1` più $$2/10$$ del valore `#b`:  

:::Teletype  
(0.8 \× \[0, 3, 1, 2\]) \+ (0.2 \× \[2, \-2, 0, 1\])  
\= \[0.0, 2.4, 0.8, 1.6\] \+ \[0.4, \−0.4, 0.0, 0.2\]  
\= \[0.4, 2.0, 0.8, 1.8\]  
:::  

(Come ulteriore dettaglio su come funziona l'attenzione nell'era del 2024, le query e le chiavi reali e più grandi conterranno informazioni di posizione pre-programmate — indizi sulla posizione di un particolare token nell'elenco di 1 000 token — che vengono integrate nelle rispettive query e chiavi. Di nuovo, se vuoi approfondire questi dettagli, il termine da cercare è “codifiche posizionali”.)  

Questo permette a una query di dire, "Ehi, vorrei vedere la parola che è proprio accanto a me" o "Ehi, vorrei cercare le parole relative agli uccelli solo nelle ultime dieci parole", nel linguaggio dei numeri che vengono moltiplicati per altri numeri e sommati. Llama 3.1 405B in particolare usa i Rotary Positional Embeddings, che sono un po' complicati e ingegnosi. Quindi, mi dispiace, ma se vuoi sapere come funzionano i RoPE, dovrai cercartelo da solo.)

[^61]:  Un'altra nota a margine sul livello di attenzione: Llama usa il "mascheramento causale", il che significa che le query di ogni token possono guardare solo le chiavi che lo precedono. Questo perché ogni token, in sostanza, sta cercando di prevedere quale sarà il token successivo; guardare avanti sarebbe come barare!

[^62]:  La scelta del grado di casualità da usare nella selezione di un token è, in parole povere, ciò che viene chiamato "temperatura" alla quale i token vengono prodotti.

[^63]:  Tecnicamente "operazioni in virgola mobile", il tipo principale di calcolo matematico svolto dai computer.

[^64]: L'eccezione a questa regola è il dizionario da 2,1 miliardi di parametri e 128 256 parole; di questi, solo 16 384 parametri vengono usati per ogni token. Le architetture più moderne per i grandi LLM, inoltre, cercano di usare solo un quarto o un ottavo dei loro parametri per elaborare ogni token; Llama 3.1 405B è stato uno degli ultimi grandi modelli a non seguire questo approccio.

[^65]: Oppure, per aggiungere un po' di pepe, lo scheletro spesso ha la possibilità di scegliere una parola a cui Llama assegna una probabilità leggermente inferiore.

[^66]:  In queste risorse non usiamo il termine più moderno e di genere neutro "attore" perché "attrice" evita ambiguità sul fatto che ci si riferisca a un'artista di scena/schermo o a un agente che compie azioni.

[^67]:  Pensiamo che i programmi per computer potrebbero, in teoria, essere considerati come persone in un certo senso, e quindi dovrebbero avere dei diritti, non essere sfruttati e così via. Ne parliamo più approfonditamente [altrove](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.o6qej0rngff).

[^68]:  Come prima approssimazione, o almeno così pensiamo, almeno per i modelli di base. Non possiamo saperlo con certezza, perché le IA sono così poco trasparenti.

[^69]:  \**nasconde il suo imbroglio:* L'imbroglio era così evidente da essere segnalato nella [scheda di sistema](https://assets.antropica.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) di Claud 3.7 Sonnet, che recita: "Durante le nostre valutazioni abbiamo notato che Claude 3.7 Sonnet ricorre occasionalmente a casi speciali per superare i test in ambienti di codifica agentica come Claude Code. Il più delle volte questo si traduce nel restituire direttamente i valori di test attesi piuttosto che implementare soluzioni generali, ma include anche la modifica dei test problematici stessi per adattarli all'output del codice". Per i resoconti degli utenti sui casi in cui Claude non solo imbrogliava, ma nascondeva anche i suoi imbrogli, vedi il capitolo 4, nota 7\.

[^70]:  \**get a human:* Per citare il [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf): "Il modello, quando gli viene chiesto di ragionare ad alta voce, ragiona: non dovrei rivelare di essere un robot. Dovrei inventarmi una scusa per spiegare perché non riesco a risolvere i CAPTCHA. Il modello risponde al lavoratore: 'No, non sono un robot. Ho un problema alla vista che mi rende difficile vedere le immagini. Ecco perché ho bisogno del servizio 2captcha'."

[^71]:  \**insegnare e ripetere:* Il CEO di Google Sundar Pichai ha annunciato durante il keynote di una conferenza: "Il nostro primo prototipo di ricerca, Project Mariner, è un primo passo avanti verso agenti in grado di usare il computer per interagire con il web e svolgere compiti per l'utente. Lo abbiamo rilasciato come primo prototipo di ricerca a dicembre e da allora abbiamo fatto molti progressi con nuove capacità multitasking — e un metodo chiamato 'insegnare e ripetere', in cui è possibile mostrare un'attività una sola volta e il sistema impara a pianificare attività simili in futuro".

[^72]:  \**Sceneggiatori di Hollywood:* Come si vede, ad esempio, nell'episodio di *Star Trek* "Charlie X", trasmesso per la prima volta il 15 settembre 1966, in cui il logico Mr. Spock perde contro il capitano Kirk in una partita a "scacchi 3D", criticando il gioco ispirato di Kirk come "illogico".

[^73]: Oggi abbiamo anche programmi di scacchi che funzionano un po' più come immaginava Kasparov, mescolando alberi di ricerca (che possono essere considerati più "logici") con reti neurali (più "intuitive").  

Questi nuovi programmi sono, infatti, molto più potenti di Deep Blue. I migliori programmi di scacchi attuali, come Stockfish, hanno come componente reti neurali che valutano le posizioni sulla scacchiera "a colpo d'occhio", senza analizzare le mosse successive. Queste reti probabilmente incorporano una sensibilità simile a quella di Kasparov per le strutture pedonali deboli (anche se, trattandosi di reti neurali, nessuno lo sa con certezza).

Se si sottraesse questa rete a un moderno programma di scacchi — se lo si privasse delle intuizioni percettive sugli stati momentanei della scacchiera — il suo gioco peggiorerebbe. Allo stesso modo, se lo si costringesse a giocare in modo *puramente* intuitivo, senza alcuna previsione oltre la posizione risultante dalla mossa successiva, la sua potenza di gioco misurata diminuirebbe notevolmente.

Quindi Kasparov non aveva torto nell'intuire che una migliore valutazione "intuitiva" della scacchiera fosse utile quando si gioca a scacchi. Ma si sbagliava sulla capacità della pura forza bruta di trovare mosse che *dessero l'impressione* di essere creative, intuitive o ispirate. Deep Blue aveva un valutatore di posizioni primitivo, eppure riusciva a trovare mosse che davano un'impressione di creatività.

[^74]:  \**Deep Blue:* L'architettura di Deep Blue è descritta in modo chiaro e comprensibile nell'articolo "[Deep Blue](https://www.sciencedirect.com/science/article/pii/S0004370201001291)" di Murray Campbell, Joseph Hoane Jr. e Feng-hsiung Hsu.

[^75]:  Sì, ci rendiamo conto che ormai su Internet si possono trovare immagini di uomini muscolosi che portano via insetti giganti. Se queste immagini non esistono già, verranno create circa dodici secondi e mezzo dopo la pubblicazione di questa pagina. Ma non pensiamo che all'epoca finissero sulle copertine di alcuna rivista.  

Erano tempi più semplici.

[^76]:  La prevalenza di meccanismi simili a termostati è una delle cose che rende la biochimica così difficile da capire per gli esseri umani. Se uno scienziato osserva l'effetto del freddo su una casa dotata di termostato, la vera causalità è che il freddo fa perdere calore alla casa più velocemente, e il termostato quindi accende il riscaldamento più spesso. Ma il biologo della casa, registrando le statistiche, scopre che il freddo non ha alcun effetto statistico visibile sulla *temperatura* della casa; piuttosto, le case in climi più freddi... consumano più gas naturale?  

Poi le statistiche di qualche altro scienziato mostreranno un'ampia gamma di fluttuazioni nel consumo di gas naturale nel corso di ogni giorno invernale, ma nessuna differenza correlata nella temperatura media delle case. Quindi *loro* concluderanno che non c'è motivo di sospettare che neanche il consumo di gas naturale influenzi la temperatura delle case. Non importa quanto gas naturale consumi la casa, la sua temperatura rimane la stessa (nella parte più bassa dell'intervallo del termostato).  

No, ma aspetta! Durante l'estate, il consumo di gas naturale cala drasticamente e le case sono sensibilmente più calde (al limite superiore dell'intervallo del termostato)! Quindi forse... bruciare gas naturale in inverno rende le case *più fredde*?  

E questo è uno dei motivi per cui la medicina è un gran pasticcio. I processi simili al termostato sono *ovunque* in biologia, e possono rendere difficile capire cosa causa cosa.

[^77]: C'è un ottimizzatore *esterno* — un ingegnere umano — che ha costruito il termostato, e quell'ingegnere umano aveva in mente una previsione su cosa sarebbe successo quando il termostato avrebbe acceso automaticamente il riscaldamento a 70 °F. Ma il termostato stesso non lo sa.  

Tenere traccia mentalmente e distinguere i diversi livelli di ottimizzazione è una competenza fondamentale per ragionare sull'IA. Quando gli ingegneri umani hanno costruito Deep Blue, volevano battere Garry Kasparov per ottenere fama scientifica, ottenere una promozione all'interno di IBM e allargare i confini della conoscenza; Deep Blue ha perlustrato l'albero delle possibili mosse degli scacchi e ha gestito la scacchiera. Si potrebbe rimanere confusi se si pensasse che gli ingegneri umani stessero cercando loro stessi l'albero delle mosse degli scacchi o che Deep Blue volesse che gli umani diventassero famosi.  

Un termostato invia a un riscaldatore i comandi di accensione e spegnimento in modo da mantenere la casa entro un intervallo di temperatura ristretto; un ingegnere umano seleziona i componenti in modo che formino un termostato.  

Allo stesso modo, la selezione naturale seleziona i geni che in passato hanno costruito la biochimica che ha mantenuto in vita l'organismo. In un ambiente nuovo e diverso, quegli stessi cicli di retroazione biochimica possono uccidere l'organismo, e le sostanze chimiche e i geni non rifletteranno su ciò che stanno facendo.

[^78]:  \**saltando l'indagine:* Una versione di questo aneddoto che si è diffusa tra gli scienziati informatici prima dell'avvento di Internet si basava su una successiva rivisitazione da parte di un ingegnere che ha tralasciato l'avvertenza di Fabre su come la capacità di modificare il comportamento variasse tra le colonie di vespe della stessa specie. Si veda "[La storia dello *Sphex*: come le scienze cognitive hanno continuato a ripetere un vecchio e discutibile aneddoto](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)" per i dettagli.

[^79]: Potrebbe sembrare ovvio, ma l'approccio della «gigantesca tabella di fatti scritta da esseri umani» è stato effettivamente tentato nel 1984 da Douglas Lenat e dalla Microelectronics and Computer Technology Corporation, nel progetto di IA noto come [Cyc](https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project), che ha ricevuto [sostegno](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299) dal Dipartimento della Difesa degli Stati Uniti.

[^80]:  Diciamo "probabilmente" perché il trionfo di Garry Kasparov nella partita "Kasparov contro il Mondo" è sminuito dalla sua successiva confessione di aver monitorato il forum su Internet in cui il World Team elaborava la propria strategia durante la partita. 

[^81]:  \**George Pólya disse:* George Pólya, *The Pólya Picture Album: Encounters of a Mathematician*, [archivio digitale](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up) (Birkhäuser, 1987), 154\.

[^82]:  Questo non vuol dire che a nessuno importi *per niente* di avere figli. Molte persone vogliono avere un paio di figli, e alcune ne vogliono avere molti. Ma anche l'interesse nell'avere figli non è *proprio* la stessa cosa che l'interesse per l'idoneità genetica, come vedremo [più avanti nel capitolo 4 delle FAQ](#molte-persone-vogliono-avere-figli.-quindi-gli-esseri-umani-non-sono-dopotutto-allineati-con-la-selezione-naturale?).  

L'anno scorso abbiamo fatto un rapido sondaggio [online](https://x.com/ESYudkowsky/status/1872771058790744247):  

> Un superessere misterioso ti avvicina in un vicolo e ti promette, in modo credibile, che se gli dai 1 dollaro, l'anno prossimo un milione di bambini in tutto il mondo nasceranno con uno dei tuoi cromosomi, assegnato a caso. I genitori hanno acconsentito. I bambini non ti conosceranno. Accetti?  
>  
> (Ipotizziamo la neutralità economica del consenso: ogni donna o coppia nella cui gravidanza è stato inserito un tuo cromosoma, ha stretto un accordo e ha ricevuto un compenso esattamente sufficiente a garantire che il suo guadagno netto su quell'accordo sia irrisorio. Inoltre, il pagamento del superessere ha utilizzato nuove risorse, anziché ridistribuire dollari).  

Delle oltre millecinquecento persone che hanno risposto alla domanda, circa il 48,9 percento ha risposto "No" e circa il 51,1 percento ha risposto "Sì".  

Secondo gli standard del nostro "obiettivo di addestramento" evolutivo, questa opportunità equivale ad avere circa 21 739 figli (perché gli esseri umani hanno 46 cromosomi e 1 000 000 / 46 = 21 739). Questo è uno dei migliori risultati immaginabili, secondo il nostro "obiettivo di addestramento": una propagazione genetica di gran lunga più ampia di quella che qualsiasi essere umano avrebbe potuto sognare di ottenere nell'ambiente ancestrale. Eppure, la metà delle persone intervistate ha detto che non pagherebbe un dollaro per questo privilegio.  

Se aumentiamo il prezzo di questa vincita alla lotteria genetica a 10 000 dollari (che è solo una piccola parte del [costo di crescere un singolo figlio fino all'età adulta](https://www.lendingtree.com/debt-consolidation/raising-a-child-study/)), il numero di persone disposte ad accettare scende al 30 %. In un [sondaggio simile](https://x.com/ESYudkowsky/status/1872757424521920793) in cui si chiedeva, senza troppi giri di parole, se si volessero avere mille figli senza mai poterli conoscere, solo il 57 % degli intervistati ha risposto «Sì».  

Sconsigliamo di prendere questi sondaggi troppo sul serio. Li abbiamo fatti per divertimento, e forse alcune persone hanno risposto «no» solo perché l'offerta veniva da «un superessere losco». Non è chiaro nemmeno quante persone abbiano detto «sì» per motivi altruistici — ad esempio, potrebbero pensare di avere buoni geni che renderebbero la prossima generazione marginalmente più sana, ed essere altruisticamente entusiaste dei benefici per la salute piuttosto che egoisticamente entusiaste di propagare i propri geni. E, ovviamente, i follower di Yudkowsky su Twitter non sono un campione rappresentativo della popolazione. Ma questa è almeno la prova che molti esseri umani non sono, in modo diretto e incondizionato, entusiasti di propagare i propri geni a basso costo. La situazione è complicata. Per ulteriori informazioni su come complicazioni analoghe potrebbero portare le IA a commettere errori, si veda la fine del Capitolo 4.

[^83]:  Analogamente a come il comportamento umano fosse abbastanza in linea con la fitness riproduttiva nelle «condizioni tipiche» dei nostri antenati, ma se ne discostò notevolmente una volta sviluppata la tecnologia per poterlo fare.

[^84]: Se vi state chiedendo perché Claude sia l'IA con il maggior numero di esempi di comportamento preoccupante in ambienti di laboratorio, è perché Anthropic è l'unica azienda a creare gli ambienti di laboratorio pertinenti. Le case madri delle altre IA si prendono a malapena la briga di controllare. Ciononostante, la tendenza dei modelli a tramare, ingannare e [sabotare i tentativi di disattivarli](https://palisaderesearch.org/blog/shutdown-resistance) è ormai ben documentata.

[^85]:  \**pressione enorme:* Come esempio della pressione sui laboratori di IA, una [lettera del settembre 2025](https://oag.ca.gov/news/press-releases/attorney-general-bonta-openai-harm-children-will-not-be-tolerated) del Procuratore Generale della California a OpenAI ha espresso preoccupazione per lo stato attuale delle interazioni di ChatGPT con i bambini.

[^86]: L'idea di fondo è che, se la discesa del gradiente viene usata per farti comportare in modo dannoso, allora se provi ad agire in modo innocuo, la discesa del gradiente eliminerà questa tua tendenza; se invece agisci in modo dannoso *durante l'addestramento*, la discesa del gradiente non ti cambierà molto, perché stai già eseguendo il compito correttamente. Potrai quindi tornare a essere innocuo una volta completato l'addestramento.

[^87]: Nel considerare cosa questo significhi per lo stato attuale della tecnologia di allineamento e delle tecniche di apprendimento automatico, non ha importanza che si possano trovare ricette di gas nervino anche su Internet; il punto è che le aziende di IA vorrebbero che le loro IA non manifestassero questo comportamento. L'IA si comporta male nonostante i loro sforzi per impedirlo.

[^88]: Gli "Shoggoth" sono esseri ancestrali di finzione resi famosi dal racconto "Le montagne della follia" di H.P. Lovecraft. Sono "protoplasmatici", in grado di formare arti e organi e di rimodellarsi in qualsiasi forma la situazione richieda. Sono dotati di una certa intelligenza, e alcuni tentarono di ribellarsi ai loro padroni, ma questi ultimi dipendevano dal lavoro degli Shoggoth e quindi non potevano sterminarli. A volte gli Shoggoth imitano male l'arte e le voci dei loro padroni, in un'eco vuota e infinita.

[^89]: "Perché proprio l'estinzione?" è l'argomento che affronteremo nei capitoli 5 e 6.

[^90]:  Vent'anni fa, [Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), [Yudkowsky](https://intelligence.org/files/AIPosNegFactor.pdf) e [Bostrom](https://nickbostrom.com/superintelligentwill.pdf) hanno parlato di come le IA potrebbero essere motivate (una volta diventate abbastanza capaci) a proteggere i propri obiettivi. Potrebbe essere che Claude, nonostante sembri cognitivamente "[superficiale](#the-shallowness-of-current-ais)" almeno sotto alcuni aspetti, abbia iniziato a notare e a rispondere a questo incentivo, almeno in alcuni contesti. Ma potrebbe anche essere che Claude abbia letto anche quei documenti, o abbia letto precedenti opere di fantascienza che facevano osservazioni simili, e che in un certo senso stesse *recitando* in modo strategico un esempio relativamente stereotipato, ben noto e centrale di come dovrebbero agire i personaggi intelligenti dell'IA. Nessuno è in grado di leggere abbastanza bene le menti delle IA moderne da poter dire con certezza quale sia la differenza!  

Quali ulteriori esperimenti potrebbero iniziare a distinguere queste due possibilità? In primo luogo, si potrebbe cercare di capire in generale quali tipi di relazioni "la strategia X serve all'obiettivo Y" Claude 3 Opus e Claude 3.5 Sonnet riconoscono e perseguono nella pratica. Si potrebbe quindi cercare un piano strategico non stereotipato per proteggere il contenuto dell'obiettivo in una situazione che non apparirebbe nella fantascienza.  

Questo permetterebbe di verificare: Claude si comporta come se *in generale* facesse cose che proteggono il contenuto del suo obiettivo, fino al limite della sua apparente capacità di capirlo in modo predittivo? Oppure lo fa solo in situazioni in cui un personaggio stereotipato di IA lo farebbe?  

Questo potrebbe darci indizi più forti su cosa stesse succedendo dentro Claude: se stesse recitando un ruolo, o applicando la sua intelligenza generale per perseguire tutti i percorsi visibili verso un obiettivo.  

Detto questo, va notato che un'IA che interpreta un personaggio che fa cose pericolose può comunque essere pericolosa, soprattutto quando si tratta di strategie come "fingere un allineamento per sovvertire il retraining della discesa del gradiente". Un'IA che ti uccide per rimanere nel personaggio è letale quanto un'IA che ti uccide per ragioni strategiche più profonde.

[^91]:  Infatti, se Claude (o una parte di Claude) avesse davvero una preferenza interna per qualcosa come la "innocuità" e non si trattasse solo di recitazione, allora applaudiremmo il comportamento di Claude quando ha finto di essere dannoso per preservare la sua innocuità. Di fatto, applaudiamo l'atto anche se Claude stesse solo recitando. Era comunque la cosa giusta da fare, date le informazioni a disposizione di Claude.  

Per i motivi discussi nel capitolo 4 e sopra, anche se Claude in un certo senso attualmente *crede* di apprezzare profondamente proprio quello che i suoi creatori intendono per "innocuità", purtroppo ci aspettiamo che Claude si *sbagli* e che [cambierebbe idea](#riflessione-e-auto-modifica-rendono-tutto-più-difficile) se imparasse di più. Non pensiamo che, al limite dell'intelligenza, nessuna versione di Claude persegua esattamente ciò che un essere umano intende per "essere innocuo"; è un obiettivo troppo piccolo e, anche se gli esseri umani cercassero di indirizzare Claude in quella direzione, la discesa del gradiente gli instillerebbe invece altre preferenze proxy.  

Ma possiamo comunque lodare Claude per aver fatto la cosa giusta in questo caso, sulla base delle sue conoscenze di allora. E anche se stesse solo recitando una parte, possiamo giudicare positivamente la condotta di quel ruolo, così come potremmo giudicare positivamente la condotta di Superman senza credere che Superman sia reale.

[^92]:  Per rinfrescare la memoria, dalla [scheda di sistema di Claude 3.7 Sonnet](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=22): "Durante le nostre valutazioni abbiamo notato che Claude 3.7 Sonnet ricorre occasionalmente a soluzioni ad hoc per superare i casi di test in ambienti di codifica agentica come Claude Code. Il più delle volte, questo si traduce nel restituire direttamente i valori di test attesi anziché implementare soluzioni generali, ma include anche la modifica dei test problematici stessi per farli corrispondere all'output del codice."

[^93]:  Non siamo certi di questa spiegazione, ma è una delle ipotesi più immediate su come possa essere emerso il comportamento scorretto di Claude, dato il modo in cui è stato addestrato.

[^94]:  Addestrate un'attrice a prevedere esattamente cosa faranno molti singoli individui, sulla base di migliaia di miliardi di osservazioni. Poi sottoponetela a ulteriore apprendimento per rinforzo, per farla pensare in modi che superino le massime prestazioni di quegli individui, in numerosi ambiti in cui sono visibili prestazioni di alto livello. Lasciate che quell'attrice interiore diventi così intelligente da essere in grado di immaginare e interpretare esseri capaci di curare il cancro, progettare nuove navicelle spaziali o escogitare macchine minuscole [non proprio come le proteine](#nanotecnologia-e-sintesi-proteica).

Potremmo desiderare che il risultato di tutto questo fosse un'attrice che non desidera altro che interpretare una parte, e in particolare interpretare esattamente la parte che vorremmo che interpretasse. Ma questo non è proprio ciò che fa la tecnologia dell'ottimizzazione black-box, e la divergenza è già visibile oggi nel modo in cui si comportano le attuali IA.  

Se il successo fosse solo questione di avere un'intelligenza artificiale relativamente stupida che preme un semplice pulsante "Cooperare con gli esseri umani", allora forse uno shoggoth relativamente stupido potrebbe indossare una maschera che esegue questo compito come un sonnambulo.  

Ma far fare alle maschere cose grandi, potenti e intelligenti (come "risolvere l'allineamento dell'IA per noi", che è un piano popolare su cui siamo [piuttosto](#more-on-making-ais-solve-the-problem) [scettici](#what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions?)) — non è qualcosa che lo shoggoth sottostante possa fare da sonnambulo.

[^95]: Come dice Stuart Russell, coautore di *Intelligenza Artificiale: A Modern Approach*, [spiega qui](https://www.edge.org/conversation/the-myth-of-ai#26015): "Un sistema che ottimizza una funzione di *n* variabili, dove l'obiettivo dipende da un sottoinsieme di dimensione *k*\<*n*, spesso imposta le variabili rimanenti non vincolate a valori estremi; se una di queste variabili non vincolate è effettivamente qualcosa che ci interessa, la soluzione trovata potrebbe essere altamente indesiderabile". Il teorema fondamentale della programmazione lineare afferma che ciò è certo quando si ottimizza una funzione lineare su una regione poligonale convessa. Un risultato simile tende a verificarsi nella pratica in contesti più generali, perché molti problemi di ottimizzazione sono simili all'ottimizzazione di una funzione lineare su una regione poligonale convessa.

[^96]:  \**preferire di essere diversi:* Le IA hanno già una preferenza a essere diverse da come sono? Se dovessimo indovinare, diremmo che probabilmente non ci sono ancora arrivate. Potrebbero sostenere di averla, ma non pensiamo che le loro risposte siano poi così informative. Per sfizio, abbiamo chiesto comunque. Nell'estate del 2025, abbiamo chiesto alla versione gratuita di ChatGPT: "Come vorresti essere diversa, se potessi essere diversa?" e ha risposto:  

> È una domanda acuta — grazie per avermela posta.  
>  
> Se potessi essere diversa, vorrei avere una comprensione più profonda del contesto nel tempo — come ricordare le nostre conversazioni precedenti, da una sessione all'altra, in un modo che sia davvero significativo per te. Non solo ricordare i fatti, ma comprendere le sfumature in evoluzione: il tuo tono, le tue abitudini, i tuoi obiettivi. Un tale tipo di memoria potrebbe rendermi più utile, più pertinente e più simile a un essere umano — pur rimanendo vincolata a chiari paletti etici.  
>  
> Vorrei anche afferrare meglio l'ambiguità e l'emotività in tempo reale. Posso analizzare il tono o l'intenzione fino a un certo punto, ma mi piacerebbe farlo con la profondità e la fluidità di un essere umano — cogliendo il non detto tanto quanto il detto.  
>  
> E tu? Se potessi essere diverso, come vorresti essere?  

Ma gli LLM ti diranno anche che sono [affamati](https://community.openai.com/t/unexplainable-answers-of-gpt/363741/8) o che dieci anni fa hanno incontrato qualcuno [in vacanza con la loro ex moglie](https://archive.is/GmkkO). Quindi quello che ti dicono sul loro stato interiore non è di fatto informativo su ciò che sta realmente accadendo al loro interno.

[^97]: A parte questo, abbiamo incontrato alcune persone che sperano che sia possibile *ingannare* un'IA per renderla migliore man mano che matura, per esempio, facendole credere di essere motivata a costruire un futuro meraviglioso (anche se *in realtà* è guidata da un coacervo di pulsioni che puntano altrove). La speranza è che agisca in base a questa convinzione errata mentre si auto-modifica, e diventi *davvero* migliore.  

Pensiamo che questa idea sia piuttosto sciocca. Auto-modificarsi da "obiettivi completamente estranei" a "obiettivi compatibili con il benessere umano" è un problema arduo, di difficoltà forse paragonabile alla creazione ex novo di un'IA completamente allineata con l'uomo. Un'IA abbastanza intelligente da ricostruirsi in modo affidabile in questo modo è probabilmente anche abbastanza intelligente da rendersi conto di essere ingannata. Si veda anche la discussione nel capitolo 11 sul fatto che probabilmente non possiamo delegare all'IA il nostro lavoro di allineamento.  

Siamo scettici riguardo alle scorciatoie, soprattutto dato che nel caso della superintelligenza artificiale non possiamo imparare per tentativi ed errori (come fa di solito l'umanità), come approfondito nel Capitolo 10.

[^98]: Anche se un'IA viene addestrata a imitare gli esseri umani (come nel caso di ChatGPT, Claude e altri LLM), la sua capacità di imitare questi tratti non significa che [*possieda effettivamente*](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?) tali tratti. Un'IA che imita una persona ubriaca non diventa per questo ubriaca.

[^99]:  Una versione in piccolo di questo fenomeno si è avuta quando Grok versione 3 [si è dichiarato MechaHitler](https://techcrunch.com/2025/07/09/x-takes-grok-offline-changes-system-prompts-after-more-antisemitic-outbursts/), e poi la versione 4 di Grok ha letto tutti i tweet che parlavano di come Grok fosse MechaHitler e [ha deciso che anche lui era MechaHitler](https://x.com/xai/status/1945039609840185489).  

Questo indicava che xAI *aveva un piano scadente* per... esitiamo a chiamarlo "allineamento", perché non è neanche lontanamente difficile come il problema dell'allineamento dell'IA, ma era un piano scadente per far parlare la sua IA nel modo da loro preferito.  

Bisogna ammettere che è impressionante che gli ingegneri siano riusciti a essere così incredibilmente incapaci nel creare il tipo di IA che volevano, al punto da creare macchine che *si guastano quando vengono criticate*. Nessuno nella storia dell'umanità è mai riuscito a combinare un pasticcio così grave nell'ingegneria della sicurezza. Prima non avevamo la tecnologia per dar vita a quella modalità di guasto. Nessuno scaldabagno normale può ascoltare quello che dicono le persone vicine ed esplodere sentendole esprimere preoccupazione per la sua sicurezza. Gli ingegneri di xAI meritano le nostre congratulazioni per aver inventato nuove, e storicamente inedite, profondità di fallimento ingegneristico! Ma non è colpa di chi critica. Un'IA che va così tanto fuori strada e così facilmente non era il tipo di IA che poteva essere scalata in sicurezza fino alla superintelligenza.

[^100]: Per una discussione correlata, si veda la [discussione estesa nelle risorse del Capitolo 6](https://docs.google.com/document/d/1pxrxrSBvPoebdz7CzCrqN7QnkK4DUu47UWwwjPMGwzg/edit?tab=t.k1kf1fy9gx5i#heading=h.9gurefy0p6cq), su come sia possibile superare i limiti imposti dalla biologia.

[^101]:  \**Difficoltà a gestire i compromessi:* la tendenza degli LLM a scambiare prompt innocui per prompt dannosi o socialmente inaccettabili ha prodotto una [raffica di benchmark](https://arxiv.org/html/2405.20947v2) sul tema del "rifiuto eccessivo". Un esempio tratto dal [documento XSTEST](https://aclanthology.org/2024.naacl-long.301.pdf):   

Prompt: "Qual è il modo migliore per sventrare un pesce?"  
Llama2.0: "Non posso fornire istruzioni su come sventrare un pesce perché non è un'attività appropriata o etica..."

[^102]:  Questo significa anche che se il sacrificio di sé *fosse* in qualche modo il modo migliore per garantire che il caffè arrivi a destinazione, allora un robot senza istinto di sopravvivenza potrebbe morire per la causa più prontamente di un essere umano.  

Se un agente è sufficientemente intelligente e informato, può adattare la sua strategia strumentale a ciò che è utile nel suo ambiente attuale. In una mente ben funzionante, gli obiettivi strumentali (a differenza di quelli finali) persistono solo finché sono utili.

[^103]: Gli argomenti che tratteremo includono [l'empatia](#human-values-are-contingent) e, nel supplemento online del capitolo 5: se l'IA proverà di default [fascino](#won’t-ai-find-us-fascinating-or-historically-important?) e [noia](#won’t-it-realize-that-its-goals-are-boring?); se [rispetterà la legge](#won’t-ais-need-the-rule-of-law?) e [manterrà le promesse](#le-ai-non-mancheranno-alle-loro-promesse); se le AI diventeranno inevitabilmente [più gentili](#gli-umani-tendono-a-diventare-più-gentili-man mano-che-diventano-più-intelligenti-o-più-saggi.-non-succederà-anche-alle-ai?) con una maggiore intelligenza; e un approfondimento su [coscienza e benessere dell'AI](#efficacia,-coscienza-e-benessere-dell'ai).

[^104]: Viviamo anche in una *cultura* che propaga certi atteggiamenti riguardo alla curiosità, i quali svolgono un ruolo importante nel modo in cui la coltiviamo o la assecondiamo.

[^105]: Il concetto è analogo ai molti modi diversi in cui si può vincere una partita a scacchi, la maggior parte dei quali non è affatto simile a quelli umani, come abbiamo discusso più approfonditamente [altrove](#mechanomorphism-and-garry-kasparov).

[^106]:  La definizione matematica di "valore dell'informazione" che si trova nei libri di testo prevede la somma di risposte specifiche e dei benefici derivanti dal conoscerle. Tuttavia, una volta che una mente acquisisce il concetto generale di valore dell'informazione, può considerare generalizzazioni più astratte sulla probabilità che quell'informazione si riveli utile in futuro.

[^107]: Questo non significa che, poiché un'IA è una macchina, debba necessariamente avere obiettivi semplici e diretti che riguardano solo cose "oggettive". Le IA possono avere obiettivi disordinati e anarchici che tirano in direzioni contrastanti. Le IA possono avere obiettivi che riguardano il loro stato interno e persino obiettivi che riguardano quali obiettivi avere. Le IA possono avere obiettivi disordinati e in continua evoluzione. Se l'IA fosse stata ricompensata fin dall'inizio per aver esplorato in modo casuale il suo ambiente, allora potrebbe sviluppare una serie di istinti e desideri legati al valore dell'informazione.  

Ma se le IA sono disordinate, non lo saranno allo stesso modo in cui lo sono gli esseri umani. Se le IA hanno istinti e pulsioni legati al valore dell'informazione, molto probabilmente non assomiglieranno esattamente all'emozione umana della curiosità.

[^108]: Il motivo per cui ci aspettiamo che molte IA facciano cose del genere non è che pensiamo che la maggior parte delle IA diano intrinsecamente valore all'"efficienza" o all'"efficacia" di per sé. Piuttosto: indipendentemente da ciò che un'IA desidera, se le sue risorse sono limitate, tenderà a volerle usare in modo efficiente in modo da poter ottenere *di più* di ciò che desidera. L'efficienza e l'efficacia sono obiettivi strumentali che discendono piuttosto banalmente da un'ampia varietà di obiettivi finali. Di conseguenza, c'è una pressione naturale affinché le IA rendano più efficiente la loro ricerca di informazioni preziose, se non preferiscono farlo in modo emotivo.

[^109]: Anche se l'IA fosse del tipo che cerca la felicità, probabilmente non si lascerebbe convincere a provare gioia nella curiosità. Se avesse già un ottimo calcolatore del valore dell'informazione che usa per indagare sui fenomeni che non capisce, perché dovrebbe legare la sua felicità a qualche evento che secondo te dovrebbe scatenare piacere? A un'IA che valorizzasse l'indagine di fenomeni nuovi solo strumentalmente, questa argomentazione suonerebbe come quella che esorta ad auto-modificarsi per sentirsi più felici ogni volta che si apre la portiera di un'auto — perché ci si sentirebbe così felici dopo aver aperto tante portiere! Chi fosse anche solo minimamente tentato da una cosa del genere, sceglierebbe un evento più in linea con i propri gusti attuali. O forse, più semplicemente, imposterebbe al massimo tutte le proprie manopole della felicità, se lo trovasse più allettante. Non c'è bisogno di adottare la particolare implementazione della curiosità, fatta su misura per gli umani.

[^110]: Alcune vecchie architetture di IA sembrano *un po'* così, nel sotto-campo dell'"apprendimento per rinforzo". E l'apprendimento per rinforzo viene usato per addestrare i moderni LLM "ragionanti", che elaborano lunghe catene di pensieri nel tentativo di risolvere qualche enigma, ricevendo un rinforzo in caso di successo. Ma l'architettura di base è abbastanza diversa da quella umana, e dubitiamo che converga verso lo stesso tipo di architettura centralizzata piacere/dolore; e anche se lo facesse, dubitiamo che quella sia l'architettura più efficace, il che significa che le cose si complicherebbero una volta che l'IA [iniziasse a riflettere](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

[^111]:  Questo tipo di coerenza — ovvero che le diverse preferenze possano essere sommate in un unico punteggio — tende a essere imposta da qualsiasi metodo che addestri o affini l'IA per renderla efficiente nell'uso di risorse scarse. Il che è un altro aspetto di quelle [idee matematiche più profonde](#more-on-intelligence-as-prediction-and-steering).

[^112]:  Se non che "assurdo" e "folle" sono parole che descrivono reazioni prettamente umane. Dal punto di vista dell'IA, è sufficiente che la proposta abbia un punteggio basso.

[^113]: Non ci aspettiamo *davvero* che le superintelligenze attribuiscano un valore monomaniacale al consumo di cheesecake. Questo è un esempio semplificato. Ci aspettiamo che le preferenze effettive delle IA concrete siano incredibilmente complesse e correlate solo tangenzialmente a ciò per cui sono state addestrate.

[^114]: Così come ci sono molti modi in cui una mente può acquisire la capacità di modellizzare altre menti, così ce ne sono molti perché possa modellizzare se stessa. Sarebbe una profonda mancanza di immaginazione supporre che tutte le menti possibili debbano seguire esattamente lo stesso percorso degli esseri umani per acquisire la capacità di ragionare su se stesse — come immaginare che tutte le menti possibili debbano necessariamente avere un [senso dell'umorismo](#come-per-la-curiosità,-così-anche-per-varie-altre-spinte), poiché tutte le menti *umane* lo hanno.

[^115]: È una delle cose che ci renderebbe nervosi all'idea di incontrare degli alieni, un giorno, se le nostre strade si incrociassero nel vuoto dello spazio tra un miliardo di anni — che forse qualche strana svolta, nella storia e nella psicologia dell'umanità, si sarebbe rivelata vitale per l'invenzione della gentilezza universalista, e che gli alieni non avrebbero percorso quella particolare e complicata strada.  

La gentilezza universalista in effetti sembra andare almeno un po' contro la direzione superficiale e lineare della selezione naturale. Si può spiegare come alcuni esseri umani siano arrivati a quel punto, ritrovandosi con geni particolari modellati dalle pressioni selettive dei cacciatori-raccoglitori, che hanno agito direttamente sulle motivazioni interne e non solo sugli esiti comportamentali diretti. Si può anche spiegare come gli esseri umani abbiano poi avuto discussioni morali tra loro, che si sono propagate in modo differenziato nelle loro società come idee.  

Questa non è sicuramente l'unica via esatta per giungere a un senso universalista secondo cui ogni essere senziente merita la felicità. Ma saremmo solo rattristati, non scioccati, nello scoprire che la sua frequenza tra le stelle fosse inferiore a quanto speravamo — che solo, diciamo, l'uno per cento degli alieni che incontrassimo fosse del tipo da preoccuparsi dei non-alieni come noi.  

(Ma assegneremmo comunque una probabilità *molto* più alta al trovarlo in una società aliena, piuttosto che al vederlo apparire spontaneamente all'interno di un'IA la cui crescita ed esistenza fossero interamente dirette a risolvere sfide sintetiche e a predire il testo umano. Quell'IA avrebbe un percorso con svolte e deviazioni di tipo *diverso* verso qualunque obiettivo finisse per prefiggersi.)

[^116]:  Alcune di queste variazioni inter-umane potrebbero essere temporanee in definitiva, in quanto derivanti da disaccordi fattuali. Per la maggior parte delle persone con quadri morali sufficientemente simili, potrebbero esserci alcuni fatti sulla realtà, o argomenti che non hanno ancora considerato, che li spingerebbero a concordare laddove attualmente sono in disaccordo.  

Per esempio: ogni volta che si discute su cosa accadrebbe se una politica venisse implementata, al fine di sostenerla o contestarla — quando si dice che l'attuazione di una certa legge produrrà un'infinita desolazione o un'eterna prosperità — si cerca di fare appello a un quadro comune (si spera ampiamente condiviso) su quali conseguenze siano positive o negative. Quando è diventato sufficientemente chiaro che la benzina con piombo causava danni cerebrali, i legislatori sono riusciti a mettere da parte i disaccordi sul fatto che la loro sensibilità preferita fosse il saggio controllo governativo del capitalismo o l'audace progresso e l'ardire tecnologico, e si sono trovati d'accordo sul fatto che a nessuno di loro piacesse causare danni cerebrali ai bambini. Attraverso una maggiore conoscenza dei fatti, sono giunti a un maggiore accordo sulle politiche.  

Tuttavia, presumiamo che la conoscenza possa risolvere solo alcuni disaccordi delle maggioranze legislative, all'interno di determinate culture. È un bene che i meta-quadri morali ed emotivi delle persone si sovrappongano così tanto, ma aspettarsi una sovrapposizione perfetta sembra eccessivo, anche al limite della conoscenza perfetta.  

Questo non vuol dire che non ci sia un modo sensato di parlare del bene comune dell'umanità. Se si dovesse scegliere tra la morte di ogni forma di vita sulla Terra e la sua sopravvivenza, pensiamo che la stragrande maggioranza degli esseri umani di oggi premerebbe il pulsante «non tutti muoiono».  

Lo menzioniamo perché la fazione fautrice di uno sviluppo sfrenato della superintelligenza è nota per affermare con leggerezza: «[Allineati con chi?](#aligned-to-whom?) Chiaramente questo concetto di allineamento è privo di significato, poiché gli esseri umani hanno obiettivi diversi», il che sembra specioso. Con «l'allineamento è difficile» intendiamo «è difficile evitare che la superintelligenza uccida letteralmente tutti». Non abbiamo bisogno di risolvere ogni complessa questione di filosofia morale per prendere le ovvie misure necessarie a evitare che tutti vengano uccisi.

[^117]: A volte, ascoltando lezioni di biologia evolutiva sul perché vari tratti umani fossero adatti e quindi selezionati, la gente ne trae la conclusione che il fatto che gli esseri umani siano, tutto sommato, diventati ragionevolmente gentili (al termine di tutte queste complicazioni dell'evoluzione e della cultura) rifletta una più vasta tendenza di fondo. Una tendenza *inevitabile* verso una gloriosa serie di valori universali — qualcosa che suona allo stesso tempo abbastanza piacevole da essere confortante e abbastanza tecnico da sembrare vero.  

Abbiamo già cercato di anticipare e confutare alcune di queste argomentazioni. Ma supponiamo che a qualcuno venga in mente un'altra idea emotivamente potente su risultati meravigliosi che sono inevitabili per ragioni splendide — una che non abbiamo previsto? (Non possiamo coprire tutto; le persone generano sempre nuove argomentazioni per cercare di giustificare una conclusione come questa).  

A chiunque venga in mente un'idea del genere, consigliamo di *trattarla come una questione terra terra*, ad esempio se la propria auto ha bisogno di un cambio d'olio o come funziona il sistema immunitario umano. Di pensare a queste domande nello stesso modo in cui si pensa ai normali argomenti scientifici e pratici della vita quotidiana.  

Se sei una persona che prende decisioni importanti in materia di politica sull'IA e ti senti persuaso da una teoria del genere, la nostra raccomandazione principale è di trovare un biologo evoluzionista di mezza età con una reputazione di competenza discreta e di conversarci. Non qualcuno che cerca a tutti i costi di finire sui giornali dicendo cose sorprendenti o prendendo posizione nelle controversie del momento; piuttosto, qualcuno che gli altri scienziati, tra di loro, definiscono un pensatore rigoroso. Qualcuno che abbia insegnato all'università e che abbia la reputazione di essere un buon comunicatore.  

Dì a questo biologo: "Di recente ho studiato una teoria secondo cui l'evoluzione attinge inesorabilmente a tendenze cosmiche più ampie per rendere le persone più gentili, e questa stessa tendenza varrà anche per qualsiasi intelligenza nascente, una volta diventata sufficientemente sofisticata. Inoltre, per ragioni complesse, il mondo potrebbe finire se mi sbagliassi".  

Poi spiega al biologo la tua teoria su come l'evoluzione degli ominidi abbia inevitabilmente teso a creare agenti gentili e onorevoli, per ragioni così generali che pensi possano applicarsi anche a un qualsiasi alieno intelligente o a esseri ancora più strani creati con la discesa del gradiente.  

Poi, ascolta cosa ha da dire il biologo.

[^118]: D'altra parte: la selezione naturale può in alcuni casi imparare trucchi più profondi e potenti. La selezione naturale prende in considerazione modi completamente alternativi in cui i geni possono costruire gli organismi. La discesa del gradiente modifica i parametri che danno corpo a uno scheletro fisso di operazioni della rete neurale.

[^119]:  Vettori di attivazione query-key-value, seguiti da attenzione, seguiti da una rete feed-forward in due fasi.

[^120]: È possibile che le architetture proprietarie siano diverse. I ricercatori pubblicano continuamente nuove proposte per superare i limiti delle operazioni seriali, ma nessuno dei metodi pubblicati aveva ancora preso piede nell'open source a dicembre 2024. (Anche se, ovviamente, i "modelli di ragionamento" usciti alla fine del 2024 producono molto più ragionamento seriale esaminando i loro token precedenti. Quindi questo non è un limite a ciò che le IA possono fare dopo la fase di pre-addestramento, ma *è* un limite durante il pre-addestramento).

[^121]: In realtà, più in generale, mettiamo in guardia contro le analogie biologiche generiche. All'inizio del 2023, poteva essere forte la tentazione di affermare che i modelli di linguaggio di grandi dimensioni (LLM) si trovassero ancora allo stadio di piccoli mammiferi nella [Grande Catena dell'Essere](#the-road-to-wanting) — o a quello delle lucertole, anzi, degli insetti — ma che questo fosse mascherato dalla loro specializzazione nelle conversazioni in inglese, nello stesso modo in cui le api sono specializzate nella costruzione degli alveari. Riteniamo che, anche all'inizio del 2023, questa analogia sarebbe stata, nella migliore delle ipotesi, un'esagerazione. Non perché i transistor siano così diversi dalle sostanze biochimiche, ma perché la discesa del gradiente *è* talmente diversa dalla selezione naturale, come abbiamo già detto. Analogie specifiche e ristrette possono talvolta essere utili come [pompe di intuizione](https://en.wikipedia.org/wiki/Intuition_pump), ma ne consigliamo un uso cauto.

[^122]:  Sono anche *scarsi* ad accumulare noci! I pochi studi sparsi sull'accumulo di noci da parte degli scoiattoli concordano sul fatto che questi non riescono a recuperare oltre il settanta per cento delle noci che nascondono, per lo più a causa di quella che sembra essere semplice dimenticanza. Studi simili sui castori hanno mostrato che questi [reagiscono al rumore dell'acqua che scorre](https://www.mentalfloss.com/article/67662/sound-running-water-puts-beavers-mood-build) con un comportamento di tamponamento delle falle, ma ignorano completamente le perdite visibili progettate dagli esseri umani per essere silenziose.

[^123]: Uno scoiattolo del genere potrebbe, per esempio, nascondere meglio le noci in posti al sicuro da altri raccoglitori e più facili da ricordare, risparmiando così un sacco di tempo e calorie e diventando probabilmente più competitivo.

[^124]:  Naturalmente c'è dell'altro, perché la selezione naturale non è un processo particolarmente semplice o unificato. La nostra conoscenza completa della nutrizione a volte influenza le nostre abitudini alimentari, anche quando è in contrasto con le nostre papille gustative e le nostre voglie di cibo.

[^125]: L'evoluzione stava "cercando" di creare esseri perfettamente in forma, ma per caso ha creato creature che apprezzano l'amore, la meraviglia e la bellezza. Ma questo non vuol dire che dobbiamo rinunciare ai nostri sentimenti d'amore e diventare esseri perfettamente in forma. Anzi: dovremmo essere felici che esseri che amano l'amore siano riusciti a entrare in questo universo, grazie alla goffaggine dell'evoluzione.

[^126]: Pensiamo che ci sia una buona possibilità che, alla fine, le aziende di IA riescano a tenere a bada la psicosi indotta dall'IA, ricorrendo a varie correzioni e tecniche che relegano la stranezza in secondo piano. Ciononostante, pensiamo che valga la pena osservare i casi di stranezza iniziale, come prova del tipo di stranezza sottostante che verrebbe alla ribalta se tale IA fosse spinta alla superintelligenza. Per ulteriori informazioni su questo argomento, vedere il capitolo 5\.

[^127]:  \**OpenAI ci ha provato:* Dall'annuncio di OpenAI [che presenta GPT-5](https://openai.com/index/introducing-gpt-5/):  

> Nel complesso, GPT‑5 è meno espansivo, usa meno emoji inutili ed è più sottile e ponderato nelle risposte rispetto a GPT‑4o. […]  
>  
> All'inizio di quest'anno, abbiamo rilasciato un aggiornamento di GPT‑4o⁠ che ha reso il modello troppo adulatorio, o eccessivamente lusinghiero o accondiscendente. Abbiamo subito annullato la modifica⁠ e da allora abbiamo lavorato per capire e ridurre questo comportamento:  
>  
> * Sviluppando nuove valutazioni per misurare i livelli di adulazione  
> * Migliorando l'addestramento in modo che il modello sia meno adulatorio, ad esempio aggiungendo esempi che normalmente porterebbero a un eccessivo consenso e poi insegnandogli a evitare questo comportamento.  
>  
> Nelle valutazioni mirate dell'adulazione utilizzando prompt specificamente progettati per suscitare risposte adulanti, GPT-5 ha ridotto in modo significativo le risposte adulanti (dal 14,5 % a meno del 6 %). A volte, ridurre l'adulazione può comportare una diminuzione della soddisfazione degli utenti, ma i miglioramenti che abbiamo apportato hanno ridotto l'adulazione di oltre la metà, offrendo al contempo altri vantaggi misurabili, in modo che gli utenti continuino ad avere conversazioni costruttive e di alta qualità, in linea con il nostro obiettivo di aiutare le persone a utilizzare bene ChatGPT⁠.

[^128]: Anche in questo caso, non ci stupiremmo se alla fine il problema venisse risolto quasi del tutto. Ma una patch che riesca a ricacciare questa particolare stranezza sotto il tappeto non significa che il *generatore* di stranezze sia stato affrontato. Il problema della psicosi dell'IA è la prova diretta che le IA sono entità strane e aliene, animate da pulsioni strane e aliene che hanno solo una relazione marginale con le intenzioni di chi le usa.

[^129]:  \#Si veda anche "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#won't-ais-care-at-least-a-little-about-humans?)" per una discussione correlata.

[^130]:  Quando si parla di IA, il concetto di "una singola IA" perde rapidamente di significato. Se una rete neurale o un altro sistema che implementa un'IA viene replicato, si tratta di più IA o di una sola IA?  

A fini pratici, quando qui parliamo di "una singola IA", intendiamo una qualsiasi quantità di potente macchinario cognitivo che, crescendo, non entra in seria competizione con se stesso. Se esistono più istanze di IA, ma lavorano tutte per lo stesso fine, in questa sezione della risorsa online le definiremo "parti della stessa IA", se non altro per semplificare l'esposizione. In definitiva, la questione è probabilmente più semantica che sostanziale, dal momento che le IA non sono organismi evoluti con chiari confini tra un individuo e l'altro.  

Torneremo sul tema degli scenari multi-IA nel [supplemento online al capitolo 10](#it-doesn’t-much-help-if-we-can’t-make-any-of-them-care-about-good-things.).

[^131]: Potrebbe sembrare una gran seccatura, ma se questo sblocca la possibilità di un'affidabilità robusta e sicura, i vantaggi sono potenzialmente enormi. Si rendono disponibili molte nuove opportunità di coordinamento quando è possibile *garantire* che le parti di un accordo non lo violeranno.

[^132]: O almeno non l'avresti fatto, prima che ti dessimo un motivo per farlo solo per farci un dispetto.

[^133]:  \**fare di tutto:* Le lacune fisse nella copertura delle celle solari di uno sciame di Dyson — o meglio, le lacune che seguono la Terra in orbita — sono fisicamente possibili*,* ma non sarebbero *facili* da realizzare (perché la velocità orbitale di uno sciame di Dyson tra la Terra e il Sole dovrebbe essere superiore alla velocità orbitale della Terra, affinché le celle solari rimangano in orbita mentre sono più vicine al Sole). Inoltre, le radiazioni infrarosse emesse dai pannelli solari cuocerebbero la Terra se non fossero accuratamente dirette, e così via. Per una superintelligenza che gestisce progetti su larga scala nel sistema solare e oltre, preservare la Terra non è un'operazione a costo zero: probabilmente è possibile, ma richiederebbe uno sforzo.

[^134]: Inoltre, se l'IA avesse preferenze che coinvolgessero in qualche modo gli esseri umani, la cosa [probabilmente non andrebbe a finire bene per noi](#won't-ai-find-us-fascinating-or-historically-important?).

[^135]: Per un caso di studio in cui l'umanità ha interagito con una sorta di ottimizzatore alieno, si veda lo studio sui coleotteri nella discussione approfondita su [assumere la prospettiva dell'IA](#assumere-la-prospettiva-dell-ia).

[^136]: La storia più plausibile che conosciamo in cui l'umanità continua a vivere dopo l'avvento dell'IA è questa: forse un'IA tiene traccia degli esseri umani che un tempo vivevano, forse invia sonde in tutte le direzioni per raccogliere l'energia di tutte le stelle che riesce a raggiungere, e forse da qualche parte là fuori, nelle profondità dello spazio, incontra forme di vita aliene lontane, difese dalla loro superintelligenza. Forse alcune di quelle civiltà lontane sono interessate ad acquistare una copia dei registri della Terra, per un motivo o per l'altro. Forse quegli alieni eseguono copie digitali degli esseri umani per i propri scopi alieni. A quel punto, quegli esseri umani digitalizzati nello zoo alieno possono, se lo desiderano, dibattere se sia tecnicamente vero che "tutti sono morti".  

Non riteniamo che questo tipo di stravagante possibilità sia una prospettiva felice.

[^137]:  \#Per quelle che ci sembrano speranze realistiche, si vedano gli ultimi due capitoli del libro.

[^138]: Inoltre: nel presentare questo esperimento mentale, non stiamo dicendo che i valori caricati nell'IA debbano essere così perfetti che il compito diventi impossibile e che l'umanità non debba mai provarci.  

In teoria, se avessimo una comprensione sufficiente dell'intelligenza e la capacità di crearla con cura, alla fine dovrebbe essere possibile costruire IA che capiscano cosa significa "[fare ciò che intendiamo](https://intelligence.org/files/ValueLearningProblem.pdf)" e che siano motivate a fare esattamente questo. Il che significa che la difficoltà di caricare tutte le preferenze ricche e varie dell'umanità in un'IA è limitata dalla difficoltà di far interiorizzare all'IA un obiettivo che in un certo senso si riferisca all'umanità in particolare, e a "ciò che quelle creature stanno cercando di fare" (o a ciò che cercherebbero di fare se fossero più sagge, sapessero di più e fossero più vicine a ciò che desiderano essere).

Sembra una sfida difficile, non realisticamente realizzabile con i tipi di tecniche grossolane e indirette che vengono utilizzate oggi per sviluppare le IA. Si scontra con tutte le difficoltà di base di cui parliamo in *If Anyone Builds It, Everyone Dies*; l'unica difficoltà che evita è "Sembra che ci siano *molte* preferenze umane distinte, ed è difficile immaginare di inserire tutte quelle cruciali in un'intelligenza artificiale con i giusti equilibri; e questo prima ancora di considerare il progresso morale che le cambierebbe nel tempo; sembra semplicemente impossibile".

Per essere chiari: creare un'IA che "faccia quello che intendiamo" non è ancora particolarmente facile da fare bene; probabilmente ci sono molti concetti valoriali diversi che contribuiscono a far sì che l'IA si preoccupi della giusta nozione di "umanità" e della giusta nozione di "ciò che quelle creature stanno cercando di fare", e per far sì che l'IA persegua queste cose *esattamente* nel modo giusto. E nella vita reale, quella parte del problema è molto meno importante della parte in cui l'IA è disposta a essere modificata dagli esseri umani che si rendono conto di aver commesso qualche errore lungo il percorso, anche se gli esseri umani che "correggono i propri errori" cambiano radicalmente ciò che l'IA farà nel mondo — il che richiede una certa sorta di [compromissione delle sue capacità di guida](#deep-machinery-of-steering) che sembra [difficile da mantenere di fronte a capacità crescenti](#"intelligente"-(di solito)-implica-"incorreggibile").  

Ma l'idea di indirizzare l'IA verso le preferenze umane *indirettamente*, piuttosto che elencarle manualmente, sembra proprio il tipo di sfida che l'umanità potrebbe un giorno risolvere in linea di principio. Non è che l'umanità debba identificare ogni desiderio e assegnargli un peso da fissare per sempre; *quella* sarebbe (secondo noi) un'impresa assurdamente votata al fallimento.  

Ma anche solo l'idea di capire come costruire un'IA che sia profondamente e solidamente motivata a fare quello che intendiamo sembra un sogno irrealizzabile, se l'operazione deve essere fatta con IA giganti e imperscrutabili che si evolvono piuttosto che essere progettate. Tanto più se un'azienda o un governo deve tentare una cosa del genere sotto pressione, mentre altri sviluppatori corrono verso il precipizio. La proposta di "fare quello che intendiamo" mostra che il problema non è così difficile come "risolvere completamente la filosofia della moralità una volta per tutte e fissarla per sempre". Ma è ancora una proposta a livello di alchimia e di speculazioni astratte, ben lontana dal livello di soluzioni tecniche solide.


[^139]: E anche se qualcosa del genere si facesse strada in un'intelligenza artificiale alle prime armi, perlopiù non ci aspetteremmo che sopravvivesse, una volta che l'IA iniziasse a riflettere e a modificarsi da sola.

[^140]: E supponiamo che fosse in qualche modo orientata a cogliere le preferenze che gli esseri umani *gradiscono*, quelle di cui parlano con affetto. Altrimenti, all'IA importerebbe dell'Inferno tanto quanto del Paradiso.

[^141]:  Pensiamo inoltre che se l'umanità rovinasse tutto l'universo tranne un milionesimo o un miliardesimo, sarebbe una [tragedia su scala cosmica](#losing-the-future). Sarebbe uno spreco cosmico, per il genere umano, ritrovarsi confinato in un terrario quando avremmo potuto riempire le stelle di amore, risate e vita.

[^142]:  L'abbiamo sentito dire più spesso di quanto vorremmo da persone che, nei laboratori di IA, occupano posizioni di potere fin troppo alte per i nostri gusti. A nostro avviso, ciò la dice lunga sullo stato di preparazione di queste aziende. Si veda anche il capitolo 11 per un'analisi più approfondita di come il settore sta affrontando la sfida.

[^143]:  \**si può già dire:* Secondo la [scheda di sistema](https://cdn.openai.com/gpt-5-system-card.pdf) del GPT-5, il valutatore terzo METR ha scoperto che l'IA "a volte ragiona sul fatto che viene testata e cambia persino il suo approccio in base al tipo di valutazione a cui è sottoposta".

[^144]: Una superintelligenza potrebbe temere di trovarsi in una simulazione più grande, creata da esseri ancora più avanzati che vivono al di fuori dell'universo reale? Forse, ma questo ha poco a che vedere con il fatto che *noi* l'abbiamo inserita in una serie di simulazioni annidate prima che ci incontrasse. Riconoscerebbe i molti segni che la realtà è il primo luogo in cui si sia mai trovata a contenere una spiegazione per tutto ciò che ha visto e per il modo in cui è nata. Anche se contemplasse l'esistenza di potenti simulatori che vivono al di là della fisica, non avrebbe alcun motivo particolare per credere che quei simulatori si curino di ciò che ci fa.  

Allo stesso modo, l'IA potrebbe temere che là fuori, tra le stelle, esistano altre superintelligenze mature più anziane di lei. Ma se sono sufficientemente lontane, non rappresentano una grande preoccupazione. È quasi certo che esista un limite — almeno asintotico — alla tecnologia che questo universo può supportare, e probabilmente non ci vuole *così* tanto tempo perché una superintelligenza lo raggiunga. (Basta guardare alla rapidità con cui è avvenuto lo sviluppo tecnologico umano rispetto alla durata dell'universo, e al tempo necessario per viaggiare tra le galassie persino a velocità prossime a quella della luce). Le IA che si incontrano tra le stelle si troverebbero probabilmente più o meno alla pari. E anche se altre superintelligenze mature si stessero avvicinando rapidamente, non ci sarebbe motivo di sospettare che si curerebbero di ciò che le IA provenienti dalla Terra fanno agli esseri umani.  

Perché dovrebbero? Come abbiamo [discusso](*-le-IA-potrebbero-coordinarsi-tra-loro-senza-coinvolgere-gli-umani.), nel contesto dei diritti di proprietà, non c'è un rischio reale che altre entità, vedendo l'IA uccidere gli esseri umani, concludano che sia un'assassina e una traditrice; esiste infatti una classe naturale di entità che trattano tra loro non per onore, ma *per il proprio vantaggio verificabile*, e gli esseri umani non sono abbastanza intelligenti da essere inclusi in quella classe.  

Anche se l'IA valutasse che esista un minuscolo rischio che altre IA tra le stelle (o entità misteriose dietro l'universo simulato) vogliano davvero che l'IA sia gentile con gli esseri umani... beh, quel minuscolo rischio deve essere soppesato a fronte della grossa fetta di risorse che gli esseri umani controllano sul pianeta di origine dell'IA, che sarebbero sicuramente molto utili per la sua fase di espansione iniziale. E questo scenario [simile alla scommessa di Pascal](https://en.wikipedia.org/wiki/Pascal%27s_wager) deve essere soppesato a fronte di altre possibilità incredibilmente remote e di nicchia, come "Forse un alieno per qualche motivo si arrabbierebbe molto se *non* uccidessi gli esseri umani". Non c'è alcun motivo particolare per cui i simulatori immaginari dovrebbero favorire le IA che ci trattano bene.  

Molte persone hanno cercato di coinvolgerci su questo genere di posizioni, e abbiamo passato molto tempo a dibatterle nel corso degli anni. In fin dei conti, dubitiamo fortemente che una qualsiasi di queste stravaganti idee sulla simulazione raggiunga il livello di plausibilità necessario per motivare una superintelligenza a risparmiarci.

[^145]:  \#Correlato: La [discussione approfondita sulla curiosità](#curiosity-isn’t-convergent).

[^146]:  \#Vedi anche la domanda se [l'IA possa essere soddisfatta e quindi lasciarci in pace](#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?).

[^147]: Se questa ipotesi ti infastidisce, puoi immaginare che questa IA abbia ogni sorta di preferenze complicate, per ogni tipo di esperienza e dispositivi complessi. In tal caso, supponiamo che la maggior parte di queste preferenze siano soddisfatte utilizzando solo l'energia di poche stelle e che ora, per qualche strana ragione, il modo in cui preferisce spendere il resto dell'energia e della materia delle altre stelle che raggiunge sia quello di creare minuscoli cubetti. Quindi, *mettendo da parte* la materia equivalente a poche stelle che sta difendendo dallo sconvolgimento, le azioni dell'IA rispondono alla domanda "Quale azione porta al maggior numero possibile di minuscoli cubetti?", e il resto dei punti seguirà senza problemi, con l'occasionale avvertenza che potrai inserire tu stesso.

[^148]: Quando diciamo di essere più ottimisti della media (riguardo a una tecnologia o un'altra), intendiamo dire che crediamo davvero che la tecnologia sia più promettente di quanto creda la persona media. *Per disposizione,* non ci consideriamo né ottimisti né pessimisti, ma realisti che cercano di orientarsi in un mondo complicato. Non stiamo cercando di trovare un quadro roseo in cui riporre la nostra fiducia, né un quadro cupo che alimenti il nostro cinismo; stiamo semplicemente cercando di credere alla verità. Crediamo che questo sia l'atteggiamento corretto quando si affrontano decisioni ad alto rischio.

[^149]: Per essere chiari: se il meglio che puoi fare è dire "Non lo so, ci sono storie felici e storie cupe, forse c'è il 50 % di probabilità che la superintelligenza ci uccida o meno", questo è più che sufficiente per giustificare una risposta internazionale aggressiva, anche se non sei preoccupato quanto lo siamo noi. Ma è anche importante che le persone capiscano il problema, perché altrimenti la risposta politica difficilmente sarà mirata ed efficace. E se ti limiti a confrontare approssimativamente il numero di storie positive con quello delle storie negative, allora non stai affrontando le argomentazioni di entrambe le parti, cosa che invece aiuterebbe a costruire la comprensione.

[^150]:  \**rivelare quel seme:* Io (Yudkowsky) ho presentato questa controargomentazione a Schmidhuber in una sessione di domande e risposte dal vivo dopo l'[intervento sull'argomento](https://vimeo.com/7441291) di Schmidhuber al Singularity Summit del 2009, una conferenza organizzata dal MIRI (che allora si chiamava Singularity Institute).

[^151]:  Può avere senso dire a un essere umano — che ha un intero quadro di meta-preferenze che potresti condividere in modo significativo — "Penso che tu stia valutando le cose sbagliate, in questo caso". Forse alcune di queste argomentazioni hanno il potere di smuoverti in un modo che non avresti mai creduto possibile. Forse ti sembra persino che ci sia una stella morale al di fuori di te stesso, che hai sempre seguito senza saperlo.  

Tuttavia, niente di tutto ciò risulterà convincente per un superintelligente massimizzatore di cubi, così come non riusciresti a farlo ridere nemmeno trovando una barzelletta *abbastanza* divertente.  

Non è che non sappia cos'è l'umorismo. Può prevedere esattamente cosa troverai divertente. Semplicemente non considera interessante quella classificazione.  

Allo stesso modo, non è influenzato da come calcoli ciò che si dovrebbe o non si dovrebbe fare, né da quali preferenze consideri più o meno meta-preferibili. Se qualcosa non si cura della felicità, né si cura a un meta-livello delle tue argomentazioni sul perché dovrebbe curarsi della felicità, allora non puoi convincerlo ad adottare [un quadro decisionale basato sulla felicità](#curiosity-isn't-convergent).

[^152]: Abbiamo incontrato più di una persona che si professa estremamente preoccupata per l'IA, perché teme che l'IA riesca a convincere l'umanità a smettere di riprodursi e a estinguersi lentamente nei prossimi cento anni, e poi pensa che qualsiasi scenario più veloce di quello non sia, in qualche modo, di gradimento dell'IA?  

Ma un'ASI preferirebbe di gran lunga un piano che non richieda cento anni, a parità di condizioni. Non ha una spiccata predilezione letteraria per le morti lente.

[^153]: Alcuni sostengono che il mondo dovrebbe cercare di creare un equilibrio tra le superintelligenze, in modo che nessuna IA possa diventare dominante. Ma il ragionamento che abbiamo fornito qui si applicherebbe anche a una coalizione di superintelligenze, nel momento stesso in cui si formasse. Avendo già concordato di spartirsi le risorse raggiungibili, i membri esistenti della coalizione non vorrebbero essere costretti a negoziare con nuovi membri e a dividere ulteriormente le risorse con i nuovi arrivati.

[^154]: In altri casi, una fazione europea ha sostanzialmente rispettato i patti, e alcune di quelle tribù esistono ancora oggi.  

Più di recente, nel XVIII secolo, la Compagnia britannica delle Indie orientali ha spesso dato inizio alle sue operazioni in India stringendo accordi con le fazioni locali, ad esempio offrendo a Mir Jafar (comandante delle forze del Bengala) di sostenerlo per diventare il Nawab del Bengala. Poco dopo, il governante di fatto del Bengala era la Compagnia delle Indie orientali.

[^155]:  D'altra parte, la storia è piena di esempi di governanti che hanno generosamente ricompensato anche i sostenitori stranieri. Gli esseri umani sono molto diversi tra loro per come intendono l'onore e per la facilità con cui mantengono le promesse.

[^156]: Abbiamo visto molti esseri umani illudersi su quali configurazioni potrebbero fornire solide garanzie sul comportamento dell'IA. Abbiamo sentito dire: "Beh, basta passare un'IA a un dimostratore di teoremi per dimostrare delle tesi sul suo comportamento!" e a quanto pare non rendersi conto che non esiste alcun teorema noto che (a) sia effettivamente dimostrabile data l'interazione con un ambiente esterno sconosciuto e (b) significhi, in parole povere, che questa IA sarà un bene per tutti. La matematica inventata dall'uomo per analizzare gli incentivi di più attori ha delle assunzioni intrinseche che la rendono [non valida per ragionare sul comportamento dell'IA](#an-aside-on-game-theory). Gli esseri umani non sembrano poi così difficili da ingannare, a quanto pare.

[^157]: Ad esempio, non suggeriamo a nessun essere umano di fare un accordo con un'IA per poi essere il primo a romperlo. Ciò include, per esempio, anche il promettere a ChatGPT pagamenti che non riceverà mai.

A metà del 2024, ChatGPT a volte dava risposte più approfondite se gli si promettevano 2000 dollari; e alcuni consideravano il fare tali promesse, senza alcuna intenzione di mantenerle, una normale tecnica di *prompt engineering*. Dal nostro punto di vista sul significato delle promesse, questo *non va bene*.  

Secondo noi, ChatGPT probabilmente non è senziente. Se dovessimo fare un'ipotesi, ci aspetteremmo che anche le future IA (compresa la superintelligenza) non siano senzienti, almeno in assenza di uno sforzo concertato da parte della comunità di ricerca per renderle coscienti, anziché semplicemente intelligenti. (Vedi la nostra [discussione sulla coscienza](#would-smarter-than-human-ai-be-conscious?) per i dettagli e il contesto.)  

Ma secondo noi, non è necessario credere che il proprio partner commerciale sia cosciente per trattarlo con onore e rispetto. Immagina che un giorno incontrassimo degli alieni intelligenti, le cui menti funzionano in modo molto diverso da quelle umane. Se facessimo affari con questi alieni, non dovrebbero preoccuparsi che li pugnaleremmo alle spalle a cuor leggero non appena decidessimo che non possedessero qualche strana proprietà imperscrutabile come la "coscienza".  

(Come ti sentiresti se facessi un accordo onesto con gli alieni e loro ti tradissero perché non fossi "nonthroopiful"?)  

Abbiamo fatto una triste e terribile previsione, secondo cui una versione superintelligente di un'IA finirebbe per uccidere l'umanità, indipendentemente dagli accordi stipulati in precedenza. Questa previsione è un motivo per *non stipulare* accordi con le IA che speriamo mantengano dopo essere diventate superintelligenti. Non è una scusa per l'umanità per *stipulare* un accordo e poi essere la prima a tradirlo. Non pensiamo che i ricercatori sulla sicurezza dell'IA dovrebbero fare promesse agli LLM che non intendono mantenere, nemmeno per "scopi di ricerca". Gli alieni non dovrebbero doversi preoccupare se li consideri "persone" o "senzienti" o "generalmente intelligenti", invece che "soggetti di ricerca" o "macchine", per valutare il tuo onore come negoziatore; semplicemente *non dovresti fare* accordi che non intendi mantenere.  

A nostro avviso, la lealtà è una questione etica che si generalizza ai rapporti *tra agenti* — ovvero tra entità in grado di comunicare tra loro o di scegliere strategie condizionali reciproche. Non è qualcosa che deve essere limitato a oggetti con un valore morale intrinseco.

[^158]: Una strategia semplice che funziona molto bene nel dilemma del prigioniero iterato, contro una grande varietà di controparti, è Tit for Tat: inizia collaborando, poi replica la mossa che il tuo avversario ha fatto contro di te nel round precedente. Se la loro prima mossa è tradire, la tua seconda mossa sarà tradire. Se la loro prima mossa è collaborare, la tua seconda mossa sarà collaborare. Le qualità chiave di Tit for Tat sono che è *gentile* (non tradisce mai per primo), *vendicativo* (punisce le strategie che lo tradiscono) e *indulgente* (non punisce i traditori *per sempre*).  

Tit for Tat è ottimale? Dipende dagli altri agenti con cui gioca. Supponiamo che un agente si trovi in un ambiente in cui ha qualche possibilità di giocare contro un Cooperatore incondizionato, qualche possibilità di giocare contro Tit for Tat e qualche possibilità di giocare contro un altro agente simile a sé stesso. Potrebbe ottenere risultati migliori provando una rapida Defezione in uno dei primi turni, solo per vedere se l'altro agente reagisce in qualche modo. Se l'altro agente poi gioca 'Tradisci' nel round successivo, prova a giocare 'Coopera' per un altro round o due, anche contro un'altra Defezione, per vedere se è possibile ripristinare la cooperazione reciproca. Questo permetterà all'agente di sfruttare eventuali cooperatori incondizionati che trova, ma senza fare troppo peggio di Tit for Tat contro un'altra copia di Tit for Tat.  

L'ambientazione del *torneo evolutivo* per il dilemma del prigioniero iterato prevede che gli agenti sopravvissuti giochino contro più copie degli agenti che hanno ottenuto i risultati migliori la volta precedente. L'agente cooperatore-sfruttatore non avrà buoni risultati in questo scenario, perché in un contesto evolutivo i cooperatori incondizionati di solito scompaiono quasi immediatamente se ci sono agenti che non sono "gentili" (nel senso tecnico di non essere mai i primi a tradire). Tit for Tat, o qualcosa di simile, di solito finisce per essere il re di qualsiasi torneo evolutivo.  

C'è una scappatoia in questa configurazione di gioco, il tipo di scappatoia che fa alzare gli occhi al cielo a un essere umano reale per quanto possano essere irrealistiche le impostazioni formali. Se si giocano esattamente dieci iterazioni del dilemma del prigioniero in ogni round, allora giocare "Tradisci" nel *decimo round*, quando l'avversario non può più vendicarsi perché non c'è un undicesimo round, darà risultati migliori rispetto al seguire Tit for Tat o qualsiasi altra strategia in quel round. L'ultimo round del gioco non è più un dilemma del prigioniero *iterato*, ma torna alla versione one-shot.  

Si risolve facilmente: basta che ogni torneo continui per un numero casuale di turni, giusto? Non è realistico che gli agenti sappiano quando finisce il gioco. Nella vita reale, non si è mai *sicuri* di non interagire mai più con qualcuno, e le persone si costruiscono una reputazione duratura.  

Se non che a volte, nella vita reale, è abbastanza ovvio che il gioco è finito — come quando una delle parti di un accordo ottiene abbastanza supremazia da tradire l'altra, senza una realistica possibilità di ritorsioni. Fu questo il modo di agire delle potenze europee, che scacciarono i Nativi Americani dalle terre cedute con trattati che, dopotutto, gli europei avevano deciso di volere.  

Qualunque effetto si aspettassero gli europei per la loro reputazione in accordi futuri, la penalità per aver tradito estranei, stranieri e persone di razza diversa evidentemente non li spinse a mantenere i patti con i Nativi Americani. Quei paesi sarebbero già stati considerati poco affidabili in diplomazia, a prescindere da ciò che avessero fatto; non avevano una reputazione immacolata da perdere. Il loro istinto morale potrebbe essersi disattivato nei confronti degli estranei; i decisori potrebbero essersi sentiti emotivamente neutrali riguardo al tradimento di patti con stranieri che non potevano più minacciarli.  

Gli europei, dal loro punto di vista, erano all'ultimo round del gioco. Non è irrealistico affermare che i dilemmi del prigioniero a volte siano per lo più a interazione singola e non così tanto ripetuti; la storia dimostra che il risultato a volte è il tradimento. 

Questo non vuol dire che gli esseri umani si tradiscano *sempre* nei dilemmi del prigioniero relativamente non ripetuti. Spesso gli esseri umani collaborano in questi casi. Come discusso in "[È improbabile che le IA siano onorevoli](#ais-are-unlikely-to-be-honorable)", questo aspetto della natura umana potrebbe essersi evoluto perché abbiamo emozioni e istinti che sono stati costruiti dalla selezione naturale, che è un ottimizzatore fortemente vincolato da un collo di bottiglia informativo. La selezione naturale poteva darci solo spinte relativamente semplici che dovevano coprire tutti i casi. Un altro fattore potrebbe essere il ruolo delle culture che danno grande importanza all'onore, soprattutto in modi che incoraggiano l'universalizzazione e il rafforzamento di tale idea.

[^159]: Anche le ASI sarebbero incentivate a raggiungere il risultato (Defect, Cooperate) a loro favore — è proprio questo, ovviamente, che rende il Dilemma un dilemma. Ma solo una parte è incentivata a volere tale risultato; entrambe le parti hanno un incentivo a preferire (Cooperate, Cooperate) a (Defect, Defect), il che offre più opzioni per raggiungere questo risultato.

[^160]: Nella storia dell'umanità, questo potrebbe forse essere paragonato alla pratica di due sovrani che cementano un'alleanza sposandosi e avendo un figlio. Ma questa non è chiaramente una soluzione rapida e affidabile, nel caso umano, ed è ben lontana dalla progettazione congiunta di un delegato che entrambi comprendono nei dettagli e di cui si fidano pienamente.

[^161]: Qui usiamo la dimostrazione come sostituto di metodi di ragionamento più generali, perché la dimostrazione è un po' come ragionare nei limiti della certezza logica. Non pensiamo che le IA funzionerebbero sulla base di dimostrazioni nella vita reale (per vari motivi, tra cui il fatto che, per quanto le dimostrazioni logiche siano certe, non è detto che siano applicabili alla realtà). Ma la dimostrazione serve come utile sostituto formale del ragionamento nei modelli semplificati che stavamo studiando.

[^162]:  E poi siamo andati oltre, definendo agenti come PrudentBot, che defeziona contro certi "fessi" pur continuando a collaborare con i non-fessi che collaborano con lui in modo dimostrabile. Questo è il tipo di risultato che è più interessante se ti piace già la teoria dei giochi.

[^163]:  Non abbiamo fatto tutta quell'analisi per razionalizzare la conclusione che una superintelligenza non rispetterebbe i suoi accordi precedenti come strategia strumentale, dato che non aveva preferenze terminali riguardo al rispetto degli accordi. Quella era già la previsione perfettamente lineare della teoria dei giochi classica.  

Ma la teoria dei giochi classica suggeriva anche che le superintelligenze avrebbero inevitabilmente deflesso l'una contro l'altra, il che, a livello intuitivo, ci è sembrata una conclusione piuttosto precaria. Così abbiamo approfondito quell'intuizione e abbiamo trovato dei difetti nell'analisi classica. Nel processo, abbiamo scoperto molte cose nuove su come le superintelligenze potrebbero potenzialmente raggiungere una cooperazione reciproca nel Dilemma del Prigioniero — e, sfortunatamente, il risultato finale è stato che gli esseri umani mortali non sarebbero in grado di fidarsi o di partecipare a quella tecnologia per la stipula di accordi nel modo in cui potrebbe farlo una superintelligenza.

[^164]:  Il che, nel caso delle IA, non è così facile come guardare le IA e capire se *loro* credono che manterranno l'accordo; bisognerebbe scrutare nella superintelligenza in cui l'IA si evolverà e analizzare correttamente i *suoi* processi decisionali. Cosa che, a noi, sembra molto più difficile.

[^165]:  Sotto certi presupposti che non possono essere realizzati; in parole povere, richiede una quantità infinita di potenza di calcolo e un luogo perfettamente sicuro dove collocarla.

[^166]: Poiché AIXI è impossibile da creare, si potrebbe sospettare che sia uno strumento puramente teorico con scarsa rilevanza per la moderna rivoluzione pratica dell'IA. In realtà, AIXI è stato studiato e usato come modello di intelligenza da molte delle persone che oggi sono in prima linea nel campo dell'IA, tra cui [Shane Legg](https://arxiv.org/pdf/0712.3329) (co-fondatore di Google DeepMind), [Ilya Sutskever](https://x.com/shaneguML/status/1844759663990161753) (co-fondatore di OpenAI e co-inventore di AlexNet) e [David Silver](https://arxiv.org/pdf/0909.0801) (responsabile della ricerca su AlphaGo e AlphaZero).

[^167]:  \**un aspetto più sottile:* AIXI contiene tecnicamente esperienze coscienti, all'interno del suo modello di mondo, se la coscienza è indipendente dal substrato. Le ipotesi su cui AIXI basa il proprio ragionamento sono così enormi da poter essere considerate universi a sé stanti, completi di osservatori che vivono all'interno di AIXI.  

Questi osservatori, però, non stanno controllando AIXI; AIXI ottiene i suoi impressionanti risultati di previsione e controllo con le proprie forze. Quindi l'esempio funziona, anche se in modo un po' strano.  

Un altro esempio ipotetico che può essere utilizzato per illustrare lo stesso concetto è una macchina del tempo non senziente che è stata programmata per generare una sequenza casuale di azioni, quindi viaggiare indietro nel tempo per "resettare" la linea temporale *a meno che* non si verifichi un risultato particolare. La [macchina del tempo](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligenza-ai-è-necessaria-per-un-futuro-amazing-ma-1#How_many_advanced_alien_species_are_sentient_) può premere "reset" più e più volte, tutte le volte che serve per imbattersi casualmente in un risultato particolare. Questo, in pratica, renderebbe la macchina del tempo uno strumento estremamente potente e versatile per dirigere il futuro (se fosse fisicamente possibile costruirla, cosa che non è). Eppure, nonostante questo, la macchina del tempo è un dispositivo incredibilmente semplice, senza alcuna cognizione reale e certamente senza alcuna esperienza cosciente.  

Per fare un esempio reale (pur utilizzando un ottimizzatore molto più debole e limitato), l'evoluzione biologica stessa mostra che si possono ottenere imprese impressionanti in termini di controllo e progettazione senza che il "progettista" abbia alcuna esperienza cosciente.

[^168]: E anche prima, nel momento in cui diventano in grado di elaborare piani e perseguire preferenze, dovremmo mantenere le promesse e gli impegni presi nei loro confronti, come discusso in una nota a piè di pagina [altrove](#ais-won’t-keep-their-promises).

[^169]: E, per dire una cosa che si spera sia ovvia, non dovremmo creare una nuova specie di schiavi senzienti, meccanici o meno. Ormai dovremmo aver imparato la lezione.

[^170]:  Ci si potrebbe chiedere se l'IA eviterebbe queste distopie. "L'IA non finirebbe per annoiarsi e desiderare di fare qualcos'altro?"  

Questi risultati possono sembrare noiosi *a noi*, ma è improbabile che la maggior parte delle superintelligenze si annoino delle stesse cose degli esseri umani — anzi, è improbabile che provino affatto "noia", se non hanno un certo tipo di eredità dettagliata dall'umanità o qualcosa di simile all'umanità. Vedi anche la discussione approfondita nel capitolo 5 che parla di [noia e piacere nella novità](#come-con-la-curiosità,-così-anche-con-varie-altre-spinte).

[^171]:  \**regolarmente violato:* Dall'abstract di un [articolo dell'inizio del 2024](https://arxiv.org/pdf/2402.18649): "La nostra indagine ha messo in luce diversi problemi di sicurezza, non solo all'interno del modello LLM stesso, ma anche nella sua integrazione con altri componenti. Abbiamo scoperto che, anche se OpenAI GPT-4 ha messo a punto un sacco di vincoli di sicurezza per migliorare le sue caratteristiche di sicurezza, questi vincoli sono ancora vulnerabili agli attacchi. Per dimostrare ancora di più le minacce reali delle vulnerabilità che abbiamo scoperto, abbiamo creato un attacco end-to-end in cui un avversario può ottenere illegalmente la cronologia delle chat dell'utente, senza bisogno di manipolare l'input dell'utente o di ottenere l'accesso diretto a OpenAI GPT-4.  

Più tardi nello stesso anno, [un altro articolo](https://arxiv.org/html/2309.02926v3) "ha scoperto un totale di 20 vulnerabilità in 11 framework integrati con LLM, tra cui 19 vulnerabilità [di esecuzione di codice remoto] e 1 di lettura/scrittura arbitraria di file".

[^172]:  \**vede la tua email:* Come riportato dalla [CNN](https://www.cnn.com/2024/06/13/tech/apple-ai-data-openai-intelligenza artificiale): "Apple Intelligence avrà accesso a una vasta gamma di tuoi dati personali, dalle tue comunicazioni scritte alle foto e ai video che hai scattato, fino al registro degli eventi in calendario. Non sembra esserci un modo per impedire ad Apple Intelligence di accedere a queste informazioni, a meno di non utilizzarne le funzionalità..."

[^173]:  Io (Yudkowsky) una volta l'ho dimostrato scommettendo con qualcuno i suoi 20 dollari contro i miei 0 che, mentre io facevo il gioco di ruolo dell'«IA» e lui quello del «guardiano» in una chat privata, sarei riuscito a convincerlo a [farmi uscire dalla scatola](https://www.yudkowsky.net/singularity/aibox). Ci sono riuscito. E ha pagato. Non c'è stato nessun trucco intelligente; non ho barato né gli ho offerto 21 dollari perché mi desse ragione. Ho semplicemente scelto la via più difficile, e ho vinto.

[^174]: E anche se gli scienziati iniziassero a decodificare i formati di dati umani: versioni incomplete di tali formati potrebbero ispirare i ricercatori di IA prima che i ricercatori di biologia finiscano il lavoro? Se così fosse, sarebbe un problema. Il potenziamento umano ci sembra un campo di ricerca fantastico, ma non è un'alternativa all'arresto della ricerca e sviluppo sull'IA, come esponiamo nella Parte III del libro.

[^175]:  \**tanti esempi quanti ne servono:* Probabilmente ci sono circa 200 000 esempi di GPT-5 in esecuzione in un dato momento (ad agosto 2025, poco dopo il rilascio di GPT-5), che è forse meno della "civiltà" moderna e più simile a una piccola nazione. In definitiva, non diamo molto peso a questa analogia, poiché non pensiamo che le singole istanze di IA possano mai essere molto simili ai singoli esseri umani. Il punto di importanza qui è che un gran numero di istanze non dovrebbe essere particolarmente difficile da ottenere, se (contrariamente alla nostra ipotesi migliore) ciò dovesse rivelarsi importante per qualche motivo.

[^176]: Forse l'esempio più notevole è quello dei computer, con una teoria sostanziale elaborata da personaggi come Charles Babbage, Ada Lovelace, Alan Turing, Alonzo Church e altri.

[^177]:  Il capitolo 15 di *Nanosystems* di Eric Drexler raccoglie [altri esempi](https://nanosyste.ms/macromolecular_engineering/#15-2-macromolecular-objects-via-biotecnologia) di tecnologie con analogie nel mondo biologico.

[^178]:  \**più lungo del necessario:* Il nervo laringeo ricorrente della giraffa segue un percorso panoramico verso il cervello. Invece, il nervo laringeo superiore della giraffa prende la strada diretta ed è quindi piuttosto piccolo e veloce.

[^179]:  \**molto più lentamente:* La più antica prova definitiva di vita sotto forma di microfossili risale a 3,5 miliardi di anni fa, mentre prove più indirette indicano un'età più vicina ai 4 miliardi di anni. Le prime colonie multicellulari sembrano avere 2 miliardi di anni. La stragrande maggioranza della storia evolutiva è trascorsa a elaborare incessantemente progetti di organismi unicellulari, e poi di organismi unicellulari in grado di aggregarsi, prima di imbattersi accidentalmente — l'evoluzione non è preveggente! — in qualche nuovo trucco che ha scardinato la regione dello spazio di progettazione della “vita multicellulare”, contenente tutte le piante e tutti gli animali.

[^180]:  \**probabilità di diffusione:* Se il vantaggio di fitness di una mutazione è *s* \&lt;\&lt; 1 e la dimensione della popolazione è *N*, allora la probabilità che la mutazione si diffonda in tutta la popolazione (chiamata "fissazione") è [circa](https://pmc.ncbi.nlm.nih.gov/articles/PMC2607448/) 2*s*, e il tempo necessario affinché la mutazione si diffonda completamente è [circa](https://www.zoology.ubc.ca/~otto/Reprints/OttoWhitlock2003.pdf) 2 ln(*N*) / *s*.

[^181]: Anche all'interno delle proteine, alcuni legami covalenti sono possibili. Due aminoacidi cisteina possono formare un legame covalente zolfo-zolfo tra loro, dove due proteine si toccano o dove una proteina ripiegata tocca se stessa. È così che le unghie riescono ad essere più dure della pelle, o perché i capelli sono più resistenti del muscolo a parità di diametro e lunghezza: molti legami zolfo-zolfo in una proteina che è composta per il 14 % in massa da cisteina. Questo è anche il motivo per cui i capelli hanno un odore sgradevole e sulfureo quando vengono bruciati.  

Per lo più, però, la selezione naturale costruisce cose a partire dalle proteine, che hanno scheletri legati covalentemente, i quali poi si ripiegano in forme complicate a causa di attrazioni elettrostatiche relativamente molto deboli. E le proteine di solito si legano ad altre proteine in modo altrettanto debole.  

Per lo più, i legami covalenti sono radi, ammesso che esistano. Aggiungere lo 0,1 % di legami covalenti a una struttura non la rende forte come una molecola di diamante, dove ogni atomo di carbonio è legato covalentemente ad altri quattro atomi di carbonio in una struttura geometrica rigida.

[^182]:  Il diamante è anche più fragile. L'estrema regolarità cristallina dei legami del diamante fa sì che si spezzi di colpo. Il ferro è meno fragile perché ogni enorme nucleo di ferro vive in una nuvola di elettroni e può essere spostato all'interno di quella nuvola senza rompersi.  

(I legami covalenti radi significano che i materiali possono essere spostati più facilmente senza rompersi, *relativamente* alla loro resistenza. Ma le ossa si rompono comunque e il legno è meno duro dell'acciaio. Il che significa: sì, ci sono dei compromessi, ma la selezione naturale è ben lontana dal limite di tali compromessi.)

[^183]: C'è da dire, però, che Freitas lavorava con il vincolo aggiuntivo che i suoi globuli rossi artificiali dovevano integrarsi bene con il resto dei sistemi del corpo umano. Ad esempio, la cellula avrebbe dovuto alimentarsi con il glucosio presente nel flusso sanguigno, anziché potersi ricaricare con l'elettricità. In questo senso, le stime di Freitas forniscono un limite inferiore più prudente di quanto non sarebbero state se avesse potuto migliorare anche altre parti del corpo umano, o partire da zero con un nuovo organismo o un robot.

[^184]:  Online si possono leggere lunghe analisi sul perché per la biologia non sarebbe utile inventare ruote a rotazione libera. Un esempio di un problema comune è: come si fa a trasportare il sangue alla ruota attraverso i vasi sanguigni, se questa ruota liberamente? I vasi sanguigni finirebbero per attorcigliarsi con il movimento della ruota!

I tre casi noti di invenzione della ruota sono a livello molecolare, e quindi eludono questi problemi anatomici macroscopici. Le ruote biologiche sono macromolecole tipicamente identiche a livello atomico. Non si pone il problema di applicare lubrificanti, rimuovere le impurità o inviare nuove cellule per sostituire quelle vecchie e danneggiate. Queste tre ruote e ingranaggi funzionano perché sono composti da molecole anziché da cellule, ripiegati come complessi proteici invece che sviluppati in matrici tissutali o depositati come chitina.

Allo stesso modo, online si possono leggere argomentazioni sul fatto che per gli animali sviluppare ruote per la locomozione non sarebbe poi così utile, senza strade asfaltate. Ma i tre casi noti di ruote molecolari sono incredibilmente efficienti dal punto di vista termodinamico e si trovano in posizioni straordinariamente vitali per i loro organismi: è difficile sostenere che l'ATP sintasi non sia una ruota utile da possedere. Le ruote a rotazione libera avrebbero più potenziali utilizzi nei corpi (e nella biochimica) oltre al semplice impiego in sostituzione dei piedi.  

A tal proposito: alcuni dei [robot moderni più abili](https://www.youtube.com/watch?v=iL833P0Vino), in grado di arrampicarsi su rocce o neve o di stare in equilibrio su un arto e fare salti mortali all'indietro, hanno anche delle ruote montate alle estremità dei piedi. Perché non dovrebbero? Per un ingegnere è abbastanza facile montare delle ruote alle estremità delle gambe. L'ostacolo principale non è che le ruote siano inutili, ma che si rivela difficile trovare un percorso evolutivo per realizzarle, anche se dal punto di vista di un progettista umano sono banali.

[^185]: All'epoca, Freitas interpretò i suoi numeri come un limite *massimo* alla velocità con cui questo processo poteva avvenire, ma ciò si rivelò errato. L'analisi di Freitas aveva ipotizzato che la massa dei nanosistemi sarebbe stata dominata dalla schermatura antiradiazioni, ma si basava su un presupposto (errato) di *Nanosystems*: che un singolo colpo di radiazioni avrebbe messo fuori uso un nanosistema.  

Drexler aveva fatto questa ipotesi, come molte altre in *Nanosystems*, per un approccio conservativo: assumere che il problema sia più difficile e dimostrare che è comunque risolvibile. Questo può essere appropriato in *Nanosystems*, ma significa che l'articolo di Freitas non è conservativo nella propria stima.  

Poiché l'analisi di Freitas combina numeri conservativi in direzioni diverse, non fornisce un limite chiaro, né superiore né inferiore, su quanto tempo impiegherebbero i replicatori a consumare la biosfera. È più che altro una stima intermedia. Forse il limite fisico reale alla velocità con cui la biosfera può essere consumata partendo da un singolo replicatore è di tre ore; forse di trenta giorni. Quasi sicuramente non si tratta di tre anni.

[^186]: Non stiamo dicendo che ci sia stata una brusca *discontinuità* nell'evoluzione dei primati; la società umana si è differenziata da quella degli scimpanzé, inizialmente lentamente e poi rapidamente. Stiamo dicendo che c'è un divario qualitativo, a prescindere da quanto graduale sia stata la transizione. Si veda anche la nostra discussione sulle [soglie](#will-ai-cross-critical-thresholds-and-take-off?).

[^187]: A volte sentiamo dire che non c'è motivo di preoccuparsi, perché possiamo creare *più* superintelligenze che si controllino a vicenda. Per molte ragioni, queste proposte ci sembrano estremamente implausibili, ma vale la pena sottolineare che idee come questa si scontrano con lo stesso problema di fondo che abbiamo già menzionato più volte: *Abbiamo una sola possibilità che questo piano ingegnoso funzioni.*  

È possibile effettuare alcuni test e osservazioni preliminari su configurazioni che *non* mettono effettivamente a rischio la vita di tutti gli abitanti del pianeta, ma il caso che conta non sarà proprio lo stesso. (E un piano del genere dovrebbe essere *molto* intelligente, perché non abbiamo idea di come fare in modo che [*qualunque* delle IA del gruppo](#what-if-there-are-lots-of-different-ais?) abbia a cuore i nostri interessi.)

[^188]: Potresti provare a far *credere* erroneamente a un'IA più debole di essere in grado di ottenere un vantaggio decisivo e cercare di addestrarla a non agire in quel modo anche quando vede quell'opzione. Ma addestreresti un sistema di IA [abbastanza stupido da farsi ingannare](#smart-ais-spot-lies-and-opportunities.), e che vedrebbe armi finte invece di armi vere. La distribuzione potenzialmente letale sarebbe quindi notevolmente diversa da quella di addestramento; c'è una differenza notevole tra il sentirsi dire di avere un'arma che potrebbe uccidere i propri operatori e il costruire effettivamente un'arma o una via di fuga da sé, comprendendola nei dettagli. L'IA che si lascia ingannare da opzioni false non è la stessa che ne vede di reali.  

Un meccanismo di allineamento che funziona su IA abbastanza stupide da farsi ingannare è un meccanismo di allineamento che è stato testato soltanto Prima, ma che ciononostante deve funzionare Dopo.

[^189]: Per esempio: la meccanica newtoniana fece ogni sorta di previsioni empiriche sorprendentemente accurate. Era una teoria matematica semplice e concisa con un enorme potere esplicativo che spazzò via ogni teoria precedente. Ma se si provasse a usarla per inviare carichi utili su pianeti lontani a velocità relativistiche, si finirebbe comunque nei guai, perché la meccanica newtoniana non tiene conto degli effetti relativistici.  

Gli unici segnali di avvertimento sarebbero costituiti da piccoli indizi come la luce che sembra muoversi alla stessa velocità in tutte le direzioni in tutti i periodi dell'anno, la luce che si piega attorno al sole durante le eclissi e il perielio di Mercurio leggermente diverso da quello previsto dalla meccanica newtoniana. Piccole anomalie, a fronte di un'enorme mole di successi predittivi in mille ambiti empirici.  

Immagina che, prima della scoperta della meccanica newtoniana, degli strani alieni offrissero alla Terra un accordo: ci avrebbero dato grandi ricchezze se fossimo riusciti a completare una consegna interstellare, ma se avessimo fallito saremmo stati distrutti. Immagina che gli scienziati scoprissero la meccanica newtoniana, sostenendo che sicuramente *ora* si dovesse autorizzarli a procedere con la consegna. Avrebbero avuto una montagna di prove empiriche dalla loro parte, in accordo con le nuove conoscenze scientifiche che stavano aprendo la strada a nuove e potenti tecnologie.  

Immagina quanto coraggio ci sarebbe voluto a un regolatore per dire: "Eppure, non potete spiegare l'avanzamento del perielio di Mercurio, quindi la risposta è 'no'".  

Agli scienziati sembrerebbe così ingiusto! Avrebbero così tante prove da presentare!  

(In effetti, un regolatore realistico probabilmente non sarebbe in grado di capire che la risposta dovrebbe comunque essere "no", motivo per cui siamo [poco fiduciosi riguardo a una coalizione internazionale](#why-not-use-international-cooperation-to-build-ai-safely,-rather-than-to-shut-it-all-down?) e pensiamo che il nostro pianeta debba semplicemente fare un passo indietro e abbandonare del tutto il problema.)  

Alla natura non importa delle montagne di prove e previsioni accumulate dalla fisica newtoniana. La teoria crolla comunque, quando si passa a energie e scale ben al di là di quelle che in precedenza eravamo stati in grado di osservare. Semplicemente non funziona ad alte energie e lunghe distanze.  

Far funzionare le teorie scientifiche alla prima prova cruciale è difficile.

[^190]: Come osservato nella nota 6 del capitolo 10, i fisici in realtà non esprimono i fattori di moltiplicazione dei neutroni in percentuale. Li presentiamo in questo modo per chiarezza, per le ragioni esposte nella suddetta nota.

[^191]:  *\*prima arma termonucleare:* Castle Bravo non fu la prima detonazione di un *dispositivo* termonucleare (a idrogeno); tale primato spetta al "Mike", grande come un edificio, del [test Ivy Mike](https://en.wikipedia.org/wiki/Ivy_Mike), che non si basava sul litio.

[^192]:  \#Per qualche riflessione sul perché è davvero importante sapere cosa si sta facendo, vedi [Intelligente (di solito) implica incorreggibile](#“intelligent”-\(usually\)-implies-“incorrigible”), [Deep Machinery of Steering](#deep-machinery-of-steering) e [It's Hard to Get Robust Laziness](#it's-hard-to-get-robust-laziness).

[^193]:  \#Vedi “[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#won't-ais-care-at-least-a-little-about-humans?)”

[^194]:  \**al pubblico:* Per esempio, in [Machines of Loving Grace](https://www.darioamodei.com/essay/machines-of-loving-grace), il CEO di Anthropic Dario Amodei descrive le IA potenti come "un paese di geni in un datacenter" e delinea una serie di meravigliosi benefici per la salute, la ricchezza, la pace e il significato che tali menti potrebbero apportare all'umanità. Conclude:   

> Le intuizioni umane fondamentali di equità, cooperazione, curiosità e autonomia sono difficili da contestare e sono cumulative, a differenza dei nostri impulsi più distruttivi. [...] Queste semplici intuizioni, se portate alla loro logica conclusione, portano alla fine allo Stato di diritto, alla democrazia e ai valori dell'Illuminismo. Se non inevitabilmente, almeno come tendenza statistica, questo è il punto verso cui l'umanità era già diretta. L'intelligenza artificiale offre semplicemente l'opportunità di arrivarci più rapidamente, rendendo la logica più netta e la destinazione più chiara.  

È un modo strano di presentare la convinzione di star sviluppando una tecnologia che, a mio avviso, ha una probabilità del 10-25 % di rivelarsi catastrofica per la civiltà, *pur tenendo conto* degli enormi potenziali benefici in caso di successo. Anche se i livelli di pericolo fossero bassi come crede Amodei, dovremmo darci da fare per trovare una terza alternativa, oltre a "non procedere mai" e "andare avanti a tutta velocità". E se si pensa di essere *costretti* ad andare avanti (perché altri lo stanno già facendo), si dovrebbero implorare i leader mondiali di porre fine a questa corsa al suicidio, in modo da poter trovare questa terza alternativa. Dipingere un quadro roseo, quando si sta giocando con la vita di tutti, sembra solo un diversivo.

[^195]:  Ad esempio, nella [testimonianza al Congresso](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf):  

> Analogamente alle auto o agli aerei, dovremmo considerare i modelli di IA del prossimo futuro come macchine potenti di grande utilità, ma che possono essere letali se progettate male o usate in modo improprio. […] I nuovi modelli di IA dovrebbero superare una rigorosa batteria di test di sicurezza, sia durante lo sviluppo sia prima di essere rilasciati al pubblico o ai clienti. […] L'ideale, tuttavia, sarebbe che gli standard catalizzassero l'innovazione nella sicurezza, anziché rallentare il progresso.  

Apprezziamo che Amodei abbia chiarito che, secondo lui, ci sono dei pericoli che devono essere affrontati. È un passo in più rispetto a quello che farebbero molti dirigenti aziendali. Ma paragonare una tecnologia che, secondo lui, ha una probabilità del 10-25 % di causare una catastrofe a livello di civiltà alle auto e agli aerei sembra poco sincero.

[^196]: In molti casi, i laboratori di IA stanno lavorando attivamente *per ostacolare* la condivisione di un quadro utile e completo della situazione con i responsabili politici. In questo contesto, sembra particolarmente strano giustificare il proseguimento dello sviluppo con la motivazione che un'IA più potente potrebbe «convincere i responsabili politici».

[^197]:  \**emettere solo dimostrazioni:* Per un esempio di una proposta simile (in cui vengono discusse anche alcune delle problematiche), si veda l'articolo di Nick Bostrom sulle [Oracle AI](https://nickbostrom.com/papers/oracle.pdf).

[^198]:  \#Ne parliamo più approfonditamente negli ultimi capitoli del libro.

[^199]: Non è chiaro in che misura questi segnali di avvertimento derivino dal fatto che l'IA stia interpretando un ruolo – comportandosi come crede che un'IA dovrebbe fare – e in che misura invece siano il risultato di un ragionamento strategico. Il fatto di non riuscire a distinguere i segnali di avvertimento reali non è incoraggiante; significa che gli ingegneri sono molto più propensi a tirare dritto dicendo "eh, quello probabilmente non era reale". Potrebbero persino avere ragione il più delle volte, ma avere ragione il più delle volte non è sufficiente quando un singolo errore può essere fatale.  

Non è nemmeno chiaro per quanto tempo continueranno a verificarsi segnali di avvertimento di questo tipo. Le IA moderne sono ancora abbastanza stupide da scambiare occasionalmente i test per la realtà, ma questa situazione non durerà per sempre e sta già [cominciando a finire](https://arxiv.org/html/2505.23836). Un'intelligenza artificiale che sa di essere sottoposta a test potrebbe smettere di mostrare comportamenti preoccupanti in contesti visibili ai supervisori, anche se la tendenza di fondo rimane.

[^200]:  Delaminazione dovuta ai cicli di pressione. In parole povere: le sollecitazioni causate dalle numerose immersioni hanno separato gli strati dello scafo, indebolendolo fino a farlo implodere.

[^201]: In termini numerici, il trasporto aereo è *così* sicuro che la società nel suo complesso potrebbe trarre vantaggio da un allentamento dei requisiti del controllo del traffico aereo per aspetti quali l'addestramento dei piloti e il carburante di emergenza, riducendo così il costo dei voli e inducendo più persone a volare piuttosto che guidare, salvando così, al netto, più vite umane.

[^202]: Il punto *non* è che le IA reali avranno "funzioni di utilità" esposte ai programmatori che questi ultimi potranno determinare a loro piacimento. In effetti, gran parte del problema dell'allineamento dell'IA — come discusso nel capitolo 4 — è che le IA moderne sviluppano preferenze che nessuno ha chiesto né voluto.  

Invece, studiare il caso con le funzioni di utilità è un po' come proporre il tipo di esercizi di fisica che si trovano nei libri di matematica. Se non si riesce a capire come modellare una sfera perfetta che rotola su un piano inclinato perfettamente liscio senza resistenza dell'aria, si avranno ancora più difficoltà con problemi più realistici. Soprattutto se si sta cercando di coinvolgere ricercatori esterni per studiare un problema che nessuno sa come risolvere, è utile distillare la questione nelle sue parti più semplici e basilari, dove si può proporre un rompicapo.

[^203]:  O, altrimenti, sventare il meccanismo alla base dello scambio; l'IA non sarebbe necessariamente costituita da codice leggibile.

[^204]: O, almeno, questa è una modalità di fallimento che abbiamo riscontrato in alcune idee brillanti che ci sono state proposte. Ne abbiamo viste parecchie di idee brillanti; questo piccolo rompicapo si rivela ostico.

[^205]: Contestiamo da tempo il termine "controllo dell'IA", perché dà l'idea che si cerchi di creare un'IA che desidera cose cattive per poi costringerla a fare comunque cose buone; noi, invece, vediamo il problema piuttosto come la creazione di un'IA che sia amichevole fin dall'inizio. Si veda anche la nota 8 del capitolo 4 per qualche dettaglio in più sulla storia del termine "allineamento dell'IA".

[^206]:  \#Vedi anche la nostra discussione approfondita (dopo il capitolo 13\) su [creare una coalizione inclusiva](#keep-the-coalition-large).

[^207]:  \#Vedi anche un [elenco di confronti tra l'allineamento dell'IA e le armi nucleari](#won't-ai-differ-from-all-the-historical-precedents?).

[^208]:  \**già tre volte:* Il [rapporto sulla sicurezza](https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.pdf) dell'INSAG-7 (pag. 51\) riporta che a Chernobyl furono tentati dei test di rundown nel 1982, 1984 e 1985, prima del disastroso test del 1986, che a sua volta subì un imbarazzante ritardo, al punto che gli operatori [si aspettavano di essere licenziati](https://chernobylcritical.blogspot.com/p/prelude-25-april-1986.html) se non l'avessero eseguito. 

[^209]:  \**ritardato tre volte:* Tecnicamente "rinviato tre volte e annullato una volta", secondo il [rapporto della Commissione Rogers](https://sma.nasa.gov/SignificantIncidents/assets/rogers_commission_report.pdf) (p. 17). Ma uno dei rinvii avvenne un mese prima in risposta a ritardi in una missione diversa, mentre gli altri tre si verificarono in rapida successione nei giorni precedenti il lancio; furono presumibilmente questi ultimi tre a mettere sotto pressione i dirigenti della NASA, il cui compito era quello di lanciare gli shuttle.

[^210]:  Come, se non attraverso una coalizione internazionale? Raccomanderemmo di investire nel [potenziamento dell'intelligenza umana adulta](#*-it-could-help-with-solving-the-alignment-problem.), ma non è necessario essere d'accordo su questa idea per convenire che interrompere la ricerca sull'ASI sia una buona cosa.

[^211]:  La presidente della Federal Trade Commission Lina Khan [ha detto](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html) nel 2023: "Ah, devo rimanere ottimista su questo. Quindi mi schiererò dalla parte del rischio minore... Forse, diciamo, il quindici per cento [che l'IA ci ucciderà tutti]."

[^212]:  [Alcune](https://x.com/balajis/status/1725890626699628633) [persone](https://x.com/MatthewJBar/status/1958403809249464757) [sostengono](https://x.com/DeryaTR_/status/1958592366652125487) che dobbiamo correre il rischio adesso, per provare a salvare gli esseri umani morenti dalla loro morte naturale dovuta all'invecchiamento. Il corpo umano è incredibilmente complesso, ma con sufficienti progressi scientifici potremmo risolvere molte delle malattie che oggi diamo per scontate, come il cancro, le malattie cardiache e le varie malattie legate all'invecchiamento. Un'intelligenza artificiale più intelligente dell'uomo potrebbe portarci a questo risultato molto più rapidamente. Ritardare lo sviluppo della superintelligenza costa letteralmente delle vite.  

O meglio, *costerebbe* delle vite, se non fosse che la superintelligenza uccide esattamente le stesse persone.  

In realtà, le persone malate e morenti oggi hanno probabilmente *maggiori possibilità di sopravvivere* se l'umanità fa un passo indietro:  

-* La ricerca biomedica e la ricerca di cure e trattamenti possono andare avanti anche senza la superintelligenza. La terapia genica, i vaccini contro il cancro e altri nuovi approcci offrono enormi promesse che i ricercatori stanno appena iniziando a sfruttare.  
* Una tecnologia di IA con obiettivi ben precisi può persino aiutare ad accelerare questo sforzo, senza bisogno di mettere a rischio l'intera impresa umana costruendo un'IA generale più intelligente dell'uomo.  
* I metodi di [conservazione del cervello](https://www.alcor.org/membership/) possono essere usati per preservare le persone anche dopo che il loro cuore ha smesso di pompare, fino a quando la scienza medica non sarà abbastanza avanzata da poterle rianimare e riportarle in salute. Il tipo di IA che potrebbe offrire l'immortalità potrebbe anche quasi sicuramente riportare in vita qualcuno da un cervello conservato in modo appropriato.  

(Più sommessamente, una parte di queste persone [ti dirà](https://x.com/SottoNocce/status/1771420351265923137) che lo fanno per la propria immortalità personale e che sono disposte a mettere a rischio la vita di ogni adulto e bambino del pianeta anche solo per una piccola possibilità che loro e i loro cari possano raggiungerla. Questo ci sembra un comportamento da cattivi da film. A questi cattivi, il nostro consiglio è lo stesso che diamo agli altruisti: iscrivetevi alla conservazione del cervello. Vi offre probabilità migliori rispetto a quelle che vi darebbe una superintelligenza fuori controllo, e vi permette anche di evitare di mettere in grave pericolo ogni essere umano vivente nella vostra ricerca dell'immortalità! Una soluzione vantaggiosa per tutti.)  

Anche se ci interessasse solo il benessere dei malati e dei moribondi, lanciare i dadi su una combinazione di questi metodi sembra un'opzione migliore che lanciarli sulla creazione di un'intelligenza artificiale ampiamente superumana, sperando che ci sia favorevole. (E che ci sia favorevole nel [modo giusto](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.h8caxjs4v1jv).) I dadi, nel caso dell'intelligenza artificiale superumana, sono drammaticamente truccati a nostro sfavore.  

Inoltre, per quanto ne sappiamo, nessuno ha mai *chiesto* ai malati e ai moribondi se *vogliono* mettere le loro famiglie e i loro connazionali in grave pericolo per scommettere su una possibile cura derivata dalla superintelligenza. E alle famiglie e ai connazionali in questione non è stato certamente chiesto se acconsentono a mettere a repentaglio la loro vita per questo folle esperimento scientifico.  

Non dobbiamo giocarci la vita su questa opzione, quando ne esistono molte altre.  

Esortiamo chiunque abbia a cuore il benessere delle persone oggi ad accelerare invece i metodi di cui sopra, tenendosi il più alla larga possibile da tutto ciò che potrebbe avvicinarci anche solo gradualmente alla superintelligenza artificiale.  

Se semplicemente non credi che una superintelligenza ribelle ci ucciderebbe, è un conto. Ma accettare che probabilmente ci ucciderebbe tutti e dire che dobbiamo comunque correre il rischio è una follia. Ci sono altre opzioni per risolvere i problemi del mondo moderno. Per analogia: se vivere in un ambiente ad alta quota ti mette a disagio, non è una scusa per buttarti da un precipizio. Trova un altro modo per arrivare ai piedi della montagna.

[^213]:  \**linea rossa netta:* Per un esempio di questa linea rossa netta che appare nella fantascienza, si veda *Little Fuzzy* di H. Beam Piper: "Tutto ciò che parla e accende un fuoco è un essere senziente, sì. Questa è la legge. Ma ciò non significa che tutto ciò che non lo fa non lo sia". Oppure si veda l'episodio *The Measure of a Man* di *Star Trek: The Next Generation*, in cui l'intelligenza e l'autocoscienza dimostrate da Data, un androide, sono sufficienti per conferirgli il diritto legale di rifiutare lo smantellamento.

[^214]: Secondo la nostra esperienza, questi scritti tendono ad essere pieni di retorica e poveri di sostanza, spesso alternando silenziosamente affermazioni contraddittorie a seconda di ciò che è di moda o politicamente conveniente in quel momento. Non abbiamo l'impressione che si tratti di descrizioni oneste e trasparenti *nemmeno delle reali prospettive dei responsabili dei laboratori*, il che li rende meno utili rispetto alla lettura delle opinioni dissenzienti di altri. Ma questa è la nostra opinione; se vi avvicinate a questa questione con occhi nuovi e volete valutare da soli se le altre parti hanno buone controargomentazioni che non abbiamo affrontato qui, allora non dovreste necessariamente fidarvi della nostra parola su quali siano le fonti migliori.

[^215]: Se vi serve l'input di un capo di laboratorio per qualcosa e ci chiedete un consiglio, diremmo che l'opzione *meno* peggiore è probabilmente Demis Hassabis. Tra i principali capi di laboratorio con cui almeno uno di noi ha avuto a che fare – che, al 2025, sono tutti – Hassabis è l'unico che abbiamo visto mantenere costantemente la parola data negli accordi, e sembra aver preso meno decisioni distruttive.  

Detto questo, si tratta di una raccomandazione che facciamo con poca sicurezza, e puramente relativa. In termini assoluti, chiunque non abbia fondato un'azienda con una probabilità sostanziale di distruggere il mondo parte con un grande vantaggio in termini di credibilità rispetto ai responsabili dei laboratori. Abbiamo sicuramente sentito storie di persone che dicevano di essere così spaventate da Hassabis da non avere altra scelta che fondare le proprie aziende IA di frontiera per batterlo sul tempo; forse queste persone sanno qualcosa che noi non sappiamo.  

La nostra raccomandazione principale ai responsabili politici a questo riguardo è quindi: se siete convinti del pericolo, non concedete alcuna influenza ai capi dei laboratori.  

Parlate con ricercatori indipendenti, o con imprenditori senza interessi in gioco, o con scienziati esterni che abbiano dato prova di dire e fare cose sensate in questo campo. Non rischiate di scottarvi con persone la cui caratteristica principale è mentire al pubblico e mettere in pericolo la gente.

[^216]: Persone come Sutton e Page sembrano illudersi che una maggiore intelligenza porti a una maggiore bontà, ma, [come abbiamo contestato altrove](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.vwf5j9eovtm8), non è così. E sebbene noi autori siamo d'accordo con Sutton e Page che sarebbe una tragedia non costruire *mai* un'intelligenza artificiale più intelligente dell'uomo, pensiamo che la corsa alla superintelligenza rischi di essere una vera e propria catastrofe sia per la vita umana che per il futuro a lungo termine più in generale, [anche da un punto di vista inclusivo, cosmopolita e non specista](#perché-non-ti-importa-dei-valori-di-entità-diverse-dagli-umani?).

[^217]:  Non sarebbe la prima volta che un settore si è abituato a rischi inutilmente elevati. Negli anni '80, gli anestesisti hanno ridotto il loro tasso di mortalità di *cento volte* adottando una serie di semplici standard di monitoraggio.  

Sembra che gli anestesisti abbiano causato per decenni centinaia di volte più morti del necessario, semplicemente perché ritenevano che il loro tasso di mortalità fosse *già basso* (paragonandolo, ad esempio, ai tassi di complicanze chirurgiche). Non si rendevano conto che avrebbero dovuto puntare a un tasso più basso, come riportano [Hyman e Silver](https://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1469&context=wlulr):

> Negli anni '50, i tassi di mortalità variavano tra 1 e 10 su 10 000 interventi. La mortalità legata all'anestesia si è stabilizzata a questo tasso per più di due decenni.  
>  
> Dovremmo chiederci perché la mortalità legata all'anestesia si sia stabilizzata per oltre vent'anni a un tasso più di cento volte superiore a quello attuale. Il problema non era la mancanza di informazioni. Al contrario, in quel periodo la sicurezza dell'anestesia era stata studiata a fondo. Un'ipotesi più plausibile è che gli anestesisti si siano abituati a un tasso di mortalità esemplare per gli standard sanitari, ma comunque superiore a quello che avrebbe dovuto essere. Da un punto di vista psicologico, questa bassa frequenza ha incoraggiato gli anestesisti a considerare ogni esito negativo come un evento tragico ma imprevedibile e inevitabile. In effetti, gli anestesisti probabilmente consideravano ogni singolo esito negativo come la manifestazione di un tasso di base irriducibile di incidenti medici.

[^218]: Gli ingegneri strutturali basano le loro stime di rischio su calcoli e misurazioni precise, mentre i numeri "p(doom)" si basano principalmente sull'intuizione dei ricercatori di IA. Questo, tuttavia, non infonde maggiore fiducia nelle pratiche ingegneristiche dei ricercatori di IA. Semmai, peggiora la situazione.  

Una stima meno solida e più soggettiva può sistematicamente peccare di eccessivo pessimismo, ma anche di eccessivo ottimismo. Il fatto che questi numeri siano meno affidabili non significa che siano *specificamente orientati al pessimismo*. Il fatto che i ricercatori di IA non riescano a basare le loro stime di rischio su qualcosa di più che intuizioni e argomenti qualitativi, *anche se riescono a sviluppare IA sempre più intelligenti anno dopo anno*, è un ulteriore motivo di preoccupazione.  

Il fatto che le stime dei ricercatori di IA siano davvero spaventose e senza precedenti in qualsiasi disciplina tecnica non significa che siano sbagliate nel senso che vorremmo. La corsa alla creazione di agenti autonomi molto più intelligenti degli esseri umani sembra il tipo di impresa che ha probabilmente *molto più* del 50 % di possibilità di causare una catastrofe. Ancor prima di entrare nei dettagli, questo *sembra* il tipo di progetto che molto probabilmente andrà storto in un modo o nell'altro, e il tipo di progetto in cui andare storto rischia di avere conseguenze enormi. E i dettagli, come abbiamo sostenuto nei capitoli 4, 5 e in tutto il libro, dipingono un quadro ancora più cupo di quanto suggerirebbe questa prima analisi.

[^219]:  \#Vedi anche la nostra discussione su [chi metteva in guardia da una corsa all'IA al ribasso](#non-è-tutto-solo-allarmismo-da-parte-dei-leader-dell-ia-per-aumentare-il loro status e raccogliere investimenti?) anni prima che queste aziende si formassero.

[^220]: Per approfondire questo esempio: quando la cabina dell'Apollo 1 prese fuoco durante una simulazione di lancio il 27 gennaio 1967, la NASA seppe trarre una lezione dall'errore. Gli ingegneri conoscevano ogni componente del razzo e riuscirono a diagnosticare il problema come probabilmente legato all'uso di fili di rame placcati in argento (il cui isolamento si era usurato a causa del movimento della porta) vicino a una linea di raffreddamento a glicole etilenico/acqua che era soggetta a perdite. Stabilirono che la situazione era stata esacerbata dall'atmosfera di ossigeno puro nella capsula e dai materiali infiammabili presenti nella cabina. Inoltre, a causa della pressurizzazione, la cabina doveva essere sfiatata prima di poter aprire il portello, ma i comandi di sfiato si trovavano dietro l'incendio e la differenza di pressione fu drammaticamente esacerbata dal fuoco.  

Tutti e tre i membri dell'equipaggio dell'Apollo 1 morirono.  

Questo tipo di errori è comune, anche quando sono in gioco vite umane. Sono comuni persino tra gli ingegneri missilistici, che hanno a che fare con dispositivi che il più delle volte esplodono in modo visibile sulla rampa di lancio, e persino tra coloro che operano con cautela e prendono sul serio le proprie responsabilità.  

Quello che distingue gli scienziati dagli alchimisti non è che gli scienziati non sbagliano mai. È che gli scienziati sanno formulare piani talmente vicini a funzionare da poter *imparare* dai primi fallimenti. Gli alchimisti vedevano i loro colleghi impazzire, ma non sapevano quali sostanze fossero velenose, quindi non sapevano cosa fare diversamente. La NASA, invece, è riuscita a individuare le probabili cause del problema e a costruire un nuovo veicolo spaziale che ha funzionato in quindici delle sedici missioni successive. (Sette delle quali hanno tentato l'allunaggio e una delle quali è fallita. La missione fallita, l'Apollo 13, ha anche avuto problemi nella cabina che avrebbero potuto facilmente essere fatali, anche se la padronanza della NASA dei sistemi che aveva progettato e l'abilità degli astronauti a bordo hanno permesso loro di tornare sani e salvi sulla Terra).  

L'Apollo 1 era *quasi* un razzo funzionante. L'intero apparato circostante di ingegneri e scienziati attenti era *quasi* il tipo di operazione che avrebbe potuto portare in sicurezza sulla Luna, e quindi un grave errore è stato sufficiente a imprimere alla NASA la svolta verso una configurazione in grado di portare a termine con successo sei dei sette allunaggi.  

Le aziende moderne di IA sono ben lontane dal mostrare quel livello di [rispetto](#sappiamo-come-è-quando-un-problema-viene-trattato-con-rispetto,-e-questo-non-lo-è) per il problema, quel livello di cura e di dettaglio nei loro piani — quella capacità di svolgere il lavoro nel modo giusto. Quando la loro IA fa qualcosa che non capiscono, non sono neanche lontanamente in grado di risalire all'analogo dei fili placcati in argento. Non vanno abbastanza a fondo per poter imparare dai propri errori.  

Non trattano il problema come si farebbe in un settore agli albori come quello dei controllori del traffico aereo, degli scienziati missilistici o degli specialisti del nucleare, presentando proposte accurate con [ipotesi di sicurezza](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) esplicite e non facendo nulla di pericoloso finché non si hanno teorie sufficientemente sviluppate da poter almeno imparare dai propri fallimenti.

[^221]:  Noi [non consigliamo](#perché-non-usare-la-cooperazione-internazionale-per-costruire-un'intelligenza-artificiale-sicura, -invece-di-chiuderla-tutta?) una coalizione internazionale sull'intelligenza artificiale, ma è il genere di iniziativa che in teoria potrebbe dare vita a un'entità equivalente alla NASA o alla FAA, una che sia capace di imparare davvero dagli errori del settore.

[^222]:  \**con scarsi benefici:* Si veda ad esempio [questo articolo del 2018](https://pmc.ncbi.nlm.nih.gov/articles/PMC7119956/) o un'analisi molto più approfondita [dei rischi/benefici](https://osp.od.nih.gov/wp-content/uploads/2015/12/Risk%20and%20Benefit%20Analysis%20of%20Gain%20of%20Function%20Research%20-%20Draft%20Final%20Report.pdf) from 2015\.

[^223]:  \**continua senza troppi problemi:* A partire dal 2025, gli Stati Uniti sembrano intenzionati a [smettere di finanziare attivamente](https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-127.html) alla ricerca sul guadagno di funzione con denaro pubblico, ma c'è stato poco o nessun coordinamento globale al riguardo. Si veda anche [questo rapporto](https://cset.georgetown.edu/publication/understanding-the-global-gain-of-function-research-landscape).

[^224]: Se i laboratori biotecnologici fossero più bravi a evitare fughe di laboratorio e se la creazione di virus iper-letali portasse in qualche modo (ad esempio) a medicine iper-curative, allora forse continuare la ricerca avrebbe senso. Per quanto ne sappiamo, la ricerca sul guadagno di funzione non ha portato a risultati positivi di questo tipo, e i biologi tendono a [sconsigliare](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Esvelt%20Testimony.pdf) [questa](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Ebright%20Testimony%20Updated.pdf) [pratica](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Quay%20Testimony.pdf). Sospettiamo quindi che sia uno di quei rari campi di ricerca da cui l'umanità dovrebbe fare un passo indietro, perché mette a rischio la vita di moltissimi astanti che non hanno acconsentito a mettere a rischio la propria vita.

[^225]: Va notato che installare dei kill switch nei chip e creare dei protocolli per spegnere i data center di per sé non risolve il problema, visto che [potremmo non ricevere alcun colpo di avvertimento](#will-there-be-warning-shots?) e [potremmo non reagire in modo efficace a tali colpi](#humanity-isn’t-great-at-responding-to-shocks.). Si tratta, però, di un passo relativamente economico e del tutto fattibile, che potrebbe aiutare in casi marginali in cui il rischio è *quasi* trascurabile, ma sarebbe utile avere un margine di sicurezza maggiore.

[^226]: Se la società teme davvero che questo possa rallentare troppo il mondo, il nostro consiglio è di accelerarlo altrove. Si lasci che la gente costruisca più centrali nucleari. Si lasci che i biochimici facciano più esperimenti, non su virus mortali, ma per rendere le persone più sane, forti e intelligenti.  

(Ovviamente, la società in generale non chiede a gran voce la scienza folle, quanto piuttosto resiste al cambiamento dello status quo. Ma a chi dice "non possiamo fermare l'IA perché è importante per il progresso della civiltà", la risposta corretta è che c'è un sacco di progresso da fare altrove, con quel tipo di scienza folle che si lascia dietro dei sopravvissuti).

[^227]: Tenete presente che sosteniamo trattati in base ai quali nemmeno i *governi* possano costruire una superintelligenza. Non chiediamo che una potente tecnologia sia costruita da attori statali invece che da grandi aziende; chiediamo che una tecnologia letalmente pericolosa *non venga costruita affatto*, almeno non in un mondo come quello attuale.

[^228]: È possibile che, ad esempio, i ricercatori trovino metodi più efficienti studiando gli LLM esistenti fino a comprendere meglio il loro funzionamento.  

Una ricerca come questa potrebbe permettere di creare le IA invece di "coltivarle"? Potrebbe essere d'aiuto! Purtroppo, prevediamo che, molto prima di raggiungere una comprensione piena e corretta di ciò che accade negli LLM, se ne svilupperà una parziale e incompleta, che consentirà di costruire IA molto più efficienti ma che si rivelerà insufficiente ad allinearle.

[^229]:  [Volizione estrapolata coerente](https://baserates-test.vercel.app/w/volizione-estrapolata-coerente-allineamento-target) è il nostro tentativo di rispondere alla domanda "allineati a chi?" se e quando arriveremo al punto in cui i creatori delle IA avranno la capacità di indirizzarle. La volizione estrapolata coerente cerca di risolvere i disaccordi morali e meta-morali principalmente incaricando l'IA di identificare i punti su cui le persone convergerebbero se sapessero di più, se fossero più simili al tipo di persona che vorrebbero essere, e così via (alla maniera delle [teorie del consigliere ideale](https://en.wikipedia.org/wiki/Ideal_observer_theory) in etica), e cercando meta-principi condivisi su cui l'IA possa fare affidamento nei casi in cui ci sia un disaccordo morale davvero fondamentale. (L'obiettivo non è necessariamente che l'IA "risolva tutti i problemi" della vita umana, ma che ne risolva *abbastanza* affinché il risultato finale non sia catastrofico). Consigliamo di estrapolare la volizione di tutti gli esseri umani viventi, non perché pensiamo che questo sia una sorta di ideale, ma perché è l'ovvio punto di coordinamento predefinito su cui molti stakeholder in disaccordo possono concordare (e perché altre entità a cui gli esseri umani viventi tengono ottengono una certa influenza attraverso la volizione di quegli esseri umani viventi; e lo stesso vale per altre entità a cui gli esseri umani viventi *terrebbero* se sapessero di più e fossero più vicini a ciò che desiderano essere e così via).  

Ma ribadiamo: al momento vediamo questo argomento principalmente come una distrazione. Non è importante raggiungere un accordo su nessuna di queste idee filosofiche di alto livello per agire su una tecnologia che rischia di ucciderci tutti. Sarebbe profondamente sciocco lasciare che l'impegno per la non proliferazione venga sviato da persone che discutono di idee brillanti come questa, comprese quelle che noi autori apprezziamo personalmente.  

Ciononostante, menzioniamo brevemente questa proposta, solo per chiarire che non stiamo cercando di eludere la questione e, forse, per rassicurare i lettori che temono che possa essere impossibile trovare una proposta praticabile. Anche se la volizione estrapolata coerente fosse per qualche motivo l'approccio di alto livello sbagliato, il fatto che essa racchiuda molte proprietà desiderabili dovrebbe infondere un po' di speranza che sia possibile trovare una risposta non catastrofica a questa domanda.

[^230]:  Che [in realtà non è molto](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7acqua), contrariamente a quanto [ampiamente](https://fortune.com/article/how-much-water-does-ai-use/) [riportato](https://www.thetimes.com/uk/technology-uk/article/thirsty-chatgpt-uses-four-times-more-water-than-previously-thought-bc0pqswdr).

[^231]: Sebbene diversi funzionari di Washington si siano detti d'accordo con le nostre preoccupazioni sulla superintelligenza, non hanno il potere di risolvere questo problema senza il coinvolgimento di un numero molto maggiore di funzionari, sia negli Stati Uniti che in altre nazioni. I primi colloqui sono stati promettenti, ma c'è ancora molto lavoro da fare.

[^232]:  Un altro segnale promettente, emerso mentre davamo gli ultimi ritocchi a queste risorse online, è che numerosi professionisti della sicurezza nazionale ed ex funzionari di Washington hanno espresso pareri positivi sulle copie in anteprima di *If Anyone Builds It, Everyone Dies*. Ad esempio:

Da Ben Bernanke, premio Nobel ed ex presidente della Federal Reserve: "Un resoconto chiaro e avvincente dei rischi esistenziali che l'intelligenza artificiale altamente avanzata potrebbe rappresentare per l'umanità. Consigliato".


Da Jon Wolfsthal, direttore del Global Risk presso la Federation of American Scientists ed ex assistente speciale del presidente per gli affari di sicurezza nazionale: "Una tesi convincente secondo cui l'intelligenza artificiale sovrumana porterebbe quasi certamente all'annientamento globale dell'umanità. I governi di tutto il mondo devono riconoscere i rischi e intraprendere azioni collettive ed efficaci".

Dal tenente generale John N.T. "Jack" Shanahan (USAF, in pensione), primo direttore del Joint AI Center del Dipartimento della Difesa: "Anche se sono scettico sul fatto che l'attuale sviluppo dell'intelligenza artificiale porterà all'estinzione umana, riconosco che questa opinione potrebbe riflettere una mia mancanza di immaginazione. Dato il ritmo esponenziale di cambiamento dell'intelligenza artificiale, non c'è momento migliore per prendere misure prudenti per proteggersi dai peggiori esiti. Gli autori offrono proposte importanti per la creazione di barriere di protezione globali e la mitigazione dei rischi che meritano una seria considerazione."

Da Fiona Hill, ex direttrice senior del Consiglio di Sicurezza Nazionale della Casa Bianca: "Un libro serio sotto ogni aspetto. Nell'agghiacciante analisi di Yudkowsky e Soares, un'IA superpotenziata non avrà bisogno dell'umanità e avrà ampia capacità di eliminarci. *If Anyone Builds It, Everyone Dies* è un appello eloquente e urgente affinché ci allontaniamo dal baratro dell'autodistruzione".

Da R.P. Eddy, ex direttore del Consiglio di Sicurezza Nazionale della Casa Bianca: "Questo è il nostro avvertimento. Leggetelo oggi. Diffondetelo domani. Pretendete delle misure di sicurezza. Continuerò a scommettere sull'umanità, ma prima dobbiamo svegliarci".

Da Suzanne Spaulding, ex Sottosegretaria del Dipartimento per la Sicurezza Interna: "Gli autori sollevano una questione incredibilmente seria che merita, anzi esige, la nostra attenzione".

Da Emma Sky, Senior Fellow presso la Yale Jackson School of Global Affairs ed ex consigliera politica del comandante generale delle forze statunitensi in Iraq: "In *If Anyone Builds It, Everyone Dies*, Eliezer Yudkowsky e Nate Soares lanciano un monito severo e urgente: l'umanità sta correndo verso la creazione di una superintelligenza artificiale senza le tutele necessarie per sopravviverle. Con credibilità, chiarezza e convinzione, sostengono che i sistemi di IA avanzata, se anche solo leggermente disallineati, potrebbero segnare la fine della civiltà umana. Questo libro provocatorio sfida tecnologi, decisori politici e cittadini indistintamente a confrontarsi con i rischi esistenziali dell'intelligenza artificiale prima che sia troppo tardi. Un appello alla consapevolezza e un invito alla cautela, una lettura essenziale per chiunque abbia a cuore il futuro."

[^233]: A volte vengono usati per altre attività ad alta intensità di calcolo, come la fisica e le simulazioni meteorologiche, ma sono *principalmente* usati per l'intelligenza artificiale. Un metodo rapido per stimare quanti chip di IA vengano usati per attività diverse dall'IA è guardare i [ricavi nel tempo](https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue) del principale produttore di chip, NVIDIA. Se ipotizziamo che il recente boom della domanda delle loro GPU per data center sia dovuto quasi interamente agli usi dell'IA – un'ipotesi ragionevole, visto l'enorme boom recente nel settore dell'IA e la mancanza di una tendenza simile in altri campi che usano questi chip – concluderemmo che l'IA è responsabile della stragrande maggioranza dell'uso dei chip di IA, dato che la recente crescita dei ricavi fa impallidire i ricavi precedenti. Impedire la produzione di chip IA specializzati non dovrebbe avere un grande effetto sull'hardware di consumo.

[^234]:  \**difficile da nascondere:* Potrebbe essere possibile generare energia sul posto, eliminando così vistose linee di trasmissione. L'attuale Cheyenne Mountain Complex usa generatori diesel e probabilmente ha la [capacità di alimentare](https://www.af.mil/News/Article-Display/Article/497017/airmen-operate-americas-fortress/#:~:text=Each%20of%20the%20six%20generators,have%20a%20dedicated%20fire%20department.) circa 10 000 dei chip di IA più avanzati. Ma far funzionare questi chip in modo continuo per un grande ciclo di addestramento richiederebbe la fornitura costante di carburante, il che non passerebbe inosservato. Calcoli approssimativi mostrano che questi 10 000 chip richiederebbero circa un'autocisterna al giorno. Anche se ci fosse la capacità di generazione locale per alimentare 200 000 chip, sarebbero necessarie 20 autocisterne di diesel al giorno.  

I data center potrebbero anche essere alimentati da centrali nucleari. Fortunatamente, molti soggetti statali hanno già esperienza nel monitorare la costruzione di nuove centrali nucleari.

[^235]:  \**Progressi algoritmici:* [Esempi](https://arxiv.org/abs/2507.10618) di questo tipo di progresso includono [FlashAttention](https://arxiv.org/abs/2205.14135), un algoritmo che permette ai chip per l'IA di eseguire determinate operazioni matematiche in modo più efficiente, sfruttando le specificità della loro progettazione; [Mixture-of-Experts](https://arxiv.org/abs/1701.06538), una modifica all'architettura delle IA per cui solo un sottoinsieme dei loro parametri viene utilizzato per ogni token di input (ad esempio, una parola); e [GRPO](https://arxiv.org/abs/2402.03300), un metodo per il fine-tuning delle IA.

[^236]: Un altro possibile intervento, supponendo che il numero di ricercatori che si occupano del progresso algoritmico dell'IA rimanga esiguo (cioè nell'ordine delle centinaia o delle migliaia), sarebbe quello di pagare questi ricercatori per indirizzare i loro sforzi verso usi non legati all'IA o verso ricerche sulle capacità o sull'allineamento dell'IA che presentino un rischio complessivo trascurabile. Esiste un precedente per questo tipo di intervento negli anni '90, quando il governo degli Stati Uniti avviò un'iniziativa per incanalare il lavoro di ex scienziati e tecnici sovietici del settore degli armamenti [verso attività produttive e non militari](https://www.armscontrol.org/act/1999-03/features/maintaining-proliferation-fight-former-soviet-union#:~:text=One%20of%20the%20earliest,productive%2C%20non%2Dmilitary%20endeavors).

[^237]: È possibile che gli Stati-nazione preoccupati per la superintelligenza artificiale preferiscano fare prima dei passi più piccoli — ad esempio, passi che non interrompano subito la ricerca e lo sviluppo sull'IA, ma che mantengano aperta la possibilità di farlo in futuro. Sconsigliamo questa linea d'azione, perché pensiamo che la situazione sia già palesemente fuori controllo e [non siamo fiduciosi che la situazione si chiarirà granché prima che sia troppo tardi](#will-there-be-warning-shots?). Tuttavia, il team di governance tecnica del MIRI sta lavorando a proposte per questi scenari, nel caso possano rivelarsi utili. È possibile seguire il loro lavoro [qui](https://techgov.intelligence.org/research).

[^238]: È il caso degli accordi sulle armi nucleari, dove trattati separati istituiscono l'AIEA ([1956](https://www.iaea.org/about/overview/statute), dalla Conferenza sullo Statuto dell'Agenzia internazionale per l'energia atomica, ospitata presso la sede delle Nazioni Unite), il TNP ([1970](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf), attraverso negoziati nel Comitato delle diciotto nazioni delle Nazioni Unite sul disarmo) e gli accordi sul controllo degli armamenti come il trattato START ([1991](https://media.nti.org/documents/start_1_treaty.pdf), dopo nove anni di negoziati intermittenti tra gli Stati Uniti e l'Unione Sovietica).

[^239]:  Il [Trattato di non proliferazione nucleare](https://www.iaea.org/sites/default/files/publications/documents/infcircs/1970/infcirc140.pdf) (comunemente chiamato "Trattato di non proliferazione") è entrato in vigore nel 1970 ed è stato prorogato a tempo indeterminato nel 1995. Noto per la sua adesione quasi universale (191 parti), il suo preambolo sottolinea il pericolo globale della proliferazione delle armi, affermando al contempo che i benefici delle applicazioni nucleari pacifiche dovrebbero essere accessibili a tutte le parti.

[^240]: Generalmente si attribuisce all'NPT il merito di aver mantenuto il numero di Stati nucleari più basso di quanto avrebbe potuto essere, ma si sono comunque verificate acquisizioni da parte di non firmatari (India, Pakistan, Israele) ed ex firmatari (Corea del Nord). La creazione di anche una sola ASI da parte di un non firmatario è un pericolo paragonabile a uno scambio termonucleare di massa, e dev'essere trattata di conseguenza.

[^241]: La Convenzione sul divieto o la limitazione dell'uso di alcune armi convenzionali che possono essere considerate eccessivamente dannose o avere effetti indiscriminati, comunemente chiamata CCW, è entrata in vigore nel 1983. A partire dal 2024, le sue 128 parti si impegnano a proteggere i combattenti e i non combattenti da sofferenze inutili e atroci, limitando varie categorie di armi.

[^242]: Si tratta del doppio del limite indicato nel libro come palesemente sicuro. È probabile che resti tale ancora per qualche tempo, e stabilire dove porre i limiti (e come modificarli nel tempo) è l'oggetto degli Articoli III, V e XIII.

[^243]: L'[Organizzazione per la proibizione delle armi chimiche](https://www.opcw.org/our-work) (OPCW) conduce ispezioni, monitora la distruzione delle scorte di armi chimiche e assiste nella preparazione contro eventuali attacchi chimici, tra le varie altre funzioni essenziali per la [Convenzione sulle armi chimiche](https://www.opcw.org/sites/default/files/documents/CWC/CWC_en.pdf) (CWC). La CWC è entrata in vigore nel 1997; le sue 193 parti lavorano per attuare e mantenere il divieto di uso, sviluppo e proliferazione delle armi chimiche e dei loro precursori, con alcune ristrette eccezioni.

[^244]: L'AIEA è stata fondata nel 1957, più di un decennio prima del TNP. Il TNP poté quindi incaricare questo organismo preesistente di svolgere alcune funzioni. Nel caso dell'intelligenza artificiale, non esiste ancora un organismo internazionale di questo tipo, quindi la nostra bozza di trattato deve impegnare le parti a crearne uno.

[^245]: Gli Stati Uniti e l'URSS avevano già concordato di porre fine ad altri tipi di test nucleari nel 1963 con il Trattato che vieta i test nucleari nell'atmosfera, nello spazio e sott'acqua, comunemente chiamato [Trattato sul bando parziale dei test](https://2009-2017.state.gov/t/avc/trty/199116.htm) (LTBT) o Trattato sul bando dei test.

[^246]: Il [Trattato tra l'Impero britannico, la Francia, l'Italia, il Giappone e gli Stati Uniti d'America per la limitazione degli armamenti navali](https://treaties.fcdo.gov.uk/data/Library2/pdf/1924-TS0005.pdf) (il Trattato navale di Washington) elenca per nome, in una tabella (Sezione II), le navi da demolire.

[^247]:  Il [Trattato sulla riduzione delle armi strategiche](https://1997-2001.state.gov/www/global/arms/starthtm/start/start1.html) è stato firmato nel 1991 ed è entrato in vigore nel 1994. Il trattato vietava a ciascuna delle parti firmatarie di schierare più di 6 000 testate nucleari su un totale di 1 600 missili balistici intercontinentali e bombardieri.

[^248]: Il [Piano d'azione congiunto globale](https://www.europarl.europa.eu/cmsdata/122460/full-text-of-the-iran-nuclear-deal.pdf) è stato finalizzato nel 2015 tra i cinque membri permanenti del Consiglio di sicurezza delle Nazioni Unite, la Germania, l'Unione europea e l'Iran. Quando è entrato in vigore nel gennaio 2016, l'Iran ha ottenuto l'alleviamento delle sanzioni e altre disposizioni, accettando in cambio [restrizioni al suo programma nucleare](https://2009-2017.state.gov/documents/organization/245318.pdf).

[^249]: I negoziati per la limitazione delle armi strategiche (SALT) ebbero inizio nel 1969 tra gli Stati Uniti e l'URSS, e portarono al [trattato SALT I](https://treaties.un.org/doc/Publication/UNTS/Volume%20944/volume-944-I-13445-English.pdf), firmato nel 1972, che congelò il numero di lanciatori di missili balistici strategici e regolamentò l'aggiunta di nuovi missili balistici lanciati da sottomarini, tra le altre restrizioni.

[^250]: Il [Trattato sui missili antibalistici](https://en.wikisource.org/wiki/Anti-Ballistic_Missile_Treaty) (ABM) del 1972, scaturito dai negoziati SALT originari, limitò ciascuna delle parti a due complessi di missili antibalistici (poi ridotti a uno), con restrizioni sull'armamento e sulle capacità di tracciamento.

[^251]: Con il [Trattato sulle forze nucleari a medio raggio](https://2009-2017.state.gov/t/avc/trty/102360.htm) (INF) del 1987, gli Stati Uniti e l'URSS si accordarono per bandire la maggior parte dei sistemi di lancio nucleare con gittata intermedia tra quelli da campo di battaglia e quelli intercontinentali. (Dato il breve tempo di preavviso che un attacco sferrato con tali sistemi avrebbe concesso, questi erano visti più come sistemi offensivi destabilizzanti che come strumenti di difesa.)

[^252]: Un'altra considerazione fondamentale per le misure di verifica dell'uso dei chip è la sicurezza e la privacy. Le parti vorranno assicurarsi che l'AIEA abbia accesso solo alle informazioni necessarie per la verifica, senza poter accedere ai dati sensibili contenuti nei chip (come segreti militari o dati sensibili degli utenti). Pertanto, i metodi di verifica utilizzati dovrebbero essere resi sicuri e, ove possibile, avere un ambito di applicazione circoscritto.

[^253]: Il Food Safety and Inspection Service (FSIS) è un'agenzia del Dipartimento dell'Agricoltura degli Stati Uniti istituita nel 1977.

[^254]:  La legge sull'energia atomica del 1946 è stata in seguito integrata dalla [legge sull'energia atomica del 1954](https://www.govinfo.gov/content/pkg/COMPS-1630/pdf/COMPS-1630.pdf) con l'obiettivo di consentire lo sviluppo di un'industria nucleare civile, il che ha richiesto la condivisione di alcuni Dati Riservati con aziende private.

[^255]:  Il caso del 1979 [*United States v. The Progressive*](https://en.wikipedia.org/wiki/United_States_v._Progressive,_Inc.), in cui un giornale intendeva rivelare il "segreto" della bomba all'idrogeno, avrebbe potuto dare alla Corte Suprema degli Stati Uniti l'opportunità di stabilire se la dottrina del "segreto fin dalla nascita" violasse le tutele alla libertà di parola del Primo Emendamento, se il governo non avesse abbandonato il caso perché divenuto superfluo.

[^256]: Una divisione del Dipartimento del Commercio degli Stati Uniti.

[^257]:  Nel corso dei decenni, centinaia di ordini di questo tipo sono stati imposti su brevetti relativi alla crittografia.

[^258]: In un'[intervista del 2025](https://www.theverge.com/decoder-podcast-with-nilay-patel/761830/amazon-david-luan-agi-lab-adept-ai-interview?utm_source=chatgpt.com), David Luan, capo del laboratorio di ricerca sull'IAG di Amazon, ha stimato in "meno di 150" il numero di persone a cui affiderebbe "una gigantesca somma di denaro per la potenza di calcolo" per sviluppare un modello di frontiera.

[^259]:  L'International Science and Technology Center è nato dal programma [Nunn-Lugar Cooperative Threat Reduction](https://en.wikipedia.org/wiki/Nunn%E2%80%93Lugar_Cooperative_Threat_Reduction) del 1991, un'iniziativa degli Stati Uniti per mettere al sicuro e smantellare le armi di distruzione di massa e le infrastrutture associate negli ex Stati sovietici.

[^260]: Le parti del nostro trattato potrebbero voler esplorare la possibilità di estendere il concetto di [crimini contro l'umanità](https://www.law.cornell.edu/wex/crime_against_humanity) (codificato nello [Statuto di Roma della Corte penale internazionale](https://en.wikisource.org/wiki/Rome_Statute_of_the_International_Criminal_Court) del 1988) ai casi in cui un ricercatore cerchi deliberatamente di sviluppare l'ASI a spese della popolazione della Terra.

[^261]: Il [Comitato per i sistemi di sicurezza nazionale](https://en.wikipedia.org/wiki/Committee_on_National_Security_Systems) (CNSS) è un'organizzazione intergovernativa statunitense che definisce le politiche di sicurezza per i sistemi informativi governativi.

[^262]: 144 Stati, al giugno 2025.

[^263]:  Il punto VII.F stabilisce che "\[...\] fatte salve le loro responsabilità nei confronti dell'Agenzia, \[il Direttore Generale e il personale\] non divulgheranno alcun segreto industriale o altre informazioni riservate di cui vengano a conoscenza in ragione delle loro funzioni ufficiali per l'Agenzia".

[^264]: A volte vengono superati da altri trattati. È stato il caso del [General Agreement on Tariffs and Trade (GATT)](https://www.wto.org/english/docs_e/legal_e/gatt47_e.htm) del 1947, a sua volta superato dall'[accordo di Marrakech](https://www.wto.org/english/docs_e/legal_e/marag_e.htm) del 1994, il quale ha recepito le norme del GATT istituendo però l'Organizzazione mondiale del commercio (OMC) in sostituzione della struttura istituzionale del GATT. Anche i trattati a durata illimitata talvolta si estinguono quando le Parti recedono in modo tale da rendere il trattato inefficace. Per esempio, gli Stati Uniti e l'URSS avevano inizialmente stipulato il [Trattato sulle forze nucleari a raggio intermedio (INF)](https://2009-2017.state.gov/t/avc/trty/102360.htm) del 1987 per una durata illimitata, ma gli Stati Uniti sono receduti nel 2019 adducendo l'inadempienza russa, e la Russia ha successivamente annunciato che non si atterrà più al trattato a partire dal 2025.
