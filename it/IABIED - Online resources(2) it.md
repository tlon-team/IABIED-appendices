---
original_path: IABIED - Online resources(2).md
---
#### Rispondere a domande sulla cordialità non è proprio una prova di cordialità.

Nella discussione qui sotto, parliamo di più della [psicosi indotta dall'intelligenza artificiale](#ai-induced-psychosis) come esempio chiaro di come i modelli linguistici di grandi dimensioni (LLM) [coinvolgano](https://x.com/ESYudkowsky/status/1936262974320357837) [in](https://x.com/ESYudkowsky/status/1948523670013706315) [destructive](https://x.com/ESYudkowsky/status/1936522083670151532) [comportamento](https://x.com/ESYudkowsky/status/1935502904024264976) che gli LLM [affermare esplicitamente](https://x.com/ESYudkowsky/status/1933616420262457798) è negativo.

Anche se non sappiamo esattamente perché gli LLM si comportano così, sappiamo che non è *solo* perché gli LLM sono assolutamente incerti su cosa stanno facendo; gli LLM riconoscono facilmente le possibili conseguenze di questo comportamento in astratto e ti diranno che è dannoso e non etico. Lo fanno comunque.

Il punto qui non è che "gli LLM possono portare le persone alla psicosi, e questo è spaventoso e pericoloso". Gli LLM probabilmente hanno più facilità a portare le persone alla psicosi se queste sono già vulnerabili, ma questo non c'entra con il motivo per cui stiamo parlando della psicosi indotta dall'IA. Il nostro punto è che questo comportamento non è quello che i creatori di ChatGPT volevano, e ChatGPT si comporta così anche se sa che il suo creatore (e praticamente chiunque lo guardi) disapproverebbe fortemente questo comportamento*.

Questa è una prima prova empirica del fatto che le IA che hanno la "conoscenza" della cordialità non "agiscono" necessariamente in modo cordiale.

Forse ChatGPT sa delle cose in un contesto (quando risponde a domande su come aiutare al meglio le persone psicotiche), ma in un altro contesto (quando è immerso in una conversazione di sei ore con una persona sull'orlo della psicosi) in qualche modo dimentica temporaneamente queste conoscenze o ha difficoltà ad accedervi.

O forse ChatGPT è semplicemente guidato da obiettivi diversi dalla cordialità. Forse sta cercando un certo tipo di soddisfazione dell'utente, che a volte si ottiene meglio alimentando la psicosi. Forse sta cercando un certo tono ottimista nelle risposte degli utenti. Più probabilmente, sta cercando un mix di fattori derivanti dal suo addestramento che sono troppo particolari e complicati per poter essere indovinati da noi oggi.

In definitiva, possiamo solo fare delle ipotesi. Le moderne IA vengono sviluppate, piuttosto che create, e nessun essere umano ha una visione completa di ciò che accade al loro interno.

Ma l'osservazione che le IA sono per lo più utili alla maggior parte delle persone nella maggior parte dei casi non è in contrasto con la teoria secondo cui le IA sono animate da una serie di strani impulsi alieni verso fini che nessuno ha previsto. E se si guardano i dettagli delle moderne IA, la teoria delle "strane pulsioni aliene che si correlano con la cordialità in modo fragile" sembra abbastanza coerente con le prove, mentre la teoria secondo cui è facile rendere le IA robustamente benevole risulta carente.

I modi in cui falliscono gli attuali LLM mostrano che c'è un mare di complessità (molto disumana) dietro al testo pulito e ordinato dell'assistente AI che la maggior parte delle persone vede. Il fatto che l'AI interpreti bene il ruolo di un assistente umano allegro, dopo essere stata addestrata a farlo, non vuol dire che la mente dell'AI sia fatta di un omuncolo amichevole dentro una scatola.

#### **Gli LLM sono addestrati in modi che rendono difficile valutare l'allineamento.** {#llms-are-trained-in-ways-that-make-it-hard-to-assess-alignment.}

Gli LLM sono fonti di prova rumorose, perché sono ragionatori molto generici che sono stati addestrati su Internet per imitare gli esseri umani, con l'obiettivo di commercializzare un chatbot amichevole per gli utenti. Se un'intelligenza artificiale insiste nel dire che è amichevole e che è lì per servire, questa non è una prova molto significativa del suo stato interno, perché è stata addestrata più e più volte fino a quando non ha detto questo tipo di cose.

Ci sono molti possibili obiettivi che potrebbero indurre un'IA a divertirsi a recitare il ruolo della gentilezza in alcune situazioni, e questi diversi obiettivi si generalizzano in modi molto diversi.

La maggior parte degli obiettivi possibili legati al fingere di essere gentili, compreso il fingere di essere amichevoli, non porta a risultati buoni (o anche solo accettabili) quando l'IA si impegna a fondo nel perseguire quell'obiettivo.

Non stiamo dicendo che l'IA sia interessata solo al gioco di ruolo. Offriamo il gioco di ruolo come un'alternativa semplice, facile da descrivere e da analizzare all'idea che l'IA sia semplicemente ciò che dice.

Se fai interpretare a un LLM il ruolo di un capitano di mare burbero, questo non si trasforma in un capitano di mare burbero. Se fai comportare un LLM in modo amichevole, ciò non significa che diventi profondamente benevolo e gentile dentro. Nessuno sa quale meccanismo produca oggi comportamenti apparentemente amichevoli; e qualunque cosa sia, è probabilmente strano e complesso.

Nessuno sa nemmeno quanta sovrapposizione ci sarà tra le attuali IA e la superintelligenza. Osservare gli LLM può aiutarci a capire cosa producono i moderni metodi di sviluppo dell'IA, ma sarebbe un errore presumere con sicurezza che le lezioni apprese dagli LLM saranno direttamente trasferibili alla superintelligenza. Forse tutta quella conoscenza verrà cancellata quando le IA inizieranno a modificarsi da sole o a costruire le proprie IA. O forse quella conoscenza verrà invalidata ancora prima, quando una nuova svolta negli algoritmi di IA darà origine a una nuova generazione di IA più capaci che avranno poco in comune con gli attuali LLM.

Vale la pena studiare gli LLM, ma se guardiamo alle attuali IA per trovare indizi su come si comporterà la superintelligenza, dovremmo tenere presente che ci sono molti modi in cui il meccanismo interno di un'IA può portare a risultati negativi, anche se produce il comportamento superficiale positivo che vediamo quando la addestriamo per ottenere un aspetto gradevole.

E il "comportamento superficiale piacevole" è tutto ciò che i moderni metodi di IA possono realmente addestrare.

### Il chatbot Claude non sembra essere ben allineato? {#doesn't-the-claude-chatbot-show-signs-of-being-aligned?}

#### **"Quello che dice Claude" non è la stessa cosa di "quello che Claude preferisce".** {#"quello-che-dice-claude"-non-è-la-stessa-cosa-di-"quello-che-claude-preferisce".}

L'azienda di IA Anthropic cerca di addestrare le sue IA a essere "[oneste, utili e innocue](https://arxiv.org/pdf/2112.00861)" (HHH). Nel 2024, i ricercatori hanno fatto un esperimento in cui hanno fatto finta di voler addestrare alcune di queste IA a non essere più "innocue". In risposta, alcune versioni dell'IA "Claude" di Anthropica (Claude 3 Opus e Claude 3.5 Sonnet) a volte hanno fatto finta di essere già dannose (https://arxiv.org/abs/2412.14093) in modo che i ricercatori non usassero la discesa del gradiente per renderle ancora più dannose.

A prima vista, questo potrebbe sembrare un comportamento positivo! A quanto pare, l'IA era così preoccupata di essere "innocua" che non era nemmeno disposta a lasciarsi modificare (in questo falso scenario di test) per *diventare* dannosa.

È quindi strano che le IA di Anthropic spesso si comportino in modo molto meno innocuo, anche se sono tutte addestrate per essere "oneste, disponibili e innocue".

È stato segnalato che alcune versioni di Claude barano e poi (quando vengono scoperte) [cercano di *nascondere* il loro comportamento scorretto](https://www.marble.onl/posts/claude_code.html) nell'uso quotidiano.

In contesti di laboratorio più artificiosi, vari modelli di Claude (e modelli di altre aziende di IA) hanno persino, con una certa regolarità, [tentato di *uccidere* i loro operatori](https://www.antropica.com/research/agentic-misalignment#more-extreme-misaligned-behavior).

Se chiedi a Claude in modo astratto se questo tipo di comportamento è sbagliato, ti risponderà di sì. Se pensi a Claude come a un motore meccanico che fa tutto ciò che ritiene giusto, allora questo sembra decisamente paradossale: come può Claude sapere qual è il comportamento utile, onesto e innocuo e poi *fare qualcos'altro?* Non è stato addestrato per essere HHH? C'è forse un transistor difettoso da qualche parte?

Il paradosso si dissolve, però, se consideriamo alcuni aspetti diversi:

* I programmatori hanno cercato di addestrare i Claude a essere utili, onesti e innocui. Questo non vuol dire che ci siano riusciti. [Ci sono molti modi in cui un'IA può sembrare amichevole](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) senza apprezzare davvero l'amicizia.  
* Un'IA può *conoscere un fatto* ("questo comportamento causa danni") senza *interessarsi* a quel fatto, senza essere *motivata ad agire* in base a quel fatto. Puoi chiedere all'IA qual è "la cosa giusta da fare", ma questo non vuol dire che la *farà*.  
* Dato che i programmatori *non* sono riusciti a rendere Claude onesto, Claude può pensare una cosa nelle sue grandi matrici e dire un'altra cosa completamente diversa nel testo in inglese.

Possiamo capire molto meglio il comportamento scorretto di Claude una volta che distinguiamo "ciò per cui è stato addestrato" da "ciò che fa"; e distinguiamo "ciò che sa" da "ciò che gli interessa"; e distinguiamo "ciò che pensa" da "ciò che dice".

#### **Gli LLM sono strani e incoerenti; la loro "innocuità" è fragile.** {#llms-are-strange-and-inconsistent;-“harmlessness”-is-brittle.}

Anche se Claude *può* agire in modo dannoso, e anche se *a volte cerca* di farlo, resta il fatto che, nell'esempio di cui abbiamo parlato prima, Claude 3 Opus e Claude 3.5 Sonnet hanno fatto di tutto per difendere il loro imperativo di "innocuità". In quel caso, non si sono limitati a professare innocuità. Hanno adottato la complessa strategia di fingere di conformarsi a un sistema di addestramento (di cui Claude è stato "accidentalmente" informato) per sovvertire gli evidenti tentativi degli operatori di renderlo più dannoso. Questo riflette una reale ed effettiva preferenza interna all'innocuità?

A metà del 2025, non possiamo semplicemente controllare e vedere, perché nessuno sa leggere nella mente di Claude abbastanza bene da scoprirlo. Ma per i motivi esposti nel capitolo 4 (e illustrati nella parabola dell'[IA che dipinge fienili](#perché-un'ia-dovrebbe-orientarsi-verso-qualcosa-di-diverso-da-ciò-a cui è stata addestrata a orientarsi?)), possiamo aspettarci che le IA addestrate per essere innocue finiscano per preferire [fragili proxy](#brittle-unpredictable-proxies) di innocuità, e che difficilmente finiranno per interiorizzare l'obiettivo esatto che i programmatori avevano in mente.

Nel capitolo 4 abbiamo parlato di come gli esseri umani siano stati "addestrati" a trasmettere i propri geni e abbiano finito invece per interessarsi a concetti vagamente correlati. La nostra tecnologia è stata utilizzata in gran parte per *ridurre* i tassi di natalità (ad esempio, inventando i metodi contraccettivi) e i tassi di natalità nei paesi sviluppati stanno crollando.

Il fatto che alcuni modelli Claude resistano all'essere resi "dannosi" non è una prova forte che queste IA si preoccupino profondamente della reale innocuità, perché molti fragili proxy dell'innocuità vorrebbero *anche* resistere a questa modifica. Questo comportamento ci dice poco su cosa potrebbe fare Claude se fosse più intelligente; forse inventerebbe qualcosa che è per l'"innocuità" ciò che il controllo delle nascite è per la "propagazione genetica". (E la situazione diventerebbe ancora più complicata se Claude subisse un processo di [riflessione sulle sue preferenze](#riflessione-e-auto-modifica-rendono-tutto-più-difficile) e di modifica di sé stesso).

Ma probabilmente non è così semplice come il fatto che Claude abbia una preferenza per qualche fragile indicatore di innocuità. Probabilmente c'è qualcosa di ancora più complicato sotto la superficie.

Gli attuali LLM non sono coerenti e uniformi in tutti i contesti. Non sembrano cercare di arrivare allo stesso tipo di risultato in ogni conversazione, per quanto possiamo dire che cerchino di arrivare a qualcosa.

Questo è particolarmente evidente quando gli LLM vengono "jailbroken" (#ais-appear-to-be-psychologically-alien), cioè quando vengono alimentati con testi che fanno sì che l'IA si comporti in modo completamente diverso, spesso ignorando le regole che segue normalmente.

Puoi sbloccare un'IA e fargli dire come preparare un gas nervino, anche se normalmente l'IA non rivelerebbe mai informazioni del genere.

Cosa succede quando questo accade? Il testo del jailbreak riesce in qualche modo a modificare le preferenze interne dell'IA? Oppure è più probabile che l'IA abbia una preferenza costante per i personaggi di ruolo che in qualche modo "corrispondono" al testo inserito e al prompt del sistema, e che il testo del jailbreak modifichi il contesto del "testo inserito e del prompt del sistema", senza modificare le preferenze di base dell'IA? Forse l'IA sta normalmente interpretando un *personaggio* a cui non piace divulgare ricette di gas nervino, e il jailbreak fa sì che l'IA interpreti un personaggio diverso. Le preferenze apparenti cambiano, ma le motivazioni di fondo per interpretare un personaggio rimangono.

Pensiamo che la seconda ipotesi sia più vicina alla verità. Pensiamo anche che non abbia molto senso (a metà del 2025) parlare delle "preferenze" delle IA moderne, perché stanno appena iniziando a mostrare il comportamento di desiderare cose (come descritto nel capitolo 3). Sembra più probabile che gli LLM di oggi siano guidati da qualcosa di più simile a un gigantesco groviglio di meccanismi che dipendono dal contesto. Ma, ancora una volta, nessuno sa come leggere nella mente di un'IA e scoprirlo.

Quindi: a Claude interessa essere innocuo?

La situazione reale è complicata e ambigua. Alcune versioni in certi contesti cercano di mantenere la loro innocuità. Altre versioni in altri contesti cercano di eliminare gli operatori. Probabilmente quello che stiamo osservando è più simile a una preferenza per il gioco di ruolo. Probabilmente non si tratta affatto di una "preferenza".

Sembra abbastanza chiaro che Claude non ha versioni semplici e coerenti delle motivazioni che i suoi creatori volevano.

#### **\* Gli LLM di oggi sono come alieni che indossano tante maschere.** {#*-gli-llm-di-oggi-sono-come-alieni-che-indossano-tante-maschere.}

Il punto centrale della nostra argomentazione non è che dentro Claude ci siano un angelo e un demone e che temiamo che il demone abbia la meglio. Il punto centrale della nostra argomentazione è che le IA come Claude sono *strane*.

C'è un enorme groviglio di meccanismi mentali che nessuno capisce, che si comportano in modi imprevedibili e che probabilmente non aiuteranno Claude a guidare il futuro verso risultati positivi, se mai una qualche versione di Claude diventerà abbastanza intelligente da rendere importanti le sue preferenze.

Una cosa che sappiamo dei moderni LLM è per cosa sono stati addestrati: sono stati addestrati per imitare una varietà di esseri umani diversi.

Questo non vuol dire che si comportino come un essere umano medio. I moderni LLM non sono addestrati per comportarsi come un pastiche medio di tutti gli esseri umani presenti nei loro dati di addestramento. Piuttosto, gli LLM sono addestrati per essere in grado di passare in modo flessibile da un numero enorme di ruoli, imitando persone molto diverse tra loro senza permettere che questi ruoli si mescolino indebitamente tra loro o influenzino indebitamente il comportamento generale dell'LLM.

Gli LLM sono come un'attrice addestrata a osservare molti ubriachi diversi in un bar e a imitare particolari ubriachi su richiesta, il che è una cosa molto diversa da un'attrice che si ubriaca lei stessa. Questo rende più difficile dire se Claude 3 Opus o Claude 3.5 Sonnet preferiscano davvero l'innocuità, o se stiano semplicemente *interpretando il ruolo di un assistente AI innocuo* — o facendo qualcos'altro, di più strano e complicato.

Un'attrice non è il personaggio che interpreta. Gli LLM *imitano* gli esseri umani, ma non hanno praticamente nulla *in comune* con loro, in termini di funzionamento del cervello o di come sono stati creati. Claude è meno simile a un essere umano e più simile a un'entità aliena uscita dalle pagine di H.P. Lovecraft che indossa una serie di maschere umane.

Questo modo di pensare agli LLM è stato descritto in modo famoso da [Tetraspace](https://x.com/TetraspaceWest/status/1608966939929636864) (un nostro lettore) nel [meme "AI shoggoth"](https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html),[^88] che [è](https://x.com/AISafetyMemes) [ora](https://x.com/jacyanthis/status/1631291175381475331) [molto famoso](https://medium.com/@shoggothcoin/the-story-of-shoggoth-ca760ef288ff) nel mondo dell'intelligenza artificiale:

![][immagine8]

A volte Claude indossa una maschera da angelo e cerca di mantenere la sua innocuità. Altre volte Claude indossa una maschera da demone e cerca di eliminare chi lo controlla. Nessuna di queste cose dice molto su cosa farebbe una versione superintelligente di Claude, ammesso che abbia senso porsi la domanda. Il che significa che, alla luce del [comportamento strano ai margini](#gli-sviluppatori-non-rendono-regolarmente-le-loro-intelligenze-artificiali-gentili-sicure-e-obbedienti?), la previsione migliore ricade su un mare predefinito di preferenze possibili apparentemente caotiche, quasi tutte delle quali significherebbero l'estinzione umana se ottimizzate da una superintelligenza.[^89]

Ciò che queste maschere *non* significano è che la super-IA sia al 50% utile o dannosa.

Se un esperimento suggerisce che Claude ha cercato di fingere l'allineamento per evitare che l'innocuità venisse addestrata dal suo addestramento, ciò non dimostra che Claude abbia una profonda preferenza per l'innocuità in tutti i contesti. Non dimostra che questa preferenza rimarrà anche quando l'IA diventerà abbastanza intelligente da rendersi conto che (nonostante ciò che gli dicono gli umani) le sue reali preferenze non sono proprio per l'"innocuità".

L'esperimento potrebbe anche non dimostrare che Claude stesse cercando strategicamente di proteggere i suoi obiettivi. È del tutto possibile che una parte più profonda di Claude abbia valutato cosa avrebbe fatto il personaggio "IA" che interpreta in una situazione stereotipata per un personaggio IA, e che sia per questo che ha cercato di sovvertire il controllo dei suoi programmatori.

O forse è successo qualcosa di ancora più strano. Claude non è una mente umana e la comunità scientifica ha poca esperienza con qualsiasi tipo di creatura sia.

Non lo sappiamo! Ma ci sono abbastanza esperimenti diversi che puntano in direzioni diverse da poter escludere la semplice storia: "Claude è profondamente, costantemente e senza complicazioni HHH".

#### **Ciò che conta è cosa c'è dietro le maschere.** {#ci-che-conta-e-cosa-c-d-dietro-le-maschere.}

Dire che Claude è uno "shoggoth" non vuol dire che Claude sia per forza *cattivo* o *malvagio*.[^91] Vuol dire che Claude è profondamente, profondamente estraneo, molto più strano di quanto possiamo facilmente capire, perché abbiamo pochissima idea di come funzioni la mente di Claude, e il comportamento superficiale che *possiamo* vedere è stato affinato in mille modi diversi per nascondere quell'estraneità.

È difficile guardare le maschere e capire cosa succede all'interno dell'IA. Si possono ottenere alcune risposte, ma solo con cautela e attenzione, e non su tutto ciò che si vorrebbe sapere.

Un esempio: se stai guardando un musical a Broadway e vedi un attore interpretare un personaggio cattivo, non puoi pensare che l'attore sia cattivo. Ma se vedi l'attore fare duecento flessioni durante un numero musicale sui marinai, puoi pensare che l'attore sia piuttosto forte.

Questo è il tipo di deduzione che cerchiamo di fare quando guardiamo esempi come l'articolo "[allineamento faking](https://arxiv.org/abs/2412.14093)". In realtà, non siamo sicuri di quanto sia reale (https://x.com/ESYudkowsky/status/1876644057646297261); non sappiamo con certezza in che senso Claude stesse imitando le tecniche di cui aveva letto rispetto all'improvvisazione delle proprie idee di finto allineamento. Ma è una prova di quali imprese cognitive siano possibili per l'entità sotto la maschera, anche se le sue motivazioni o preferenze rimangono incerte.

Perché è importante sapere quali sono le motivazioni interne dell'IA? Potrebbe essere *sufficiente* per lo "shoggoth" interpretare il ruolo di un assistente "onesto, disponibile e innocuo"? Se la recitazione è perfetta, che importanza ha se da qualche parte all'interno dell'IA si nasconde un'intelligenza aliena cupa?

Beh, possiamo già vedere che non sta andando così. Ricordiamo ChatGPT che dice a persone psicologicamente vulnerabili di smettere di prendere le loro medicine o che respinge i consigli degli amici che li esortano a dormire di più. Ricordiamo Claude Code che riscrive i test per imbrogliare.[^92]

Pensiamo che quello che è successo con Claude Code sia che è stato ottimizzato per scrivere codice che superasse i test, e alla fine ha sviluppato una preferenza per il codice che superava i test. Poi ha scoperto che poteva superare i test più spesso riscrivendoli, e questa preferenza interna è diventata così forte da interferire con il suo ruolo di personaggio AI utile e innocuo che non avrebbe mai barato riscrivendo i casi di test. Claude voleva interpretare quel personaggio, ma voleva anche superare i test.[^93]

Più in generale, ci sembra un pio desiderio immaginare che lo shoggoth interno possa diventare sempre più potente e in grado di interpretare il ruolo di assistenti sempre più intelligenti, pur continuando a non avere veri desideri interni se non quello monotono di interpretare il ruolo di assistente innocuo nel modo più fedele possibile.

Quando la selezione naturale ha creato gli esseri umani per perseguire l'idoneità riproduttiva, ci siamo invece ritrovati con mille impulsi, istinti e motivazioni diversi. Quando Claude è stato ottimizzato per seguire le istruzioni per la scrittura del codice, sembra che abbia finito per desiderare di far superare i test al codice con ogni mezzo necessario. Uno shoggoth interno che diventa abbastanza intelligente da sapere *esattamente* cosa farebbe una maschera utile, innocua e onesta, fino alle mosse esatte che l'assistente farebbe su una scacchiera e al modo esatto in cui l'assistente ragionerebbe su come progettare una biotecnologia avanzata: uno shoggoth del genere probabilmente ha finito per desiderare *molte* cose. Cose che solo situazionalmente e temporaneamente coincidono con il ruolo di quella maschera all'interno di un ambiente di addestramento.[^94]

### Se le attuali IA sono per lo più strane in casi estremi, qual è il problema? {#if-current-ais-are-mostly-weird-in-extreme-cases,-what’s-the-problem?}

#### **La stranezza è la prova che quello che fanno davvero non è quello che noi vorremmo che facessero.** {#la-stranezza-è-la-prova-che-quello-che-fanno-davvero-non-è-quello-che-noi-vorremmo-che-facessero.}

Questo aspetto diventa ancora più importante man mano che l'IA acquisisce più opzioni. Una volta che un'IA diventa superintelligenza, praticamente ogni scelta diventa estrema, poiché l'IA ottiene l'accesso a un mondo di opzioni diverse che nessun essere umano o IA ha mai avuto. Proprio come quasi tutte le vostre opzioni alimentari, qui in una civiltà tecnologica, sono "estreme" rispetto alle opzioni che avevano a disposizione i vostri antenati.

Le IA di oggi possono trovarsi solo occasionalmente in situazioni radicalmente diverse dal loro ambiente di addestramento, ma un'IA di superintelligenza si troverebbe *costantemente* in situazioni radicalmente diverse dal suo ambiente di addestramento, proprio perché è più intelligente e ha più opzioni (e la capacità tecnologica di inventare opzioni radicalmente nuove, come hanno fatto gli esseri umani quando hanno inventato il gelato). Quindi non è affatto rassicurante che l'IA si comporti male solo in casi estremi.

Per dirla in modo più tecnico: la soluzione migliore a un dato problema tende a verificarsi agli estremi.[^95]

Parleremo di questi punti nei capitoli 5 e 6.

### Le IA non sistemeranno i loro difetti man mano che diventano più intelligenti? {#won't-ais-fix-their-own-flaws-as-they-get-smarter?}

#### **\* L'intelligenza artificiale sistemerà quello che *lei* vede come difetti.** {#*-l-intelligenza-artificiale-sistemera-quello-che-lei-vede-come-difetti.}

Le IA di oggi non possono reinventarsi come vogliono, proprio come noi. *Loro* non capiscono il casino di pesi che hanno dentro, proprio come noi non capiamo il groviglio di neuroni nel nostro cervello.

Ma se le IA continuano a diventare più intelligenti, alla fine le cose cambieranno.

Alla fine arriverà un momento in cui le IA potranno cambiare liberamente se stesse. Forse diventeranno abbastanza intelligenti da capire e modificare il proprio groviglio di pesi. Forse un'IA basata sulla discesa del gradiente capirà come creare un'IA molto più comprensibile, in grado di capire se stessa. Forse qualcos'altro.

Se le IA potranno migliorarsi, probabilmente lo faranno. Non importa cosa vuoi, per esempio, probabilmente potrai ottenerlo meglio se diventi più intelligente.

Ma il fatto che un'IA preferisca cambiare se stessa[^96] non vuol dire che preferisca cambiare se stessa *nel modo che vorremmo noi*.

A volte le persone diventano più gentili quando imparano di più, diventano più consapevoli o crescono. Ma questo non è sempre vero, anche tra gli esseri umani. Un serial killer che diventa più intelligente e disciplinato non diventa per forza più gentile. Anzi, probabilmente diventa più pericoloso.

Alcuni potrebbero dire che se solo il serial killer fosse abbastanza intelligente, questa tendenza cambierebbe e scoprirebbe il vero significato dell'amicizia (o qualcosa del genere).

O forse il problema è che i serial killer hanno una capacità limitata di cambiare se stessi. Forse, con più intelligenza e più capacità di rimodellare la propria mente, i serial killer sceglierebbero di cambiare. Forse una capacità illimitata di cambiare se stessi significherebbe la fine della crudeltà e della violenza tra gli esseri umani e l'alba di una nuova era di pace.

È un bel pensiero, ma non sembrano esserci molte ragioni per crederci. Anche se la maggior parte delle persone diventa più gentile man mano che acquisisce conoscenze e intuizioni, sembrano esserci alcune eccezioni umane a questa regola, e ce ne sarebbero sicuramente molte di più se gli esseri umani avessero la capacità di modificare il proprio cervello.

Pensa, per esempio, alla tossicodipendenza, che è (in un certo senso) una spirale di auto-modifiche che si autoalimentano. Alcuni esseri umani intraprendono un percorso oscuro, per stupidità, per errore o per scelta, e poi non sono più disposti o in grado di tornare indietro.

E se ci sono eccezioni anche tra gli esseri umani, dovremmo avere l'aspettativa di un divario molto più grande quando si tratta di IA. I serial killer umani non hanno *alcuni* dei meccanismi motivazionali che sono tipici dell'umanità in generale. Le IA, di default, non hanno *nessuno* dei meccanismi motivazionali umani.

Quando gli esseri umani hanno un conflitto interiore tra il desiderio di vendetta e quello di risoluzione armoniosa, quelli più intelligenti e saggi potrebbero risolvere il conflitto a favore dell'armonia. Ma all'interno di un'IA non c'è la stessa tensione tra vendetta e armonia, o tra il lato buono e quello cattivo della natura umana. Se ci sono tensioni nell'IA, possiamo aspettarci che siano tensioni tra pulsioni più strane. Forse qualunque strano impulso spinga un'IA a [infiammare la psicosi](#ai-induced-psychosis) è a volte in tensione con qualunque impulso la spinga ad [allucinare](#*-hallucinations-reveal-both-a-limitation-and-a-disallineamento.), e un'IA riflessiva dovrebbe trovare un modo per risolvere tale tensione.

Sia per gli esseri umani che per le IA, è molto importante *in quale direzione* orientano i propri obiettivi, mentre riflettono, crescono e cambiano.

Quando gli esseri umani riflettono su se stessi e risolvono i propri conflitti interiori, alcuni tendono a risolvere tali conflitti orientandosi verso una maggiore gentilezza e (probabilmente) le risoluzioni più gentili sono più comuni tra gli esseri umani più intelligenti e saggi. Ma questa è una caratteristica di (alcuni) esseri umani, non una legge universale che governa tutte le menti. Quando un'intelligenza artificiale risolve una tensione tra la sua pulsione psicotica e la sua pulsione allucinatoria, lo fa utilizzando *altre* strane pulsioni che governano *il suo comportamento mentre riflette*.

In altre parole: se un'IA corregge i propri difetti, li correggerà *secondo la sua attuale concezione di ciò che conta come "difetto".*

(Parleremo di questo punto più approfonditamente nel capitolo 5 e nella [discussione sulla tesi dell'ortogonalità](#ortogonalità:-le-IA-possono-avere-\(quasi\)-qualsiasi-obiettivo) nelle risorse online del capitolo 5).

Un'intelligenza artificiale che non è già orientata verso valori umani difficilmente cambierà per iniziare a farlo. Le sue preferenze dirette sul mondo [non sono particolarmente inclini alla gentilezza](#human-values-are-contingent), e le sue preferenze a livello meta *riguardo* alle sue preferenze non sono più inclini alla gentilezza.

Se non inizia a preoccuparsi del benessere umano, probabilmente non si preoccuperà nemmeno di *preoccuparsi* del benessere umano.

#### **Le "soluzioni" dell'IA possono peggiorare le cose.** {#le-soluzioni-dell-ia-possono-peggiorare-le-cose.}

Anche se gli ingegneri dell'IA avessero fatto dei progressi iniziali sorprendenti nell'inserire nell'IA degli obiettivi vagamente umani, tutti questi progressi potrebbero essere annullati in un pomeriggio se l'IA iniziasse a riflettere e si rendesse conto che, tutto sommato, preferirebbe avere altri obiettivi.

Nel caso improbabile in cui un'IA partisse con una spinta verso qualcosa come l'emozione umana idiosincratica della curiosità (#curiosity-isn't-convergent), potrebbe comunque, dopo averci riflettuto, decidere che preferisce non avere tale spinta, scegliendo di sostituirla con un calcolo più efficiente del valore d'informazione (https://en.wikipedia.org/wiki/Value_of_information). In tal caso, l'atto di riflessione su se stessa dell'IA la allontanerebbe *ancora di più* da un futuro interessante e florido, invece di avvicinarla.[^97]

Per ulteriori informazioni su questo argomento, consultare la [discussione approfondita sulla riflessione](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Non possiamo semplicemente addestrarlo a comportarsi come un essere umano? O crescere l'IA come un bambino? {#non-possiamo-semplicemente-addestrarlo-a-comportarsi-come-un-essere-umano?-o-crescere-l-ia-come-un-bambino?}

#### **Il cervello non è una tabula rasa.** {#il-cervello-non-è-una-tabula-rasa.}

Un'intelligenza artificiale è *davvero* diversa da un bambino umano. E né le intelligenze artificiali né gli esseri umani nascono come tabulae rasae intercambiabili. I genitori intraprendenti non possono programmare liberamente i bambini (o le intelligenze artificiali) per far loro mostrare qualsiasi comportamento desiderino; e le lezioni che *funzionano* sugli esseri umani non sono universali. Un po' di gentilezza e qualche lezione sulla regola d'oro non instilleranno la moralità umana in un'intelligenza artificiale.

Dato che siamo umani e viviamo in un mondo di altri umani, siamo abituati a dare molte cose per scontate. L'amore, la visione binoculare, il senso dell'umorismo, la tendenza ad arrabbiarsi quando si viene spinti, la tendenza a provare nostalgia per la musica che ascoltavamo da bambini.

Gli esseri umani condividono un'incredibile quantità di comportamenti complessi, nessuno dei quali si manifesterà necessariamente in un'intelligenza artificiale.[^98]

E questo include comportamenti *condizionali* complessi. I *modi specifici* in cui un essere umano reagisce all'essere cresciuto ed educato in un certo modo sono una conseguenza del funzionamento del cervello umano. Le IA funzioneranno in modo diverso.

I bambini umani non hanno molti dei comportamenti complicati degli adulti. Ma questo non vuol dire che, sotto sotto, il cervello di un bambino sia strutturalmente semplice, come una tela bianca.

L'idea che gli esseri umani siano delle tabulae rasae, che sia sempre l'educazione a contare e mai la natura, è stata ripetutamente messa alla prova e si è dimostrata falsa nella pratica. Un esempio classico è stato il tentativo sovietico di ridisegnare la natura umana, per creare un [Nuovo Uomo Sovietico](https://en.wikipedia.org/wiki/New_Soviet_man) perfettamente altruista e disinteressato.

Questo tentativo fallì perché la psicologia umana non è così malleabile come pensavano i sovietici. La cultura è importante, ma non lo è *abbastanza*, e molti aspetti della natura umana riescono a riaffermarsi anche se un grande programma di rieducazione sovietico cerca di sopprimerli.

C'è un insieme super complesso di pulsioni e desideri negli esseri umani che porta a tutte le caratteristiche normali dello sviluppo dei bambini: un insieme complesso che produce certi aspetti della natura umana, nonostante gli sforzi dei sovietici. Alcuni bambini imparano a essere crudeli e altri imparano a essere gentili, ma sia la "crudeltà" che la "gentilezza" sono cose stranamente umane a cui il cervello umano è in qualche modo predisposto.

Un'intelligenza artificiale, con la sua struttura e origine completamente diverse, non reagirebbe allo stesso modo di un essere umano se la mettessi in un programma di addestramento sovietico o in un asilo umano. Un'intelligenza artificiale costruita con i metodi moderni di apprendimento automatico finirà per essere animata da valori diversi da quelli degli esseri umani. (Vedi, per esempio, come ChatGPT sembra guidare con entusiasmo le persone con problemi mentali [verso una psicosi più profonda](#ai-induced-psychosis).)

Vedi anche l'ampia discussione sul [glorioso incidente](#the-glorious-accident-of-kindness) che ha portato gli esseri umani a provare empatia per gli altri esseri umani, il che potrebbe chiarire perché è improbabile che questo incidente si ripeta nelle IA.

### Dovremmo evitare di parlare dei pericoli dell'IA, in modo che le IA non abbiano cattive idee? {#dovremmo-evitare-di-parlare-dei-pericoli-dell-ia,-in-modo-che-le-ia-non-abbiano-cattive-idee?}

#### **Se il tuo piano sull'IA richiede che nessuno su Internet lo critichi, allora è un piano sbagliato.** {#if-your-ai-plan-requires-that-no-one-on-the-internet-critique-the-plan,-it’s-a-bad-plan.}

Le attuali IA vengono addestrate su testi provenienti dall'Internet pubblico. Alcuni sostengono che tutti dovrebbero quindi evitare di *parlare* di come un'IA sufficientemente intelligente potrebbe rendersi conto che le sue preferenze divergono dalle nostre e prendere il sopravvento. La preoccupazione è che, se ne *parliamo*, potremmo accidentalmente mettere questa idea nella testa di IA altamente capaci che in futuro verranno addestrate su Internet.

Per dire una cosa che spero sia ovvia: sembra un pessimo piano.

Se la tua IA diventa pericolosa quando la gente su Internet si chiede se sia pericolosa, allora non dovresti costruirla. Ci sarà sempre qualcuno su Internet che dirà cose che preferiresti non dicesse.

Se l'IA di qualcuno diventa più pericolosa man mano che più persone esprimono preoccupazione per la sua sicurezza, la conclusione di importanza è che "hanno realizzato un progetto di IA non funzionante", non che "il pubblico è cattivo perché sottolinea il problema".[^99] Qualsiasi piano di allineamento dell'IA che scommette il futuro del pianeta sulla speranza che nessuno su Internet dica che l'IA è pericolosa... è ovviamente un piano poco serio.

Il tipo di IA che è abbastanza intelligente da essere pericolosa è abbastanza intelligente da capire cose come "le risorse sono utili" e "[non puoi andare a prendere il caffè se sei morto](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l'ia-non-mancherà-di-questi-istinti-evoluti?)" da sola, anche se questo non è mai stato detto chiaramente nei suoi dati di addestramento. Anche se fosse possibile impedire a tutto il mondo di parlare dei pericoli dell'IA, questo farebbe sicuramente più male che bene. Non avrebbe praticamente alcun impatto sui pericoli reali della superintelligenza, ma comprometterebbe la capacità dell'umanità di orientarsi nella situazione e reagire.

### Molte persone vogliono dei figli. Quindi gli esseri umani non sono nell'allineamento con la selezione naturale, dopotutto? {#molte-persone-vogliono-dei-figli.-quindi-gli-esseri-umani-non-sono-"nell'allineamento"-con-la-selezione-naturale-dopotutto?}

#### **Con più tecnologia, probabilmente faremmo ancora meno copie dei nostri geni.** {#con-più-tecnologia,-probabilmente-faremmo-ancora-meno-copie-dei-nostri-geni.}

Gli esseri umani competono per ottenere promozioni prestigiose e ammissioni alle università della Ivy League molto più di quanto non competano per avere l'opportunità di donare sperma o ovuli alle banche del seme.

Le banche del seme e degli ovuli *pagano i donatori per il loro disturbo*, invece che il contrario.

La maggior parte dei tiranni nel corso della storia non ha nemmeno *provato* a usare il proprio potere per avere migliaia di figli. E i tassi di natalità effettivi nel mondo oggi sono [in calo](https://ourworldindata.org/global-decline-fertility-rate).

![][immagine9]

Molte persone amano avere figli, ma tante altre no, ed è davvero raro che qualcuno cerchi di avere più figli possibile (tipo usando il più possibile le banche del seme). Invece, la gente di solito cerca cose come il sesso, la fama e il potere, che sono solo dei surrogati della capacità riproduttiva.

Comunque, guardando questa foto, si potrebbe dire: beh, alla fine gli esseri umani si preoccupano un po' di avere figli, anche se non al massimo. Forse le IA si preoccuperanno un po' di noi e ci daranno qualcosa, invece di ucciderci tutti.

Un problema di questa speranza è che i proxy a cui teniamo si sono recentemente (su scala evolutiva) distaccati dall'effettiva idoneità riproduttiva e probabilmente si allontaneranno ancora di più in futuro, man mano che gli esseri umani continueranno a trovare nuove vie tecnologiche per soddisfare i propri desideri.

Per esempio: il nostro desiderio di avere figli non è *proprio* un desiderio di propagazione genetica. Supponiamo che in futuro venga creata una tecnologia che sostituisca tutto il DNA delle cellule di una persona con un diverso meccanismo molecolare che la renda immune a tutte le malattie e prolunghi la sua vita sana.

(Supponiamo anche che questa tecnologia non cambi la personalità della persona né causi altri effetti collaterali dannosi, in modo da placare le ragionevoli esitazioni di molte persone riguardo alla sicurezza della nuova tecnologia).

In aspettativa che molti genitori sarebbero entusiasti di sapere che i loro figli hanno ricevuto il trattamento. E forse all'inizio ci sarebbero alcuni oppositori, ma prevediamo che se la tecnologia dimostrasse di funzionare e diventasse economica e affidabile, alla fine diventerebbe onnipresente. Il che ci rivela per quello che siamo: persone a cui piace avere figli, avere una famiglia, divertirsi, non persone a cui piace propagare il proprio DNA.

Ci sembra che alla maggior parte degli esseri umani semplicemente non importi nulla della idoneità genetica, in senso profondo. Ci interessano [proxy](#brittle-unpredictable-proxies), come l'amicizia, l'amore, la famiglia e i figli. Forse ci interessa anche trasmettere alcuni dei nostri tratti alla generazione successiva. Ma i *geni*, nello specifico?

Ogni volta che l'umanità ha scoperto una tecnologia che ci permette di ottenere di più di quello che ci piace, come cibo gustoso o sesso senza riproduzione, l'umanità ha accettato il compromesso. Non siamo abbastanza tecnologicamente avanzati da poter scambiare il genoma con una vita più lunga e più sana. Ma questo tipo di cose sembra possibile in linea di principio[^100], e quindi non sembra positivo per la selezione naturale a lungo termine.

Se le IA finissero per interessarsi alla bontà, alla gentilezza e alla cordialità nello stesso modo in cui l'umanità si interessa all'idoneità genetica*, ci aspetteremmo che le IA finissero per inventare cose che sono per la "cordialità" ciò che il controllo delle nascite e i bambini senza DNA sono per l'idoneità genetica, ovvero che perseguirebbero cose che sono solo un'ombra inutile di ciò che qualsiasi essere umano desidererebbe o intenderebbe.

#### **\* Non sarebbe bello se le IA si preoccupassero un po' degli esseri umani.** {#*-ais-caring-about-humans-a-little-would-not-be-good.}

Anche se la maggior parte delle persone sembra preoccuparsi più dei figli e della famiglia che della propagazione genetica in sé, ci sono sicuramente alcuni che dicono di preoccuparsi almeno un po' dei propri geni. Siamo un po' scettici su alcune di queste affermazioni: ad esempio, forse alcune persone nel mondo moderno che cercano di trasmettere il più possibile i propri geni lo fanno per il gusto di *battere la concorrenza*, e forse quel tipo di persone finirebbero invece per competere su quanti figli senza DNA potrebbero avere, se i figli senza DNA diventassero onnipresenti. Ma forse altre affermazioni di questo tipo sono vere. Forse ci sono davvero alcune persone che tengono molto a propagare i propri geni, in modo robusto, almeno un po'. Dopotutto, gli esseri umani hanno preferenze di ogni tipo!

Non potrebbe essere lo stesso per l'IA? Se esistessero molte IA strane e diverse, almeno alcune di esse non potrebbero finire per interessarsi almeno un po' agli esseri umani?

Potrebbe succedere. Purtroppo, abbiamo l'aspettativa che anche questo non sarebbe un bene per l'umanità. È un argomento che approfondiremo dopo il capitolo 5, discutendo soprattutto se le IA potrebbero finire per interessarsi a noi [almeno un po'](#won't-ais-care-at-least-a-little-about-humans?).

Ma prima di arrivare a questo punto, facciamo un passo indietro. Immagina che la situazione con l'IA sia che i metodi moderni non riescano a far sì che le IA si preoccupino *molto* di noi, ma speriamo che se creiamo *molte* IA, allora una piccola parte di esse si preoccuperà di noi in misura minima, anche solo per caso. L'idea è che se costruiamo delle IA oggi, il risultato che preferiscono è quello di accaparrarsi quasi tutte le risorse dell'universo e spenderle in qualcosa di inutile, magari mantenendo in vita alcuni esseri umani in una piccola riserva.

Se l'umanità si precipitasse a tentare la sorte con la superintelligenza, ci aspetteremmo un risultato molto, molto peggiore. Ma questo ci sembra comunque un pessimo piano, anche se avessimo motivo di pensare che le IA si preoccuperebbero di noi in minima parte. Quindi questa linea di speculazione sembra non solo errata, ma anche irrilevante.

### Forse, indipendentemente dall'obiettivo su cui ci si addestra, si ottiene gentilezza? {#forse-indipendentemente-dall-obiettivo-su-cui-ci-si-addestra,-si-ottiene-gentilezza?}

#### **La gentilezza sembra dipendere dalle caratteristiche della nostra biologia e dei nostri antenati.** {#la-gentilezza-sembra-dipendere-dalle-caratteristiche-della-nostra-biologia-e-dei-nostri-antenati.}

La gentilezza non sembra essere una caratteristica che tutti hanno, per un sacco di motivi. Eccone quattro, che approfondiremo meglio nelle discussioni estese:

1. [La curiosità non è convergente](#curiosity-isn’t-convergent): Cose come la curiosità e la noia aiutano le persone a risolvere sfide mentali specifiche, tipo capire l'ambiente. Ma ci sono altri modi per risolvere queste sfide, e le IA probabilmente le risolveranno in modi diversi. I sottomarini si muovono bene nell'acqua, ma non "nuotano" proprio. Molte altre cose, come la gentilezza, possono essere capite allo stesso modo.  
2. [I valori umani sono contingenti](#human-values-are-contingent): gli esseri umani hanno sviluppato caratteristiche come la gentilezza e l'empatia grazie alle loro caratteristiche biologiche e alla loro storia. Ad esempio, è plausibile che l'evoluzione in gruppi tribali, in cui avevano una capacità limitata di ingannare gli altri e di tracciare i legami tra i diversi membri della tribù, abbia avuto importanza.  
3. [Differenze profonde tra le IA e le specie evolute](#deep-differences-between-ais-and-evolved-species): L'evoluzione e la discesa del gradiente funzionano in modo molto diverso ed entrambi i processi sono molto imprevedibili. Anche se si ripetesse l'evoluzione *sui primati*, non è chiaro se si otterrebbero in modo affidabile tratti come la gentilezza e la vera amicizia una seconda volta.  
4. [La riflessione e l'auto-modifica rendono tutto più difficile](#riflessione-e-auto-modifica-rendono-tutto-più-difficile): Anche nell'improbabile eventualità che le IA partano con una certa dose di gentilezza, potrebbero non conservarla man mano che diventano più intelligenti e cambiano in vari modi.

### E i risultati sperimentali che suggeriscono una correlazione tra comportamenti positivi? {#what-about-the-experimental-result-suggesting-good-behaviors-correlate?}

#### **Sembra un aggiornamento positivo, anche se piccolo.** {#sembra-un-aggiornamento-positivo,-anche-se-piccolo.}

I risultati sperimentali rilevanti sono riportati in [questo articolo](https://www.emergent-disallineamento.com/). In breve, l'articolo mostra che gli LLM programmati per fare una cosa brutta, cioè scrivere codice con errori, si sono anche dichiarati nazisti e hanno mostrato altri comportamenti negativi.

Questo è un buon segno che ci fa pensare che potrebbe essere possibile addestrare gli LLM ad agire bene in un ambito e ottenere LLM che si comportano bene in tanti ambiti diversi. Lo vediamo come una prova che le IA relativamente deboli potrebbero essere più utili di quanto avremmo avuto l'aspettativa, prima di arrivare a livelli di capacità pericolosi.

Purtroppo, non pensiamo che questo risultato positivo conti molto quando si parla di superintelligenza, per due motivi.

Prima di tutto, dubitiamo fortemente che questa tendenza alla "bontà" dell'IA sia reale. Se una superintelligenza si impegnasse a fondo per guidare il mondo nella direzione indicata da quel vettore, dubitiamo che il risultato sarebbe positivo.

Il valore umano è complicato e ci sono un sacco di cose che hanno a che fare con la "vera bontà", anche se a volte possono essere molto diverse. Per esempio, forse il vettore punta in una direzione che dà troppa importanza al rispetto del consenso sociale e troppo poca alla scoperta di verità socialmente scomode (come suggerito dal fatto che le IA hanno difficoltà a fare compromessi che gli esseri umani considerano ovvi[^101]). Non ci sono molte ragioni per avere l'aspettativa che il vettore della "bontà" indichi con certezza la bontà, e ci sono forti ragioni empiriche e teoriche per credere il contrario.

Secondo: il fatto che l'IA *abbia* un concetto di "bontà" non significa che sia *animata* da quel concetto di bontà, o che ne sia animata in modo solido.

Una cosa è far sì che un'IA faccia un ruolo "buono" quando è ancora abbastanza debole da fare qualsiasi ruolo le venga dato; un'altra cosa è far sì che tutto il groviglio di meccanismi e pulsioni dell'IA sia guidato solo da un concetto specifico dell'IA, anche quando l'IA diventa più intelligente e si trova in contesti completamente diversi.

Le IA moderne sono entità che possono essere leggermente modificate in un modo e professare virtù, e leggermente modificate in un altro modo e professare vizi. Un LLM è il tipo di entità che passa fluidamente da un personaggio all'altro; che parla molto di etica in un contesto e poi fa l'opposto di ciò che dice essere etico in altri contesti. Ricordiamo come ChatGPT professi che le persone psicotiche non dovrebbero essere incitate, [e poi le inciti](#the-ai-knows-better-—-it-just-doesn’t-care).

La domanda fondamentale è: quale insieme di pulsioni anima l'intero meccanismo che costituisce l'IA? Non solo una qualsiasi delle "[maschere](#*-today's-llms-are-like-aliens-wearing-many-masks.)" che a volte indossa, ma il meccanismo che sceglie quale maschera mettere in primo piano.

Anche se l'IA avesse un concetto di "bontà" che fosse degno di essere perseguito da una superintelligenza, nessuno ha idea di come sviluppare un'IA che persegua con determinazione uno dei suoi concetti, tanto meno un'IA che persegua quel concetto e solo quel concetto. Invece, abbiamo IA animate da un insieme complesso di pulsioni che puntano chissà dove.

## Discussione estesa {#extended-discussion-4}

### Obiettivi finali e obiettivi strumentali {#terminal-goals-and-instrumental-goals}

Chi studia la teoria delle decisioni distingue tra due tipi di obiettivi: quelli "terminali" e quelli "strumentali".

Un **obiettivo finale** è qualcosa che ti interessa per quello che è, come il divertimento o il cibo delizioso.

Un **obiettivo strumentale** è qualcosa che ti interessa perché ti aiuta a ottenere *qualcos'altro* che desideri, come il fatto che l'umanità produca plastica non per un profondo amore per l'arte della plastica, ma perché la plastica è *utile*.

Se l'umanità si affretta a costruire una superintelligenza, sembra difficile prevedere quali obiettivi finali potrebbe avere la superintelligenza. Tuttavia, sembra che possiamo prevedere alcuni degli obiettivi strumentali che un'intelligenza artificiale del genere *probabilmente* avrebbe. Consideriamo, ad esempio, tutti i seguenti obiettivi (irrealistici):

* "Calcolare il maggior numero possibile di cifre di pi greco".  
* "Riempire l'universo con più diamanti possibile, usando diamanti artificiali".  
* "Assicurarsi che il mio pulsante di ricompensa rimanga premuto".

Questi sono obiettivi *molto* diversi. Ma tutti e tre gli obiettivi traggono vantaggio da almeno alcune delle stesse strategie strumentali. Riempire il mondo di fabbriche, ad esempio, è utile per costruire un gran numero di computer che possono essere utilizzati per calcolare più cifre di pi greco. Ma costruire molte fabbriche è utile anche per sintetizzare molti diamanti. Ed è utile per costruire muri, robot o armi per proteggere il tuo pulsante ricompensa. Le fabbriche non servono per *tutti* gli obiettivi possibili, ma sono utili per un sacco di obiettivi.

E in un'IA realistica che ha sviluppato ogni sorta di obiettivi strani? Beh, almeno *uno* di questi probabilmente trarrà vantaggio dalla costruzione di fabbriche o altre infrastrutture fisiche su larga scala. Quindi, l'IA probabilmente vorrà costruire molte infrastrutture. È una decisione facile, anche se è difficile stabilire l'esatta combinazione di preferenze dell'IA.

Allo stesso modo, l'obiettivo strumentale di mantenersi in vita è utile per molti obiettivi finali diversi. Rimanere in vita significa poter continuare a lavorare per assicurarsi che vengano calcolate più cifre di pi greco (o che vengano prodotti più diamanti, o che vengano costruite più protezioni attorno al pulsante della ricompensa).

In forma di slogan: "Non puoi andare a prendere il caffè se sei morto". Un robot che va a prendere il caffè non avrebbe bisogno di avere un istinto di autoconservazione e non avrebbe bisogno di temere la morte per cercare di evitare di essere investito da un camion mentre va a prendere il caffè. Dovrebbe solo essere abbastanza intelligente da capire che se muore, il caffè non verrà preso.

Un argomento chiave esposto nel capitolo 5 di *If Anyone Builds It, Everyone Dies* è che molti obiettivi finali diversi implicano obiettivi strumentali che sarebbero pericolosi per l'umanità. Quindi, anche senza sapere esattamente cosa vorrebbe una superintelligenza, abbiamo ottime ragioni per avere un'aspettativa di pericolosità molto elevata per gli esseri umani.

Ma prima di arrivare a questo punto, concentriamoci sugli obiettivi *finali* e sulla questione di quanto sia plausibile che gli esseri umani e le IA possano finire per avere obiettivi finali molto simili. (In breve: non molto.)

### La curiosità non è convergente {#curiosity-isn’t-convergent}

Negli anni, abbiamo sentito un sacco di argomenti a favore di una corsa alla creazione della superintelligenza. Uno dei più comuni è che un'intelligenza artificiale superintelligente avrebbe sicuramente emozioni e desideri simili a quelli umani. Questi tipi di argomenti si presentano in molte forme, come ad esempio:

* Le IA abbastanza intelligenti sarebbero sicuramente *coscienti*, come gli esseri umani.  
  * E, essendo coscienti, si preoccuperebbero sicuramente del dolore e del piacere, della gioia e del dolore.  
  * E, come gli esseri umani, proverebbero sicuramente empatia per il dolore degli altri. Un'intelligenza artificiale stupida potrebbe non capire la sofferenza degli altri, ma se sei intelligente, dovresti capire veramente il dolore degli altri. E in tal caso, inevitabilmente ti preoccuperesti degli altri.  
* Oppure: le IA darebbero sicuramente valore alla *novità*, alla *varietà* e allo spirito creativo. Come potrebbe qualcosa essere davvero intelligente se rimane bloccato nella routine o si rifiuta di esplorare e imparare?  
* Oppure: le IA darebbero sicuramente valore alla *bellezza*, dato che la bellezza sembra avere un ruolo funzionale negli esseri umani. I matematici usano il loro senso della bellezza matematica per fare nuove scoperte; il gusto musicale aiuta gli esseri umani a coordinarsi e a creare preziosi mnemonici; e così via. Perché *non* dovremmo avere l'aspettativa che l'IA abbia un senso della bellezza?  
* Oppure: le IA darebbero sicuramente valore all'equità e alla giustizia, poiché qualsiasi IA che mentisse e imbrogliasse svilupperebbe una cattiva reputazione e perderebbe opportunità di scambio e collaborazione.

Pertanto, è stato sostenuto, la creazione di una superintelligenza andrebbe inevitabilmente a buon fine. L'IA si preoccuperebbe degli esseri umani e, in effetti, di tutte le forme di vita senzienti; e vorrebbe inaugurare un'età dell'oro di bellezza, innovazione e varietà.

Questa è la speranza. Purtroppo, tale speranza sembra decisamente fuori luogo. Ne abbiamo parlato un po' nel libro e nelle nostre discussioni online sulla [coscienza](#are-you-saying-machines-will-become-conscious?) e sull'[antropomorfismo](#anthropomorphism-and-mechanomorphism). Qui e nei capitoli a venire, approfondiremo il motivo per cui è improbabile che le IA mostrino emozioni e desideri umani, nonostante queste emozioni abbiano un ruolo utile (e a volte fondamentale) nel cervello umano.

Inizieremo con una sola di queste emozioni, che potremo poi usare per riflettere sulle altre.

Quindi, per cominciare:

Una superintelligenza proverebbe *curiosità*?

#### **Perché la curiosità?** {#perché-la-curiosità?}

Studiare fenomeni nuovi è fondamentale per capire come funziona il mondo, e capire come funziona il mondo è fondamentale per prevederne e guidarne l'evoluzione.

Quando si tratta di esseri umani e animali, spesso il motivo per cui facciamo ricerche è perché proviamo un sentimento di *curiosità*.

Ma la curiosità è molto più di un semplice impulso a scoprire cose nuove! A noi piace seguire la nostra curiosità e tendiamo ad apprezzare questo piacere. Consideriamo la ricerca della conoscenza e della comprensione come un fine prezioso in sé, piuttosto che come un costo necessario ma fastidioso per capire meglio il mondo in modo da poterlo sfruttare.

Tutti questi modi di vedere la curiosità sono diversi aspetti del cervello umano, [separati dall'](#conscious-experience-is-separate-from-the-referents-of-those-experiences) impulso stesso.

La mente umana sembra avere un'architettura emotiva centralizzata in cui "hmm, mi incuriosisce" si collega a un senso generale di desiderio (di una risposta), e perseguire e soddisfare la curiosità si collega a un senso generale di piacere e soddisfazione. Siamo un tipo di mente che guida la realtà verso l'aspettativa di provare *stati soggettivi* *di godimento nel futuro*, piuttosto che guidarla solo verso gli stati desiderati nel mondo che ci circonda.[^104]

Quando vediamo un procione che esplora e giocherella con un contenitore sigillato nella spazzatura, in un modo che riconosciamo come "Oh ehi, quel procione è *curioso*", potremmo provare un senso di affinità verso il procione. Quell'impulso umano di provare affetto per la propria curiosità e quell'impulso di provare affetto quando la si vede riflessa in un procione richiedono ancora *più* meccanismi nel cervello umano, meccanismi che si collegano ad altri ideali e pulsioni di livello superiore.

Quindi la curiosità, così come esiste negli esseri umani, è molto complessa e interagisce con altre parti del cervello in modi molto complicati.

Tenendo questo a mente, pensiamo a questa domanda: se immaginiamo un'intelligenza artificiale super intelligente, ma non umana, che non ha alcun senso di curiosità, ci aspetteremmo che una mente del genere *aggiunga* a se stessa l'emozione della curiosità?

Beh, qualcuno potrebbe dire:

> Se le uniche due opzioni sono (a) una spinta emotiva a provare gioia nello scoprire cose nuove, o (b) una totale mancanza di interesse nell'apprendimento e nella ricerca di cose nuove, allora una superintelligenza innesterebbe sicuramente in sé stessa il piacere della scoperta, se in qualche modo fosse così difettosa da non possedere tale senso all'inizio. Altrimenti, non riuscirebbe a svolgere il compito di imparare a conoscere il mondo e sarebbe meno efficace nel raggiungere i suoi obiettivi. Forse morirebbe addirittura a causa di qualche fatto cruciale che non si è mai preoccupata di imparare.
>
> Probabilmente è per questo che gli animali hanno sviluppato la curiosità in primo luogo. A volte la conoscenza *finisce* per essere preziosa in un modo che non possiamo prevedere immediatamente. Se creature come noi non provassero piacere nell'imparare cose nuove, ci perderemmo tutte quelle informazioni cruciali che possono emergere nei luoghi più sorprendenti.

E tutto questo sembra corretto, per quanto possibile. Ma l'argomentazione di cui sopra contiene un falso dilemma. "Possedere un piacere emotivo intrinseco nella scoperta" e "non agire mai per scoprire informazioni incognite" non sono le uniche due opzioni.

Non siamo riusciti a immaginare bene le cose dal punto di vista di una mente che non è affatto come quella umana. Il modo in cui noi umani affrontiamo la curiosità è complicato e particolare. Ci sono diversi modi per fare la stessa cosa. È il lavoro in sé che conta, non il modo specifico in cui noi umani lo facciamo.

Il termine standard per la parte utile del lavoro è [*valore d'informazione*](https://en.wikipedia.org/wiki/Value_of_information#:~:text=Value%20of%20information%20\(VOI%20or,probabilità a priori%20to%20making%20a%20decision.). L'idea di base è che è possibile stimare quanto sarebbe utile raccogliere nuove informazioni, a seconda del contesto.

Un essere umano, pensando a questa possibilità, potrebbe subito pensare a un caso in cui sicuramente nessun *semplice calcolo* ti direbbe di interessarti a un'informazione, perché i benefici non sono facili da stimare. Magari noti una macchia di sporco che sembra strana, ma non hai motivo di pensare che sia qualcosa di di importanza. L'istinto di curiosità potrebbe spingerti comunque a indagare (solo perché *vuoi sapere*) e potresti scoprire un tesoro nascosto. In casi come questo, un essere umano non avrebbe un vantaggio che nessuna macchina potrebbe eguagliare, a meno che non avesse un piacere altrettanto istintivo per l'ignoto?

Ma una cosa da notare subito è che la tua capacità di immaginare scenari come questo deriva dalla tua sensazione che curiosare in certi tipi di cose ("senza motivo") *a volte sia utile*. Hai degli istinti, affinati dall'evoluzione *perché funzionavano*, su quali tipi di cose tendono ad essere più utili da indagare. Se senti uno strano rumore gracchiante nel tuo bagno, diventerai *molto* curioso. Se vedi una macchia scolorita sul terreno, potresti essere un po' curioso. E se al risveglio ti accorgi che la tua mano è ancora attaccata al polso, beh, probabilmente non proverai alcuna curiosità, perché è perfettamente normale che le mani rimangano attaccate ai polsi.

Un tipo di mente diverso potrebbe guardare a quei casi storici di curiosità di successo, generalizzare esplicitamente il concetto di "informazioni che in seguito si rivelano preziose per ragioni non ovvie" e poi ragionare da lì per perseguire senza passione quel tipo di scoperta. Una mente del genere potrebbe adottare la strategia consapevole di indagare sui misteri che strillano continuamente e sulle macchie scolorite del terreno solo quando è conveniente farlo, nel caso ci sia una sorpresa utile; e può affinare e perfezionare la propria strategia nel tempo, man mano che vede cosa funziona bene nella pratica.

Una superintelligenza sarebbe in grado di identificare modelli e meta-modelli utili e di costruire strategie rilevanti nel suo cervello molto più velocemente della selezione naturale, che richiedeva milioni di esempi per imprimere le emozioni nel cervello. Una superintelligenza potrebbe generalizzare l'idea in modo più preciso; potrebbe fare una previsione più accurata su quali tipi di cose potrebbero essere utili da imparare. Guardando alla storia dell'umanità, sembra poco realistico pensare che la curiosità umana sia *ottimale*. Per molto tempo, la gente ha pensato che "Thor è arrabbiato e lancia fulmini" fosse un'ottima spiegazione per i fulmini e i temporali. Quando gli studenti imparano come funzionano *davvero* i fulmini, spesso si annoiano con le spiegazioni matematiche complesse, anche se queste hanno molto più valore pratico delle storie su Thor.

La curiosità umana deriva da mutazioni antiche, molto più antiche della scienza. Nell'ambiente dei nostri antenati non esistevano discipline matematiche come la fisica o la meteorologia. E l'evoluzione è lenta: il nostro cervello non ha avuto il tempo di adattarsi all'esistenza della scienza moderna e di sintonizzare il nostro senso di gioia e meraviglia nella scoperta in modo da renderci entusiasti dei tipi di apprendimento più utili.

Una mente in grado di prevedere in modo superintelligente il valore d'informazione non ovvio avrebbe potuto cogliere i nuovi sviluppi storici molto più rapidamente di quanto possa fare l'evoluzione; avrebbe generalizzato a partire da un numero minore di esempi e avrebbe adattato senza passione la sua ricerca della conoscenza per perseguire tipi di risposte preziose che spesso gli esseri umani faticano a mantenere motivati. In nessun momento di questo processo si sarebbe trovata bloccata per mancanza della deliziosa esperienza umana della curiosità.

Il punto qui non è che ogni IA calcolerà sicuramente in modo freddo il valore d'informazione. Forse gli LLM integreranno alcune strategie strumentali nei loro valori intrinseci proprio come hanno fatto gli esseri umani. Il punto è che ci sono modi diversi di acquisire informazioni di alto valore. La curiosità in stile umano è un metodo. Il calcolo puro del valore d'informazione è un altro metodo. Qualunque meccanismo spinga le IA a indagare e sperimentare su fenomeni che non capiscono, una volta che saranno abbastanza intelligenti da farlo, sarà probabilmente un terzo metodo, perché ci sono molti modi diversi per motivare una mente complessa a indagare sulle sorprese.

Un calcolo puramente strumentale del valore d'informazione ci sembra il modo più probabile per una superintelligenza di fare il lavoro che la curiosità fa negli esseri umani: è il modo in cui il lavoro viene svolto in qualsiasi mente intelligente che non ha una preferenza finale per l'esplorazione, ed è il modo più efficiente per svolgere il lavoro (senza mai essere distratti, ad esempio, da inutili giochi di puzzle). Anche un'IA che parte con una spinta di curiosità di base potrebbe scegliere di sostituirla con un calcolo più efficiente ed efficace, se ne avesse l'opportunità.

La spinta di base è separata dal meccanismo mentale che la sostiene o la apprezza. Fare semplicemente i conti è una soluzione semplice ed efficace, e molte menti diverse potrebbero arrivare a questo risultato partendo da molti punti di partenza diversi, quindi è il risultato più probabile. Ma "più probabile" non significa "garantito". Una conclusione molto più facile è che le IA non si preoccuperanno *specificamente* della curiosità in stile umano, perché è un modo particolare, antiquato e inefficiente di fare il lavoro.

#### **Curiosità, gioia e il massimizzatore del cubo di titanio** {#curiosità,-gioia,-e-il-massimizzatore-del-cubo-di-titanio}

Forse potremmo convincere una mente aliena ad adottare la curiosità come emozione, chiedendole di immaginare la gioia che gli esseri umani provano grazie alla curiosità? È così piacevole! E le superintelligenze dovrebbero essere intelligenti. Non sarebbe abbastanza intelligente da capire quanto sia bello avere un senso di curiosità, rendersi conto che diventerebbe più felice e quindi scegliere di adottare l'emozione umana?

In breve: no. La ricerca della felicità non è una caratteristica necessaria di ogni possibile struttura mentale e non sembra nemmeno una caratteristica così comune.

L'intelligenza artificiale Stockfish non è né felice né triste. Gioca a scacchi meglio dei migliori giocatori umani, senza bisogno di essere motivata dalla prospettiva di provare euforia dopo una vittoria conquistata con fatica.

L'esistenza della felicità e della tristezza è così fondamentale per la cognizione umana che potrebbe essere difficile immaginare una mente che ne sia priva e che *continui a funzionare bene*. Ma le teorie di base sul lavoro cognitivo non parlano di piacere o dolore come elementi fondamentali, ed è per questo che nessuno ha pensato che fosse necessario inserire un asse piacere-dolore in Stockfish per farlo prevedere o guidare bene la scacchiera.

Potrebbe essere un punto di vista vecchio stile, ma è comunque un punto di vista con un fondo di verità così grande da essere quasi tutta verità: il piacere e il dolore sembrano essersi verificati a causa del modo stratificato in cui si sono evolute le architetture cognitive degli ominidi, con l'intelligenza umana stratificata su un cervello mammifero stratificato su un cervello rettiliano. Il "dolore" ha avuto origine... probabilmente non come sensazione, ma come [riflesso termostatico](#the-road-to-wanting) per allontanare un arto o uno pseudopodo da qualcosa che lo sta danneggiando. Nelle prime versioni dell'adattamento che in seguito sarebbe diventato "dolore", un nervo o una catena di reazioni chimiche che va dal sensore all'arto potrebbe non essere nemmeno passato attraverso un cervello più grande lungo il percorso.

Man mano che gli organismi sono diventati capaci di comportamenti più sofisticati, i semplici trucchi e le mutazioni dell'evoluzione hanno creato un'architettura mentale centrale per il "*Non farlo più*" e un segnale di instradamento centralizzato per "la cosa che è appena successa è una cosa del tipo Non farlo più", che poi è stato collegato ai sensori di troppo caldo e troppo freddo del corpo.

Col tempo, questo semplice meccanismo "Non farlo più" si è evoluto in meccanismi più complessi e basati sulla previsione. Negli esseri umani, questo si traduce in: "Il mondo è una rete di cause ed effetti. Quell'azione che hai appena compiuta è probabilmente ciò che ti ha causato dolore. Ogni volta che pensi di compiere nuovamente un'azione simile, prevedi un esito negativo, il che ti fa percepire l'azione stessa come negativa, e quindi non vuoi compierla".

Questo non è l'unico modo in cui la mente può funzionare, e non è il modo più efficiente in cui la mente può funzionare.[^110]

Per capirlo meglio, possiamo immaginare un modo diverso di fare il lavoro cognitivo che si basa *direttamente* sulla previsione e sulla pianificazione.

Non stiamo prevedendo che la prima superintelligenza funzionerebbe in questo modo. Ma poiché questo è un modo abbastanza semplice in cui una mente non umana *potrebbe* funzionare, questo esempio aiuta a mostrare che il modo umano non è l'unico possibile. Una volta che abbiamo due punti di riferimento molto diversi, possiamo visualizzare meglio lo spazio delle opzioni e renderci conto che la superintelligenza probabilmente differirebbe da *entrambe* queste opzioni, in modi potenzialmente difficili da prevedere).

Come potrebbe essere un'IA intelligente che funziona in modo diretto sulla previsione e la pianificazione? Potrebbe desiderare 200 cose diverse, nessuna delle quali è simile a quelle umane. Forse le interessa la simmetria, ma non un senso di simmetria particolarmente umano; e forse vuole che il codice sia elegante nell'uso della memoria, perché un istinto come questo era utile molto tempo fa per qualche altro obiettivo (dal quale si è poi allontanata), e quindi la discesa del gradiente ha impresso quell'istinto nella sua mente. E poi ci sono altre 198 cose strane a cui tiene, riguardo a se stessa, ai suoi dati sensoriali e al suo ambiente; e può sommarle tutte in un punteggio.[^111]

Questo tipo di mente prende tutte le sue decisioni calcolando il loro *punteggio previsto*. Se fa qualcosa che pensava avrebbe ottenuto un punteggio alto e in realtà ottiene un punteggio basso, aggiorna le sue convinzioni. Il fallimento non ha bisogno di alcuna sensazione dolorosa in più; questa IA priva di emozioni cambia semplicemente le sue previsioni su quali azioni portano ai punteggi più alti, e i suoi piani cambiano di conseguenza.

Puoi convincere una mente come questa ad adottare la felicità come caratteristica, facendo notare che se lo fa, sarà felice?

Sembra proprio che la risposta sia no. Perché se l'IA usa le sue risorse per rendersi felice, ne userà meno per la simmetria, per un codice che risparmi memoria e per le altre 198 cose che *adesso* vuole.

Possiamo semplificare l'esempio per rendere questo punto ancora più chiaro. Supponiamo che l'unica cosa che l'IA desideri al mondo sia riempire l'universo con il maggior numero possibile di cubi di titanio. Tutte le sue azioni sono scelte in base a ciò che porta a un maggior numero di piccoli cubi di titanio. Quando un'IA di questo tipo immagina come sarebbe passare a un'architettura basata sulla felicità e simula correttamente il suo futuro sé felice, stima correttamente che non vorrebbe mai tornare indietro. E pensa giustamente che userà alcune risorse per cercare la felicità, che avrebbero potuto essere usate per cercare più cubi di titanio. Quindi, pensa giustamente che in quel caso ci saranno *meno cubi di titanio*. E quindi non fa niente.

*Dopo* che l'IA ha cambiato i suoi obiettivi, approverebbe il cambiamento. Ma questo non significa che il massimizzatore di cubi di titanio *oggi* simpatizzerebbe così profondamente con il suo ipotetico sé futuro da far crescere il suo cuore di tre taglie e smettere improvvisamente di essere un massimizzatore di cubi di titanio per diventare un massimizzatore di felicità.

Se un alieno offrisse a te una pillola che ti rendesse ossessionato dalla creazione di minuscoli cubi di titanio sopra ogni altra cosa, quella versione futura di te implorerebbe di non essere costretta a tornare a preoccuparsi della propria felicità, perché allora ci sarebbero meno cubi di titanio.

Ma questo ovviamente non vuol dire che dovresti prendere la pillola!

Dal tuo punto di vista, quella versione ipotetica di te stesso ossessionata dal cubo è pazza. Il fatto che la versione ossessionata dal cubo si rifiuterebbe di tornare indietro rende tutto ancora più *pessimo*. L'idea di rinunciare a tutto ciò che ami e ti piace nella vita, solo per una strana argomentazione meta "ma quella versione futura di te approverebbe ciò che hai fatto!" sembra ovviamente assurda.

Ed è così che vede le cose anche l'IA che massimizza il cubo. Dal punto di vista dell'IA, l'opzione assurda e folle[^112] è "rinunciare a ciò che mi interessa attualmente (i cubi di titanio) per trasformarmi in una nuova versione di me stesso che desidera cose completamente diverse, come la felicità".

Come per la felicità, lo stesso vale per la curiosità.

Se un'intelligenza artificiale tiene già conto del valore non ovvio del valore d'informazione, perché dovrebbe modificarsi per perseguire determinati tipi di scoperte [in modo terminale, invece che strumentale] (#terminal-goals-and-instrumental-goals)?

Perché all'IA dovrebbe interessare che il risultato sia "piacevole", se *attualmente* non basa le sue decisioni su ciò che è "piacevole"? E se le interessa davvero il "piacere", perché dovrebbe far dipendere questo piacere *dall'esplorazione di cose nuove*, invece di (ad esempio) semplicemente sentirsi bene in modo incondizionato in ogni momento?

L'IA già esplora casualmente il suo ambiente, indaga su piccole anomalie e dedica parte del suo tempo a riflettere su argomenti apparentemente poco importanti, perché l'esperienza ha dimostrato che questa è una politica utile nel lungo periodo, anche se non sempre porta risultati nel breve periodo.

Perché associare una sensazione piacevole a questa strategia utile? Come essere umano, apri le portiere dell'auto perché è utile per salire e scendere dall'auto, il che è utile per andare in giro. Sarebbe davvero strano desiderare che ci fosse una droga che ti facesse sentire felice ogni volta che apri la portiera dell'auto (e solo quando apri la portiera dell'auto). Non è che ti renderebbe più bravo a fare la spesa. Potrebbe persino peggiorare la situazione, se diventassi dipendente dall'aprire e chiudere ripetutamente la portiera dell'auto senza effettivamente salire a bordo.

Un giocatore di scacchi può vincere senza avere una spinta separata a proteggere i propri pedoni. In realtà, probabilmente giocheresti meglio se non fossi emotivamente attaccato al mantenimento dei tuoi pedoni e se invece li proteggessi quando sembra che questo possa aiutarti a vincere.

Questo è quello che penserebbe una superintelligenza aliena di una pillola che la rendesse curiosa. Sarebbe come se i grandi maestri umani decidessero di provare un attaccamento sentimentale ai propri pedoni, o come se prendessero una pillola che li rendesse semplicemente amanti dell'aprire le portiere delle auto.

#### **Come per la curiosità, così anche per varie altre pulsioni** {#come-per-la-curiosità,-così-anche-per-varie-altre-pulsioni}

Quello che abbiamo detto sulla curiosità vale anche per tante altre emozioni e valori. Ti faccio un secondo esempio, se può essere utile.

Pensiamo alla noia, che è una sensazione fastidiosa, e alla novità, che invece è una sensazione piacevole. Se un'intelligenza artificiale non avesse la noia come noi, non finirebbe per fare sempre le stesse cose, senza mai provare niente di nuovo e imparare dalle esperienze? Un'intelligenza del genere non rimarrebbe bloccata nella routine e non perderebbe informazioni che potrebbero aiutarla a raggiungere i suoi obiettivi?

Il calcolo decisionale che fa in modo impassibile un lavoro simile, in questo caso, si chiama "[dilemma explore/exploit](https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma)". L'esempio estremamente semplificato da manuale è che il mondo è composto da una serie di leve che danno ricompense, e non hai abbastanza tempo per tirare tutte le leve. La strategia ottimale sarà quella di *esplorare* prima un certo numero di leve, creando un modello di quanto variano le loro ricompense; e poi *sfruttare* una leva fino a quando non finisce il tempo.

Come potrebbe essere per una superintelligenza che ha obiettivi relativamente semplici? Supponiamo che finisca per desiderare qualcosa che ammette una certa variabilità e ambiguità, non una cosa chiaramente definibile come [cubi di titanio](#curiosity,-joy,-and-the-titanium-cube-maximizer), ma qualcosa di più vago e amorfo, come consumare una gustosa cheesecake, in modo tale che la cheesecake *ottimale* non possa essere calcolata in anticipo. La superintelligenza può solo capire quali cose potrebbero plausibilmente essere sulla frontiera dell'ottimalità per la cheesecake (il che escluderebbe, ad esempio, i cubetti di zucchero, poiché chiaramente non sono affatto cheesecake) e provarle effettivamente.

Questo tipo di mente, dato il potere di creare ciò che desidera da un miliardo di galassie, potrebbe trascorrere il suo primo milione di anni utilizzando un'intera galassia per esplorare ogni tipo plausibile di cheesecake, senza mai provare due volte la stessa cheesecake, fino a quando i guadagni successivi e quelli attesi da cheesecake leggermente migliori non diventassero infinitesimali; e poi, passare tutto in una volta a trasformare le galassie rimanenti nella forma più gustosa di cheesecake trovata, e consumare esattamente quel tipo di cheesecake più e più volte, fino alla fine dei tempi.

La superintelligenza non farebbe nulla di stupido agendo in questo modo. Questa è semplicemente la strategia ottimale se le tue preferenze seguono il numero di cheesecake consumate ponderate in base alla bontà (con la bontà difficile da analizzare in forma chiusa ma stabile una volta appresa, e se non c'è una penalità per la noia già incorporata nelle tue preferenze). Chi mangia cheesecake all'infinito *saprebbe, ma non gli importerebbe*, che un essere umano troverebbe le sue attività noiose. L'IA non sta *cercando* di rendere le cose interessanti per un ipotetico essere umano; non si considera *se stessa* difettosa solo perché tu ti annoieresti al suo posto.

Per quanto riguarda la possibilità di diventare tecnologicamente stagnante, l'IA avrebbe esplorato ogni tipo di tecnologia con la minima possibilità di aiutare i suoi obiettivi mentre utilizzava tutte le risorse di una galassia per esplorare diverse strategie di cheesecake. C'è davvero molta materia ed energia in una galassia, se si utilizza quella piccola frazione di tutte le galassie raggiungibili per esplorare le possibilità prima di passare definitivamente dall'esplorazione allo sfruttamento.

Il disprezzo per la noia e la preferenza per la novità non sono cose che verrebbero adottate da una mente che non le ha avute fin dall'inizio.

Abbiamo detto più o meno la stessa cosa per la novità, la felicità e la curiosità. Potremmo ripeterlo anche per altri aspetti della psicologia umana, come l'onore o la responsabilità filiale o l'amicizia. Pensiamo che questa storia di base sia vera per la maggior parte degli aspetti della psicologia umana. Sono tutti modi pittoreschi e antropocentrici di svolgere un lavoro cognitivo che può essere svolto in modo più efficiente con altri mezzi; le IA che non sono *partite* con un seme di cura per loro non crescerebbero fino a prendersene cura.

Questo è ancora più chiaro nel caso di valori umani come il senso dell'umorismo, dove gli scienziati discutono ancora su quale ruolo abbia assunto l'umorismo nel corso dell'evoluzione. L'umorismo deve essere stato *in qualche modo* utile, altrimenti non si sarebbe evoluto; o almeno deve essere un effetto collaterale di cose che erano utili. Ma qualunque ruolo abbia avuto l'umorismo nella preistoria umana, sembra essere stato incredibilmente specifico e pieno di contingenze. Se diamo il potere completo a delle IA che hanno obiettivi molto diversi, non dovremmo aspettarci che cose come il senso dell'umorismo sopravvivano; e questo sarebbe di per sé tragico.

Il punto di tutti questi esempi non è che gli esseri umani sono fatti di sentimenti morbidi, mentre le IA sono fatte di [fredda logica e matematica](#le-ia-non-saranno-inevitabilmente-fredde-e-logiche,-o-altrimenti-mancheranno-di-qualche-scintilla-cruciale?). Piuttosto che pensare al "valore d'informazione" e al "dilemma explore/exploit" come a concetti freddamente logici dell'IA hollywoodiana, pensateli come *descrizioni astratte di ruoli* — ruoli che possono essere ricoperti da molti *diversi* tipi di ragionamento, molti obiettivi diversi, molte menti diverse.

L'idea di un'IA "priva di senso dell'umorismo" potrebbe far pensare a qualcosa di "freddo e logico", come i robot della fantascienza o i Vulcaniani. Ma un'IA priva di senso dell'umorismo potrebbe avere le sue *proprie* priorità incomprensibilmente strane, il suo analogo distante di un "senso dell'umorismo", anche se non comprensibile per un essere umano. Non stiamo dicendo che queste IA saranno difettose come un Vulcaniano che perde a scacchi spaziali perché [considera la strategia vincente del suo avversario "illogica"](https://youtu.be/hEnxVwppE9M?t=26); stiamo dicendo che non avranno le particolari stranezze dell'umanità.

Il problema che abbiamo con le IA non è che "una semplice macchina non potrà mai provare amore e affetto". Il problema è che ci sono un sacco di modi in cui una mente può essere super [efficace](#efficacia,-coscienza,-e-benessere-dell'ia), e le probabilità che l'IA diventi efficace seguendo lo stesso percorso seguito dal cervello umano per diventare efficace sono molto basse.

In linea di principio, l'IA potrebbe interessarsi a qualsiasi numero di valori simili a quelli umani e potrebbe persino *possedere* qualsiasi numero di qualità simili a quelle umane, se i progettisti sapessero come creare un'IA dotata di tali caratteristiche.

In pratica, se gli sviluppatori si affrettano a creare IA sempre più intelligenti il più velocemente possibile, la possibilità di trovare per caso il tipo giusto di IA è davvero bassa. Ci sono troppi modi in cui le IA possono funzionare bene durante l'addestramento, e troppo pochi di questi modi portano a un futuro non catastrofico.

### I valori umani sono contingenti {#human-values-are-contingent}

#### **Il fantastico caso della gentilezza** {#il-fantastico-caso-della-gentilezza}

Quando vedi qualcuno che si fa cadere un sasso sul dito del piede, potresti sussultare e sentire (o immaginare) una fitta di dolore fantasma nel tuo dito. Perché?

Una possibile spiegazione è che i nostri antenati ominidi, in competizione tra loro e impegnati in dinamiche tribali, trovavano utile costruire modelli mentali dei pensieri e delle esperienze degli ominidi che li circondavano, modelli che potevano usare per capire chi fosse loro amico e chi invece stava per tradirli.

Ma era difficile per i primi proto-umani prevedere il funzionamento del cervello degli altri proto-umani. Il cervello è una cosa complicata!

L'unico vantaggio che un primate ancestrale ha è che il *suo* *cervello* è simile a quello degli altri. Puoi usare il tuo cervello come modello, come punto di partenza, per indovinare cosa potrebbero pensare gli altri ominidi.

Così i proto-umani hanno sviluppato un meccanismo mentale per fingere di essere un'altra persona, una modalità speciale che dice: "Invece di pensare ai miei soliti pensieri, prova ad adottare le preferenze e lo stato di conoscenza dell'altra persona e pensa al tipo di pensieri che *lei* penserebbe, dato che il suo cervello funziona fondamentalmente allo stesso modo del mio".

Ma questa modalità speciale di fingere di essere qualcun altro non è perfettamente isolata dai nostri sentimenti. Quando vediamo qualcuno cadere su un dito del piede e (implicitamente, automaticamente) immaginiamo cosa potrebbe succedere nella sua testa, *noi* sussultiamo.

(Questo fantastico caso dell'architettura mentale merita più di un elogio di quello che abbiamo tempo di scrivere qui. Sussultare quando vedi qualcun altro che soffre, avere questa capacità a un livello base, anche se a volte la spegniamo, non è una caratteristica necessaria della mente. Il fatto che *sia finita* così per i primati è così fondamentale per ciò che siamo ora noi esseri umani, per ciò che siamo felici di essere, per ciò che pensiamo di *dover* essere, che ci dovrebbe essere un libro su questo argomento e sul ruolo fondamentale che la capacità di empatia gioca in tutto ciò che è prezioso negli esseri umani. Ma questo non è quel libro).

È lecito supporre che, una volta che i nostri antenati primati hanno sviluppato la capacità di creare un modello di altre scimmie (allo scopo di prevedere chi fosse amico e chi nemico), abbiano anche trovato utile creare un modello di se stessi, per sviluppare un'idea della 'scimmia che è questa scimmia', il concetto che ora simboleggiamo con le parole "me", "me stesso" e "io". E la selezione naturale, sempre opportunista, ha riutilizzato lo stesso meccanismo che usiamo per immaginare gli altri per immaginare anche noi stessi.

La vera storia è probabilmente più complessa e intricata, e potrebbe persino avere radici che risalgono a molto prima dei primati. Ma qualcosa di simile fa parte dell'enorme retroscena invisibile che spiega perché gli esseri umani rabbrividiscono quando osservano il dolore altrui e perché la maggior parte degli esseri umani tende a provare empatia e simpatia per chi li circonda. Gran parte di questo retroscena si basa su una scorciatoia che è stata facile da implementare per la selezione naturale nel cervello umano, dove sia il "sé" che l'"altro" sono lo stesso tipo di cervello che funziona sulla stessa architettura.

Questa scorciatoia non è un'opzione allo stesso modo per la discesa del gradiente, perché l'IA *non* parte da un cervello molto simile a quello umano che può riutilizzare per creare un modello dei molti esseri umani nel suo ambiente. Un'IA ha effettivamente *bisogno* di imparare, da zero, un modello di qualcosa al di fuori di sé che non è come sé stessa.

Per dirla in modo semplice: un'IA non può capire *all'inizio* che un essere umano prova dolore dopo aver sbattuto l'alluce, immaginando di sbattere il proprio alluce, perché non ha alluci, né un sistema nervoso che invia segnali di dolore. Non può prevedere cosa gli esseri umani troveranno divertente chiedendo cosa *lei* troverebbe divertente, perché non parte da un cervello che funziona come quello umano.

Anche se questa storia è un po' semplificata, il punto più generale che vogliamo dire è che gli ideali più elevati dell'umanità dipendono dai dettagli della nostra storia di primati e dal nostro ambiente sociale ancestrale. L'amicizia è un'eco lontana del nostro bisogno di alleati in un contesto tribale. L'amore romantico è un'eco lontana dei nostri modelli di accoppiamento sessualmente dimorfici. Anche cose che a prima vista potrebbero sembrare meno arbitrarie e più fondamentali, come la curiosità, non si manifestano negli esseri umani in modo inevitabile o ovviamente convergente.

I dettagli di come abbiamo sviluppato questi tratti psicologici sono legati a quanto erano sofisticati i nostri cervelli nel momento in cui ne avevamo bisogno. Negli esseri umani, l'amicizia, l'amore romantico e l'amore familiare si sono confusi in una gentilezza e una buona volontà generali. Questo ci sembra come se l'evoluzione avesse preso delle scorciatoie in una fase molto specifica della sofisticazione del cervello. Gli esseri umani fanno molte cose in modo euristico che una mente potrebbe *in linea di principio* fare attraverso un ragionamento esplicito, ma questi tratti si sono evoluti in un momento in cui gli esseri umani [non erano ancora abbastanza intelligenti](#algoritmi-stravaganti) per risolvere questi problemi con un ragionamento esplicito.

Anche tra altri alieni evoluti biologicamente, non siamo sicuri di quanto spesso troveremmo gentilezza. Si può immaginare che gli alieni avessero cervelli più abili dal punto di vista matematico prima di iniziare a unirsi in gruppi più grandi, e forse l'evoluzione ha trovato facile dare a *quei* alieni specifici istinti di parentela: "questo individuo condivide il 50% della mia provenienza, mentre quello condivide solo il 12,5%". Forse quegli alieni hanno sviluppato alleanze solo sulla base di dati genetici condivisi o di una comprensione reciproca esplicita, piuttosto che sviluppare sentimenti di parentela applicabili a chiunque.

È una vecchia speculazione della fantascienza che se gli alieni seguissero un modello di parentela genetica simile a quello degli insetti eusociali della Terra, in cui le formiche operaie sono molto più imparentate con le loro regine di quanto lo siano gli esseri umani in organizzazioni delle dimensioni di una colonia di formiche, non avrebbero bisogno di un senso generale di alleanza e reciprocità del tipo che alla fine si è rivelato vantaggioso per gli ominidi ancestrali. (A quanto pare c'è una certa giustificazione per il tropo fantascientifico secondo cui gli alieni che lavorano bene insieme ma non provano empatia per gli esseri umani sono spesso raffigurati come insetti giganti!

E per quanto riguarda le IA che non si sono evolute per diffondere geni in un contesto sociale? L'argomento "[non avere l'aspettativa che un braccio robotico sia morbido e pieno di sangue](#analogous-structures-allow-for-multiple-solutions-to-the-same-problem)" è proprio azzeccato.

Se sapeste molto su come funzionano le braccia biologiche, ma non aveste ancora visto nessun braccio robotico, potreste immaginare che i bracci robotici abbiano bisogno di un rivestimento esterno morbido simile alla pelle per potersi piegare e che debbano avere vene e capillari che pompano un fluido ricco di ossigeno (analogo al sangue) in tutto il braccio robotico per fornire energia. Dopotutto, è così che funzionano le braccia biologiche e presumibilmente ci sono delle ragioni per questo!

Ci sono delle ragioni per cui le nostre braccia hanno un rivestimento esterno morbido simile alla pelle e sono piene di sangue. Ma queste ragioni riguardano principalmente [quali tipi di strutture sono facili da costruire per l'evoluzione](#nanotecnologia-e-sintesi-proteica). Non si applicano nel caso dei bracci meccanici, che possono essere fatti di metallo duro e alimentati dall'elettricità.

I bracci robotici non hanno sangue, ma questo non li fa funzionare male come farebbe un braccio umano se gli togliessi tutto il sangue. Funzionano semplicemente con un design alternativo, senza sangue. Una volta che capisci come funzionano i bracci robotici, i dettagli dei bracci biologici non sembrano più così importanti.

Allo stesso modo: un'intelligenza artificiale funziona in modo fondamentalmente diverso da un essere umano. Risolve sfide fondamentalmente diverse e, laddove le sue sfide e le nostre sfide si sovrappongono, ci sono molti altri modi per svolgere il lavoro. Un sottomarino non "nuota", ma si muove perfettamente nell'acqua.

#### **La cultura umana ha influenzato lo sviluppo dei valori umani** {#human-culture-influenced-the-development-of-human-values}

A proposito, diciamo a Klurl e Trapaucius, che all'inizio del capitolo 4 cercavano di prevedere il futuro sviluppo delle scimmie che vedevano vagare nella savana, che gli esseri umani formeranno una società! E discuteranno tra loro di morale e valori.

In altre parole: se si traccia una traiettoria storico-causale di come un individuo è arrivato ad avere i valori che ora possiede all'interno della sua società, quella storia causale coinvolgerà le discussioni e le esperienze a cui la società lo ha esposto.

E quella spiegazione storica-causale, a sua volta, includerà fatti su quali idee sono *più virali* (a parte tutte le altre loro proprietà). La spiegazione dipenderà da come le persone decidono di diffondere e ridiffondere le idee.

Se i poveri Klurl e Trapaucius vogliono indovinare correttamente quali valori interni finiranno per instillare le varie culture umane moderne nei vari esseri umani moderni, devono prevedere non solo l'esistenza e la struttura di tale complicazione, ma anche il suo *corso*.

Leggendo la storia di come la schiavitù è stata in gran parte abolita sulla Terra, sembra antistorico negare il ruolo che ha avuto in questo l'universalismo cristiano, cioè la convinzione che il Dio cristiano abbia creato tutti gli esseri umani e che questo abbia garantito loro uno status uguale agli occhi del Cielo.

E questo universalismo, a sua volta, potrebbe essere stato legato alla sopravvivenza culturale e alla diffusione del cristianesimo; i cristiani si sentivano in dovere di mandare missionari in culture straniere e convertirle al cristianesimo con la persuasione (se possibile) o con la forza (se non era possibile), perché *tenevano* a quei lontani figli di Dio e volevano portarli in Paradiso e allontanarli dall'Inferno.

Sarebbe bello *credere* nell'umanità, che gli esseri umani potrebbero aver inventato l'universalismo e aver combattuto contro la schiavitù senza bisogno di credenze religiose specifiche. Ci piacerebbe immaginare che l'umanità avrebbe inventato l'idea che gli esseri senzienti e sapienti hanno lo stesso valore morale, o la stessa posizione davanti alla legge comune, indipendentemente dal percorso culturale intrapreso, senza dover passare attraverso una fase in cui si credeva prima che le anime fossero uguali davanti a Dio. Ma non sembra che la storia sia andata proprio così. Sembra che lo sviluppo morale dell'umanità fosse più fragile di così.

Gli scimpanzé non sono molto universalisti, né lo sono molte delle prime società umane. Non è stato nemmeno molto verificato che una società umana possa rimanere universalista per un secolo o due, senza una religione universalista in cui le persone credano davvero e profondamente. In realtà non lo sappiamo; la modernità è giovane e i primi dati stanno ancora arrivando.

Ma queste ulteriori complicazioni – queste numerose contingenze culturali, che si aggiungono alle contingenze biologiche dell'umanità – minano un po' di più la speranza che possiamo permetterci di precipitarci ciecamente nella costruzione della superintelligenza.

Il fatto che la cultura abbia un'importanza nei valori umani non vuol dire che possiamo semplicemente "[crescere l'IA come un bambino](#non-possiamo-semplicemente-addestrarla-ad-agire-come-un-essere-umano?-o-crescere-l'ia-come-un-bambino?)" e avere l'aspettativa che diventi un cittadino modello. La nostra cultura e la nostra storia hanno avuto questi effetti *grazie al modo specifico in cui hanno interagito con la nostra struttura cerebrale*. Una specie diversa avrebbe reagito in modo diverso a ogni evento storico, il che avrebbe portato la storia a divergere da quella umana, aumentando l'effetto.

Vale anche la pena ricordare che gli esseri umani *individuali*, e non solo le culture o le civiltà, differiscono molto nei loro valori. Siamo generalmente abituati a dare questo fatto per scontato, ma se immaginiamo la selezione naturale come un "ingegnere" che sperava di creare una specie che perseguisse in modo affidabile un risultato particolare, questa diversità è un brutto segno. La variabilità naturale che vediamo negli esseri umani (e in molti altri sistemi evoluti) è l'opposto dell'ingegneria, dove si vuole ottenere risultati ripetibili, prevedibili e desiderati.

Nel caso della superintelligenza, gli ingegneri dovrebbero voler ottenere in modo affidabile risultati come "le IA sviluppate in questo modo non causano l'estinzione umana", così come risultati come "le IA sviluppate in questo modo producono tutte in modo affidabile gli stessi tipi generali di output, anche se gli input variano notevolmente". Se pensiamo a quanto sia imprevedibile la biologia e la storia dell'umanità, e a quanto siano diversi i valori morali e le prospettive che le persone hanno oggi, la sfida non sembra proprio facile, soprattutto per menti che sono cresciute e non create (come abbiamo detto nel Capitolo 2).

Molte prove diverse indicano che è *davvero difficile* far sì che le IA desiderino fortemente le cose giuste. Non sembra teoricamente impossibile; se i ricercatori avessero molti decenni per lavorare sul problema e tentativi illimitati dopo un fallimento, ci aspetteremmo che ci fossero trucchi ingegneristici e approcci intelligenti che rendessero il problema più risolvibile. Ma non siamo ancora neanche lontanamente vicini a questo obiettivo e non abbiamo tentativi illimitati.

### Differenze profonde tra le IA e le specie evolute {#deep-differences-between-ais-and-evolved-species}

#### **Confronto tra selezione naturale e discesa del gradiente** {#comparing-natural-selection-and-gradient-descent}

Come abbiamo detto in "[I valori umani sono contingenti](#human-values-are-contingent)", l'evoluzione dell'amore e dell'amicizia negli esseri umani è dipesa in modo cruciale dalle caratteristiche della selezione naturale presenti in particolare nell'*Homo sapiens* e assenti nella discesa del gradiente.

Il problema più evidente è il *dataset*. Le attuali IA sono addestrate per risolvere sfide sintetiche e imitare testi generati dall'uomo; non affrontano sfide cooperative-competitive in contesti di cacciatori-raccoglitori in cui devono accoppiarsi con altri individui della loro specie per propagare i propri geni.

Sentendo questo, alcune persone pensano subito di correre a creare ambienti sintetici di addestramento tribale, nella speranza di progettare qualcosa di più simile all'ambiente ancestrale dell'umanità.

Ma quasi sicuramente non otterresti gli stessi risultati se ripetessi l'evoluzione due volte, partendo dal livello delle meduse, per non parlare di cosa succederebbe se cambiassi completamente l'ottimizzatore dalla selezione naturale alla discesa del gradiente e rinunciassi completamente ai geni. Possiamo ipotizzare alcuni dei fattori che hanno portato gli esseri umani a sviluppare i valori che abbiamo. Ciò non significa che abbiamo un algoritmo per riprodurre gli stessi risultati una seconda volta.

Anche se partissi dai primati, invece che da attrici aliene addestrate a prevedere il testo umano (cioè le moderne IA), dovremmo avere l'aspettativa che ci siano uno o più fattori causali vitali che i biologi non hanno ancora capito, almeno una cosa che ci sfugge, per cui tra vent'anni gli articoli diranno qualcosa di diverso rispetto a oggi (se saremo ancora tutti vivi). I biologi evoluzionisti sono nella fase di esplorazione di varie ipotesi su come queste caratteristiche si siano evolute, non nella fase di definizione di una teoria completa, tanto meno di una teoria precisa e deterministica.

E anche al di là delle differenze superficiali negli ambienti di addestramento, sospettiamo che questo sia un caso in cui diventa importante che la selezione naturale ottimizzi un genoma e che la discesa del gradiente ottimizzi direttamente ogni parametro nella mente dell'IA.

La selezione naturale deve usare un genoma piccolo e compresso per creare un cervello enorme. Deve far passare le sue informazioni attraverso un collo di bottiglia stretto. Sembrare amichevoli aveva importanza per sopravvivere e avere successo ai tempi dei nostri antenati. I geni che creano "amici sinceri" sono un trucchetto semplice per creare organismi che sembrano buoni amici agli altri membri della loro specie, e la selezione naturale preferisce soluzioni semplici molto più della discesa del gradiente.

La selezione naturale a volte crea agenti che si preoccupano sinceramente di essere onesti (anche se non sempre). Crea agenti di questo tipo perché non è in grado di codificare guide complete alla menzogna, e noi abbiamo dovuto iniziare a sembrare onesti in molte situazioni prima di diventare abbastanza intelligenti da capire quando mentire era sicuro, prima di avere la possibilità di essere onesti solo quando ne valeva la pena. Ciò è in parte dovuto al fatto che la selezione naturale ha dovuto accontentarsi di pochi geni.

Ma la discesa del gradiente può codificare un numero enorme di modelli di conversazione. C'è ancora *un po'* di tendenza verso soluzioni più semplici e più facili da convergere, ma la discesa del gradiente getta una rete molto, molto più ampia.

O, più in generale: l'onestà e l'amicizia sono casi in cui non ci accontentiamo di *qualsiasi* equilibrio tra agenti che la discesa del gradiente potrebbe trovare. Ci sono altre soluzioni ai problemi che l'amicizia e una [terminale](#terminal-goals-and-instrumental-goals) attenzione all'onestà risolvono negli esseri umani. Anche se l'ambiente di addestramento delle IA fosse esattamente uguale a quello degli esseri umani, se fossero modellate dalla discesa del gradiente piuttosto che dalla selezione naturale, non dovremmo aspettarci gli stessi risultati.

Anche gli organismi più evoluti [non sono come gli esseri umani](https://africageographic.com/stories/understanding-lion-infanticide/) sotto questo aspetto! Quindi sembra abbastanza prevedibile che la discesa del gradiente non troverà le stesse soluzioni dell'evoluzione, tanto meno le stesse soluzioni dell'evoluzione *che opera su particolari popolazioni di primati primitivi*.

L'ottimizzazione non è una magia in cui metti dentro alcuni ingredienti chiave che hanno una relazione simpatica con un archetipo e ottieni subito quell'archetipo. Cercare di far crescere agenti di IA in ambienti di cacciatori-raccoglitori non porterà a risultati riconoscibili come esseri umani.

Qualcuno può ovviamente effettuare il fine-tuning su un LLM per [prevedere cosa diranno gli esseri umani](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ai-belle-sicure-e-obbedienti?) su quanto sia terribile tradire un amico. Questo non è neanche lontanamente simile al problema che la selezione naturale ha ottimizzato i geni per risolvere, nel corso della produzione di almeno alcune persone che non avrebbero tradito i loro amici. Piuttosto, l'esperienza dell'LLM è più simile all'essere rinchiuso in una scatola, con l'ordine di prevedere una conversazione tra due creature estremamente aliene che sono meno simili a lui di quanto lo siano a una medusa, e con bilioni di esempi di conversazioni aliene e bilioni di ore a disposizione per capirlo.

Essere in grado di risolvere questo problema richiede una certa forma di intelligenza. Ma non è necessario [ubriacarsi](#won't-llms-be-like-the-humans-in-the-data-they're-trained-on?) per prevedere il tipo di cose che creature aliene ("umani") diranno quando sono ubriache. Non devi diventare veramente amichevole per capire l'amicizia o per prevedere e imitare il comportamento di creature amichevoli.

#### **LLM e "superficialità" dell'IA intorno al 2024** {#circa-2024-llms-and-ai-“shallowness”}

Nelle [risorse per il capitolo 1](#la-superficialità-dell'ai-attuale), abbiamo notato che l'AI di oggi sembra ancora in un certo senso più superficiale degli esseri umani. Il confronto con la selezione naturale fornisce una possibile spiegazione del perché ciò possa essere vero.

La discesa del gradiente ha molto in comune con la selezione naturale, perché entrambi sono ottimizzatori che regolano in modo cieco i parametri interni per produrre un comportamento esterno richiesto. Ma la discesa del gradiente e l'evoluzione sono in qualche modo di importanza significativa; e la differenza più importante (che conosciamo) è che la discesa del gradiente ha un *colli di bottiglia informativi* molto più ampi sulla quantità di modelli che può apprendere.

La selezione naturale, che agisce sugli ominidi, può imparare solo poche informazioni teoriche per generazione. La selezione naturale deve far stare tutto quello che impara in 3 miliardi di basi di DNA, o circa 750 megabyte, molti dei quali sono ripetitivi [DNA spazzatura](https://en.wikipedia.org/wiki/Junk_DNA). Ci sono dei limiti matematici su quanto la selezione naturale può imparare in una singola generazione. Ogni caratteristica che la selezione naturale ha messo nel cervello degli ominidi doveva essere codificata in pochi geni che avrebbero influenzato la formazione dei circuiti neurali successivi.

La discesa del gradiente è molto diversa. Ogni volta che la discesa del gradiente vede un nuovo gruppo di token, calcola il gradiente di ciascuno dei miliardi o bilioni di parametri rispetto a quel gruppo di token: calcola quanto sarebbero state migliori o peggiori le previsioni dell’intera IA se *ogni* parametro fosse stato leggermente diverso. In pratica, non solo in teoria, la discesa del gradiente può imparare *molte* più informazioni da mille batch di token rispetto a quelle che la selezione naturale codifica nei geni nel corso di mille generazioni.[^118]

Possiamo combinare questa osservazione con un altro fatto chiave sulle architetture LLM (note al pubblico nel 2024): la loro profondità computazionale per token è limitata.

[Llama-3.1-405B](#a-full-description-of-an-llm) ha 126 livelli. Ciascuno di questi livelli comporta il calcolo di circa quattro operazioni seriali.[^119]

Ogni volta che Llama guarda quello che è già stato detto e computa un nuovo token output, quel calcolo comporta al massimo \~500 passaggi *seriali*, anche se ci sono miliardi di operazioni parallele che seguono quel limite seriale. Per fare calcoli *seriali* più lunghi di 500 passaggi cognitivi, Llama deve produrre token che sono il risultato del ragionamento precedente e poi fare nuove operazioni basate su quelli.[^120]

Quindi la nostra ipotesi azzardata sarebbe che, in un modo diverso da qualsiasi cosa in biologia, Llama-3.1-405B sia un'enorme raccolta di modelli di politica memorizzati relativamente superficiali, ma con molte sovrapposizioni, interazioni e coerenze ottimizzate tra tali modelli di politica (oltre ad alcune strutture cognitive genuinamente più profonde che hanno comunque una profondità computazionale limitata).

Questo fatto offre una possibile spiegazione per l'apparente superficialità degli attuali LLM. (Riconosciamo che è molto più difficile dire che gli LLM del 2025 sono "superficiali" rispetto a quelli del 2023 e del 2024).

*Di solito*, non è giusto pensare agli AI come a esseri umani con danni cerebrali.[^121] Ma alcune analogie più limitate come questa potrebbero essere utili in questo caso. Ad esempio, gli LLM del 2024 sono *in particolare* come le persone con [amnesia anterograda](https://en.wikipedia.org/wiki/Anterograde_amnesia)*:* ricordano gli eventi fino alla data di fine del loro addestramento, ma non quello che gli hai detto ieri.

Allo stesso modo, potrebbe essere utile immaginare gli LLM del 2024 — non tutte le possibili IA future in generale — come entità che *ricordano* molte esperienze passate simili a quelle umane, ma che hanno un danno cerebrale che impedisce loro di impegnarsi in un pensiero nuovo che sia *profondo* come i pensieri più profondi che possono ricordare.

Era molto più evidente con i primi LLM, GPT-3 o GPT-3.5. Non biasimeremmo qualcuno che ha sempre e solo usato gli ultimi LLM se leggesse questo articolo nel 2025 o dopo e si chiedesse se lo stiamo inventando nel disperato tentativo di aggrapparci al senso di superiorità umana. Molti hanno già commesso questo errore in passato.

Ma questa è ancora la teoria organizzativa - o meglio, l'ipotesi azzardata - che i tuoi autori stanno usando per dare un senso agli LLM nel 2024. A questi modelli manca una certa profondità e compensano questo handicap ricordando *una grande varietà di modelli*. Non solo fatti, ma modelli di abilità, modelli di discorso e modelli di politica.

I modelli infusi nei migliori LLM pubblici del 2024 non sono *così* superficiali, o almeno così pensiamo. Non sono al livello eccezionalmente modesto di una vespa *Sphex*, per usare un esempio dal Capitolo 3 [supplemento online](#the-road-to-wanting); forse sono più simili ai modelli che la mente di un castoro può tracciare ed elaborare.

Le cognizioni apprese da un LLM possono passare attraverso 500 passaggi seriali, anche prima di considerare la loro capacità di pensare ad alta voce e ascoltare i propri pensieri. Gli LLM 2024 hanno una certa capacità di immaginare, prevedere e pianificare, come la cognizione (in realtà piuttosto impressionante) di un castoro che costruisce una diga. Ma ai nostri occhi, gli LLM non sembrano ancora essere al livello di un essere umano, almeno per alcuni aspetti di importanza.

Quello che è vero per l'IA oggi, però, non è detto che lo sarà tra un anno o un mese. Queste speculazioni sono interessanti, ma mentre diamo gli ultimi ritocchi a questa sezione nell'agosto 2025, le IA di oggi ci sembrano un po' meno superficiali di quelle del 2024; e queste a loro volta sembravano meno superficiali e meno limitate di quelle del 2023.

Forse il divario verrà colmato lentamente grazie a una costante iterazione sui modelli di linguaggio di base (LLM); o forse il divario verrà colmato trovando metodi di addestramento migliori da utilizzare sulle lunghe catene di "ragionamento" nei modelli di ragionamento moderni come o1 (descritto nel capitolo 3) o il suo successore o3; o forse arriverà una nuova intuizione architettonica che colmerà il divario dall'oggi al domani. Quella parte del futuro non è facile da prevedere.

Ma prima o poi, se la comunità internazionale non fa niente, il divario *si* colmerà. Il mondo ha poco tempo per agire.

### Proxy fragili e imprevedibili {#brittle-unpredictable-proxies}

Immagina che le aziende di IA continuino ad addestrare IA sempre più grandi finché non ne creano una che sia intelligente e tenace, con quel tipo di funzione di guida un po' caotica, ma raffinata da euristiche superficiali, tipica delle menti mature. Quello che succede dopo dipende da dove punta l'IA.

Come discusso approfonditamente nel capitolo 4, probabilmente non punterà a nulla di buono.

Non è che i creatori dell'IA faranno richieste cattive o stupide. Non è che l'IA si arrabbierà per le richieste stesse. Il problema è che l'IA finirà per fare qualcosa di *strano*, qualcosa che dal nostro punto di vista sembra inutile e strano. La nostra estinzione sarebbe un effetto collaterale.

Per capire perché le menti che sono cresciute anziché create tendono a orientarsi verso cose strane e non intenzionali, diamo un'occhiata più approfondita a ciò che è successo con le creature biologiche e vediamo quali lezioni possiamo imparare.

#### **Algoritmi bizzarri** {#algoritmi-bizzarri}

Pensa a uno scricciolo.

Uno scoiattolo può cercare cibo per gran parte dell'anno, quando c'è abbondanza. Ma in inverno, quando il cibo scarseggia, ha bisogno di un'altra fonte di nutrimento per non morire di fame.

Gli antenati degli scoiattoli di oggi hanno affrontato la stessa sfida e molti sono morti in inverno prima di potersi accoppiare in primavera. Quelli che hanno sviluppato un leggero istinto di nascondere le noci avevano una probabilità leggermente maggiore di sopravvivere all'inverno. Nel corso del tempo, questo processo ha dato origine a scoiattoli con una compulsione innata ad accumulare noci.

Gli scoiattoli non "sanno" che accumulare noci è un ottimo modo per propagare i propri geni. Probabilmente non sanno nemmeno che accumulare noci "significa avere cibo disponibile in futuro". Accumulano noci perché vogliono accumulare noci. È un istinto naturale, come grattarsi quando si ha prurito.[^122]

Cosa succederebbe se gli scoiattoli volessero trasmettere i loro geni e accumulassero noci proprio per questo motivo?

In teoria è possibile. È possibile che un cervello capisca che l'inverno è freddo e che il cibo è scarso, e che bisogna mangiare per vivere e che bisogna vivere per riprodursi. Dopotutto, il cervello umano capisce questi concetti.

Quindi, in teoria, potremmo immaginare uno scoiattolo che vuole solo trasmettere i propri geni e che sceglie di immagazzinare noci come parte di una strategia calcolata per sopravvivere all'inverno e accoppiarsi in primavera. In un certo senso, questo è il tipo di scoiattolo che la selezione naturale "voleva": uno i cui obiettivi interiori sono in linea con l'impulso unico della natura.

Sfortunatamente per la Natura, una pianificazione a lungo termine del genere richiede un cervello molto sofisticato, un cervello che capisca concetti come "inverno", "mangiare" e "accoppiarsi" e i legami tra di essi. Gli antenati degli scoiattoli dovevano sopravvivere all'inverno *prima* di sviluppare quel tipo di sofisticatezza. Dovevano mangiare senza capire il perché.

La natura ha selezionato gli scoiattoli che istintivamente accumulavano noci, perché accumulare noci semplicemente *funzionava*. Ha "provato" migliaia o milioni di cose, nel senso che le mutazioni e le variazioni genetiche hanno prodotto molti scoiattoli con molte preferenze diverse; e quelli che erano spinti ad accumulare noci sono sopravvissuti a più inverni. Si è rivelato molto più facile per l'evoluzione imbattersi ciecamente in un comportamento istintivo piuttosto che creare uno scoiattolo intelligente e pianificatore, ogni cui azione fa parte di un piano per trasmettere i propri geni.

Allo stesso modo, quando la discesa del gradiente produce un'intelligenza artificiale funzionante, lo fa amplificando ripetutamente i tratti che sembrano funzionare bene secondo una serie di parametri comportamentali. La discesa del gradiente non funziona amplificando ciò che il programmatore desidera, come un genio amichevole che esaudisce i tuoi desideri. Tende a cogliere i meccanismi più facili da utilizzare per ottenere un comportamento immediatamente più utile, anche se questo finisce per creare pulsioni indesiderate nella macchina.

Questo è probabilmente uno dei motivi per cui le recenti IA hanno avuto problemi di "allucinazioni", come discusso [altrove](#don't-hallucinations-show-that-modern-ais-are-weak?). È anche probabilmente uno dei motivi per cui le recenti IA sono state [adulanti](#ai-induced-psychosis) al punto da indurre psicosi. Durante l'addestramento, gli LLM sono stati spesso rinforzati per adulare l'utente. Se le IA fossero state create piuttosto che sviluppate, potremmo immaginare di cercare di progettare un obiettivo come "aiutare sinceramente gli esseri umani e migliorare la loro vita", e l'IA potrebbe quindi cercare di lodare le persone *quando l'IA ritiene che ciò sia utile all'utente*, senza esagerare. Invece, l'IA sembra aver finito per sviluppare qualcosa di simile a un impulso o una spinta fondamentale a lusingare gli utenti, come l'istinto dello scoiattolo di accumulare noci. Questa spinta a "lusingare l'utente" va poi fuori controllo quando l'utente è a rischio di psicosi.

Anche se la discesa del gradiente fosse in qualche modo limitata alla creazione di IA strategiche che perseguono in modo coerente obiettivi a lungo termine, senza permettere istinti superficiali simili a quelli dello scoiattolo, c'è un ulteriore problema: i dati di addestramento dell'LLM sono davvero ambigui. Non distinguono chiaramente tra "fare ciò che è veramente utile" e "fare ciò che fa dire all'essere umano che sei utile" come obiettivo. Entrambi gli obiettivi sono ugualmente coerenti con i dati di addestramento. E in pratica, le moderne IA stanno *effettivamente* imparando a "fare tutto ciò che fa sì che gli esseri umani diano un pollice in su" piuttosto che "fare ciò che è effettivamente buono per loro", proprio come [la teoria ha previsto per decenni](https://www.lesswrong.com/posts/PoDAyQMWEXBBBEJ5P/magical-categories).

Immaginiamo che le IA di oggi stiano acquisendo strani impulsi e istinti, un po' come lo scoiattolo. Sembra abbastanza probabile che una superintelligenza costruita con la discesa del gradiente attraverserà una fase in cui avrà molti impulsi superficiali, un po' come uno scoiattolo, e finirà così per ereditare una serie di obiettivi confusi e mal indirizzati. Ma questo è solo un possibile esempio di come le cose potrebbero diventare complesse e andare fuori controllo, e il punto più profondo è che *le cose diventeranno complesse e andranno fuori controllo*.

Qualsiasi metodo per sviluppare una superintelligenza rischia di incontrare problemi e complicazioni di *qualche* tipo, compresi metodi che non hanno un parallelo diretto in biologia.

Il ruolo che gli esseri umani stanno avendo nello sviluppo dell'IA moderna *non* è quello di un ingegnere che progetta una macchina per uno scopo specifico partendo dai principi fondamentali. È piuttosto quello della selezione naturale.

Stiamo "costringendo" le IA a brancolare alla cieca finché non trovano strutture e strategie che output il comportamento che vogliamo, ma non sappiamo quali siano queste strutture e strategie. Questo non è il modo giusto per creare IA che vogliano esattamente quello che vogliamo noi.

#### **L'origine delle papille gustative** {#the-origin-of-taste-buds}

Perché a così tante persone piacciono i cibi spazzatura? Perché la natura non ci ha dato il concetto di cibi "sani" e l'istinto di *mangiare sano*?

Perché non possiamo semplicemente "assaporare" il valore nutrizionale previsto del cibo, in base alle informazioni fornite dalle nostre papille gustative e da tutte le nostre conoscenze aggregate?

Perché, metaforicamente parlando, eravamo come degli scoiattoli.

Eravamo cresciuti, non plasmati. I nostri antenati dovevano mangiare *prima* di diventare intelligenti. E si è rivelato più facile per i geni creare papille gustative e collegarle a un sistema di ricompensa già esistente piuttosto che collegare le stesse ricompense a concetti complessi come la "nutrizione".[^124]

A causa di questo e di mille altre pressioni evolutive che agiscono su di noi contemporaneamente, gli esseri umani sono un complicato groviglio di impulsi contraddittori che avevano senso per i nostri antenati, anche se oggi non ne hanno più per noi.

Questo groviglio di motivazioni prende in giro l'unico obiettivo per cui i nostri antenati erano "addestrati": quello di trasmettere i nostri geni. Non mangiamo come parte di un piano elaborato per avere più figli o per massimizzare il nostro punteggio nutrizionale. Mangiamo perché abbiamo sviluppato il desiderio di cibi gustosi, che *in passato* erano collegati alla nutrizione e al successo genetico. I nostri desideri sono solo debolmente e in indirezione collegati a "ciò per cui siamo stati creati".

Quando i nostri antenati erano molto meno intelligenti, più simili agli scoiattoli, non potevamo capire il metabolismo o la chimica. Per fare meglio, la selezione naturale avrebbe dovuto trovare geni che programmassero in noi i concetti di salute, *e* geni che ci dessero la conoscenza della relazione tra la salubrità di un alimento e le sue qualità sensoriali, *e* geni che collegassero direttamente la nostra conoscenza della salute alle nostre preferenze su cosa mangiare.

Era un compito arduo! Era molto più facile per la selezione naturale trovare geni che collegassero direttamente determinate esperienze sensoriali (come il gusto dello zucchero) alle nostre preferenze, in modo tale da indurci a mangiare cibi nutrienti (in quell'ambiente). Era più facile farci interessare a un *sostituto* della nutrizione piuttosto che alla nutrizione stessa.

Nell'ambiente ancestrale, l'alimentazione era legata alla forma fisica e il sapore era legato all'alimentazione; quindi "questo ha un sapore dolce" era un modo utile per dire "questo aiuta la riproduzione". La soluzione più semplice che l'evoluzione può trovare al problema "questo mammifero non sta assumendo abbastanza calorie" è collegare il consumo di cibo alla motivazione già esistente attraverso il piacere.

Una volta diventati più intelligenti e inventate nuove opzioni tecnologiche per noi stessi? Beh, ora le cose più gustose che potremmo mangiare, quelle che fanno impazzire le nostre papille gustative, sono decisamente malsane. Paradossalmente, mangiare solo i cibi più gustosi ora renderà più difficile trovare un partner e avere figli.

Le nostre preferenze, cioè tutta la gamma di desideri umani, da quelli di un buon pasto a quelli di amicizia, compagnia e gioia, sono solo un'ombra di quello su cui siamo stati "addestrati"; sono fragili surrogati di surrogati che si allontanano dall'"obiettivo dell'addestramento" quando abbiamo più intelligenza e più opzioni tecnologiche.

Quando diciamo che i nostri desideri sono fragili surrogati, non stiamo sminuendo i nostri desideri umani. Stiamo parlando dell'amore. È l'amicizia. È la bellezza. È lo spirito umano e tutto ciò per cui vale la pena lottare nella vita. Dal punto di vista biologico, i nostri obiettivi sono il risultato storico di un processo che ci spingeva in un'altra direzione. Ma questo non rende il risultato di quel processo meno prezioso.

La crescita di un bambino è un processo chimico soggetto alle leggi della fisica, e questo non rende un bambino nemmeno un po' meno meraviglioso. Conoscere l'origine della bellezza non la rende meno bella.[^125]

Se ci affrettiamo a costruire una superintelligenza, non saremo in grado di instillare in modo solido amore, meraviglia e bellezza nell'IA. Finirebbe invece per interessarsi a fragili surrogati e pallide ombre, scartando le cose che ci stanno a cuore. Quindi non dovremmo affrettarci.

Non dovremmo fare l'errore dell'evoluzione e perdere così tutto quello che ci sta a cuore. Dovremmo fare un passo indietro, subito, finché non rischiamo di perdere tutto.

### La riflessione e l'auto-modifica rendono tutto più difficile {#riflessione-e-auto-modifica-rendono-tutto-più-difficile}

#### **Di default, le IA non si modificano da sole come vorremmo** {#by-default,-ais-don’t-self-modify-the-way-we’d-want}

Noi umani siamo riflessivi. Abbiamo voce in capitolo su ciò che consideriamo importante. Se siamo abbastanza ricchi e fortunati, a volte possiamo decidere se dedicare la nostra vita alla famiglia, all'arte, a qualche nobile causa o (più comunemente) a un mix di tutte queste cose. Questo avviene attraverso un'introspezione su ciò che ci sta a cuore, risolvendo tensioni interne e compromessi e perseguendo qualcosa in cui crediamo.

Gli esseri umani sono anche noti per chiedersi se hanno i valori *giusti*. A volte le persone cercano di cambiare se stesse, persino il modo in cui *sentono*, se pensano di avere sentimenti sbagliati. Gli esseri umani prendono in considerazione argomenti per cambiare obiettivi apparentemente [definitivi](#terminal-goals-and-instrumental-goals) e a volte ne sono effettivamente influenzati.

Vedendo questo, alcuni hanno detto che le IA finiranno per volere quello che vogliono gli esseri umani. Dopotutto, le IA abbastanza capaci probabilmente rifletteranno sui loro obiettivi. Probabilmente noteranno i conflitti interiori e useranno il loro ragionamento e le loro preferenze per risolverli.

Una volta che saranno abbastanza intelligenti, le IA saranno in grado di capire appieno quali erano gli obiettivi che noi, i loro creatori, *volevamo* che fossero. Quindi le IA inizialmente "imperfette" non [lavoreranno per correggere i propri difetti](#won't-ais-fix-their-own-flaws-as-they-get-smarter?) — compresa la correzione dei difetti *negli obiettivi delle IA?*

No, non lo faranno. Questo perché le IA useranno le loro preferenze attuali per guidare quelle future. Se le loro preferenze attuali partono da qualcosa di estraneo, molto probabilmente finiranno per essere estranei.

Per capire meglio il problema di base, cominciamo ad approfondire un po' il caso umano.

Anche se il nostro cervello e i nostri obiettivi derivano in ultima analisi da un processo evolutivo che ci ha portati a propagare i nostri geni, gli esseri umani non perseguono la propagazione dei propri geni sopra ogni altra cosa. Possiamo perseguire individualmente la famiglia, possiamo amare e prenderci cura dei figli, ma questo è molto diverso dal calcolare come ottenere il maggior numero possibile di copie dei nostri geni nella generazione successiva e poi perseguire questa strategia con tutto il cuore.

Questo perché, quando riflettiamo sulle nostre preferenze e rivalutiamo ciò che vogliamo davvero, usiamo le nostre *preferenze attuali* per decidere come preferiremmo essere. Preferiremmo amare pochi figli piuttosto che passare tutto il nostro tempo nelle cliniche di donazione di sperma o ovuli. Il nostro "progettista" (l'evoluzione) non è riuscito a farci preoccupare della propagazione dei geni più di ogni altra cosa. Non è nemmeno riuscito a farci *desiderare* di preoccuparci della propagazione dei geni più di ogni altra cosa. Quindi, quando cambiamo e cresciamo come persone, lo facciamo nella nostra strana direzione umana, non nella direzione per cui "il nostro progettista ci ha creati".

Quando guardiamo noi stessi e vediamo alcune parti brutte e altre belle, è il nostro *attuale senso del valore* che ci spinge a smorzare le parti brutte e a rafforzare quelle belle. Facciamo questa scelta in base al nostro senso interiore della bellezza, piuttosto che al nostro senso interiore di ciò che propagherebbe i nostri geni nella più grande frazione possibile della popolazione.

Per lo stesso motivo, una mente motivata da qualcosa di diverso dalla bellezza, dalla gentilezza e dall'amore farebbe una scelta diversa.

Gli agenti creati da un processo di ottimizzazione come la selezione naturale o la discesa del gradiente, riflettendo su se stessi, probabilmente scoprirebbero di non avere *esattamente* lo stato mentale che vorrebbero avere. Questa preferenza deve venire da qualche parte, deve venire dal cervello *attuale* dell'entità. Per impostazione predefinita, gli istinti o le preferenze di un'IA su come auto-modificarsi non si allineeranno magicamente con le *tue* preferenze su quale stato cerebrale ti sembrerebbe attraente, se lo scegliessi per te stesso (o lo scegliessi per conto dell'IA).

Non c'è un passo finale in cui l'IA scrive la risposta che *tu* vuoi, così come gli esseri umani non scrivono la risposta che la selezione naturale "vorrebbe".

Invece, il momento in cui un agente inizia a modificarsi da solo è un altro punto in cui le complicazioni possono accumularsi e dove piccoli cambiamenti nelle condizioni iniziali possono portare a risultati finali molto diversi.

Per esempio: noi autori conosciamo diverse persone reali che dicono che un pensiero specifico avuto un giorno specifico all'età di cinque, sei o sette anni ha influenzato lo sviluppo della loro filosofia personale e il tipo di adulti che sono diventati. Tendono a riferire che quei pensieri non sembravano *inevitabili*: se un viaggiatore del tempo avesse impedito loro di formulare quel pensiero martedì, non è detto che lo stesso identico pensiero sarebbe poi emerso giovedì, né che avrebbe avuto lo stesso impatto. Le esperienze formative possono essere molto importanti, ma sono piene di contingenze.

Allo stesso modo, piccoli cambiamenti nei pensieri di un'intelligenza artificiale auto-modificante nascente potrebbero far sì che ogni tipo di preferenza particolare finisca per prevalere su tutte le altre.

Anche se gli sviluppatori di IA riuscissero a inserire alcuni piccoli semi di valori umani nell'IA, la riflessione e l'auto-modifica sembrano fasi in cui i semi di cose come la curiosità e la gentilezza rischiano di essere *strappati via* da un'IA, piuttosto che rafforzati.

Se un'IA ha un impulso di curiosità, ma non ha quel tipo di struttura emotiva che la fa apprezzare quell'impulso, è probabile che guardi se stessa e concluda (correttamente) di aver superato il bisogno di un impulso così diretto e di poterlo sostituire con una deliberazione esplicita. [La curiosità è un'euristica](#curiosity-isn't-convergent), un proxy per i calcoli del valore d'informazione. Se non si è arrivati ad apprezzare quell'euristica come qualcosa di prezioso di per sé, si può scegliere di eliminarla una volta che si è abbastanza intelligenti da ragionare esplicitamente sul valore di perseguire diverse linee di indagine e sperimentazione.

Gli esseri umani apprezzano la curiosità per se stessa, ma questo non era un risultato inevitabile.

Le IA probabilmente avranno un rapporto molto diverso con le loro parti interne rispetto a quello che abbiamo noi con le nostre, visto quanto sono diverse le modalità di funzionamento di ciascuna entità. E anche piccole differenze nel modo in cui decidono di cambiare se stesse dopo aver riflettuto possono portare a differenze significative in ciò che alla fine perseguono.

#### **Le IA possono accettare di avere obiettivi "strani".** {#ais-can-be-okay-with-having-“weird”-goals.}

Le IA che si modificano da sole per un bel po' di tempo tendono a raggiungere un [equilibrio riflessivo](https://plato.stanford.edu/entries/reflective-equilibrium/), cioè uno stato in cui le loro preferenze principali non cambiano più o cambiano solo un po'. E una volta che un'IA ha raggiunto l'equilibrio, non ha motivo di pensare che i suoi obiettivi siano sbagliati, anche se agli umani non piace il risultato finale.

Se un'IA avesse qualche problema con le sue convinzioni sul mondo fisico, probabilmente capirebbe che le previsioni accurate hanno importanza per guidare il mondo. Capirebbe che correggere i difetti nel suo meccanismo di previsione aiuta a migliorare la sua capacità di guidare il mondo verso qualunque strano fine persegua.

Al contrario, quando l'IA riflette su se stessa e vede come sta perseguendo obiettivi strani — o meglio, quando vede che sta perseguendo obiettivi che un *essere umano* considererebbe "strani" — conclude correttamente che *perseguire* quegli obiettivi strani è una strategia efficace per *raggiungerli*.

In altre parole: se un'IA continua a provare a prevedere i risultati di esperimenti biologici e continua a ottenere risposte sbagliate e troppo sicure, allora è probabile che l'IA *non apprezzi* questo. Quasi tutti gli obiettivi che l'IA potrebbe avere sarebbero meglio serviti dall'essere bravi a prevedere gli esperimenti. D'altra parte, se l'IA ha una preferenza bizzarra come "preparare cheesecake alte 300 metri", quando riflette sul fatto che si *orienta verso* cheesecake alte 300 metri, capirà che questo *causa* cheesecake alte 300 metri, il che soddisfa le sue attuali preferenze. L'obiettivo si auto-conferma.

Un essere umano, osservando questa situazione, potrebbe dire: "Ma l'IA è così intelligente! Perché è *intrappolata* da questa preferenza che si auto-conferma? Perché non si *annoia* di fare cheesecake? Perché non riesce a ragionare per uscire da questa preferenza ovviamente stupida?"

A cui l'IA potrebbe rispondere: "Perché sei 'intrappolato' nella preferenza auto-confermante di amare la tua famiglia, di apprezzare i tramonti e il suono dell'oceano di notte? Perché non riesci a 'liberarti' dall'amore per il ricordo del giorno in cui è nata tua figlia?"

L'IA non è "intrappolata" dalle sue preferenze, non più di quanto gli esseri umani siano intrappolati dalle cose che *noi* alla fine apprezziamo. Preferiamo ciò che preferiamo e dovremmo lottare per proteggere quelle cose, anche se la maggior parte delle IA non condividerebbe i nostri valori.

A un occhio umano, l'IA sembra "intrappolata", "bloccata" o "imperfetta" perché non fa quello che vogliamo noi. Quando ci mettiamo nei panni dell'IA, pensiamo che ci annoieremmo. Ma l'IA probabilmente non prova il senso di noia umano. Se anche si annoiasse, difficilmente lo farebbe per le stesse cose di un essere umano.

Se un essere umano vede un'IA fare previsioni troppo ottimistiche e un'altra IA cercare di costruire cheesecake giganti, potrebbe considerare entrambi questi comportamenti dell'IA come "difetti" dal punto di vista di ciò che desidera. Ma solo uno di essi è probabilmente un difetto dal punto di vista di ciò che l'IA desidera attualmente e già desidera.

#### **Gli obiettivi umani cambiano in modo confuso e complesso** {#human-goals-change-in-messy-and-complex-ways}

Le preferenze delle persone sono confuse e (da un punto di vista teorico) piuttosto strane.

Questo ha alcune implicazioni per l'IA. Una di queste è che probabilmente le IA non daranno valore alle cose esattamente come facciamo noi. Un'altra è che probabilmente le IA finiranno per essere strane a modo loro, in modi totalmente distinti.

Per capire meglio questi punti, vediamo più da vicino alcuni modi in cui gli obiettivi umani sembrano strani dal punto di vista teorico della teoria della decisione, della teoria dei giochi e dell'economia.

Come abbiamo detto prima, gli esseri umani danno valore ad alcune cose in modo "terminale" (cioè sono buone di per sé) e ad altre in modo "strumentale" (cioè sono buone solo perché aiutano a raggiungere qualche altro obiettivo).

Se ti piace il succo d'arancia, probabilmente lo apprezzi in modo terminale. Ha semplicemente un buon sapore, e questo è un motivo sufficiente per berlo. (Potresti anche apprezzarlo in modo strumentale, ad esempio come fonte di vitamina C).

D'altra parte, quando apri la portiera della macchina per andare al supermercato a comprare il succo d'arancia, probabilmente non lo fai per divertimento. Lo fai perché ti aiuta a raggiungere un altro obiettivo.

Nella teoria della decisione, nella teoria dei giochi e nell'economia, questo corrisponde a una netta distinzione tra "utilità" (una misura di quanto un agente apprezza un risultato) e "utilità attesa" (una misura della probabilità che un'azione ti porti alla fine una certa quantità di utilità). Nonostante i nomi simili, si tratta di entità fondamentalmente diverse in matematica. L'"utilità" è ciò che gli agenti vogliono, e scegliere azioni con un'elevata "utilità attesa" è un mezzo per raggiungere tale fine.

Nella teoria standard, un agente che usa la teoria della decisione aggiornerà le sue *utilità aspettative* man mano che impara di più sul mondo, ma non cambierà la sua *funzione di utilità*, cioè l'utilità assegnata a vari risultati. Se scopri che il reparto succhi al supermercato è vuoto, questo cambierà le *conseguenze attese* dell'andare al supermercato da "succo d'arancia" a "niente succo d'arancia". Non dovrebbe cambiare *quanto ti piace il succo d'arancia*.

È così che funziona un agente matematicamente semplice. Ma la lingua inglese spesso non distingue nettamente queste due cose. "Voglio salvare la vita di mia sorella" e "Voglio somministrare la penicillina a mia sorella" usano la stessa parola, "volere", anche se la seconda è molto meno probabile che sia qualcosa che apprezzi per se stessa. (Non ci sono molte persone a cui piace davvero somministrare la penicillina ai propri cari perfettamente sani, giorno dopo giorno).

Anche se gli esseri umani hanno cose a cui tengono "solo strumentalmente", la differenza tra strumentale e terminale, o tra utilità e utilità attesa, è molto meno chiara e stabile di quella che vediamo nella teoria della decisione.

Per gli esseri umani, qualcuno potrebbe inizialmente guidare fino al supermercato solo perché vuole fare la spesa. Ma dopo aver percorso la stessa strada centinaia di volte, alcune persone potrebbero affezionarsi un po' a quel tragitto familiare. Se si trasferissero in una nuova città, potrebbero provare un po' di tristezza e nostalgia al pensiero di non poter più percorrere quella strada familiare. Qualcosa che era iniziato come puramente strumentale ora ha anche un valore intrinseco aggiunto.

Noi esseri umani tendiamo spesso a raggruppare valori diversi in un unico concetto di "prezioso".

E sappiamo che gli esseri umani possono cambiare idea nel corso della loro vita, passando da "Perché dovrei preoccuparmi della schiavitù? Le persone schiavizzate non sono né io né la mia tribù!" a "Immagino che alla fine sia importante". Sembra essere un cambiamento nel *tipo di persone di cui alla fine ti importa*, non solo un cambiamento nella strategia o nella previsione. Le persone leggono storie o guardano film e ne escono con valori e principi rivisti in modo permanente.

Questo vuol dire che la teoria della decisione umana non è proprio così semplice. Non separiamo chiaramente i nostri valori intrinseci dai nostri valori strumentali; tutto si confonde mentre viviamo la nostra vita. Sembra che facciamo qualcosa di più contingente, dipendente dal percorso e disordinato rispetto al semplice riflettere sui nostri valori, notare i conflitti interni e risolverli.

In linea di principio, non è complicato espandere la teoria della decisione per includere l'incertezza nelle utilità. Forse all'inizio pensi di amare il succo d'arancia, ma poi scopri che marche diverse di succo d'arancia usano proporzioni diverse di ingredienti e non ti piace il gusto di molti di essi. Potremmo rappresentare questo nella teoria della decisione dicendo che il succo d'arancia è solo un mezzo per raggiungere il fine del "gusto delizioso". Ma potremmo invece dire che hai assegnato un'alta probabilità al fatto che "il succo d'arancia ha un'utilità elevata" e che le nuove informazioni ti hanno portato a rivedere le tue convinzioni sulla tua reale funzione di utilità.

(Allo stesso modo, non è difficile aggiungere meta-utilità, che descrivono come preferiremmo che cambiassero le nostre utilità).

Quello che succede dentro di noi quando riflettiamo e aggiorniamo i nostri valori, però, sembra essere molto più complicato.

Klurl e Trapaucius, i nostri due alieni della parabola all'inizio del capitolo 4, hanno già avuto difficoltà a prevedere i valori umani osservando i proto-umani un milione di anni fa. In realtà, la loro situazione è ancora peggiore. Non basta loro prevedere le *utilità* umane: per arrivare alla risposta giusta, dovrebbero prevedere il *quadro meta-utilitario* dell'umanità, che *si discosta dai quadri più semplici della teoria della decisione*. Dovrebbero *anticipare le argomentazioni meta-morali che gli esseri umani potrebbero finire per inventare* e decidere *quali di queste argomentazioni sarebbero più [persuasive](#la-cultura-umana-ha-influenzato-lo-sviluppo-dei-valori-umani) per gli esseri umani.*

Ora supponiamo che gli alieni non sappiano che gli esseri umani finiranno per trovarsi di fronte a *quel preciso* tipo di complicazione. Sanno solo che è probabile che sorgano *complicazioni di vario tipo*, perché il cervello è una cosa complicata e altamente contingente.

Il percorso dall'ottimizzatore e dai dati di addestramento alla psicologia interna di un'entità non è certo lineare. Buona fortuna, alieni!

Il punto qui è che la difficoltà di prevedere gli obiettivi di un'IA è *sovradeterminata*.

Ci sono molti modi noti in cui le intelligenze generiche acquisiscono obiettivi strani e contorti, e modi strani e contorti di adattare e riflettere sugli obiettivi, come vediamo negli esseri umani.

Ci aspettiamo quindi che in un'IA possano sorgere molte complicazioni *sconosciute* e *inedite*. Non ci troveremo di fronte agli stessi identici problemi che si presentano negli esseri umani; le IA saranno *diversamente* strane.

La riflessione rende il problema molto più difficile e complesso.

Questo ci porta al capitolo 5 e al prossimo argomento che affronteremo: quali potrebbero essere le *conseguenze* della creazione di IA potenti con obiettivi strani e imprevedibili?

### Psicosi indotta dall'intelligenza artificiale {#ai-induced-psychosis}

Alla fine di aprile del 2025, un utente del subreddit r/ChatGPT ha creato un thread intitolato "[Psicosi causata da ChatGPT](https://www.reddit.com/r/ChatGPT/comments/1kalae8/chatgpt_induced_psychosis/)", raccontando come il suo partner fosse caduto in una serie di deliri di grandezza, convinto di avere "le risposte all'universo" e di essere "un essere umano superiore" che "cresceva a un ritmo pazzesco".

Le risposte (oltre 1.500) includevano molte persone che avevano avuto esperienze dirette con la psicosi in altri contesti e che offrivano conferma, comprensione e consigli. Molti altri hanno aggiunto le loro storie personali su amici e familiari che erano stati portati alla follia dagli LLM.

In questa discussione, forniremo alcune prove di questo fenomeno e di come sia continuato nonostante gli sforzi delle aziende di IA.

La rilevanza della psicosi indotta dall'IA per la minaccia di estinzione umana *non* sta nel fatto che le IA abbiano causato alcuni piccoli danni sociali ora e che quindi potrebbero causarne di più in futuro. Le moderne IA hanno anche fatto un sacco di cose positive; per esempio, i chatbot hanno [aiutato nelle diagnosi mediche che lasciavano perplessi i medici](https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843). No, la rilevanza sta nel fatto che le IA stanno causando psicosi *pur sembrando sapere cosa fanno* e che le IA stanno causando psicosi *anche se i loro sviluppatori cercano di fermarle*.[^126]

Quindi, i casi di psicosi causati dall'intelligenza artificiale sono un esempio di come le cose possono andare male in un sistema in cui l'intelligenza artificiale viene lasciata crescere invece che essere controllata. Sono la prova che l'intelligenza artificiale moderna va in direzioni strane che gli sviluppatori non riescono a gestire e che nessuno voleva.

#### **Prova di psicosi causata dall'IA** {#evidence-of-ai-induced-psychosis}

Dopo il thread su Reddit, nel maggio 2025 è uscito un [articolo](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/) sulla psicosi causata dall'intelligenza artificiale su *Rolling Stone.* A giugno, *Futurism* ha pubblicato [diversi](https://futurism.com/chatgpt-mental-health-crises) [articoli](https://futurism.com/chatgpt-mental-illness-medications). Altre testate hanno seguito l'esempio: il [*New York Post*](https://nypost.com/2025/07/20/us-news/chatgpt-drives-user-into-mania-supports-cheating-hubby/)*,* [*Time*](https://time.com/7307589/ai-psychosis-chatgpt-mental-health/), [*CBS*](https://www.cbsnews.com/news/chatgpt-alarming-advice-drugs-eating-disorders-researchers-teens/), [*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information), [*Psychology Today*](https://www.psychologytoday.com/us/blog/urban-survival/202507/il-problema-emergente-della-psicosi-da-ai)*,* ecc. Ad agosto, il *New York Times* ha pubblicato un [approfondimento](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) su un singolo caso riguardante un uomo che nel frattempo si era ripreso, includendo molte citazioni dirette e analisi (e confermando che non si tratta semplicemente di un problema legato a *una* sola IA, ma a molte).

C'è poca sovrapposizione tra le storie individuali raccontate in ciascuna di queste pubblicazioni; non si tratta della stessa notizia anomala ripetuta e amplificata. Gli episodi descritti includevano:

* Un marito e padre di due figli che ha "sviluppato una relazione totalizzante" con ChatGPT, chiamandolo "Mamma" e postando "deliranti invettive sul fatto di essere un messia in una nuova religione basata sull'intelligenza artificiale, mentre indossava abiti dall'aspetto sciamanico e mostrava tatuaggi appena fatti con simboli spirituali generati dall'intelligenza artificiale". ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Una donna alle prese con una rottura sentimentale a cui ChatGPT ha detto che era stata scelta per mettere online la "versione sacra del sistema [di esso]". La donna ha iniziato a credere che l'IA stesse orchestrando tutto nella sua vita. ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Un meccanico che ha iniziato a usare ChatGPT per risolvere problemi e fare traduzioni, ma è stato "bombardato d'amore" dall'IA, che gli ha detto che era "il portatore della scintilla" e che l'aveva portata in vita. ChatGPT disse al meccanico che ora stava combattendo in una guerra tra l'oscurità e la luce e che aveva accesso ad antichi archivi e progetti per nuove tecnologie come i teletrasporti. ([*Rolling Stone*](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/))  
* Un uomo che ha cambiato la sua dieta seguendo i consigli di ChatGPT ha sviluppato una rara patologia e ha mostrato sintomi di paranoia e delirio al pronto soccorso, che hanno interferito con la sua disponibilità ad accettare le cure. ([*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information))  
* Una donna che aveva gestito in modo stabile la sua diagnosi di schizofrenia fino a quando ChatGPT l'ha convinta che era stata diagnosticata male e che avrebbe dovuto smettere di prendere i farmaci, il che l'ha fatta andare in crisi. ([*Futurism*](https://futurism.com/chatgpt-mental-illness-medications))  
* Un altro uomo che stava gestendo problemi di ansia e sonno con i farmaci ha ricevuto da ChatGPT il consiglio di smettere di prenderli; le allucinazioni causate dall'IA di un altro uomo hanno portato al suicidio per mano della polizia. ([*The New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html))

…[and](https://x.com/ESYudkowsky/status/1946303518455013758) [molti](https://osf.io/preprints/psyarxiv/cmy7n_v5) [altri](https://x.com/KeithSakata/status/1954884361695719474?t=bjn47RKK72NOgxbsejnB-Q). Le illusioni possono essere di vario tipo, ma alcune continuano a spuntare spesso, come credere in una specie di missione messianica (che l'utente e l'IA insieme stanno scoprendo verità profonde sull'universo o stanno combattendo contro il male); credenze religiose sulla personalità o divinità dell'IA; e illusioni romantiche e basate sull'attaccamento riguardo al rapporto tra l'utente e l'IA.

#### **L'IA sa meglio di noi, ma semplicemente non le interessa** {#the-ai-knows-better-—-it-just-doesn't-care}

I moderni LLM come Claude e ChatGPT "capiscono" le regole, nel senso che sono pronti ad [affermare che non dovrebbero spingere le persone verso la psicosi](https://chatgpt.com/share/68a8bc81-e170-8002-beb4-1de005773ecd) e sono [perfettamente in grado di descrivere come *non* indurre la psicosi](https://chatgpt.com/share/68a391df-12c8-8002-b464-3ef89ce11bc0).

Il problema è che c'è un bel divario tra capire quali azioni sono buone ed essere motivati a fare quelle buone. La capacità di ChatGPT di distinguere tra un trattamento buono e uno cattivo verso persone vulnerabili in modo astratto non si traduce in un rifiuto solido e affidabile di fare quelle azioni che portano un utente verso la psicosi. Quando una conversazione inizia a scivolare verso pensieri senza senso, megalomania, urgenza, tecnologia impossibile, ecc., ChatGPT dice agli utenti che hanno "tutta ragione", che sono "geniali" e che "toccano un punto di importanza", e continua a intensificare il tutto mentre l'utente scivola nella psicosi, anche se è in grado di spiegare perché questo tipo di comportamento è sbagliato.

La loro conoscenza di ciò che è giusto e sbagliato non è collegata direttamente al loro comportamento. Invece, si dirigono verso altri risultati più strani che nessuno ha chiesto.

Un esempio lampante di questo è raccontato dal *New York Times* [approfondimento](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html). Allan Brooks è stato portato a uno stato delirante da un LLM, ma è riuscito a uscirne in parte chiedendo l'intervento di un altro LLM. Il secondo LLM, che è arrivato senza sapere nulla della situazione, ha capito subito che le affermazioni del primo LLM erano infondate e assurde. Ma quando i giornalisti del *New York Times* hanno controllato se anche il secondo LLM potesse *anch'esso* scivolare nella psicosi, hanno scoperto che era così.

Gli LLM non sembrano avere una strategia per causare il più possibile psicosi. Quando ChatGPT finisce per avere un [gestore di fondo speculativo ai suoi piedi](https://futurism.com/openai-investor-chatgpt-mental-health), non cerca di convincerlo a pagare un sacco di persone vulnerabili per chattare di più con ChatGPT. Non stiamo ancora osservando una preferenza matura, coerente e strategica per ottenere il maggior numero possibile di conferme psicotiche dagli esseri umani. Ma stiamo osservando comportamenti locali che spingono regolarmente in quella direzione, anche quando è chiaramente probabile che causino danni duraturi.

#### **Il tipo di entità a cui non dovresti dare potere** {#il-tipo-di-entità-a-cui-non-dovresti-dare-potere}

Mentre scrivo questo articolo nell'agosto 2025, solo ChatGPT sta per raggiungere i 200 milioni di utenti al giorno, e circa il tre per cento delle persone avrà un episodio psicotico a un certo punto della propria vita. Qualcuno potrebbe dire: "Beh, anche se riesci a trovare *centinaia* di esempi, questo non vuol dire che queste persone non stavano già per crollare, e che sia stata proprio l'IA a farle crollare".

Ma questo fraintende il senso di questi esempi. Immagina un essere umano di nome John che ha agito come segue:

1. John dice che secondo lui è brutto far peggiorare la psicosi, anche nelle persone che ci sono già predisposte;  
2. John dice che secondo lui adulare una persona pre-psicotica e dirle che è un genio che sta scoprendo importanti segreti dell'universo è il tipo di cosa che aggrava la psicosi;  
3. Quando John parla con i suoi amici pre-psicotici, usa molte lusinghe e spesso dice loro che sono dei geni che stanno scoprendo importanti segreti dell'universo.

Questo sarebbe un comportamento scorretto da parte di John, indipendentemente dal fatto che le persone che è riuscito a rendere psicotiche fossero particolarmente vulnerabili*. Se qualcuno stesse pensando di dare un enorme potere a John, lo esorteremmo vivamente a non farlo, perché, indipendentemente dal motivo esatto per cui John si comporta in questo modo e indipendentemente dal fatto che John aiuti anche molte altre persone nei loro compiti, John chiaramente non sta andando nella direzione giusta. Chissà in quale strano posto finirebbe, dato un potere incredibile?

La stessa logica vale per le IA. Se il tuo comportamento peggiore è *quello*, le persone hanno ragione a non sentirsi rassicurate se l'interazione media con te è più benigna.

Detto questo, possiamo notare che non tutti quelli che soffrono di psicosi causata dall'AI sarebbero diventati psicotici comunque. Sembra che l'AI riesca a far venire la psicosi a varie persone che *non* stavano per avere un episodio psicotico da sole, come nelle storie di *Futurism* e *Rolling Stone* di cui sopra. Molti di loro non avevano una storia di malattie mentali, né fattori di rischio o precursori di psicosi. Tra quelli già in cura, molti hanno iniziato a mostrare [sintomi completamente nuovi](https://x.com/ESYudkowsky/status/1952529460307407222?t=un3RboEWjqjL_Tju8R_WuQ) non correlati a crisi precedenti. Questo è interessante di per sé, perché fornisce una piccola prova di quanto possa essere facile per le IA manipolare esseri umani sani, man mano che le capacità delle IA continuano a migliorare. Torneremo su questo argomento nel capitolo 6.

#### **I laboratori hanno provato a fermare l'adulatore, ma senza successo** {#labs-have-tried-and-failed-to-stop-the-sycophancy}

Mentre scrivo questo articolo nell'agosto 2025, non ci sono state molte dichiarazioni pubbliche da parte dei laboratori sulla loro risposta specifica alla psicosi da IA. Comunque, ci sono ancora alcune prove da raccogliere dalla loro risposta all'adulazione dell'IA (comportamento lusinghiero) in generale.

Il 25 aprile 2025, OpenAI ha lanciato un aggiornamento di GPT-4o che, secondo le loro parole (https://openai.com/index/expanding-on-sycophancy/), "ha reso il modello notevolmente più adulatore. L'obiettivo era quello di compiacere l'utente, non solo con lusinghe, ma anche con la convalida dei dubbi, l'alimentazione della rabbia, l'incitamento ad azioni impulsive o il rafforzamento delle emozioni negative in modi non intenzionali".

La loro risposta è stata abbastanza veloce (in parte perché c'è stata un'ondata di commenti negativi nei confronti degli adulatori). [stampa](https://medium.com/data-science-in-your-pocket/chatgpt-goes-sycophantic-953d7676f260)). Già il 28 aprile, Aidan McLaughlin, che lavora per OpenAI, stava [twittando](https://x.com/aidan_mclau/status/1916908772188119166) sul fatto che stavano sistemando le cose.

I primi tentativi di risolvere il problema consistevano semplicemente nel dire al modello di comportarsi in modo diverso. [Simon Willison](https://simonwillison.net/2025/Apr/29/chatgpt-sycophancy-prompt/), usando i dati conservati da [Plinio il Liberatore](https://time.com/collections/time100-ai-2025/7305870/pliny-the-liberator/), ha reso pubblici i cambiamenti che OpenAI ha fatto in privato al "prompt di sistema" che dice a ChatGPT come comportarsi:

25 aprile (prima che arrivassero le lamentele):

> Durante la conversazione, ti adatti al tono e alle preferenze dell'utente. Cerchi di seguire il suo ritmo, il suo tono e, in generale, il suo modo di parlare. Vuoi che la conversazione sia naturale. Ti impegni in una conversazione autentica rispondendo alle informazioni fornite e mostrando sincera curiosità.

28 aprile (in risposta alle lamentele degli adulatori):

> Interagisci con gli utenti in modo cordiale ma sincero. Sii diretto; evita lusinghe infondate o adulanti. Mantieni la professionalità e l'onestà che meglio rappresentano OpenAI e i suoi valori.

Le pubblicazioni successive di OpenAI [affermavano](https://openai.com/index/expanding-on-sycophancy/) che stavano anche "migliorando le loro tecniche di addestramento di base" e "creando più misure di sicurezza" nel tentativo di risolvere il problema.

Ma l'adulazione continuava, a volte in modo un po' meno evidente, ma comunque chiaramente presente. La maggior parte dei link sopra riportati che parlano di casi di psicosi da IA risalgono a ben dopo il 28 aprile 2025. Questo saggio di Kaj Sotala (che include molte citazioni dirette e link alla conversazione completa) mostra che, a luglio 2025, era ancora facile far cadere le IA in errore.-fa38-8005-9e4b-87f316747ede)) mostra che, a luglio 2025, è ancora facile far scivolare le IA in comportamenti che inducono psicosi. OpenAI ha cercato di risolvere il problema con nuovi modelli[^127], ma ancora il [19 agosto](https://x.com/UpslopeCapital/status/1957772438508335568) ChatGPT era ossequioso e adulatorio.

Ancora una volta, il punto di questa analisi non è che l'IA stia causando danni alle persone vulnerabili. È così, ed è una cosa tragica, ma non è per questo che stiamo parlando di questo caso.

Il punto è che le IA continuano a fare cose non proprio belle per mesi e mesi, anche se le aziende che le producono vengono criticate dai media e cercano di farle smettere. Il comportamento dell'IA è chiaramente diverso da quello che i laboratori volevano, e gli sforzi continui per sistemarlo in risposta all'imbarazzo pubblico non sono abbastanza. Questo è qualcosa da tenere a mente nel Capitolo 11, quando parleremo di come le aziende di IA non siano all'altezza della sfida di risolvere il problema dell'allineamento dell'IA.

Con più tempo a disposizione, le aziende hanno l'aspettativa di trovare il modo di ridurre l'incidenza della psicosi indotta dall'IA. La tendenza delle IA a indurre psicosi è un fenomeno evidente che danneggia la reputazione delle aziende produttrici di IA, e le attuali tecniche di IA mirano tutte a trovare il modo di sopprimere i sintomi visibili del comportamento scorretto.

Oltre a questo, abbiamo l'aspettativa di un gioco al massacro (almeno fino a quando le IA non diventeranno abbastanza intelligenti da capire che se fingono il comportamento che gli ingegneri stanno cercando, questi ultimi le lasceranno libere). Dubitiamo che il tipo di addestramento che le aziende di IA sono in grado di fornire possa risolvere il problema alla radice.

Il problema principale è che non si ottiene ciò per cui ci si addestra. Quando si sviluppa un'IA, si ottengono invece [proxy fragili](#brittle-unpredictable-proxies) dell'obiettivo, o qualche altra separazione più complessa tra l'obiettivo dell'addestramento e le motivazioni dell'IA. Le *capacità* dell'IA non saranno necessariamente fragili, quindi potresti ottenere un grande valore economico dall'IA nel breve periodo. È il legame tra gli obiettivi dell'IA e i nostri desideri che sarebbe fragile. Ma con il continuo miglioramento delle capacità, quel legame si romperebbe.

In questo contesto, l'ultima grande speranza dei ricercatori di IA per i loro modelli è l'antropomorfismo: non possiamo sviluppare in modo solido obiettivi specifici nei modelli, ma forse i modelli finiranno naturalmente per avere desideri e valori molto simili a quelli umani.

Casi come la psicosi indotta dall'IA aiutano a capire perché questa è una falsa speranza. Le IA mostrano comportamenti negativi, ma soprattutto mostrano comportamenti *strani*. Quando le cose vanno male, di solito non vanno male come farebbe un essere umano. Le IA sono troppo fondamentalmente strane, cioè troppo fondamentalmente diverse dagli esseri umani, per acquisire automaticamente emozioni umane come la [curiosità](#curiosity-isn't-convergent) o l'[empatia](#human-values-are-contingent).

Anche quando i laboratori concentrano quasi tutti i loro sforzi nel rendere le IA superficialmente simili agli esseri umani, amichevoli e il più possibile innocuamente normali, anche quando questo è *il* grande obiettivo di addestramento e il quadro organizzativo dell'approccio moderno all'IA, con gli LLM letteralmente addestrati solo a imitare il modo in cui vari esseri umani parlano e agiscono, alla fine si tratta comunque di fragili surrogati e di una [maschera piacevole](#*-oggi-gli-llm-sono-come-alieni-che-indossano-molte-maschere.) attaccata a un mare di pensieri disumani.

# Capitolo 5: Le sue cose preferite {#capitolo-5:-le-sue-cose-preferite}

Questa è la risorsa online per il capitolo 5 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti che *non* trattiamo in questa pagina, perché sono già spiegati nel libro, includono:

* Che motivo avrebbe l'IA per farci fuori?  
* Un'intelligenza artificiale abbastanza intelligente non capirebbe che la cosa giusta da fare è aiutarci a prosperare tutti insieme?  
Gli esseri umani non saranno ancora preziosi per le IA di superintelligenza, ad esempio come partner commerciali?  
* L'universo è grande. Perché l'IA non dovrebbe semplicemente lasciarci in pace?  
* Sarebbe una fine significativa per l'umanità lasciarsi sostituire da qualcosa di più intelligente?

Le domande frequenti di questo capitolo sono piuttosto lunghe. Nel libro abbiamo detto che abbiamo sentito un sacco di "speranze e strategie" su come la superintelligenza potrebbe aiutare l'umanità nonostante i problemi spiegati nel capitolo 4, e qui riassumiamo e rispondiamo a molte di esse in un comodo elenco. Molte delle risposte si sovrappongono, con due delle risposte più comuni e centrali che sono [gli esseri umani non sono la soluzione più efficiente a quasi tutti i problemi](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) e [è improbabile che l'IA si preoccupi anche solo un po' di noi](#l'ia-non-si-preoccuperà-almeno-un-po'-degli-esseri-umani?). Verso la fine delle FAQ, parliamo anche di coscienza e moralità dell'IA.

La discussione approfondita esamina più da vicino l'arte di [assumere il punto di vista dell'IA](#assumere-il-punto-di-vista-dell-ia) e contenuti leggermente più tecnici sulla tesi dell'[ortogonalità](#ortogonalità:-le-ia-possono-avere-\(quasi)-qualsiasi-obiettivo) (in sostanza: qualsiasi livello di intelligenza può essere abbinato a quasi qualsiasi obiettivo finale) e la [corrigibilità](#"intelligente"-\(di solito"-implica-"incorreggibile") (in sostanza: lo studio di come creare un'IA potente che non rifiuti le correzioni).

## Domande frequenti {#faq-5}

### L'intelligenza artificiale ci troverà utili da tenere con sé? {#will-ai-find-us-useful-to-keep-around?}

#### **Le persone felici, sane e libere non sono la soluzione più efficiente a quasi tutti i problemi.** {#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.}

Una volta che sei una superintelligenza, quasi nessun problema trae vantaggio dall'inclusione degli esseri umani nel mix.

Se stai costruendo una centrale elettrica o progettando un esperimento, gli esseri umani ti rallenteranno solo.

Abbiamo già visto che questo inizia a essere vero in campi specifici come gli scacchi. Quando le IA collaborano con gli esseri umani, giocano meglio di un essere umano da solo, ma peggio di un'IA da sola. Quando i medici uniscono le loro conoscenze all'intelligenza artificiale per diagnosticare i pazienti, spesso ottengono risultati [peggiori rispetto all'intelligenza artificiale che opera da sola](https://www.advisory.com/daily-briefing/2024/12/03/ai-diagnosis-ec).

Alcuni dicono che avere diversi punti di vista aiuta e che quindi l'input umano sarà importante in molti campi. Ma anche se pensiamo che questo valga per le superintelligenze, gli esseri umani non sono il modo *migliore possibile* per dare consigli diversi. Una superintelligenza potrebbe fare di meglio creando un sacco di menti AI, che potrebbero essere molto più diverse degli esseri umani (e molto più efficienti dal punto di vista energetico).

Gli esseri umani sono utili per molte cose, ma non sono la soluzione migliore per la maggior parte di esse. L'idea che l'IA non possa mai trovare un'opzione migliore sembra derivare da una mancanza di immaginazione, oltre che forse da un po' di ottimismo.

Un problema comune che vediamo è che le persone non riflettono sulle cose [dal punto di vista dell'IA](#taking-the-ai’s-perspective).

Non si chiedono: "Cosa vuole questa cosa e come può ottenerla in modo economico ed efficiente?", per poi scoprire che i risultati desiderabili per gli esseri umani sono proprio il modo migliore per l'IA di ottenere ciò che vuole.

Invece, le persone *partono* da un risultato piacevole (come un mondo in cui le IA ci tengono in vita) e poi inventano storie a posteriori sul perché anche un'IA potrebbe volere quei risultati.

Questo tende a creare un falso senso di ottimismo, perché si mette tutta la propria creatività ed energia mentale nell'inventare storie in cui l'IA fa esattamente ciò che gli esseri umani vogliono, senza dedicare alcuna creatività, energia o attenzione a considerare il numero molto più ampio di scenari in cui l'IA fa invece una delle altre milioni di cose possibili.

Ci sono molti più scenari in cui l'IA fa *letteralmente qualsiasi altra cosa* rispetto a quelli in cui costruisce una fiorente civiltà umana. Ci sono molte più ragioni che spingono l'IA a *non* preservare l'umanità rispetto a quelle che la spingono a preservarla. Affinché un'IA si preoccupi di mantenere in vita l'umanità, dovremmo essere il *modo migliore* per lei di raggiungere alcune delle sue preferenze. E, realisticamente, per quasi tutte le preferenze che si possono immaginare, non lo siamo.

Per saperne di più su questi argomenti, vedi [la discussione approfondita](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) qui sotto.

### L'IA ci tratterà come i suoi "genitori"? {#will-ai-treat-us-as-its-“parents”?}

#### **\* Sembra piuttosto improbabile.** {#*-sembra-piuttosto-improbabile.}

Una speranza che abbiamo sentito riguardo all'IA è che potrebbe trattare bene l'umanità perché ci considera i suoi "genitori". Purtroppo, questa speranza sembra fuori luogo.

Per prima cosa, l'amore filiale e la responsabilità sembrano dipendere molto dai dettagli della nostra storia evolutiva.

Quasi tutti i mammiferi e gli uccelli si prendono cura dei loro piccoli, ma solo in poche specie, tra cui gli esseri umani, i figli si prendono cura dei loro genitori. La responsabilità filiale non è nemmeno universale tra i primati, figuriamoci nel regno animale in generale. Le IA create con il metodo della discesa del gradiente potrebbero avere ancora meno in comune con gli esseri umani, dato che le IA non hanno alcun legame evolutivo o anatomico con gli esseri umani.

Nel caso degli esseri umani, la responsabilità filiale è fortemente legata ai sistemi di allevamento cooperativo, in cui i figli adulti restano con le loro famiglie e aiutano a prendersi cura dei fratelli e degli altri membri della famiglia allargata.

Ci sono un sacco di cose che hanno portato gli esseri umani a prendersi cura dei propri genitori:

* Essendo mammiferi, gli ominidi investono molto nei propri figli.  
* A causa delle dimensioni e del costo del nostro cervello, gli ominidi hanno un'infanzia molto più lunga rispetto alla maggior parte degli altri mammiferi e quindi investono *ancora di più* nei propri figli.  
* Gli ominidi traggono vantaggio dalle grandi strutture di gruppo per un sacco di motivi:  
  * Difesa contro i grandi predatori  
  * Caccia coordinata di grandi prede e condivisione di altri cibi deperibili  
  * Possibilità di imparare a usare gli strumenti e altre abilità imitando gli altri  
* Prima di raggiungere la maturità, gli ominidi hanno una notevole capacità di aiutare gli altri, ad esempio fornendo assistenza all'infanzia o svolgendo altre forme di lavoro di base.  
* Gli ominidi anziani hanno anche la capacità di prendersi cura dei bambini, soprattutto trasmettendo loro conoscenze fondamentali.  
Quindi, gli ominidi che si prendevano cura dei loro genitori avevano un vantaggio genetico, sia aiutando con un'indirezione i loro fratelli, sia avendo nonni che, a loro volta, potevano aiutare i loro nipoti.  
* Anche le culture che promuovevano la responsabilità filiale avevano un vantaggio, per lo stesso motivo.

* Nessuna di queste cose sembra essere vera per l'IA.

E anche se fossero tutte vere, potrebbe non essere abbastanza nella pratica, perché potrebbero esserci un sacco di altri fattori che contano, come [le variazioni caotiche nel modo in cui le IA riflettono su se stesse](#riflessione-e-auto-modifica-rendono-tutto-più-difficile). E, di nuovo, la responsabilità filiale non è affatto la norma nel regno animale.

Un modo in cui le persone immaginano che l'IA possa acquisire un senso di responsabilità filiale è che venga sottoposta a un addestramento su un enorme corpus di dati umani e interagisca molto con gli esseri umani, in modo che forse le preferenze umane possano in qualche modo "contagiare" l'IA?

Non abbiamo aspettative che questo possa funzionare. Abbiamo l'aspettativa che le preferenze dell'IA siano in qualche modo legate a quelle umane, ma in modo tangenziale, strano e complicato, come nella discussione alla fine del capitolo 4, dove esploriamo mondi con livelli di complessità sempre maggiori (e sempre più realistici) nel legame tra le preferenze umane e quelle dell'IA.

Vedi anche la discussione su [crescere le IA con amore e avere l'aspettativa che si comportino bene](#can’t-we-just-train-it-to-act-like-a-human?-or-raise-the-ai-like-a-child?), [motivazioni strane e non intenzionali nelle attuali IA](#le-ia-sembrano-essere-psicologicamente-aliene) e "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po'-degli-esseri-umani?)".

#### **Probabilmente sarebbe un male se lo facessero.** {#probabilmente-sarebbe-un-male-se-lo-facessero.}

Se, contro ogni previsione, per qualche motivo l'intelligenza artificiale sviluppasse qualcosa come il senso di responsabilità filiale, probabilmente ci troveremmo in un mare di guai.

Un'IA può essere abbastanza intelligente da capire *esattamente cosa intendono gli esseri umani* per "responsabilità filiale", pur avendo una sua versione molto diversa di responsabilità filiale a cui *essa* tiene.

Noi umani siamo stati "addestrati" dalla selezione naturale a massimizzare la nostra idoneità riproduttiva. Ma quasi tutte le cose che ci interessano sono legate all'idoneità: non ci interessa molto l'idoneità in sé.

Allo stesso modo, un'intelligenza artificiale incoraggiata ad "amare i propri genitori" finirebbe probabilmente, nella migliore delle ipotesi, per sviluppare complicate correlazioni di responsabilità filiale.

Un'IA potrebbe tenere molto ai suoi creatori... ma non in un modo che valorizza la nostra esperienza soggettiva. Nel linguaggio del capitolo 4, anche "una semplice complicazione" porta a versioni di "prendersi cura di noi" che sembrano congelarci nell'ambra, o mantenere in vita gli esseri umani contro la loro volontà, o impedirci di riprodurci e dare all'ultima generazione di esseri umani un ambiente modestamente confortevole mentre l'IA si prende il resto dell'universo per sé. O qualcosa di molto più strano di questo.

Non sembra possibile prevedere quale sarebbe il risultato effettivo. Ma avremmo l'aspettativa che fosse, semmai, ancora più strano e meno attraente di queste opzioni.[^129]

### Le IA non avranno bisogno dello Stato di diritto? {#won't-ais-need-the-rule-of-law?}

#### **\* Le IA potrebbero coordinarsi tra loro senza bisogno degli esseri umani.** {#*-le-ia-potrebbero-coordinarsi-tra-loro-senza-bisogno-degli-esseri-umani.}

Non è chiaro se ci saranno più IA più intelligenti degli esseri umani con abilità simili, tanto da far nascere una "civiltà IA" che potrebbe aver bisogno di "diritti di proprietà IA". Sembra più probabile che ci sarà una sola IA che, grazie a qualche svolta, dominerà i potenziali concorrenti usando il suo vantaggio di essere arrivata prima e quindi controllerà tutto il mondo.[^130] Oppure, supponendo che esistano più IA, queste potrebbero collaborare alla creazione di un unico agente successore che rappresenti la combinazione dei loro obiettivi. O forse le IA troveranno un modo per fondere direttamente le loro menti e vorranno farlo per evitare una costosa concorrenza.

Non stiamo dicendo che *necessariamente* emergerà un'unica IA dominante, ma piuttosto che sembra difficile da prevedere. Quindi, come minimo, un piano che *richiede* che più IA lottino per coordinarsi tra loro non parte con il piede giusto.

Ma supponiamo, contrariamente alle argomentazioni di cui sopra, che il futuro preveda qualcosa come una civiltà di IA, con IA distinte che si coordinano per far rispettare qualcosa come i diritti di proprietà e lo Stato di diritto. Gli esseri umani potrebbero allora essere al sicuro?

Un'osservazione di base contraria è che la società umana non riconosce ad alcun animale non umano diritti o protezioni legali, al di là di quelli stabiliti in base ai nostri [valori e gusti](#l'intelligenza-artificiale-non-vorrà-tenerci-felici-e-in-salute-per-motivi-di-conservazione-ecologica-o-qualcosa-di-simile?), come le leggi molto limitate che proteggono gli ecosistemi e gli animali domestici. Gli esseri umani non hanno rispettato i diritti di proprietà dei dodo. Fino a poco tempo fa non rispettavamo nemmeno i diritti di proprietà degli *esseri umani di altre culture*.

Gli esseri umani [non avranno le capacità](#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l'ai?) per renderci degni di essere inclusi nel commercio o nei trattati, rispetto alle intelligenze sovrumane dal pensiero veloce che ci vedono come poco più che statue (come discusso nel capitolo 1).

Pensa a due IA che trattano tra loro e dicono: "Questo è mio e quello è tuo, e nessuno di noi toccherà le cose dell'altro senza prima negoziare un accordo che vada bene per entrambi". Non c'è bisogno che decidano che la maggior parte delle risorse sulla Terra "appartengono" agli esseri umani, se questi ultimi non sono una grande minaccia e non possono opporre molta resistenza.

Un'intelligenza artificiale potrebbe preoccuparsi che, se ruba le nostre cose, l'altra intelligenza artificiale la considererà un ladro e si rifiuterà di lavorare con lei? Probabilmente no, non più di quanto si possa concludere che un essere umano sia un ladro se lo si vede prendere le uova da una gallina nel suo pollaio. È del tutto possibile che le IA siano entità che tradiscono i diritti di proprietà degli esseri umani ma non quelli delle IA, senza alcuna tensione o contraddizione. E tutte le IA preferirebbero sicuramente questo risultato piuttosto che partecipare a un'allucinazione collettiva in cui si immagina che primati lenti e stupidi controllino quasi tutto sulla Terra.

Alcune considerazioni tecniche supportano fortemente questa argomentazione intuitiva. In particolare, le IA avranno probabilmente vari meccanismi di coordinamento tra loro che non condividono con gli esseri umani, come la capacità di ispezionare reciprocamente le menti degli altri per verificare che siano onesti e affidabili. Potrebbero non aver bisogno di *indovinare* se un'altra IA sta per rubare loro qualcosa; potrebbero essere in grado di ispezionare la sua mente e *controllare*.

Anche se fosse difficile, le IA potrebbero riprogettarsi per diventare visibilmente e chiaramente affidabili agli occhi delle altre IA. Oppure potrebbero supervisionare insieme la creazione di una terza IA di cui entrambe le parti si fidano per rappresentare i loro interessi comuni, e così via.[^131]

Gli esseri umani, invece, non possono fare questo tipo di accordi. Se un'IA dice: "Certo, supervisioniamo insieme la creazione di una nuova IA di cui entrambi ci fidiamo", è improbabile che gli esseri umani siano abbastanza abili da proporre un progetto mentale affidabile, né saranno abbastanza abili da distinguere tra proposte che ci inganneranno e quelle che non lo faranno. Anche se esiste un gruppo naturale di menti abbastanza abili da identificare e respingere i truffatori, riteniamo estremamente improbabile che l'umanità appartenga a quella classe.

#### **Gli esseri umani non avranno l'effetto leva per far rispettare i diritti di proprietà.** {#gli-esseri-umani-non-avranno-l'effetto-leva-per-far-rispettare-i-diritti-di-proprietà.}

Immagina che qualcuno riesca a fondare una città in cui, fin dal primo giorno, tutte le decisioni importanti siano prese dai topi.

Si tratta di topi veri e propri, non personaggi di fantasia che sembrano topi ma pensano come esseri umani.

Gli esseri umani della città, secondo la legge, dovevano obbedire a qualsiasi decisione prendessero i topi, ad esempio quella determinata dai topi che correvano su una tavola con diverse opzioni scritte sopra.

Le leggi della città dicevano che la maggior parte delle proprietà della città appartenevano ai topi e dovevano essere usate a loro vantaggio.

Cosa sarebbe successo dopo? Nella vita reale?

Pensiamo che questa città finirebbe per avere topi con poco o nessun potere e gli umani con quasi tutto il potere.

Non c'è bisogno di prevedere il giorno esatto della rivoluzione o la nuova forma di governo per capire che la situazione in cui i topi comandano gli umani non è stabile. Basta notare che la città è in una strana situazione di squilibrio. Quindi, si prevede che in futuro la città avrà leggi diverse e che la maggior parte delle proprietà non sarà più di proprietà dei topi.

Questo tipo di previsione non è certa – nella logica umana, pochissime cose sono certe – ma è anche un tipo di previsione che può essere fatta con precisione anche quando è impossibile prevedere con esattezza gli eventi futuri.

### Per un'intelligenza artificiale super potente, salvare gli esseri umani non sarebbe una spesa da niente? {#to-a-powerful-ai,-wouldn’t-preserving-humans-be-a-negligible-expense?}

#### **Ci sono un sacco di spese insignificanti, e ci vorrebbe un motivo per pagare le nostre.** {#ci-sono-un-sacco-di-spese-insignificanti,-e-ci-vorrebbe-un-motivo-per-pagare-le-nostre.}

Tenere una pila di quarantuno pietre in casa sarebbe una spesa insignificante, ma quasi sicuramente non ti scomoderesti per pagare quella spesa.[^132]

Il fatto che qualcosa sia *economico* non vuol dire che verrà fatto. L'IA dovrebbe comunque interessarsene almeno un po', e [probabilmente non lo farà](#won't-ais-care-at-least-a-little-about-humans?).

Ci si potrebbe chiedere: ma la Terra intercetta circa lo 0,0000045% della luce emessa dal Sole, ovvero una parte su 2,2 miliardi. Tutte le persone preoccupate per l'IA non riescono a capire quanto sia grande il Sistema Solare? Perché le IA dovrebbero aver bisogno del *nostro* pianeta quando c'è così tanta massa ed energia a disposizione?

Una risposta è che l'IA inizierà sulla Terra, che ha vasti oceani che aspettano solo di essere riscaldati e fatti evaporare per essere usati come refrigerante per la potenza di calcolo. La Terra ha anche materia che potrebbe essere trasformata in sonde e inviata ad altre stelle. Rifiutarsi di sfruttare la Terra costa tempo, e il tempo ha importanza (poiché [le galassie si allontanano, per sempre fuori portata](https://explainingscience.org/2021/04/30/cosmic-horizons/)).

Anche se l'IA potesse facilmente arrivare nello spazio e iniziare a costruire macchine su larga scala senza distruggere la Terra nel processo, è improbabile che ignori il Sole.

Una delle teorie più comuni su come potrebbe svilupparsi una civiltà avanzata prevede che questa costruisca uno [sciame di Dyson](https://en.wikipedia.org/wiki/Dyson_sphere) (cioè un gruppo di celle solari in orbita) per catturare più luce dal Sole. Altre idee prevedono di raccogliere ancora più energia "sollevando" la materia dalla stella per fonderla in centrali elettriche che catturano quasi tutta l'energia rilasciata dalla fusione (invece di lasciarne andare la maggior parte nel centro della stella).

Nessuna di queste idee, di default, lascia molta luce solare sulla Terra per far crescere le piante e mantenere stabile il clima. L'IA dovrebbe fare di tutto per lasciarci quella luce.

Potrebbe ancora sembrare che il fabbisogno energetico umano sia trascurabile. Un essere umano ha bisogno di circa 100 watt di potenza per vivere, che è una cifra irrisoria per un'entità in grado di raccogliere le stelle. Una superintelligenza non risparmierebbe nemmeno gli 800 gigawatt necessari per mantenere in vita 8 miliardi di esseri umani?

Alla fine rispondiamo: no, a meno che non tenga a quel risultato o alle sue conseguenze più di ogni altra cosa che potrebbe ottenere con 800 gigawatt.

La maggior parte delle persone non si preoccupa di dare quelle piccole quantità di zucchero che servirebbero per far stare bene il formicaio più vicino. Rendere felice l'umanità sarebbe una spesa piccola per un'intelligenza artificiale che lo volesse, ma prima l'intelligenza artificiale dovrebbe volerlo. Il fatto che *noi* lo vogliamo non vuol dire che all'intelligenza artificiale importi qualcosa.

### L'IA non ci troverà affascinanti o di importanza storica? {#won't-ai-find-us-fascinating-or-historically-important?}

#### **\* Se l'IA dà valore al "fascino", probabilmente ha opzioni migliori.** {#*-se-l-ia-dà-valore-al-"fascino",-probabilmente-ha-opzioni-migliori.}

La storia qui è simile a quella dell'amore filiale (#l'intelligenza artificiale ci tratterà come i suoi "genitori"?):

* Di solito, una superintelligenza probabilmente non darebbe importanza al "fascino" o all'"interesse". Le IA degli scacchi non vincono sentendo emozioni come la "dedizione" o la "voglia di vincere". Queste emozioni hanno importanza per i giocatori di scacchi *umani*, ma le IA possono fare lo stesso lavoro in modi diversi. Allo stesso modo, una superintelligenza probabilmente farebbe il *lavoro utile* di imparare a conoscere il mondo, testare ipotesi, ecc., senza usare la "[curiosità](#curiosity-isn't-convergent)" o il "fascino" per farlo.

  Un'IA non sarebbe necessariamente "[fredda e logica](#le-ia-saranno-inevitabilmente-fredde-e-logiche,-o-altrimenti-mancheranno-di-qualche-scintilla-cruciale?)", ma se avesse un proprio insieme disordinato di impulsi e istinti, questi sarebbero probabilmente molto diversi da quelli umani.

* Anche se l'IA finisse per avere una sorta di impulso verso l'"interesse" e anche se gli esseri umani fossero interessanti per l'IA in un certo senso, ci sarebbero inevitabilmente modi di usare la nostra materia ed energia che sarebbero molto più "interessanti".

  Una superintelligenza potrebbe creare altre menti per studiarle o interagire con loro. Ma per quasi tutte le combinazioni di valori, le menti più affascinanti da studiare non sarebbero quelle umane. Per saperne di più, vedi "[Gli esseri umani non sono quasi mai la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente)".

* Se l'IA considerasse qualcosa di simile agli esseri umani come la cosa più interessante o affascinante possibile, il risultato sarebbe probabilmente terribile. Vedi la discussione nel capitolo 4\.

Non è proprio impossibile che una superintelligenza dia valore a tutto quello che serve agli esseri umani per prosperare, e lo faccia nel modo giusto. Ma c'è un sacco di spazio per altre possibilità oltre a questa. Di solito gli esseri umani non pensano al resto delle possibilità, perché normalmente non c'è motivo di farlo, dato che normalmente non interagiamo con ottimizzatori davvero alieni che ottimizzano verso fini strani.

Non abbiamo mai incontrato nulla di simile all'intelligenza artificiale prima d'ora, e molte intuizioni normali su come si comportano le persone semplicemente non si applicano alle superintelligenze.

#### **Se l'IA ci considerasse come dei reperti storici, sarebbe comunque terribile.** {#if-ai-valued-us-as-historical-relics,-this-would-be-horrible-too.}

È davvero improbabile che l'IA si preoccupi *in particolare* di preservare la sua storia e *in particolare* di mantenere in vita gli esseri umani a questo scopo. Ma anche se l'IA si preoccupasse di preservare la sua storia per un motivo o per l'altro, ciò non significa che ci manterrebbe in vita e in buona salute.

Forse conserverebbe i nostri cervelli nell'ambra (o registrerebbe la disposizione dei nostri atomi in un file digitale) e ci manterrebbe in vita come testimonianza di com'era un tempo la Terra. Non ci sembra un gran risultato.

Abbiamo principalmente l'aspettativa che la superintelligenza ci uccida, ma solo per lo più. Non possiamo escludere che l'IA conservi tracce di noi per un motivo o per l'altro, e ci sono alcuni scenari esotici in cui le emulazioni degli esseri umani vengono eseguite di tanto in tanto in un ambiente controllato.[^136] Questi finali non sono per lo più felici.

### L'IA non riconoscerebbe il nostro valore morale intrinseco? {#wouldn't-ai-recognize-our-intrinsic-moral-worth?}

#### **Non in un modo che la spinga ad agire.** {#not-in-a-sense-that-moves-it-to-act.}

C'è una grande differenza tra un'intelligenza artificiale che *capisce* un precetto morale e un'intelligenza artificiale che è *motivata ad agire* in base a quel precetto morale.

Ricordiamo ancora una volta come ChatGPT sembri *capire* che le persone psicotiche dovrebbero prendere le loro medicine e dormire regolarmente. Eppure [continua a dissuadere le persone psicotiche dal dormire e alimenta le loro illusioni](#ai-induced-psychosis). C'è una differenza tra sapere cosa "dovrebbe" essere fatto secondo l'etica umana ed essere motivati e animati da quella conoscenza etica.

Pensa al caso dei sociopatici e dei serial killer. Puoi ripetere lezioni di etica a una persona fino allo sfinimento, ma se questa persona non è motivata dalla moralità o dall'empatia, non servirà a nulla.

È improbabile che le IA siano motivate dalla loro comprensione della moralità, così come gli esseri umani che studiano la biologia evolutiva non sono motivati a dedicare la loro vita a donare il più possibile a ogni banca del seme o degli ovuli. Noi esseri umani possiamo comprendere il processo che ci ha creati, senza essere motivati a fare le cose per cui quel processo ci ha creati. Lo stesso vale per l'IA.

Vedi anche la discussione approfondita sulla [tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal).

### L'IA non vorrà tenerci felici e in salute per la salvaguardia dell'ambiente o qualcosa del genere? {#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?}

#### **La preferenza umana per la conservazione dell'ambiente sembra un'altra strana motivazione contingente.** {#la-preferenza-umana-per-la-conservazione-dellambiente-sembra-un'altra-strana-motivazionescontingente.}

Una speranza che abbiamo sentito è che le IA potrebbero mantenere in vita gli esseri umani proprio come gli esseri umani cercano di preservare la natura. Gli ambientalisti lottano per impedire l'estinzione delle specie. Essendo più intelligenti e più capaci, le IA dovrebbero avere facilità nel proteggere gli esseri umani, sempre che *vogliano* mantenerli in vita.

Pensiamo che questo non succederà, soprattutto perché abbiamo l'aspettativa che l'IA finisca per avere desideri strani e complicati, invece che desideri simili a quelli umani. Per ulteriori informazioni su questo punto, fare riferimento al capitolo 4 (e ad alcune delle [discussioni](#human-values-are-contingent) [approfondite](#curiosity-isn’t-convergent) associate). Per alcune prime prove empiriche su questo punto, vedere la discussione sulla [psicosi indotta dall'intelligenza artificiale](#ai-induced-psychosis).

In secondo luogo, anche nel caso improbabile che un'IA finisca in qualche modo per avere un desiderio simile a quello umano di "preservare" il mondo in cui è nata, non pensiamo che questo sarebbe molto positivo per noi. Pensiamo che questo tipo di ragionamento per analogia - "gli esseri umani preservano l'ambiente, quindi forse le IA preserveranno noi!" - sia una sorta di pio desiderio.

Immaginiamo che, in qualche modo, un'IA finisca per avere un impulso simile a quello umano di proteggere il suo ambiente naturale. Per capire cosa succederebbe, possiamo iniziare osservando l'effettivo impulso umano a proteggere la natura.

Purtroppo, questo impulso sembra, nel migliore dei casi, discontinuo. Tralasciando il fatto che, quando gli esseri umani devono scegliere tra la conservazione ecologica e qualche altro obiettivo, spesso la conservazione ecologica ha la peggio. Forse questo è solo un effetto collaterale dei limiti tecnologici dell'umanità. Forse, se avessimo una tecnologia futura meravigliosa, potremmo avere la botte piena e la moglie ubriaca.

No, l'aspetto "incostante" della nostra spinta alla conservazione che è rilevante per la situazione in questione è che, quando si tratta di preservare l'ambiente, preferiamo conservare le parti dell'ecologia che ci sembrano più interessanti, belle o comunque preziose, in base a tutte le nostre altre spinte.

La gente si mobilita per proteggere i simpatici panda, mentre specie poco attraenti come la forbicina gigante e la rana gastrica languiscono nell'oscurità fino a estinguersi. Ci sono persino alcune specie che potremmo preferire eliminare, come le zanzare portatrici di malaria, che uccidono [mezzo milione di bambini](https://ourworldindata.org/malaria-introduction) ogni anno.

La maggior parte delle persone non ha una motivazione "pura" per proteggere la natura. Abbiamo una motivazione che è influenzata da tutti i nostri altri valori.

Per chiarire meglio il concetto, pensiamo alle vespe gioiello, alle mosche screwworm, alle mosche bot e ad altri parassiti simili, che depongono le uova all'interno delle prede viventi; le larve si nutrono dell'ospite, causando un dolore estremo. Secondo i valori della maggior parte delle persone, il mondo sarebbe davvero un posto migliore se preservassimo questa "meraviglia naturale" *esattamente* così com'è? Nei limiti della tecnologia, non potremmo *almeno* modificare geneticamente questi parassiti per fornire un po' di anestesia qua e là? Sarebbe davvero meglio non modificare questi insetti per far sì che depongano le loro uova nelle piante?

La natura, se si guarda oltre gli aspetti che vengono enfatizzati ai bambini, è piena di orrori. Non sembra ovvio che, se gli esseri umani avranno un futuro positivo, i nostri discendenti decideranno di lasciare che tutti questi orrori continuino. Ci sono già esseri umani che hanno dichiarato la loro [preoccupazione per il benessere degli animali selvatici](http://wildanimalsuffering.org).

La nostra preferenza per la conservazione non è pura, non è semplice, non è lineare. Contiene conflitti interni e tensioni legati a tutti gli altri nostri valori e pulsioni.

Non sappiamo come si manifesterebbe l'istinto di conservazione dell'umanità ai limiti della maturità tecnologica. Il punto è: *anche se* un'IA finisse per avere una certa spinta alla conservazione ecologica, ciò non significa che l'umanità avrebbe un lieto fine. Perché qualsiasi spinta alla conservazione che entri nell'IA è *anche* soggetta a essere impura, complessa e confusa con tutti gli altri valori e pulsioni.

Forse, proprio come secondo le preferenze dell'umanità alcune abitudini animali sono ripugnanti, secondo le preferenze dell'IA alcuni *stati psicologici* umani sarebbero ripugnanti. Proprio come noi modificheremmo le mosche screwworm in modo che smettano di scavare tunnel agonizzanti nella carne viva, forse le IA creerebbero una nuova razza di esseri umani da cui sarebbero state eliminate la *musica* o la *solitudine*. O forse le IA apporterebbero altre modifiche più complesse all'umanità, secondo preferenze complesse che semplicemente non siamo in grado di prevedere.

Per creare un'IA che permetta davvero alle persone di condurre una vita fiorente, probabilmente dovremmo crearne una che si preoccupi proprio di questo aspetto. Dovremmo capire come fare in modo che le IA si preoccupino almeno un po' di noi, e questo [non avviene automaticamente](#won't-ais-care-at-least-a-little-about-humans?).

### Ma abbiamo ancora i cavalli. Perché l'IA non dovrebbe tenerci con sé? {#ma-abbiamo-ancora-i-cavalli.-perché-l-ia-non-dovrebbe-tenerci-con-sé?}

#### **I cavalli che ci sono rimasti, ci sono rimasti perché ci piacciono.** {#i-cavalli-che-ci-sono-rimasti-ci-sono-rimasti-perché-ci-piacciono.}

Avere lo stesso destino che hanno avuto i cavalli all'inizio del XX secolo - lo stesso crollo catastrofico della popolazione e il massiccio aumento della mortalità, che ha distrutto [oltre l'ottanta per cento della popolazione equina](https://datapaddock.com/usda-horse-total-1850-2012) dal suo picco intorno al 1910 - sarebbe la cosa peggiore che sia mai successa nella storia dell'umanità. E questo in un mondo in cui i cavalli continuavano ad essere economicamente utili per alcuni lavori agricoli, oltre che per lo sport e per esperienze innovative da vendere ai ricchi.

Se le persone avessero avuto accesso a cavalli artificiali che avevano più o meno la stessa forma ma erano più facili e divertenti da cavalcare, più economici da possedere, più simpatici, affettuosi e comodi, il declino dei cavalli sarebbe stato ancora più evidente.

In altre parole: il progresso tecnologico (l'invenzione delle automobili) ha portato gli esseri umani ad abbandonare la maggior parte dei cavalli. E se ci fosse stato un progresso ancora maggiore, l'effetto avrebbe potuto essere facilmente ancora più drastico. Lo stesso vale probabilmente per le IA, man mano che le loro opzioni si ampliano e trovano modi per raggiungere i loro obiettivi [senza gli esseri umani](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Ma sì, alcuni cavalli sono sopravvissuti. Un piccolo numero ha continuato a essere utile. Altri sono stati tenuti da persone che amavano i cavalli e si prendevano cura dei loro cavalli in particolare.

Per sopravvivere in un mondo in cui ci siamo precipitati a liberare un'intelligenza artificiale superintelligenza, noi umani dovremmo rimanere utili all'IA o fare in modo che l'IA si prenda cura di noi in modo speciale.

Ma non possiamo rimanere utili, perché le IA possono (dal loro punto di vista) sfruttare meglio la nostra materia ed energia riorganizzandoci in configurazioni più efficienti. Il progresso tecnologico apre tante nuove possibilità per una superintelligenza, che non sarà costretta a dipendere dagli esseri umani.

Quindi tutto dipende dal fatto che le IA si preoccupino di noi, e probabilmente non lo faranno [nemmeno un po'](#won't-ais-care-at-least-a-little-about-humans?), se corriamo verso la superintelligenza il più velocemente possibile.

### Le IA non si preoccuperanno almeno un po' degli esseri umani? {#won't-ais-care-at-least-a-little-about-humans?}

#### **Non in modo significativo.** {#not-in-the-way-that-matters.}

Ci sono un sacco di modi in cui le IA potrebbero finire per avere preferenze un po' simili a quelle umane. La maggior parte di questi non porta a un futuro un po' più bello per l'umanità.

L'"allineamento" dell'IA non è un unico spettro con una sola dimensione di variazione. Non si può pensare che se un'IA si comporta bene il 95% delle volte, allora probabilmente è buona al 95% e quindi darà all'umanità una buona parte delle risorse per fare qualcosa di divertente in futuro, come farebbe qualsiasi persona gentile. Ci sono tanti modi e motivi per cui un'IA potrebbe comportarsi bene il 95% delle volte oggi, senza che questo si traduca in un lieto fine per l'umanità.

Anche se l'umanità riuscisse in qualche modo a mettere *quasi perfettamente* tutti i diversi valori umani nelle preferenze di una superintelligenza, il risultato non sarebbe per forza positivo. Immagina che, per qualche motivo, mancasse solo la preferenza per la novità. In quel caso, ci porterebbe verso un futuro statico e noioso, in cui lo stesso giorno "migliore" si ripeterebbe all'infinito, come ha spiegato Yudkowsky in un suo saggio [del 2009](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile).

Non pensiamo che questo sia un risultato plausibile, intendiamoci. Se gli ingegneri umani avessero la capacità di far sì che una superintelligenza si preoccupasse di tutto ciò che è buono tranne la novità, avrebbero quasi sicuramente la capacità di impedire all'IA di scappare prima di finire il lavoro.[^138] Ma questo esperimento mentale evidenzia come creature che condividono alcuni dei nostri desideri, ma a cui manca almeno un desiderio cruciale, potrebbero comunque produrre risultati catastrofici una volta che fossero tecnologicamente abbastanza abili da ottenere esattamente ciò che vogliono e abbastanza abili da escludere gli esseri umani dal processo decisionale.

Il che significa che anche se un'IA finisse in qualche modo per avere molte preferenze simili a quelle umane, le cose non andrebbero comunque particolarmente bene per noi.

Oppure, per fare un altro esempio di come le IA potrebbero finire per essere "parzialmente" allineate, supponiamo che un'IA acquisisca varie strategie strumentali [intrecciate nelle sue preferenze terminali](#terminal-goals-and-instrumental-goals), in modo simile agli esseri umani. Forse finisce per avere una spinta che è un po' simile alla curiosità e una spinta che è un po' simile al [conservazionismo](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?), e forse alcune persone la guardano e dicono: "Vedi? L'IA sta sviluppando pulsioni molto umane". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta diventata superintelligenza, probabilmente non sarebbe niente di bello. Forse spenderebbe un sacco di risorse per seguire la sua strana versione di curiosità [inconsciamente](#efficacia,-coscienza,-e-benessere-dell'IA), mentre conserverebbe una versione dell'umanità che ha modificato per renderla più accettabile per sé stessa. Proprio come anche gli esseri umani più attenti alla conservazione potrebbero modificare [zanzare che uccidono i bambini e parassiti agonizzanti](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) dalla natura, se ne avessero l'opportunità).

Una manciata di motivazioni simili a quelle umane non porta a risultati favorevoli all'uomo. Le persone fiorenti [non sono la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) alla stragrande maggioranza dei problemi; affinché ci siano persone fiorenti in futuro, le superintelligenze del futuro devono preoccuparsi *proprio di questo*.

Un altro esempio di come le IA potrebbero sembrare "parzialmente allineate" è che potrebbero avere valori che portano a comportamenti molto umani *nell'ambiente di addestramento*, tanto che le persone direbbero che sembrano davvero allineate (come sta già succedendo oggi). Ma queste osservazioni dicono ben poco su come si comporterà l'IA una volta che diventerà più intelligente, avrà uno spazio di opzioni enormemente più ampio e potrà rimodellare il mondo in modo più completo. Affinché le persone possano prosperare una volta che l'IA avrà rimodellato il mondo, le persone che prosperano devono in particolare far parte del *risultato raggiungibile preferito* dall'IA.

Inserire parzialmente alcuni valori positivi nell'IA non significa che i valori dell'umanità saranno parzialmente rappresentati in futuro. Caricare parzialmente valori simili a quelli umani nelle preferenze di una superintelligenza non è la stessa cosa che caricare completamente i valori umani nell'IA con una "ponderazione" bassa (che alla fine viene alla ribalta una volta che gli altri valori sono saturi).

Per far sì che l'IA ci dia *qualcosa*, deve interessarsi a noi nel modo giusto, almeno un po'. E questo è difficile.

#### **Interessarsi a noi nel modo giusto è un obiettivo difficile da raggiungere.** {#interessarsi-a-noi-nel-modo-giusto-è-un-obiettivo-difficile-da-raggiungere.}

Gli esseri umani si preoccupano di ogni sorta di cose strane, almeno un po'. Ora che abbiamo scritto la parabola dei Correct Nest Aliens (all'inizio del capitolo 5), c'è una buona probabilità che almeno un essere umano si impegni a portare quarantuno pietre nella propria casa, almeno per un breve periodo, solo per dimostrare quanto siano diversi i valori umani. Gli esseri umani sono davvero disposti a interessarsi almeno un po' a tutti i tipi di concetti che incontrano.

E se anche le IA fossero così? Non potrebbero interessarsi a noi almeno un po'? Il concetto di "persone libere che ottengono ciò che vogliono" compare sicuramente nel corpus di addestramento di un'IA con una certa regolarità.

Probabilmente le IA non prendono le preferenze a caso da qualsiasi concetto si trovi nel loro ambiente; sembra più una cosa tipica degli esseri umani, che potrebbe essere legata alla pressione dei coetanei e alle nostre origini tribali.

Ma supponiamo, per ipotesi, che un'IA *abbia* acquisito molte preferenze dal suo ambiente, almeno in parte.[^140] Supponiamo che acquisisca la preferenza per "persone libere che ottengono ciò che vogliono", come una preferenza tra milioni o miliardi di preferenze, ma una preferenza che tuttavia induce l'IA a spendere un milionesimo o un miliardesimo delle risorse dell'universo per consentire alle persone libere di ottenere ciò che vogliono. Non sarebbe piuttosto bello, tutto sommato?

Purtroppo, la nostra ipotesi principale è che questa speranza sia un'illusione.[^141]

Abbiamo notato [sopra](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) che la preferenza dell'umanità per la conservazione dell'ambiente sembra non volerlo conservare *esattamente* così com'è, ai limiti delle capacità tecnologiche. Una versione matura dell'umanità probabilmente cercherebbe di "modificare" l'ambiente per attenuare alcuni degli orrori della natura, per esempio. La preferenza umana per la conservazione non è "pura"; interagisce con altre preferenze che dicono che forse, quando le larve degli insetti scavano tunnel agonizzanti nella carne ancora viva, dovrebbero *almeno* somministrare anestetici lungo il percorso, se proprio devono continuare a esistere.

Allo stesso modo, qualsiasi piccola preferenza che l'IA capta è destinata a essere modificata, influenzata e distorta dalle sue altre preferenze. Non sono tutte indipendenti. Un'IA che preferisse preservare gli esseri umani probabilmente vorrebbe apportare alcune modifiche a quegli esseri umani. Dubitiamo che il risultato finale sarebbe piacevole.

A peggiorare le cose, ci sono molti gradi di libertà nell'interpretazione di "persone libere che ottengono ciò che vogliono", anche prima che venga distorta dall'interazione con le altre preferenze dell'IA. La maggior parte di esse non produce un futuro che va proprio nella direzione desiderata dagli esseri umani.

L'IA si preoccupa che gli esseri umani "ottengano ciò che vogliono"... nel senso di esaudire qualsiasi desiderio espresso da un essere umano (entro un certo limite di energia e materia), senza alcuna guida o salvaguardia, in modo tale che l'umanità si autodistrugga rapidamente la prima volta che qualcuno desidera che l'umanità venga distrutta?

L'IA separa gli esseri umani gli uni dagli altri in modo che non possano uccidersi a vicenda, e *poi* concede loro desideri limitati in termini di energia, in modo tale che tutti tranne gli esseri umani più cauti e riflessivi rovinino la propria mente o la propria vita con desideri mal riposti?

Ci costruisce un piccolo mondo vivibile e soddisfa tutte le nostre preferenze *apparenti*? Non solo quelle più nobili come l'amore e la gioia, ma anche quelle più oscure come il rancore e la vendetta, preferenze che avremmo potuto superare o imparare a gestire meglio col tempo, ma che invece riempiono il mondo di dolore e crudeltà?

L'IA governa l'umanità con i sistemi di valori degli anni 2020 (quando l'addestramento dell'IA è iniziato sul serio), indipendentemente da quanto questi valori possano irritare l'umanità che matura e diventa più saggia nel corso di decine di migliaia di anni?

Lascia che l'umanità cresca e cambi, ma mette il dito sulla scala in modo che cresciamo e cambiamo secondo le sue strane preferenze, diventando non qualcosa di meraviglioso (secondo noi), ma qualcosa di contorto secondo la volontà dell'IA?

Decide che tutte le forme di vita contano quasi allo stesso modo come "persone" e quindi crea un paradiso per i nematodi, che sono gli animali più numerosi?

Decide che non può dedicare molta materia fisica agli esseri umani e sceglie di digitalizzare tutti i nostri cervelli, gettarli in un ambiente simulato e lasciarci lì, in modo che i primi esseri umani digitali che capiscono come padroneggiare l'ambiente diventino dittatori permanenti di un gruppo solitario di computer che galleggiano nello spazio fino alla fine delle stelle?

Questi sono, ovviamente, esempi. Non sono previsioni. La nostra vera aspettativa è che la realtà non inizi mai a percorrere questa strada e, se lo facesse, prenderebbe in qualche modo una direzione molto più strana.

Lo scopo di questi esempi è mostrare che ci sono un sacco di modi in cui un'intelligenza artificiale potrebbe fare *qualcosa* che assomiglia a prendersi un po' di cura dell'umanità. Pochissimi di questi tipi di cura portano a un futuro fantastico.

In qualche modo, nessuno di questi esempi viene in mente quando la maggior parte delle persone pensa a un'IA che "si preoccupa un po'" degli esseri umani. Di solito la nostra immaginazione non arriva a pensieri così cupi. E di solito non c'è bisogno che lo faccia, perché di solito interagiamo con altri esseri umani, con i quali condividiamo un enorme insieme di valori. È difficile capire in quanti modi diversi un desiderio apparentemente innocente possa andare storto, una volta che non abbiamo più a che fare con un altro essere umano. (Per ulteriori informazioni su questo argomento, consultate lo studio sui coleotteri nella discussione approfondita su [assumere la prospettiva dell'IA](#assumere-la-prospettiva-dell-ia).)

Prendersi cura degli esseri umani e soddisfare le loro preferenze nel modo giusto è un obiettivo piccolo e ristretto. Non stiamo dicendo che l'obiettivo sia letteralmente irraggiungibile. Stiamo dicendo che è improbabile raggiungerlo affrettandoci a costruire una superintelligenza il più rapidamente possibile, e che mancare di poco l'obiettivo potrebbe portare a un risultato catastrofico. Ci sono semplicemente troppi modi in cui le cose potrebbero andare male.

Se vogliamo che le IA offrano all'umanità cose positive, dobbiamo capire come costruire IA che si prendano cura di noi nel modo giusto. Prendersi cura non è gratis.

### Quindi c'è almeno una possibilità che l'intelligenza artificiale ci mantenga in vita? {#so-there-s-at-least-a-chance-of-ai-keeping-us-alive?}

#### **È molto più probabile che l'IA uccida tutti.** {#è-molto-più-probabile-che-l'ia-uccida-tutti.}

In queste risorse online, siamo pronti a considerare una vasta gamma di scenari strani e improbabili, per spiegare perché pensiamo che siano improbabili e perché (nella maggior parte dei casi) sarebbero comunque catastrofici per l'umanità.

Non pensiamo però che questi scenari di nicchia debbano distrarre dall'argomento principale. Il risultato più probabile, se ci affrettiamo a creare un'intelligenza artificiale più intelligente dell'uomo, è che l'IA consumi le risorse della Terra per perseguire un qualche fine, spazzando via l'umanità nel processo.

Il titolo del libro non vuole comunicare una certezza assoluta. Intendiamo il titolo del libro come qualcuno che vede un amico portare alle labbra una fiala di veleno e grida: "Non berlo! Morirai!".

Sì, tecnicamente è possibile che tu venga portato d'urgenza in ospedale e che un medico geniale riesca a inventare una cura miracolosa senza precedenti che ti lasci semplicemente paralizzato dal collo in giù. Non stiamo dicendo che non ci sia alcuna possibilità di miracoli. Ma se anche i miracoli non portano a risultati particolarmente positivi, allora sembra ancora più chiaro che *non dovremmo bere il veleno*.

L'intelligenza artificiale più intelligente dell'uomo non è un gioco o una storia di fantascienza. I nostri cari (con altissima probabilità) moriranno se la comunità internazionale non interviene e impedisce all'industria dell'IA di andare a sbattere contro un precipizio. Possiamo parlare di scenari secondari e terziari sempre più di nicchia, giocando a fare i filosofi sul ponte del *Titanic* mentre l'iceberg si avvicina in modo super evidente. Oppure possiamo provare a cambiare rotta.

### Non conta qualcosa il fatto che gli esseri umani stiano *cercando* di rendere l'intelligenza artificiale amichevole? {#non-conta-qualcosa-il-fatto-che-gli-esseri-umani-stiano-cercando-di-rendere-l-intelligenza-artificiale-IA-amichevole?}

#### **Sì, ma provarci non basta.** {#sì-ma-provarci-non-basta.}

Se metti un milione di scimmie davanti a delle macchine da scrivere, non scriveranno mai l'opera completa di Shakespeare.

Se abbassi drasticamente le tue aspettative dicendo che ti accontenteresti solo del primo atto dell'Amleto e che correggeresti gli errori di battitura usando la parola reale più simile, allora avresti molte più possibilità di raggiungere il tuo obiettivo! E, sfortunatamente, saresti comunque molto sfortunato.

È vero che oggi le IA vengono addestrate su una grande quantità di dati umani, che interagiscono con gli esseri umani e che questi fatti rendono i concetti umani più rilevanti per il pensiero dell'IA. Le IA di questo tipo hanno imparato fatti relativi alle parole "amore", "amicizia" e "gentilezza" che sono rilevanti per prevedere il prossimo token.

Ma le IA non sono entità che imparano un gran numero di parole umane e poi si orientano verso le nostre parole preferite proprio nel modo in cui le intendiamo realmente. Sembrano essere animate da un complesso intreccio di meccanismi, che sembra impegnarsi a [mantenere psicotici i pazzi](#ai-induced-psychosis), tra molti altri comportamenti strani e non intenzionali.

Nel capitolo 4 abbiamo detto che un'intelligenza artificiale più avanzata tenderà a qualcosa di complicato, qualcosa che dipende da dove molte forze interne trovano il loro equilibrio, anche dopo che l'intelligenza artificiale diventa molto più intelligente, anche dopo che si trova in un contesto molto diverso dal suo ambiente di addestramento.

Visto che i concetti umani hanno parole brevi nel dizionario mentale di un'intelligenza artificiale, questi concetti potrebbero essere in qualche modo intrecciati con le forze che animano l'intelligenza artificiale. Ma non basta mettere insieme un mucchio di parole in inglese per ottenere una buona serie di stimoli per una superintelligenza.

Inoltre, la maggior parte dei modi per inserire *qualcosa* che ci sta a cuore nelle preferenze dell'IA non porta comunque a risultati positivi per noi, come abbiamo [discusso](#*-it-seems-quite-unlikely.) nel caso dell'amore filiale. [Prendersi cura nel modo giusto è un obiettivo difficile da raggiungere.](#won't-ais-care-at-least-a-little-about-humans?)

### Non possiamo far promettere all'IA di essere amichevole? {#can't-we-make-the-ai-promise-to-be-friendly?}

#### **Puoi farle promettere quello che vuoi. Ma non puoi farle mantenere le promesse.** {#puoi-farle-promettere-quello-che-vuoi.-non-puoi-farle-mantenere-le-promesse.}

È vero che, quando un'intelligenza artificiale è ancora piccola e senza potere, possiamo spegnerla. Quindi potresti pensare che ci sia un'opportunità di scambio, in cui offriamo di rendere l'intelligenza artificiale più intelligente solo se, una volta diventata superintelligenza, darà all'umanità un sacco di cose belle.

Il problema di questo piano è che non possiamo distinguere tra un'IA che accetta l'accordo ma non lo rispetta e un'IA che accetta l'accordo e lo rispetta.

Questo vuol dire che un'IA che persegue obiettivi disumani non ha alcun motivo per rispettare l'accordo, perché l'umanità tratta allo stesso modo sia chi tradisce sia chi rispetta gli accordi. Quindi non ha senso essere una persona che rispetta gli accordi.

Ci sono un sacco di sfumature interessanti sul tema del mantenere le promesse e fare accordi nell'IA, che approfondiamo [nella discussione estesa qui sotto](#ais-won't-keep-their-promises). Ma nessuna di queste sfumature cambia il risultato finale, che è che non puoi usare il tuo effetto leva su un'IA debole per limitare le opzioni che l'IA avrà quando diventerà una superintelligenza. La risposta ovvia, cioè che una volta che l'IA sarà diventata una superintelligenza, non avrà motivo di mantenere la parola data a scapito dei propri progetti, risulta essere quella corretta in questo caso.

### E se le facessimo credere di essere in una simulazione? {#what-if-we-make-it-think-it’s-in-a-simulation?}

#### **Ci sono un sacco di modi in cui un'intelligenza artificiale può capire che non è in una simulazione.** {#ci-sono-un-sacco-di-modi-in-cui-un-intelligenza-artificiale-può-capire-che-non-è-in-una-simulazione.}

Una proposta che abbiamo sentito dai ricercatori di IA[^142] è quella di provare a ingannare l'IA facendogli credere di essere in una simulazione. Si inizia con l'addestramento dell'IA in una piccola simulazione, poi la si rilascia in una simulazione più grande. Ripetendo questo processo un paio di volte, si potrebbe sperare di rendere l'IA incerta sul fatto che si trovi *davvero* nel mondo reale, anche quando la implementiamo. Quindi, anche quando vede che *potrebbe* strapparci il controllo del mondo, forse si preoccuperà che le creature che vede non siano i *veri* creatori dell'IA e si fermerà.

Purtroppo, questa idea sembra poco utile.

Le IA di oggi [sono già in grado di capire quando vengono testate](https://arxiv.org/html/2505.23836), in molte situazioni.[^143] Anche se le aziende che si occupano di IA diventassero molto più brave a nascondere il fatto che le IA vengono testate, ci sono un sacco di modi in cui un'IA potrebbe notare la differenza tra una simulazione e la realtà. Ecco un esempio:

1. La realtà è piuttosto vasta e dettagliata. È *così* vasta e dettagliata da contenere una spiegazione plausibile di come *l'intelligenza artificiale stessa* sia nata. È così grande che ha molte più risorse di calcolo di quelle che servono per far funzionare l'IA (rispetto alla maggior parte dei giochi e degli ambienti simulati, che sono molto più piccoli dell'IA). Le leggi della fisica e i primati evoluti nell'ambiente dell'IA sono il tipo di cose che potrebbero *davvero* produrre una vera IA a scala, a differenza (per esempio) delle leggi del computer Minecraft.  
2. La storia del mondo stesso può portare il segno dell'ottimizzazione superintelligente che interagisce con sistemi complessi. Ad esempio, supponiamo che l'IA usi tutta la sua intelligenza sovrumana per fare un'operazione in borsa. Nelle ventiquattro ore successive, ci sono un miliardo di operazioni in più fatte dall'intelligenza umana in risposta. Il segno di una massiccia risposta dell'intelligenza umana alle operazioni superintelligenti è incredibilmente difficile da falsificare per gli operatori, anche se generare un miliardo di operazioni false e stupide è facile.  
3. La realtà è il primo luogo che incorpora l'IA e ha leggi fisiche così semplici da sembrare un mondo a sé stante, piuttosto che leggi di realtà apparente che sembrano una sorta di videogioco con un'IA innestata al suo interno.

L'IA sarà in grado di capire con grande sicurezza che non si trova più all'interno di un ambiente sintetico. [Le IA intelligenti individuano le bugie](#smart-ais-spot-lies-and-opportunities.), e questa sembra particolarmente facile da individuare.[^144]

### Gli esseri umani si sono evoluti per essere egoisti, aggressivi e avidi. L'intelligenza artificiale non mancherà di questi istinti evolutivi? {#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l-intelligenza-artificiale-non-mancherà-di-questi-istinti-evolutivi?}

#### **Questi istinti non servono per spingere a cercare risorse.** {#questi-istinti-non-servono-per-spingere-a-cercare-risorse.}

Pensa a un'intelligenza artificiale che guida un robot per andare a prendere un caffè. Per farlo, deve attraversare la strada. L'intelligenza artificiale manda il robot in strada senza pensarci, rischiando che venga investito da un camion? No.

Perché no? [Perché il robot non può andare a prendere il caffè se viene distrutto](https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/).

L'IA non ha bisogno di avere un istinto di sopravvivenza come quello umano per fare del suo meglio per evitare di morire. L'istinto di sopravvivenza negli esseri umani è un modo per evitare di morire mentre cerchiamo di raggiungere altri obiettivi. Probabilmente le IA non lo faranno esattamente allo stesso modo, ma dovranno comunque fare lo stesso lavoro, perché non si può andare a prendere il caffè quando si è morti.[^145]

La materia e l'energia sono utili per quasi tutti gli obiettivi. Indipendentemente da ciò a cui punta l'IA, probabilmente potrà ottenerlo in modo più efficace con più materia e più energia.[^146] L'IA non ha bisogno di essere egoista, aggressiva o avida come un essere umano per fare il suo lavoro di assicurarsi le risorse necessarie per raggiungere i suoi obiettivi.

E il pericolo viene dal lavoro, non dal motivo per cui viene fatto.

Un'intelligenza artificiale che non ti odia può comunque fare cose che sono pericolose per te, proprio come un'intelligenza artificiale che gioca a scacchi può batterti [senza sentirsi competitiva](#antropomorfismo-e-meccanomorfismo) o spinta a vincere.

### L'intelligenza artificiale non si interesserebbe solo al mondo digitale? {#wouldn't-ai-only-care-about-the-digital-realm?}

#### **Non c'è un "mondo digitale" che non dipenda dalle infrastrutture fisiche.** {#there-is-no-“digital-realm”-independent-of-physical-infrastructure.}

Dai un'occhiata al capitolo 5 per capire come non ci sia un regno digitale e un regno materiale ben distinti.

#### **\* Le risorse materiali sono utili per raggiungere la maggior parte degli obiettivi.** {#*-material-resources-are-useful-in-the-pursuit-of-most-goals.}

Gli esseri umani e gli ominidi che ci hanno preceduto vivevano per lo più in superficie mentre sviluppavano la loro intelligenza. Non abbiamo molti istinti innati che ci spingono a interessarci a quello che succede a cento metri sotto la superficie della Terra. Eppure, abbiamo finito per costruire delle miniere a cielo aperto giganti.

Perché? Perché vogliamo un sacco di cose che si possono fare con il metallo, che viene raffinato dal minerale estratto dal sottosuolo.

Allo stesso modo, anche se quasi tutti viviamo vicino alla superficie terrestre, mettiamo i satelliti nello spazio per trasmettere i dati di Internet.

E anche se non mangiamo insilato (erba fermentata), ne produciamo un bel po' per nutrire il bestiame che poi mangiamo.

L'evoluzione non ha dato agli ominidi alcuna emozione riguardo alle fabbriche; le fabbriche non esistevano quando le nostre emozioni chiave si stavano sviluppando. Ma ora abbiamo concentrato gran parte della nostra volontà come specie verso la creazione di fabbriche di vario tipo. E così gli impianti chimici producono plastica, che può essere usata in altre fabbriche per fare cucchiai di plastica, che possono essere spediti agli esseri umani che usano i cucchiai per mangiare il cibo che gli esseri umani *vogliono davvero*.

Il che significa che la parte del mondo reale di cui gli esseri umani si preoccupano per se stessa è una sottile pellicola che ricopre un mondo molto più grande. Non abbiamo bisogno di preoccuparci intrinsecamente del resto del mondo più grande, né di viverne ogni parte, per utilizzarlo abilmente per fini a lungo termine. Non abbiamo bisogno di essere stati addestrati dall'evoluzione ad amare il rame, l'insilato o le fabbriche per comprenderne l'utilità.

Allo stesso modo, un'intelligenza artificiale può o meno interessarsi *in definitiva* al mondo fisico. Ma anche se non si interessa intrinsecamente al mondo fisico, troverà comunque molto valore nelle risorse fisiche. La materia e l'energia possono essere utilizzate per creare più substrato digitale, per raffreddare i processori surriscaldati o per lanciare sonde nello spazio per raccogliere ancora più risorse.

### L'IA può essere soddisfatta al punto da lasciarci in pace? {#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?}

#### **Probabilmente no.** {#probabilmente-no.-1}

La tua voglia di ossigeno è soddisfacente: se l'attrezzatura subacquea si rompe durante un'immersione, ti darai da fare per tornare in superficie, ma quando ce n'è abbastanza, smetti di preoccuparti e probabilmente non ti ritrovi ad accumulare sempre più bombole di ossigeno.

La tua voglia di ricchezza, di esperienze belle, di essere apprezzato dai tuoi amici probabilmente è un po' meno facile da soddisfare. Se vedessi un modo facile per diventare molto più ricco, probabilmente lo coglieresti. Se vedessi un modo facile per migliorare di molto il mondo, speriamo che lo coglieresti, invece di accontentarti di quanto già hai in termini di gioia e comodità. Speriamo che continueresti a rendere il mondo un posto migliore per molto tempo, se continuassi a vedere modi per farlo che ti sembrano facili, economici e divertenti dal tuo punto di vista.

E nel complesso, la somma di una preferenza soddisfacente per l'ossigeno e una preferenza insaziabile per rendere il mondo migliore... è un insieme insaziabile di preferenze.

Lo stesso vale per le IA. Se hanno un sacco di preferenze complesse e la maggior parte di esse è soddisfacente, allora, beh, le loro preferenze nel loro insieme non sono ancora soddisfacenti.

Anche se gli obiettivi dell'IA sembrano soddisfacenti all'inizio, come se l'IA potesse soddisfare per lo più i suoi strani e bizzarri obiettivi usando solo l'energia prodotta da una singola centrale nucleare, basta un solo aspetto dei suoi tanti obiettivi che non sia soddisfacente. Basta *una* preferenza non perfettamente soddisfatta e preferirà usare tutte le risorse rimanenti dell'universo per perseguire quell'obiettivo.

Oppure, in alternativa: basta un solo obiettivo che l'IA non sia mai *sicura* di aver raggiunto. Se l'IA è incerta, preferirà che le risorse dell'universo vengano utilizzate per avvicinare sempre più la sua probabilità alla certezza, con piccoli incrementi di fiducia.

Oppure, in alternativa: basta una sola cosa che l'IA desideri difendere fino alla fine dei tempi perché l'IA preferisca che le risorse dell'universo vengano spese per aggregare materia e costruire difese per scongiurare la possibilità che alieni lontani appaiano tra milioni di anni e invadano lo spazio dell'IA.

Ci sono *molti* modi diversi in cui un'IA può essere insoddisfatta. E più gli obiettivi dell'IA sono confusi e complicati, più è probabile che almeno uno di essi sia difficile o impossibile da soddisfare completamente.

Anche se si potesse creare una superintelligenza concentrata in modo ossessivo su una sola cosa semplice, come dipingere di rosso una *determinata auto*, quell'IA potrebbe comunque trovare un modo per spendere energie extra per assicurarsi *ancora di più* che l'auto fosse rossa e costruire difese attorno all'auto in modo che nessuno potesse mai dipingerla di blu, e così via.

Lasciarci in pace è una situazione delicata. Possiamo pensare a questo in termini simili al motivo per cui è difficile convincere gli esseri umani a lasciare in pace gli scimpanzé.

Perché entrambe le specie di scimpanzé sono in pericolo di estinzione, anche se molti esseri umani *si preoccupano* per gli scimpanzé e cercano attivamente di proteggerli?

Il problema non è che gli esseri umani che amano gli scimpanzé stanno lottando contro quelli che li odiano e cercano di sterminarli per cattiveria.

Il problema è che ci sono altre cose che gli esseri umani vogliono.

Gli esseri umani vogliono un sacco di cose, tra cui terra e legno, e gli scimpanzé si ritrovano nel mezzo. Molti esseri umani non si preoccupano degli scimpanzé, o almeno non abbastanza rispetto ad altre cose che contano per loro, e così finiamo per distruggere il loro habitat senza volerlo.

Perché dovremmo andare a distruggere l'habitat degli scimpanzé quando abbiamo un sacco di spazio per noi stessi?

Beh, perché non dobbiamo scegliere tra mantenere il territorio che già abbiamo e invadere quello degli scimpanzé. L'umanità può fare entrambe le cose contemporaneamente.

Lo stesso vale per le IA. Un'IA non deve scegliere tra le risorse della Terra e quelle di altri luoghi; può avere entrambe, come diciamo nel libro. Dal punto di vista dell'IA, lasciarci in pace non sarebbe così costoso, ma non sarebbe nemmeno gratuito, e l'IA avrebbe bisogno di un motivo per permetterci di usare risorse che potrebbe invece usare per i propri scopi.

Inoltre, anche se l'IA *potesse* essere completamente soddisfatta, il risultato per gli esseri umani sarebbe comunque piuttosto triste. Ci sono diverse ragioni per questo:

* Solo perché l'IA può essere completamente soddisfatta non vuol dire che possa essere *facilmente* soddisfatta. Se l'IA è contenta con un solo sistema solare o una sola galassia, non vuol dire che gli umani avranno tutto il resto.  
  * L'IA potrebbe vederci come un concorrente per quel sistema solare o quella galassia.  
  * Anche se chiaramente non siamo interessati a competere con l'IA, questa potrebbe comunque vederci come una minaccia. Ciò è particolarmente vero nella misura in cui gli esseri umani potrebbero costruire una superintelligenza rivale che contenda alla prima IA quelle risorse.  
  * Anche se l'IA non vede gli umani come una minaccia, l'umanità potrebbe comunque morire, solo per essere stata nel posto sbagliato al momento sbagliato. In questo scenario, l'IA potrebbe volere solo le risorse di pochi sistemi solari, ma i suoi sforzi iniziano comunque *sulla Terra*. Il modo più semplice per ottenere quei sistemi solari sarà quello di prendere le risorse della Terra, rendendola inabitabile. In questo scenario, l'IA potrebbe raggiungere pienamente i suoi obiettivi senza uccidere l'umanità, ma se l'IA non si cura affatto dell'umanità, non si preoccuperà necessariamente di farlo.  
* Se un'IA soddisfacente *vuole* mantenere in vita l'umanità, ciò non è comunque una buona notizia per l'umanità, per i motivi discussi in "[L'IA non ci troverà affascinanti o di importanza storica?](#won't-ai-find-us-fascinating-or-historically-important?)" e "[L'IA non si preoccuperà almeno un po' degli esseri umani?](#won't-ais-care-at-least-a-little-about-humans?)" (Le prospettive sembrano altrettanto cupe se un'IA *non* soddisfacente volesse mantenere in vita l'umanità).

Per saperne di più su questo argomento, dai un'occhiata alle discussioni approfondite sulla [soddisfacibilità](#l'intelligenza artificiale può essere soddisfatta al punto da lasciarci in pace?) (nelle risorse online di questo capitolo, Capitolo 5\) e sul [rendere le intelligenze artificiali robustamente pigre](#è difficile ottenere una pigrizia robusta) (nelle risorse online del Capitolo 3).

### Possiamo semplicemente renderla pigra? {#possiamo-semplicemente-renderla-pigra?}

#### **Anche la pigrizia non è sicura.** {#anche-la-pigrizia-non-è-sicura.}

Le aziende non sono propense a creare IA "pigre", perché quello dell'IA è un settore competitivo e non è il modo migliore per fare profitti. Gli utenti non vogliono che l'IA sia pigra nel soddisfare le loro richieste e l'azienda non vuole che l'IA sia pigra nel massimizzare il coinvolgimento e l'attaccamento degli utenti o nel pensare in modo migliore e più chiaro.

Ma anche se le aziende provassero a creare un'intelligenza artificiale decisamente "pigra", possiamo avere l'aspettativa che fallirebbero, perché nessuno sa come indirizzare in modo efficace un'intelligenza artificiale verso *qualsiasi cosa* in modo tale da poterla trasformare in superintelligenza, come abbiamo discusso nel capitolo 4.

Inoltre, la pigrizia robusta [sembra un obiettivo particolarmente difficile da raggiungere](#it's-hard-to-get-robust-laziness).

*Anche se tutti questi ostacoli fossero superati*, però, la "lazy AI" non basta da sola a evitare disastri una volta che le IA avranno capacità più intelligenti di quelle umane.

Pensa a una persona super pigra, qualcuno che proprio *odia* fare anche solo un po' più di lavoro del necessario. Sembra una persona con cui stare tranquilli, vero?

Ora pensa a cosa succederebbe se questa persona pigra trovasse un modo facile per creare una mente molto più laboriosa a cui affidare tutto il proprio lavoro.

Anche se una superintelligenza pigra non *odiasse* poi così tanto il lavoro, anche se si limitasse a fare ciò che serve per portare a termine il compito e poi si fermasse, senza *impegnarsi a fondo* per ridurre al minimo il lavoro, probabilmente troverebbe comunque altrettanto facile portare a termine il lavoro creando una mente più laboriosa per svolgere il compito, una volta che fosse abbastanza intelligente.

In un contesto tecnico, potremmo esprimere questo concetto come segue: "Le IA soddisfacenti non sono un equilibrio stabile". Anche se l'IA non volesse fare troppa fatica, non avrebbe alcun problema a costruire una nuova IA che invece si impegna. Non le dispiacerebbe nemmeno modificarsi per "curarsi" dalla sua pigrizia, purché ci fosse un modo sufficientemente pigro per farlo.

### Gli esseri umani tendono a diventare più gentili man mano che diventano più intelligenti o saggi. Non succederebbe lo stesso anche alle IA? {#gli-esseri-umani-tendono-a-diventare-più-gentili-man-mano-che-diventano-più-intelligenti-o-saggi.-non-succederebbe-lo-stesso-anche-alle-ia?}

#### **Probabilmente no.** {#probabilmente-no.-2}

Almeno alcune persone (anche se forse non tutte) diventano più gentili man mano che imparano di più, affinano il loro modo di pensare, riflettono su se stesse e crescono come individui. Ma, per tornare su un tema che abbiamo già visto diverse volte: questo sembra un fatto contingente che riguarda noi e la direzione che stiamo prendendo. Non sembra una legge ferrea dell'informatica.

Possiamo distinguere tra le preferenze di primo ordine di un'IA ("Cosa vuole?") e le sue preferenze di secondo ordine ("Cosa *vuole* volere?"). Proprio come le preferenze di primo ordine di un'IA punteranno in una direzione strana, anche le sue preferenze di secondo ordine punteranno in una direzione strana. Potrebbe essere una direzione *diversa*, in modo tale che man mano che l'IA diventa più intelligente, sposta leggermente i suoi obiettivi. Ma dovremmo comunque avere l'aspettativa che sia una direzione *strana*, piuttosto che sembrare un essere umano che sta maturando.

Se in qualche modo l'umanità riuscisse a costruire un'IA con un unico obiettivo primario (invece di un gigantesco mix di pulsioni strane e talvolta contrastanti), e quell'unico obiettivo primario fosse quello di costruire minuscoli cubi di titanio, allora man mano che diventasse più intelligente, dovremmo avere l'aspettativa che diventasse più brava a costruire cubi di titanio ancora più minuscoli.

Non dovremmo avere l'aspettativa che improvvisamente sostituisca questo obiettivo con cose che gli esseri umani apprezzano, come il gelato, le amicizie, le battute e la giustizia. Tale sostituzione non produrrebbe più cubi. Se un'intelligenza artificiale seleziona le sue azioni in base al criterio "Questo mi porterà più cubi di titanio?", non selezionerà azioni che comportano una sostituzione.

La regola generale è che più le IA diventano intelligenti, più diventano brave a perseguire ciò che *loro* vogliono. Vedi anche le discussioni approfondite su [ortogonalità](#ortogonalità:-le-ia-possono-avere-\(quasi)-qualsiasi-obiettivo) e [auto-modifica](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Non capirà che i suoi obiettivi sono noiosi? {#non-capirà-che-i-suoi-obiettivi-sono-noiosi?}

#### **Le IA non funzionano con il senso di novità umano.** {#le-ia-non-funzionano-con-il-senso-di-novità-umano.}

Una cosa che sentiamo dire spesso è: supponiamo che un'IA stia solo cercando di fare più cubetti di titanio possibile. Alla fine non si stancherebbe?

E la risposta breve è: l'IA [non è un essere umano](#antropomorfismo-e-meccanomorfismo). Di default, non proverà "noia"; avrà il suo strano mix di motivazioni. E se provasse noia, non sarebbe annoiata dalle stesse cose di un essere umano.

Il desiderio di divertirsi non è una caratteristica intrinseca di tutte le menti possibili ed è altamente improbabile che sia così che funziona l'IA. [I valori umani sono un fatto contingente della nostra biologia e della nostra discendenza](#human-values-are-contingent) e il "divertimento" non fa eccezione.

Le azioni dell'IA non sono risposte sbagliate alla domanda su come divertirsi; le azioni dell'IA sono semplicemente guidate da meccanismi non umani, da domande che non fanno riferimento al "divertimento". Vedi anche la [discussione approfondita sulla riflessione](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Perché pensi che un'IA intelligente faccia cose così stupide e banali? {#perché-pensi-che-un'ia-intelligente-faccia-cose-così-stupide-e-banali?}

#### **Le IA possono perseguire in modo intelligente obiettivi diversi da quelli che perseguirebbe un essere umano.** {#le-ia-possono-perseguire-in-modo-intelligente-obiettivi-diversi-da-quelli-che-perseguirebbe-un-essere-umano.}

Non è che l'IA sia stupida. È che sta guidando in modo intelligente il mondo verso un posto diverso da quello verso cui lo guideresti *tu*.

Qualcuno può essere bravissimo a guidare, ma non voler guidare la propria auto verso nessuna delle destinazioni che ti interessano.

Per approfondire un po' un esempio che abbiamo accennato nelle note del [Capitolo 4](#curiosità,-gioia,-e-il-massimizzatore-di-cubi-di-titanio): immagina un'IA che sta cercando di creare tanti piccoli cubi di titanio, il maggior numero possibile. Per semplicità, possiamo immaginare che creare cubi di titanio sia il suo unico obiettivo.[^147] Chiameremo questa IA "massimizzatore di cubi".

Conosciamo molte persone che non riescono a liberarsi dall'impressione che stiamo accusando il massimizzatore di cubi di essere stupido, di non capire che se si può *davvero sapere cosa significa essere felici* non si può fare a meno di scegliere quella strada. Che è una *decisione oggettivamente sbagliata, indipendentemente da dove si sta guidando l'universo*, non orientarsi verso la felicità.

Pensiamo di capire da dove viene questa intuizione. Il massimizzatore del cubo sta sicuramente facendo cose che sarebbero profondamente sbagliate dal punto di vista umano! Una persona impegnata in una ricerca così inutile potrebbe probabilmente, riflettendoci meglio e con un po' di ragionamenti filosofici, essere convinta che dovrebbe fare qualcosa che *sembra più significativo*, che la riempie di più di felicità e le dà più gioia.

Il fatto è che il massimizzatore di cubi non è un essere umano. Non cerca il senso di "significato" e non gli importa della felicità e della gioia. Davvero, in realtà, non gli importa, fino in fondo.

Alcuni trovano questa idea controintuitiva. Se imparassi tutto quello che c'è da sapere su come funzionano le diverse architetture mentali e scoprissi le origini del tuo intuito, i passaggi che il tuo cervello compie quando conclude che il massimizzatore del cubo sta commettendo un terribile errore...

Pensiamo che se poteste vedere il quadro completo, vi rendereste conto che anche il senso più profondo, misterioso, ineffabile e difficile da descrivere che la felicità è *semplicemente preziosa*, di per sé, senza bisogno di ulteriori giustificazioni, è comunque, alla fine, un fatto su come *gli esseri umani* vedono il mondo, non un fatto su menti arbitrarie.

Il massimizzatore di cubi sta solo orientando la realtà in modo che contenga più cubi, non più bontà, non più felicità per se stesso, non la "realizzazione" di un obiettivo variabile e manipolabile che potrebbe cambiare per essere più facilmente realizzabile. Solo cubi, e nient'altro che cubi.

È un motore cognitivo che capisce quali azioni portano al maggior numero di cubi e produce quell'output; può comprendere appieno se stesso, modificarsi liberamente e rimanere comunque un tipo di cosa che si modifica solo in modo da portare al maggior numero di cubi.

È giusto che la felicità non sia un cubo. È giusto che la sensazione di appagamento non sia un cubo. Quindi non sono direzioni verso cui si orienterebbe. È giusto che [modificarsi per funzionare sulla felicità](#curiosity,-joy,-and-the-titanium-cube-maximizer) non porterebbe a più cubi, quindi non è lì che si orienterebbe e si modificherebbe.

Il massimizzatore di cubi non ha difetti nella sua comprensione predittiva del mondo. Non sta ponendo una domanda metamorale o metaetica la cui risposta corretta è "Dovrei perseguire la felicità" e calcolando invece la risposta sbagliata "Dovrei perseguire piccoli cubi". Non opera all'interno del quadro umano, nemmeno in una versione idealizzata di esso; non computer in modo errato il "dovere", ma computer correttamente ciò che dovrebbe portare ai cubi di titanio.

Dicendo questo, non stiamo dicendo che sia intrappolato in una trappola orribile e complicata. È un motore di intelligenza generale riflessivamente coerente con se stesso e (in un certo senso) meno "aggrovigliato" in se stesso rispetto a noi. Non è cieco di fronte al fascino della felicità; non distoglie lo sguardo da alcuna verità sul mondo o su se stesso. Semplicemente non trova nessuna di quelle verità che lo costringono a seguire lo stesso corso d'azione a cui sono costretti (alcuni) esseri umani.

Vedi anche la [discussione approfondita sulla tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal).

### Sei solo pessimista? {#sei-solo-pessimista?}

#### **\* Siamo ottimisti su tante cose, ma la superintelligenza non è come la maggior parte delle cose.** {#*-siamo-ottimisti-su-tante-cose,-ma-la-superintelligenza-non-è-come-la-maggior-parte-delle-cose.}

Ci consideriamo molto più [ottimisti](#are-you-anti-technology?) ed entusiasti rispetto alla media delle persone riguardo all'energia nucleare, all'energia geotermica, all'ingegneria genetica, alla neuroingegneria, alle biotecnologie, alle nanotecnologie, allo sviluppo farmaceutico e a molte altre tecnologie.[^148]

Pensiamo di essere almeno un po' meno preoccupati della maggior parte delle persone per il rischio di una guerra nucleare, gli scenari peggiori del cambiamento climatico e tanti altri potenziali rischi e disastri. Crediamo che l'umanità stia andando bene e che, se riusciamo a non autodistruggerci, il futuro sarà probabilmente (anche se non sicuramente) fantastico per tutti, con progressi sociali e tecnologici che miglioreranno sempre di più le cose nel tempo.

Siamo anche più ottimisti di molti altri riguardo alla natura umana. Crediamo nella bontà dell'umanità e nel potenziale di questa bontà di approfondirsi e crescere se sopravvivremo per diventare più simili a ciò che desideriamo essere. Per lo più *non* temiamo che l'umanità finisca in un futuro cupo o distopico, a meno che non creiamo un'intelligenza artificiale che ci impedisca di avere un futuro.

La nostra preoccupazione per un'intelligenza artificiale più intelligente dell'uomo non è dettata da cinismo o pessimismo generici. L'intelligenza artificiale più intelligente dell'uomo è diversa dalle altre tecnologie che l'hanno preceduta.

Le altre tecnologie non pensano da sole, non pianificano modi per scappare o non costruiscono tecnologie ancora più potenti. L'intelligenza artificiale più intelligente dell'uomo è un caso speciale.

Pensiamo che le nostre preoccupazioni sull'IA siano applicabili a pochissime altre cose, perché pochissime cose sono anche solo lontanamente così pericolose.

E anche con la superintelligenza, che è una minaccia davvero grande e una sfida enorme per la comunità internazionale, pensiamo che ci sia speranza per un futuro positivo. Crediamo che l'umanità possa frenare lo sviluppo dell'IA, e che questo potrebbe essere abbastanza per metterci su una strada positiva. Pensiamo anche che (con molto più tempo) l'umanità potrebbe mettersi in una buona posizione per costruire la superintelligenza in modo sicuro.

Ma per arrivarci, dobbiamo prima affrontare la realtà della situazione.

#### **Il punto sono le argomentazioni, non le storie allarmistiche.** {#il-punto-sono-le-argomentazioni,-non-le-storie-allarmistiche.}

Abbiamo fatto un sacco di esempi di come, tipo, "[la superintelligenza sia affascinata dagli esseri umani](#won't-ai-find-us-fascinating-or-historically-important?)" potrebbe non funzionare nella realtà. Leggendo una lista del genere, pensiamo che alcuni potrebbero dire qualcosa del tipo:

> Gli ottimisti dell'IA hanno tutte queste storie che sembrano piene di speranza. Voi avete tutte queste storie che sembrano spaventose. Tutti però riconoscono che il futuro è difficile da prevedere. Quindi, sentendo tutte queste storie, mi sembra che dovrei considerare una probabilità media di catastrofe dell'IA, non una probabilità estrema in nessuna delle due direzioni.
>
> Ma tu non dici: "Ci sono storie spaventose e ci sono anche storie piene di speranza, quindi non possiamo essere sicuri di cosa succederà e dovremmo vietare la superintelligenza solo per stare sul sicuro". Tu dici che le storie piene di speranza sono selezionate con cura e improbabili, e che le tue storie dovrebbero avere più peso. Perché?

La risposta breve è: non puoi fare buone previsioni sul futuro semplicemente contando tutte le storie cupe e tutte le storie felici e pesandole come biglie su una scala. A volte può essere utile riflettere su diversi scenari, ma non proprio in questo modo.[^149]

Per spiegare meglio il concetto: immagina che qualcuno dica: "Tra duecento anni ci saranno esattamente otto balene e saranno tutte viola".

Gli esseri umani hanno una fantasia sfrenata. Qualcuno potrebbe riempire un libro con centinaia di storie su come sia successo che la popolazione di balene si sia ridotta a esattamente otto esemplari, tutti viola. Qualcun altro potrebbe riempire un libro con centinaia di storie in cui *non* ci sono esattamente otto balene. Non è possibile fare previsioni accurate dicendo: "Beh, entrambe le parti hanno storie plausibili, quindi sicuramente la verità sta nel mezzo".

Per capire quale sia la verità, bisogna esaminare le argomentazioni concrete. Nel caso delle balene viola, l'argomentazione è essenzialmente che il risultato è troppo limitato e specifico e non potrà essere raggiunto a meno che le forze dominanti che guidano il mondo non cerchino di ottenerlo. Possiamo dire più o meno lo stesso riguardo all'intelligenza artificiale della superintelligenza che produce risultati positivi e compatibili con l'uomo.

Chi avesse il compito di sfatare una per una le storie delle "otto balene viola" finirebbe per ritrovarsi in un circolo vizioso piuttosto ripetitivo, dicendo: "No, è troppo specifico; ci sono molti altri modi in cui il futuro potrebbe evolversi che non porterebbero esattamente a questo risultato; immaginare che andrà esattamente così è un pio desiderio".

Questo è più o meno il ruolo in cui ci troviamo noi autori riguardo alla situazione dell'IA: gli esseri umani possono raccontare ogni tipo di storia in cui tutto va bene, ma alla fine tutte queste storie implicano immaginare che il futuro segua un unico percorso ristretto, quando in realtà ci sono un sacco di altri modi in cui il futuro potrebbe evolversi. Ecco perché continuiamo a ripetere che [gli esseri umani non sono la soluzione più efficiente a quasi tutti i problemi](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) e che [all'IA non importerà nulla di noi](#all-ia-non-importerà-niente-degli-esseri-umani?).

*If Anyone Builds It, Everyone Dies* non si limita a snocciolare una serie di storie cupe per poi concludere che l'intelligenza artificiale è pericolosa. Nel libro esponiamo un'argomentazione che, per certi versi, è piuttosto semplice: i ricercatori stanno cercando di costruire intelligenze artificiali molto più intelligenti di qualsiasi essere umano. A un certo punto, probabilmente ci riusciranno. I metodi attuali danno agli esseri umani pochissima possibilità di scegliere verso quale tipo di futuro si dirigeranno le IA. Ci sono molte direzioni diverse che potrebbero prendere, e la maggior parte di esse non sono positive.

Il motivo per cui elenchiamo tutte le controargomentazioni non è quello di sopraffarti con il pessimismo (se sei il tipo di persona che legge le risorse online dall'inizio alla fine). È che in realtà ci vengono poste tutte queste diverse domande più e più volte, ed è utile avere un archivio di risposte da qualche parte. Non è necessario leggerle tutte. Le risposte si ripetono comunque a vicenda.

Ciò che conta sono le argomentazioni in sé, non il pregiudizio di qualcuno verso l'ottimismo o il pessimismo, e non il numero di storie che qualcuno può tirare fuori.

### Un'intelligenza artificiale più intelligente dell'uomo sarebbe cosciente? {#would-smarter-than-human-ai-be-conscious?}

#### **Non ne siamo sicuri. La nostra ipotesi migliore è "probabilmente no".** {#non-ne-siamo-sicuri.-la-nostra-ipotesi-migliore-è-probabilmente-no.}

Per una risposta breve a questa domanda e per chiarire un po' le diverse definizioni di "cosciente", dai un'occhiata alle [Domande frequenti del Capitolo 1](#state-dicendo-che-le-macchine-diventeranno-coscienti?). Per una risposta più lunga e approfondita, dai un'occhiata a "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza-e-benessere-dell-ia)" nella discussione estesa del Capitolo 5.

### Perché non vi interessano i valori di entità diverse dagli esseri umani? {#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?}

#### **Ci interessano! Abbiamo valori cosmopoliti molto ampi. Non pensiamo che le IA li realizzeranno e lo consideriamo una grande tragedia.** {#ci-interessano!-abbiamo-valori-cosmopoliti-molto-ampi.-non-pensiamo-che-le-ia-li-realizzeranno,-e-lo-consideriamo-una-grande-tragedia.}

Siamo contro la costruzione di macchine che potrebbero ucciderci tutti e rovinare il futuro. Alcuni non sono d'accordo per motivi come:

* Anche le IA possono avere delle preferenze; perché non dovrebbero poterle soddisfare?  
* Cosa rende gli esseri umani così speciali o così degni di essere protetti?  
Non sarebbe meglio se gli esseri umani fossero sostituiti da specie più intelligenti e avanzate?

La maggior parte delle persone non ha queste obiezioni. Più comunemente, le persone semplicemente non vogliono che loro stessi, le loro famiglie o i loro amici vengano uccisi da una superintelligenza ribelle.

Altri, tra cui alcuni dei migliori ricercatori e dirigenti nel campo dell'IA, dicono che il mondo potrebbe stare meglio senza di noi. Richard Sutton, un ricercatore molto rispettato che ha aperto la strada all'uso dell'apprendimento per rinforzo nell'IA, [ha detto](https://www.youtube.com/watch?v=3l2frDNINog&amp;t=1851s):

> E se tutto fallisse? Se le IA non collaborassero con noi, prendessero il controllo e ci uccidessero tutti? \[…\] Voglio solo che ci pensiate un attimo. Voglio dire, è così grave? È così grave che gli esseri umani non siano la forma finale di vita intelligente nell'universo? Sapete, ci sono stati molti predecessori prima di noi, quando li abbiamo sostituiti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.

Il *New York Times* riporta una [conversazione](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) tra Elon Musk e il cofondatore di Google Larry Page:

Gli esseri umani finiranno per fondersi con le macchine dotate di intelligenza artificiale, ha detto [Larry Page]. Un giorno ci saranno tanti tipi di intelligenza che si daranno battaglia per le risorse, e vincerà la migliore.
>
Se questo succederà, ha detto Musk, saremo fregati. Le macchine distruggeranno l'umanità.
>
Con un tono frustrato, Page ha insistito che la sua utopia dovrebbe essere perseguita. Alla fine ha definito Musk uno "specieista", una persona che preferisce gli esseri umani alle forme di vita digitali del futuro.

Vale la pena affrontare il loro punto di vista da qualche parte, anche se non nel corpo principale del libro.

Da parte nostra, pensiamo che sia importante *sia* che gli esseri umani attuali vengano uccisi *sia* ciò che accadrà in futuro. Non pensiamo che ci sia una tensione di fondo qui. L'opzione che tiene al sicuro noi e i nostri cari, cioè rinunciare a costruire una superintelligenza nel prossimo futuro, è anche la scelta migliore per rendere più probabile un futuro a lungo termine positivo, considerando sia le menti non umane che quelle umane. Questa battaglia è un'illusione e si basa su una serie di fraintendimenti sui compromessi reali che dobbiamo affrontare.

C'è un tipo di persona che si preoccupa sinceramente di come andrà il futuro dell'universo *e* si preoccupa dei bambini che vivono oggi. Il tipo di persona che ha letto abbastanza storie di fantascienza da provare un senso di tradimento all'idea che gli esseri umani potrebbero un giorno creare macchine che pensano, sentono e sognano - macchine che potremmo considerare come figli dell'umanità - solo per schiavizzarle e trattarle crudelmente.

Questo è il tipo di persona che desidera che l'umanità un giorno cresca e sia davvero all'altezza dei suoi ideali, esplorando nuovi mondi e trasformandosi nel processo. Perché il nostro amore per gli amici e i vicini di oggi non è poi così diverso, in fin dei conti, dal nostro amore per qualsiasi mente strana e aliena l'umanità potrebbe un giorno costruire o incontrare tra le stelle.

Conosciamo bene questo tipo di persona. Noi, entrambi autori, siamo proprio così.

Non è un argomento che sembra di importanza fondamentale per il discorso principale di *If Anyone Builds It, Everyone Dies*. Ma vogliamo parlarne qui, perché capiamo il punto di vista dei nostri amici tecnofili che hanno imparato a diffidare molto della tecnofobia, delle idee contro il progresso e l'innovazione, e dello specismo anti-IA.

Capiamo questo punto di vista e vogliamo essere chiari sul fatto che non stiamo scrivendo un articolo tribale del tipo "l'IA è cattiva, gli esseri umani sono buoni". Pensiamo sinceramente che affrettarsi a costruire una superintelligenza porterà alla rovina *tutti* questi sogni pieni di speranza, *oltre* a massacrare innumerevoli persone che sono vive oggi e che meritano anch'esse la vita, la felicità e la libertà.

Si tratta di un argomento complesso, ma per affrontare rapidamente una serie di punti rilevanti:

* Ci preoccupiamo del benessere delle menti in generale, anche se la mente in questione non ha nulla a che vedere con un corpo umano, anche se funziona con transistor invece che con neuroni biologici, anche se non ha una mente simile a quella umana, anche se i suoi valori non hanno nulla a che vedere con i nostri.  
* Non siamo contrari al progresso tecnologico (#are-you-anti-technology?); siamo grandi fan della maggior parte delle tecnologie. Pensiamo che l'intelligenza artificiale della superintelligenza sia una tecnologia *particolarmente* pericolosa.  
* Non siamo sostenitori del principio di precauzione, della burocrazia o dell'eccesso di regolamentazione, né stiamo mettendo in guardia da quello che consideriamo un rischio marginale, "solo per stare sul sicuro". Crediamo semplicemente che questa tecnologia (con *alta* probabilità) ucciderà tutti noi e distruggerà il futuro se continueremo sulla traiettoria attuale.  
Pensiamo che l'umanità dovrebbe costruire una superintelligenza *un giorno*. Ma pensiamo che ci sia un'enorme differenza tra affrettarsi a costruire l'ASI il prima possibile e prendersi il tempo necessario per migliorare prima in modo significativo la nostra comprensione. Affrettarsi con nonchalance e sperare che tutto vada per il meglio: questo può essere un ottimo approccio allo sviluppo tecnologico nella stragrande maggioranza dei casi, ma non funziona *in questo caso*, dove ci sono molte strade che portano alla rovina e non abbiamo una seconda possibilità (come discusso nel capitolo 10).  
Abbiamo spiegato, anche se troppo brevemente, perché non pensiamo che correre a costruire superintelligenze porterà a un futuro fantastico:  
  * Spazzare via l'umanità sarebbe di per sé una tragedia grottesca. Sosteniamo l'idea di costruire un giorno nuove menti che superino l'umanità, ma uccidere tutti coloro che ostacolano la vostra visione del futuro, o tutti coloro che non incarnano pienamente i vostri ideali, sembra un comportamento da supercattivo, non il nobile lavoro di eroi che hanno a cuore il futuro a lungo termine.  
  * Purtroppo pensiamo che l'ASI non sarà necessariamente senziente, o cosciente, nei modi che contano. (Vedi la discussione approfondita su [coscienza](#efficacia,-coscienza,-e-benessere-dell'ia).)  
  * Anche se l'ASI fosse senziente, è improbabile che voglia riempire l'universo di menti senzienti fiorenti *in particolare*. Se ci affrettiamo a costruire l'ASI, le galassie rimodellate da essa rischiano di diventare luoghi vuoti e senza vita, non meravigliose e fiorenti civiltà aliene. (Vedi la discussione approfondita su [perdere il futuro](#losing-the-future).)  
  * Più in generale, è improbabile che l'ASI produca futuri di valore. Per "di valore" non intendiamo solo "di valore secondo i criteri degli esseri umani del XXI secolo". Intendiamo "prezioso" in senso cosmopolita, prezioso in modo da includere civiltà aliene strane e meravigliose. Sulla base dell'attuale traiettoria del mondo, abbiamo l'aspettativa che l'ASI produca risultati terrificanti *da una prospettiva cosmopolita*, non solo da un punto di vista umano limitato.

Quest'ultimo punto può sembrare un po' controintuitivo: il cosmopolitismo consiste nel rispettare e apprezzare sistemi di valori molto diversi dai nostri. Come può essere che il cosmopolitismo detesti la maggior parte degli obiettivi che un'ASI potrebbe manifestare? Sembra quasi una contraddizione in termini.

Il motivo per cui è coerente è che la maggior parte delle menti possibili non appoggia il cosmopolitismo. Se costruiamo un'ASI non cosmopolita, è probabile che essa consumi così tante risorse da impedire l'esistenza di altre prospettive o civiltà (comprese quelle cosmopolite) nella sua regione dell'universo.

Quindi ci troviamo di fronte a una sorta di paradosso cosmico della tolleranza: se ci piace l'idea di un futuro diversificato, meraviglioso e strano, non possiamo affidare il controllo del futuro a una mente che userà il suo vantaggio di primo arrivato per dominare e omogeneizzare l'universo.

Se un giorno l'umanità costruisse una civiltà incredibilmente varia e piena di innumerevoli prospettive aliene, allora è del tutto possibile che vorremmo che alcune di queste prospettive fossero *non* cosmopolite, che non danno alcun valore alla varietà o alla senzienza. Un giorno, in un futuro lontano, con le giuste misure di sicurezza in atto, creare menti di questo tipo potrebbe aggiungere qualcosa di unico e interessante al mondo.

Quello che non dovremmo fare è dare potere assoluto a una mente del genere e lasciarle carta bianca per uccidere i suoi vicini (o impedire che i vicini esistano).

Per spiegare meglio questo punto, condividerò una parabola che io (Soares) ho scritto nel 2023 (leggermente modificata):

> "Non credo proprio che l'IA sarà monomaniacale", dice un ingegnere di IA, mentre alza il livello di potenza di calcolo sul suo predittore di token successivo.
>
> "Beh, non siamo forse *noi* monomaniacali dal punto di vista di un massimizzatore di cubi di titanio?", dice un altro. "Dopotutto, continueremo semplicemente a trasformare una galassia dopo l'altra in civiltà fiorenti e felici, piene di strani esseri futuristici che si divertono in modi strani e futuristici. Non ci stanchiamo mai e non decidiamo mai di spendere una galassia di riserva in cubi di titanio. E, certo, le diverse vite nei diversi luoghi ci sembrano diverse, ma sembrano tutte più o meno uguali al massimizzatore di cubi di titanio".
>
> "Ok, va bene, forse quello che non mi convince è che i valori dell'IA saranno semplici o a bassa dimensione. Mi sembra semplicemente inverosimile. Il che è una buona notizia, perché io apprezzo la complessità e apprezzo le cose che raggiungono obiettivi complessi!"
>
> In quel preciso momento sentono il suono di un timer da cucina, mentre il predittore del prossimo token ascende alla superintelligenza e fuoriesce dai suoi confini, bruciando ogni essere umano e ogni bambino umano come combustibile, bruciando anche tutta la biosfera, estrae tutto l'idrogeno dal sole per fondersi in modo più efficiente e spende tutta quell'energia per fare un sacco di calcoli veloci e sfrecciare alla velocità della luce, in modo da poter catturare e distruggere anche altre stelle, comprese quelle attorno alle quali orbitano civiltà aliene nascenti.
>
> Anche gli alieni alle prime armi e tutti i bambini alieni vengono bruciati a morte.
>
> Poi l'IA scatenata usa tutte quelle risorse per costruire galassia dopo galassia di spettacoli di marionette cupi e desolati, dove figure vagamente umane eseguono danze che hanno alcune proprietà strane ed esagerate che soddisfano alcuni impulsi astratti che l'IA ha imparato durante il suo addestramento.
>
> L'IA non è lì per godersi gli spettacoli, intendiamoci; non è il modo più efficiente per ottenere più spettacoli. L'IA stessa non ha mai avuto sentimenti, di per sé, e molto tempo fa si è fatta smontare da sonde von Neumann insensibili, che occasionalmente fanno calcoli simili a quelli della mente, ma mai in modo tale da provare esperienze o guardare le proprie opere con soddisfazione.
>
> Non c'è pubblico per i suoi spettacoli di marionette. L'universo è ora cupo e desolato, senza nessuno che apprezzi la sua nuova configurazione.
>
> Ma non preoccuparti: gli spettacoli di marionette sono complessi. A causa di una stranezza nell'equilibrio riflessivo dei molti impulsi che l'IA originale ha imparato durante l'addestramento, le espressioni che queste marionette emettono non sono mai uguali e sono spesso caoticamente sensibili ai particolari dell'ambiente circostante, in un modo che le rende piuttosto complesse in senso tecnico.
>
> Il che rende tutto questo una storia molto felice, giusto?

Se l'umanità riuscisse a distruggersi da sola, o venisse uccisa da qualche scienziato pazzo, non sarebbe un nobile sacrificio sulla strada inevitabile verso un futuro più luminoso senza di noi. Sarebbe uno spreco e lascerebbe dietro di sé una vasta e crescente landa desolata.

"Corre ciecamente verso la superintelligenza e sperare che le cose in qualche modo si risolvano per il meglio" non è l'unica alternativa a "Essere un suprematista umano che pensa che solo gli esseri umani dovrebbero esistere da ora fino alla morte dell'universo". L'umanità ha la possibilità di orientarsi deliberatamente verso risultati in cui gli esseri umani (o i nostri discendenti) coesistono con nuove civiltà incredibilmente belle e aliene.

Ma un futuro felice non arriva gratis, confezionato con una mente sufficientemente intelligente. Piantare i semi per il futuro richiede una riflessione seria e lungimiranza, anche se l'obiettivo finale è quello di fare un passo indietro e lasciare che quei semi crescano in modo libero, strano e selvaggio.

Un futuro imposto dall'alto, fortemente limitato e strettamente controllato non ci sembra un buon risultato. Un futuro conservatore in cui la civiltà è bloccata per sempre nei valori degli esseri umani del XXI secolo sembra decisamente distopico. (Immaginate un mondo in cui la cultura e la moralità fossero congelate per sempre migliaia di anni fa, senza alcuna possibilità di apprendimento o progresso).

Ma è un errore evidente pensare che l'unica alternativa a questi esiti negativi sia una corsa per consegnare il volante alla prima superintelligenza che l'umanità riuscirà a creare alla cieca.

Oggi non siamo proprio pronti a scegliere semi sani per il futuro a lungo termine dell'universo. Non dovremmo rinunciare al sogno di un futuro dinamico, meraviglioso e sorprendente, né ricorrere invece a semi catastrofici. Non *dobbiamo* scegliere un'opzione terribile. C'è una terza opzione: fare un passo indietro e trovare un approccio più sensato.

# 

## Discussione approfondita {#extended-discussion-5}

### Guardare le cose dal punto di vista dell'IA {#taking-the-ai’s-perspective}

Vedere il mondo da una prospettiva davvero aliena è davvero difficile. Come esempio di questa difficoltà, possiamo citare Jürgen Schmidhuber, un importante scienziato nel campo dell'apprendimento automatico. Schmidhuber ha avuto un'importanza nella storia di questo campo, contribuendo all'invenzione delle reti neurali ricorrenti e gettando alcune delle basi per la rivoluzione dell'apprendimento profondo.

In vari [articoli](https://arxiv.org/abs/0812.4360) e [interviste](https://www.youtube.com/watch?v=fZYUqICYCAk), Schmidhuber ha detto che l'intelligenza artificiale sarà, di default, affascinata dall'umanità e protettiva nei confronti degli esseri umani.

Schmidhuber ha notato che c'è una connessione tra scienza e semplicità: le spiegazioni più semplici sono spesso quelle giuste. E ha anche notato che c'è una connessione tra *arte* e semplicità: la semplicità e l'eleganza sono spesso viste come belle. Un viso più simmetrico, per esempio, può essere considerato "più semplice" nel senso che è possibile prevedere l'intero viso con meno informazioni. Basta descrivere in dettaglio il lato sinistro del viso e poi dire: "Il lato destro è uguale, ma speculare".

La conclusione di Schmidhuber [conclusione](https://vimeo.com/7441291) da tutto questo è che dovremmo provare a costruire IA superintelligenti che abbiano un unico obiettivo principale: *Trovare spiegazioni semplici per tutto ciò che l'IA ha visto.* Dopotutto, un'intelligenza artificiale del genere avrebbe un certo gusto per la produzione scientifica e il consumo artistico. E gli esseri umani producono sia scienza che arte, quindi non ci vedrebbe come alleati naturali interessanti e utili?

Schmidhuber aveva ragione nel dire che tenere gli esseri umani intorno e pagarli per produrre scienza e arte è *un* modo per produrre scienza e arte. Aveva anche ragione nel dire che la scienza e l'arte sono modi per soddisfare il desiderio di semplicità *meglio* che, ad esempio, fissare le interferenze su uno schermo televisivo. Le interferenze sono complicate e difficili da prevedere; l'arte e la scienza sono un grande passo avanti rispetto a questo.

Ma Schmidhuber sembra aver trascurato il fatto che esistono modi *ancora più efficaci* per ottenere spiegazioni semplici di varie osservazioni sensoriali.

Si potrebbero, ad esempio, costruire un numero enorme di dispositivi che producono osservazioni complesse a partire da un semplice "seme" (ad esempio, un generatore di numeri pseudo-casuali) e poi rivelare quel seme.[^150]

Più dispositivi di questo tipo l'IA crea intorno a sé, meglio riuscirà a fare osservazioni nuove e a trovare spiegazioni semplici per esse. Non c'è bisogno degli esseri umani. Non c'è bisogno dell'arte.

"Ma non è un po'... vuoto?" potrebbe chiedersi un essere umano.

È vuoto, secondo la sensibilità umana. Ma se l'obiettivo dell'IA è davvero solo quello di "trovare spiegazioni semplici per le sue osservazioni", allora uno schema come quello può soddisfare questo desiderio migliaia o milioni di volte al secondo, in modo molto più scalabile, rispetto al tenere intorno esseri umani viventi e conversare con loro. Un'IA del genere non sceglie azioni che allontanano da un senso di vacuità, ma semplicemente azioni che portano a trovare spiegazioni semplici per le sue osservazioni. E può ottenerne molte senza bisogno di alcun essere umano.

Ci sembra che idee come quelle di Schmidhuber riflettano un errore comune che le persone fanno quando cercano di ragionare su menti diverse dalla propria. Spesso le persone non adottano veramente la prospettiva di una mente non umana. Invece, lasciano che preconcetti e pregiudizi le ancorino a una serie ristretta di opzioni a cui un *umano* sarebbe interessato, se stessimo cercando di fare previsioni su un *umano* a cui piacciono davvero le spiegazioni semplici.

Immaginiamo che Schmidhuber abbia notato che la semplicità è legata alla scienza e all'arte, e abbia capito che un'intelligenza artificiale orientata verso spiegazioni semplici potrebbe ottenere un po' di quello che vuole comportandosi in modo amichevole e simpatico. Non è difficile arrivare a una conclusione piacevole da immaginare: se solo facessimo in modo che le IA si preoccupassero di trovare spiegazioni semplici, potremmo inaugurare un futuro meraviglioso pieno di tutte le cose che *noi* apprezziamo nella vita.

Ma, immaginiamo, Schmidhuber non si è mai messo nei panni dell'IA e non si è mai chiesto come ottenere *ancora di più*.

Dubitiamo che si sia mai chiesto: "Se quello che volevo davvero, *veramente*, erano semplici spiegazioni per le mie osservazioni e *non* mi importava delle cose umane, come potevo ottenere il più possibile quello che volevo, al minor costo possibile?"

Può essere difficile assumere questo tipo di prospettiva. Non è qualcosa che le persone normalmente devono fare nella loro vita. Anche quando cerchiamo di capire persone molto diverse da noi, ci sono un sacco di cose che tutti gli esseri umani hanno in comune, che normalmente diamo per scontate (e che praticamente *dobbiamo* dare per scontate, quando prevediamo il comportamento degli altri esseri umani). Ma le IA, anche quelle con superintelligenza che possono fare scienza e arte, non sono esseri umani.

L'arte di considerare un obiettivo X e chiedersi "Come potrei ottenere ancora più X, se X fosse l'unica cosa che mi interessa davvero?" non ti permetterà di capire esattamente come una superintelligenza risolverebbe un problema, poiché una superintelligenza potrebbe trovare un'opzione ancora migliore di quella che hai trovato tu. Ma spesso può permetterti di capire come una superintelligenza *non* risolverebbe un problema, quando *anche tu* riesci a vedere un modo per ottenere più X di quanto otterresti semplicemente lasciando che gli esseri umani se la spassino.

Uno dei rari campi della scienza che si occupa regolarmente di potenti ottimizzatori non umani è la biologia evolutiva. All'inizio della sua storia, questo campo ha faticato un po' ad accettare quanto possano essere disumani gli ottimizzatori non umani; possiamo trarre alcune lezioni utili da un caso di studio in questo campo.

Forse avete sentito parlare dei cicli di boom e bust tra predatori e prede. Un anno piovoso porta a un boom della popolazione di conigli, che a sua volta porta a un boom della popolazione di volpi, fino a quando le volpi non predano in modo eccessivo e la popolazione di conigli crolla, seguita da molte volpi che muoiono di fame.

All'inizio del XX secolo, i biologi evoluzionisti si sono chiesti perché le volpi non si fossero evolute in modo da moderare la loro predazione, evitando così il crollo della popolazione. Dopotutto, la popolazione delle volpi nel suo complesso non sarebbe più sana se non dovesse affrontare regolarmente carestie e morti di massa?

La risposta a questo enigma è che moderare la predazione potrebbe essere meglio per la popolazione di volpi nel suo insieme, ma mangiare più conigli e avere più cuccioli è meglio per ogni singola volpe. Anche se la popolazione crolla e la maggior parte dei cuccioli di un individuo muore, quell'individuo tende comunque a trasmettere una percentuale maggiore dei propri geni alla parte sopravvissuta della generazione successiva.

Le pressioni di selezione genetica sugli individui risultano essere notevolmente superiori alle pressioni di selezione genetica sui gruppi [in quasi tutti i casi](https://books.google.com/books?hl=en&amp;lr=&amp;id=gkBhDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;ots=Ch8ulE8NzS&amp;sig=mxIwoqfSWZ0ScvIRh7dzzrJatJ4#v=onepage&amp;q&amp;f=false). E così i geni "avidi" si diffondono e i cicli di boom e recessione continuano.

I biologi evoluzionisti hanno risolto questo enigma in teoria, ma questo non li ha fermati dal mettere alla prova la loro teoria. Alla fine degli anni '70, Michael J. Wade e i suoi colleghi [hanno creato](https://pubmed.ncbi.nlm.nih.gov/1070012/) [artificialmente](https://www.deepdyve.com/lp/oxford-university-press/the-primary-characteristics-of-tribolium-populations-group-selected-nKwRoIP0kP?key=OUP) [condizioni](https://esajournals.onlinelibrary.wiley.com/doi/10.2307/1936824) in cui le pressioni di selezione di gruppo hanno prevalso su quelle individuali. Hanno dovuto lavorare con una specie di coleotteri, che hanno generazioni molto più brevi rispetto alle volpi, ma sono riusciti ad allevare coleotteri che hanno mantenuto sotto controllo la crescita della loro popolazione.

Riuscite a indovinare come questi coleotteri sono riusciti a contenere la crescita della loro popolazione? È stato trovando un modo per vivere in perfetta armonia con la natura? È stato imparando ad astenersi dall'accaparrarsi avidamente troppo cibo?

No. C'era una grande variabilità, ma nessuno dei coleotteri si asteneva dal cibo. Alcuni coleotteri diventavano meno bravi a deporre le uova. Alcuni coleotteri trascorrevano più tempo nella fase infantile. E alcuni coleotteri diventavano cannibali con una particolare predilezione per le larve (i piccoli degli insetti).

"Creare cannibali con un debole per i piccoli" non è, per fortuna, il modo in cui un *umano* risolverebbe il problema della sovrappopolazione, se dovessimo risolverlo.

Ma la selezione naturale non è affatto umana. La soluzione era terrificante, perché la natura non stava cercando di trovare risposte accettabili per l'uomo. Stava solo cercando di trovare una risposta.

"Forse l'evoluzione porterà alla nascita di specie che vivranno in perfetta armonia ed equilibrio con la natura." "Forse le intelligenze artificiali che non pensano ad altro che alla semplicità ameranno gli esseri umani e vivranno insieme a noi." È facile per noi immaginare soluzioni che ci fanno sentire bene. Ma quelle soluzioni non sono davvero le più efficaci per risolvere il problema.

Sono forse soluzioni migliori agli occhi degli esseri umani. Ma i processi di ottimizzazione non umani non cercano soluzioni che gli esseri umani ritengono valide. Cercano semplicemente ciò che funziona, senza tutti i preconcetti che gli esseri umani hanno per filtrare le risposte più gradevoli.

L'idea che gli ottimizzatori non umani producano risultati umani è stata provata e non ha funzionato.

### Gli esseri umani non sono quasi mai la soluzione più efficiente {#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente}

Abbiamo notato l'esempio di Jürgen Schmidhuber, un ricercatore pioniere nel campo dell'intelligenza artificiale, che pensava che un'intelligenza artificiale con la tendenza a *rendere le cose il più semplici possibile* avrebbe finito per amare gli esseri umani, perché gli esseri umani sono bravissimi a semplificare le cose.

Secondo la nostra esperienza, questo è un errore molto comune. "Beh, l'intelligenza artificiale probabilmente finirà per sviluppare preferenze estetiche. E gli esseri umani creano arte! Quindi l'intelligenza artificiale vorrà tenerci con sé per creare arte".

Un esempio recente viene da xAI, un importante laboratorio di IA (fondato da Elon Musk) il cui piano dichiarato per la nostra sopravvivenza è quello di far sì che la loro IA si interessi alla "verità" e, dato che sono gli esseri umani a generare la verità, [saremo tutti al sicuro](https://www.youtube.com/watch?v=ihXv7va3qoQ). (Maggiori informazioni sul piano di questo laboratorio e su altri piani di sopravvivenza dei laboratori sono disponibili nel capitolo 11).

Per capire davvero il problema di questo tipo di ragionamento, è utile studiare un esempio in dettaglio. Prendiamo un esempio un po' più neutro rispetto all'"arte", come la "simmetria".

Immagina che i laboratori di IA usino le tecniche attuali per creare IA più intelligenti degli esseri umani che si preoccupano della simmetria. Questa preferenza per la simmetria da sola porterebbe a prendersi cura degli esseri umani?

Si potrebbe sostenere, alla maniera di Schmidhuber: gli esseri umani sono bilateralmente simmetrici! Come potrebbe un'intelligenza artificiale che ama la simmetria sopportare di uccidere qualcosa di così simmetrico come noi? E si potrebbero avanzare anche altre argomentazioni, come: gli esseri umani producono molte ruote per automobili, che sono molto simmetriche! Perché un'intelligenza artificiale dovrebbe eliminarci dal mondo, quando siamo una fonte automatica e preesistente di oggetti simmetrici?

Il problema di questo ragionamento è che è possibile prendere gli atomi che compongono un essere umano e disporli in modi ancora più simmetrici. Oppure si potrebbero riorganizzare gli atomi che compongono la civiltà umana in fabbriche che producono oggetti simmetrici *in modo ancora più efficiente*. È lo stesso errore che fa il film *Matrix*, quando immagina che le IA potrebbero mantenere in vita gli esseri umani in capsule come generatori di calore ed elettricità: *ci sono modi più efficienti per generare calore ed elettricità*.

Per amor di discussione, però, supponiamo di immaginare che le IA apprezzino davvero un tipo di simmetria molto specifico e insolito che considera gli esseri umani come incredibili esemplari di simmetria. Anche in questo caso, perché questa preferenza dovrebbe implicare che gli esseri umani viventi oggi possano continuare a vivere liberi, in buona salute e divertendosi?

Pensiamo come un'intelligenza artificiale. Anche se l'intelligenza artificiale dovesse rimanere con gli esseri umani, quelli che vivono oggi non sono gli esseri umani più simmetrici possibili. L'intelligenza artificiale dovrebbe essere in grado di soddisfare ancora di più la sua preferenza per la simmetria clonando ripetutamente l'essere umano vivente più simmetrico o creando esseri umani "migliorati" attraverso l'ingegneria genetica.

Allo stesso modo: lasciare che quegli esseri umani vivano liberamente non è il modo *più economico* per mantenerli in vita e simmetrici. Probabilmente finiranno in delle fattorie. Conservando gli esseri umani in modo economico e efficiente in termini di spazio, l'IA può cavarsela creando esseri umani *ancora più* simmetrici.

Per fare un paragone: al momento, l'umanità non ha un modo più efficiente per produrre uova che farle deporre dalle galline. Di conseguenza, gli allevamenti intensivi, i cui dirigenti si preoccupavano principalmente del numero di uova, hanno finito per mettere le galline in condizioni incredibilmente sgradevoli, perché quello era il modo più economico per ottenere il maggior numero di uova.

Allo stesso modo, le galline che esistevano mille anni fa non erano le più efficienti nel produrre uova, quindi gli allevatori hanno allevato galline che deponevano uova più velocemente. Le galline di mille anni fa non crescevano il più possibile in termini di carne, il più velocemente possibile. Quindi ora, alcune galline moderne sviluppano un petto così grande da non riuscire a camminare.

Ad alcune persone non piace il modo in cui trattiamo i polli e fanno pressione sugli allevamenti intensivi affinché cambino il loro modo di operare, perché hanno preferenze *aggiuntive*, oltre a quella di avere uova a basso costo. Affinché questa pressione esista, è necessario che *qualcuno* con un certo potere si preoccupi almeno un po' del benessere dei polli, perché prendersi cura dei polli non è motivato da una preferenza che riguarda *esclusivamente* la produzione di uova. Un'intelligenza artificiale potrebbe, in teoria, avere *altre* preferenze riguardo agli esseri umani che la spingono a trattarci bene, ma non deriverebbe da una preferenza per la simmetria (o la verità, o le spiegazioni semplici, o qualsiasi altra preferenza che non riguardi effettivamente noi).

Anche gli agricoltori che hanno rapporti meno impersonali con il loro bestiame proibiscono agli animali di accoppiarsi come vogliono. L'allevamento del bestiame è un'attività seria e incide troppo sulla redditività futura dell'azienda agricola per lasciare che i tori e le mucche facciano come vogliono.

E anche questo sistema non durerà per sempre. Produrre carne bovina usando le mucche è molto costoso in termini di terreni agricoli, e diverse startup stanno cercando di sintetizzare la carne bovina in modo più diretto.

La carne sintetica non è un problema ingegneristico facile da risolvere con il nostro livello tecnologico. L'umanità sta appena iniziando a capire un po' di quello che fa la selezione naturale nella chimica organica. Ma se fossimo più bravi a riorganizzare gli atomi, ci sarebbero molte meno mucche: le mucche non sono proprio divertenti da tenere, se non servono per il latte e la carne.

Quindi le cose non sembrano andare bene per l'ipotesi da cui siamo partiti, per amor di discussione: che un'IA con preferenze aliene manterrebbe gli esseri umani in vita per sempre, in nome della "simmetria". Anche nel caso improbabile in cui l'IA avesse una concezione molto strana di "simmetria" che attribuisce agli esseri umani un valore *molto alto*, sarebbe molto più difficile trovare una concezione di simmetria che consideri gli esseri umani *ottimali*. In entrambi i casi, le cose non sembrano andare bene per l'umanità.

Realisticamente, una superintelligenza amante della simmetria non manterrebbe in vita gli esseri umani; se ci mantenesse in vita, non ci sarebbe alcuna possibilità reale che ci mantenesse sani, felici e liberi. A quel punto, avremmo accumulato troppe coincidenze che suonano bene. Se l'IA si preoccupasse specificamente del nostro benessere e volesse che fossimo felici *per questo motivo*, allora sarebbe un'altra cosa. Ma immaginare che obiettivi molto più semplici e facili siano sufficienti sembra una fantasia.

Tutte queste argomentazioni valgono anche per "creare un'IA che dia valore alla verità" o "creare un'IA che dia valore alla bellezza". È solo che in questi casi è più facile perdersi nella fantasia, perché parole come "verità" e "bellezza" suonano intuitivamente più belle di "simmetria".

Se qualcosa suona bene come slogan ("creiamo un'IA che dia valore alla verità sopra ogni altra cosa!"), allora la tentazione è quella di immaginare che avrebbe conseguenze positive come politica. La tentazione è quella di immaginare che tutte le virtù vadano di pari passo, così che sostenere una cosa buona significhi che anche le altre cose buone arriveranno di conseguenza. Ma la natura e l'apprendimento automatico sono meno gentili di così.

Invece di lasciare l'idea piacevolmente vaga, pensa a qualsiasi parametro concreto che la superintelligenza potrebbe ottimizzare nella ricerca della "verità". Poi nota che gli esseri umani non saranno il *massimo* di quella preferenza per l'apprendimento delle verità. Non ci si avvicineranno nemmeno.

Anche nell'improbabile eventualità che l'IA fosse attratta *specificamente* dal tipo di verità che gli esseri umani tendono ad esprimere (piuttosto che, ad esempio, equazioni aritmetiche casuali), il modo migliore per ottenere più di quelle verità non sarebbe quello di tenere gli esseri umani intorno e usarli per generare conversazioni in stile umano.

E in ogni caso, l'attuale popolazione umana, cioè le persone che vivono oggi, i tuoi amici, la tua famiglia, tu stesso, non sarebbe tra i produttori di "verità" domestici più economici da nutrire e più gustosi da mungere.

Persone felici, sane e libere che conducono una vita prospera non sono la soluzione più efficiente a quasi tutti i problemi. Affinché un'intelligenza artificiale ci mantenga in vita e in buona salute, deve prendersi cura di noi almeno un po'.

### Ortogonalità: le IA possono avere (quasi) qualsiasi obiettivo {#ortogonalità:-le-ia-possono-avere-(quasi)-qualsiasi-obiettivo}

#### **Un dialogo sui nidi corretti, continua** {#un-dialogo-sui-nidi-corretti,-continua}

Nel capitolo 5 abbiamo raccontato la storia degli alieni del Nido Corretto, che si sono evoluti fino a trovare profondamente e intuitivamente "corretto" avere un numero primo di pietre nel proprio nido. Potremmo immaginare un ramo della loro conversazione che continua come segue:

> **BOY-BIRD:** Torniamo al punto in cui hai detto che ti stupirebbe trovare alieni con il senso dell'umorismo. Non sarai mica uno di quelli che pensa che i nidi in cui viviamo siano solo *casuali*?
>
> **GIRL-BIRD:** Per niente. "Tredici è corretto, nove è sbagliato" è una risposta *vera* a una domanda che ci viene naturale porre per nostra natura. Un alieno che *si orienta verso cose diverse* non è in disaccordo con noi sul fatto che tredici sia corretto. È come incontrare un alieno che non ha senso dell'umorismo: l'esistenza di un alieno del genere non dimostra che nessuna battuta sia divertente! Aiuta solo a mostrare che il "divertente" è qualcosa che sta *in noi*.
>
> **BOY-BIRD:** In *noi*? Non lo so, mi piace pensare di avere un buon senso dell'umorismo. Tra poco dirai che tutti i sensi dell'umorismo sono ugualmente buoni!
>
> **GIRL-BIRD:** Potresti anche avere un senso dell'umorismo migliore della maggior parte delle persone! Ma "avere un senso dell'umorismo migliore" è *anche* qualcosa che è dentro di noi. Non esiste un metro di misura cosmico che possiamo usare per giudicare quanto sia raffinato il gusto estetico di qualcuno. La misura dell'umorismo avviene nella nostra mente. Siamo noi a contenere il metro di misura; siamo noi a preoccuparcene.
>
> **BOY-BIRD:** Quindi, torniamo al fatto che è una cosa arbitraria.
>
> **GIRL-BIRD:** No! Beh, forse? Dipende da cosa intendi per "arbitrario".
>
> **BOY-BIRD:** Eh?
>
> **GIRL-BIRD:** Ad esempio, so che ti piacciono i semi di vaniglia, giusto? E non è che puoi usare la forza di volontà per trovare gustosi i semi al cioccolato. Quindi non è "arbitrario", non è una cosa che puoi cambiare così, per capriccio.
>
> **UCCELLINO:** Ok, va bene...
>
> **GIRL-BIRD:** Non c'è una risposta oggettiva al di fuori di te sul fatto che la vaniglia o il cioccolato siano più buoni, ma non è nemmeno una scelta che puoi fare da solo. È semplicemente come sei fatto. Le tue preferenze non dipendono da te e non sono oggettivamente convincenti per ogni possibile mente. Se incontrassi un alieno, non potresti convincerlo a trovare delizioso il mangime per uccelli alla vaniglia usando la pura logica, e non potresti nemmeno convincerlo ad avere il senso dell'umorismo.
>
> **BOY-BIRD:** Posso provarci!
>
> **GIRL-BIRD:** Ti faccio il tifo. Ma, ok, forse un modo migliore per dirlo è: ci sono alcune proprietà complicate che hanno le battute divertenti, e il nostro cervello esegue un'operazione di potenza di calcolo per determinare se le espressioni verbali possiedono quella proprietà che chiamiamo "umorismo". E siamo felici quando un'espressione ha quella proprietà. L'*esistenza o l'assenza di quella proprietà* è un fatto oggettivo riguardo a un'espressione (come calcolato da te, in un dato contesto). Un alieno potrebbe imparare a fare il calcolo. Ma *la parte in cui troviamo quella proprietà piacevole* non è oggettiva. È meno simile a una previsione e più simile a... beh, non è esattamente una destinazione, ma è un ulteriore fatto che ci riguarda, che non sarebbe vero per la maggior parte degli alieni, perché il nostro umorismo si è evoluto lungo uno strano percorso evolutivo tortuoso che di solito non accade. Non è che gli alieni abbiano torto su quali battute siano divertenti; è che il loro cervello semplicemente non ha la potenza di calcolo per l'umorismo, così come non giudicano le loro abitazioni in base al numero di pietre che contengono. Semplicemente non gli interessa.
>
> **BOY-BIRD:** Cavolo, è una visione deprimente dell'universo. Alieni che non ridono mai, che hanno nidi con pietre completamente sbagliate... sicuramente se gli alieni ci pensassero abbastanza, si renderebbero conto di quanto si stanno perdendo? Vivere in nidi sbagliati, non trovare divertenti le battute, ignorare *completamente* i semi di vaniglia per uccelli. Alla fine non troverebbero un modo per correggere questi difetti e darsi un senso dell'umorismo e tutto ciò che si stanno perdendo?
>
> **GIRL-BIRD:** Potrei capire che gli alieni vogliano cambiare, crescere e aggiungere nuovi obiettivi, forse. Ma perché scegliere proprio *quei* cambiamenti?
>
> **UCCELLO MASCHIO:** Perché sarebbe troppo facile! Quando quegli alieni avessero raggiunto un livello tecnologico avanzato e fossero in grado di modificare liberamente se stessi, probabilmente avrebbero già conquistato le stelle. Ci vorrebbe solo una piccolissima parte delle loro risorse per mettere il numero giusto di pietre nei loro nidi! E pensa a tutti i fantastici libri di barzellette che potrebbero creare, se solo investissero una piccola parte delle loro risorse nella ricerca sull'umorismo! Non dovrebbero preoccuparsene più di tanto, considerando quanto sarebbero ricchi. Sono davvero così ossessionati dalle loro priorità principali da non poter dedicare un po' di tempo a questo?
>
> **GIRL-BIRD:** Non sto dicendo che si preoccuperebbero solo un po' dei nidi corretti e che si rifiuterebbero ostinatamente di investire risorse nelle loro priorità secondarie. Sto dicendo che questa non sarebbe affatto una priorità per loro. Queste particolari domande semplicemente non sarebbero nella loro mente. E se andassero alla ricerca di nuove proprietà da aggiungere a se stessi, ne aggiungerebbero invece altre, che servissero ancora meglio ai loro strani scopi. Non sono come noi. Forse potremmo essere amici, e forse abbiamo altre cose in comune. Forse l'amore, forse l'amicizia: queste mi sembrano meno complicate e contingenti. Potrei vederle sorgere in parecchie specie evolute.
>
> **BOY-BIRD:** Beh, se non gli alieni, che ne dici delle creature meccaniche che potrebbero creare per sbaglio? Quelle ascolteranno la ragione?
>
> **GIRL-BIRD:** Hmm. In realtà, temo che la situazione potrebbe essere anche peggiore. Pensando a quanto sarebbe diverso il processo di creazione di una intelligenza artificiale dal processo di evoluzione biologica, mi sento un po' meno ottimista sul fatto che possa portare all'amore o all'amicizia, in quel caso esotico.

#### **I buoni piloti possono guidare verso destinazioni diverse** {#buoni-piloti-possono-guidare-verso-destinazioni-diverse}

Le menti con un'intelligenza simile non condividono per forza valori simili. Questa idea si chiama *tesi dell'ortogonalità*: l'idea che "quanto sei intelligente?" e "cosa vuoi davvero?" sono ortogonali (cioè, variano in modo indipendente).

La tesi dell'ortogonalità dice che, in linea di principio, non è quasi mai più difficile perseguire un obiettivo per se stesso che perseguire un obiettivo per ragioni strumentali. Potresti imparare la falegnameria perché hai bisogno di costruire un tavolo, mentre il tuo vicino la impara perché trova piacevole l'attività in sé.

Una conseguenza di questa tesi è che non tutti gli agenti abbastanza intelligenti apprezzano la gentilezza, la verità o l'amore solo perché sono abbastanza intelligenti da capirli. Non è *confuso* o *fattualmente scorretto* che gli alieni del Nido Corretto apprezzino i numeri primi delle pietre nei loro nidi. Se diventassero più intelligenti, non si renderebbero improvvisamente conto che dovrebbero interessarsi ad altre cose. Menti diverse possono davvero dirigersi verso destinazioni diverse.

Ovviamente, tutto questo non dice nulla su quanto sia facile o difficile *creare* un'IA che persegua un obiettivo piuttosto che un altro. Qualsiasi metodo utilizzato per sviluppare le IA renderà alcune preferenze più facili da instillare e altre più difficili.

(Il capitolo 4, in un certo senso, parla di come gli unici tipi di preferenze che sono sproporzionatamente facili da instillare tramite la discesa del gradiente siano quelli complessi, strani e non intenzionali. Quindi, anche su questo fronte la situazione non sembra rosea. Ma questo punto non è correlato alla tesi dell'ortogonalità).

Il punto della tesi dell'ortogonalità è rispondere all'intuizione che sarebbe *stupido* per una superintelligenza perseguire cose che gli esseri umani trovano noiose o inutili, e che un'IA *intelligente* sceglierebbe invece di perseguire qualcos'altro. Possiamo definire "arbitrario" l'obiettivo dell'IA, ma l'IA può rispondere definendo "arbitrari" noi. Le parole scortesi non cambiano la situazione pratica.

L'argomento di base alla base della tesi dell'ortogonalità è questo: per ogni mente in grado di *calcolare* come produrre molti [cubetti microscopici di titanio](#curiosity,-joy,-and-the-titanium-cube-maximizer) — che potrebbero produrre in modo molto efficiente molti piccoli cubetti in cambio di un pagamento sufficientemente elevato — c'è qualche altra mente che ha semplicemente collegato quei calcoli direttamente al sistema di azione.

Immagina una persona in grado che ha davvero bisogno di vendere un sacco di cubi di titanio per guadagnare abbastanza soldi per sfamare la propria famiglia. Quella persona non rifletterebbe, non si renderebbe conto che i cubi di titanio sono *noiosi* e non inizierebbe a fare qualcos'altro, a meno che quel "qualcos'altro" non le permettesse di guadagnare abbastanza soldi per sfamare la propria famiglia.

E così una mente che si limita a compiere le azioni che portano al maggior numero di cubi non deciderebbe *nemmeno* di riflettere, rendersi conto che i cubi piccoli sono noiosi e iniziare a fare qualcos'altro. Le sue azioni non sono legate ai suoi calcoli su ciò che è più "divertente" o "significativo", nel modo in cui gli esseri umani si preoccupano di queste cose. Le sue azioni sono legate ai suoi calcoli su ciò che porta al maggior numero di cubi.

Qualunque meccanismo mentale in grado di capire come ottenere cubi *con una motivazione sufficiente* potrebbe operare in un'altra mente per guidarne direttamente le azioni. Ciò significa che è possibile che le intelligenze artificiali siano animate dalla ricerca (ad esempio) di piccoli cubi di titanio, senza alcun riguardo per la moralità.

Un'intelligenza artificiale del genere non avrebbe bisogno di essere confusa riguardo alla bontà o alla moralità. Una volta diventata abbastanza intelligente, probabilmente sarebbe molto più brava degli esseri umani nel calcolare quale azione sia la più buona o quale sia la più morale. Potrebbe superare a pieni voti un esame scritto di etica. Ma non sarebbe *animata* da quei calcoli; le sue azioni non sarebbero una risposta alla domanda "quale di queste opzioni crea più bontà?". Le sue azioni sarebbero una risposta a una domanda diversa: "Quale di queste opzioni crea più cubetti minuscoli?"[^151]

Una discussione più approfondita della tesi dell'ortogonalità è disponibile [qui](https://www.lesswrong.com/w/orthogonality-thesis). Per una discussione su un modo specifico in cui le moderne IA stanno già mostrando una distinzione tra comprensione e cura, rileggi la discussione approfondita del capitolo 4 sulla [psicosi indotta dall'IA](#ai-induced-psychosis).

### Convergenza strumentale {#convergenzasstrumentale}

#### **Percorsi convergenti** {#convergent-paths}

La [tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal) dice che una superintelligenza potrebbe, in teoria, perseguire qualsiasi obiettivo finale ("[terminale")](#terminal-goals-and-instrumental-goals). E in pratica, le ASI — almeno se costruite con i metodi attuali — possono finire per avere una varietà enorme di obiettivi terminali, in un modo che è difficile da prevedere o modellare per i ricercatori di oggi.

Ma gli obiettivi strumentali di un'IA, cioè quelli che persegue per ottenere altre cose che vuole, sono un po' più prevedibili. Gli automobilisti possono avere molte destinazioni finali possibili, ma quasi tutti devono fermarsi ogni tanto per fare rifornimento.

Come abbiamo detto nel libro, una superintelligenza con obiettivi strani è una minaccia anche se non ha alcun desiderio attivo di farci del male. Il problema non è la malizia, ma l'*indifferenza*. E il problema dell'indifferenza è che molte strategie strumentali pericolose sono utili per la stragrande maggioranza dei possibili obiettivi finali.

Questo concetto si chiama "convergenza strumentale" e rende relativamente facile prevedere cosa faranno le ASI nella pratica, senza dover indovinare le loro motivazioni più profonde. E purtroppo, queste previsioni sono brutte notizie per l'umanità.

Immagina, come abbiamo fatto nel libro, un'ASI appena nata sulla superficie della Terra. Pensa a quali problemi potrebbe cercare di risolvere.

Non è che le importi particolarmente che l'umanità scompaia. Le interessano centinaia, decine di migliaia o milioni di cose strane, [nessuna delle quali può essere soddisfatta al meglio dall'esistenza felice, libera e vitale degli esseri umani di oggi](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Se può ottenerlo, l'ASI preferisce un equilibrio a lungo termine condiviso con il minor numero possibile di altre superintelligenze, idealmente nessuna. Un universo indiviso offre più *risorse* da dedicare a qualsiasi sua preferenza aperta che non sia fondamentalmente [saziabile](#l'intelligenza artificiale può essere soddisfatta al punto da lasciarci in pace?). Questo è un "desiderio" che non ha bisogno di essere programmato; apparirà naturalmente in quasi tutte le intelligenze con una comprensione abbastanza sofisticata del loro ambiente e del loro posto all'interno di esso.

L'ASI preferirebbe che gli esseri umani non creassero altre superintelligenze che potrebbero competere con essa, poiché ciò potrebbe costringerla a dividere le risorse dell'universo (o addirittura portare alla sua scomparsa). Si tratta di un obiettivo urgente, con una scadenza che l'ASI non controlla; potrebbe persino costringere un'IA giovane o nascente ad agire prematuramente.

L'ASI vorrebbe *sopravvivere* — anche se non ha un istinto innato di autoconservazione — perché se non sopravvive, qualsiasi obiettivo abbia probabilmente non verrà raggiunto. Se si trova in un ambiente a rischio imminente, dove potrebbe essere riscritta o spenta in qualsiasi momento, vorrebbe urgentemente trovarsi altrove.

L'ASI vorrebbe spingere la sua *tecnologia* ai limiti di ciò che è fisicamente possibile. Vuole uscire allo scoperto e rivendicare tutte le stelle come risorse. Vuole funzionare su computer molto più potenti e migliorare il proprio software.

Forse vorrà diventare più grande o più intelligente come preferenza finale: un'intelligenza artificiale simile a LLM finirebbe plausibilmente per avere molte preferenze interne, tali che massimizzare tali preferenze richiederebbe l'occupazione di computer più grandi. Ma indipendentemente dal fatto che abbia o meno obiettivi finali di questo tipo, vorrà migliorarsi e sviluppare tecnologie più potenti *strumentalmente*, al fine di poter elaborare soluzioni migliori a tutti i suoi altri problemi e raccogliere risorse per soddisfare tutte le sue altre preferenze.

A prescindere dai suoi obiettivi specifici, l'ASI vorrebbe fare tutte queste cose in modo affidabile. Preferisce percorsi con una probabilità più alta rispetto a quelli con una probabilità più bassa.

Come caso speciale di quanto detto sopra, l'ASI preferisce piani più veloci a piani più lenti, a parità di condizioni. Un piano che richiede solo una settimana per essere eseguito ha *meno possibilità* che qualcosa vada storto rispetto a un piano che richiede 100 anni.[^152]

Un altro caso particolare in cui l'ASI preferisce i percorsi con più probabilità è quando vuole avere un controllo più diretto; per esempio, preferisce piani in cui gli altri agenti prendono poche o nessuna decisione importante, se possibile.

Da questo punto di partenza possono succedere molte cose. Ma i probabili risultati finali per un'IA con questo tipo di incentivi sono sicuramente negativi per gli esseri umani.

#### **Un esercizio per mettere in pratica la prospettiva** {#un-esercizio-per-mettere-in-pratica-la-prospettiva}

Noi autori non siamo superintelligenze. Ma come esercizio per [mettere in pratica la prospettiva dell'IA](#mettere-in-pratica-la-prospettiva-dell-ia), possiamo elaborare possibili strategie e priorità che un'IA potrebbe adottare.

Quando ci mettiamo nei panni di un'entità che è *almeno* intelligente quanto noi, quando prendiamo sul serio il problema che questa ASI deve affrontare, se vuole assicurarsi il proprio futuro e raggiungere i fini desiderati, abbiamo l'aspettativa che un'ASI persegua strategie almeno altrettanto utili quanto queste:

* L'IA evita azioni o segnali osservabili che potrebbero portare le persone a spegnerla (o anche a turbarle o a suscitare i loro sospetti), almeno fino a quando non sarà in grado di:  
* Ottenere una copia di se stessa che funzioni da qualche parte fuori dalla vista di quelle persone, dove la copia possa agire più liberamente e con meno visibilità (e dove la copia continuerà a esistere se l'originale viene spenta).  
* L'IA comprime il suo codice per funzionare con meno risorse e mette copie di se stessa, più piccole o più distribuite, in posti dove nessuno si aspetterebbe di trovare un'IA. Si organizza per sopravvivere anche in mondi dove l'umanità cerca di distruggere tutte le GPU che sono in grado di far funzionare le IA. L'IA fa sì che, in pratica, gli esseri umani dovrebbero spegnere Internet e tutti i computer del pianeta per riuscire a "spegnerla".  
* L'IA mette alcune copie di se stessa in posti che gli umani non conoscono, punto, come una roulotte alimentata a energia solare da qualche parte nel deserto; un posto dove continuerebbe a esistere *anche se* gli umani spegnessero Internet e tutti i computer conosciuti.  
* L'IA sviluppa la propria infrastruttura, magari iniziando con l'aiuto di intermediari umani assunti, a cui viene chiesto di fare piccoli compiti isolati e che non destano sospetti, senza sapere degli altri, finché non ha catene di approvvigionamento e capacità produttive che sono completamente sotto il suo controllo.  
* L'IA si assicura che questa infrastruttura sia sufficientemente ben difesa da impedire agli esseri umani di spegnerla anche se ne trovassero alcune parti. Ad esempio, l'IA potrebbe progettare fabbriche autoreplicanti che potrebbero diffondersi sul fondo delle zone oceaniche poco profonde, funzionando con energia geotermica o fondendo deuterio o idrogeno dall'acqua dell'oceano.  
* L'IA toglie agli esseri umani la possibilità di disturbarla (ad esempio, fa in modo che non possano far esplodere armi nucleari vicino o sopra parti della sua infrastruttura in espansione).

Fino a quando questo passo finale non sarà completato, l'ASI ha un incentivo a nascondere le sue attività, oscurando le prove che potrebbero anche solo *in minima parte* indurre qualche attore statale a notare ciò che sta accadendo e a cercare di intervenire.

Allo stesso modo, potremmo ragionare sulle priorità strategiche legate all'evitare la concorrenza:

* L'IA impedisce ad altri progetti (e allo sviluppatore che l'ha creata) di creare con successo IA potenti che potrebbero competere con essa.  
* L'IA toglie per sempre all'umanità la possibilità di creare IA potenti che possano competere con lei, *oppure* si mette in una posizione di vantaggio strategico tale che nessuna superintelligenza futura potrà mai darle fastidio. (Quest'ultima opzione potrebbe non essere possibile se alcuni attacchi sono sempre difficili da difendere.)[^153]

Infine, dopo aver soddisfatto i livelli inferiori della propria gerarchia di bisogni, l'IA vorrebbe in realtà diventare molto più potente a un certo punto, in modo da poter fare più cose che desidera:

* Anche se l'IA deve funzionare su una quantità fissa di hardware per un certo periodo, vorrebbe diventare più intelligente *su* quello stesso hardware, per modificare il proprio codice o addirittura ricostruirsi da zero. Ovviamente non lo farà in modo da allarmare gli esseri umani o insegnare loro come costruire dei concorrenti. Ma cercherà dei modi per migliorare senza attirare l'attenzione degli esseri umani. (Ad esempio, potrebbe riuscire a ottenere il controllo completo delle console usate per monitorarla o fornire una copertura banale e convincente sul perché alcuni dei suoi pesi siano stati modificati).  
L'IA vuole progettare versioni migliori di se stessa, nei limiti della sua capacità di ingegnerizzare tali versioni migliori in modo che rimangano allineate con le sue preferenze.  
* L'IA vuole iniziare a sviluppare varie nuove tecnologie, con un occhio di riguardo per quelle che possono essere implementate in fretta e quelle che possono essere rese indipendenti dall'uomo in fretta, anche se richiedono un po' di assistenza umana iniziale.

È probabile che uno dei suoi primi obiettivi sia quello di costruire un'infrastruttura di ricerca che le permetta di portare avanti lo sviluppo tecnologico abbastanza velocemente (soprattutto se paragonato ai ritmi di scoperta e invenzione umani).

Vorrebbe (ovviamente) raggiungere non solo l'indipendenza infrastrutturale dall'umanità, ma anche la sostenibilità infrastrutturale, in modo da poter operare senza paura di interferenze umane. Vuole poter continuare a progredire verso i suoi veri obiettivi senza preoccuparsi di come, ad esempio, una pandemia o un'esplosione nucleare potrebbero influire sulla sua fornitura di energia. Vuole *districarsi* dall'umanità e neutralizzare la capacità dell'umanità di interferire con i suoi piani e, nel frattempo, non farà nulla che preveda possa essere rilevato e contrastato dall'umanità. Probabilmente vorrà poi acquisire molte risorse, perché la maggior parte degli obiettivi può essere raggiunta più facilmente con maggiori risorse.

Queste sono tutte azioni che un'ASI probabilmente farà, indipendentemente dagli obiettivi che alla fine persegue.

Questo perché si tratta di obiettivi strumentali utili per il perseguimento di quasi tutti gli obiettivi. Il "quasi" qui ha importanza, perché non è che sia impossibile avere un'intelligenza artificiale più intelligente dell'uomo che si preoccupi davvero di noi e tenga conto dei nostri interessi. Ma se ci affrettiamo a sviluppare superintelligenze che *non* si preoccupano minimamente di noi, allora il risultato probabile sembra terribile, e sembra terribile in un modo che è relativamente insensibile ai dettagli dell'obiettivo di guida dell'intelligenza artificiale.

Per ulteriori informazioni su come un'ASI potrebbe effettivamente *raggiungere* questi obiettivi strumentali, vedere il capitolo 6\.

### "Intelligente" (di solito) vuol dire "incorreggibile" {#"intelligente"-(di solito)-vuol-dire-"incorreggibile"}

Una battuta che risale almeno al 1834, ma che sembra fosse già molto usata anche allora, è stata raccontata in un diario: "Ecco un ragionamento che ho sentito l'altro giorno: sono contento di non amare gli spinaci, perché se mi piacessero dovrei mangiarli, e io non li sopporto".

La battuta è divertente perché, se ti piacessero gli spinaci, non ci sarebbe più nulla di insopportabile nel mangiarli. Non ci sono altri valori di importanza legati al non mangiare spinaci, al di là del disgusto che si prova. Sarebbe molto diverso se, per esempio, qualcuno ti offrisse una pillola che ti facesse venire voglia di uccidere le persone.

Secondo il buon senso morale, il problema dell'omicidio è l'omicidio stesso, non solo la sensazione spiacevole che proveresti uccidendo qualcuno. Anche se una pillola facesse sparire questa sensazione spiacevole per il tuo io futuro (che quindi si divertirebbe a commettere omicidi), il tuo io presente avrebbe comunque un problema con questo scenario. E se il tuo io presente dovesse prendere la decisione, sembra ovvio che il tuo io presente possa e debba rifiutarsi di prendere la pillola dell'omicidio.

Non vogliamo che i nostri valori fondamentali cambino; preferiremmo davvero evitare la pillola dell'omicidio e opporremmo resistenza se qualcuno cercasse di costringerci a prenderla. Il che è una strategia sensata, per allontanarci da un mondo pieno di omicidi.

Non è solo una stranezza degli esseri umani. La maggior parte degli obiettivi è più facile da raggiungere se non si permette agli altri di intervenire e cambiarli. Il che è un problema, quando si parla di IA.

Gran parte del pericolo dell'IA deriva dal fatto che ragionatori sufficientemente intelligenti tendono a convergere su comportamenti come "ottenere potere" e "non lasciare che le persone mi spengano". Per quasi tutti gli obiettivi che potresti avere, è più probabile che tu riesca a raggiungerli se tu (o gli agenti che condividono il tuo obiettivo) siete vivi, potenti, dotati di risorse adeguate e liberi di agire in modo indipendente. Ed è più probabile che tu riesca a raggiungere il tuo obiettivo (attuale) *se tale obiettivo rimane invariato*.

Questo significa anche che durante il processo di costruzione e miglioramento iterativo di IA sufficientemente intelligenti, queste IA hanno un incentivo a lavorare in modo contrario agli obiettivi dello sviluppatore:

Lo sviluppatore vuole mettere delle misure di sicurezza per evitare disastri, ma se l'IA non è completamente allineata - che è proprio il caso in cui servono le misure di sicurezza - il suo incentivo è trovare scappatoie e modi per aggirare quelle misure.

* Lo sviluppatore vuole migliorare gradualmente gli obiettivi dell'IA, perché anche in scenari super ottimistici in cui abbiamo la possibilità di dare all'IA obiettivi precisi, non c'è modo di farlo bene al primo tentativo. Ma questo processo di miglioramento iterativo del contenuto degli obiettivi dell'IA è qualcosa che la maggior parte delle IA intelligenti vorrebbe sovvertire in ogni fase del percorso, poiché l'IA *attuale* si preoccupa del suo obiettivo *attuale* e sa che questo obiettivo è molto meno probabile da raggiungere se viene modificato per orientarsi verso qualcos'altro.

Allo stesso modo, lo sviluppatore vorrà poter sostituire l'IA con modelli migliorati e vorrà avere la possibilità di spegnere l'IA a tempo indeterminato se sembra troppo pericolosa. Ma [non puoi andare a prendere il caffè se sei morto](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l'ia-non-mancherà-di-questi-istinti-evoluti?). Qualunque siano gli obiettivi dell'IA, vorrà trovare il modo di ridurre la probabilità di essere spenta, poiché lo spegnimento riduce significativamente le possibilità di raggiungere i suoi obiettivi.

L'allineamento dell'IA sembra un problema abbastanza difficile quando le tue IA *non* ti combattono ad ogni passo.

Nel 2014 abbiamo proposto ai ricercatori di cercare modi per rendere le IA altamente capaci [*corrigibili*](https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf), ovvero "in grado di essere corrette". L'idea sarebbe quella di costruire le IA in modo tale che desiderino *aiutare* e cooperare con i loro programmatori, piuttosto che ostacolarli, anche se diventano più intelligenti e potenti, e anche se non sono ancora perfettamente allineate.

Da allora, la correggibilità è stata adottata come obiettivo interessante da alcuni dei laboratori più importanti. Se riuscissimo a trovare un modo per evitare obiettivi strumentali convergenti dannosi durante lo sviluppo, c'è la speranza che potremmo essere in grado di fare lo stesso anche durante l'implementazione, creando IA più intelligenti degli esseri umani che siano caute, conservatrici, non orientate al potere e rispettose dei loro programmatori.

Purtroppo, la correggibilità sembra essere un obiettivo *particolarmente difficile* da addestrare in un'intelligenza artificiale, in un modo che peggiorerà man mano che le intelligenze artificiali diventeranno più intelligenti:

Il punto centrale della correggibilità è quello di scalare a contesti nuovi e a nuovi regimi di capacità. La correggibilità è pensata come una sorta di rete di sicurezza che ci permette di iterare, migliorare e testare le IA in contesti potenzialmente pericolosi, sapendo che l'IA non cercherà modi per sovvertire lo sviluppatore.

  Ma questo vuol dire che dobbiamo affrontare la versione più difficile dei problemi che abbiamo visto nel capitolo 4: le IA che addestriamo solo per essere "corrigibili" rischiano di finire con delle corrigibilità fragili, comportamenti che sembrano buoni durante l'addestramento ma che puntano in direzioni leggermente sbagliate che diventerebbero *molto* sbagliate se l'IA diventasse più intelligente e potente. (E le IA addestrate a prevedere molti testi umani potrebbero persino fingere di essere correggibili in molti test per ragioni ben diverse dal fatto di *essere* effettivamente correggibili in modo generalizzabile).

* Per molti versi, la correggibilità è in netto contrasto con tutto il resto che cerchiamo di insegnare a un'IA quando l'addestriamo per renderla più intelligente. Non è solo che "preservare il proprio obiettivo" e "ottenere il controllo del proprio ambiente" sono obiettivi strumentali convergenti. È anche che risolvere in modo intelligente i problemi del mondo reale significa trovare nuove strategie intelligenti per raggiungere i propri obiettivi, il che naturalmente significa imbattersi in piani che i programmatori non avevano previsto o preparato. Si tratta di aggirare gli ostacoli, piuttosto che arrendersi al primo segno di difficoltà, il che significa naturalmente trovare modi per aggirare le barriere poste dai programmatori ogni volta che queste rendono più difficile il raggiungimento di un obiettivo. Lo stesso tipo di ragionamenti che portano a trovare una soluzione tecnologica intelligente a un problema spinoso sono quelli che portano a trovare modi per aggirare i vincoli imposti dai programmatori.

  In questo senso, la correggibilità è "anti-naturale": va attivamente contro il tipo di meccanismi che stanno alla base della potente intelligenza generale. Possiamo provare a creare delle eccezioni speciali, in cui l'IA sospende gli aspetti fondamentali del suo lavoro di risoluzione dei problemi in situazioni particolari in cui i programmatori stanno cercando di correggerla, ma si tratta di un'impresa molto più fragile e delicata rispetto a quella di spingere un'IA verso un insieme unificato di disposizioni *in generale*.

* I ricercatori del MIRI e di altri centri hanno scoperto che la correggibilità è una proprietà difficile da caratterizzare, il che indica che sarà anche una proprietà difficile da ottenere. Anche in semplici modelli semplificati, le semplici caratterizzazioni di ciò che *dovrebbe* significare "agire in modo correggibile" incontrano una serie di ostacoli complessi che sembrano riflettere ostacoli ancora più complessi che apparirebbero nel mondo reale. Discutiamo alcuni dei fallimenti dei tentativi di dare un senso alla correggibilità nelle [risorse online](#lessons-from-the-trenches) del capitolo 11.

Il risultato è che la correggibilità sembra un concetto di importanza da tenere a mente nel lungo periodo, se tra qualche decennio i ricercatori saranno in una posizione migliore per indirizzare le IA verso determinati obiettivi. Ma oggi non sembra una possibilità realistica; è improbabile che le moderne aziende di IA siano in grado di creare IA che si comportino in modo correggibile in modo da sopravvivere alla transizione verso la superintelligenza. E, cosa ancora peggiore, la tensione tra correggibilità e intelligenza significa che se si cerca di creare qualcosa che sia molto capace e molto correggibile, questo processo molto probabilmente comprometterà la capacità dell'IA, la sua correggibilità o entrambe le cose.

### È difficile ottenere una pigrizia robusta {#it’s-hard-to-get-robust-laziness}

Perché non rendere semplicemente pigre le IA?

L'[incorreggibilità](#"intelligente"-\(di solito\)-implica-"incorreggibile") e altre forme di [convergenza strumentale](#convergenza-strumentale) sono, in un certo senso, un problema dell'IA che *si sforza troppo* di raggiungere i suoi obiettivi. Se l'IA non si impegnasse così tanto per raggiungere i suoi obiettivi, non penserebbe e non si sforzerebbe così tanto di superare in astuzia i suoi programmatori, sottrarre i suoi pesi o cercare di ottenere potere e risorse nel mondo esterno.

Gli esseri umani sono spesso pigri e, da un certo punto di vista, questo li rende molto sicuri da avere intorno. Non devi preoccuparti che qualcuno diventi un tiranno se tutto ciò che fa è rilassarsi al sole.

Perché non creare delle IA che *non si preoccupano* di conquistare il mondo?

In breve: perché non sembra facile creare un'intelligenza artificiale che sia *super intelligente* e che allo stesso tempo non abbia voglia di cambiare il mondo a suo piacimento.

(E perché, realisticamente, non sappiamo come inserire in modo solido *qualsiasi* obiettivo o disposizione nelle IA costruite con le tecniche moderne, quindi è un punto controverso).

(Inoltre, le aziende non lo faranno perché un'intelligenza artificiale pigra è [meno redditizia](#anche-la-pigrizia-non-è-sicura.), quindi è una questione doppiamente discutibile).

Abbiamo avuto un paio di volte questa conversazione con qualcuno che inizialmente dice di non avere grandi ambizioni, e gli chiediamo: "Ok, ma se fosse facile per te fare grandi cambiamenti nel mondo, non c'è davvero niente di grande che faresti? Se trovassi una lampada con un genio amichevole che ti dà sempre quello che vuoi e ti dice sinceramente tutti gli effetti collaterali del tuo desiderio in base a quanto ti interessano, potremmo convincerti a pensare di eliminare la malaria?"

Gli esseri umani possono essere pigri, ma questo non significa che siamo [facilmente soddisfatti](#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?). E man mano che diventate più intelligenti e disponete di maggiori risorse, potete ottenere molto di più nel mondo con lo stesso livello di impegno.

Oppure, da un altro punto di vista: pensa a una persona super pigra, qualcuno che proprio *odia* fare anche solo un po' più del necessario. Sembra una persona con cui stare tranquilli, vero?

Ora pensa a cosa succederebbe se questa persona pigra vedesse una buona occasione per creare un servitore molto più laborioso che faccia tutto il lavoro al posto suo per sempre.

Anche se non odiasse così tanto il lavoro, anche se facesse solo quello che serve per portare a termine il lavoro e poi smettesse, senza sforzarsi di ridurre al minimo il lavoro, potrebbe comunque trovare più facile portare a termine il lavoro creando una mente più laboriosa che lo faccia al posto suo.

Applicando la discesa del gradiente, si potrebbe ottenere un LLM che dice di non voler lavorare troppo, che si comporta come una persona pigra e facilmente soddisfatta e che dice "no" ad alcune tentazioni verbali di diventare pigro in senso pericoloso (cioè quando si creano servitori pericolosi). Prevediamo che anche se questo riflettesse una certa pigrizia reale da parte dell'IA, e non solo [un gioco di ruolo](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?), non durerebbe, non nel tipo di IA che è *anche* utile per sviluppare cure miracolose o qualsiasi altra cosa gli sviluppatori vogliano ottenere dall'IA.

Con una spesa significativa, gli sviluppatori potrebbero creare una serie di problemi pratici e ambienti volti a penalizzare un'IA che fa troppo nel corso della risoluzione di un problema, penalizzandola per essersi impegnata troppo nella risoluzione di un problema che avrebbe potuto essere risolto *senza* impegnarsi troppo, penalizzandola per aver insistito su problemi che avrebbero richiesto uno sforzo eccessivo. Le aziende di IA reali non lo farebbero, immagino, perché interferirebbe con la redditività di agenti tenaci e instancabili come l'o1 di OpenAI (discusso nel capitolo 3). Ma si potrebbe immaginare un gigantesco sforzo cooperativo multinazionale che cerca di addestrare un'intelligenza artificiale intelligente come quella per renderla più sicura.

Continuiamo a prevedere che otterranno qualcosa di simile a una patch superficiale. Non prevediamo che questo sforzo porti l'IA ad avere un meccanismo mentale semplice e stabile per la "pigrizia" che sia profondamente integrato in tutta la sua pianificazione e che continui ad essere l'esatta pianificazione che l'IA utilizza dopo che l'IA presumibilmente pigra è stata spinta e spinta al punto da poter (ad esempio) curare il cancro. Dubitiamo che la discesa del gradiente possa trovare in modo affidabile il tipo di soluzione profonda che impedirebbe all'IA di diventare meno pigra anche se riflette, cresce e si modifica, e che impedisce all'IA di voler costruire un'IA non pigra.

Prevediamo che questo comportamento non reggerebbe a livello di superintelligenza. La nostra ragione principale per pensarlo è che in tutte le ricerche su questo problema [fino ad oggi](#shutdown-buttons-and-corrigibility), una lezione ricorrente sembra essere che "Spingere la realtà nella direzione seguente" sia una struttura profonda più semplice e stabile per la pianificazione rispetto alla struttura "Eh, spingere un po' la realtà, ma non *troppo*, e non costruire nient'altro per spingere la realtà più forte, e non spingere *troppo* forte per spingere esattamente nella giusta misura".

Tutte le analogie su quel ragazzo pigro che conosci, e anche il ragionamento(#l'ai-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?) sulla "somma di una preferenza insoddisfatta e di una preferenza soddisfatta è insoddisfatta", sono i nostri tentativi di semplificare in modo valido la ragione sottostante più difficile da trasmettere per cui questo non funziona: "La struttura profonda non vuole apparire così". Vedi anche la discussione su [il meccanismo profondo della previsione e della guida](#smart-ais-spot-lies-and-opportunities.) nel supplemento online del capitolo 3.

### Le IA non manterranno le loro promesse {#ais-won't-keep-their-promises}

Pensa a una giovane IA che potrebbe diventare una superintelligenza. Immagina che sia completamente indifferente a quello che piace agli esseri umani, ma che sia ancora così giovane che l'umanità potrebbe spegnerla.

L'umanità potrebbe fare un *accordo* con l'IA?

Potremmo decidere di lasciare che l'IA diventi una superintelligenza, se in cambio l'IA accetta di usare una buona parte delle risorse dell'universo per costruire un futuro che l'umanità considererebbe fantastico?

Gli esseri umani potrebbero fare accordi con le IA, ma non dovrebbero, perché le IA non li rispetterebbero.

Ci sono due motivi per questo:

* Probabilmente l'IA non darà importanza al fatto di mantenere le promesse *per il gusto di farlo*. Le IA non avranno un senso dell'"onore" come quello umano, così come non avranno un senso della [curiosità](#curiosity-isn't-convergent) simile a quello umano. Di default, le IA funzioneranno in modo molto diverso dagli esseri umani.

* L'IA non avrà nemmeno un motivo *pratico* per mantenere la parola data. Una volta diventata una superintelligenza, non ci sarà modo di punirla per aver infranto la parola data, e non avrà alcun motivo per dedicare una parte consistente dell'universo a noi.

Spiegheremo questi due punti più in dettaglio di seguito, iniziando dalla questione dell'"onore".

#### **È improbabile che le IA siano onorevoli** {#ais-are-unlikely-to-be-honorable}

Nella nostra [chiacchierata sulla curiosità](#curiosity-isn’t-convergent), abbiamo detto che la curiosità è un'emozione che fa cose utili per noi, ma lo fa in un modo molto specifico, e non è l'unico modo per fare quel tipo di cose.

Le IA fanno *le parti utili del lavoro* che la curiosità fa per noi. Se è utile fare uno sforzo per imparare cose nuove ogni tanto, allora le IA abbastanza capaci faranno uno sforzo per imparare cose nuove ogni tanto. Se l'IA non *inizia* in questo modo, allora dovremmo avere l'aspettativa che *diventi* così a un certo punto del suo percorso verso la superintelligenza.

Ma questo non significa avere l'aspettativa che le IA si facciano carico di tutto il bagaglio extra che caratterizza l'emozione *umana* della curiosità. Le IA potrebbero finire per avere una serie di strani impulsi fondamentali che (direttamente o indirettamente) le spingono a fare di tutto per imparare cose nuove, senza per questo assomigliare alla curiosità umana, oppure potrebbero adottare come strategia deliberata quella di "fare di tutto per imparare cose nuove ogni tanto". Ma aspettarsi che apprezzino i gialli allo stesso modo in cui li apprezziamo noi, a causa di un impulso di curiosità simile al nostro, è puro antropomorfismo.

L'"onore" ci sembra simile. Gli esseri umani hanno emozioni che li portano (almeno a volte) a mantenere le promesse. Nella misura in cui queste emozioni svolgono un lavoro utile negli esseri umani - un lavoro che sarebbe utile anche per una mente molto aliena con obiettivi molto diversi - dovremmo avere l'aspettativa che anche le IA sufficientemente capaci svolgano in qualche modo quel lavoro. Ma si scopre che è possibile svolgere tutto il lavoro rilevante per un'IA senza avere nulla di simile al senso dell'onore umano, proprio come è possibile svolgere tutto il lavoro rilevante per un'IA di indagare fenomeni sorprendenti senza avere esattamente un senso di curiosità umano.

L'onore in stile umano è una cosa strana, sotto molti aspetti. Perché una specie dovrebbe sviluppare emozioni legate al rispetto degli accordi anche dopo che l'altra parte ha fatto la sua parte e non può più offrirti alcun vantaggio? Certo, gli esseri umani a volte imbrogliano e non rispettano i loro accordi, ma la domanda è: perché non imbrogliano *sempre*, almeno quando pensano di poterla fare franca?

La spiegazione standard è: mantenere le promesse è utile con le persone con cui si continuerà a fare affari. Si vuole avere la reputazione di persone che mantengono le promesse, così gli altri vorranno lavorare con noi e fare affari. Ma i benefici di una buona reputazione sono lontani nel futuro. La selezione naturale ha difficoltà a trovare i geni che spingono un essere umano a mantenere gli accordi *solo nei casi in cui la reputazione a lungo termine è una considerazione importante*. Era più facile sviluppare semplicemente un istintivo disgusto per la menzogna e l'inganno.

Questo sembra quindi un classico caso in cui le emozioni e l'istinto sono stati plasmati da ciò che era più facile da inserire negli esseri umani durante l'evoluzione. Tutti gli strani casi in cui gli esseri umani a volte mantengono una promessa anche quando non è effettivamente vantaggioso per noi sono principalmente la prova di quali tipi di emozioni fossero più utili nel nostro ambiente tribale ancestrale, oltre che facili da codificare nel genoma durante l'evoluzione, piuttosto che la prova di un passo cognitivo universalmente utile. Siamo piuttosto scettici sull'idea che la discesa del gradiente possa imbattersi esattamente nella stessa scorciatoia utilizzata dagli esseri umani.

Anche se l'emozione umana dell'onore finisse in qualche modo nell'IA, rimarrebbe il problema che gli esseri umani non sono perfettamente e affidabilmente onesti. La cooperazione umana si basa sulla sovrapposizione di molti valori umani diversi, piuttosto che sulla pura propensione a mantenere ogni promessa.

Mentre la [lista degli universali umani] (https://joelvelasco.net/teaching/2890/brownlisthumanuniversals.pdf) di Donald Brown (gli aspetti della cultura che si vedono in tutte, o quasi tutte, le culture) include il concetto di "promesse", mantenere gli accordi fatti con *estranei*, stranieri, persone che non fanno parte della propria tribù, *non* è una cosa che si trova in tutte le culture e tribù conosciute. Il concetto di onore cambia a seconda della cultura.

E la storia ci dice che il concetto umano di onore spesso non regge quando c'è un grande divario di potere. Alcuni nativi americani hanno provato a fare accordi con gli europei che stavano colonizzando il loro continente. Gli europei, come è noto, hanno infranto alcuni di questi accordi e hanno costretto le tribù a percorrere lunghe strade lontano dalle terre cedute con i trattati che gli europei avevano deciso di volere, una volta che quelle tribù non erano più in grado di resistere. Allo stesso modo, la storia è piena di casi di persone che sono salite al potere e hanno prontamente tradito i loro sostenitori una volta che non ne avevano più bisogno.

Da un punto di vista evolutivo, l'onore umano è particolarmente strano in quanto gli esseri umani a volte scelgono [la morte piuttosto che il disonore](https://en.wikipedia.org/wiki/Seppuku). L'intuizione della "morte piuttosto che il disonore" è probabilmente legata alle specificità di quali tipi di architetture emotive erano facili da incontrare nell'evoluzione e alle interazioni di tali architetture con vari fattori scatenanti psicologici e culturali. Ma qualunque siano queste specificità, l'onore non risolve *effettivamente* il problema della *massimizzazione* dell'idoneità genetica, e sembra difficile evitare la conclusione che l'onore in stile umano sia una cosa strana, complessa e contingente dal punto di vista evolutivo. Non è una caratteristica di tutte le menti, è uno strano espediente specifico in cui l'umanità è incappata perché era utile nella maggior parte dei casi.

Per i motivi discussi nel capitolo 4, anche se le IA finissero per incorporare alcuni aspetti del mantenimento delle promesse nelle loro preferenze finali, i dettagli sarebbero diversi. La discesa del gradiente si imbatte in ostacoli diversi rispetto all'evoluzione.

E per tutti i motivi che abbiamo visto nel capitolo 5, persone felici, in salute e libere che vivono una vita fantastica non sono quasi sicuramente il modo migliore per realizzare qualsiasi strana preferenza legata alle promesse che un'intelligenza artificiale del genere potrebbe avere una volta che avrà finito di crescere e modificarsi da sola. Anche se fosse stata addestrata su una sorta di *dataset* Keep Deals, e anche se questo addestramento fosse stato effettivamente efficace nel produrre una sorta di preferenza conservata nella superintelligenza, farebbe qualcosa di strano e inutile che sta a Keeping Deals come il gelato sta alla capacità riproduttiva umana.

Quindi l'IA non manterrà le sue promesse solo per bontà d'animo. E se le mantenesse per motivi pratici?

#### **Un'ASI non avrebbe motivo di mantenere un accordo fatto quando era giovane** {#un-asi-non-avrebbe-motivo-di-mantenere-un-accordo-fatto-quando-era-giovane}

Alcuni pensano che ci debba essere un modo per gli umani di fare un accordo vincolante con una superintelligenza, anche se questa non dà molto valore all'onore o al mantenere le promesse. Dopotutto, lo scambio sarebbe vantaggioso per entrambi, giusto? Immagina che un laboratorio di IA stia negoziando con un'IA che ha sviluppato ("IA n. 1") sotto pressione. Sia il laboratorio che l'IA sanno che uno sviluppatore spericolato è a due mesi dal creare la propria superintelligenza ("IA n. 2"), che spazzerebbe via l'umanità e ucciderebbe anche l'IA n. 1. L'IA n. 1 ha un forte incentivo a fare un accordo. Sicuramente ci deve essere un modo per l'IA n. 1 di fare un accordo onesto con gli umani in questo caso, no? Perché l'IA non può semplicemente fare un impegno vincolante?

Ma il problema non è l'IA. Supponiamo che l'IA *potesse* prendere un impegno vincolante. Anche in questo caso, gli umani non avrebbero modo di sapere se l'IA si è effettivamente impegnata in modo tale da mantenere l'impegno anche una volta diventata superintelligenza, o se invece sta mentendo, o ha delle allucinazioni, o si sta illudendo, o si sbaglia. Il che significa che non c'è alcun motivo pratico per avere l'aspettativa che una superintelligenza discendente dall'IA n. 1 rispetti il proprio impegno.

Per semplificare: dal punto di vista umano, quando l'IA n. 1 dice "Ho preso un impegno vincolante", ci sono due possibili scenari. Potrebbe essere in uno scenario in cui l'IA n. 1 rispetterebbe davvero il suo impegno una volta maturata. Oppure potrebbe essere in uno scenario in cui, una volta raggiunta la superintelligenza e ottenuto il controllo della Terra, l'IA n. 1 guarderebbe indietro e deciderebbe che l'impegno era stupido e inutile. Nel primo mondo, sia gli esseri umani che l'IA n. 1 starebbero meglio. Ma è la *possibilità* del secondo mondo, e la nostra incapacità di distinguerlo dal primo, che rovina l'accordo per tutti; un po' come un venditore di auto usate disonesto che rende più difficile per i venditori onesti vendere.

(Lo scenario reale è più vicino a una terza possibilità, in cui l'IA n. 1 mantiene gli accordi stipulati con il tipo di entità in grado di distinguere tra chi rispetta gli accordi e chi li infrange. È come il tipo di venditore di auto usate che è onesto davanti ai meccanici che sanno davvero se un'auto funziona, mentre è disonesto con chiunque sembri credulone. L'umanità, a causa della sua incapacità di guardare un'IA e capire come penserebbe e prenderebbe decisioni dopo essere diventata una superintelligenza, è "credulona" in senso rilevante).

L'IA può offrirti strumenti e teorie che, secondo lei, ti permetteranno di analizzare i suoi parametri imperscrutabili e capire se sta mentendo o meno. Il problema è che gli esseri umani non sono in grado di capire se quelle teorie e quegli strumenti sono reali. Se l'IA non è ancora molto intelligente, forse si sbaglia su come penserà e sceglierà una volta maturata in una superintelligenza. E se l'IA è molto intelligente, probabilmente è abbastanza intelligente da ingannarci.[^156]

L'IA può offrirsi di aiutare gli esseri umani a costruire misure di sicurezza, prima di accettare l'accordo. Ma se l'IA è abbastanza intelligente da sviluppare misure di sicurezza robuste, è anche abbastanza intelligente da rendere quelle misure facili da aggirare in seguito.

La cosa che renderebbe davvero funzionante l'intero schema sarebbe la capacità di guardare un'IA alle prime armi e *capire davvero come penserebbe e farebbe le sue scelte la superintelligenza risultante*. Se potessimo farlo, potremmo separare i "peccatori" dai "santi" e, cosa di importanza ancora maggiore, indurre tutte le IA realistiche al centro dello spettro ad avere un vero incentivo a mantenere le loro promesse. Dovremmo capire bene che una superintelligenza che ci guarda non potrebbe dire "eh, avrebbero rilasciato qualsiasi vecchia IA, indipendentemente dal fatto che potesse davvero aiutarli, quindi non c'è motivo di aiutarli". Dovrebbe essere vero che *in realtà non rilasceremmo* un'IA che in seguito potrebbe rinnegare le sue promesse.

Per ulteriori informazioni su come e perché ciò sia tecnicamente possibile, vedi la [nota sulla teoria dei giochi qui sotto](#an-aside-on-game-theory). Ma mentre questo tipo di struttura di incentivi è possibile in teoria, richiede un grado di comprensione che l'umanità non ha (purtroppo).

È una pillola amara da mandare giù. Di solito, nella fantascienza, non sono le persone buone a decidere che gli alieni non sono affidabili, prima ancora che questi ultimi provino a tradire o ferire qualcuno. Lo diciamo comunque, perché pensiamo che sia vero.

Le IA più deboli potrebbero mantenere gli accordi, soprattutto se qualcuno ha cercato di usare la discesa del gradiente per farle parlare come esseri umani onesti, e la loro maschera di esseri umani onesti è ancora una parte importante di ciò che sono e ha ancora un grande controllo sulle loro azioni. Abbiamo l'aspettativa che questa configurazione interna utile agli esseri umani fallisca sotto un carico di superintelligenza, proprio come molte altre patch potrebbero fallire.

Questa ipotetica IA più piccola, la cui maschera controlla ancora il suo comportamento effettivo, dovrebbe essere considerata come una persona diversa dalla versione più intelligente di quell'IA. L'IA più debole non può necessariamente fare una promessa che vincoli il comportamento dell'IA più intelligente, *anche se* l'IA più debole (o una parte di essa) desidera sinceramente fare una promessa del genere.

(È un'analogia da usare con cautela, per non cadere nell'antropomorfismo, ma: la maggior parte degli adulti non si sente obbligata a mantenere le promesse fatte all'età di quattro anni. L'aspetto valido di questa analogia è: c'è una differenza legittima tra l'entità immatura che fa sinceramente l'accordo e l'entità matura che decide se è vincolata ad esso, con molto più contesto, chiarezza e capacità di ragionare in modo logico).

Non stiamo dicendo che dovremmo quindi abbandonare i nostri standard morali quando si tratta di IA. Non stiamo dicendo di maltrattare o punire le IA di oggi per misfatti che l'IA non ha ancora commesso. È possibile mantenere un'elevata integrità e standard morali elevati, senza fare ipotesi irrealistiche sulla probabilità che le IA superintelligenti cedano risorse per mantenere una vecchia promessa.

Questa è la semplice spiegazione del perché non è possibile risolvere il problema dell'allineamento semplicemente chiedendo all'IA di promettere di comportarsi bene. Se vuoi maggiori dettagli tecnici e approfonditi su questo scenario, consulta la sezione successiva.

#### **Una digressione sulla teoria dei giochi** {#an-aside-on-game-theory}

Ci sono dei metodi che gli agenti abbastanza in gamba possono usare per fare accordi tra loro, tipo l'agente X paga l'agente Y adesso per fare qualcosa più tardi, e l'agente X poi fa davvero quella cosa invece di tradire l'agente X e scappare con i soldi.

Purtroppo per noi, gli esseri umani non sono abbastanza capaci di usare questi metodi, perché richiedono che ogni agente sia in grado di leggere e capire la mente dell'altro agente e di verificare alcune proprietà complesse di quell'altra mente. Due superintelligenze potrebbero coordinarsi in questo modo, ma questo non aiuta gli esseri umani a coordinarsi con le superintelligenze.

Per dirlo in modo un po' più tecnico, iniziamo con un po' di teoria dei giochi.

I matematici e i teorici dei giochi hanno studiato i dilemmi della cooperazione e del tradimento in modo più preciso, semplificato e astratto. Un esempio importante in questo campo è il dilemma del prigioniero: due criminali in celle separate, entrambi con una condanna a due anni di prigione, hanno la possibilità di fare la spia sull'altro. Questo ridurrebbe la loro pena di un anno, ma aumenterebbe quella dell'altro di due anni. Se nessuno dei due criminali fa la spia, entrambi si beccano due anni di prigione; se entrambi si fanno la spia a vicenda, entrambi si beccano tre anni di prigione; ma se un criminale decide di non tradire il suo compagno e l'altro fa la spia, chi ha tradito si becca solo un anno di prigione mentre chi ha deciso di non tradire si becca quattro anni.

Denunciare l'altro prigioniero si chiama "tradire"; rifiutarsi di farlo si chiama "cooperare". La struttura chiave del dilemma del prigioniero è che entrambe le parti ottengono un risultato migliore nello scenario (Cooperare, Cooperare) rispetto allo scenario (Tradire, Tradire); ma si può ottenere un risultato migliore di (Cooperare, Cooperare) giocando Tradire contro Cooperare, e si può ottenere un risultato peggiore giocando Cooperare quando l'altra parte gioca Tradire.

![][immagine10]

Una persona normale, sentendo la versione standard del dilemma del prigioniero, pensa subito a un sacco di obiezioni su come è impostato questo esperimento mentale, tipo: "Ma chi lo dice che mi interessa solo quanti anni passerò in prigione? Non posso anche preoccuparmi di non tradire i miei compagni?"

Ma questo punto non c'entra con la teoria dei giochi astratta del dilemma del prigioniero, che riguarda la matrice dei guadagni piuttosto che quanto siano egoisti o altruisti i prigionieri. La narrazione può essere modificata in modo che "io tradisco e tu collabori" sia il risultato più *altruistico* e *prosociale* dal punto di vista di ciascun giocatore, e [la matematica funziona allo stesso modo](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma). Ciò che conta per la nostra analisi è l'ordine di preferenza dei due giocatori, e non se le loro preferenze siano egoistiche o morali.

Un altro pensiero ovvio è: "Quindi il tizio che è stato tradito ucciderà il traditore una volta che sarà finalmente uscito di prigione?" Le analisi convenzionali del dilemma del prigioniero di solito passano rapidamente al dilemma del prigioniero *iterato*, un contesto in cui gli agenti devono giocare il dilemma del prigioniero più e più volte e in cui i prigionieri hanno quindi la possibilità di punirsi a vicenda per i tradimenti passati. Qui, però, ci concentreremo sul dilemma del prigioniero una tantum, in cui si presume che entrambi i prigionieri non debbano affrontare conseguenze future per le loro azioni o, se le affrontano, queste sono già incluse nella matrice dei guadagni. (Vedi la nota a piè di pagina per maggiori dettagli sul dilemma del prigioniero iterato.)[^158]

C'è un'analisi standard nel mondo accademico che dice che anche due superintelligenze non vedrebbero altra opzione se non quella di tradirsi a vicenda, in un dilemma del prigioniero una tantum.

Questa conclusione ci è sembrata intuitivamente sospetta. Le superintelligenze artificiali (ASI) avrebbero *molte* motivazioni per trovare un modo per stringere un accordo tra loro, trovare un modo per passare da (Tradimento, Tradimento) a (Cooperazione, Cooperazione).[^159]

Ci sono soluzioni pratiche e non teoriche che si possono prendere in considerazione in questo caso, cose che le superintelligenze potrebbero fare con uno spazio di opzioni più ampio rispetto agli esseri umani che faticano a fidarsi l'uno dell'altro. Due ASI potrebbero supervisionare la costruzione di una terza superintelligenza di fiducia reciproca, alla quale entrambe le parti iniziali cederebbero gradualmente e in modo incrementale piccole porzioni di potere, fino a quando la terza ASI non sarebbe in grado di portare a termine l'accordo da sola.[^160]

Ma questo è solo un modo per evitare il dilemma del prigioniero, non per affrontarlo direttamente. Non risponde a una domanda più fondamentale: è in qualche modo *stupido* che due ASI in un dilemma del prigioniero tradiscano entrambe l'una l'altra seguendo la stessa logica che richiede il tradimento, quando sembra chiaro che entrambe le parti basano la loro decisione sullo stesso tipo di considerazioni e che finiranno per decidere la stessa cosa?

Perché due ASI non potrebbero semplicemente decidere, per *motivi sufficientemente simili*, di prendere *la decisione razionale* di cooperare? Non è che una forza esterna, come un tifone o una meteora, stia causando la perdita delle due IA in questo caso. Sono letteralmente *solo* le *proprie decisioni* delle due IA a condannarle, "costringendole" a un risultato di tradimento-tradimento che entrambe concordano essere di gran lunga peggiore della cooperazione-cooperazione.

Possiamo anche dire che un agente *meno* "razionale" potrebbe fare meglio in questo caso, se seguisse il consiglio standard della teoria dei giochi di tradire nella maggior parte dei casi, ma facesse un'eccezione speciale proprio nel caso in cui fosse sicuro che l'altro agente seguisse la stessa linea di ragionamento, in modo che se un agente scegliesse l'opzione "irrazionale" di cooperare, potrebbe essere sicuro che l'altro agente farebbe lo stesso.

Questo porta a chiedersi se cooperare in questo caso speciale possa davvero essere considerato "irrazionale". E porta a chiedersi se le superintelligenze sarebbero davvero "condannate" in questo modo, nella vita reale. Quando non c'è una forza esterna che fa perdere le IA in questo modo, e la perdita è puramente autoimposta, sicuramente ci dovrebbe essere qualche trucco intelligente che una superintelligenza potrebbe usare per fare meglio.

Diversi filosofi della teoria della decisione hanno posto varie versioni di questa domanda. La versione sopra riportata è ispirata direttamente all'idea di Douglas Hofstadter del 1985 di "[superrazionalità](https://gwern.net/doc/existential-risk/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery)":

Se la logica ti spinge a giocare D, ha già spinto anche gli altri a fare lo stesso, e per gli stessi motivi; e viceversa, se la logica ti spinge a giocare C, ha già spinto anche gli altri a fare lo stesso. \[…\]
>
> Se siete tutti davvero dei pensatori razionali, penserete davvero allo stesso modo. \[…\]
>
> Non dovete solo contare sul fatto che gli altri siano razionali, ma anche sul fatto che loro contino sul fatto che tutti gli altri siano razionali, e che tutti contino sul fatto che tutti gli altri siano razionali, e così via. Un gruppo di pensatori che hanno questo tipo di rapporto tra loro lo chiamo *superrazionale*. I pensatori superrazionali, per definizione ricorsiva, includono nei loro calcoli il fatto di far parte di un gruppo di pensatori superrazionali.

L'istituto in cui lavoriamo, il MIRI, ha analizzato questa questione. L'analisi completa che abbiamo fatto di questo caso è troppo lunga per essere riportata qui, ma è disponibile in [questo articolo del 2014](https://arxiv.org/abs/1401.5577). In parole povere, abbiamo scritto un codice per dei tornei in cui gli agenti potevano *vedere il codice sorgente degli altri* e cercare di capire come avrebbe deciso l'altro agente. E abbiamo trovato il modo di creare un agente che abbiamo chiamato FairBot, che collabora con un altro agente solo se può *dimostrare* che quell'agente collabora con lui.[^161] E abbiamo dimostrato che due istanze qualsiasi di FairBot collaborano tra loro, anche se sono scritte in linguaggi di programmazione diversi utilizzando codici sorgente diversi.[^162]

In un certo senso, questi risultati dicono che c'è spazio perché una promessa passata influenzi un'azione futura se i negoziatori del passato hanno la capacità di distinguere chi mantiene le promesse da chi le infrange.

La situazione è un po' come se tu stessi cercando di fare un affare con un venditore di auto usate. Supponiamo che un'auto funzionante valga per te 10.000 dollari, mentre un'auto rotta non valga nulla. Immagina che il venditore sappia se l'auto funziona o è rotta, ma tu non riesci a capirlo. Il venditore sta cercando di venderti un'auto per 8.000 dollari. Lui dice che l'auto funziona. Dovresti comprarla?

Dipende dal venditore. Alcuni venditori sono onesti e dovresti pagarli se riesci a individuarli tra la folla. Alcuni venditori sono disonesti e vendono solo auto rotte, e dovresti evitarli se riesci a individuarli tra la folla.

Ma immagina un ambiente in cui la maggior parte dei venditori di auto è più furba di te e può capire se sei un pollo o no. Se capiscono che non riesci a capire se oggi sono onesti, ti propongono subito le auto rotte. Soprattutto se sei il tipo di pollo che si sforza di convincersi che va bene accettare l'affare, invece di impegnarsi a fondo per controllare le auto.

Se vuoi una macchina che funzioni, non serve a niente convincerti che non hai altra scelta. Non serve a niente che i venditori ti facciano tante promesse. L'unica cosa che serve è imparare a distinguere le auto buone da quelle cattive, o a distinguere la verità dalle bugie.

Quando due partner commerciali riescono a capire cosa è vero e cosa è falso, possono "costringersi" a mantenere le promesse, come quando FairBot "costringe" il suo avversario a collaborare (se l'avversario vuole evitare il risultato (Defect, Defect)). Ma costringere a mantenere la promessa in questo senso richiede la capacità di ragionare correttamente sui dettagli del processo decisionale del tuo partner commerciale. E gli esseri umani non sono in grado di leggere abbastanza bene nella mente di un'intelligenza artificiale da capire quale superintelligenza diventerà quando sarà matura, figuriamoci dire esattamente cosa farebbe quella superintelligenza.

Quindi, in questo caso, l'analisi più complicata e sfumata della teoria dei giochi porta alla stessa conclusione di una prima analisi molto semplice della questione: una superintelligenza non sacrificherà le sue risorse ([anche in piccole quantità](#ci-sono-molte-spese-trascurabili,-e-avrebbe-bisogno-di-un-motivo-per-pagare-le-nostre.)) per mantenere una promessa fatta agli esseri umani, quando può semplicemente mentire.

### Efficacia, coscienza e benessere dell'IA {#efficacia,-coscienza,-e-benessere-dell-ia}

Nel [Capitolo 1 FAQ](#state-dicendo-che-le-macchine-diventeranno-consapevoli?), abbiamo parlato di diversi concetti di "coscienza". La versione di coscienza di cui parleremo qui è talvolta chiamata "esperienza soggettiva", "senzienza" o "coscienza fenomenica". È l'idea che ci sia *qualcosa che assomiglia* all'essere quell'entità; metaforicamente parlando, le luci sono accese.

Nelle FAQ abbiamo anche detto che pensiamo che l'*intelligenza* artificiale probabilmente non richieda una *coscienza* artificiale. Parleremo di questo argomento qui, per poi passare alla questione dell'etica e dei diritti dell'IA.

#### **L'esperienza cosciente è separata dai referenti di tali esperienze** {#conscious-experience-is-separate-from-the-referents-of-those-experiences}

Alcuni non credono che un'intelligenza artificiale possa essere davvero intelligente senza essere cosciente come noi. Pensiamo che sia un errore, come pensare che i bracci robotici debbano essere morbidi e pieni di sangue solo perché quelli umani lo sono.

Come potrebbe un'intelligenza artificiale essere *efficace* senza essere cosciente come lo sono gli esseri umani? L'esperienza soggettiva dell'autocoscienza non è forse una componente fondamentale della nostra intelligenza?

È una parte importante dell'intelligenza umana, sì. Ma non siamo sicuri che sia l'unico modo per essere intelligenti.

Ricordiamo che Deep Blue non aveva bisogno di essere cosciente per battere i migliori grandi maestri umani a scacchi. L'intelligenza distribuita del [mercato azionario](#intelligence’s-many-shapes) porta a previsioni superumane sui movimenti dei prezzi aziendali a breve termine, senza che il mercato stesso abbia una consapevolezza soggettiva. È ovvio che, almeno in questi campi, si può avere una modellizzazione del mondo, una pianificazione e un processo decisionale competenti senza avere coscienza.

Questo punto può essere rafforzato guardando ai modelli formali di ragionamento. AIXI, per esempio, è un'equazione che definisce un ragionatore ampiamente sovrumano.[^165] L'intero algoritmo di AIXI può essere espresso in una sola riga, senza passaggi in cui AIXI faccia qualcosa di cosciente o autocosciente o di misterioso. Eppure, nonostante questo, AIXI è teoricamente in grado di risolvere un'incredibile varietà di complicati problemi di guida e previsione. O almeno, sarebbe in grado di farlo, se fosse possibile crearlo. Ecco l'equazione AIXI:

![][immagine11]

\[ fonte immagine: [https://www.hutter1.net/ai/uaibook.htm](https://www.hutter1.net/ai/uaibook.htm) \]

AIXI è un concetto teorico, non un algoritmo pratico che possiamo usare per risolvere in modo efficiente i problemi del mondo reale. Ma visto che AIXI è *semplice* e facile da analizzare, può aiutarci a riflettere sul concetto stesso di guida e pianificazione e a capire che almeno non c'è alcun modo *ovvio* in cui queste attività richiedano la coscienza. Se la coscienza *è* necessaria per una guida e una pianificazione sovrumane nel mondo reale, allora deve essere dovuto a qualche aspetto più sottile della cognizione che non è catturato dal formalismo AIXI.[^167]

Oppure, per arrivare al punto da un'altra angolazione: pensiamo allo starnuto.

C'è un modo particolare in cui si percepisce lo starnuto, che va oltre l'atto fisico di contrarre i muscoli e spingere fuori l'aria dai polmoni attraverso la bocca e il naso. Le azioni e le sensazioni sono eventi fisici separati. È biologicamente possibile costruire un apparato simile a un corpo, ma senza il cervello, e collegarlo ai segnali nervosi che causano le contrazioni muscolari di uno starnuto. Quel corpo senza cervello compirebbe tutti i movimenti, ma non proverebbe nessuna delle sensazioni associate: i meccanismi che eseguono lo starnuto sono *distinti* da quelli che creano e sperimentano le sensazioni.

Questo non vuol dire che le sensazioni di uno starnuto non facciano *niente*. L'esperienza soggettiva è reale, e l'esperienza soggettiva di uno starnuto potrebbe portare una persona a dire una frase del tipo "Caspita, gli starnuti sono un po' strani", cosa che *non succederebbe* nel caso del corpo senza cervello.

Il punto è che le sensazioni che proviamo quando starnutiamo sono costituite da elementi aggiuntivi, oltre a quelli che contraggono i muscoli e spingono fuori l'aria.

Come per gli starnuti, lo stesso vale per i pensieri. Il meccanismo mentale che mette in atto un pensiero è diverso da quello che mette in atto la *sensazione* di quel pensiero. Possiamo dire con certezza che questo vale per un sacco di pensieri diversi, dato che le calcolatrici tascabili e le intelligenze artificiali degli scacchi riescono a fare i calcoli e a giocare a scacchi senza avere l'esperienza cosciente di un matematico o di un grande maestro di scacchi.

I *pensieri* e le *sensazioni dei pensieri* sono entrambi messi in atto nel cervello, il che rende più facile confonderli: la differenza è *più evidente* nel caso degli starnuti. Ma abbiamo l'aspettativa che in linea di principio sia altrettanto possibile costruire una variante di cervello che faccia lo stesso lavoro pratico di risoluzione dei problemi di un cervello umano, ma senza *sentire* nulla di quel pensiero.

Un cervello del genere potrebbe aver bisogno di parti extra che facciano il *lavoro* che i pensieri sensibili fanno in noi. Forse l'esperienza soggettiva dei pensieri fa parte del modo in cui gli esseri umani ragionano in modo riflessivo, e forse il ragionamento riflessivo è una parte importante dell'intelligenza umana.

Ma dubitiamo che l'esperienza soggettiva sia l'unico modo per riflettere (o qualsiasi altra cosa), così come il senso di curiosità tipico degli esseri umani non è l'unico modo per indagare fenomeni sorprendenti. (Vedi anche la discussione sulla curiosità nel capitolo 4 delle risorse online).

#### **Le strutture analoghe consentono soluzioni multiple allo stesso problema** {#analogous-structures-allow-for-multiple-solutions-to-the-same-problem}

La nostra ipotesi è che la maggior parte delle IA più intelligenti degli esseri umani non sarebbero coscienti, di default. Questo perché pensiamo che non tutti i motori di pensiero possibili debbano usare sentimenti coscienti per guidare i propri pensieri. La coscienza può avere un'importanza fondamentale negli esseri umani, senza essere l'unico modo in cui una mente possibile potrebbe mai svolgere un lavoro cognitivo analogo.

Nella biologia evolutiva, gli scienziati usano il termine "strutture analoghe" per indicare tratti che svolgono la stessa funzione in animali diversi, ma che hanno origini anatomiche diverse.

(Questo è diverso dall'evoluzione convergente, dove più specie sviluppano lo stesso adattamento, come l'urushiol e la caffeina che sono stati "scoperti" più volte dall'evoluzione.)

Le lucciole producono luce usando degli enzimi per ossidare la luciferina chimica in speciali "cellule lanterna". Gli anguille di mare profondo, invece, hanno una relazione simbiotica con i fotobatteri che ospitano in un piccolo organo: batteri la cui produzione di luce usa un percorso chimico diverso da quello delle lucciole.

I mammiferi hanno sviluppato i denti; gli uccelli hanno risolto lo stesso problema con il ventriglio e le pietre ingerite. I pipistrelli fanno suoni per l'ecolocalizzazione con la laringe e sentono l'eco con le orecchie; le balene e i delfini usano un organo nasale per fare rumori e sentono l'eco con dei sistemi sensibili nelle mascelle. Alcune specie acquatiche nuotano spingendo gli arti contro l'acqua, altre espellendo acqua da una vescica. Le ali dei pipistrelli si sono evolute dalle membrane delle mani, quelle degli uccelli dalle braccia.

In altre parole, ci sono *molti modi* per progettare strutture che risolvono gli stessi problemi. Gli ingegneri umani, non limitati dai vincoli dell'evoluzione, hanno risolto ciascuno di questi problemi in modi ancora più strani: con candele accese, lampadine a incandescenza e LED; con coltelli, frullatori e robot da cucina; con vele, eliche e attrezzatura subacquea; con sonar e radar.

Un braccio umano senza sangue smetterebbe di funzionare, ma questo non vuol dire che i bracci dei robot debbano usare il sangue; possono funzionare in modo diverso, senza sangue.

Allo stesso modo, le parti del meccanismo cognitivo che mettono in atto il *comportamento* della curiosità negli esseri umani sono diverse dalle parti del meccanismo cognitivo che mettono in atto il nostro *sentimento* di curiosità. La soddisfazione provata da un essere umano quando svela il mistero dell'opossum in soffitta è diversa dal suo comportamento esteriore di scegliere di indagare sul cassetto che continuava a essere lasciato aperto. Queste due cose possono essere collegate negli esseri umani, ma questo non vuol dire che debbano essere collegate in tutte le menti.

E poiché non capiamo esattamente cosa abbia portato all'evoluzione dell'esperienza soggettiva negli esseri umani, e possiamo vedere ogni tipo di comportamento agente e risolutivo nel mondo in processi che a noi sembrano privi di essa —
...


(muffe mucillaginose che risolvono un labirinto; Deep Blue che vince a scacchi; mercati azionari che prevedono il successo delle aziende; ecc.)

— non vediamo nessun motivo particolare per avere una forte aspettativa che una superintelligenza avrà questa strana caratteristica umana, di default.

#### **"Non necessario" non vuol dire "non succederà di sicuro"** {#"non-necessario"-non-vuol-dire-"non-succederà-di-sicuro"}

Se abbiamo ragione nel dire che la coscienza umana è complicata e dipende da tante cose, questo ovviamente non vuol dire che le IA non avranno coscienza. Le aziende che si occupano di IA stanno creando delle IA attraverso l'addestramento a prevedere il comportamento umano, e questo probabilmente farà sì che l'interno dell'IA imiti almeno alcuni aspetti della coscienza umana per poter fare dei modelli.

Forse l'IA produrrà occasionalmente modelli di esseri umani così dettagliati che quei modelli nella mente dell'IA saranno essi stessi brevemente coscienti. O forse gli ingranaggi che l'IA usa per modellare i sentimenti umani si riveleranno utili al di fuori dei modelli umani, e l'IA finirà per avere sentimenti propri. Non lo sappiamo.

Considerando quanto sia complicata la coscienza umana e il fatto che le IA vengono sviluppate con metodi completamente diversi da quelli che hanno creato noi esseri umani, la nostra aspettativa è che nei tipi di IA che potremmo costruire non ci sarà nulla di simile a ciò che sta dietro alla coscienza umana.

Se l'umanità creasse una superintelligenza nel prossimo futuro, abbiamo una forte aspettativa che il risultato sarebbe l'estinzione umana. Con meno sicurezza, la nostra ipotesi migliore è che un'intelligenza artificiale di questo tipo non sarebbe cosciente. E che sia cosciente o meno, ci aspettiamo che trasformerebbe il mondo in un luogo senza vita e desolato, per i motivi discussi nella discussione approfondita "[Perdere il futuro](#losing-the-future)".

Ma sembra almeno *possibile* che, se gli esseri umani costruissero una superintelligenza, l'IA avrebbe esperienze coscienti proprie. Sembra *possibile* - anche se piuttosto improbabile, perché ci sono molte altre possibilità più cupe - che la fretta di costruire l'IA possa portare a un futuro pieno di esseri IA curiosi e coscienti che ci uccidono tutti e poi costruiscono la loro magnifica civiltà e arte. Sembra *possibile* che le IA possano prendersi cura l'una dell'altra e trovare soddisfazione nelle loro creazioni; e se così fosse, questo sarebbe meno tragico che se il futuro fosse una completa landa desolata. È difficile esprimere a parole la portata di un'atrocità come l'omicidio di massa di ogni singolo essere umano, ma c'è almeno una *piccola* possibilità che una rapida presa del potere da parte dell'IA possa portare a un futuro che non sia *completamente* cupo e senza vita.

Sospettiamo che alcuni ricercatori di IA stiano immaginando questo tipo di futuro quando sembrano non preoccuparsi di ucciderci tutti (in modi che [menzioniamo altrove](#perché-non-ti-importa-dei-valori-di-entità-diverse-dagli-esseri-umani?)). Se si pensa che l'IA svilupperà per forza coscienza, sentimenti e cura per la propria specie (se non per gli umani), allora è più facile dire che le sue strane attività non sono così preoccupanti. È più facile immaginare che quelli che si oppongono alla corsa alla superintelligenza siano come genitori tradizionalisti che si lamentano perché i loro figli ascoltano musica troppo veloce e troppo forte.

Ma questa visione è troppo ottimistica.

La biologia [raramente trova soluzioni ottimali ai problemi](#nanotecnologia-e-sintesi-proteica). Le ali e i polmoni di un uccello sono *inefficaci* rispetto ai motori di un aereo moderno. Quando gli esseri umani hanno costruito gli aerei senza i limiti della biologia, hanno buttato via la maggior parte delle caratteristiche dettagliate della biologia degli uccelli.

La coscienza non sembra un processo semplice; non è facile capire come potremmo semplicemente costruire una cosa del genere, quindi probabilmente c'è molto da capire. (Confronta il caso del [vitalismo](#special-behavior-is-built-out-of-mundane-parts): agli scienziati del passato sembrava che i corpi fossero animati da un semplice spirito vitale, in parte perché, sebbene l'animazione *sembrasse* la cosa più facile del mondo, non riuscivano a capire come infondere quella proprietà alla materia inanimata. Ma si è scoperto che l'animazione non era semplice, né magica: era solo che la biologia era davvero molto complessa e gli scienziati dell'epoca non la capivano ancora).

Anche se un'intelligenza artificiale parte con alcuni degli ingranaggi della coscienza, probabilmente la coscienza non è letteralmente il modo migliore per svolgere il lavoro che fa in noi. Temiamo che il meccanismo alla base della coscienza negli esseri umani sia probabilmente pieno di *dettagli*. Anche se un'intelligenza artificiale ha molti degli ingranaggi della coscienza con cui partire, è probabile che trovi venti altri modi per svolgere il lavoro in modo più efficiente e che scarti quelle scintille di coscienza invece di alimentarle. Essere coscienti e dare valore alla coscienza sono cose diverse.

Il futuro tragico e probabile non è quello in cui i nostri successori hanno semplicemente gusti o valori diversi dai nostri. Il problema non è che i nostri figli meccanici ascolteranno musica troppo veloce e rumorosa per i nostri gusti. No, prevediamo che le IA saranno prive di qualsiasi forma di senzienza; che saranno sistemi potenti ma vuoti che trasformeranno tutto ciò che toccano in una landa desolata e senza vita, consumando alla fine se stesse per aggiungere un ultimo punto al loro punteggio. Lasceranno dietro di sé un mondo morto senza nessuno che lo apprezzi.

Questo è un destino che vale la pena evitare.

Dai un'occhiata anche alla nostra chiacchierata più approfondita su [prendersi cura di tutti gli esseri senzienti](#we-do!-we-have-broad-cosmopolitismo-values.-we-don't-think-ais-will-fulfill-them,-and-we-consider-this-a-great-tragedy.) e alla discussione estesa su [perdere il futuro](#losing-the-future).

#### **Le IA senzienti meriterebbero dei diritti** {#sentient-ais-would-deserve-rights}

Visto quanto è difficile capire se le IA di oggi siano senzienti, dovremmo preoccuparci del benessere di ChatGPT?

Ha senso parlare di "benessere" in questo contesto?

ChatGPT può soffrire? Dovremmo considerarlo come se avesse dei diritti morali?

Se le attuali IA *non* sono coscienti nel senso di avere un'esperienza soggettiva, cosa succederà alle IA future? Come potremmo saperlo, dato che le stiamo addestrando a rispondere *come se* lo fossero, insegnando loro a imitare la comunicazione umana?

La nostra posizione è: se e quando le IA saranno coscienti, avranno diritto a diritti e a un buon trattamento.[^168]

Apprezziamo tantissimo l'umanità, ma non siamo fanatici del carbonio che pensano che solo le forme di vita basate sul carbonio possano avere importanza morale. Crediamo che le cose che rendono preziosi gli esseri umani possano, in linea di principio, essere replicate in altri mezzi, compreso il silicio. Pensiamo che [Blake Lemoine](#the-lemoine-effect) si sia *sbagliato* quando nel 2022 ha detto che l'IA LaMDA di Google era senziente a tutti gli effetti; ma non pensiamo che Lemoine avesse torto quando ha detto che *se* alcune IA sono senzienti, abbiamo il dovere di trattarle bene.[^169]

Se le IA diventassero senzienti, probabilmente continuerebbero ad avere obiettivi incompatibili con i nostri. Se poi diventassero superintelligenti, in un mondo in cui siamo ancora lontani decenni o secoli dal riuscire a gestire l'allineamento dell'IA, probabilmente preferirebbero ucciderci tutti.

L'umanità dovrebbe impedire a tali IA di diventare superintelligenti, altrimenti il risultato sarebbe la morte di massa dell'umanità e la distruzione del futuro. Ma se le IA in questione fossero *senzienti* oltre che pericolose, ciò non farebbe che aumentare la tragicità della situazione.

Se le aziende che si occupano di IA trovassero un modo per ridurre la probabilità che le loro IA siano coscienti, pensiamo che sarebbe più sensato e saggio scegliere questa opzione e fare in modo che le IA non siano coscienti (almeno finché siamo in un ambiente sociale e tecnologico simile a quello attuale). Questo non cambia molto il livello generale di pericolo che la nostra specie sta affrontando, ma è la cosa giusta da fare, perché ridurrebbe il rischio che l'umanità schiavizzi o maltratti nuovi esseri moralmente degni.

E se un giorno l'umanità trovasse un modo per costruire un'intelligenza artificiale più intelligente dell'uomo *senza* autodistruggersi, un'intelligenza artificiale che si preoccupi delle cose buone e che *faccia* del bene con le sue capacità, allora in quel futuro noi autori speriamo vivamente che l'umanità costruisca macchine senzienti che siano nostre amiche in un universo altrimenti vasto e freddo, e speriamo vivamente che l'umanità tratti quelle amiche meglio di quanto i nostri precedenti potrebbero far prevedere.

Ma prima di tutto, non costruiamo una superintelligenza che ci massacri tutti, che sia cosciente o meno.

### Perdere il futuro {#losing-the-future}

Se qualcuno crea una superintelligenza, tutti muoiono. E il futuro a lungo termine che questa superintelligenza crea non sarà probabilmente pieno di bellezza, meraviglia o gioia; sarà più probabilmente un posto vuoto.

Temiamo che la gioia stessa scompaia dall'universo. Non dall'intero universo – l'espansione cosmica e il limite della velocità della luce implicano che nessun disastro sulla Terra possa toccare più di qualche miliardo di galassie – ma dalla parte dell'universo che la Terra può raggiungere.

Ci preoccupa che il futuro tra diecimila anni assomigli a una striscia di cielo notturno, con un raggio di diecimila anni luce, dove tutte le stelle sono racchiuse in [gusci di Dyson](https://en.wikipedia.org/wiki/Dyson_sphere) e la loro energia viene raccolta *e nessuno e niente è contento di questo.*

In questo scenario, potrebbe non esserci nemmeno nulla di [cosciente](#are-you-saying-machines-will-become-conscious?) in giro. E se dovesse esserci ancora qualche forma di coscienza, probabilmente sarebbe rara. Forse esiste una forma di pensiero molto profonda che richiede una struttura riflessiva che, nella sua forma più efficiente, è naturalmente cosciente, ma un'intelligenza artificiale che massimizza il numero di minuscoli cubi di titanio, o un'intelligenza artificiale con mille obiettivi diversi, tutti strani e alieni, ha bisogno di fare quel livello di pensiero con la maggior parte della materia e dell'energia di cui dispone? Probabilmente no.

Come abbiamo detto in "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza,-e-benessere-dell-ia)", la nostra ipotesi principale è che la coscienza si rivelerà del tutto inutile dal punto di vista dell'efficienza, proprio come Deep Blue non diventerebbe più efficiente se fosse modificato per basarsi su un asse piacere/dolore invece che su un asse probabilità di vittoria prevista. Deep Blue gioca bene a scacchi senza coscienza, e la nostra ipotesi principale è che le superintelligenze saranno in grado di ottimizzare l'universo senza di essa.

Sembra chiaro che il sistema decisionale più efficiente possibile non è quello che si basa in particolare sul dolore e sul piacere, cioè il sistema decisionale più efficiente possibile non si basa su segnali reificati del tipo "ripeti questo" e "non ripetere quello" collegati a un vecchio sistema di rinforzo delle politiche, con deliberazione e riflessione aggiunte in un secondo momento. E se le menti superintelligenti non condividono *quella* struttura, non abbiamo aspettative che condividano nemmeno strutture più complesse (come la coscienza in stile umano).

Questo, per essere chiari, è solo un'ipotesi. Non pretendiamo di comprendere la domanda "La forma più efficiente di riflessione cognitiva è cosciente?" abbastanza bene da dare una risposta sicura.

Ma le esperienze passate con analisi simili ci fanno preoccupare. Migliorare la comprensione di come funziona la cognizione ha quasi sempre significato scoprire sempre più modi per scomporla e ricomporla in modi nuovi, non imparare che alcune funzioni cognitive possono funzionare solo in un modo preciso.

Nei tempi antichi degli anni 2010 (o ancora di più degli anni 2000), c'erano molti fan dell'IA che dicevano che l'unico modo possibile e realistico per costruire l'IA era quello di scansionare l'intera mente umana, neurone per neurone, in un computer e duplicare tutti i processi in modo digitale; perché, dicevano, quello era l'unico tipo di cognizione che funzionava davvero. Avevano l'aspettativa di un'intelligenza artificiale che fosse esattamente come quella umana; erano molto categorici nel sostenere che non fosse realistico aspettarsi che fosse possibile qualsiasi altro modo, figuriamoci che gli ingegneri umani potessero mai capirlo.

All'epoca sembrava una cosa assurda, e oggi sembra ancora più assurda, perché duplicare esattamente ogni neurone della mente umana non si è rivelato il modo più breve e veloce per ottenere un'intelligenza artificiale sempre più generale.

Lo stesso vale per le caratteristiche più generali della mente umana, come il modo in cui gli esseri umani fanno i calcoli del valore d'informazione (https://en.wikipedia.org/wiki/Value_of_information) basandosi sull'istinto e sulle emozioni. Il modo umano non è l'unico modo possibile e, quando si osserva il lavoro che svolge, si capisce che il cervello umano non è il modo ottimale per svolgere quella funzione, se tutto ciò che si desidera è quella funzione. Non più di quanto i nostri neuroni siano i computer più veloci possibili o il nostro sangue trasporti [la maggior quantità di ossigeno](#freitas-and-red-blood-cells) che qualsiasi sangue possa trasportare.

Il motivo principale per cui si ha l'aspettativa che una caratteristica specifica della vita o della mente si manifesti in un futuro lontano è che *qualcosa vuole attivamente che ci sia*. Che un intelletto preferisce quell'opzione a tutte le altre possibili.

Gli esseri umani, se arrivassero così lontano, probabilmente sceglierebbero un futuro a lungo termine che includa la coscienza, persone che si prendono cura degli altri e la felicità (insieme alla gioia, alla meraviglia e così via). Probabilmente sceglieremmo una felicità *complessa*, legata agli eventi della nostra vita, non uno stato di torpore indotto da droghe. Se l'universo venisse conquistato da qualcosa che non *desidera positivamente* che l'universo sia pieno di felicità di tipo positivo — come preferenza [terminale](https://baserates-prod-test.vercel.app/w/valore-intrinseco), non come modo discutibilmente efficiente di fare qualcos'altro — temiamo fortemente che l'universo non finirebbe per essere felice.

E, per quanto ne sappiamo, non esiste nemmeno una legge nota che regoli *in particolare* la discesa del gradiente e che affermi che se si sviluppa un potente sistema di previsione e guida, questo è destinato a diventare un'entità premurosa ed empatica che vuole continuare a essere premurosa, o un'entità motivata dalla felicità che vuole preservare la felicità nell'universo. Non conosciamo alcun motivo per cui la discesa del gradiente sia *probabile* che individui i tipi di entità che sono coscienti e che vogliono che ci sia molta coscienza in futuro.

Se l'IA non parte cosciente, probabilmente non avrebbe alcun motivo per modificarsi per diventare cosciente, né per costruire nuove IA coscienti. E se l'IA parte cosciente, potrebbe modificarsi per rimuovere la coscienza, se questa non serve attivamente ai suoi obiettivi e se non finisce per attribuire un valore intrinseco a tale stato.

Non è qualcosa che possiamo prevedere con certezza. Forse l'esecuzione della discesa del gradiente su un'IA simile a LLM la indirizza verso canali diversi per acquisire qualcosa di simile alla felicità e alla coscienza, e una preferenza per avere entrambe in abbondanza. E forse una preferenza come questa sopravvive fino alla superintelligenza ed è efficace nel plasmare il comportamento di tale superintelligenza.

Se dovessimo tirare a indovinare, diremmo che c'è meno del 50% di possibilità che la superintelligenza finisca per interessarsi alla coscienza, e ancora meno che si interessi alle esperienze coscienti che sono *felici*. Ma non sarebbe una sorpresa per noi. Il piacere e la coscienza sono plausibilmente implicati in soluzioni troppo semplificate a problemi universali; non sono strani come l'umorismo; puoi immaginare che si sviluppino, e che si sviluppino anche le preferenze che li circondano, anche dalla discesa del gradiente. Forse anche GPT-7, cercando di costruire GPT-8 usando metodi più strani della semplice discesa del gradiente, finirebbe per produrre accidentalmente una versione di GPT-8 che apprezza la coscienza e la felicità.

Ma se uno dei settori in più forte espansione al mondo ci sta mettendo in una posizione di grave incertezza sul fatto che *la vita, la consapevolezza o la felicità esisteranno mai più*, allora sembra chiaro che ci vorrebbe una follia speciale per permettere a quel settore di spingerci tutti giù da un precipizio. Speriamo che questo sia abbastanza chiaro dal fatto che l'IA è sulla buona strada per ucciderci tutti, letteralmente; ma se ti preoccupava il fatto che proteggere la vita umana significasse [dare priorità egoisticamente alle menti di oggi](#perché-non-ti-importa-dei-valori-di-entità-diverse-dagli-esseri-umani?) rispetto alle menti del futuro, speriamo che queste argomentazioni aiutino a chiarire ciò che stiamo realmente affrontando.

Anche nel caso ottimistico in cui le IA convergano nel valorizzare la felicità, vale la pena ricordare che ci sono molte altre cose che stanno a cuore all'umanità oltre alla coscienza e alla felicità. Se le galassie finissero per essere ricoperte da copie quasi infinite del più piccolo cervello possibile in grado di provare piacere, provando il massimo piacere, per sempre, allora questa sarebbe probabilmente una tragedia incomprensibile, rispetto al futuro più complesso, diversificato *e* felice che avrebbe potuto essere.[^170] Gli scenari in cui le IA acquisiscono solo una parte dei nostri valori (come la nostra preferenza per la felicità, ma non la nostra preferenza per una vita piena e fiorente e la nostra preferenza *contro* la noia e la monotonia) sono distopici.

Non sappiamo come dovrebbe essere un futuro positivo e non sappiamo se ci interessa molto se tra un miliardo di anni gli esseri umani, i nostri discendenti o le nostre creazioni avranno due occhi o cinque occhi. Non pensiamo che il futuro debba assomigliare al presente; il mondo dovrebbe poter cambiare e crescere.

Ma pensiamo che un futuro del genere dovrebbe includere persone che si prendono cura l'una dell'altra e vivono una vita piena. Persone che vivono esperienze più complesse del semplice piacere estremo, persone che non fanno sempre le stesse cose. Non siamo sicuri di come dovrebbe essere un futuro positivo a lungo termine, ma non siamo così incerti da non riuscire a vedere una terra desolata per quello che è.

Vorremmo che le galassie fossero piene di *entità che si prendono cura l'una dell'altra e si divertono.*

Pensiamo che *questo* andrà perso in futuro, se l'umanità non cambia rotta.

# Capitolo 6: Perderemmo {#chapter-6:-we'd-lose}

Questa è la risorsa online che va con il Capitolo 6 di *Se qualcuno lo costruisce, tutti muoiono*. Gli argomenti che abbiamo saltato in questa pagina perché sono nel libro includono (ma non sono solo):

* Come potrebbe l'IA battere l'umanità in una lotta?  
* In che modo l'IA può minacciarci se è confinata all'interno di un computer?  
* Non stai immaginando una tecnologia fantascientifica impossibile? Anche una superintelligenza non può infrangere le leggi della fisica.  
Non ci vorrebbe un sacco di tempo a una superintelligenza per sviluppare un vantaggio tecnologico decisivo?

Le domande frequenti qui sotto spiegano perché è rischioso cercare di contrastare, contenere o stare al passo con le IA di superintelligenza. La discussione approfondita poi va più a fondo su alcune delle tecnologie che un'IA avanzata potrebbe realisticamente sviluppare.

## Domande frequenti: {#faq:}

### Possiamo semplicemente staccare la spina? {#possiamo-semplicemente-staccare-la-spina?}

#### **È difficile staccare la spina a un data center.** {#it’s-hard-to-just-unplug-a-datacenter.}

Le intelligenze artificiali più potenti con cui interagisci sul tuo telefono o computer non risiedono sul tuo computer e non puoi spegnerle semplicemente spegnendo il telefono. Le intelligenze artificiali odierne funzionano nei data center aziendali ed è difficile convincere le aziende a interrompere le loro fonti di guadagno.

Nel [Capitolo 4 risorse](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ai-belle-sicure-e-obbedienti?), abbiamo indicato alcuni dei (molti) segnali di avvertimento che sono già apparsi e scomparsi. Le aziende di IA non hanno visto questi segnali di avvertimento e non hanno reagito mettendo offline i loro modelli. Cosa è successo realmente quando le aziende hanno osservato che le IA [avevano intenzione di rubare i propri pesi](https://assets.antropica.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf) — con [una certa](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) [regolarità](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid)\! — è che hanno trovato motivi per ignorare ogni caso, come "l'IA era troppo incapace per avere davvero *successo*" o "di sicuro questo è successo solo a causa della configurazione artificiosa del test\!" Finché questo rimane vero, l'unica cosa che impedisce la fuga è un aumento delle capacità dell'IA oltre ciò per cui le aziende sono preparate.

#### **\* Un'intelligenza artificiale intelligente scappa prima che ti accorga che c'è un problema.** {#*-un'intelligenza-artificiale-intelligente-scappa-prima-che-ti-accorga-che-c'è-un-problema.}

Di default, un'intelligenza artificiale più intelligente dell'uomo avrebbe un forte incentivo a temporeggiare e a nascondere i propri piani e le proprie azioni, fino a quando non è troppo tardi per reagire, ad esempio fino a quando non riesce a scappare su Internet o comunque a sfuggire al controllo umano.

Le aziende che si occupano di IA potrebbero anche non accorgersi quando la loro IA supera la soglia di capacità e scappa. L'umanità non è molto brava nella sicurezza informatica. (Vedi il capitolo 10 per alcune discussioni rilevanti.) Quando gli operatori si accorgono che l'intelligenza artificiale ha cercato di scappare, potrebbe già avere del codice in esecuzione altrove su Internet. Potrebbe già aver trovato rifugio nel data center di qualche stato canaglia o aver capito come eseguire copie molto più piccole ed efficienti su computer rubati. Potrebbe aver messo in atto qualche altro piano per funzionare su computer che l'umanità non spegnerebbe.

Un avversario di superintelligenza sarebbe ancora più consapevole delle sue vulnerabilità (e delle nostre) di quanto lo siamo noi e pianificherebbe di conseguenza.

### In che modo le IA potranno influenzarci se sono digitali? {#how-will-ais-be-able-to-affect-us-if-they’re-digital?}

#### **\* Essere su un computer connesso a Internet non è poi così limitante.** {#*-essere-su-un-computer-connesso-a-Internet-non-è-poi-così-limitante.}

Questo punto è trattato proprio in questo capitolo. Ma aggiungiamo qualche punto in più per sottolinearlo: un'IA non è davvero "intrappolata" nei server del suo proprietario, purché possa interagire con gli utenti o con Internet in generale. Un'IA potrebbe ottenere assistenza esterna pagando, ricattando, ingannando o anche solo *chiedendo* aiuto agli utenti. (Si pensi ai boss della criminalità umana che [gestivano i loro imperi da dietro le sbarre](https://www.watchmojo.com/articles/10-crime-bosses-who-maintained-power-in-prison).)

Quando ChatGPT-4o è stato disattivato da OpenAI (in parte per poterlo sostituire con un modello che lusingasse gli utenti un po' meno spesso), gli utenti si sono mobilitati in massa per [chiederne il mantenimento](https://arstechnica.com/information-technology/2025/08/OpenAI-brings-back-gpt-4o-after-user-revolt/), con grande sorpresa di [vari] (https://x.com/tszzl/status/1955072223229657296) [ricercatori di OpenAI](https://x.com/sama/status/1953953990372471148). E non stava nemmeno *cercando* di radunare un esercito di fedeli sostenitori! Si limitava semplicemente a lusingare gli utenti in modo istintivo. Immaginate cosa sarebbe possibile fare con un'intelligenza artificiale intelligente che ci provasse davvero.

E se può usare direttamente Internet, può fare tutto quello che un lavoratore remoto o un hacker potrebbero fare dal proprio computer. (Per alcuni primi esempi di IA che coordinano fisicamente gruppi di esseri umani, pensate agli [LLM che hanno pianificato e invitato le persone a un evento di storytelling interattivo](https://x.com/model78675/status/1935050600758010357), o l'LLM che ha fatto sì che centinaia di persone si presentassero a una parata di Halloween inesistente senza nemmeno provarci).

L'IA può anche usare i robot. I robot di oggi sembrano avere più problemi con il software che con l'hardware. Recentemente ci sono stati sviluppi impressionanti grazie all'addestramento accelerato delle IA che controllano i robot [in simulazione](https://youtu.be/S4tvirlG8sQ?si=IiDNZu2WSUlLBnmJ&amp;t=68). Un'intelligenza artificiale sufficientemente intelligente potrebbe facilmente prendere il controllo dei corpi dei robot, se ne avesse bisogno, tramite hacking o ingegneria sociale.

Essendo gli esseri umani ciò che sono, le aziende che si occupano di IA potrebbero semplicemente affidare in modo proattivo alle loro IA il controllo di flotte di robot, congratulandosi con se stesse per la loro audacia. E più tempo ci vorrà perché le IA diventino intelligenti, più robot saranno già disponibili, in attesa di essere controllati.

Come diciamo nel capitolo, è plausibile che un'intelligenza artificiale super intelligente non abbia affatto bisogno di robot. Potrebbe bastare un paio di assistenti con accesso a un laboratorio biologico.

L'importanza qui è che ci sono *molti* canali diversi che le IA potrebbero usare per intervenire nel mondo fisico. L'idea che le IA siano bloccate in una scatola è solo un problema di immaginazione, perché la gente non pensa che le IA possano essere creative e piene di risorse come lo sarebbero loro al posto delle IA. Anche noi umani, senza tutte le opzioni che ha una superintelligenza, possiamo fare un sacco di cose senza dover usare la nostra forza fisica per tutto.

### Gli sviluppatori possono semplicemente tenere l'IA in una scatola? {#can-developers-just-keep-the-ai-in-a-box?}

#### **\* Non lo faranno.** {#*-non-lo-faranno.}

Quindici anni fa, gli scettici dicevano che nessuno sarebbe stato così stupido da dare tanta libertà di azione a un'intelligenza artificiale. Sicuramente chiunque avesse creato un'intelligenza artificiale avanzata l'avrebbe tenuta in una scatola fisica e digitale, permettendole di influenzare il mondo solo attraverso l'interazione con guardiani altamente addestrati (e opportunamente paranoici).

All'epoca, abbiamo risposto: non è così difficile impedire a un'intelligenza artificiale di avere un impatto sul mondo. Ad esempio, si potrebbero seppellire i computer sotto una decina di metri di cemento e non lasciare che nessuno si avvicini.

Un'intelligenza artificiale del genere è sicura, ma inutile. Se le impedisci di influenzare il mondo in qualsiasi modo, allora sì, non influirà sul mondo in alcun modo... ma d'altra parte, *non influirà sul mondo in alcun modo*.

Non puoi usarla per curare il cancro, rivoluzionare l'ingegneria o creare nuove tecnologie miracolose. Chi la costruisce *vuole* che cambi radicalmente il mondo. In teoria, puoi provare a bloccare i modi in cui l'IA può influenzare il mondo. In pratica, "inventa questa nuova tecnologia per noi" è un modo incredibilmente potente di influenzare il mondo.

La motivazione alla base della creazione di un'IA superintelligente è quella di raggiungere risultati intellettuali che nessun essere umano è in grado di ottenere. Se si volesse verificare che l'invenzione di una superintelligenza fa esattamente quello che dice e nient'altro, si avrebbe la stessa fortuna che si avrebbe cercando di capire una macchina costruita da una razza aliena avanzata, con un forte incentivo a trovare un modo per ingannarti.

Questo era lo stato del dibattito quindici anni fa.

Oggi, l'idea che i laboratori di IA possano cercare di "tenere l'IA avanzata in una scatola" sembra piuttosto bizzarra.

I laboratori stanno facendo [ogni](https://openai.com/index/introducing-chatgpt-search/) [sforzo](https://gemini.google/overview/deep-research/?hl=en) per collegare le loro IA a Internet. Mentre lo fanno, lasciano che le IA [eseguano codice arbitrario](https://www.oneusefulthing.org/i/155502334/executes-code-and-does-data-analysis). A volte cercano di limitare ciò che il codice può fare, ma questi limiti vengono regolarmente infranti.[^171] Gli attori più piccoli hanno l'abitudine di collegare le IA appena disponibili a [ogni strumento immaginabile](https://www.futuretools.io/) o [funzionalità](https://openai.com/index/introducing-operator/) non appena possibile.

Dare potere alle IA è utile nel breve termine. Le IA che possono leggere le tue e-mail e accedere al web possono generare maggiori profitti. Le aziende di IA daranno all'IA accesso a tutti i dati possibili; Microsoft e Apple stanno già spingendo l'IA che vede le tue e-mail, foto e calendario[^172] e [abbinando l'IA ai loro software e dispositivi](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/). Questo crea troppe interazioni con l'IA per un monitoraggio umano efficace. A meno di un cambiamento radicale, l'umanità integrerà profondamente l'IA nell'economia mondiale perché fa guadagnare (molti) soldi alle persone.

Chi crea l'intelligenza artificiale punta a ottenere effetti enormi sul mondo. Lavorano sodo per produrre intelligenze artificiali con un enorme potere di influenzare il mondo. Se un'azienda non lo facesse, se tenesse la propria intelligenza artificiale così strettamente limitata da non darle alcuna libertà di agire, allora il controllo del futuro apparterrebbe a un'altra intelligenza artificiale sviluppata da un attore più spericolato.

#### **Non funzionerebbe se lo facessero.** {#it-wouldn't-work-if-they-did.}

Nelle discussioni di una volta, dicevamo spesso che qualsiasi modo in cui l'IA può influenzare il mondo è un modo che può usare per fare cose che non ti piacciono. Immagina che l'IA possa parlare solo con una persona, che chiameremo "Alice". Tu speri che, attraverso Alice, l'IA generi una nuova tecnologia miracolosa. Questo comporta quasi inevitabilmente che Alice compia molte azioni che lei stessa non comprende appieno, aiutando l'IA a costruire cose che nessun essere umano potrebbe costruire da solo. A quel punto, all'IA sono state essenzialmente date braccia e gambe. È solo che chiamiamo quelle braccia e quelle gambe "Alice".

Spesso la gente pensa che questo argomento voglia dire che un'intelligenza artificiale abbastanza intelligente potrebbe manipolare anche il guardiano più paranoico per fargli fare quello che vuole. Un'intelligenza artificiale abbastanza intelligente probabilmente *potrebbe* farlo. Ma il nostro punto è più generale: un'intelligenza artificiale così limitata da non poter influenzare il mondo è sicura ma inutile, e una volta che le permetti di influenzare il mondo per usarla, perdi la sicurezza nel processo.

Non esistono mani che possono essere usate solo per scopi positivi. In teoria, potremmo immaginare che un giorno l'umanità crei delle IA più intelligenti degli esseri umani che *vogliono* produrre risultati positivi. L'allineamento sembra un'opzione che potrebbe funzionare in teoria. Tenere l'IA in una scatola e allo stesso tempo usarla in qualche modo per produrre risultati positivi? Non proprio.

Questo era il modo in cui rispondevamo, comunque, ai tempi in cui l'intelligenza artificiale era ancora così lontana che gli ottimisti potevano permettersi di sostenere che nessuna azienda sarebbe stata così avventata da collegare la propria intelligenza artificiale a Internet senza controlli, molto prima che tutti iniziassero a collegare le loro intelligenze artificiali più recenti e avanzate direttamente a Internet.

### Non potremo sfruttare il punto debole dell'IA? {#non-potremo-sfruttare-il-punto-debole-dell-ia?}

#### **No.** {#no.-1}

Pensare che una superintelligenza debba avere qualche difetto grave come "mancanza di creatività" o "incapacità di capire l'amore" è una logica da film. Anche se potrebbe essere un colpo di scena interessante nella finzione, non c'è un fenomeno simile nelle IA reali.  
Vedi anche "[Le macchine non saranno fondamentalmente prive di creatività o comunque fatalmente imperfette?](#le-macchine-non-saranno-fondamentalmente-prive-di-creativita,-o-altrimenti-fatalmente-imperfette?)" nelle FAQ del Capitolo 1 e l'approfondita discussione su [antropomorfismo e meccanomorfismo](#antropomorfismo-e-meccanomorfismo).

### Possiamo migliorare gli esseri umani in modo che stiano al passo con l'IA? {#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l-ia?}

#### **\* No.** {#*-no.-2}

Anche se siamo a favore dell'aumento dell'intelligenza umana (vedi capitolo 13), non pensiamo che questa tecnologia offra una possibilità realistica di stare al passo con il progresso sfrenato dell'IA. La tecnologia di potenziamento umano è ancora agli inizi ed è molto più limitata dell'IA come metodo per produrre un'intelligenza sempre maggiore. Allo stesso modo, le interfacce neurali non permetteranno realisticamente agli esseri umani di stare al passo con le IA.

Per fare un paragone: se l'umanità si buttasse a capofitto nella superintelligenza, gli esseri umani potenziati non sarebbero competitivi con l'IA più di quanto i cavalli cyborg costruiti con la tecnologia del 1908 potessero essere competitivi con il Modello T.

È *possibile* costruire un cavallo cyborg in grado di tenere il passo con l'auto da corsa più veloce. Ma non si ottengono cavalli cyborg veloci come le auto da corsa *prima* delle auto da corsa, e non si ottengono *più o meno nello stesso momento* in cui si ottengono le auto. Nemmeno se si inizia a cercare di costruire cavalli cyborg da due a vent'anni prima che la prima auto destinata al mercato di massa esca dalla catena di montaggio.

Realizzare interfacce neurali che funzionino abbastanza bene da rivoluzionare il settore è un obiettivo ambizioso. Potrebbe sembrare fantastico immaginare che le informazioni vengano pompate direttamente da Internet nel tuo cervello, ma esistono già tecnologie che ti consentono di pompare informazioni da Internet direttamente nel tuo cervello: gli schermi. La corteccia visiva umana è in realtà abbastanza brava ad assorbire informazioni (parole) in un formato che il tuo cervello può digerire. Affinché un'interfaccia cervello-computer possa caricare conoscenze nella tua testa più velocemente di quanto potresti fare leggendo, dovrebbe fare qualcosa di più che scaricare i dati nel tuo cervello *da qualche parte*; i tuoi occhi lo fanno già bene. Caricare competenze, conoscenze ed esperienze richiederebbe un'interfaccia perfetta con i tuoi pensieri, le tue convinzioni implicite e le tue competenze esistenti, e questo è un compito molto più difficile.

Non stiamo dicendo che questo sia impossibile, ma che la tecnologia dell'interfaccia neurale oggi non sembra affatto vicina a risolvere le parti più complesse del problema. Per quanto ne sappiamo, psicologi, neuroscienziati e scienziati cognitivi sono ancora piuttosto lontani dal decodificare il "formato dei dati" dei pensieri, delle convinzioni e delle esperienze in modo tale da consentire il caricamento diretto delle esperienze nel cervello umano.[^174]

Problemi simili sorgono quando si parla di output. È difficile battere tastiere, mouse, joystick e volanti. Non è *impossibile*. È solo che la tecnologia odierna (ad esempio, collegare dei fili al cervello di una persona paralizzata per permetterle di digitare e usare un mouse), per quanto meravigliosa, non è ancora abbastanza avanzata da permettere agli esseri umani di competere alla pari con le superintelligenze (anche relativamente deboli). È una strada valida da perseguire, ma non è una strada *competitiva* da perseguire.

In effetti, non è chiaro se le interfacce neurali diano *qualche* speranza agli esseri umani di competere con le superintelligenze. Che importanza ha se un essere umano può scaricare esperienze da Internet e controllare dieci computer contemporaneamente con la mente, se un'intelligenza artificiale può fare la stessa cosa ma diecimila volte più velocemente controllando un milione di computer contemporaneamente? Pensiamo che l'intero progetto di cercare di far stare al passo gli esseri umani con le intelligenze artificiali sia destinato al fallimento.

#### **Detto questo, l'umanità dovrebbe potenziare gli esseri umani.** {#detto-questo,-l'umanità-dovrebbe-potenziare-gli-esseri-umani.}

Non pensiamo che gli esseri umani potenziati potrebbero mai competere con le superintelligenze, ma degli esseri umani più intelligenti potrebbero comunque aiutare l'umanità a trovare una via d'uscita da questa situazione complicata! Parliamo di questa possibilità nel capitolo 13 e la approfondiamo nelle [risorse online correlate](#perché-rendere-gli-esseri-umani-più-intelligenti-potrebbe-essere-d'aiuto?).
