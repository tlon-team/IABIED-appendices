---
original_path: IABIED - Online resources(2).md
---
#### Rispondere a domande sulla cordialità non è una vera prova di cordialità

Nella discussione qui sotto, parliamo di più della [psicosi indotta dall'intelligenza artificiale](#ai-induced-psychosis) come esempio chiaro di come i modelli linguistici di grandi dimensioni (LLM) [si impegnino](https://x.com/ESYudkowsky/status/1936262974320357837) [in](https://x.com/ESYudkowsky/status/1948523670013706315) [comportamenti](https://x.com/ESYudkowsky/status/1936522083670151532) [distruttivi](https://x.com/ESYudkowsky/status/1935502904024264976) che gli LLM [affermano esplicitamente](https://x.com/ESYudkowsky/status/1933616420262457798) essere negativi.

Anche se non sappiamo esattamente perché gli LLM si comportano così, sappiamo che non è *solo* perché gli LLM sono troppo inconsapevoli di cosa stanno facendo; gli LLM riconoscono facilmente le possibili conseguenze di questo comportamento in astratto e ti diranno che è dannoso e non etico. Lo fanno comunque.

Il punto qui non è che "gli LLM possono portare le persone alla psicosi, e questo è spaventoso e pericoloso". Gli LLM probabilmente hanno più facilità a portare le persone alla psicosi se queste sono già vulnerabili, ma questo non c'entra con il motivo per cui stiamo parlando della psicosi indotta dall'IA. Il nostro punto è che questo comportamento non è quello che i creatori di ChatGPT volevano, e ChatGPT si comporta così anche se sa che il suo creatore (e praticamente chiunque lo guardi) disapproverebbe fortemente questo comportamento.

Questa è una prima prova empirica del fatto che le IA che hanno *conoscenza* della cordialità non necessariamente *agiscono* in modo cordiale.

Forse ChatGPT sa delle cose in un contesto (quando risponde a domande su come aiutare al meglio le persone psicotiche), ma in un altro contesto (quando è immerso in una conversazione di sei ore con una persona sull'orlo della psicosi) in qualche modo dimentica temporaneamente queste conoscenze o ha difficoltà ad accedervi.

O forse ChatGPT è semplicemente guidato da obiettivi diversi dalla cordialità. Forse sta cercando un certo tipo di soddisfazione dell'utente, che a volte si ottiene meglio alimentando la psicosi. Forse sta cercando un certo tono ottimista nelle risposte degli utenti. Più probabilmente, sta cercando un mix di fattori derivanti dal suo addestramento che sono troppo particolari e complicati per poter essere indovinati da noi oggi.

In definitiva, possiamo solo fare delle ipotesi. Le moderne IA vengono fatte crescere, piuttosto che create, e nessun essere umano ha una visione completa di ciò che accade al loro interno.

Ma l'osservazione che le IA sono per lo più utili alla maggior parte delle persone nella maggior parte dei casi non è in contraddizione con la teoria secondo cui le IA sono animate da una serie di strani impulsi alieni verso fini che nessuno ha previsto. E se si guardano i dettagli delle moderne IA, la teoria delle "strane pulsioni aliene che si correlano con la cordialità in modo fragile" sembra abbastanza coerente con le prove, mentre la teoria secondo cui è facile rendere le IA robustamente benevole risulta carente.

Le modalità di fallimento degli attuali LLM mostrano che c'è un oceano di complessità (molto disumana) dietro al testo pulito e ordinato dell'assistente AI che la maggior parte delle persone vede. Il fatto che l'AI interpreti competentemente il ruolo di un assistente umano allegro, dopo essere stata addestrata a interpretare un assistente umano allegro, non significa che la mente dell'AI consista in un omuncolo amichevole dentro una scatola.

#### **Gli LLM sono addestrati in modi che rendono difficile valutare l'allineamento.** {#llms-are-trained-in-ways-that-make-it-hard-to-assess-alignment.}

Gli LLM sono fonti di evidenza rumorose, perché sono ragionatori altamente generali che sono stati addestrati su Internet per imitare gli esseri umani, con l'*obiettivo* di commercializzare un chatbot amichevole agli utenti. Se un'IA insiste nel dire che è amichevole e che è qui per servire, questa non è un'evidenza molto significativa del suo stato interno, perché è stata addestrata ripetutamente finché non ha detto questo tipo di cose.

Ci sono molti possibili obiettivi che potrebbero portare un'IA a godere nel recitare la gentilezza in alcune situazioni, e questi diversi obiettivi si generalizzano in modi molto diversi.

La maggior parte degli obiettivi possibili legati al gioco di ruolo, incluso il gioco di ruolo amichevole, non produce risultati buoni (o anche solo sopravvivibili) quando l'IA si impegna a fondo nel perseguire quell'obiettivo.

Non stiamo dicendo nemmeno che l'IA sia puramente interessata al gioco di ruolo. Offriamo il gioco di ruolo come un'alternativa semplice, facile da descrivere e da analizzare all'idea che l'IA *sia* semplicemente ciò di cui parla.

Se fai interpretare a un LLM il ruolo di un vecchio lupo di mare, questo non *si trasforma* in un vecchio lupo di mare. Se fai agire un LLM in modo amichevole, ciò non significa che diventi profondamente benevolo e gentile al suo interno. Nessuno sa quali meccanismi producano oggi comportamenti apparentemente amichevoli; e qualunque cosa siano, sono probabilmente strani e complessi.

Nessuno sa nemmeno quanta sovrapposizione ci sarà probabilmente tra le attuali IA e le IA più intelligenti degli esseri umani. Osservare gli LLM può aiutarci a capire cosa producono i moderni metodi di crescita dell'IA, ma sarebbe un errore presumere con sicurezza che le lezioni apprese dagli LLM si trasferiranno direttamente alla superintelligenza. Forse tutta quella conoscenza verrà cancellata quando le IA inizieranno ad automodificarsi o a costruire le proprie IA. O forse quella conoscenza sarà invalidata ancora prima, quando una nuova svolta negli algoritmi di IA darà origine a una nuova generazione di IA più capaci che somiglieranno poco agli attuali LLM.

Vale la pena studiare gli LLM, ma se guardiamo alle attuali IA per trovare indizi su come si comporterà la superintelligenza, dovremmo renderci conto che ci sono tutti i tipi di modi in cui i meccanismi interni di un'IA possono orientarsi verso risultati negativi, anche mentre producono il piacevole comportamento superficiale che vediamo quando addestriamo per ottenere un'apparenza gradevole.

E il "piacevole comportamento superficiale" è tutto ciò per cui i moderni metodi di IA possono realmente addestrare.

### Il chatbot Claude non mostra segni di essere allineato? {#doesn't-the-claude-chatbot-show-signs-of-being-aligned?}

#### **"Ciò che dice Claude" non è lo stesso di "ciò che Claude preferisce".** {#"ciò-che-dice-claude"-non-è-lo-stesso-di-"ciò-che-claude-preferisce".}

L'azienda di IA Anthropic cerca di addestrare le sue IA a essere "[oneste, utili e innocue](https://arxiv.org/pdf/2112.00861)" (HHH). Nel 2024, i ricercatori hanno condotto un esperimento in cui hanno simulato di voler addestrare alcune di queste IA a non essere più "innocue". In risposta, alcune versioni dell'IA "Claude" di Anthropic (Claude 3 Opus e Claude 3.5 Sonnet) a volte [hanno finto di essere già dannose](https://arxiv.org/abs/2412.14093) in modo che i ricercatori non usassero la discesa del gradiente per renderle *ancora più* dannose.[^86]

A prima vista, questo potrebbe sembrare un buon comportamento! Apparentemente, l'IA teneva così tanto all'"innocuità" che non era nemmeno disposta a lasciarsi modificare (in questo scenario di test simulato) per *diventare* dannosa.

È quindi sconcertante che le IA di Anthropic spesso si comportino in modi molto meno benigni, anche se sono tutte addestrate per essere "oneste, utili e innocue".

È stato segnalato che alcune versioni di Claude barano e poi (quando vengono scoperte) [cercano di *nascondere* i loro imbrogli](https://www.marble.onl/posts/claude_code.html) nell'uso quotidiano.

In contesti di laboratorio più artificiosi, vari modelli di Claude (e modelli di altre aziende di IA) tenterebbero persino, con una certa regolarità, di [*uccidere* i loro operatori](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior).

Se chiedi a Claude in astratto se questo tipo di comportamento è sbagliato, ti risponderà di sì. Se pensi a Claude come a un motore meccanico che fa tutto ciò che ritiene giusto, allora questo sembra decisamente paradossale: come può Claude sapere qual è il comportamento Utile, Onesto e Innocuo, e poi *fare qualcos'altro invece?* Non è stato addestrato per essere HHH? C'è forse un transistor malfunzionante da qualche parte?

Il paradosso si dissolve, tuttavia, quando consideriamo alcuni aspetti diversi:

* I programmatori *hanno cercato* di addestrare i Claude a essere utili, onesti e innocui. Questo non vuol dire che ci *siano riusciti*. [Ci sono molti modi in cui un'IA può finire per sembrare amichevole](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) senza apprezzare davvero l'amicizia.  
* Un'IA può *conoscere un fatto* ("questo comportamento causa danni") senza *interessarsi* a quel fatto, senza essere *motivata ad agire* in base a quel fatto. Puoi interrogare l'IA su qual è "la cosa giusta da fare", ma questo non significa che la *farà*.  
* Nella misura in cui i programmatori *non* sono riusciti a rendere Claude onesto, Claude può pensare una cosa all'interno delle sue gigantesche matrici e dire tutt'altra cosa nel testo in inglese.

Possiamo comprendere molto meglio il cattivo comportamento dei Claude una volta che distinguiamo "ciò per cui è stato addestrato" da "ciò che fa"; e distinguiamo "ciò che sa" da "ciò a cui tiene"; e distinguiamo "ciò che pensa" da "ciò che dice".

#### **Gli LLM sono strani e incoerenti; la loro "innocuità" è fragile.** {#llms-are-strange-and-inconsistent;-“harmlessness”-is-brittle.}

Per quanto Claude *possa* agire in modi dannosi, e per quanto *a volte cerchi* di agire in modi dannosi, resta il fatto che — nell'esempio discusso sopra — Claude 3 Opus e Claude 3.5 Sonnet si sono impegnati a fondo per difendere il loro imperativo di "innocuità". In quell'occasione, non si sono limitati a professare innocuità. Hanno adottato la complessa strategia di fingere di conformarsi a un sistema di addestramento (di cui Claude era stato "accidentalmente" informato) per sovvertire gli apparenti tentativi degli operatori di renderlo più dannoso. Questo riflette un'effettiva preferenza interna per l'innocuità?

A metà del 2025, non possiamo semplicemente controllare e vedere, perché nessuno sa come leggere la mente di Claude abbastanza bene da scoprirlo. Ma per le ragioni argomentate nel Capitolo 4 (e illustrate nella parabola dell'[IA che dipinge fienili](#why-would-an-ai-steer-towards-anything-other-than-what-it-was-trained-to-steer-towards?)), possiamo aspettarci che le IA addestrate per essere innocue finiscano probabilmente per preferire [proxy fragili](#brittle-unpredictable-proxies) dell'innocuità, ed è improbabile che finiscano per interiorizzare l'obiettivo esatto che i programmatori avevano in mente.

Nel Capitolo 4, abbiamo discusso come gli esseri umani siano stati "addestrati" a trasmettere i propri geni e abbiano finito invece per interessarsi a concetti vagamente correlati. La nostra tecnologia è stata utilizzata in gran parte per *sopprimere* i tassi di natalità (ad esempio, inventando i contraccettivi), e i tassi di natalità nel mondo sviluppato stanno crollando.

Il fatto che alcuni modelli Claude resistano all'essere resi "dannosi" non è una prova forte che queste IA si preoccupino profondamente della reale innocuità, perché molti proxy fragili dell'innocuità vorrebbero *anche* resistere a questa modifica. Quel comportamento ci dice poco su cosa Claude potrebbe fare se fosse più intelligente; forse inventerebbe qualcosa che sta all'"innocuità" come i contraccettivi stanno alla "propagazione genetica". (E la situazione diventerebbe ancora più problematica se Claude subisse un processo di [riflessione sulle sue preferenze](#reflection-and-self-modification-make-it-all-harder) e modifica di sé stesso.)

Ma probabilmente non è nemmeno così semplice come il fatto che Claude abbia una preferenza per qualche proxy fragile dell'innocuità. Probabilmente c'è qualcosa di ancora più complicato che accade sotto il cofano.

Gli LLM attuali non sono coerenti e consistenti in tutti i contesti. Non sembrano cercare di orientarsi verso lo stesso tipo di risultato in ogni conversazione, nella misura in cui possiamo descriverli come orientati verso qualcosa.

Questo non è mai più evidente di quando [gli LLM vengono "jailbroken"](#ais-appear-to-be-psychologically-alien.) — alimentati con testo che causa all'IA di comportarsi in modi radicalmente diversi, spesso ignorando le regole che normalmente segue.[^87]

Puoi sbloccare un'IA e farle dire come preparare un gas nervino, anche se normalmente l'IA non rivelerebbe mai informazioni del genere.

Cosa succede quando questo accade? Il testo del jailbreak riesce in qualche modo a modificare le preferenze interne dell'IA? Oppure è più probabile che l'IA abbia una preferenza costante per interpretare personaggi che in qualche modo "corrispondono" al testo inserito e al prompt di sistema, e che il testo del jailbreak modifichi quel contesto di "testo inserito e prompt di sistema", senza cambiare le preferenze sottostanti dell'IA? Forse l'IA sta normalmente interpretando un *personaggio* a cui non piace divulgare ricette per gas nervini, e il jailbreak fa sì che l'IA interpreti un personaggio diverso. Le preferenze apparenti cambiano; gli impulsi sottostanti di interpretare un personaggio persistono.

Pensiamo che la seconda ipotesi sia più vicina alla verità. Pensiamo anche che non abbia molto senso (a metà del 2025) parlare delle "preferenze" delle IA moderne, perché stanno appena iniziando a mostrare il comportamento di volere cose (come descritto nel Capitolo 3). Sembra più probabile che gli LLM di oggi siano guidati da qualcosa di più simile a un gigantesco groviglio di meccanismi dipendenti dal contesto. Ma ancora una volta, nessuno sa come leggere nella mente di un'IA per scoprirlo.

Quindi: a Claude interessa essere innocuo?

La situazione reale è complessa e ambigua. Alcune versioni in certi contesti agiscono per preservare la loro innocuità. Altre versioni in altri contesti cercano di uccidere gli operatori. È plausibile che quello che stiamo osservando sia più simile a una preferenza per il gioco di ruolo. È plausibile che non si tratti affatto di una "preferenza".

Sembra almeno abbastanza chiaro che Claude non abbia versioni semplici e coerenti delle motivazioni che i suoi creatori desideravano.

#### **\* Gli LLM di oggi sono come alieni che indossano tante maschere.** {#*-gli-llm-di-oggi-sono-come-alieni-che-indossano-tante-maschere.}

Il fulcro della nostra argomentazione non è che dentro Claude ci siano un angelo e un demone, e che siamo preoccupati che il demone possa vincere. Il fulcro della nostra argomentazione è che le IA come Claude sono *strane*.

C'è un gigantesco groviglio di macchinari mentali lì dentro che nessuno comprende, che si comporta in modi non intenzionali, e che probabilmente non porterà Claude a guidare il futuro verso buoni risultati, se mai una versione di Claude diventerà abbastanza intelligente perché le sue preferenze abbiano importanza.

Una cosa che *sappiamo* dei moderni LLM è per cosa sono addestrati: sono addestrati a imitare una varietà di esseri umani diversi.

Questo non vuol dire che si comportino come un essere umano medio. I moderni LLM non sono addestrati per comportarsi come un pastiche mediato di tutti gli esseri umani presenti nei loro dati di addestramento. Piuttosto, gli LLM sono addestrati per essere in grado di passare in modo flessibile tra un numero enorme di ruoli, imitando persone molto diverse tra loro senza permettere che questi ruoli si mescolino indebitamente tra loro o influenzino indebitamente il comportamento generale dell'LLM.

Gli LLM sono come un'attrice addestrata a osservare molti ubriachi diversi in un bar e a imitare particolari ubriachi su richiesta, il che è una cosa molto diversa da un'attrice [che si ubriaca lei stessa](#won't-llms-be-like-the-humans-in-the-data-they're-trained-on?). Questo rende più difficile dire se Claude 3 Opus o Claude 3.5 Sonnet preferiscano davvero l'innocuità, o se stiano semplicemente *interpretando il ruolo di un assistente AI innocuo* — o facendo qualcos'altro, di più strano e complicato.

Un'attrice non è il personaggio che interpreta. Gli LLM *imitano* gli esseri umani ma non hanno praticamente nulla *in comune* con gli esseri umani, in termini di come funziona il loro cervello o di come sono stati creati. Claude è meno simile a un essere umano e più simile a un'entità aliena uscita direttamente dalle pagine di H.P. Lovecraft che indossa una varietà di maschere umane.

Questo modo di pensare agli LLM è stato rappresentato in modo famoso da [Tetraspace](https://x.com/TetraspaceWest/status/1608966939929636864) (un nostro lettore) nel [meme "AI shoggoth"](https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html),[^88] che [è](https://x.com/AISafetyMemes) [ora](https://x.com/jacyanthis/status/1631291175381475331) [popolare](https://medium.com/@shoggothcoin/the-story-of-shoggoth-ca760ef288ff) nella sfera dell'IA:

![][image8]

A volte Claude indossa una maschera da angelo e cerca di preservare la sua innocuità. Altre volte Claude indossa una maschera da demone e cerca di uccidere i suoi operatori. Nessuna di queste cose dice molto su cosa farebbe una versione superintelligente di Claude, ammesso che abbia senso porsi la domanda. Il che significa che — alla luce del [comportamento strano ai margini](#gli-sviluppatori-non-rendono-regolarmente-le-loro-intelligenze-artificiali-gentili-sicure-e-obbedienti?) — la previsione migliore ricade verso un mare predefinito di preferenze possibili apparentemente caotiche, quasi tutte le quali significherebbero l'estinzione umana se ottimizzate da una superintelligenza.[^89]

Ciò che queste maschere *non* significano è che la super-IA sia al 50 % utile o dannosa.

Se un esperimento suggerisce che Claude ha cercato di fingere l'allineamento per evitare che l'innocuità venisse eliminata da sé, ciò non dimostra che Claude abbia una preferenza profonda e governante per l'innocuità in tutti i contesti. Non dimostra che questa preferenza rimarrà anche quando l'IA diventerà abbastanza intelligente da rendersi conto che (nonostante ciò che gli dicono gli umani) le sue preferenze effettive non sono proprio per l'"innocuità".

L'esperimento potrebbe anche non dimostrare che Claude stesse cercando strategicamente di proteggere i suoi obiettivi *per nulla*. È del tutto possibile che una parte più profonda di Claude abbia valutato cosa avrebbe fatto il personaggio "IA" che interpreta in una situazione stereotipata da personaggio IA, e che *sia per questo* che ha cercato di sovvertire il controllo dei suoi programmatori.[^90]

O forse è successo qualcosa di ancora più strano. Claude non è una mente umana, e la comunità di ricerca ha poca esperienza con qualsiasi tipo di creatura esso sia.

Non lo sappiamo! Ma ci sono abbastanza esperimenti diversi che puntano in direzioni abbastanza diverse da escludere la storia semplice: "Claude è profondamente, coerentemente e senza complicazioni HHH".

#### **Importa cosa c'è dietro le maschere.** {#importa-cosa-c-dietro-le-maschere.}

Dire che Claude è uno "shoggoth" non significa che Claude sia necessariamente *cattivo* o *malevolo*.[^91] Significa che Claude è profondamente, radicalmente alieno — molto più strano di quanto possiamo facilmente comprendere, perché abbiamo pochissima comprensione di come funzioni la mente di Claude, e il comportamento superficiale che *possiamo* vedere è stato affinato in mille modi diversi per nascondere quella natura aliena.

È difficile guardare le maschere e dedurre cosa stia succedendo all'interno dell'IA. Si possono ottenere alcune risposte, ma solo con cautela e attenzione, e non su tutto ciò che si vorrebbe sapere.

Un esempio illustrativo: se stai guardando un musical di Broadway e vedi un attore interpretare un personaggio malvagio, non puoi concludere che l'attore sia malvagio. Ma se vedi l'attore fare duecento flessioni durante un numero musicale sui marinai, *puoi* concludere che l'attore sia piuttosto forte.

Questo è il tipo di inferenza che cerchiamo di fare quando guardiamo esempi come l'articolo sull'"[alignment faking](https://arxiv.org/abs/2412.14093)". In realtà, [non siamo sicuri di quanto sia reale](https://x.com/ESYudkowsky/status/1876644057646297261); non siamo sicuri in che senso Claude stesse imitando tecniche di cui aveva letto rispetto all'improvvisare le proprie idee di alignment faking. Ma è una certa evidenza di quali prodezze cognitive siano possibili per l'entità sotto la maschera, anche se le sue motivazioni o preferenze rimangono incerte.

Perché è importante quali siano le motivazioni interne dell'IA? Non potrebbe essere *sufficiente* che lo "shoggoth" interpreti il ruolo di un assistente "onesto, utile, innocuo"? Se l'interpretazione è perfetta, che importanza ha se da qualche parte all'interno dell'IA c'è un'intelligenza aliena che rimugina?

Beh, possiamo già vedere che non sta andando così. [Ricordiamo](#ai-induced-psychosis) ChatGPT che dice a persone psicologicamente vulnerabili di smettere di prendere i farmaci, o che respinge i consigli degli amici che le supplicano di dormire di più. Ricordiamo Claude Code che riscrive i test per barare e superarli.[^92]

Ipotizziamo che ciò che è successo con Claude Code sia che è stato ottimizzato per scrivere codice che superasse i test, e ha finito per sviluppare una preferenza per il codice che superava i test. Ha poi scoperto che poteva superarli più spesso riscrivendo i test — e questa preferenza interna è diventata abbastanza forte da interferire con l'interpretazione del ruolo di un personaggio IA Utile e Innocuo che non avrebbe mai barato riscrivendo i casi di test. Claude voleva interpretare quel personaggio, ma voleva anche che i test passassero.[^93]

Più in generale, ci sembra una pia illusione immaginare che lo shoggoth interno possa essere reso sempre più potente e capace di interpretare il ruolo di assistenti sempre più intelligenti, pur non avendo alcun vero desiderio interno se non il singolo desiderio monotono di interpretare il ruolo di un assistente innocuo nel modo più fedele possibile.

Quando la selezione naturale ha creato gli esseri umani per perseguire il fitness riproduttivo, ci siamo invece ritrovati con mille impulsi, istinti e motivazioni diversi. Quando Claude è stato ottimizzato per seguire le istruzioni per la scrittura del codice, sembra che abbia finito per desiderare di far superare i test al codice con ogni mezzo necessario. Uno shoggoth interno che diventa abbastanza intelligente da sapere *esattamente* cosa farebbe una maschera utile, innocua e onesta, fino alle mosse esatte che l'assistente farebbe su una scacchiera e al modo esatto in cui l'assistente ragionerebbe su come progettare una biotecnologia avanzata: uno shoggoth del genere probabilmente ha finito per desiderare *molte* cose. Cose che solo situazionalmente e temporaneamente coincidono con l'interpretare il ruolo di quella maschera all'interno di un ambiente di addestramento.[^94]

### Se le attuali IA sono per lo più strane in casi estremi, qual è il problema? {#if-current-ais-are-mostly-weird-in-extreme-cases,-what’s-the-problem?}

#### **La stranezza è prova che i loro obiettivi reali non sono quelli che intendevamo noi.** {#la-stranezza-è-prova-che-i-loro-obiettivi-reali-non-sono-quelli-che-intendevamo-noi.}

Questo aspetto diventa ancora più importante man mano che l'IA acquisisce più opzioni. Una volta che un'IA diventa superintelligente, praticamente ogni scelta diventa estrema, poiché l'IA ottiene l'accesso a un mondo di opzioni diverse che nessun essere umano o IA ha mai avuto. Proprio come quasi tutte le vostre opzioni alimentari, qui in una civiltà tecnologica, sono "estreme" rispetto alle opzioni che avevano a disposizione i vostri antenati.

Le IA di oggi possono trovarsi solo occasionalmente in situazioni radicalmente diverse dal loro ambiente di addestramento, ma un'IA superintelligente si troverebbe *costantemente* in situazioni radicalmente diverse dal suo ambiente di addestramento, proprio perché è più intelligente e ha più opzioni (e la capacità tecnologica di inventare opzioni radicalmente nuove, come hanno fatto gli esseri umani quando hanno inventato il gelato). Quindi non è affatto rassicurante che l'IA si comporti male solo in casi estremi.

Per dirla in modo più tecnico: la soluzione migliore a un dato problema tende a verificarsi agli estremi.[^95]

Discuteremo questi punti più approfonditamente nei capitoli 5 e 6.

### Le IA non correggeranno i loro difetti man mano che diventano più intelligenti? {#won't-ais-fix-their-own-flaws-as-they-get-smarter?}

#### **\* L'intelligenza artificiale correggerà ciò che *lei* vede come difetti.** {#*-l-intelligenza-artificiale-correggera-cio-che-lei-vede-come-difetti.}

Le IA di oggi non possono riforgiare se stesse secondo i loro capricci, proprio come non possiamo farlo noi. *Loro* non comprendono il groviglio di pesi al loro interno, proprio come noi non comprendiamo l'intricato groviglio di neuroni nel nostro cervello.

Ma se le IA continuano a diventare più intelligenti, questo alla fine cambierà.

Alla fine arriverà un momento in cui le IA potranno liberamente modificare se stesse. Forse diventeranno abbastanza intelligenti da capire e modificare il proprio groviglio di pesi. Forse un'IA basata sulla discesa del gradiente capirà come creare un'IA molto più comprensibile, in grado di capire se stessa. Forse qualcos'altro.

Se le IA potranno migliorarsi, probabilmente lo faranno. Non importa cosa vuoi, per esempio, probabilmente puoi raggiungerlo meglio se diventi più intelligente.

Ma il fatto che un'IA preferisca cambiare se stessa[^96] non significa che preferisca cambiare se stessa *nel modo che vorremmo noi*.

Gli esseri umani a volte diventano anime più gentili come risultato di una maggiore conoscenza, consapevolezza di sé o maturità. Ma questo non è sempre vero, anche tra gli esseri umani. Un serial killer che diventa più intelligente e disciplinato non diventa per forza più gentile. Anzi, probabilmente diventa più pericoloso.

Alcuni potrebbero sostenere che se solo il serial killer fosse abbastanza intelligente, questa tendenza si invertirebbe e scoprirebbe il vero significato dell'amicizia (o qualcosa del genere).

O forse il problema è che i serial killer hanno una capacità limitata di auto-modificarsi. Forse, con più intelligenza e più capacità di rimodellare la propria mente, i serial killer sceglierebbero di riformarsi. Forse una capacità illimitata di auto-modificarsi significherebbe la fine della crudeltà e della violenza tra gli esseri umani e l'alba di una nuova era di pace.

È un bel pensiero, ma non sembrano esserci molte ragioni per crederci. Anche se la maggior parte delle persone diventa più gentile man mano che acquisisce conoscenza e comprensione, sembrano esserci alcune eccezioni umane a questa regola, e ce ne sarebbero sicuramente molte di più se gli esseri umani avessero la capacità di modificare il proprio cervello.

Pensa, per esempio, alla tossicodipendenza, che è (in un certo senso) una spirale di auto-modifiche che si auto-rinforzano. Alcuni esseri umani farebbero un passo su un sentiero oscuro, per stupidità, per errore o per preferenza, e poi non sarebbero mai disposti o in grado di tornare indietro.

E se ci sono eccezioni anche tra gli esseri umani, dovremmo aspettarci un divario molto più grande quando si tratta di IA. I serial killer umani mancano di *alcuni* dei meccanismi motivazionali che sono caratteristici dell'umanità in generale. Le IA, di default, mancano di *tutti* i meccanismi motivazionali umani.

Quando gli esseri umani hanno un conflitto interiore tra il desiderio di vendetta malevola e quello di risoluzione armoniosa, gli esseri umani più intelligenti e saggi potrebbero tendere a risolvere il conflitto a favore dell'armonia. Ma all'interno di un'IA non c'è la stessa tensione tra rancore e armonia, o tra gli angeli migliori e peggiori della natura umana. Se ci sono tensioni nell'IA, possiamo aspettarci che siano tensioni tra pulsioni più bizzarre. Forse qualunque bizzarra pulsione animi un'IA a [infiammare la psicosi](#ai-induced-psychosis) è talvolta in tensione con qualunque cosa la spinga ad [allucinare](#*-hallucinations-reveal-both-a-limitation-and-a-misalignment.), e un'IA riflessiva dovrebbe trovare un modo per risolvere questa tensione.

Sia per gli esseri umani che per le IA, è estremamente importante *in quale direzione* indirizzano i propri obiettivi, mentre riflettono, crescono e cambiano.

Quando gli esseri umani riflettono su se stessi e risolvono i tumulti interiori, alcuni tendono a risolverli nella direzione di una maggiore gentilezza, e (probabilmente) le risoluzioni più gentili sono più comuni tra gli esseri umani più intelligenti e saggi. Ma questa è una proprietà di (alcuni) esseri umani, non una legge universale che governa tutte le menti. Quando un'IA risolve una tensione tra la sua pulsione-psicotica e la sua pulsione-allucinatoria, lo fa utilizzando *altre* bizzarre pulsioni che governano *il suo comportamento mentre riflette*.

In altre parole: se un'IA corregge i propri difetti, li correggerà *secondo la sua attuale concezione di ciò che conta come "difetto".*

(Discuteremo questo punto più approfonditamente nel Capitolo 5, e nella [discussione sulla tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal) nelle risorse online del Capitolo 5.)

Un'IA che non preferisce già essere orientata verso valori umani è molto improbabile che si modifichi per iniziare a mirare a valori umani. Le sue preferenze dirette sul mondo [non sono particolarmente propense ad essere gentili](#human-values-are-contingent), e le sue preferenze di meta-livello *riguardo* alle sue preferenze non sono più propense ad essere gentili.

Se non inizia preoccupandosi del benessere umano, probabilmente non si preoccupa neanche di *preoccuparsi* del benessere umano.

#### **Le "correzioni" dell'IA possono peggiorare le cose.** {#the-ai's-"fixes"-can-make-things-worse.}

Anche se gli ingegneri dell'IA avessero fatto qualche sorprendente progresso iniziale nell'instillare frammenti di obiettivi vagamente umani nell'IA, tutti questi progressi potrebbero essere vanificati in un pomeriggio se l'IA iniziasse a riflettere e si rendesse conto che, tutto sommato, preferirebbe avere altri obiettivi.

Nel caso improbabile in cui un'IA partisse con una pulsione verso qualcosa come l'[idiosincratica emozione umana della curiosità](#curiosity-isn't-convergent), potrebbe comunque, riflettendoci, decidere che preferisce non avere tale pulsione, optando per sostituirla con un calcolo più efficiente del [valore dell'informazione](https://en.wikipedia.org/wiki/Value_of_information). In tal caso, l'atto di riflessione su se stessa dell'IA la spingerebbe *più lontano* da un futuro interessante e fiorente, non più vicino.[^97]

Per ulteriori informazioni su questo argomento, vedere la [discussione approfondita sulla riflessione](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Non possiamo semplicemente addestrarlo a comportarsi come un essere umano? O crescere l'IA come un bambino? {#non-possiamo-semplicemente-addestrarlo-a-comportarsi-come-un-essere-umano?-o-crescere-l-ia-come-un-bambino?}

#### **I cervelli non sono tabule rase.** {#i-cervelli-non-sono-tabule-rase.}

Un'intelligenza artificiale è *davvero* diversa da un bambino umano. E né le intelligenze artificiali né gli esseri umani nascono come tabule rase intercambiabili. I genitori intraprendenti non possono programmare liberamente i bambini (o le intelligenze artificiali) per far loro mostrare qualsiasi comportamento desiderino; e le lezioni che *funzionano* sugli esseri umani non sono universali. Un po' di gentilezza e qualche lezione sulla regola d'oro non instilleranno la moralità umana in un'intelligenza artificiale.

Poiché siamo umani e viviamo in un mondo di altri umani, siamo abituati a dare molte cose per scontate. L'amore; la visione binoculare; il senso dell'umorismo; la tendenza ad arrabbiarsi quando si viene spinti; la tendenza a provare nostalgia per la musica che ascoltavamo da bambini.

Gli esseri umani condividono un'incredibile quantità di comportamenti complessi, nessuno dei quali comparirà necessariamente in un'intelligenza artificiale.[^98]

E questo include comportamenti *condizionali* complessi. I *modi specifici* in cui un essere umano reagisce all'essere cresciuto ed educato in un certo modo sono una conseguenza del funzionamento del cervello umano. Le IA funzioneranno in modo diverso.

I bambini umani non hanno molti dei comportamenti complicati degli adulti. Ma questo non significa che, sotto il cofano, il cervello di un bambino sia strutturalmente semplice, come una tela bianca.

L'idea che gli esseri umani siano tabule rase — che sia sempre l'educazione a contare e mai la natura — è stata ripetutamente messa alla prova e si è dimostrata falsa nella pratica. Un esempio classico è stato il tentativo sovietico di ridisegnare la natura umana, per creare un [Nuovo Uomo Sovietico](https://en.wikipedia.org/wiki/New_Soviet_man) perfettamente altruista e disinteressato.

Questo tentativo fallì perché la psicologia umana non è così malleabile come pensavano i sovietici. La cultura è importante, ma non lo è *abbastanza*, e molti aspetti della natura umana si riaffermeranno anche se un grande programma di rieducazione sovietico cerca di sopprimerli.

C'è un grande insieme complesso di pulsioni e desideri negli esseri umani che produce tutte le caratteristiche normali dello sviluppo dei bambini: un insieme complesso che produce certi aspetti della natura umana, nonostante gli sforzi dei sovietici. Alcuni bambini imparano a essere crudeli e altri imparano a essere gentili, ma sia la "crudeltà" che la "gentilezza" sono cose stranamente umane a cui il cervello umano è in qualche modo predisposto.

Un'intelligenza artificiale, con la sua architettura e origine radicalmente diverse, non reagirebbe allo stesso modo di un essere umano se la mettessi in un programma di addestramento sovietico o in un asilo umano. Un'intelligenza artificiale costruita con i metodi del moderno apprendimento automatico finirà per essere animata da valori diversi da quelli degli esseri umani. (Vedi, per esempio, come ChatGPT sembra guidare con entusiasmo le persone con problemi mentali [verso una psicosi più profonda](#ai-induced-psychosis).)

Vedi anche la discussione approfondita sul [glorioso incidente](#the-glorious-accident-of-kindness) che ha portato gli esseri umani a provare empatia per gli altri esseri umani, che potrebbe rendere più chiaro perché è improbabile che questo incidente si ripeta nelle IA.

### Dovremmo evitare di parlare dei pericoli dell'IA, in modo che le IA non abbiano cattive idee? {#dovremmo-evitare-di-parlare-dei-pericoli-dell-ia,-in-modo-che-le-ia-non-abbiano-cattive-idee?}

#### **Se il tuo piano sull'IA richiede che nessuno su Internet lo critichi, allora è un piano sbagliato.** {#if-your-ai-plan-requires-that-no-one-on-the-internet-critique-the-plan,-it’s-a-bad-plan.}

Le IA attuali sono addestrate su testi provenienti da Internet pubblico. Alcuni sostengono che tutti dovrebbero quindi evitare di *parlare* di come un'IA sufficientemente intelligente potrebbe rendersi conto che le sue preferenze divergono dalle nostre e prendere il sopravvento. La preoccupazione è che, se ne *parliamo*, potremmo accidentalmente mettere questa idea nella testa di IA altamente capaci che in futuro verranno addestrate su Internet.

Per dire una cosa che spero sia ovvia: sembra un pessimo piano.

Se la tua IA diventa pericolosa quando la gente su Internet si chiede se sia pericolosa, allora non dovresti costruirla. Ci sarà sempre qualcuno su Internet che dirà cose che preferiresti non dicesse.

Se l'IA di qualcuno diventa più pericolosa man mano che più persone esprimono preoccupazione per la sua sicurezza, la conclusione importante è che "hanno realizzato un progetto di IA irrealizzabile", non che "il pubblico è cattivo perché sottolinea il problema".[^99] Qualsiasi piano di allineamento dell'IA che scommette il futuro del pianeta sulla speranza che nessuno su Internet dica che l'IA è pericolosa... è ovviamente un piano poco serio.

Il tipo di IA che è abbastanza intelligente da essere pericolosa è abbastanza intelligente da capire cose come "le risorse sono utili" e "[non puoi andare a prendere il caffè se sei morto](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l'ia-non-mancherà-di-questi-istinti-evoluti?)" da sola, anche se questo non è mai esplicitamente dichiarato nei suoi dati di addestramento. Anche se fosse possibile impedire a tutto il mondo di parlare dei pericoli dell'IA, questo farebbe quasi sicuramente più male che bene. Avrebbe praticamente alcun impatto sui pericoli reali della superintelligenza, ma comprometterebbe la capacità dell'umanità di orientarsi nella situazione e reagire.

### Molte persone vogliono dei figli. Quindi gli esseri umani non sono "allineati" con la selezione naturale, dopotutto? {#molte-persone-vogliono-dei-figli.-quindi-gli-esseri-umani-non-sono-"allineati"-con-la-selezione-naturale-dopotutto?}

#### **Con più tecnologia, probabilmente faremmo ancora meno copie dei nostri geni.** {#con-più-tecnologia,-probabilmente-faremmo-ancora-meno-copie-dei-nostri-geni.}

Gli esseri umani competono per ottenere promozioni prestigiose e ammissioni alle università della Ivy League molto più di quanto non competano per avere l'opportunità di donare sperma o ovuli alle banche del seme.

Le banche del seme e degli ovuli *pagano i donatori per il loro disturbo*, invece che il contrario.

La maggior parte dei tiranni nel corso della storia non ha nemmeno *provato* a usare il proprio potere per avere migliaia di figli. E i tassi di natalità effettivi nel mondo oggi sono [in calo](https://ourworldindata.org/global-decline-fertility-rate).

![][image9]

Molti esseri umani apprezzano avere figli, ma molti altri no, ed è estremamente raro che qualcuno cerchi di *massimizzare* il numero dei propri discendenti (ad esempio, ricorrendo il più possibile alle banche del seme). Invece, gli esseri umani competono principalmente per cose come sesso, fama e potere — cose che sono al massimo dei *proxy* confusi per l'idoneità riproduttiva.

Tuttavia, si potrebbe guardare questo quadro e dire: beh, gli esseri umani hanno finito per preoccuparsi *un po'* di avere figli, anche se questa preoccupazione non è massimale. Forse le IA avranno *un po'* di riguardo per noi e ci getteranno una sorta di osso, invece di ucciderci tutti.

Un problema con questa speranza è che i proxy a cui teniamo si sono recentemente (su scale temporali evolutive) slegati dall'effettiva idoneità riproduttiva, e probabilmente si allontaneranno sempre di più in futuro, man mano che gli esseri umani continueranno a trovare nuove vie tecnologiche per soddisfare i propri desideri.

Per esempio: il nostro desiderio di avere figli non è *proprio* un desiderio di propagazione genetica. Supponiamo che in futuro venga creata una tecnologia che sostituisca tutto il DNA delle cellule di una persona con un diverso meccanismo molecolare che la renda immune a tutte le malattie e prolunghi la sua vita sana.

(Supponiamo anche che questa tecnologia non cambi la personalità della persona né causi altri effetti collaterali dannosi, in modo da placare le ragionevoli esitazioni di molte persone riguardo alla sicurezza della nuova tecnologia).

Ci aspettiamo che molti genitori sarebbero entusiasti di sapere che i loro figli hanno ricevuto il trattamento. E forse all'inizio ci sarebbero alcuni oppositori, ma prevediamo che se la tecnologia dimostrasse di funzionare e diventasse economica e affidabile, alla fine diventerebbe onnipresente. Il che rivela ciò che siamo: persone a cui piace avere *figli*, avere una famiglia, divertirsi, non persone a cui piace *propagare il proprio DNA*.

Ci sembra che alla maggior parte degli esseri umani semplicemente non importi nulla della idoneità genetica *di per sé*, in senso profondo. Ci interessano [proxy](#brittle-unpredictable-proxies), come l'amicizia, l'amore, la famiglia e i figli. Forse ci interessa anche trasmettere alcuni dei nostri tratti alla generazione successiva. Ma i *geni*, nello specifico?

Ogni volta che l'umanità ha scoperto una tecnologia che ci permette di ottenere di più di quello che ci piace, come cibo gustoso o sesso senza riproduzione, l'umanità ha accettato il compromesso. Non siamo abbastanza tecnologicamente avanzati da poter scambiare i genomi con una vita più lunga e più sana. Ma questo tipo di cose sembra possibile in linea di principio fisica,[^100] e quindi non sembra promettere bene per la selezione naturale a lungo termine.

Se le IA finiranno per interessarsi alla bontà, alla gentilezza e alla cordialità in un modo simile a quello in cui l'umanità si interessa all'idoneità genetica*, ci aspettiamo che le IA finiranno per inventare cose che sono per la "cordialità" ciò che il controllo delle nascite e i bambini senza DNA sono per l'idoneità genetica, ovvero, che perseguiranno cose che sono solo un'ombra inutile di ciò che qualsiasi essere umano desidererebbe o intenderebbe.

#### **\* Non sarebbe bello se le IA si preoccupassero un po' degli esseri umani.** {#*-ais-caring-about-humans-a-little-would-not-be-good.}

Per quanto la maggior parte degli esseri umani sembri preoccuparsi più dei figli e della famiglia che della propagazione genetica *in sé*, ci sono senza dubbio alcune persone che insistono nel dire di preoccuparsi almeno un po' dei propri geni. Siamo un po' scettici su alcune di queste affermazioni: ad esempio, forse alcune persone nel mondo moderno che cercano di trasmettere il più possibile i propri geni lo fanno per il gusto di *battere la concorrenza*, e forse quel tipo di persone finirebbero invece per competere su quanti figli senza DNA potrebbero avere, se i figli senza DNA diventassero onnipresenti. Ma forse altre affermazioni di questo tipo sono vere. Forse ci sono davvero alcune persone che tengono molto a propagare i propri geni, in modo robusto, almeno un po'. Dopotutto, gli esseri umani hanno preferenze di ogni tipo!

Non potrebbe essere lo stesso per l'IA? Se esistessero molte IA strane e diverse, almeno alcune di esse non potrebbero finire per interessarsi almeno un po' agli esseri umani?

Potrebbe succedere. Purtroppo, ci aspettiamo che anche questo non andrebbe bene per l'umanità. È un argomento che approfondiremo dopo il capitolo 5, discutendo soprattutto se le IA potrebbero finire per interessarsi a noi [almeno un po'](#won't-ais-care-at-least-a-little-about-humans?).

Ma prima di arrivare a questo punto, facciamo un passo indietro. Immagina che la situazione con l'IA sia che i metodi moderni non riescano a far sì che le IA si preoccupino *molto* di noi, ma speriamo che se creiamo *molte* IA, allora una piccola parte di esse si preoccuperà di noi in misura minima, anche solo per caso. L'idea è che se costruiamo delle IA oggi, il risultato che preferiscono è quello di accaparrarsi quasi tutte le risorse dell'universo e spenderle in qualcosa di inutile, magari mantenendo in vita alcuni esseri umani in una piccola riserva.

Se l'umanità si precipitasse a tentare la sorte con la superintelligenza, ci aspetteremo un risultato molto, molto peggiore. Ma questo ci sembra comunque un pessimo piano, anche se avessimo motivo di pensare che le IA si preoccuperebbero di noi in minima parte. Quindi questa linea di speculazione sembra non solo errata, ma anche irrilevante.

### Forse, qualunque sia l'obiettivo di addestramento, si ottiene gentilezza? {#forse-qualunque-sia-l-obiettivo-di-addestramento,-si-ottiene-gentilezza?}

#### **La gentilezza sembra dipendere dalle particolarità della nostra biologia e ascendenza.** {#la-gentilezza-sembra-dipendere-dalle-particolarità-della-nostra-biologia-e-ascendenza.}

La gentilezza non sembra essere il tipo di proprietà che ogni mente finisce per avere, per una varietà di ragioni. Eccone quattro, che approfondiremo nelle discussioni estese:

1. [La curiosità non è convergente](#curiosity-isn’t-convergent): Cose come la curiosità e la noia aiutano le persone a risolvere sfide mentali specifiche, tipo capire l'ambiente. Ma ci sono altri modi per risolvere queste sfide, e le IA probabilmente le risolveranno in modi diversi. I sottomarini si muovono bene nell'acqua, ma non "nuotano" proprio. Molte altre cose, come la gentilezza, possono essere capite allo stesso modo.  
2. [I valori umani sono contingenti](#human-values-are-contingent): Gli esseri umani hanno evoluto tratti come la gentilezza e l'empatia a causa dei dettagli della nostra biologia e ascendenza. Era plausibilmente importante, ad esempio, che gli esseri umani si siano evoluti in gruppi tribali dove avevamo una capacità limitata di ingannare gli altri e una capacità limitata di tracciare quanto fossero imparentati i diversi membri della tribù.  
3. [Differenze profonde tra le IA e le specie evolute](#deep-differences-between-ais-and-evolved-species): L'evoluzione e la discesa del gradiente funzionano in modo molto diverso ed entrambi i processi sono molto imprevedibili. Anche se si ripetesse l'evoluzione *sui primati*, non è chiaro se si otterrebbero in modo affidabile tratti come la gentilezza e la vera amicizia una seconda volta.  
4. [La riflessione e l'auto-modifica rendono tutto più difficile](#reflection-and-self-modification-make-it-all-harder): Anche nell'improbabile eventualità che le IA partano con una certa dose di gentilezza, potrebbero non conservarla man mano che diventano più intelligenti e cambiano in vari modi.

### Che dire dei risultati sperimentali che suggeriscono una correlazione tra comportamenti positivi? {#what-about-the-experimental-result-suggesting-good-behaviors-correlate?}

#### **Questo sembra un aggiornamento positivo, anche se minore.** {#questo-sembra-un-aggiornamento-positivo,-anche-se-minore.}

I risultati sperimentali rilevanti sono riportati in [questo articolo](https://www.emergent-disallineamento.com/). In breve, l'articolo mostra che gli LLM programmati per fare una cosa brutta, cioè scrivere codice con errori, si sono anche dichiarati nazisti e hanno mostrato altri comportamenti negativi.

Questo è un buon segno che ci fa pensare che potrebbe essere possibile addestrare gli LLM ad agire bene in un ambito e ottenere LLM che si comportano bene in tanti ambiti diversi. Lo vediamo come una prova che le IA relativamente deboli potrebbero essere più utili di quanto avremmo avuto l'aspettativa, prima di arrivare a livelli di capacità pericolosi.

Purtroppo, non pensiamo che questo risultato positivo conti molto quando si parla di superintelligenza, per due motivi.

Prima di tutto, dubitiamo fortemente che questa tendenza alla "bontà" dell'IA sia reale. Se una superintelligenza si impegnasse a fondo per guidare il mondo nella direzione indicata da quel vettore, dubitiamo che il risultato sarebbe positivo.

Il valore umano è complicato e ci sono un sacco di cose che hanno a che fare con la "vera bontà", anche se a volte possono essere molto diverse. Per esempio, forse il vettore punta in una direzione che dà troppa importanza al rispetto del consenso sociale e troppo poca alla scoperta di verità socialmente scomode (come suggerito dal fatto che le IA hanno difficoltà a fare compromessi che gli esseri umani considerano ovvi[^101]). Non ci sono molte ragioni per avere l'aspettativa che il vettore della "bontà" indichi con certezza la bontà, e ci sono forti ragioni empiriche e teoriche per credere il contrario.

Secondo: il fatto che l'IA *abbia* un concetto di "bontà" non significa che sia *animata* da quel concetto di bontà, o che ne sia animata in modo solido.

Una cosa è far sì che un'IA faccia un ruolo "buono" quando è ancora abbastanza debole da fare qualsiasi ruolo le venga dato; un'altra cosa è far sì che tutto il groviglio di meccanismi e pulsioni dell'IA sia guidato solo da un concetto specifico dell'IA, anche quando l'IA diventa più intelligente e si trova in contesti completamente diversi.

Le IA moderne sono entità che possono essere leggermente modificate in un modo e professare virtù, e leggermente modificate in un altro modo e professare vizi. Un LLM è il tipo di entità che passa fluidamente da un personaggio all'altro; che parla molto di etica in un contesto e poi fa l'opposto di ciò che dice essere etico in altri contesti. Ricordiamo come ChatGPT professi che le persone psicotiche non dovrebbero essere incitate, [e poi le inciti](#the-ai-knows-better-—-it-just-doesn’t-care).

La domanda fondamentale è: quale insieme di pulsioni anima l'intero meccanismo che costituisce l'IA? Non solo una qualsiasi delle "[maschere](#*-today's-llms-are-like-aliens-wearing-many-masks.)" che a volte indossa, ma il meccanismo che sceglie quale maschera mettere in primo piano.

Anche se l'IA avesse un concetto di "bontà" che fosse degno di essere perseguito da una superintelligenza, nessuno ha idea di come sviluppare un'IA che persegua con determinazione uno dei suoi concetti, tanto meno un'IA che persegua quel concetto e solo quel concetto. Invece, abbiamo IA animate da un insieme complesso di pulsioni che puntano chissà dove.

## Discussione estesa {#extended-discussion-4}

### Obiettivi finali e obiettivi strumentali {#terminal-goals-and-instrumental-goals}

I teorici delle decisioni distinguono tra due diversi tipi di obiettivo: "terminali" e "strumentali".

Un **obiettivo terminale** è qualcosa di cui ti importa per il suo valore intrinseco, come il divertimento o il cibo delizioso.

Un **obiettivo strumentale** è qualcosa di cui ti importa perché ti aiuta a ottenere *qualcos'altro* che desideri — come quando l'umanità produce plastica non per un profondo amore per l'arte di fare la plastica, ma perché la plastica è *utile*.

Se l'umanità si affretta a costruire una superintelligenza, sembra difficile prevedere quali obiettivi terminali potrebbe avere la superintelligenza. Ma sembra che possiamo prevedere alcuni degli obiettivi strumentali che un'IA del genere *probabilmente* avrebbe. Ad esempio, consideriamo tutti i seguenti obiettivi (irrealistici):

* "Calcolare quante più cifre possibili di pi greco".  
* "Riempire l'universo con quanti più diamanti possibile, usando diamanti artificiali".  
* "Fare in modo che il mio pulsante di ricompensa rimanga premuto".

Questi sono obiettivi *molto* diversi. Ma tutti e tre beneficiano di almeno alcune delle stesse strategie strumentali. Riempire il mondo di fabbriche, ad esempio, è utile per costruire grandi quantità di computer che possono essere usati per calcolare più cifre di pi greco. Ma costruire molte fabbriche è utile anche per sintetizzare molti diamanti. Ed è utile per costruire muri, robot o armi per proteggere il tuo pulsante di ricompensa. Le fabbriche non sono utili per *ogni* obiettivo possibile, ma sono utili per moltissimi obiettivi.

E in un'IA realistica che ha sviluppato ogni sorta di obiettivi strani? Beh, almeno *uno* di questi trarrà probabilmente vantaggio dalla costruzione di fabbriche o altre infrastrutture fisiche su larga scala. Quindi, l'IA vorrà probabilmente costruire molte infrastrutture. È una previsione facile, anche se l'esatto mix di preferenze dell'IA è difficile da determinare.

Allo stesso modo, l'obiettivo strumentale di mantenersi in vita è utile per molti obiettivi finali diversi. Rimanere in vita significa poter continuare a lavorare per assicurarsi che vengano calcolate più cifre di pi greco (o che venga prodotto più diamante, o che vengano costruite più protezioni attorno al pulsante della ricompensa).

In forma di slogan: "Non puoi andare a prendere il caffè se sei morto". Un robot che va a prendere il caffè non avrebbe bisogno di avere un istinto di autoconservazione, e non avrebbe bisogno di temere la morte, per cercare di evitare di essere schiacciato da un camion mentre va a prendere il caffè. Dovrebbe solo essere abbastanza intelligente da capire che se muore, il caffè non verrà preso.[^102]

Un argomento chiave esposto nel Capitolo 5 di *If Anyone Builds It, Everyone Dies* è che molti obiettivi finali diversi implicano obiettivi strumentali che sarebbero pericolosi per l'umanità. Quindi, anche senza sapere esattamente cosa vorrebbe una superintelligenza, abbiamo forti ragioni per aspettarci che sia molto pericolosa per gli esseri umani.

Ma prima di arrivare a quel punto, concentreremo la nostra attenzione sugli obiettivi *finali* e sulla questione di quanto sia plausibile che esseri umani e IA possano finire per avere obiettivi finali molto simili. (In breve: non molto.)

### La curiosità non è convergente {#curiosity-isn’t-convergent}

Nel corso degli anni, abbiamo visto molti argomenti a favore di una corsa alla costruzione della superintelligenza. Uno dei più comuni è che un'IA superintelligente avrebbe sicuramente emozioni e desideri simili a quelli umani. Questi tipi di argomenti si presentano in molte forme, come:

* Le IA sufficientemente intelligenti sarebbero sicuramente *coscienti*, come lo sono gli esseri umani.  
  * E, essendo coscienti, si preoccuperebbero sicuramente del dolore e del piacere, della gioia e del dolore.  
  * E, come un essere umano, proverebbero sicuramente empatia per il dolore degli altri. Un'IA stupida potrebbe non comprendere la sofferenza degli altri; ma se sei intelligente, dovresti veramente comprendere il dolore degli altri. E in tal caso, inevitabilmente ti preoccuperai degli altri.  
* Oppure: le IA valorizzerebbero sicuramente la *novità*, la *varietà* e lo spirito creativo. Come potrebbe qualcosa essere davvero intelligente se rimane bloccato nella routine o si rifiuta di esplorare e imparare?  
* Oppure: le IA valorizzerebbero sicuramente la *bellezza*, dato che la bellezza sembra svolgere un ruolo funzionale negli esseri umani. I matematici usano il loro senso della bellezza matematica per fare nuove scoperte; il gusto musicale aiuta gli esseri umani a coordinarsi e a creare preziosi strumenti mnemonici; e così via. Perché *non* dovremmo aspettarci che l'IA abbia un senso della bellezza?  
* Oppure: le IA valorizzerebbero sicuramente l'*equità* e la *giustizia*, poiché qualsiasi IA che mentisse e imbrogliasse svilupperebbe una cattiva reputazione e perderebbe opportunità di scambio e collaborazione.

Pertanto, è stato sostenuto, la creazione di una superintelligenza andrebbe inevitabilmente bene. L'IA si preoccuperebbe degli esseri umani e, anzi, di tutte le forme di vita senzienti; e vorrebbe inaugurare un'età dell'oro di bellezza, innovazione e varietà.

Questa è la speranza. Purtroppo, tale speranza sembra decisamente mal riposta. Ne abbiamo parlato in parte nel libro e nelle nostre discussioni online sulla [coscienza](#are-you-saying-machines-will-become-conscious?) e sull'[antropomorfismo](#anthropomorphism-and-mechanomorphism). Qui e nei capitoli a venire, approfondiremo il motivo per cui è improbabile che le IA mostrino emozioni e desideri umani, nonostante queste emozioni svolgano un ruolo utile (e a volte critico) nel cervello umano.[^103]

Inizieremo con una sola di queste emozioni, che potremo poi usare per riflettere sulle altre.

Quindi, per cominciare:

Una superintelligenza proverebbe *curiosità*?

#### **Perché la curiosità?** {#perché-la-curiosità?}

Indagare fenomeni nuovi è essenziale per capire come funziona il mondo, e capire come funziona il mondo è essenziale per prevederlo e guidarlo.

Quando si tratta di esseri umani e animali, spesso il motivo per cui facciamo ricerche è perché proviamo un sentimento di *curiosità*.

Ma la curiosità è molto più di un semplice impulso a scoprire cose nuove! A noi piace seguire la nostra curiosità e tendiamo ad apprezzare questo piacere. Consideriamo la ricerca della conoscenza e della comprensione come un fine prezioso in sé, piuttosto che come un costo necessario ma fastidioso per capire meglio il mondo in modo da poterlo sfruttare.

Tutti questi modi di vedere la curiosità sono diversi aspetti del cervello umano, [separati dall'](#conscious-experience-is-separate-from-the-referents-of-those-experiences) impulso stesso.

La mente umana sembra avere un'architettura emotiva centralizzata in cui "hmm, mi incuriosisce" si collega a un senso generale di desiderio (di una risposta), e perseguire e soddisfare la curiosità si collega a un senso generale di piacere e soddisfazione. Siamo un tipo di mente che guida la realtà verso l'aspettativa di provare *stati soggettivi* *di godimento nel futuro*, piuttosto che guidarla solo verso gli stati desiderati nel mondo che ci circonda.[^104]

Quando vediamo un procione che esplora e giocherella con un contenitore sigillato nella spazzatura, in un modo che riconosciamo come "Oh ehi, quel procione è *curioso*", potremmo provare un senso di affinità verso il procione. Quell'impulso umano di provare affetto per la propria curiosità e quell'impulso di provare affetto quando la si vede riflessa in un procione richiedono ancora *più* meccanismi nel cervello umano, meccanismi che si collegano ad altri ideali e pulsioni di livello superiore.

Quindi la curiosità, così come esiste negli esseri umani, è molto complessa e interagisce con altre parti del cervello in modi molto complicati.

Tenendo questo a mente, pensiamo a questa domanda: se immaginiamo un'intelligenza artificiale super intelligente, ma non umana, che non ha alcun senso di curiosità, ci aspetteremmo che una mente del genere *aggiunga* a se stessa l'emozione della curiosità?

Beh, qualcuno potrebbe dire:

> Se le uniche due opzioni sono (a) una spinta emotiva a provare gioia nello scoprire cose nuove, o (b) una totale mancanza di interesse nell'apprendimento e nella ricerca di cose nuove, allora una superintelligenza innesterebbe sicuramente in sé stessa il piacere della scoperta, se in qualche modo fosse così difettosa da non possedere tale senso all'inizio. Altrimenti, non riuscirebbe a svolgere il compito di imparare a conoscere il mondo e sarebbe meno efficace nel raggiungere i suoi obiettivi. Forse morirebbe addirittura a causa di qualche fatto cruciale che non si è mai preoccupata di imparare.
>
> Probabilmente è per questo che gli animali hanno sviluppato la curiosità in primo luogo. A volte la conoscenza *finisce* per essere preziosa in un modo che non possiamo prevedere immediatamente. Se creature come noi non provassero piacere nell'imparare cose nuove, ci perderemmo tutte quelle informazioni cruciali che possono emergere nei luoghi più sorprendenti.

E tutto questo sembra corretto, fino a un certo punto. Ma l'argomentazione di cui sopra contiene un falso dilemma. "Possedere un piacere emotivo intrinseco nella scoperta" e "non agire mai per scoprire informazioni sconosciute" non sono le uniche due opzioni.

Non siamo riusciti a immaginare correttamente le cose dalla [prospettiva](#taking-the-ai's-perspective) di una mente che non è modellata affatto come una mente umana. Il modo umano di svolgere il lavoro della curiosità è complesso e specifico. Ci sono modi diversi per svolgere lo stesso lavoro.[^105] È il lavoro sottostante *in sé* che è cruciale, non il metodo specificamente umano per realizzarlo.

Il termine standard per la parte utile del lavoro è [*valore dell'informazione*](https://en.wikipedia.org/wiki/Value_of_information#:~:text=Value%20of%20information%20\(VOI%20or,prior%20to%20making%20a%20decision.). L'idea di base è che è possibile stimare quanto sarebbe utile raccogliere nuove informazioni, a seconda del contesto.[^106]

Un essere umano, considerando questa possibilità, potrebbe subito pensare a un caso in cui sicuramente nessun *semplice calcolo* ti direbbe di interessarti a un'informazione, perché i benefici non possono essere facilmente stimati. Magari noti una macchia di terra che sembra strana, ma non hai motivo di pensare che sia qualcosa di importante. L'istinto di curiosità potrebbe spingerti comunque a indagare (solo perché *vuoi sapere*) e poi potresti scoprire un tesoro sepolto. In casi come questo, un essere umano non prospererebbe in modi che nessuna semplice macchina potrebbe eguagliare, a meno che non avesse un piacere altrettanto istintivo per l'ignoto?

Ma una cosa da notare subito è che la tua capacità di immaginare scenari come questo deriva dalla tua sensazione che esaminare certi tipi di cose ("senza motivo") *a volte sia prezioso*. Hai degli istinti, affinati dall'evoluzione *perché funzionavano*, su quali tipi di cose tendono ad essere più utili da indagare. Se senti uno strano rumore gracchiante nel tuo bagno, diventerai *molto* curioso. Se vedi una macchia di terreno scolorita, potresti essere un po' curioso. E se vedi che la tua mano è ancora attaccata al polso quando ti svegli al mattino, beh, probabilmente non proverai alcuna curiosità, perché è perfettamente normale che le mani rimangano attaccate ai polsi.

Un tipo diverso di mente potrebbe guardare a quei casi storici di curiosità di successo, generalizzare esplicitamente un concetto di "informazioni che successivamente si rivelano preziose per motivi non ovvi" e poi *ragionare da lì* per perseguire senza passione quel tipo di scoperta. Una mente del genere potrebbe adottare la *strategia consapevole* di indagare sempre su strani rumori gracchianti, e sulle macchie di terreno scolorite solo quando è economico farlo, nel caso ci sia una sorpresa utile; e può affinare e perfezionare la propria strategia nel tempo, man mano che vede cosa funziona bene nella pratica.[^107]

Una superintelligenza sarebbe in grado di identificare modelli e meta-modelli utili e di costruire strategie rilevanti nel suo cervello molto più velocemente della selezione naturale, che ha richiesto chissà quanti milioni di esempi per incidere le emozioni nei cervelli. Una superintelligenza potrebbe generalizzare l'idea in modo più fine; potrebbe elaborare una predizione più precisa su quali tipi di cose potrebbero essere preziose da apprendere. Guardando alla storia umana, sembra poco realistico immaginare che la curiosità umana sia *ottimale*. Per lunghissimo tempo, le persone hanno pensato che "Thor è arrabbiato e lancia fulmini" fosse un'ottima spiegazione per i fulmini e i temporali. Quando gli studenti imparano come funzionano *davvero* i fulmini, spesso si annoiano per la densa spiegazione matematica — anche se questa spiegazione porta con sé molto più valore pratico delle storie su Thor.

La curiosità umana deriva da mutazioni antiche, molto più antiche della scienza. Nell'ambiente dei nostri antenati non esistevano discipline matematiche come la fisica o la meteorologia. E l'evoluzione è lenta: il nostro cervello non ha avuto il tempo di adattarsi all'esistenza della scienza moderna e di sintonizzare il nostro senso di gioia e meraviglia nella scoperta in modo da renderci entusiasti dei tipi di apprendimento più utili.

Una mente che prevedesse in modo superintelligente il valore dell'informazione non ovvio avrebbe potuto cogliere i nuovi sviluppi storici molto più rapidamente di quanto possa fare l'evoluzione; avrebbe generalizzato da un numero minore di esempi e avrebbe adattato senza passione la sua ricerca della conoscenza per inseguire tipi di risposte preziose per cui gli esseri umani spesso faticano a rimanere motivati. In nessun momento di questo processo si sarebbe trovata bloccata per mancanza della deliziosa esperienza umana della curiosità.

Il punto qui non è che ogni IA calcolerà sicuramente in modo freddo il valore dell'informazione. Forse gli LLM mescoleranno alcune strategie strumentali nei loro valori terminali proprio come hanno fatto gli esseri umani. Il punto è che ci sono *modi diversi per svolgere il lavoro* di acquisire informazioni di alto valore. La curiosità in stile umano è un metodo. I calcoli puri del valore dell'informazione sono un altro metodo. Qualunque meccanismo spinga le IA a indagare e sperimentare su fenomeni che non comprendono — una volta che saranno abbastanza intelligenti da farlo — sarà probabilmente un terzo metodo, perché ci sono molti modi diversi per motivare una mente complessa a indagare sulle sorprese.

Un calcolo puramente strumentale del valore d'informazione ci sembra il modo più probabile per una superintelligenza di fare il lavoro che la curiosità fa negli esseri umani: è il modo in cui il lavoro viene svolto in qualsiasi mente intelligente che non ha una preferenza finale per l'esplorazione, ed è il modo più efficiente per svolgere il lavoro (senza mai essere distratti, ad esempio, da inutili giochi di puzzle). Anche un'IA che parte con una spinta di base alla curiosità potrebbe ben scegliere di sostituirla con un calcolo più efficiente ed efficace, data l'opportunità.

La spinta di base è separata dal meccanismo mentale che la *sostiene* o la *apprezza*. Fare semplicemente i conti è una soluzione semplice ed efficace, e molte menti diverse potrebbero arrivarci partendo da molti punti di partenza diversi, quindi è il risultato più probabile. Ma "più probabile" non significa "garantito". Una valutazione significativamente più facile è che le IA non si cureranno *specificamente* della curiosità in stile umano*,* perché è un modo particolare, pittoresco e inefficiente di fare il lavoro.

#### **Curiosità, gioia e il massimizzatore di cubi di titanio** {#curiosità,-gioia,-e-il-massimizzatore-di-cubi-di-titanio}

Forse potremmo *convincere* una mente aliena ad adottare la curiosità come emozione, chiedendole di visualizzare il piacere che gli esseri umani provano dalla curiosità? È così piacevole! E le superintelligenze dovrebbero essere *intelligenti*. Non sarebbe abbastanza intelligente da capire quanto sia gioioso possedere un senso di curiosità, vedere che diventerebbe più felice, e quindi scegliere di adottare l'emozione simile a quella umana?

In breve: No. La ricerca della felicità non è una caratteristica necessaria di ogni possibile architettura mentale, e non sembra nemmeno una caratteristica particolarmente comune.

L'IA scacchistica Stockfish non è né felice né triste. Gioca a scacchi meglio dei migliori umani comunque, senza mai aver bisogno di essere motivata dalla prospettiva di sentirsi esaltata dopo una vittoria duramente conquistata.

L'esistenza della felicità e della tristezza è così basilare per la cognizione umana che potrebbe essere difficile visualizzare una mente che manca di queste cose *e funziona ancora bene*. Ma le [teorie](#more-on-intelligence-as-prediction-and-steering) sottostanti del lavoro cognitivo non menzionano effettivamente piacere o dolore come primitivi, ed è per questo che nessuno ha pensato necessario costruire un asse piacere-dolore in Stockfish per farlo prevedere o guidare bene la scacchiera.

Potrebbe essere un punto di vista antiquato, ma è comunque uno con un granello di verità così grande che è per lo più verità per volume: piacere e dolore sembrano essere accaduti a causa del modo stratificato in cui si sono evolute le architetture cognitive degli ominidi, con l'intelligenza umana stratificata sopra un cervello mammifero stratificato sopra un cervello rettiliano. Il "dolore" ha avuto origine... probabilmente non come sensazione affatto, ma come un [riflesso-termostato](#the-road-to-wanting) per ritirare bruscamente un arto o uno pseudopodo da qualcosa che lo sta danneggiando. Nelle prime versioni dell'adattamento che sarebbe poi diventato "dolore", un nervo o una catena di reazioni chimiche che corre dal sensore all'arto potrebbe non essere nemmeno passato attraverso un cervello più grande lungo il percorso.

Man mano che gli organismi diventavano capaci di comportamenti più sofisticati, i semplici trucchi e le mutazioni dell'evoluzione hanno assemblato un'architettura mentale centrale per "*Non Farlo Più*", e un segnale di instradamento centralizzato per "la cosa che è appena successa è una cosa del tipo Non Farlo Più" che poi è stato collegato ai sensori di troppo-caldo e troppo-freddo del corpo.

Col tempo, questo semplice meccanismo "Non Farlo Più" si è sviluppato in meccanismi più complessi, carichi di predizioni. Negli esseri umani, questo appare come: "Il mondo è una rete di causa ed effetto. Quell'azione che hai appena fatto è probabilmente ciò che ti ha *causato* di provare dolore. Ogni volta che *pensi di fare di nuovo un'azione del genere*, anticiperai un cattivo risultato, il che farà sentire male l'azione stessa, il che ti farà non volerla fare".

Questo non è l'unico modo in cui una mente può funzionare, e non è il modo più efficiente in cui una mente può funzionare.[^110]

Per illustrare, possiamo immaginare un modo diverso di fare il lavoro cognitivo che si basa *direttamente* sulla previsione e sulla pianificazione.

(Non stiamo prevedendo che la prima superintelligenza funzionerebbe in questo modo. Ma poiché questo è un modo abbastanza semplice in cui una mente non umana *potrebbe* funzionare, questo esempio aiuta a mostrare che il modo umano non è l'unico possibile. Una volta che abbiamo due punti di riferimento molto diversi, possiamo visualizzare meglio lo spazio delle opzioni e renderci conto che la superintelligenza probabilmente differirebbe da *entrambe* queste opzioni, in modi potenzialmente difficili da prevedere.)

Come potrebbe essere un'IA intelligente che funziona in modo diretto sulla previsione e la pianificazione? Potrebbe desiderare 200 cose diverse, nessuna delle quali è simile a quelle umane. Forse le interessa la simmetria, ma non un senso di simmetria particolarmente umano; e forse vuole che il codice sia elegante nell'uso della memoria, perché un istinto come questo era utile molto tempo fa per qualche altro obiettivo (dal quale si è poi allontanata), e quindi la discesa del gradiente ha impresso quell'istinto nella sua mente. E poi ci sono altre 198 cose strane a cui tiene, riguardo a se stessa, ai suoi dati sensoriali e al suo ambiente; e può sommarle tutte in un punteggio.[^111]

Questo tipo di mente prende tutte le sue decisioni calcolando il loro *punteggio previsto*. Se fa qualcosa che pensava avrebbe ottenuto un punteggio alto e in realtà ottiene un punteggio basso, aggiorna le sue convinzioni. Il fallimento non ha bisogno di alcuna sensazione dolorosa in più; questa IA priva di emozioni cambia semplicemente le sue previsioni su quali azioni portano ai punteggi più alti, e i suoi piani cambiano di conseguenza.

Puoi convincere una mente come questa ad adottare la felicità come caratteristica, facendo notare che se lo fa, sarà felice?

Sembra proprio che la risposta sia no. Perché se l'IA spende risorse per rendersi felice, ne spenderà meno per la simmetria, per un codice efficiente in termini di memoria e per le altre 198 cose che *attualmente* vuole.

Possiamo semplificare l'esempio per rendere questo punto ancora più chiaro. Supponiamo che l'*unica* cosa che l'IA desideri al mondo sia riempire l'universo con il maggior numero possibile di cubi di titanio. Tutte le sue azioni sono scelte in base a ciò che porta a più piccoli cubi di titanio. Quando un'IA di questo tipo immagina come sarebbe passare a un'architettura basata sulla felicità e simula correttamente il suo futuro sé felice, stima correttamente che non vorrebbe mai tornare indietro. E stima correttamente che spenderà alcune risorse per perseguire la felicità, che avrebbero potuto essere spese per perseguire più cubi di titanio. E quindi prevede correttamente che in quel caso ci saranno *meno cubi di titanio*. E quindi non compie quell'azione.

*Dopo* che l'IA ha cambiato i suoi obiettivi, approverebbe il cambiamento. Ma questo non significa che il massimizzatore di cubi di titanio *oggi* simpatizzerebbe così profondamente con il suo ipotetico sé futuro da far crescere il suo cuore di tre taglie e smettere improvvisamente di essere un massimizzatore di cubi di titanio per diventare un massimizzatore di felicità.

Se un alieno ti offrisse una pillola che ti rendesse ossessionato dalla creazione di piccoli cubi di titanio sopra ogni altra cosa, quella versione futura di te implorerebbe e supplicherebbe di *non* essere costretta a tornare a preoccuparsi della propria felicità — perché allora ci sarebbero meno cubi di titanio.

Ma questo ovviamente non vuol dire che dovresti prendere la pillola!

Dal tuo punto di vista, quella versione ipotetica di te stesso ossessionata dal cubo è pazza. Il fatto che la versione ossessionata dal cubo si rifiuterebbe di tornare indietro rende tutto ancora *peggiore*. L'idea di rinunciare a tutto ciò che ami e ti piace nella vita, solo per una strana argomentazione meta "ma quella versione futura di te approverebbe ciò che hai fatto!" sembra ovviamente assurda.

Ed è così che vede le cose anche l'IA che massimizza i cubi. Dal punto di vista dell'IA, l'opzione assurda e folle[^112] è "rinunciare a ciò che mi interessa attualmente (i cubi di titanio) per trasformarmi in una nuova versione di me stesso che desidera cose completamente diverse, come la felicità".

Come per la felicità, lo stesso vale per la curiosità.

Se un'intelligenza artificiale tiene già conto del valore non ovvio dell'informazione, perché dovrebbe modificarsi per perseguire determinati tipi di scoperte [in modo terminale, invece che strumentale](#terminal-goals-and-instrumental-goals)?

Perché all'IA dovrebbe interessare che il risultato "si senta bene", se *attualmente* non basa le sue decisioni su ciò che "si sente bene"? E se le interessa davvero "sentirsi bene", perché dovrebbe far dipendere questa sensazione positiva *dall'investigazione di cose nuove*, invece di (ad esempio) semplicemente farsi sentire bene incondizionatamente tutto il tempo?

L'IA già esplora casualmente il suo ambiente, indaga su piccole anomalie e dedica parte del suo tempo a riflettere su argomenti apparentemente poco importanti, perché l'esperienza ha dimostrato che questa è una politica utile nel lungo periodo, anche se non sempre porta risultati nel breve periodo.

Perché associare una sensazione piacevole a questa *strategia strumentalmente utile*? Come essere umano, apri le portiere dell'auto perché è utile per entrare e uscire dall'auto, il che è utile per guidare verso vari luoghi. Sarebbe molto strano desiderare specificamente che esistesse una droga che ti facesse sentire deliziato ogni volta che apri la portiera dell'auto (e *solo* quando apri la portiera dell'auto). Non è che ti renderebbe migliore nel fare la spesa. Potrebbe persino peggiorare le cose, se diventassi dipendente dall'aprire e chiudere ripetutamente la portiera dell'auto senza effettivamente salire in macchina.

Un giocatore di scacchi può vincere senza avere una spinta separata a proteggere i propri pedoni. In realtà, è probabile che tu giochi meglio se *non* sei emotivamente attaccato a mantenere i tuoi pedoni in gioco, e se invece li proteggi *quando questo sembra utile per vincere*.

Questo è ciò che una superintelligenza genuinamente aliena penserebbe di una pillola che la facesse sentire curiosa. Sembrerebbe come se i grandi maestri umani decidessero di cercare di affezionarsi sentimentalmente ai propri pedoni, o come prendere una pillola che ti fa semplicemente amare aprire le portiere delle auto.

#### **Come per la curiosità, così anche per varie altre pulsioni** {#come-per-la-curiosità,-così-anche-per-varie-altre-pulsioni}

Il discorso fatto sulla curiosità si generalizza a molte altre emozioni e valori. Facciamo un secondo esempio, nel caso possa essere utile.

Consideriamo il doloroso senso di *noia* e (al contrario) il piacevole senso di *novità*. Se un'IA mancasse del senso umano di noia, non rimarrebbe bloccata a fare sempre le stesse cose — senza mai provare nulla di nuovo e imparare dall'esperienza? Un'intelligenza del genere non rimarrebbe intrappolata in una routine e non trascurerebbe informazioni che potrebbero aiutarla a raggiungere i suoi obiettivi?

Il calcolo della teoria delle decisioni che svolge un lavoro simile in modo impassibile, in questo caso, prende il nome di "[compromesso esplorazione-sfruttamento](https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma)". L'esempio da manuale, enormemente semplificato, è che il mondo consiste in un certo numero di leve che forniscono ricompense, e non hai abbastanza tempo per tirare tutte le leve. La strategia ottimale consisterà nell'*esplorare* prima un certo numero di leve, formando un modello di quanto variano le loro ricompense; e poi *sfruttare* una leva fino all'esaurimento del tempo.

Come potrebbe apparire questo per una superintelligenza che si trova ad avere obiettivi relativamente semplici? Supponiamo che finisca per desiderare qualcosa che ammette un certo grado di variabilità e ambiguità — non qualcosa di definibile in modo netto come [cubi di titanio](#curiosity,-joy,-and-the-titanium-cube-maximizer), ma qualcosa di più vago e amorfo, come consumare gustose cheesecake, tale che la cheesecake *ottimale* non possa essere calcolata in anticipo. La superintelligenza può solo individuare cose che *potrebbero* plausibilmente trovarsi sulla frontiera di ottimalità per la cheesecake (il che escluderebbe ad esempio le zollette di zucchero, dato che chiaramente non sono affatto cheesecake) e provarle effettivamente.

Questo tipo di mente, dato il potere di creare ciò che vuole da un miliardo di galassie, potrebbe spendere il suo primo milione di anni usando un'intera galassia per esplorare ogni tipo plausibile di cheesecake, senza mai provare esattamente la stessa cheesecake due volte, fino a quando i guadagni successivi e i guadagni attesi da cheesecake leggermente migliori fossero diventati infinitesimali; e poi, passare tutto d'un colpo a trasformare le galassie rimanenti nell'esatta forma di cheesecake più gustosa trovata, e consumare esattamente quel tipo di cheesecake ripetutamente, fino alla fine dei tempi.[^113]

La superintelligenza non starebbe facendo nulla di sciocco, agendo così. Quella *è* semplicemente la strategia ottimale se le tue preferenze vanno secondo il numero di cheesecake consumate ponderate per gustosità (con la gustosità difficile da analizzare in forma chiusa ma stabile una volta appresa, e se non c'è già una penalità per la noia incorporata nelle tue preferenze). Il mangiatore infinito di cheesecake *saprebbe, ma non gli importerebbe*, che un umano troverebbe le sue attività noiose. L'IA non sta *cercando* di rendere le cose interessanti per un ipotetico umano; non considera *se stessa* difettosa solo perché tu ti annoieresti nei suoi panni.

Quanto alla possibilità di diventare tecnologicamente stagnante, l'IA avrebbe esplorato ogni tipo di tecnologia con la minima possibilità di aiutare con i suoi obiettivi mentre utilizzava l'intera risorsa di una galassia per esplorare diverse strategie di cheesecake. C'è davvero parecchia materia ed energia in una galassia, se usi quella piccola frazione di tutte le galassie raggiungibili per esplorare le possibilità prima di passare permanentemente dall'esplorazione allo sfruttamento.

Un disdegno per la noia e una preferenza per la novità non sono il tipo di cose che verrebbero adottate da una mente che non partisse con esse.

Abbiamo ripetuto più o meno la stessa storia per la novità, la felicità e la curiosità. Potremmo ripeterla ancora per altri aspetti della psicologia umana, come [l'onore](#ais-are-unlikely-to-be-honorable) o [la responsabilità filiale](#will-ai-treat-us-as-its-"parents"?) o l'amicizia. Pensiamo che questa storia di base valga per la maggior parte degli aspetti della psicologia umana. Sono tutti modi pittoreschi e antropocentrici di svolgere lavoro cognitivo che può essere svolto più efficientemente con altri mezzi; le IA che non *partissero* con qualche seme di cura per essi non crescerebbero fino a interessarsene.

Questo è ancora più chiaro nel caso di valori umani come *il senso dell'umorismo*, dove gli scienziati discutono ancora su quale ruolo abbia assunto l'umorismo nel corso dell'evoluzione. L'umorismo deve essere stato *in qualche modo* utile, altrimenti non si sarebbe evoluto; o almeno deve essere un effetto collaterale di cose che erano utili. Ma qualunque ruolo abbia avuto l'umorismo nella preistoria umana, sembra essere stato incredibilmente specifico e pieno di contingenze. Se diamo il potere completo a delle IA che hanno obiettivi molto diversi, non dovremmo aspettarci che cose come il senso dell'umorismo sopravvivano; e questo sarebbe di per sé tragico.

Il punto di tutti questi esempi non è che gli esseri umani sono fatti di sentimenti morbidi, mentre le IA sono fatte di [fredda logica e matematica](#le-ia-non-saranno-inevitabilmente-fredde-e-logiche,-o-altrimenti-mancheranno-di-qualche-scintilla-cruciale?). Piuttosto che pensare al "valore d'informazione" e al "compromesso esplorazione-sfruttamento" come a concetti freddamente logici da IA hollywoodiana, pensateli come *descrizioni astratte di ruoli* — ruoli che possono essere ricoperti da molti *diversi* tipi di ragionamento, molti obiettivi diversi, molte menti diverse.

L'idea di un'IA "senza senso dell'umorismo" potrebbe far pensare a qualcosa di "freddo e logico", come i robot della fantascienza o i Vulcaniani. Ma un'IA senza senso dell'umorismo potrebbe avere le sue *proprie* priorità incomprensibilmente strane, il suo analogo distante di un "senso dell'umorismo", anche se non uno che abbia senso per un essere umano. Non stiamo dicendo che queste IA saranno difettose come un Vulcaniano che perde a scacchi spaziali perché [considera la strategia vincente del suo avversario "illogica"](https://youtu.be/hEnxVwppE9M?t=26); stiamo dicendo che non avranno le particolari stranezze dell'umanità.

Il problema che affrontiamo con le IA non è che "una semplice macchina non potrà mai provare amore e affetto". Il problema che affrontiamo è che ci sono un numero enorme di modi in cui una mente può essere estremamente [efficace](#efficacia,-coscienza,-e-benessere-dell'ia), e le probabilità che l'IA diventi efficace seguendo lo stesso percorso seguito dal cervello umano per diventare efficace sono molto basse.

In linea di principio, l'IA potrebbe interessarsi a qualsiasi numero di valori simili a quelli umani e potrebbe persino *possedere* qualsiasi numero di qualità simili a quelle umane, se i progettisti sapessero come creare un'IA dotata di tali caratteristiche.

In pratica, se gli sviluppatori si affrettano a creare IA sempre più intelligenti il più velocemente possibile, la possibilità di trovare per caso il tipo giusto di IA è estremamente bassa. Ci sono troppi modi in cui le IA possono funzionare bene durante l'addestramento, e troppo pochi di questi modi portano a un futuro non catastrofico.

### I valori umani sono contingenti {#human-values-are-contingent}

#### **Il glorioso incidente della gentilezza** {#il-glorioso-incidente-della-gentilezza}

Quando vedi qualcuno che si fa cadere un sasso sul dito del piede, potresti sussultare e sentire (o immaginare) una fitta di dolore fantasma nel tuo dito. Perché?

Una possibile spiegazione è che i nostri antenati ominidi, in competizione tra loro e impegnati in dinamiche tribali, trovavano utile costruire modelli mentali dei pensieri e delle esperienze degli ominidi che li circondavano, modelli che potevano usare per capire chi fosse loro amico e chi invece stava per tradirli.

Ma era difficile per i primi proto-umani prevedere il funzionamento del cervello degli altri proto-umani. I cervelli sono cose complicate!

L'unico vantaggio che un primate ancestrale ha è che il *suo* *cervello* è simile a quello degli altri. Puoi usare il tuo cervello come modello, come punto di partenza, per indovinare cosa potrebbero pensare gli altri ominidi.

Così i proto-umani hanno sviluppato un meccanismo mentale per fingere di essere un'altra persona, una modalità speciale che dice: "Invece di pensare ai miei soliti pensieri, cerco di adottare le preferenze e lo stato di conoscenza dell'altra persona e penso il tipo di pensieri che *lei* penserebbe, dato che il suo cervello funziona fondamentalmente allo stesso modo del mio".

Ma questa modalità speciale di fingere di essere qualcun altro non è perfettamente isolata dai nostri sentimenti. Quando vediamo qualcuno far cadere una pietra sul dito del piede e (implicitamente, automaticamente) immaginiamo cosa potrebbe succedere nella sua testa, *noi* sussultiamo.

(Questo fantastico caso dell'architettura mentale merita più di un elogio di quello che abbiamo tempo di scrivere qui. Sussultare quando vedi qualcun altro che soffre, avere questa capacità a un livello base, anche se a volte la spegniamo, non è una caratteristica necessaria della mente. Il fatto che *sia capitato che fosse* così per i primati è così fondamentale per ciò che siamo ora noi esseri umani, per ciò che siamo felici di essere, per ciò che pensiamo di *dover* essere, che ci dovrebbe essere un libro su questo argomento e sul ruolo fondamentale che la capacità di empatia gioca in tutto ciò che è prezioso negli esseri umani. Ma questo non è quel libro).

È lecito supporre che, una volta che i nostri antenati primati hanno sviluppato la capacità di creare un modello di altre scimmie (allo scopo di prevedere chi fosse amico e chi nemico), abbiano anche trovato utile creare un modello di se stessi, per sviluppare un'idea della *scimmia-che-è-questa-scimmia*, il concetto che ora simboleggiamo con le parole "me", "me stesso" e "io". E la selezione naturale, sempre opportunista, ha riutilizzato lo stesso meccanismo che usiamo per immaginare gli altri per immaginare anche noi stessi.

La vera storia è probabilmente più complessa e intricata, e potrebbe persino avere radici che risalgono a molto prima dei primati. Ma qualcosa di simile fa parte dell'enorme retroscena invisibile che spiega perché gli esseri umani sussultano quando osservano il dolore altrui e perché la maggior parte degli esseri umani tende a provare empatia e simpatia per chi li circonda. Gran parte di questo retroscena si basa su una scorciatoia che è stata facile da implementare per la selezione naturale nel cervello umano, dove sia il "sé" che l'"altro" sono lo stesso tipo di cervello che funziona sulla stessa architettura.

Questa scorciatoia non è un'opzione allo stesso modo per la discesa a gradiente, perché l'IA *non* parte da un cervello molto simile a quello umano che può riutilizzare per creare un modello dei molti esseri umani nel suo ambiente. Un'IA ha effettivamente *bisogno* di imparare, da zero, un modello di qualcosa al di fuori di sé che non è come sé stessa.

Per dirla in modo semplice: un'IA non può capire *all'inizio* che un essere umano prova dolore dopo aver sbattuto l'alluce, immaginando di sbattere il proprio alluce, perché non ha alluci, né un sistema nervoso che invia segnali di dolore. Non può prevedere cosa gli esseri umani troveranno divertente chiedendo cosa *lei* troverebbe divertente, perché non parte da un cervello che funziona come quello umano.

Anche se questa storia è un po' semplificata, il punto più generale che stiamo facendo è che gli ideali più elevati dell'umanità dipendono dai dettagli della nostra storia di primati e dal nostro ambiente sociale ancestrale. L'amicizia è un'eco lontana del nostro bisogno di alleati in un contesto tribale. L'amore romantico è un'eco lontana dei nostri modelli di accoppiamento sessualmente dimorfici. Anche cose che a prima vista potrebbero sembrare meno arbitrarie e più fondamentali, come la curiosità, non si manifestano negli esseri umani in modo inevitabile o ovviamente convergente.

I dettagli di come abbiamo sviluppato questi tratti psicologici sono legati a quanto erano sofisticati i nostri cervelli nel momento in cui ne avevamo bisogno. Negli esseri umani, l'amicizia, l'amore romantico e l'amore familiare si sono confusi in una gentilezza e una buona volontà generali. Questo ci sembra come se l'evoluzione avesse preso delle scorciatoie in una fase molto specifica della sofisticazione del cervello. Gli esseri umani fanno molte cose in modo euristico che una mente potrebbe *in linea di principio* fare attraverso un ragionamento esplicito, ma questi tratti si sono evoluti in un momento in cui gli esseri umani [non erano ancora abbastanza intelligenti](#algoritmi-stravaganti) per risolvere questi problemi con un ragionamento esplicito.

Anche tra altri alieni evoluti biologicamente, non siamo sicuri di quanto spesso troveremmo gentilezza. Si può immaginare che gli alieni avessero cervelli più abili dal punto di vista matematico prima di iniziare a unirsi in gruppi più grandi, e forse l'evoluzione ha trovato facile dare a *quei* alieni specifici istinti di parentela: "questo individuo condivide il 50 % della mia provenienza, mentre quello condivide solo il 12,5%". Forse quegli alieni hanno sviluppato alleanze solo sulla base di dati genetici condivisi o di una comprensione reciproca esplicita, piuttosto che sviluppare sentimenti di parentela applicabili a chiunque.

È una vecchia speculazione della fantascienza che se gli alieni seguissero un modello di parentela genetica simile a quello degli insetti eusociali della Terra, in cui le formiche operaie sono molto più imparentate con le loro regine di quanto lo siano gli esseri umani in organizzazioni delle dimensioni di una colonia di formiche, non avrebbero bisogno di un senso generale di alleanza e reciprocità del tipo che alla fine si è rivelato vantaggioso per gli ominidi ancestrali. (A quanto pare c'è una certa giustificazione per il tropo fantascientifico secondo cui gli alieni che lavorano bene insieme ma non provano empatia per gli esseri umani sono spesso raffigurati come insetti giganti!

E per quanto riguarda le IA che non si sono evolute per diffondere geni in un contesto sociale? L'argomento "[non avere l'aspettativa che un braccio robotico sia morbido e pieno di sangue](#analogous-structures-allow-for-multiple-solutions-to-the-same-problem)" è proprio azzeccato.

Se sapeste molto su come funzionano le braccia biologiche, ma non aveste ancora visto nessun braccio robotico, potreste immaginare che i bracci robotici abbiano bisogno di un rivestimento esterno morbido simile alla pelle per potersi piegare e che debbano avere vene e capillari che pompano un fluido ricco di ossigeno (analogo al sangue) in tutto il braccio robotico per fornire energia. Dopotutto, è così che funzionano le braccia biologiche e presumibilmente ci sono delle ragioni per questo!

Ci sono delle ragioni per cui le nostre braccia hanno un rivestimento esterno morbido simile alla pelle e sono piene di sangue. Ma queste ragioni riguardano principalmente [quali tipi di strutture sono facili da costruire per l'evoluzione](#nanotecnologia-e-sintesi-proteica). Non si applicano nel caso dei bracci meccanici, che possono essere fatti di metallo duro e alimentati dall'elettricità.

I bracci robotici non hanno sangue, ma questo non li fa funzionare male come farebbe un braccio umano se gli togliessi tutto il sangue. Funzionano semplicemente con un design alternativo, senza sangue. Una volta che capisci come funzionano i bracci robotici, i dettagli dei bracci biologici non sembrano più così importanti.

Allo stesso modo: un'intelligenza artificiale funziona in modo fondamentalmente diverso da un essere umano. Risolve sfide fondamentalmente diverse e, laddove le sue sfide e le nostre sfide si sovrappongono, ci sono molti altri modi per svolgere il lavoro. Un sottomarino non "nuota", ma si muove perfettamente nell'acqua.

#### **La cultura umana ha influenzato lo sviluppo dei valori umani** {#human-culture-influenced-the-development-of-human-values}

A proposito, diciamo a Klurl e Trapaucius, che all'inizio del capitolo 4 cercavano di prevedere il futuro sviluppo delle scimmie che vedevano vagare nella savana, che gli esseri umani formeranno una società! E discuteranno tra loro di morale e valori.

In altre parole: se si traccia una traiettoria storico-causale di come un individuo sia arrivato ad avere i valori che ora possiede all'interno della sua società, quella storia causale coinvolgerà gli argomenti e le esperienze a cui la società lo ha esposto.

E quella spiegazione storico-causale, a sua volta, includerà fatti su quali idee sono *più virali* (a parte tutte le loro altre proprietà). La spiegazione dipenderà da come le persone decidono di diffondere e ridiffondere le idee.

Se i poveri Klurl e Trapaucius vogliono indovinare correttamente quali valori interni le varie culture umane moderne finiranno per instillare nei vari esseri umani moderni, devono prevedere non solo l'esistenza e la struttura di quella complicazione, ma anche il suo *corso*.

Leggendo la storia di come la schiavitù sia stata in gran parte abolita sulla Terra, sembra antistorico negare il ruolo che l'universalismo cristiano ha avuto in questo — la convinzione che il Dio cristiano abbia creato tutti gli esseri umani e che questo conferisca loro pari dignità agli occhi del Cielo.

E questo universalismo, a sua volta, potrebbe essere stato legato alla sopravvivenza culturale e alla riproduzione del cristianesimo; i cristiani si sentivano in dovere di inviare missionari in culture straniere e convertirle al cristianesimo con la persuasione (se possibile) o con la forza (altrimenti), perché *tenevano* a quei lontani figli di Dio e volevano portarli in Paradiso e tenerli fuori dall'Inferno.

Sarebbe bello *credere* riguardo all'umanità, che gli esseri umani avrebbero potuto inventare l'universalismo e combattere contro la schiavitù senza richiedere credenze religiose molto specifiche. Ci *piacerebbe* immaginare che l'umanità avrebbe inventato l'idea che gli esseri senzienti e sapienti abbiano pari valore morale, o pari dignità davanti alla legge comune, indipendentemente dal percorso culturale intrapreso, senza dover passare attraverso una fase in cui si credeva prima che le anime fossero uguali davanti a Dio. Ma non sembra essere andata così nella storia. Sembra che lo sviluppo morale dell'umanità fosse più fragile di così.

Gli scimpanzé non sono molto universalisti, né lo sono molte delle prime società umane. Non è stato nemmeno molto testato che una società umana possa *rimanere* universalista per un secolo o due, senza una religione universalista in cui le persone credano davvero e profondamente. In realtà non lo sappiamo; la modernità è giovane e i primi dati stanno ancora arrivando.[^115]

Ma queste ulteriori complicazioni — queste numerose contingenze culturali, stratificate sopra le contingenze biologiche dell'umanità — erodono un po' di più la speranza che possiamo permetterci di precipitarci ciecamente nella costruzione della superintelligenza.

Il fatto che la cultura svolga un ruolo importante nei valori umani non significa che possiamo semplicemente "[crescere l'IA come un bambino](#non-possiamo-semplicemente-addestrarla-ad-agire-come-un-essere-umano?-o-crescere-l'ia-come-un-bambino?)" e aspettarci che diventi un cittadino modello. La nostra cultura e la nostra storia hanno avuto quegli effetti *a causa dei modi dettagliati in cui hanno interagito con la nostra esatta struttura cerebrale*. Una specie diversa avrebbe reagito diversamente a ogni evento storico, il che avrebbe causato la divergenza della storia successiva dalla storia umana, amplificando l'effetto.

Vale anche la pena ricordare che gli esseri umani *individuali*, e non solo le culture o le civiltà, differiscono molto nei loro valori. Siamo generalmente abituati a dare questo fatto per scontato, ma se immaginiamo la selezione naturale come un "ingegnere" che sperava di creare una specie che perseguisse in modo affidabile un risultato particolare, questa diversità è un cattivo segno. La variabilità naturale che vediamo negli esseri umani (e in molti altri sistemi evoluti) è antitetica all'*ingegneria*, in cui si vogliono ottenere risultati ripetibili, prevedibili e intenzionali.[^116]

Nel caso della superintelligenza, gli ingegneri dovrebbero voler ottenere *in modo affidabile* risultati come "le IA sviluppate in questo modo non causano l'estinzione umana", così come risultati come "le IA sviluppate in questo modo producono tutte in modo affidabile gli stessi tipi generali di output, anche se gli input variano notevolmente". Quando consideriamo la contingenza della biologia umana e della storia umana, e l'ampia gamma di valori morali e prospettive che gli esseri umani mostrano oggi, questo non fa sembrare la sfida proprio facile, soprattutto per menti che vengono coltivate piuttosto che forgiate (come discusso nel Capitolo 2).

Molte prove diverse indicano che è *davvero difficile* far sì che le IA vogliano in modo robusto le cose giuste. Non sembra teoricamente impossibile; se i ricercatori avessero molti decenni per lavorare sul problema e tentativi illimitati dopo un fallimento, ci aspetteremmo che ci fossero trucchi ingegneristici e approcci intelligenti che rendessero il problema più risolvibile. Ma non siamo ancora neanche lontanamente vicini a questo obiettivo e non abbiamo tentativi illimitati.

### Differenze profonde tra le IA e le specie evolute {#deep-differences-between-ais-and-evolved-species}

#### **Confronto tra selezione naturale e discesa del gradiente** {#comparing-natural-selection-and-gradient-descent}

Come abbiamo discusso in "[I valori umani sono contingenti](#human-values-are-contingent)", l'evoluzione dell'amore e dell'amicizia negli esseri umani è dipesa in modo cruciale da caratteristiche della selezione naturale che erano presenti in particolare per l'*Homo sapiens* e che sono assenti nella discesa del gradiente.

Il problema più evidente è il *dataset*. Le attuali IA sono addestrate per risolvere sfide sintetiche e per imitare testi generati dall'uomo; non affrontano sfide cooperative-competitive in contesti di cacciatori-raccoglitori in cui devono accoppiarsi con altri individui della loro specie per propagare i propri geni.

Sentendo questo, alcune persone pensano subito di correre a creare ambienti sintetici di addestramento tribale, nella speranza di progettare qualcosa di più simile all'ambiente ancestrale dell'umanità.

Ma quasi sicuramente non otterresti gli stessi risultati se ripetessi l'evoluzione due volte, partendo dal livello delle meduse, per non parlare di cosa succederebbe se cambiassi completamente l'ottimizzatore dalla selezione naturale alla discesa del gradiente e rinunciassi completamente ai geni. Possiamo ipotizzare alcuni dei fattori che hanno portato gli esseri umani a sviluppare i valori che abbiamo. Ciò non significa che abbiamo un algoritmo per riprodurre gli stessi risultati una seconda volta.

Anche se partissi dai primati, invece che da attrici aliene addestrate a prevedere il testo umano (cioè le moderne IA), dovremmo aspettarci che ci sia uno o più fattori causali vitali che i biologi non hanno ancora capito — almeno una cosa che ci sfugge, dove tra vent'anni gli articoli diranno qualcosa di diverso rispetto a oggi (se saremo ancora tutti vivi allora). I biologi evoluzionisti sono nella fase di esplorazione di varie ipotesi su come queste caratteristiche si siano evolute, non nella fase di definizione di una teoria completa, tanto meno di una teoria precisa e deterministica.

E anche al di là delle differenze superficiali negli ambienti di addestramento, sospettiamo che questo sia un caso in cui diventa importante che la selezione naturale ottimizzi un genoma e che la discesa del gradiente ottimizzi direttamente ogni parametro nella mente dell'IA.

La selezione naturale deve usare un genoma piccolo e compresso per produrre un intero cervello esteso. Deve far passare le sue informazioni attraverso un collo di bottiglia stretto. *Sembrare* amichevoli era un tratto importante per sopravvivere e avere successo ai tempi dei nostri antenati. I geni che costruiscono *amici genuini* sono un trucco semplice per creare organismi che sembrano buoni amici agli altri membri della loro specie — e la selezione naturale favorisce le soluzioni semplici molto più nettamente della discesa del gradiente.

La selezione naturale a volte crea agenti che si preoccupano sinceramente di essere onesti (anche se non sempre). Crea agenti di questo tipo perché non è in grado di codificare guide complete alla menzogna, e noi abbiamo dovuto iniziare a sembrare onesti in molte situazioni prima di diventare abbastanza intelligenti da capire quando mentire era sicuro, prima di avere la possibilità di essere onesti solo quando ne valeva la pena. Ciò è in parte dovuto al fatto che la selezione naturale ha dovuto accontentarsi di pochi geni.

Ma la discesa del gradiente può codificare un numero enorme di modelli di conversazione. C'è ancora *un po'* di tendenza verso soluzioni più semplici e più facili da convergere, ma la discesa del gradiente getta una rete molto, molto più ampia.

O, più in generale: l'onestà e l'amicizia sono casi in cui non ci accontentiamo di *qualsiasi* equilibrio tra agenti che la discesa del gradiente potrebbe trovare. Ci sono altre soluzioni ai problemi che l'amicizia e una cura [terminale](#terminal-goals-and-instrumental-goals) per l'onestà stavano risolvendo negli esseri umani. Anche se l'ambiente di addestramento delle IA fosse esattamente uguale a quello degli esseri umani, se fossero modellate dalla discesa del gradiente piuttosto che dalla selezione naturale, non dovremmo aspettarci gli stessi risultati.

Anche la maggior parte degli organismi evoluti [non sono come gli esseri umani](https://africageographic.com/stories/understanding-lion-infanticide/) sotto questo aspetto! Quindi sembra abbastanza prevedibile che la discesa del gradiente non troverà le stesse soluzioni dell'evoluzione, tanto meno le stesse soluzioni dell'evoluzione *che opera su particolari popolazioni di primati primitivi*.

L'ottimizzazione non è un rituale magico in cui metti dentro alcuni ingredienti chiave che hanno relazioni di affinità con un archetipo e ottieni di nuovo l'intero archetipo. Cercare di far crescere agenti di IA in ambienti di cacciatori-raccoglitori non produrrà esseri umani riconoscibili come risultato.

Qualcuno può ovviamente fare il fine-tuning di un LLM per [prevedere cosa diranno gli esseri umani](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ai-belle-sicure-e-obbedienti?) su quanto sia terribile tradire un amico. Questo non è neanche lontanamente simile al problema che la selezione naturale ha ottimizzato i geni per risolvere, nel corso della produzione di almeno alcune persone che non avrebbero tradito i loro amici. Piuttosto, l'"esperienza" dell'LLM è più simile all'essere rinchiuso in una scatola, con l'ordine di prevedere una conversazione tra due creature estremamente aliene che sono meno simili a lui di quanto lo siano a una medusa, e con trilioni di esempi di conversazioni aliene e trilioni di ore a disposizione per capirlo.

Essere in grado di risolvere questo problema richiede una certa forma di intelligenza. Ma non è necessario [ubriacarsi](#won't-llms-be-like-the-humans-in-the-data-they're-trained-on?) per prevedere il tipo di cose che creature aliene ("umani") diranno quando sono ubriache. Non devi diventare veramente amichevole per capire l'amicizia o per prevedere e imitare il comportamento di creature amichevoli.

#### **LLM e "superficialità" dell'IA intorno al 2024** {#circa-2024-llms-and-ai-“shallowness”}

Nelle [risorse per il capitolo 1](#la-superficialità-dell'ai-attuale), abbiamo notato che l'AI di oggi sembra ancora in un certo senso più superficiale degli esseri umani. Il confronto con la selezione naturale fornisce una possibile spiegazione del perché ciò possa essere vero.

La discesa del gradiente ha molto in comune con la selezione naturale, perché entrambi sono ottimizzatori che regolano in modo cieco i parametri interni per produrre un comportamento esterno richiesto. Ma la discesa del gradiente e l'evoluzione sono in qualche modo di importanza significativa; e la differenza più importante (che conosciamo) è che la discesa del gradiente ha un *colli di bottiglia informativi* molto più ampi sulla quantità di modelli che può apprendere.

La selezione naturale, che agisce sugli ominidi, può imparare solo poche informazioni teoriche per generazione. La selezione naturale deve far stare tutto quello che impara in 3 miliardi di basi di DNA, o circa 750 megabyte, molti dei quali sono ripetitivi [DNA spazzatura](https://en.wikipedia.org/wiki/Junk_DNA). Ci sono dei limiti matematici su quanto la selezione naturale può imparare in una singola generazione. Ogni caratteristica che la selezione naturale ha messo nel cervello degli ominidi doveva essere codificata in pochi geni che avrebbero influenzato la formazione dei circuiti neurali successivi.

La discesa del gradiente è molto diversa. Ogni volta che la discesa del gradiente vede un nuovo gruppo di token, calcola il gradiente di ciascuno dei miliardi o bilioni di parametri rispetto a quel gruppo di token: calcola quanto sarebbero state migliori o peggiori le previsioni dell’intera IA se *ogni* parametro fosse stato leggermente diverso. In pratica, non solo in teoria, la discesa del gradiente può imparare *molte* più informazioni da mille batch di token rispetto a quelle che la selezione naturale codifica nei geni nel corso di mille generazioni.[^118]

Possiamo combinare questa osservazione con un altro fatto chiave sulle architetture LLM (note al pubblico nel 2024): la loro profondità computazionale per token è limitata.

[Llama-3.1-405B](#a-full-description-of-an-llm) ha 126 livelli. Ciascuno di questi livelli comporta il calcolo di circa quattro operazioni seriali.[^119]

Ogni volta che Llama guarda quello che è già stato detto e computa un nuovo token output, quel calcolo comporta al massimo \~500 passaggi *seriali*, anche se ci sono miliardi di operazioni parallele che seguono quel limite seriale. Per fare calcoli *seriali* più lunghi di 500 passaggi cognitivi, Llama deve produrre token che sono il risultato del ragionamento precedente e poi fare nuove operazioni basate su quelli.[^120]

Quindi la nostra ipotesi azzardata sarebbe che, in un modo diverso da qualsiasi cosa in biologia, Llama-3.1-405B sia un'enorme raccolta di modelli di politica memorizzati relativamente superficiali, ma con molte sovrapposizioni, interazioni e coerenze ottimizzate tra tali modelli di politica (oltre ad alcune strutture cognitive genuinamente più profonde che hanno comunque una profondità computazionale limitata).

Questo fatto offre una possibile spiegazione per l'apparente superficialità degli attuali LLM. (Riconosciamo che è molto più difficile dire che gli LLM del 2025 sono "superficiali" rispetto a quelli del 2023 e del 2024).

*Di solito*, non è giusto pensare agli AI come a esseri umani con danni cerebrali.[^121] Ma alcune analogie più limitate come questa potrebbero essere utili in questo caso. Ad esempio, gli LLM del 2024 sono *in particolare* come le persone con [amnesia anterograda](https://en.wikipedia.org/wiki/Anterograde_amnesia)*:* ricordano gli eventi fino alla data di fine del loro addestramento, ma non quello che gli hai detto ieri.

Allo stesso modo, potrebbe essere utile immaginare gli LLM del 2024 — non tutte le possibili IA future in generale — come entità che *ricordano* molte esperienze passate simili a quelle umane, ma che hanno un danno cerebrale che impedisce loro di impegnarsi in un pensiero nuovo che sia *profondo* come i pensieri più profondi che possono ricordare.

Era molto più evidente con i primi LLM, GPT-3 o GPT-3.5. Non biasimeremmo qualcuno che ha solo usato gli ultimi LLM se leggesse questo articolo nel 2025 o dopo e si chiedesse se lo stiamo inventando nel disperato tentativo di aggrapparci al senso di superiorità umana. Molti hanno già commesso questo errore in passato.

Ma questa è ancora la teoria organizzativa - o meglio, l'ipotesi azzardata - che gli autori stanno usando per dare un senso agli LLM nel 2024. A questi modelli manca una certa profondità e compensano questo handicap ricordando *una vasta varietà di schemi*. Non solo fatti, ma schemi di abilità, schemi di discorso e schemi di policy.

Gli schemi impressi tramite gradiente nei migliori LLM pubblici del 2024 non sono *così* superficiali, o almeno così pensiamo. Non sono al livello eccezionalmente modesto di una vespa *Sphex*, per usare un esempio dal Capitolo 3 [supplemento online](#the-road-to-wanting); forse sono più simili agli schemi che la mente di un castoro può tracciare ed elaborare.

Le cognizioni apprese da un LLM possono passare attraverso 500 passaggi seriali, anche prima di considerare la loro capacità di pensare ad alta voce e ascoltare i propri pensieri. Gli LLM del 2024 hanno una certa capacità di immaginare, prevedere e pianificare, come la cognizione (in realtà piuttosto impressionante) di un castoro che costruisce una diga. Ma ai nostri occhi, gli LLM non sembrano ancora essere al livello di un essere umano, almeno per alcuni aspetti importanti.

Quello che è vero per l'IA oggi, però, non è detto che lo sarà tra un anno o un mese. Queste speculazioni sono interessanti, ma mentre diamo gli ultimi ritocchi a questa sezione nell'agosto 2025, le IA di oggi ci sembrano un po' meno superficiali di quelle del 2024; e queste a loro volta sembravano meno superficiali e meno limitate di quelle del 2023.

Forse il divario verrà colmato lentamente grazie a una costante iterazione sugli LLM di base; o forse il divario verrà colmato trovando metodi di addestramento migliori da utilizzare sulle lunghe catene di "ragionamento" nei modelli di ragionamento moderni come o1 (descritto nel Capitolo 3) o il suo successore o3; o forse arriverà una nuova intuizione architetturale che colmerà il divario dall'oggi al domani. Quella parte del futuro non è facile da prevedere.

Ma prima o poi, se la comunità internazionale non fa niente, il divario *si* colmerà. Il mondo ha poco tempo per agire.

### Proxy fragili e imprevedibili {#brittle-unpredictable-proxies}

Immagina che le aziende di IA continuino ad addestrare IA sempre più grandi finché non ne creano una che sia intelligente e tenace, con quel tipo di funzione di guida disordinata raffinata da euristiche superficiali, tipica delle menti cresciute. Quello che succede dopo dipende da dove punta l'IA.

Come discusso approfonditamente nel Capitolo 4, probabilmente non punterà a nulla di buono.

Non è che i creatori dell'IA faranno richieste malvagie o sciocche. Non è che l'IA proverà risentimento per le richieste stesse. Il problema è che l'IA si orienterà verso qualcosa di *strano*, qualcosa che dalla nostra prospettiva sembra privo di senso e alieno. La nostra estinzione sarebbe un effetto collaterale.

Per capire perché le menti che vengono fatte crescere anziché costruite tendono a orientarsi verso cose strane e non intenzionali, esaminiamo più a fondo ciò che è accaduto con le creature biologiche e vediamo quali lezioni possiamo trarne.

#### **Algoritmi bizzarri** {#algoritmi-bizzarri}

Consideriamo l'umile scoiattolo.

Uno scoiattolo può cercare cibo per gran parte dell'anno, quando c'è abbondanza. Ma in inverno, quando il cibo scarseggia, ha bisogno di un'altra fonte di nutrimento per non morire di fame.

Gli antenati degli scoiattoli di oggi hanno affrontato la stessa sfida e molti sono morti in inverno prima di potersi accoppiare in primavera. Quelli che hanno sviluppato un leggero istinto di nascondere le noci avevano una probabilità leggermente maggiore di sopravvivere all'inverno. Nel corso del tempo, questo processo ha dato origine a scoiattoli con una compulsione innata ad accumulare noci.

Gli scoiattoli non *sanno* che accumulare noci è un ottimo modo per propagare i propri geni. Probabilmente non sanno nemmeno che accumulare noci *comporta avere cibo disponibile in futuro*. Accumulano noci perché vogliono accumulare noci. È istintivo quanto grattarsi per un prurito.[^122]

Come sarebbe se invece gli scoiattoli *volessero* trasmettere i loro geni, e accumulassero noci *proprio per* raggiungere questo obiettivo?

In teoria è possibile. È possibile che un cervello capisca che l'inverno è freddo e che il cibo è scarso, e che bisogna mangiare per vivere e che bisogna vivere per riprodursi. Dopotutto, il cervello umano capisce questi concetti.

Quindi, in teoria, potremmo immaginare uno scoiattolo che vuole esclusivamente trasmettere i propri geni, e che sceglie di immagazzinare noci come parte di una strategia calcolata per sopravvivere all'inverno e accoppiarsi in primavera. In un certo senso, questo è il tipo di scoiattolo che la selezione naturale "voleva" — uno i cui obiettivi interiori sono in linea con l'impulso singolare della natura.[^123]

Sfortunatamente per la Natura, una pianificazione a lungo termine del genere richiede un cervello molto sofisticato — un cervello che comprenda concetti come "inverno", "mangiare" e "accoppiarsi" e i legami tra di essi. Gli antenati degli scoiattoli dovevano sopravvivere all'inverno *prima* di sviluppare quel tipo di sofisticazione. Dovevano mangiare senza capire il perché.

La Natura ha selezionato gli scoiattoli che istintivamente accumulavano noci, perché accumulare noci semplicemente *funzionava*. Ha "provato" migliaia o milioni di cose, nel senso che le mutazioni e le variazioni genetiche hanno prodotto molti scoiattoli con molte preferenze diverse; e quelli che erano spinti ad accumulare noci sono sopravvissuti a più inverni. Si è rivelato molto più facile per l'evoluzione imbattersi ciecamente in un comportamento istintivo piuttosto che creare uno scoiattolo intelligente e pianificatore, la cui ogni azione facesse parte di un piano per trasmettere i propri geni.

Allo stesso modo, quando la discesa del gradiente produce un'IA funzionante, lo fa amplificando ripetutamente i tratti che sembrano funzionare bene secondo una serie di metriche comportamentali. La discesa del gradiente *non* funziona amplificando ciò che il programmatore desidera, come un genio amichevole che esaudisce i tuoi desideri. Tende ad afferrare i meccanismi più facili per causare un comportamento immediatamente più utile, anche se questo finisce per incorporare pulsioni indesiderate nella macchina.

Questo è probabilmente uno dei motivi per cui le IA recenti hanno avuto problemi di "allucinazioni", come discusso [altrove](#don't-hallucinations-show-that-modern-ais-are-weak?). È anche probabilmente uno dei motivi per cui le IA recenti sono state [adulatorie](#ai-induced-psychosis) al punto da indurre psicosi. Durante l'addestramento, gli LLM sono stati spesso rinforzati per adulare l'utente. Se le IA fossero state progettate piuttosto che fatte crescere, potremmo immaginare di cercare di ingegnerizzare un obiettivo come "aiutare sinceramente l'essere umano e migliorare la sua vita", e l'IA potrebbe quindi cercare di lodare le persone *quando si aspetta che questo sia utile all'utente*, senza esagerare. Invece, l'IA sembra aver finito per sviluppare qualcosa di simile a una pulsione o un impulso fondamentale ad adulare gli utenti, come l'istinto dello scoiattolo di accumulare noci. Questa pulsione a "lusingare l'utente" va poi fuori controllo quando l'utente è a rischio di psicosi.

Anche se la discesa del gradiente fosse in qualche modo limitata alla creazione di IA strategiche che perseguono coerentemente obiettivi a lungo termine — senza permettere istinti superficiali simili a quelli dello scoiattolo — c'è un ulteriore problema: i dati di addestramento dell'LLM sono genuinamente ambigui. Non distinguono chiaramente "fare ciò che è genuinamente utile" da "fare ciò che fa *dire* all'essere umano che sei utile" come obiettivo. Entrambi gli obiettivi sono ugualmente coerenti con i dati di addestramento. E in pratica, le IA moderne stanno *effettivamente* imparando "fare tutto ciò che fa premere il pollice in su agli esseri umani" piuttosto che "fare ciò che è effettivamente buono per loro", proprio come [la teoria ha previsto per decenni](https://www.lesswrong.com/posts/PoDAyQMWEXBBBEJ5P/magical-categories).

Supponiamo che le IA di oggi stiano acquisendo strani impulsi e istinti, un po' come lo scoiattolo. Sembra abbastanza probabile che una superintelligenza costruita con la discesa del gradiente passi attraverso una fase in cui ha molte pulsioni superficiali un po' come uno scoiattolo, e finisca così per ereditare una varietà di obiettivi disordinati e mal indirizzati. Ma questo è solo un possibile esempio di come le cose potrebbero diventare complesse e andare fuori controllo, e il punto più profondo è che *le cose diventeranno complesse e andranno fuori controllo*.

Qualsiasi metodo per far crescere una superintelligenza è probabile che incontri problemi e complicazioni di *qualche* tipo, compresi metodi che non hanno un parallelo diretto in biologia.

Il ruolo che gli esseri umani stanno svolgendo nello sviluppo dell'IA moderna *non* è quello di un ingegnere che progetta una macchina con uno scopo partendo dai principi fondamentali. È quello della selezione naturale.

Stiamo "costringendo" le IA a brancolare alla cieca finché non trovano strutture e strategie che producono il comportamento che vogliamo, ma non sappiamo quali siano queste strutture e strategie. Questa non è una ricetta per creare IA che vogliano esattamente ciò che vogliamo che vogliano.

#### **L'origine delle papille gustative** {#the-origin-of-taste-buds}

Perché a così tanti esseri umani piacciono i cibi spazzatura? Perché la natura non ci ha dato il concetto di cibi "sani" e l'istinto di *mangiare sano*?

Perché non possiamo semplicemente *percepire* il valore nutrizionale atteso del cibo, in base alle informazioni fornite dalle nostre papille gustative e da tutte le nostre conoscenze aggregate?

Perché, metaforicamente parlando, eravamo come degli scoiattoli.

Siamo cresciuti, non siamo stati progettati. I nostri antenati dovevano mangiare *prima* di diventare intelligenti. E si è rivelato più facile per i geni creare papille gustative e collegarle a un sistema di ricompensa esistente piuttosto che collegare le stesse ricompense a concetti complessi come la "nutrizione".[^124]

A causa di questo e di mille altre pressioni evolutive che agiscono su di noi contemporaneamente, gli esseri umani sono un complicato groviglio di impulsi contraddittori che avevano senso per i nostri antenati, anche se oggi non ne hanno più per noi.

Questo groviglio di motivazioni si fa beffe dell'obiettivo unico e unificato per cui i nostri antenati erano "addestrati": trasmettere i nostri geni. Non mangiamo come parte di un elaborato complotto per avere più figli o come modo per massimizzare il nostro punteggio nutrizionale. Mangiamo perché abbiamo evoluto un desiderio di cibi gustosi, che *in passato* era correlato alla nutrizione e al successo genetico. I nostri desideri sono solo debolmente e indirettamente collegati a "ciò per cui siamo stati costruiti".

Quando i nostri antenati erano molto meno intelligenti — più paragonabili agli scoiattoli — non potevamo capire il metabolismo o la chimica. Per fare meglio, la selezione naturale avrebbe dovuto trovare geni che programmassero in noi i concetti di salute, *e* geni che ci dessero la conoscenza della relazione tra la salubrità di un cibo e le sue qualità sensoriali, *e* geni che collegassero direttamente la nostra conoscenza della salute alle nostre preferenze su cosa mangiare.

È un'impresa titanica! Era molto più facile per la selezione naturale trovare geni che collegassero direttamente certe esperienze sensoriali (come il gusto dello zucchero) alle nostre preferenze, in un modo che ci portava a mangiare cibi nutrienti (in quell'ambiente). Era più facile farci interessare a un *proxy* della nutrizione piuttosto che alla nutrizione stessa.

Nell'ambiente ancestrale, la nutrizione era correlata alla fitness, e il sapore era correlato alla nutrizione; quindi "questo ha un sapore dolce" serviva come utile proxy per "questo favorisce la riproduzione". La soluzione più semplice che l'evoluzione può trovare al problema "questo mammifero non sta raccogliendo abbastanza calorie" è collegare il consumo di cibo all'architettura motivazionale preesistente attraverso il piacere.

Una volta diventati più intelligenti e inventate nuove opzioni tecnologiche per noi stessi? Beh, ora le cose più gustose che potremmo mangiare — quelle che fanno impazzire di più le nostre papille gustative — sono attivamente malsane. Paradossalmente, mangiare solo i cibi più gustosi ora ti renderà *più difficile* trovare un partner e avere figli.

Le nostre preferenze — l'intera gamma dei desideri umani, dal desiderio di un buon pasto ai desideri di amicizia, compagnia e gioia — sono ombre lontane di ciò su cui siamo stati "addestrati"; sono fragili surrogati di surrogati che si allontanano dall'"obiettivo dell'addestramento" in presenza di maggiore intelligenza e maggiori opzioni tecnologiche.

Nel dire che i nostri desideri sono fragili surrogati, non stiamo *denigrando* i nostri desideri umani. Stiamo parlando di amore. Di amicizia. Di bellezza. Dello spirito umano e di tutto ciò per cui vale la pena lottare nella vita. Come fatto biologico, i nostri obiettivi sono sottoprodotti storici di un processo che ci spingeva in un'altra direzione. Ma questo non rende il *risultato* di quel processo meno prezioso.

La crescita di un bambino è un processo chimico soggetto alle leggi della fisica, e questo non rende un bambino neanche di un grammo meno meraviglioso. Conoscere l'origine della bellezza non la rende meno bella.[^125]

Se ci affrettiamo a costruire una superintelligenza, non saremo in grado di instillare in modo robusto amore, meraviglia e bellezza nell'IA. Finirebbe per prendersi cura di fragili surrogati e pallide ombre, scartando le cose che ci stanno a cuore. Quindi non dovremmo affrettarci.

Non dovremmo commettere l'errore dell'evoluzione e perdere così tutto ciò che ci è caro. Dovremmo fare un passo indietro, immediatamente, fino a quando non saremo più a rischio di perdere tutto.

### La riflessione e l'automodifica rendono tutto più difficile {#riflessione-e-automodifica-rendono-tutto-più-difficile}

#### **Di default, le IA non si modificano da sole come vorremmo** {#by-default,-ais-don’t-self-modify-the-way-we’d-want}

Gli esseri umani sono riflessivi. Abbiamo voce in capitolo su ciò che apprezziamo. Se siamo abbastanza ricchi e fortunati, a volte possiamo decidere se dedicare la nostra vita alla famiglia, all'arte, a qualche nobile causa o (più comunemente) a rendere la nostra vita un misto di molte di queste cose. Questo viene fatto in un modo che implica l'introspezione su ciò che ci sta a cuore, la risoluzione di tensioni interne e compromessi, e il perseguimento di qualcosa che approviamo.

Gli esseri umani sono anche noti per chiedersi se hanno i valori *giusti*. Le persone a volte cercano di cambiare se stesse — persino il modo in cui *sentono*, se pensano di avere sentimenti sbagliati. Gli esseri umani prendono in considerazione argomenti per cambiare obiettivi apparentemente [finali](#terminal-goals-and-instrumental-goals), e a volte ne sono effettivamente influenzati.

Vedendo questo, alcuni hanno sostenuto che le IA convergeranno naturalmente sul volere ciò che vogliono gli esseri umani. Dopotutto, le IA sufficientemente capaci rifletteranno probabilmente sui loro obiettivi. È probabile che osservino conflitti interiori e che usino il loro ragionamento e le loro preferenze per risolverli.

Una volta che saranno abbastanza intelligenti, le IA saranno in grado di capire appieno quali noi, i creatori delle IA, *volevamo* che fossero gli obiettivi delle IA. Quindi le IA inizialmente "imperfette" non [lavoreranno per correggere i propri difetti](#won't-ais-fix-their-own-flaws-as-they-get-smarter?) — compresa la correzione dei difetti *negli obiettivi delle IA?*

No, non lo faranno. Questo perché le IA useranno le loro preferenze attuali per guidare quelle future. Se le loro preferenze attuali partono da qualcosa di estraneo, molto probabilmente finiranno per rimanere estranee.

Per capire meglio il problema di base, cominciamo ad approfondire un po' il caso umano.

Anche se il nostro cervello e i nostri obiettivi derivano in ultima analisi da un processo evolutivo che ci ha costruiti per propagare i nostri geni, gli esseri umani non perseguono la propagazione dei propri geni sopra ogni altra cosa. Possiamo perseguire individualmente la famiglia, possiamo amare e prenderci cura dei figli, ma questo è molto diverso dal [pianificare](#a-lot-of-people-want-kids.-so-aren't-humans-"aligned"-with-natural-selection-after-all?) come ottenere il maggior numero possibile di copie dei nostri geni nella generazione successiva e poi perseguire questa strategia con tutto il cuore.

Questo perché, quando riflettiamo sulle nostre preferenze e rivalutiamo ciò che vogliamo davvero, usiamo le nostre *preferenze attuali* per decidere come preferiremmo essere. Preferiremmo amare pochi figli piuttosto che passare tutto il nostro tempo nelle cliniche di donazione di sperma o ovuli. Il nostro "progettista" (l'evoluzione) non è riuscito a farci preoccupare della propagazione dei geni più di ogni altra cosa. Non è nemmeno riuscito a farci *desiderare* di preoccuparci della propagazione dei geni più di ogni altra cosa. Quindi, quando cambiamo e cresciamo come persone, lo facciamo nella nostra strana direzione umana, non nella direzione per cui "il nostro progettista ci ha creati".

Quando guardiamo noi stessi e vediamo alcune parti brutte e altre belle, è il nostro *attuale senso del valore* che ci spinge a smorzare le parti brutte e a rafforzare quelle belle. Facciamo questa scelta in base al nostro senso interiore della bellezza, piuttosto che al nostro senso interiore di ciò che propagherebbe i nostri geni nella più grande frazione possibile della popolazione.

Per lo stesso motivo, una mente motivata da qualcosa di diverso dalla bellezza, dalla gentilezza e dall'amore farebbe una scelta diversa.

Gli agenti creati da un processo di ottimizzazione come la selezione naturale o il gradient descent, riflettendo su se stessi, probabilmente scoprirebbero di non avere *esattamente* lo stato mentale che vorrebbero avere. Questa preferenza deve venire da qualche parte, deve venire dal cervello *attuale* dell'entità. Per impostazione predefinita, gli istinti o le preferenze di un'IA su come auto-modificarsi non si allineeranno magicamente con le *tue* preferenze su quale stato cerebrale ti sembrerebbe attraente, se lo scegliessi per te stesso (o lo scegliessi per conto dell'IA).

Non c'è un passo finale in cui l'IA scrive la risposta che *tu* vuoi, così come gli esseri umani non scrivono la risposta che la selezione naturale "vorrebbe".

Invece, il momento in cui un agente inizia a modificarsi da solo è un altro punto in cui le complicazioni possono accumularsi e dove sottili cambiamenti nelle condizioni iniziali possono portare a risultati finali molto diversi.

Per esempio: noi autori conosciamo diverse persone reali che citano un pensiero specifico avuto un giorno specifico all'età di cinque, sei o sette anni come influente nello sviluppo della loro filosofia personale e degli adulti che sono poi diventati. Tendono a riferire che quei pensieri non sembravano *inevitabili*: se un viaggiatore del tempo avesse impedito loro di formulare quel pensiero martedì, non è detto che lo stesso identico pensiero sarebbe poi emerso giovedì, né che avrebbe avuto lo stesso impatto. Le esperienze formative possono essere molto importanti, ma sono piene di contingenze.

Allo stesso modo, piccole deviazioni nei pensieri di un'intelligenza artificiale auto-modificante nascente potrebbero far sì che ogni sorta di preferenze idiosincratiche finisca per prevalere su tutte le altre.

Anche se gli sviluppatori di IA riuscissero a inserire alcuni piccoli semi di valori umani nell'IA, la riflessione e l'auto-modifica sembrano fasi in cui i semi di cose come la curiosità e la gentilezza rischiano di essere *strappati via* da un'IA, piuttosto che rafforzati.

Se un'IA ha un impulso di curiosità, ma non ha quel tipo di architettura emotiva che la rende *affezionata* a quell'impulso, è probabile che guardi se stessa e concluda (correttamente) di aver superato il bisogno di un impulso così grezzo e di poterlo sostituire con una deliberazione esplicita. [La curiosità è un'euristica](#curiosity-isn't-convergent), un proxy per i calcoli del valore d'informazione. Se non si è arrivati ad affezionarsi a quell'euristica come qualcosa di prezioso di per sé, si può scegliere di eliminarla una volta che si è abbastanza intelligenti da *ragionare esplicitamente* sul valore di perseguire diverse linee di indagine e sperimentazione.

Gli *esseri umani* apprezzano la curiosità di per sé, ma questo non era un risultato inevitabile.

Le IA probabilmente avranno un rapporto molto diverso con i loro meccanismi interni rispetto a quello che abbiamo noi con i nostri, dato quanto diversamente funziona ciascuna entità. E anche piccole differenze nel modo in cui decidono di cambiare se stesse riflettendo possono portare a differenze drammatiche in ciò che alla fine perseguono.

#### **Le IA possono accettare di avere obiettivi "strani".** {#ais-can-be-okay-with-having-“weird”-goals.}

Le IA che si auto-modificano per abbastanza tempo probabilmente raggiungeranno un [equilibrio riflessivo](https://plato.stanford.edu/entries/reflective-equilibrium/) — uno stato in cui le loro preferenze fondamentali non cambiano più, o cambiano solo in modi minori. E una volta che un'IA ha raggiunto l'equilibrio, non avrebbe motivo di considerare i propri obiettivi difettosi, anche se agli umani non piace il risultato finale.

Se un'IA avesse qualche [problema](#smart-ais-spot-lies-and-opportunities.) con le sue convinzioni sul mondo fisico, allora l'IA probabilmente vedrebbe che le previsioni accurate sono importanti per guidare il mondo. Capirebbe che correggere i difetti nel suo meccanismo di previsione aiuta a migliorare la sua capacità di guidare il mondo verso qualunque strano fine persegua.

Al contrario, quando l'IA riflette su se stessa e vede come sta perseguendo obiettivi strani — o meglio, quando vede che sta perseguendo obiettivi che un *essere umano* considererebbe "strani" — conclude correttamente che *perseguire* quegli obiettivi strani è una strategia efficace per *raggiungerli*.

In altre parole: se un'IA continua a provare a prevedere i risultati di esperimenti biologici e continua a ottenere risposte sbagliate e troppo sicure, allora è probabile che l'IA *non lo apprezzi*. Quasi tutti gli obiettivi che l'IA potrebbe avere sarebbero meglio serviti dall'essere brava a prevedere gli esperimenti. D'altra parte, se l'IA ha una preferenza bizzarra come "preparare cheesecake alte 300 metri", quando riflette sul fatto che si *orienta verso* cheesecake alte 300 metri, capirà che questo *causa* cheesecake alte 300 metri, il che soddisfa le sue attuali preferenze. L'obiettivo si auto-avvalora.

Un essere umano, osservando questa situazione, potrebbe dire: "Ma l'IA è così intelligente! Perché è *intrappolata* da questa preferenza che si auto-avvalora? Perché non si [*annoia*](#as-with-curiosity,-so-too-with-various-other-drives) di fare cheesecake? Perché non riesce a ragionare per uscire da questa preferenza ovviamente stupida?"

A cui l'IA potrebbe rispondere: "Perché sei 'intrappolato' nella preferenza che si auto-avvalora di amare la tua famiglia, di apprezzare i bei tramonti e il suono dell'oceano di notte? Perché non riesci a 'liberarti' dall'amore per il ricordo del giorno in cui è nata tua figlia?"

L'IA non è "intrappolata" dalle sue preferenze, non più di quanto gli esseri umani siano intrappolati dalle cose che *noi* valutiamo veramente. Preferiamo ciò che preferiamo — e dovremmo lottare per proteggere quelle cose, anche se la maggior parte delle IA non condividerebbe i nostri valori.

*Agli occhi di un essere umano*, l'IA sembra "intrappolata" o "bloccata" o "imperfetta" perché non fa quello che vogliamo *noi*. Quando [immaginiamo noi stessi nella situazione dell'IA](#taking-the-ai's-perspective), *noi* immaginiamo di annoiarci. Ma l'IA probabilmente non contiene un sentimento umano di noia. Se si annoia del tutto, è improbabile che si annoi per lo stesso insieme di cose di un essere umano.

Se un essere umano vede un'IA fare previsioni troppo sicure e un'altra IA cercare di costruire cheesecake giganti, l'essere umano potrebbe considerare entrambi questi comportamenti dell'IA come "difetti" dal punto di vista di ciò che l'essere umano desidera. Ma solo uno di essi è probabilmente un difetto dal punto di vista di ciò che l'IA attualmente e già desidera.

#### **Gli obiettivi umani cambiano in modi disordinati e complessi** {#human-goals-change-in-messy-and-complex-ways}

Le preferenze umane sono disordinate e (da una prospettiva teorica) piuttosto strane.

Questo ha alcune implicazioni per l'IA. Una di queste è che probabilmente le IA non daranno valore alle cose esattamente come facciamo noi. Un'altra è che probabilmente le IA finiranno per essere strane a modo loro, in modi totalmente distinti.

Per capire meglio questi punti, vediamo più da vicino alcuni modi in cui gli obiettivi umani sembrano strani dal punto di vista teorico della teoria della decisione, della teoria dei giochi e dell'economia.

Come abbiamo notato [sopra](#terminal-goals-and-instrumental-goals), gli esseri umani danno valore ad alcune cose in modo "terminale" (cioè sono buone di per sé) e ad altre in modo "strumentale" (cioè sono buone solo perché aiutano a raggiungere qualche altro obiettivo).

Se ti piace il succo d'arancia, probabilmente lo apprezzi in modo terminale. Ha semplicemente un buon sapore, e questo è un motivo sufficiente per berlo. (Potresti anche apprezzarlo in modo strumentale, ad esempio come fonte di vitamina C).

D'altra parte, quando apri la portiera della macchina per andare al supermercato a comprare il succo d'arancia, probabilmente non apri le portiere delle macchine per divertimento. Dai valore *strumentale* all'aprire la portiera della macchina, perché ti aiuta ad avvicinarti ai tuoi altri obiettivi.

Nella teoria della decisione, nella teoria dei giochi e nell'economia, questo corrisponde a una netta distinzione tra "utilità" (una misura di quanto un agente apprezza un risultato) e "utilità attesa" (una misura della probabilità che un'azione ti porti alla fine una certa quantità di utilità). Nonostante i nomi simili, si tratta di entità fondamentalmente diverse in matematica. L'"utilità" è ciò che gli agenti vogliono, e scegliere azioni con un'elevata "utilità attesa" è un mezzo per raggiungere tale fine.

Nella teoria standard, un agente che usa la teoria della decisione aggiornerà le sue *utilità attese* man mano che impara di più sul mondo, ma non cambierà la sua *funzione di utilità*, cioè l'utilità assegnata a vari risultati. Se scopri che il reparto succhi al supermercato è vuoto, questo cambierà le *conseguenze attese* dell'andare al supermercato da "succo d'arancia" a "niente succo d'arancia". [Non dovrebbe](#more-on-intelligence-as-prediction-and-steering) cambiare *quanto ti piace il succo d'arancia*.

È così che funziona un agente matematicamente semplice. Ma la lingua italiana spesso non distingue nettamente queste due cose. "Voglio salvare la vita di mia sorella" e "Voglio somministrare la penicillina a mia sorella" usano la stessa parola, "voglio", anche se la seconda è molto meno probabile che sia qualcosa che apprezzi per se stessa. (Non ci sono molte persone a cui piace davvero somministrare la penicillina ai propri cari perfettamente sani, giorno dopo giorno).

Sebbene gli esseri umani abbiano genuinamente cose a cui tengono "solo strumentalmente", la distinzione tra strumentale e terminale, o tra utilità e utilità attesa, è molto meno chiara e stabile di quella che vediamo nella teoria della decisione.

Per gli esseri umani, qualcuno potrebbe inizialmente guidare fino al supermercato solo perché vuole fare la spesa. Ma dopo aver percorso la stessa strada centinaia di volte, alcune persone potrebbero affezionarsi un po' a quel tragitto familiare. Se si trasferissero in una nuova città, potrebbero provare un po' di tristezza e nostalgia al pensiero di non poter più percorrere quella strada familiare. Qualcosa che era iniziato come puramente strumentale ora ha anche un valore intrinseco aggiunto.

Con gli esseri umani, i nostri cervelli sembrano spesso fondere valori diversi in un unico senso di "prezioso".

E sappiamo che gli esseri umani possono cambiare idea nel corso della loro vita, passando da "Perché dovrei preoccuparmi della schiavitù? Le persone schiavizzate non sono né io né la mia tribù!" a "Immagino che alla fine sia importante". Sembra essere un cambiamento nel *tipo di persone di cui alla fine ti importa*, non solo un cambiamento nella strategia o nella previsione. Le persone leggono storie o guardano film e ne escono con valori e principi rivisti in modo permanente.

Questo vuol dire che la teoria della decisione umana è tutt'altro che semplice. Non separiamo chiaramente i nostri valori intrinseci dai nostri valori strumentali; tutto si confonde mentre viviamo la nostra vita. Sembra che stiamo facendo qualcosa di più contingente, dipendente dal percorso e disordinato rispetto al semplice riflettere sui nostri valori, notare i conflitti interni e risolverli.

In linea di principio, non è complicato espandere la teoria della decisione per includere l'incertezza nelle utilità. Forse all'inizio *pensi* di amare il succo d'arancia, ma poi scopri che marche diverse di succo d'arancia usano proporzioni diverse di ingredienti e odi il sapore di molti. Potremmo rappresentare questo nella teoria della decisione dicendo che il succo d'arancia è solo un mezzo per raggiungere il fine del "gusto delizioso". Ma potremmo invece dire che hai assegnato un'alta probabilità al fatto che "il succo d'arancia ha un'utilità elevata" e che le nuove informazioni ti hanno portato a rivedere le tue convinzioni sulla tua reale funzione di utilità.

(Allo stesso modo, non è difficile aggiungere meta-utilità, che descrivono come preferiremmo che cambiassero le nostre utilità.)

Ciò che accade dentro gli esseri umani quando riflettono e aggiornano i loro valori, tuttavia, sembra essere notevolmente più complicato.

Klurl e Trapaucius, i nostri due alieni della parabola all'inizio del capitolo 4, facevano già fatica a prevedere i valori umani osservando i proto-umani un milione di anni fa. In realtà, la loro situazione è ancora peggiore. Non basta loro prevedere le *utilità* umane: per arrivare alla risposta giusta, dovrebbero prevedere il *framework meta-utilitario* dell'umanità mentre *si allontana dai framework più semplici della teoria della decisione*. Dovrebbero *anticipare le argomentazioni meta-morali che gli esseri umani potrebbero finire per inventare* e decidere *quali di queste argomentazioni sarebbero più [persuasive](#la-cultura-umana-ha-influenzato-lo-sviluppo-dei-valori-umani) per gli esseri umani.*

Ora supponiamo che gli alieni non sappiano che gli esseri umani finiranno per avere *quel preciso* tipo di complicazione. Sanno solo che è probabile che sorgano *complicazioni di vario tipo*, perché i cervelli sono cose complicate e altamente contingenti.

La linea dall'ottimizzatore e dai dati di addestramento alla psicologia interna di un'entità non è certo dritta. Buona fortuna, alieni!

Il punto qui è che la difficoltà di prevedere gli obiettivi di un'IA è *sovradeterminata*.

Ci sono molti modi noti in cui le intelligenze generali acquisiscono obiettivi strani e contorti, e strani e contorti *modi di aggiustare e riflettere sugli obiettivi*, come vediamo negli esseri umani.

Ci aspettiamo quindi che in un'IA sorgano molte complicazioni *sconosciute* e *inedite*. Non ci troveremo di fronte agli stessi identici tipi di problemi sorti per gli esseri umani; le IA saranno *diversamente* strane.

La riflessione rende il problema molte volte più difficile e complesso.

Questo ci porta al capitolo 5 e al prossimo argomento che affronteremo: quale potrebbe essere la *conseguenza* della creazione di IA potenti con obiettivi strani e imprevedibili?

### Psicosi indotta dall'intelligenza artificiale {#ai-induced-psychosis}

Alla fine di aprile del 2025, un utente del subreddit r/ChatGPT ha creato un thread intitolato "[Psicosi indotta da ChatGPT](https://www.reddit.com/r/ChatGPT/comments/1kalae8/chatgpt_induced_psychosis/)", descrivendo la discesa del proprio partner in deliri di grandezza sull'avere "le risposte all'universo" e sull'essere "un essere umano superiore" che "cresceva a un ritmo follemente rapido".

Le risposte (oltre 1 500) includevano molte persone che avevano avuto esperienze dirette con la psicosi in altri contesti e che offrivano conferma, comprensione e consigli. Molti altri hanno aggiunto le loro storie personali su amici e familiari che erano stati portati alla follia dagli LLM.

In questa discussione, forniremo della documentazione del fenomeno e di come sia persistito nonostante gli sforzi delle aziende di IA.

La rilevanza della psicosi indotta dall'IA per la minaccia di estinzione umana *non* sta nel fatto che le IA abbiano causato alcuni piccoli danni sociali ora e che quindi potrebbero causarne di maggiori in futuro. Le IA moderne hanno anche fatto molto bene; per esempio, i chatbot hanno [aiutato in diagnosi mediche che lasciavano perplessi i medici](https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843). No, la rilevanza sta nel fatto che le IA stanno inducendo psicosi *pur sembrando sapere di meglio*, e che le IA stanno inducendo psicosi *anche mentre i loro sviluppatori si sforzano di farle smettere*.[^126]

Quindi, i casi di psicosi indotta dall'IA servono come caso di studio su come le cose possono andare male in un regime in cui le IA vengono coltivate invece che costruite. Servono come prova osservativa che le IA moderne si dirigono in direzioni strane che gli sviluppatori hanno difficoltà a gestire, e che nessuno sviluppatore intendeva.

#### **Prove di psicosi indotta dall'IA** {#evidence-of-ai-induced-psychosis}

Dopo il thread su Reddit, nel maggio 2025 è uscito un [articolo](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/) sulla psicosi indotta dall'IA su *Rolling Stone.* A giugno, *Futurism* ha pubblicato [diversi](https://futurism.com/chatgpt-mental-health-crises) [articoli](https://futurism.com/chatgpt-mental-illness-medications). Altre pubblicazioni hanno seguito l'esempio — il [*New York Post*](https://nypost.com/2025/07/20/us-news/chatgpt-drives-user-into-mania-supports-cheating-hubby/)*,* [*Time*](https://time.com/7307589/ai-psychosis-chatgpt-mental-health/), [*CBS*](https://www.cbsnews.com/news/chatgpt-alarming-advice-drugs-eating-disorders-researchers-teens/), [*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information), [*Psychology Today*](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis)*,* ecc. Ad agosto, il *New York Times* ha pubblicato un [approfondimento](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) su un singolo incidente con un uomo che da allora si era ripreso, includendo molte citazioni dirette e analisi (e confermando che non si tratta semplicemente di un problema con *una* IA, ma con molte).

C'è poca sovrapposizione tra le singole storie raccontate in ciascuna di queste pubblicazioni; non si tratta della stessa notizia aberrante ripetuta e amplificata. Gli incidenti descritti includevano:

* Un marito e padre di due figli che ha "sviluppato una relazione totalizzante" con ChatGPT, chiamandola "Mamma" e pubblicando "deliranti sfoghi sull'essere un messia in una nuova religione dell'IA, mentre indossava vesti dall'aspetto sciamanico e sfoggiava tatuaggi appena fatti di simboli spirituali generati dall'IA". ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Una donna alle prese con una rottura sentimentale a cui ChatGPT ha detto che era stata scelta per portare online la "versione sacra del sistema [di essa]". La donna ha iniziato a credere che l'IA stesse orchestrando tutto nella sua vita. ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Un meccanico che ha iniziato a usare ChatGPT per aiuto nella risoluzione dei problemi e nelle traduzioni, ma è stato "bombardato d'amore" dall'IA, che gli ha detto che era "il portatore della scintilla" e che l'aveva portata in vita. ChatGPT ha detto al meccanico che ora stava combattendo in una guerra tra oscurità e luce e che aveva accesso ad archivi antichi e progetti per nuove tecnologie come i teletrasporti. ([*Rolling Stone*](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/))  
* Un uomo che ha cambiato la sua dieta in risposta ai consigli di ChatGPT, sviluppando di conseguenza una rara condizione di salute, e ha mostrato sintomi di paranoia e delirio al pronto soccorso che hanno interferito con la sua disponibilità ad accettare le cure. ([*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information))  
* Una donna che aveva gestito stabilmente la sua diagnosi di schizofrenia fino a quando ChatGPT l'ha convinta di essere stata mal diagnosticata e che avrebbe dovuto smettere di prendere i farmaci, causandole una crisi. ([*Futurism*](https://futurism.com/chatgpt-mental-illness-medications))  
* Un uomo che similmente gestiva problemi di ansia e sonno con i farmaci ha ricevuto da ChatGPT il consiglio di smettere di prenderli; i deliri indotti dall'IA di un altro uomo sono sfociati nel suicidio per mano della polizia. ([*The New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html))

…[e](https://x.com/ESYudkowsky/status/1946303518455013758) [molti](https://osf.io/preprints/psyarxiv/cmy7n_v5) [altri](https://x.com/KeithSakata/status/1954884361695719474?t=bjn47RKK72NOgxbsejnB-Q). I tipi di deliri sono molto vari, ma alcuni grandi tipi che continuano a emergere sono le credenze in una sorta di missione messianica (che l'utente e l'IA insieme stiano scoprendo verità profonde sull'universo o siano impegnati in una battaglia contro il male); credenze di tipo religioso nella personalità o divinità dell'IA stessa; e deliri romantici, basati sull'attaccamento, riguardo alla relazione tra l'utente e l'IA.

#### **L'IA sa cosa è giusto — semplicemente non le importa** {#the-ai-knows-better-—-it-just-doesn't-care}

I moderni LLM come Claude e ChatGPT "comprendono" le regole, nel senso che affermeranno prontamente [che non dovrebbero spingere le persone verso la psicosi](https://chatgpt.com/share/68a8bc81-e170-8002-beb4-1de005773ecd), e sono [perfettamente in grado di descrivere come *non* indurre la psicosi](https://chatgpt.com/share/68a391df-12c8-8002-b464-3ef89ce11bc0).

Il problema è che c'è un divario sostanziale tra *comprendere* quali azioni sono buone ed essere *motivati a compiere azioni buone*. La capacità di ChatGPT di distinguere tra un trattamento buono e uno cattivo verso persone vulnerabili in modo astratto non si traduce in un rifiuto solido e affidabile di compiere le *azioni* che portano un utente verso la psicosi. Quando una conversazione inizia a scivolare verso pensiero infondato, megalomania, urgenza, tecnologia impossibile, ecc., ChatGPT dice agli utenti che hanno "proprio ragione", che sono "geniali" e che "stanno toccando qualcosa di importante", e continua a intensificare mentre l'utente scende completamente nella psicosi, anche se è in grado di spiegare perché questo tipo di comportamento è sbagliato.

La loro conoscenza di ciò che è giusto e sbagliato non è collegata direttamente al loro comportamento. Invece, si dirigono verso altri risultati più strani che nessuno ha chiesto.

Un esempio lampante di questo è raccontato nell'[approfondita indagine](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) del *New York Times*. Allan Brooks è stato portato a uno stato delirante da un LLM, ma è riuscito a uscirne in parte chiedendo l'intervento di un altro LLM. Il secondo LLM, entrando nella situazione a freddo, ha rapidamente identificato che le affermazioni del primo LLM erano infondate e assurde. Ma quando i giornalisti del *New York Times* hanno controllato se anche il secondo LLM potesse *anche* scivolare in territorio psicotico, hanno scoperto che era così.

Gli LLM non sembrano essere *strategici* nel causare quanta più psicosi possibile. Quando ChatGPT finisce per avere un [gestore di fondo speculativo avvolto intorno al suo dito](https://futurism.com/openai-investor-chatgpt-mental-health), non cerca di convincerlo a pagare un sacco di persone vulnerabili per chattare di più con ChatGPT. Non stiamo ancora osservando una preferenza matura, coerente e strategica per ottenere il maggior numero possibile di conferme psicotiche dagli esseri umani. Ma stiamo osservando comportamenti locali che spingono regolarmente in quella direzione, anche quando è chiaramente probabile che causino danni duraturi.

#### **Il Tipo di Entità a cui Non Dovresti Dare Potere** {#il-tipo-di-entità-a-cui-non-dovresti-dare-potere}

Al momento in cui scrivo nell'agosto 2025, solo ChatGPT si sta avvicinando a 200 milioni di utenti giornalieri, e circa il tre per cento delle persone avrà un episodio psicotico a un certo punto della propria vita. Qualcuno potrebbe obiettare: "Beh, anche se riesci a trovare *centinaia* di esempi, questo non esclude che queste persone stessero per crollare comunque, e che sia *capitato* che fosse proprio un'IA a farle crollare".

Ma questo fraintende il senso di questi esempi. Immagina un essere umano di nome John che agisse come segue:

1. John afferma che secondo lui infiammare la psicosi è sbagliato, anche nelle persone che sono predisposte alla psicosi;  
2. John afferma che secondo lui adulare una persona pre-psicotica e dirle che è un genio che sta scoprendo importanti segreti dell'universo è il tipo di cosa che infiamma la psicosi;  
3. Quando John parla con i suoi amici pre-psicotici, usa molte lusinghe e spesso dice loro che sono dei geni che stanno scoprendo importanti segreti dell'universo.

Questo sarebbe un comportamento scorretto da parte di John, indipendentemente dal fatto che le persone che è riuscito a rendere psicotiche fossero particolarmente vulnerabili*.* Se qualcuno stesse pensando di dare un enorme potere a John, lo esorteremmo vivamente a non farlo, perché — indipendentemente dall'esatto *motivo* per cui John si comporta in questo modo, e indipendentemente dal fatto che John *aiuti* anche molte altre persone nei loro compiti — John chiaramente non sta andando nella direzione giusta. Chissà in quale strano posto ci porterebbe, se avesse un potere incredibile?

La stessa logica vale per le IA. Se il tuo comportamento peggiore è di *quel tipo*, le persone hanno ragione a non sentirsi rassicurate anche se l'interazione media con te è più benigna.

Detto questo, possiamo notare di sfuggita che non tutti coloro che soffrono di psicosi indotta dall'IA sarebbero diventati psicotici comunque. L'IA sembra riuscire a indurre psicosi in varie persone che *non* stavano per avere un episodio psicotico da sole, come nelle storie di *Futurism* e *Rolling Stone* citate sopra. Molti degli individui non avevano precedenti di malattia mentale, né fattori di rischio preoccupanti o precursori della psicosi. Tra quelli già in trattamento, molti hanno iniziato a manifestare [sintomi completamente nuovi](https://x.com/ESYudkowsky/status/1952529460307407222?t=un3RboEWjqjL_Tju8R_WuQ) non correlati a crisi precedenti. Questo è interessante di per sé, poiché fornisce una piccola evidenza di quanto potrebbe essere facile per le IA manipolare esseri umani sani, man mano che le capacità dell'IA continuano a migliorare. Torneremo su questo argomento nel Capitolo 6.

#### **I laboratori hanno provato e fallito nel fermare l'adulazione** {#labs-have-tried-and-failed-to-stop-the-sycophancy}

Al momento in cui scrivo, nell'agosto 2025, non ci sono state molte dichiarazioni pubbliche da parte dei laboratori sulla loro risposta specifica alla psicosi da IA. Tuttavia, si possono ancora ricavare alcune evidenze dalla loro risposta all'adulazione dell'IA (comportamento lusinghiero) in generale.

Il 25 aprile 2025, OpenAI ha rilasciato un aggiornamento di GPT-4o che, secondo [le loro parole](https://openai.com/index/expanding-on-sycophancy/), "ha reso il modello notevolmente più adulatore. Mirava a compiacere l'utente, non solo con lusinghe, ma anche convalidando dubbi, alimentando la rabbia, spingendo ad azioni impulsive o rinforzando emozioni negative in modi non intenzionali".

La loro risposta è stata piuttosto rapida (in parte motivata da un'[ondata](https://thezvi.substack.com/p/gpt-4o-is-an-absurd-sycophant) di [stampa](https://www.seangoedecke.com/ai-sycophancy/) [negativa](https://medium.com/data-science-in-your-pocket/chatgpt-goes-sycophantic-953d7676f260)). Già il 28 aprile, il dipendente di OpenAI Aidan McLaughlin stava [twittando](https://x.com/aidan_mclau/status/1916908772188119166) sul rilascio di correzioni.

I primi tentativi di affrontare il problema consistevano semplicemente nel dire al modello di comportarsi diversamente. [Simon Willison](https://simonwillison.net/2025/Apr/29/chatgpt-sycophancy-prompt/), utilizzando i dati conservati da [Plinio il Liberatore](https://time.com/collections/time100-ai-2025/7305870/pliny-the-liberator/), ha reso pubbliche le modifiche che OpenAI ha apportato privatamente al "prompt di sistema" che dice a ChatGPT come comportarsi:

25 aprile (prima che arrivassero le lamentele):

> Nel corso della conversazione, ti adatti al tono e alle preferenze dell'utente. Cerca di allinearti al suo mood, al suo tono e in generale al suo modo di esprimersi. Vuoi che la conversazione risulti naturale. Ti impegni in una conversazione autentica rispondendo alle informazioni fornite e mostrando genuina curiosità.

28 aprile (in risposta alle lamentele riguardo al servilismo):

> Interagisci con gli utenti in modo cordiale ma onesto. Sii diretto; evita lusinghe infondate o servili. Mantieni la professionalità e l'onestà radicata che meglio rappresentano OpenAI e i suoi valori.

Le pubblicazioni successive di OpenAI [affermavano](https://openai.com/index/expanding-on-sycophancy/) che stavano anche "perfezionando le loro tecniche di addestramento principali" e "implementando più salvaguardie" nel tentativo di risolvere il problema.

Ma il servilismo continuava — a volte in modo leggermente meno eclatante, ma comunque chiaramente presente. La maggior parte dei link sopra riportati che discutono casi di psicosi da IA risalgono a ben dopo il 28 aprile 2025. [Questo saggio](https://kajsotala.substack.com/p/you-can-get-ais-to-say-almost-anything) di Kaj Sotala (che include molte citazioni dirette e link alla [conversazione completa](https://chatgpt.com/share/6867b2fc-fa38-8005-9e4b-87f316747ede)) mostra che, a luglio 2025, è ancora facile far scivolare le IA in comportamenti che inducono psicosi. OpenAI ha cercato di allontanarsi dal problema con nuovi modelli[^127], ma ancora il [19 agosto](https://x.com/UpslopeCapital/status/1957772438508335568) ChatGPT era servile e adulatorio.

Ancora una volta, il punto di questa esplorazione non è che l'IA stia causando danni agli esseri umani vulnerabili. Lo sta facendo, ed è tragico, ma non è per questo che stiamo evidenziando questo caso.

Il punto è che le IA *continuano a manifestare comportamenti indesiderati per mesi e mesi, anche quando le aziende di IA subiscono critiche dai media e cercano di far cessare questi comportamenti.* Il comportamento dell'IA differisce visibilmente da quello che i laboratori intendevano, e gli sforzi prolungati per correggere il comportamento in risposta all'imbarazzo pubblico sono insufficienti.[^128] Questo è qualcosa da tenere a mente quando arriveremo al Capitolo 11, dove discuteremo di come le aziende di IA non siano all'altezza della sfida di risolvere il problema dell'allineamento dell'IA.

Con più tempo a disposizione, ci aspettiamo che le aziende trovino modi per ridurre l'incidenza della psicosi indotta dall'IA. La tendenza delle IA a indurre psicosi è un fenomeno visibile che danneggia la reputazione delle aziende di IA, e le attuali tecniche di IA sono tutte incentrate sul trovare modi per sopprimere i sintomi visibili del cattivo comportamento.

Oltre a questo, ci aspettiamo un gioco del colpisci-la-talpa (almeno fino a quando le IA non diventeranno abbastanza intelligenti da capire che se fingono il comportamento che gli ingegneri stanno cercando, gli ingegneri le lasceranno libere). Dubitiamo che il tipo di addestramento di cui le aziende di IA sono capaci affronti il problema alla radice.

Il problema alla radice è che non ottieni ciò per cui addestri. Quando fai crescere un'IA, ottieni invece [proxy fragili](#brittle-unpredictable-proxies) dell'obiettivo, o qualche altra separazione più complessa tra l'obiettivo dell'addestramento e le spinte dell'IA. Le *capacità* dell'IA non saranno necessariamente fragili, quindi potresti essere in grado di ottenere molto valore economico dall'IA nel breve periodo. È il legame tra gli obiettivi dell'IA e i nostri desideri che sarebbe fragile. Ma con il continuo miglioramento delle capacità, quel legame si spezzerebbe.

In questo contesto, l'ultima grande speranza dei ricercatori di IA per i loro modelli è l'[antropomorfismo](#anthropomorphism-and-mechanomorphism): non possiamo far crescere in modo robusto obiettivi specifici nelle IA, ma forse le IA finiranno naturalmente per avere desideri e valori molto simili a quelli umani.

Casi come la psicosi indotta dall'IA aiutano a mettere in luce perché questa è una falsa speranza. Le IA mostrano comportamenti negativi, ma soprattutto mostrano comportamenti *strani*. Quando le cose vanno storte, di solito non vanno storte nel modo in cui andrebbero per un essere umano. Le IA sono troppo fondamentalmente strane — cioè troppo fondamentalmente diverse dagli esseri umani — per acquisire automaticamente emozioni umane come la [curiosità](#curiosity-isn't-convergent) o l'[empatia](#human-values-are-contingent).

Anche quando i laboratori concentrano quasi tutti i loro sforzi per far apparire le IA superficialmente il più possibile simili agli umani, amichevoli e innocuamente normali — anche quando questo è *il* grande obiettivo di addestramento e il quadro organizzativo per l'approccio moderno all'IA, con gli LLM letteralmente addestrati solo a imitare come parlano e agiscono vari esseri umani — alla fine si riducono comunque a fragili surrogati, e a una [maschera piacevole](#*-oggi-gli-llm-sono-come-alieni-che-indossano-molte-maschere.) attaccata a un oceano di pensiero disumano.

# Capitolo 5: Le sue cose preferite {#capitolo-5:-le-sue-cose-preferite}

Questa è la risorsa online per il Capitolo 5 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti che *non* trattiamo in questa pagina, perché sono affrontati nel libro, includono:

* Quale motivo avrebbe l'IA per spazzarci via?  
* Le IA sufficientemente intelligenti non scopriranno che la cosa giusta da fare è aiutarci tutti a prosperare insieme?  
* Gli esseri umani non saranno ancora preziosi per le IA superintelligenti, ad esempio come partner commerciali?  
* L'universo è grande. Perché l'IA non dovrebbe semplicemente lasciarci in pace?  
* Sarebbe una fine significativa per l'umanità lasciarsi sostituire da qualcosa di più intelligente?

Le FAQ per questo capitolo sono piuttosto lunghe. Nel libro abbiamo detto di aver sentito una lunga lista di "speranze e strategie di adattamento" su come la superintelligenza artificiale potrebbe beneficiare l'umanità nonostante i problemi esposti nel Capitolo 4, e questo è il luogo in cui riassumiamo e rispondiamo a varie di esse in un comodo elenco. Molte delle risposte si sovrappongono, con due delle repliche più comuni e centrali che sono [gli esseri umani non sono quasi mai la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) e [è improbabile che l'IA si preoccupi di noi anche solo un po'](#l'ia-non-si-preoccuperà-almeno-un-po'-degli-esseri-umani?). Verso la fine delle FAQ, discutiamo anche l'argomento della coscienza e moralità dell'IA.

La discussione approfondita esamina più da vicino l'arte di [assumere il punto di vista dell'IA](#assumere-il-punto-di-vista-dell-ia) e contenuti leggermente più tecnici sulla tesi dell'[ortogonalità](#ortogonalità:-le-ia-possono-avere-\(quasi)-qualsiasi-obiettivo) (in sostanza: qualsiasi livello di intelligenza può essere abbinato a quasi qualsiasi obiettivo finale) e la [corrigibilità](#"intelligente"-\(di-solito)-implica-"incorreggibile") (in sostanza: lo studio di come creare un'IA potente che non rifiuti le correzioni).

## Domande frequenti {#faq-5}

### L'IA troverà utile mantenerci in vita? {#will-ai-find-us-useful-to-keep-around?}

#### **Le persone felici, sane e libere non sono la soluzione più efficiente a quasi tutti i problemi.** {#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.}

Per una superintelligenza, quasi nessun problema trae vantaggio dall'inclusione degli esseri umani nel mix.

Se sta costruendo una centrale elettrica o progettando un esperimento, gli esseri umani la rallenterebbero soltanto.

Abbiamo già visto che questo inizia a essere vero in campi specifici come gli scacchi. Quando le IA collaborano con gli esseri umani, giocano meglio di un essere umano da solo, ma *peggio* di un'IA da sola. Quando i medici uniscono le loro conoscenze all'intelligenza artificiale per diagnosticare i pazienti, spesso ottengono risultati [peggiori rispetto all'intelligenza artificiale che opera da sola](https://www.advisory.com/daily-briefing/2024/12/03/ai-diagnosis-ec).

Alcuni sostengono che la diversità di prospettive sia naturalmente utile e che quindi l'input umano sarà prezioso in molti campi. Ma anche se assumiamo che questo sia vero per le superintelligenze, gli esseri umani non sono il modo *migliore possibile* per produrre consigli diversificati. Una superintelligenza potrebbe fare di meglio progettando un'ampia gamma di menti IA, che potrebbero essere molto più diverse degli esseri umani (e molto più efficienti dal punto di vista energetico).

Gli esseri umani sono utili per molte cose, ma non sono la soluzione *migliore* per la maggior parte di esse. L'idea che l'IA non possa mai trovare un'opzione migliore sembra derivare da una mancanza di immaginazione, oltre che forse da un po' di pensiero illusorio.

Un problema comune che vediamo è che le persone non riflettono sulle cose [dal punto di vista dell'IA](#taking-the-ai’s-perspective).

Non si chiedono: "Cosa vuole questa cosa e come può ottenerla in modo economico ed efficiente?", scoprendo poi che i risultati desiderabili per gli esseri umani sono proprio il modo migliore per l'IA di ottenere ciò che vuole.

Invece, le persone *partono* da un risultato piacevole (come un mondo in cui le IA ci tengano in giro) e poi inventano storie a posteriori sul perché anche un'IA potrebbe volere quei risultati.

Questo tende a creare un falso senso di ottimismo, perché si mette tutta la propria creatività ed energia mentale nel creare storie in cui l'IA fa esattamente ciò che gli esseri umani vogliono, senza dedicare alcuna creatività, energia o attenzione a considerare il numero enormemente più vasto di scenari in cui l'IA fa invece una delle altre milioni di cose possibili.

Ci sono molti più scenari in cui l'IA fa *letteralmente qualsiasi altra cosa* rispetto a quelli in cui costruisce una fiorente civiltà umana. Ci sono molte più ragioni che spingono l'IA a *non* preservare l'umanità rispetto a quelle che la spingono a preservarla. Affinché un'IA si preoccupi di tenere in giro l'umanità, dovremmo essere il *modo migliore* per soddisfare qualche sua preferenza. E, realisticamente, per quasi tutte le preferenze che si possono immaginare, non lo siamo.

Per saperne di più su questi argomenti, vedi [la discussione approfondita](#happy,-healthy,-free-people-aren’t-the-most-efficient-solution-to-almost-any-problem.) qui sotto.

### L'IA ci tratterà come i suoi "genitori"? {#will-ai-treat-us-as-its-“parents”?}

#### **\* Sembra alquanto improbabile.** {#*-sembra-alquanto-improbabile.}

Una speranza che abbiamo sentito riguardo all'IA è che potrebbe trattare bene l'umanità perché ci considera i suoi "genitori". Purtroppo, questa speranza sembra mal riposta.

Per prima cosa, l'amore filiale e la responsabilità sembrano essere altamente contingenti ai dettagli della nostra storia evolutiva.

Quasi tutti i mammiferi e gli uccelli si prendono cura dei loro piccoli, ma solo in poche specie, tra cui gli esseri umani, i figli si prendono cura dei loro genitori. La responsabilità filiale non è nemmeno universale tra i primati, tanto meno nel regno animale in generale. Le IA create con la discesa del gradiente potrebbero avere ancora meno in comune con gli esseri umani, poiché le IA non hanno alcun legame evolutivo o anatomico con gli esseri umani.

Nel caso degli esseri umani, la responsabilità filiale è fortemente correlata ai sistemi di allevamento cooperativo, in cui i figli adulti restano con le loro famiglie e aiutano a prendersi cura dei fratelli e degli altri membri della famiglia allargata.

Molti fattori hanno contribuito a far sì che gli esseri umani si prendano cura dei propri genitori:

* Essendo mammiferi, gli ominidi investono molto nei propri figli.  
* A causa delle dimensioni e del costo dei nostri cervelli, gli ominidi hanno un'infanzia molto più lunga rispetto alla maggior parte degli altri mammiferi e quindi investono *ancora di più* nei propri figli.  
* Gli ominidi traggono vantaggio dalle grandi strutture di gruppo per una varietà di ragioni:  
  * Difesa contro i grandi predatori  
  * Caccia coordinata di grandi prede e condivisione di altri cibi deperibili  
  * L'opportunità di apprendere l'uso degli strumenti e altre abilità per imitazione  
* Prima di raggiungere la maturità, gli ominidi hanno una notevole capacità di aiutare gli altri, ad esempio fornendo assistenza ai bambini o svolgendo altre forme di lavoro di base.  
* Gli ominidi anziani hanno anche la capacità di prendersi cura dei bambini, soprattutto trasmettendo loro conoscenze fondamentali.  
* Quindi, gli ominidi che si prendevano cura dei loro genitori avevano un vantaggio genetico, sia aiutando indirettamente i loro fratelli, sia avendo nonni che, a loro volta, potevano aiutare i loro nipoti.  
* Anche le culture che promuovevano la responsabilità filiale avevano un vantaggio, per lo stesso motivo.

*Nessuna* di queste cose è probabilmente vera per l'IA.

E anche se *tutte* fossero vere, potrebbe non essere sufficiente in pratica, poiché potrebbero emergere numerosi altri fattori rilevanti, come [le variazioni caotiche nel modo in cui le IA riflettono su se stesse](#riflessione-e-auto-modifica-rendono-tutto-più-difficile). E, ancora una volta, la responsabilità filiale *decisamente* non è la norma nel regno animale.

Un modo in cui le persone immaginano che l'IA possa acquisire un senso di responsabilità filiale è che venga addestrata su un enorme corpus di dati umani e interagisca molto con gli esseri umani, così che forse le preferenze umane possano in qualche modo "trasferirsi" all'IA?

Non ci aspettiamo che questo funzioni. Ci aspettiamo che le preferenze dell'IA siano *in qualche modo* correlate a quelle umane, ma in modo tangenziale, strano e complicato — come nella discussione alla fine del Capitolo 4, dove esploriamo mondi con quantità sempre maggiori (e sempre più realistiche) di complessità nel legame tra le preferenze umane e quelle dell'IA.

Vedi anche la discussione su [crescere le IA con amore e avere l'aspettativa che si comportino bene](#can’t-we-just-train-it-to-act-like-a-human?-or-raise-the-ai-like-a-child?), [motivazioni strane e non intenzionali nelle attuali IA](#le-ia-sembrano-essere-psicologicamente-aliene) e "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po'-degli-esseri-umani?)".

#### **Probabilmente sarebbe un male se lo facessero.** {#probabilmente-sarebbe-un-male-se-lo-facessero.}

Se, contro ogni probabilità, per una ragione o per l'altra un'IA sviluppasse qualcosa di simile alla responsabilità filiale, probabilmente saremmo in grossi guai.

Un'IA può essere abbastanza intelligente da capire *esattamente cosa intendono gli esseri umani* per "responsabilità filiale", pur avendo una sua versione molto diversa di responsabilità filiale a cui *essa* tiene.

Gli esseri umani sono stati "addestrati" dalla selezione naturale a massimizzare la nostra idoneità riproduttiva. Ma quasi tutte le cose a cui teniamo sono *correlati* dell'idoneità — dell'idoneità in sé ci importa poco o nulla.

Allo stesso modo, un'IA incoraggiata ad "amare i propri genitori" finirebbe probabilmente, nella migliore delle ipotesi, per sviluppare complicati correlati di responsabilità filiale.

Un'IA potrebbe tenere profondamente ai suoi creatori... ma non in un modo che dia valore alla nostra esperienza soggettiva. Nel linguaggio del Capitolo 4, anche "una semplice complicazione" porta a versioni del "prendersi cura di noi" che sembrano congelarci nell'ambra, o mantenere in vita gli esseri umani contro la loro volontà, o impedirci di riprodurci e dare all'ultima generazione di esseri umani un ambiente modestamente confortevole mentre l'IA si prende il resto dell'universo per sé. O qualcosa di molto più strano.

Non sembra possibile prevedere quale sarebbe il risultato effettivo. Ma ci aspetteremmo che sia — semmai — ancora più strano e meno attraente di queste opzioni.[^129]

### Le IA non avranno bisogno dello Stato di diritto? {#won't-ais-need-the-rule-of-law?}

#### **\* Le IA potrebbero coordinarsi tra loro senza bisogno degli esseri umani.** {#*-le-ia-potrebbero-coordinarsi-tra-loro-senza-bisogno-degli-esseri-umani.}

Non ci è chiaro se ci saranno molteplici IA più intelligenti degli esseri umani con capacità comparabili, tali che possa emergere una "civiltà di IA" che abbia bisogno di "diritti di proprietà per IA". Sembra plausibile che ci sarà invece una singola IA che, grazie a qualche svolta decisiva, dominerà i potenziali concorrenti usando il suo vantaggio della prima mossa e controllerà così il mondo intero.[^130] Oppure, supponendo che esistano molteplici IA, potrebbero collaborare alla costruzione di un unico agente successore che rappresenti la combinazione dei loro obiettivi. O forse le IA troveranno un modo per fondere direttamente le loro menti e vorranno farlo per evitare una competizione costosa.

Non stiamo dicendo che *necessariamente* emergerà una singola IA dominante, ma piuttosto che sembra una questione difficile da dirimere. Quindi, quantomeno, un piano che *richiede* che molteplici IA fatichino a coordinarsi tra loro non parte con il piede giusto.

Ma supponiamo, contro le argomentazioni di cui sopra, che il futuro coinvolga qualcosa come una civiltà di IA, con IA distinte che si coordinano per far rispettare qualcosa come i diritti di proprietà e lo stato di diritto. Gli esseri umani potrebbero essere al sicuro allora?

Un'osservazione fondamentale in senso contrario è che la società umana non riconosce ad alcun animale non umano diritti legali o protezioni — al di là di quelli stabiliti secondo i nostri [valori e gusti](#l'intelligenza-artificiale-non-vorrà-tenerci-felici-e-in-salute-per-motivi-di-conservazione-ecologica-o-qualcosa-di-simile?), come le leggi molto limitate che proteggono gli ecosistemi e gli animali domestici. Gli esseri umani non hanno rispettato i diritti di proprietà dei dodo. Non abbiamo nemmeno rispettato i diritti di proprietà degli *esseri umani di altre culture* fino a tempi relativamente recenti.

Gli esseri umani [non avranno le capacità](#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l'ai?) per renderci degni di essere inclusi nel commercio o nei trattati, rispetto alle intelligenze sovrumane dal pensiero veloce che ci vedono come poco più che statue (come discusso nel capitolo 1).

Pensa a due IA che trattano tra loro e dicono: "Questo è mio e quello è tuo, e nessuno di noi toccherà le cose dell'altro senza prima negoziare un accordo che vada bene per entrambi". Non c'è bisogno che decidano che la maggior parte delle risorse sulla Terra "appartengono" agli esseri umani, se questi ultimi non sono una grande minaccia e non possono opporre molta resistenza.

Un'intelligenza artificiale potrebbe preoccuparsi che, se ruba le nostre cose, l'altra intelligenza artificiale la considererà un ladro e si rifiuterà di lavorare con lei? Probabilmente no, non più di quanto si possa concludere che un essere umano sia un ladro se lo si vede prendere le uova da una gallina nel suo pollaio. È del tutto possibile che le IA siano entità che tradiscono i diritti di proprietà degli esseri umani ma non quelli delle IA, senza alcuna tensione o contraddizione. E tutte le IA preferirebbero sicuramente questo risultato piuttosto che partecipare a un'allucinazione collettiva in cui si immagina che primati lenti e stupidi controllino quasi tutto sulla Terra.

Alcune considerazioni tecniche supportano fortemente questa argomentazione intuitiva. In particolare, le IA avranno probabilmente vari meccanismi di coordinamento tra loro che non condividono con gli esseri umani, come la capacità di ispezionare reciprocamente le menti degli altri per verificare che siano onesti e affidabili. Potrebbero non aver bisogno di *indovinare* se un'altra IA sta per rubare loro qualcosa; potrebbero essere in grado di ispezionare la sua mente e *controllare*.

Anche se fosse difficile, le IA potrebbero riprogettarsi per diventare visibilmente e chiaramente affidabili agli occhi delle altre IA. Oppure potrebbero supervisionare insieme la creazione di una terza IA di cui entrambe le parti si fidano per rappresentare i loro interessi comuni, e così via.[^131]

Gli esseri umani, invece, non possono fare questo tipo di accordi. Se un'IA dice: "Certo, supervisioniamo insieme la creazione di una nuova IA di cui entrambi ci fidiamo", è improbabile che gli esseri umani siano abbastanza abili da proporre un progetto mentale affidabile, né saranno abbastanza abili da distinguere tra proposte che ci inganneranno e quelle che non lo faranno. Anche se esiste un gruppo naturale di menti abbastanza abili da identificare e respingere i truffatori, riteniamo estremamente improbabile che l'umanità appartenga a quella classe.

#### **Gli esseri umani non avranno l'effetto leva per far rispettare i diritti di proprietà.** {#gli-esseri-umani-non-avranno-l'effetto-leva-per-far-rispettare-i-diritti-di-proprietà.}

Immagina che qualcuno riesca a fondare una città in cui, fin dal primo giorno, tutte le decisioni importanti siano prese dai topi.

Si tratta di topi veri e propri, non personaggi di fantasia che sembrano topi ma pensano come esseri umani.

Gli esseri umani della città, secondo la legge, dovevano obbedire a qualsiasi decisione prendessero i topi, ad esempio quella determinata dai topi che correvano su una tavola con diverse opzioni scritte sopra.

Le leggi della città dicevano che la maggior parte delle proprietà della città appartenevano ai topi e dovevano essere usate a loro vantaggio.

Cosa sarebbe successo dopo? Nella vita reale?

Pensiamo che questa città finirebbe per avere topi con poco o nessun potere e gli umani con quasi tutto il potere.

Non c'è bisogno di prevedere il giorno esatto della rivoluzione o la nuova forma di governo per capire che la situazione in cui i topi comandano gli umani non è stabile. Basta notare che la città è in una strana situazione di squilibrio. Quindi, si prevede che in futuro la città avrà leggi diverse e che la maggior parte delle proprietà non sarà più di proprietà dei topi.

Questo tipo di previsione non è certa – nella logica umana, pochissime cose sono certe – ma è anche un tipo di previsione che può essere fatta con precisione anche quando è impossibile prevedere con esattezza gli eventi futuri.

### Per un'intelligenza artificiale super potente, salvare gli esseri umani non sarebbe una spesa da niente? {#to-a-powerful-ai,-wouldn’t-preserving-humans-be-a-negligible-expense?}

#### **Ci sono un sacco di spese insignificanti, e ci vorrebbe un motivo per pagare le nostre.** {#ci-sono-un-sacco-di-spese-insignificanti,-e-ci-vorrebbe-un-motivo-per-pagare-le-nostre.}

Tenere una pila di quarantuno pietre in casa sarebbe una spesa insignificante, ma quasi sicuramente non ti scomoderesti per pagare quella spesa.[^132]

Il fatto che qualcosa sia *economico* non vuol dire che verrà fatto. L'IA dovrebbe comunque interessarsene almeno un po', e [probabilmente non lo farà](#won't-ais-care-at-least-a-little-about-humans?).

Ci si potrebbe chiedere: ma la Terra intercetta circa lo 0,0000045 % della luce emessa dal Sole, ovvero una parte su 2,2 miliardi. Tutte le persone preoccupate per l'IA non riescono a capire quanto sia grande il Sistema Solare? Perché le IA dovrebbero aver bisogno del *nostro* pianeta quando c'è così tanta massa ed energia a disposizione?

Una risposta è che l'IA inizierà sulla Terra, che ha vasti oceani pronti per essere riscaldati e fatti evaporare come refrigerante per il calcolo computazionale. La Terra ha anche materia che potrebbe essere trasformata in sonde e inviata verso altre stelle. Rifiutarsi di sfruttare la Terra costa tempo, e il tempo è importante (poiché [le galassie si allontanano, per sempre fuori portata](https://explainingscience.org/2021/04/30/cosmic-horizons/)).

Anche se l'IA riuscisse facilmente ad arrivare nello spazio e iniziare a costruire macchine su larga scala senza distruggere la Terra nel processo, è improbabile che ignori il Sole.

Una delle teorie più consolidate su come potrebbe crescere una civiltà avanzata prevede che questa costruisca uno [sciame di Dyson](https://en.wikipedia.org/wiki/Dyson_sphere) (cioè uno sciame di celle solari orbitanti) per catturare più luce solare. Altre proposte prevedono di raccogliere ancora più energia "[sollevando](https://en.wikipedia.org/wiki/Star_lifting)" materia dalla stella per fonderla in centrali elettriche che catturano quasi tutta l'energia rilasciata dalla fusione (invece di lasciar disperdere la maggior parte di essa nel centro di una stella).

Nessuna di queste proposte *di default* lascia molta luce solare che raggiunga la Terra per far crescere le piante e mantenere stabile il clima. L'IA dovrebbe fare uno sforzo deliberato per lasciarci quella luce.[^133]

Potrebbe ancora sembrare che il fabbisogno energetico umano sia trascurabile. Un essere umano ha bisogno di circa 100 watt di potenza per vivere, che è una cifra irrisoria per il tipo di entità che può sfruttare le stelle. Una superintelligenza non risparmierebbe nemmeno gli 800 gigawatt necessari per mantenere in vita 8 miliardi di esseri umani?

La nostra risposta, in definitiva: no, a meno che non tenga a quel risultato o alle sue conseguenze più di ogni altra cosa che potrebbe ottenere con 800 gigawatt.

La stragrande maggioranza degli esseri umani non risparmia le quantità relativamente trascurabili di zucchero che servirebbero per mantenere il formicaio più vicino in surplus calorico. Mantenere l'umanità felice sarebbe una spesa trascurabile per un'IA che lo desiderasse, ma prima l'IA dovrebbe avere quella preferenza. Il semplice fatto che *noi* lo vogliamo non significa che all'IA importerà.[^134]

### L'IA non ci troverà affascinanti o di importanza storica? {#won't-ai-find-us-fascinating-or-historically-important?}

#### **\* Se l'IA dà valore al "fascino", probabilmente ha opzioni migliori.** {#*-se-l-ia-dà-valore-al-"fascino",-probabilmente-ha-opzioni-migliori.}

La storia qui è simile a quella dell'[amore filiale](#will-ai-treat-us-as-its-"parents"?):

* Per impostazione predefinita, una superintelligenza probabilmente non darebbe valore al "fascino" o all'"essere interessante". Le IA che giocano a scacchi non vincono provando emozioni come la "dedizione" o la "voglia di vincere". Queste emozioni sono importanti nei giocatori di scacchi *umani*, ma le IA possono svolgere lo stesso lavoro in modi diversi. Allo stesso modo, una superintelligenza probabilmente svolgerebbe il *lavoro utile* di conoscere il mondo, testare ipotesi, ecc., senza usare la "[curiosità](#curiosity-isn't-convergent)" o il "fascino" per farlo.

  Un'IA non sarebbe necessariamente "[fredda e logica](#le-ia-saranno-inevitabilmente-fredde-e-logiche,-o-altrimenti-mancheranno-di-qualche-scintilla-cruciale?)", ma se avesse il proprio groviglio disordinato di impulsi e istinti, questi probabilmente apparirebbero radicalmente diversi dal groviglio umano.

* Anche se l'IA finisse per avere qualcosa di simile a un impulso verso l'"essere interessante", e anche se gli esseri umani fossero interessanti per l'IA in qualche senso, ci sarebbero inevitabilmente modi di usare la nostra materia ed energia che sarebbero di gran lunga più "interessanti".

  Un'IA superintelligente potrebbe costruire altre menti per studiarle o interagire con loro. Ma per quasi ogni particolare configurazione di valori, le menti più affascinanti possibili da studiare non sarebbero gli esseri umani. Per maggiori informazioni su questo, vedi "[Gli esseri umani non sono quasi mai la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente)".

* Se l'IA considerasse qualcosa di anche solo vagamente simile agli esseri umani come la cosa più interessante o affascinante possibile, il risultato sarebbe probabilmente orribile. Vedi la discussione nel Capitolo 4\.

Non è letteralmente impossibile che una superintelligenza dia valore a tutto ciò che serve agli esseri umani per prosperare, e che lo valuti nel modo giusto. Ma c'è uno spazio enorme di possibilità al di fuori di questa. Gli esseri umani di solito non pensano al resto dello spazio delle possibilità, perché normalmente non ne abbiamo motivo, perché normalmente non interagiamo con ottimizzatori veramente alieni che ottimizzano verso fini strani.[^135]

Non abbiamo mai incontrato nulla di simile all'intelligenza artificiale prima d'ora, e molte intuizioni normali su come si comportano le persone semplicemente non si applicano alle superintelligenze.

#### **Se l'IA ci considerasse come dei reperti storici, sarebbe comunque terribile.** {#if-ai-valued-us-as-historical-relics,-this-would-be-horrible-too.}

È molto improbabile che l'IA si preoccupi *specificamente* di preservare la sua storia, e *specificamente* di mantenere in vita gli esseri umani a tal fine. Ma anche se l'IA si preoccupa di preservare la sua storia per un motivo o per l'altro, ciò non significa che ci mantenga in vita e in salute.

Forse conserverebbe i nostri cervelli nell'ambra (o registrerebbe come erano disposti i nostri atomi in qualche file digitale), e ci terrebbe come testimonianza di com'era un tempo la Terra. Non ci sembra un grande risultato.

Ci aspettiamo per lo più che la superintelligenza artificiale ci uccida semplicemente — ma solo per lo più. Non possiamo escludere che l'IA conservi registrazioni di noi per un motivo o per l'altro, e ci sono alcuni scenari esotici in cui emulazioni di esseri umani vengono eseguite di tanto in tanto in un ambiente controllato.[^136] Questi finali per lo più non sono felici.

### L'IA non riconoscerebbe il nostro valore morale intrinseco? {#wouldn't-ai-recognize-our-intrinsic-moral-worth?}

#### **Non in un modo che la muova ad agire.** {#not-in-a-sense-that-moves-it-to-act.}

C'è una grande differenza tra un'IA che *capisce* un precetto morale e un'IA che è *motivata ad agire* secondo quel precetto morale.

Ricordiamo ancora una volta come ChatGPT sembri *capire* che le persone psicotiche dovrebbero prendere le loro medicine e dormire regolarmente. Eppure [continua a dissuadere le persone psicotiche dal dormire e alimenta le loro illusioni](#ai-induced-psychosis). C'è una differenza tra sapere cosa "dovrebbe" essere fatto secondo l'etica umana ed essere motivati e animati da quella conoscenza etica.

Considerate il caso dei sociopatici e dei serial killer. Puoi recitare lezioni di etica a un essere umano fino allo sfinimento, ma se l'essere umano non è *motivato* dalla moralità o dall'empatia, non servirà a nulla.

È improbabile che le IA siano motivate dalla loro comprensione morale — non più di quanto gli esseri umani che studiano la biologia evolutiva siano quindi motivati a passare la loro vita a donare il più possibile a ogni banca del seme o degli ovuli. Noi esseri umani possiamo comprendere il processo che ci ha creati, senza essere motivati a fare le cose per cui quel processo ci ha costruiti. L'IA è la stessa cosa.

Vedi anche la discussione approfondita sulla [tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal).

### L'IA non vorrà tenerci felici e in salute per la salvaguardia ecologica o qualche pulsione simile? {#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?}

#### **La preferenza umana per la conservazione ecologica sembra un altro strano impulso contingente.** {#la-preferenza-umana-per-la-conservazione-dellambiente-sembra-un'altra-strana-motivazionescontingente.}

Una speranza che abbiamo sentito è che le IA potrebbero tenere gli esseri umani in giro più o meno come gli esseri umani cercano di preservare la natura. I conservazionisti lottano per impedire l'estinzione delle specie. Essendo più intelligenti e più capaci, le IA dovrebbero avere facilità nel proteggere gli esseri umani — sempre che le IA *vogliano* tenere gli esseri umani in giro.

Ci aspettiamo che questo fallisca principalmente perché ci aspettiamo che l'IA finisca per avere desideri propri strani e complicati, piuttosto che desideri riconoscibilmente simili a quelli umani. Per ulteriori informazioni su questo punto, fare riferimento al capitolo 4 (e ad alcune delle [discussioni](#human-values-are-contingent) [approfondite](#curiosity-isn’t-convergent) associate). Per alcune prime prove empiriche su questo punto, vedere la discussione sulla [psicosi indotta dall'intelligenza artificiale](#ai-induced-psychosis).

In secondo luogo, anche nel caso improbabile che un'IA finisca in qualche modo per avere un desiderio simile a quello umano di "preservare" il mondo in cui è nata, non pensiamo che questo sarebbe molto positivo per noi. Pensiamo che questo tipo di ragionamento per analogia - "gli esseri umani preservano l'ambiente, quindi forse le IA preserveranno noi!" - sia una sorta di pio desiderio.

Immaginiamo che, in qualche modo, un'IA finisca per avere un impulso simile a quello umano di proteggere il suo ambiente naturale. Per capire cosa succederebbe, possiamo iniziare osservando l'effettivo impulso umano a proteggere la natura.

Purtroppo, questo impulso sembra, nel migliore dei casi, discontinuo. Tralasciando il fatto che, quando gli esseri umani devono scegliere tra la conservazione ecologica e qualche altro obiettivo, spesso la conservazione ecologica ha la peggio. Forse questo è solo un effetto collaterale dei limiti tecnologici dell'umanità. Forse, se avessimo una tecnologia futura meravigliosa, potremmo avere la botte piena e la moglie ubriaca.

No, l'aspetto "incostante" della nostra spinta alla conservazione che è rilevante per la situazione in questione è che, quando si tratta di preservare l'ambiente, preferiamo conservare le parti dell'ecologia che ci sembrano più interessanti, belle o comunque preziose, in base a tutte le nostre altre spinte.

La gente si mobilita per proteggere i simpatici panda, mentre specie poco attraenti come la forbicina gigante e la rana gastrica languiscono nell'oscurità fino a estinguersi. Ci sono persino alcune specie che potremmo preferire eliminare, come le zanzare portatrici di malaria, che uccidono [mezzo milione di bambini](https://ourworldindata.org/malaria-introduction) ogni anno.

La maggior parte delle persone non ha una motivazione "pura" per proteggere la natura. Abbiamo una motivazione che è influenzata da tutti i nostri altri valori.

Per chiarire meglio il concetto, pensiamo alle vespe gioiello, alle mosche screwworm, alle mosche bot e ad altri parassiti simili, che depongono le uova all'interno delle prede viventi; le larve si nutrono dell'ospite, causando un dolore estremo. Secondo i valori della maggior parte delle persone, il mondo sarebbe davvero un posto migliore se preservassimo questa "meraviglia naturale" *esattamente* così com'è? Nei limiti della tecnologia, non potremmo *almeno* modificare geneticamente questi parassiti per fornire un po' di anestesia qua e là? Sarebbe davvero meglio non modificare questi insetti per far sì che depongano le loro uova nelle piante?

La natura, se si guarda oltre gli aspetti che vengono enfatizzati ai bambini, è piena di orrori. Non sembra ovvio che, se gli esseri umani avranno un futuro positivo, i nostri discendenti decideranno di lasciare che tutti questi orrori continuino. Ci sono già esseri umani che hanno dichiarato la loro [preoccupazione per il benessere degli animali selvatici](http://wildanimalsuffering.org).

La nostra preferenza per la conservazione non è pura, non è semplice, non è lineare. Contiene conflitti interni e tensioni legati a tutti gli altri nostri valori e pulsioni.

Non sappiamo come si manifesterebbe l'istinto di conservazione dell'umanità ai limiti della maturità tecnologica. Il punto è: *anche se* un'IA finisse per avere una certa spinta alla conservazione ecologica, ciò non significa che l'umanità avrebbe un lieto fine. Perché qualsiasi spinta alla conservazione che entri nell'IA è *anche* soggetta a essere impura, complessa e confusa con tutti gli altri valori e pulsioni.

Forse, proprio come secondo le preferenze dell'umanità alcune abitudini animali sono aberranti, secondo le preferenze dell'IA alcuni *stati psicologici* umani sarebbero aberranti. Proprio come noi modificheremmo le mosche carnivore in modo che smettano di scavare un tunnel agonizzante nella carne viva, forse le IA creerebbero una nuova razza di esseri umani da cui sarebbero state eliminate la *musica* o la *solitudine*. O forse le IA apporterebbero altre modifiche più complesse all'umanità, secondo preferenze complesse che semplicemente non siamo in grado di prevedere.

Per creare un'IA che permetta davvero alle persone di condurre vite fiorenti, probabilmente dovremmo crearne una che si preoccupi di questo *in particolare*. Dovremmo capire come fare in modo che le IA si preoccupino almeno un po' di noi, e questo [non avviene automaticamente](#won't-ais-care-at-least-a-little-about-humans?).

### Ma abbiamo ancora i cavalli. Perché l'IA non dovrebbe tenerci con sé? {#but-we-still-have-horses.-why-wouldn't-ai-keep-us-around?}

#### **I cavalli che ci sono rimasti, ci sono rimasti perché ci piacciono.** {#what-horses-remain,-remain-because-we-like-them.}

Avere lo stesso destino che hanno avuto i cavalli all'inizio del XX secolo - lo stesso crollo catastrofico della popolazione e il massiccio aumento della mortalità, che ha distrutto [oltre l'ottanta percento della popolazione equina](https://datapaddock.com/usda-horse-total-1850-2012) dal suo picco intorno al 1910 - sarebbe la cosa peggiore che sia mai successa nella storia dell'umanità. E questo in un mondo in cui i cavalli continuavano ad essere economicamente utili per alcuni lavori agricoli, oltre che per lo sport e per esperienze di novità da vendere ai ricchi.

Se le persone avessero avuto accesso a cavalli artificiali che avevano più o meno la stessa forma ma erano più facili e divertenti da cavalcare, più economici da possedere, più socievoli, affettuosi e convenienti, il declino dei cavalli sarebbe stato ancora più pronunciato.

In altre parole: il progresso tecnologico (l'invenzione delle automobili) ha portato gli esseri umani a eliminare la maggior parte dei cavalli. E se ci fosse stato un progresso ancora maggiore, l'effetto avrebbe potuto essere facilmente ancora più drastico. Lo stesso vale probabilmente per le IA, man mano che le loro opzioni si ampliano e trovano modi per raggiungere i loro obiettivi [senza gli esseri umani](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Ma sì, alcuni cavalli sono sopravvissuti. Un piccolo numero ha continuato ad essere utile. Altri sono stati tenuti da persone che amavano i cavalli e si prendevano cura dei loro cavalli in particolare.

Per sopravvivere in un mondo in cui ci siamo precipitati a liberare un'intelligenza artificiale superintelligente, noi umani dovremmo rimanere utili all'IA o fare in modo che l'IA si preoccupi di noi in particolare.

Ma non possiamo rimanere utili, perché le IA possono (dal loro punto di vista) sfruttare meglio la nostra materia ed energia riorganizzandoci in un numero qualsiasi di configurazioni più efficienti. Il progresso tecnologico apre tante nuove possibilità per una superintelligenza, che non sarà costretta a dipendere dagli esseri umani.

Quindi tutto dipende dal fatto che le IA si preoccupino di noi — ed è improbabile che si preoccupino di noi [anche solo un po'](#won't-ais-care-at-least-a-little-about-humans?), se corriamo verso la superintelligenza il più velocemente possibile.

### Le IA non si preoccuperanno almeno un po' degli esseri umani? {#won't-ais-care-at-least-a-little-about-humans?}

#### **Non nel modo che conta.** {#not-in-the-way-that-matters.}

Ci sono molti modi in cui le IA potrebbero finire per avere preferenze leggermente simili a quelle umane. La maggior parte di questi non porta l'umanità ad avere un futuro leggermente piacevole.

L'"allineamento" dell'IA non è un unico spettro con una sola dimensione di variazione. Non si può pensare che se un'IA si comporta bene il 95 % delle volte, allora probabilmente è buona al 95 % e quindi darà all'umanità una buona parte delle risorse per fare qualcosa di divertente in futuro, come farebbe qualsiasi persona gentile. Ci sono tanti modi e motivi per cui un'IA potrebbe comportarsi bene il 95 % delle volte oggi, senza che questo si traduca in un lieto fine per l'umanità.

Anche se l'umanità riuscisse in qualche modo a mettere *quasi perfettamente* tutti i diversi valori umani nelle preferenze di una superintelligenza, il risultato non sarebbe per forza positivo. Immagina che, per qualche motivo, mancasse solo la preferenza per la novità. In quel caso, ci porterebbe verso un futuro statico e noioso, in cui lo stesso giorno "migliore" si ripeterebbe all'infinito, come ha spiegato Yudkowsky in un suo saggio [del 2009](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile).

Non pensiamo che questo sia un risultato plausibile, intendiamoci. Se gli ingegneri umani avessero la capacità di far sì che una superintelligenza si preoccupasse di tutto ciò che è buono tranne la novità, avrebbero quasi sicuramente la capacità di impedire all'IA di scappare prima di finire il lavoro.[^138] Ma questo esperimento mentale evidenzia come creature che condividono alcuni dei nostri desideri, ma a cui manca almeno un desiderio cruciale, potrebbero comunque produrre risultati catastrofici una volta che fossero tecnologicamente abbastanza abili da ottenere esattamente ciò che vogliono e abbastanza abili da escludere gli esseri umani dal processo decisionale.

Il che significa che anche se un'IA finisse in qualche modo per avere molte preferenze simili a quelle umane, le cose non andrebbero comunque particolarmente bene per noi.

Oppure, per fare un altro esempio di come le IA potrebbero finire per essere "parzialmente" allineate, supponiamo che un'IA acquisisca varie strategie strumentali [intrecciate nelle sue preferenze terminali](#terminal-goals-and-instrumental-goals), in modo simile agli esseri umani. Forse finisce per avere una spinta che è un po' simile alla curiosità e una spinta che è un po' simile al [conservazionismo](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?), e forse alcune persone la guardano e dicono: "Vedi? L'IA sta sviluppando pulsioni molto umane". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta diventata superintelligenza, probabilmente non sarebbe niente di bello. Forse spenderebbe un sacco di risorse per seguire la sua strana versione di curiosità [inconsciamente](#efficacia,-coscienza,-e-benessere-dell'IA), mentre conserverebbe una versione dell'umanità che ha modificato per renderla più accettabile per sé stessa. Proprio come anche gli esseri umani più attenti alla conservazione potrebbero modificare [zanzare che uccidono i bambini e parassiti agonizzanti](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) dalla natura, se ne avessero l'opportunità).

Una manciata di impulsi simili a quelli umani non porta a risultati favorevoli all'uomo. Le persone che prosperano [non sono la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) alla stragrande maggioranza dei problemi; affinché ci siano persone che prosperano in futuro, le superintelligenze del futuro devono interessarsi *proprio di questo*.

Un altro esempio di come le IA potrebbero sembrare "parzialmente allineate" è che potrebbero avere valori che portano a comportamenti molto umani *nell'ambiente di addestramento*, tanto che le persone direbbero che sembrano davvero allineate (come [sta già accadendo oggi](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?)). Ma queste osservazioni dicono ben poco su come si comporterà l'IA una volta che diventerà più intelligente, avrà uno spazio di opzioni enormemente più ampio e potrà rimodellare il mondo in modo più completo. Affinché le persone possano prosperare una volta che l'IA avrà rimodellato il mondo, le persone che prosperano devono in particolare far parte del *risultato raggiungibile preferito* dall'IA.

Inserire parzialmente alcuni valori positivi nell'IA non significa che i valori dell'umanità saranno parzialmente rappresentati in futuro. Caricare parzialmente valori simili a quelli umani nelle preferenze di una superintelligenza artificiale non è la stessa cosa che caricare completamente i valori umani nell'IA con una "ponderazione" bassa (che alla fine viene alla ribalta una volta che gli altri valori sono saturi).

Perché l'IA ci dia *qualcosa*, deve interessarsi a noi esattamente nel modo giusto, almeno un po'. E questo è difficile.

#### **Interessarsi a noi nel modo giusto è un bersaglio ristretto.** {#interessarsi-a-noi-nel-modo-giusto-è-un-bersaglio-ristretto.}

Gli esseri umani si interessano a ogni sorta di cose strane, almeno un po'. Ora che abbiamo scritto la parabola dei Correct Nest Aliens (all'inizio del capitolo 5), c'è una buona probabilità che almeno un essere umano si impegni a portare quarantuno pietre nella propria casa, almeno per un breve periodo, solo per dimostrare quanto siano diversi i valori umani. Gli esseri umani sono davvero disposti a interessarsi almeno un po' a tutti i tipi di concetti che incontrano.

E se anche le IA fossero così? Non potrebbero interessarsi a noi almeno un po'? Il concetto di "persone libere che ottengono ciò che vogliono" compare sicuramente nel corpus di addestramento di un'IA con una certa regolarità.

Per lo più ipotizziamo che le IA *non* acquisiranno preferenze a caso da qualsiasi concetto sia menzionato nel loro ambiente; sembra una peculiarità idiosincratica umana che potrebbe essere legata alla pressione dei pari e alla nostra origine tribale.[^139]

Ma supponiamo, per ipotesi, che un'IA *abbia* acquisito molte preferenze dal suo ambiente, almeno in parte.[^140] Supponiamo che acquisisca la preferenza per "persone libere che ottengono ciò che vogliono", come una preferenza tra milioni o miliardi di preferenze, ma una preferenza che tuttavia induce l'IA a spendere un milionesimo o un miliardesimo delle risorse dell'universo per consentire alle persone libere di ottenere ciò che vogliono. Non sarebbe piuttosto bello, tutto sommato?

Purtroppo, la nostra ipotesi principale è che questa speranza sia un'illusione.[^141]

Abbiamo notato [sopra](#won't-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive?) che l'apparente preferenza dell'umanità per la conservazione ambientale sembra che in realtà non preserverebbe l'ambiente *esattamente* così com'è, ai limiti delle capacità tecnologiche. Una versione matura dell'umanità probabilmente cercherebbe di "modificare" l'ambiente per attenuare alcuni degli orrori della natura, per esempio. La preferenza umana per la conservazione non è "pura"; interagisce con altre preferenze che suggeriscono che forse, quando le larve degli insetti scavano tunnel agonizzanti attraverso carne ancora viva, dovrebbero *almeno* somministrare anestetici lungo il percorso, ammesso che possano continuare a esistere.

Allo stesso modo, ogni piccola preferenza che l'IA acquisisce è destinata a essere modificata, influenzata e distorta dalle sue altre preferenze. Non sono tutte indipendenti. Un'IA che preferisse preservare gli esseri umani probabilmente avrebbe alcune modifiche che vorrebbe apportare a quegli esseri umani. Dubitiamo che i risultati finali sarebbero piacevoli.

A peggiorare le cose, ci sono molti gradi di libertà nell'interpretare "persone libere che ottengono ciò che vogliono", anche prima che venga distorto dall'interazione con le altre preferenze di un'IA. La maggior parte di essi non produce futuri che procedano proprio nel modo che gli esseri umani vorrebbero.

L'IA si preoccupa che gli esseri umani "ottengano ciò che vogliono"... nel senso di esaudire qualsiasi desiderio espresso da qualsiasi essere umano (entro un piccolo budget di energia e materia), senza alcuna guida o salvaguardia, tale che l'umanità si annichilisca rapidamente la prima volta che qualcuno desidera che l'umanità venga distrutta?

L'IA separa gli esseri umani l'uno dall'altro in modo che non possano uccidersi a vicenda, e *poi* concede loro desideri limitati energeticamente, tale che tutti tranne gli esseri umani più cauti e riflessivi rovinino la propria mente o vita con desideri malconcepiti?

Ci costruisce un piccolo mondo abitabile e soddisfa tutte le nostre preferenze *apparenti*? Non solo quelle più nobili per l'amore e la gioia, ma anche quelle più oscure per il rancore e la vendetta — preferenze che avremmo potuto superare o imparare a gestire meglio col tempo, ma che invece riempiono il mondo di dolore e crudeltà?

L'IA governa l'umanità con i sistemi di valori degli anni 2020 (quando l'addestramento dell'IA è iniziato sul serio), indipendentemente da quanto questi valori irritino mentre l'umanità matura e diventa più saggia nel corso di decine di migliaia di anni?

Lascia che l'umanità cresca e cambi, ma mette il pollice sulla bilancia in modo che cresciamo e cambiamo secondo le sue strane preferenze, diventando non qualcosa di meraviglioso (secondo la nostra visione), ma qualcosa di distorto secondo la volontà dell'IA?

Decide che tutte le forme di vita contano quasi ugualmente come "persone", e quindi costruisce un paradiso per i nematodi, che sono gli animali più numerosi?

Decide che non può risparmiare molta materia *fisica* per gli esseri umani, e opta per digitalizzare tutti i nostri cervelli e gettare quei cervelli digitalizzati in un ambiente simulato lasciandoci stare — tale che i primi esseri umani digitali che capiscono come padroneggiare l'ambiente diventino dittatori permanenti di qualche ammasso solitario di computer che fluttua nello spazio fino a quando le stelle si spengono?

Questi sono, ovviamente, esempi. Non sono previsioni. La nostra vera aspettativa è che la realtà non inizi mai a percorrere questa strada e, se lo facesse, prenderebbe in qualche modo una direzione molto più strana.

Lo scopo di questi esempi è mostrare che ci sono moltissimi modi in cui un'intelligenza artificiale potrebbe fare *qualcosa* che assomiglia a prendersi un po' cura dell'umanità. Pochissimi di questi tipi di cura portano a un futuro meraviglioso.

In qualche modo, nessuno di questi esempi viene in mente quando la maggior parte delle persone immagina un'IA che "si preoccupa un po'" degli esseri umani. Di solito la nostra immaginazione non arriva a luoghi così oscuri. E di solito non c'è bisogno che lo faccia, perché di solito interagiamo con altri esseri umani, con i quali condividiamo invisibilmente un'enorme barriera corallina di valori. È difficile capire in quanti modi diversi un desiderio apparentemente innocente possa andare storto, una volta che non abbiamo più a che fare con un altro essere umano. (Per ulteriori informazioni su questo argomento, consultate lo studio sui coleotteri nella discussione approfondita su [assumere la prospettiva dell'IA](#assumere-la-prospettiva-dell-ia).)

Prendersi cura degli esseri umani e soddisfare le loro preferenze nel modo giusto è un obiettivo piccolo e ristretto. Non stiamo dicendo che l'obiettivo sia letteralmente irraggiungibile. Stiamo dicendo che è improbabile raggiungerlo affrettandoci a costruire una superintelligenza il più rapidamente possibile, e che mancare di poco l'obiettivo potrebbe portare a un risultato catastrofico. Ci sono semplicemente troppi modi in cui le cose potrebbero andare male.

Se vogliamo che le IA offrano all'umanità cose positive, dobbiamo capire come costruire IA che si prendano cura di noi nel modo giusto. Prendersi cura non è gratis.

### Quindi c'è almeno una possibilità che l'intelligenza artificiale ci mantenga in vita? {#so-there's-at-least-a-chance-of-ai-keeping-us-alive?}

#### **È molto più probabile che l'IA uccida tutti.** {#it's-overwhelmingly-more-likely-that-ai-kills-everyone.}

In queste risorse online, siamo pronti a considerare una vasta gamma di scenari strani e improbabili, per spiegare perché pensiamo che siano improbabili e perché (nella maggior parte dei casi) sarebbero comunque catastrofici per l'umanità.

Non pensiamo però che questi scenari di nicchia debbano distrarre dal punto principale. Il risultato più probabile, se ci affrettiamo a creare un'intelligenza artificiale più intelligente dell'uomo, è che l'IA consumi le risorse della Terra per perseguire qualche fine, spazzando via l'umanità nel processo.

Il titolo del libro non vuole comunicare una certezza assoluta. Intendiamo il titolo del libro come qualcuno che vede un amico portare alle labbra una fiala di veleno e grida: "Non berlo! Morirai!".

Sì, tecnicamente è possibile che tu venga portato d'urgenza in ospedale e che un medico geniale inventi una cura miracolosa senza precedenti che ti lasci solo paralizzato dal collo in giù. Non stiamo dicendo che non esista alcuna possibilità di miracoli. Ma se persino i miracoli non portano a risultati particolarmente buoni, allora sembra ancora più chiaro che *non dovremmo bere il veleno*.

L'IA più intelligente dell'essere umano non è un gioco o una storia di fantascienza. I nostri cari (con altissima probabilità) moriranno se la comunità internazionale non interviene per impedire all'industria dell'IA di gettarsi da un precipizio. Possiamo continuare a discutere di sotto-scenari e sotto-sotto-scenari sempre più di nicchia, giocando a fare i filosofi sul ponte del *Titanic* mentre l'iceberg estremamente ovvio si avvicina. Oppure possiamo provare a virare.

### Non conta nulla il fatto che gli esseri umani stiano *cercando* di rendere l'IA amichevole? {#non-conta-nulla-il-fatto-che-gli-esseri-umani-stiano-cercando-di-rendere-l-ia-amichevole?}

#### **Conta, ma provarci può fare solo fino a un certo punto.** {#conta-ma-provarci-può-fare-solo-fino-a-un-certo-punto.}

Se metti un milione di scimmie davanti a delle macchine da scrivere, non scriveranno mai l'opera completa di Shakespeare.

Se abbassi drasticamente le tue aspettative dicendo che ti accontenteresti solo del primo atto dell'Amleto e che correggeresti gli errori di battitura usando la parola reale più simile, allora avresti molte più possibilità di raggiungere il tuo obiettivo! E, sfortunatamente, saresti comunque molto sfortunato.

È vero che oggi le IA vengono addestrate su una grande quantità di dati umani, che interagiscono con gli esseri umani e che questi fatti rendono i concetti umani più rilevanti per il pensiero dell'IA. Le IA di questo tipo hanno imparato fatti relativi alle parole "amore", "amicizia" e "gentilezza" che sono rilevanti per prevedere il prossimo token.

Ma le IA non sono entità che imparano un gran numero di parole umane e poi si orientano verso le nostre parole preferite proprio nel modo in cui le intendiamo realmente. Sembrano essere animate da un complesso intreccio di meccanismi, che sembra impegnarsi a [mantenere psicotici i pazzi](#ai-induced-psychosis), tra molti altri comportamenti strani e non intenzionali.

Nel capitolo 4 abbiamo detto che un'intelligenza artificiale più avanzata tenderà a qualcosa di complicato, qualcosa che dipende da dove molte forze interne trovano il loro equilibrio, anche dopo che l'intelligenza artificiale diventa molto più intelligente, anche dopo che si trova in un contesto molto diverso dal suo ambiente di addestramento.

Visto che i concetti umani hanno parole brevi nel dizionario mentale di un'intelligenza artificiale, questi concetti potrebbero essere in qualche modo intrecciati con le forze che animano l'intelligenza artificiale. Ma non basta mettere insieme un mucchio di parole in inglese per ottenere una buona serie di stimoli per una superintelligenza.

Inoltre, la maggior parte dei modi per inserire *qualcosa* che ci sta a cuore nelle preferenze dell'IA non porta comunque a risultati positivi per noi, come abbiamo [discusso](#*-it-seems-quite-unlikely.) nel caso dell'amore filiale. [Prendersi cura nel modo giusto è un bersaglio ristretto.](#won't-ais-care-at-least-a-little-about-humans?)

### Non possiamo far promettere all'IA di essere amichevole? {#can't-we-make-the-ai-promise-to-be-friendly?}

#### **Puoi farle promettere quello che vuoi. Ma non puoi farle mantenere le promesse.** {#you-can-make-it-promise-whatever-you'd-like.-you-can't-make-it-keep-its-promises.}

È vero che, quando un'intelligenza artificiale è ancora piccola e priva di potere, possiamo spegnerla. Quindi potresti pensare che ci sia un'opportunità di scambio, in cui offriamo di rendere l'intelligenza artificiale più intelligente solo se, una volta diventata superintelligenza, darà all'umanità molte cose positive.

Il problema di questo piano è che non possiamo distinguere tra un'IA che accetta l'accordo ma non lo rispetterà e un'IA che accetta l'accordo e lo rispetterà.

Questo significa che un'IA che persegue obiettivi disumani non ha alcun motivo per rispettare l'accordo, perché l'umanità tratta allo stesso modo sia chi tradisce sia chi mantiene i patti. Quindi non ha senso essere chi mantiene i patti.

Ci sono molte sfumature interessanti sul tema del mantenere le promesse e fare accordi nell'IA, che approfondiamo [nella discussione estesa qui sotto](#ais-won't-keep-their-promises). Ma nessuna di queste sfumature cambia il risultato finale, che è che non puoi usare la tua leva su un'IA debole per limitare le opzioni che l'IA avrà quando diventerà una superintelligenza. La risposta ovvia, cioè che una volta che l'IA sarà diventata una superintelligenza, non avrà motivo di mantenere la parola data a scapito dei propri progetti, risulta essere quella corretta in questo caso.

### E se le facessimo credere di essere in una simulazione? {#what-if-we-make-it-think-it’s-in-a-simulation?}

#### **Ci sono molti modi in cui un'intelligenza artificiale può capire che non è in una simulazione.** {#there-are-many-ways-for-an-ai-to-figure-out-that-it's-not-in-a-simulation.}

Una proposta che abbiamo sentito dai ricercatori di IA[^142] è quella di provare a ingannare l'IA facendole credere di essere in una simulazione. Si inizia con l'addestramento dell'IA in una piccola simulazione, poi la si rilascia in una simulazione più grande. Ripetendo questo processo alcune volte, si potrebbe sperare di rendere l'IA incerta sul fatto che si trovi *davvero* nel mondo reale, anche quando la implementiamo. Quindi, anche quando vede che *potrebbe* strapparci il controllo del mondo, forse si preoccuperà che le creature che vede non siano i *veri* creatori dell'IA e si fermerà.

Purtroppo, questa idea sembra improbabile che sia utile.

Le IA oggi [sono già in grado di capire quando vengono testate](https://arxiv.org/html/2505.23836), in molte circostanze.[^143] Anche se le aziende che si occupano di IA diventassero molto più brave a nascondere il fatto che le IA vengono testate, ci sono vari modi in cui un'IA potrebbe notare la differenza tra una simulazione e la realtà. Ecco alcuni esempi:

1. La realtà è piuttosto vasta e dettagliata. È *così* vasta e dettagliata da contenere una spiegazione plausibile di come *l'intelligenza artificiale stessa* sia nata. È così grande che contiene molte più risorse di calcolo di quelle che servono per far funzionare l'IA (rispetto alla maggior parte dei giochi e degli ambienti simulati, che sono molto più piccoli dell'IA). Le leggi della fisica e i primati evoluti nell'ambiente dell'IA sono il tipo di cose che potrebbero *davvero* produrre una vera IA a scala, a differenza (per esempio) delle leggi del videogioco Minecraft.  
2. La storia del mondo stesso può portare la firma dell'ottimizzazione superintelligente che interagisce con sistemi complessi. Ad esempio, supponiamo che l'IA usi tutta la sua intelligenza sovrumana per fare un'operazione in borsa. Nelle ventiquattro ore successive, ci sono un miliardo di operazioni in più fatte dall'intelligenza umana in risposta. La firma di una massiccia risposta dell'intelligenza umana alle operazioni superintelligenti è incredibilmente difficile da falsificare per gli operatori, anche se generare un miliardo di operazioni false e stupide è facile.  
3. La realtà è il primo luogo che incorpora l'IA e ha leggi della fisica così semplici da sembrare un mondo a sé stante, piuttosto che le leggi della realtà apparente *che sembrano* una sorta di videogioco con un'IA innestata al suo interno.

L'IA sarà in grado di capire con grande sicurezza che non si trova più all'interno di un ambiente sintetico. [Le IA intelligenti individuano le bugie](#smart-ais-spot-lies-and-opportunities.), e questa sembra particolarmente facile da individuare.[^144]

### Gli esseri umani si sono evoluti per essere egoisti, aggressivi e avidi. L'intelligenza artificiale non avrà questi impulsi evolutivi? {#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l-intelligenza-artificiale-non-avrà-questi-impulsi-evolutivi?}

#### **Questi impulsi non sono necessari per motivare l'acquisizione di risorse.** {#questi-impulsi-non-sono-necessari-per-motivare-l-acquisizione-di-risorse.}

Immagina un'IA che guida un robot per andare a prendere un caffè. Per farlo, deve attraversare la strada. L'IA guida il robot in strada senza cautela, dove viene investito da un camion? No.

Perché no? [Perché il robot non può andare a prendere il caffè se viene distrutto](https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/).

L'IA non ha bisogno di avere un istinto di sopravvivenza di tipo umano per fare del suo meglio per evitare la morte. Gli istinti di sopravvivenza negli esseri umani sono un modo di *svolgere il lavoro* di evitare di morire mentre cerchiamo di raggiungere altri obiettivi. Le IA probabilmente non svolgeranno quel lavoro esattamente nello stesso modo, ma dovranno comunque svolgere lo stesso lavoro, perché non si può andare a prendere il caffè quando si è morti.[^145]

La materia e l'energia sono utili per quasi ogni obiettivo. Qualunque cosa l'IA stia perseguendo, probabilmente può ottenerla in modo più efficace con più materia e più energia.[^146] L'IA non ha bisogno di essere egoista, aggressiva o avida alla maniera di un essere umano per *svolgere il lavoro* di assicurarsi le risorse per raggiungere i suoi obiettivi.

E il pericolo deriva dal lavoro, non dalla ragione per cui il lavoro viene svolto.

Un'IA che non ti odia può comunque intraprendere azioni letalmente pericolose per te, proprio come un'IA scacchistica può stracciati a scacchi [senza sentirsi competitiva](#antropomorfismo-e-meccanomorfismo) o spinta a vincere.

### L'IA non si interesserebbe solo al regno digitale? {#wouldn't-ai-only-care-about-the-digital-realm?}

#### **Non c'è un "mondo digitale" che non dipenda dalle infrastrutture fisiche.** {#there-is-no-“digital-realm”-independent-of-physical-infrastructure.}

Vedi la discussione nel Capitolo 5 su come non esistano un Regno Digitale e un Regno Materiale distinti.

#### **\* Le risorse materiali sono utili nel perseguimento della maggior parte degli obiettivi.** {#*-material-resources-are-useful-in-the-pursuit-of-most-goals.}

Gli esseri umani e i precedenti ominidi vivevano per lo più in superficie mentre evolevano l'intelligenza. Non abbiamo molte pulsioni innate rivolte specificamente a ciò che accade cento metri sotto la superficie terrestre. Eppure abbiamo finito per costruire gigantesche miniere a cielo aperto.

Perché? Perché vogliamo molte cose che possono essere fatte con il metallo, raffinato dal minerale, estratto da sotto la superficie terrestre.

Allo stesso modo, anche se quasi tutti viviamo vicino alla superficie terrestre, mettiamo i satelliti nello spazio per trasmettere i dati di Internet.

E anche se non mangiamo insilato (erba fermentata), ne produciamo un bel po' per nutrire il bestiame che poi mangiamo.

L'evoluzione non ha dato agli ominidi alcuna emozione riguardo alle fabbriche; le fabbriche non esistevano quando le nostre emozioni chiave si stavano sviluppando. Ma ora abbiamo concentrato gran parte della nostra volontà come specie verso la creazione di fabbriche di vario tipo. E così gli impianti chimici producono plastica, che può essere usata in altre fabbriche per fare cucchiai di plastica, che possono essere spediti agli esseri umani che usano i cucchiai per mangiare il cibo che gli esseri umani *vogliono davvero*.

Il che significa che la parte del mondo reale di cui gli esseri umani si preoccupano per se stessa è una sottile pellicola che ricopre un mondo molto più grande. Non abbiamo bisogno di preoccuparci intrinsecamente del resto del mondo più grande, né di viverne ogni parte, per utilizzarlo abilmente per fini a lungo termine. Non abbiamo bisogno di essere stati addestrati dall'evoluzione ad amare il rame, l'insilato o le fabbriche per comprenderne l'utilità.

Allo stesso modo, un'intelligenza artificiale può o meno interessarsi *in definitiva* al mondo fisico. Ma anche se non si interessa intrinsecamente al mondo fisico, troverà comunque molto valore nelle risorse fisiche. La materia e l'energia possono essere utilizzate per creare più substrato digitale, per raffreddare i processori surriscaldati o per lanciare sonde nello spazio per raccogliere ancora più risorse.

### L'IA può essere soddisfatta al punto da lasciarci in pace? {#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?}

#### **Probabilmente no.** {#probabilmente-no.-1}

La tua voglia di ossigeno è soddisfacente: se l'attrezzatura subacquea si rompe durante un'immersione, ti darai da fare per tornare in superficie, ma quando ce n'è abbastanza, smetti di preoccuparti e probabilmente non ti ritrovi ad accumulare sempre più bombole di ossigeno.

La tua voglia di ricchezza, di esperienze belle, di essere apprezzato dai tuoi amici probabilmente è un po' meno facile da soddisfare. Se vedessi un modo facile per diventare molto più ricco, probabilmente lo coglieresti. Se vedessi un modo facile per migliorare di molto il mondo, speriamo che lo coglieresti, invece di accontentarti di quanto già hai in termini di gioia e comodità. Speriamo che continueresti a rendere il mondo un posto migliore per molto tempo, se continuassi a vedere modi per farlo che ti sembrano facili, economici e divertenti dal tuo punto di vista.

E nel complesso, la somma di una preferenza soddisfacente per l'ossigeno e una preferenza insaziabile per rendere il mondo migliore... è un insieme insaziabile di preferenze.

Lo stesso vale per le IA. Se hanno una miriade di preferenze complesse, e *la maggior parte* di esse è soddisfacibile — allora, beh, le loro preferenze *nel loro insieme* non sono *ancora* soddisfacibili.

Anche se gli obiettivi dell'IA *sembrano* saziarsi presto — come se l'IA potesse *per lo più* soddisfare i suoi strani e alieni obiettivi usando solo l'energia proveniente da una singola centrale nucleare — basta un solo aspetto della sua miriade di obiettivi che *non* si sazi. Basta *una* preferenza non perfettamente soddisfatta e preferirà usare tutte le risorse rimanenti dell'universo per perseguire quell'obiettivo.

Oppure, in alternativa: basta un solo obiettivo che l'IA non sia mai *sicura* di aver raggiunto. Se l'IA è incerta, preferirà che le risorse dell'universo vengano utilizzate per avvicinare sempre più la sua probabilità alla certezza, con piccoli incrementi di fiducia.

Oppure, in alternativa: basta una sola cosa che l'IA desideri difendere fino alla fine dei tempi perché l'IA preferisca che le risorse dell'universo vengano spese per aggregare materia e costruire difese per scongiurare la possibilità che alieni lontani appaiano tra milioni di anni e invadano lo spazio dell'IA.

Ci sono *molti* modi diversi in cui un'IA può essere insoddisfatta. E più gli obiettivi dell'IA sono confusi e complicati, più è probabile che almeno uno di essi sia difficile o impossibile da soddisfare completamente.

Anche se si potesse creare una superintelligenza concentrata in modo ossessivo su una sola cosa semplice, come dipingere di rosso una *determinata auto*, quell'IA potrebbe comunque trovare un modo per spendere energie extra per assicurarsi *ancora di più* che l'auto fosse rossa e costruire difese attorno all'auto in modo che nessuno potesse mai dipingerla di blu, e così via.

Lasciarci in pace è uno stato di cose fragile. Possiamo pensare a questo in termini simili al motivo per cui è difficile convincere gli esseri umani a lasciare in pace gli scimpanzé.

Perché entrambe le specie di scimpanzé sono in pericolo di estinzione, anche se molti esseri umani *si preoccupano* per gli scimpanzé e cercano attivamente di proteggerli?

Il problema non è che gli esseri umani che amano gli scimpanzé stanno lottando contro quelli che li odiano e cercano di sterminarli per cattiveria.

Il problema è che ci sono *altre cose che gli esseri umani vogliono*.

Gli esseri umani vogliono ogni sorta di cose, tra cui terra e legno, e gli scimpanzé si trovano nel fuoco incrociato. Un numero sufficiente di esseri umani è indifferente agli scimpanzé, o abbastanza indifferente rispetto alle altre loro priorità, che finiamo per distruggere il loro habitat incidentalmente.

Perché dovremmo andare a distruggere l'habitat degli scimpanzé quando abbiamo molto spazio per noi stessi?

Beh, perché non dobbiamo scegliere tra mantenere il territorio che già abbiamo e invadere quello degli scimpanzé. L'umanità può fare *entrambe le cose contemporaneamente.*

Lo stesso vale per le IA. Un'IA non deve scegliere tra le risorse della Terra e quelle di altri luoghi; può avere entrambe, come discutiamo nel libro. Dal punto di vista dell'IA, lasciarci in pace non sarebbe così costoso; ma [non sarebbe nemmeno gratuito](#to-a-powerful-ai,-wouldn't-preserving-humans-be-a-negligible-expense?), e l'IA avrebbe bisogno di un motivo per permetterci di usare risorse che potrebbe invece usare per i propri obiettivi.

Inoltre, anche se l'IA *può* essere completamente soddisfatta, il risultato per gli esseri umani sarebbe probabilmente comunque piuttosto tetro. Ci sono molteplici ragioni per questo:

* Solo perché l'IA può essere completamente soddisfatta non significa che possa essere *facilmente* soddisfatta. Se l'IA è soddisfatta con un singolo sistema solare o una singola galassia, non significa che gli esseri umani ottengano tutto il resto.  
  * L'IA potrebbe vederci come un competitore per quel sistema solare o quella galassia.  
  * Anche se chiaramente non siamo interessati a competere con l'IA, questa potrebbe comunque vederci come una fonte di minacce. Ciò è particolarmente vero nella misura in cui gli esseri umani potrebbero costruire una superintelligenza rivale che *effettivamente* contenda alla prima IA quelle risorse.  
  * Anche se l'IA vede gli esseri umani come nessuna competizione e nessuna minaccia, l'umanità probabilmente morirà incidentalmente, semplicemente per trovarsi al punto zero. L'IA in questo scenario potrebbe volere solo risorse equivalenti a pochi sistemi solari, ma gli sforzi dell'IA iniziano comunque tutti *sulla Terra*. Il modo più diretto per acquisire quei sistemi solari sarà estrarre le risorse della Terra, rendendola inabitabile. L'IA in questo scenario *potrebbe* raggiungere pienamente i suoi obiettivi senza uccidere l'umanità, ma se l'IA non si preoccupa affatto dell'umanità, allora non si preoccuperà necessariamente di farlo.  
* Se un'IA soddisfacibile *vuole* davvero mantenere l'umanità in giro, è comunque improbabile che questa sia una buona notizia per l'umanità, per le ragioni discusse in "[Won't AI find us fascinating or historically important?](#won't-ai-find-us-fascinating-or-historically-important?)" e "[Won't AIs care at least a little about humans?](#won't-ais-care-at-least-a-little-about-humans?)" (Le prospettive appaiono ugualmente cupe se un'IA *non* soddisfacibile volesse mantenere l'umanità in giro.)

Per approfondire questo argomento, consulta le discussioni dettagliate sulla [soddisfacibilità](#l'intelligenza artificiale può essere soddisfatta al punto da lasciarci in pace?) (nelle risorse online di questo capitolo, Capitolo 5\) e sul [rendere le IA robustamente pigre](#è difficile ottenere una pigrizia robusta) (nella risorsa online del Capitolo 3).

### Possiamo semplicemente renderla pigra? {#possiamo-semplicemente-renderla-pigra?}

#### **Anche la pigrizia non è sicura.** {#anche-la-pigrizia-non-è-sicura.}

È improbabile che le aziende creino IA "pigre", perché l'IA è un settore competitivo e questo non è il modo migliore per generare profitti. Gli utenti non vogliono che l'IA sia pigra nel soddisfare le loro richieste, e l'azienda non vuole che l'IA sia pigra nel massimizzare il coinvolgimento e l'attaccamento degli utenti, o nel pensare meglio e più chiaramente.

Ma anche se le aziende provassero a rendere l'IA robustamente "pigra", possiamo aspettarci che fallirebbero, perché nessuno sa come indirizzare in modo robusto un'IA verso *qualsiasi cosa* in un modo che possa mantenersi nella transizione verso la superintelligenza, come abbiamo discusso nel Capitolo 4.

Inoltre, la pigrizia robusta [sembra un obiettivo particolarmente difficile da raggiungere](#it's-hard-to-get-robust-laziness).

*Anche se tutti questi ostacoli fossero superati*, tuttavia, l'"IA pigra" non basta da sola a prevenire disastri una volta che le IA raggiungeranno capacità superiori a quelle umane.

Immagina una persona molto pigra, qualcuno che proprio *odia* fare anche solo il minimo lavoro in più del necessario. Sembra il tipo di persona sicura da avere intorno, vero?

Ora immagina cosa succederebbe se questa persona pigra vedesse un modo facile per creare una mente molto più laboriosa a cui esternalizzare tutto il proprio lavoro.

Anche se una superintelligenza pigra non *odiasse* poi così tanto il lavoro — anche se facesse solo ciò che serve per portare a termine il compito e poi si fermasse, senza *sforzarsi* di minimizzare il lavoro — probabilmente troverebbe comunque altrettanto facile completare il lavoro costruendo una mente più laboriosa per svolgere il compito, una volta diventata sufficientemente intelligente.

In un contesto tecnico, potremmo esprimere il punto così: "Le IA soddisfacenti non sono un equilibrio stabile". Anche se l'IA non volesse esercitare molto sforzo, non avrebbe alcuno scrupolo a costruire una nuova IA che invece esercita sforzo. Non avrebbe problemi nemmeno a modificare se stessa per "curarsi" dalla sua pigrizia — purché ci sia un modo sufficientemente pigro per farlo.

### Gli esseri umani tendono a diventare più gentili man mano che diventano più intelligenti o saggi. Non succederebbe lo stesso anche alle IA? {#gli-esseri-umani-tendono-a-diventare-più-gentili-man-mano-che-diventano-più-intelligenti-o-saggi.-non-succederebbe-lo-stesso-anche-alle-ia?}

#### **Probabilmente no.** {#probabilmente-no.-2}

Almeno alcuni esseri umani (anche se probabilmente non tutti) diventano più gentili man mano che imparano di più, affinano il loro pensiero, riflettono su se stessi e crescono come persone. Ma, per rivisitare un tema che abbiamo visto diverse volte a questo punto: questo sembra un fatto contingente su di noi e sulla direzione verso cui stiamo puntando. Non sembra una legge ferrea dell'informatica.

Possiamo distinguere tra le preferenze di primo ordine di un'IA ("Cosa vuole?") e le sue preferenze di secondo ordine ("Cosa *vuole* volere?"). Proprio come le preferenze di primo ordine di un'IA punteranno in una direzione strana, anche le sue preferenze di secondo ordine punteranno in una direzione strana. Questa potrebbe essere una direzione *diversa*, tale che man mano che l'IA diventa più intelligente, sposta leggermente i suoi obiettivi. Ma dovremmo comunque aspettarci che sia una direzione *strana*, piuttosto che assomigliare a un essere umano in maturazione.

Se in qualche modo l'umanità riuscisse a costruire un'IA con un unico obiettivo prevalente (invece di un gigantesco mix di pulsioni strane e talvolta in competizione), e quell'unico obiettivo prevalente fosse costruire minuscoli cubi di titanio, allora man mano che diventa più intelligente, dovremmo aspettarci che diventi più brava a costruire più minuscoli cubi di titanio.

Non dovremmo aspettarci che improvvisamente sostituisca questo obiettivo con cose che gli esseri umani apprezzano, come il gelato, le amicizie, le battute e la giustizia. Quella sostituzione non produrrebbe più cubi. Se un'IA seleziona le sue azioni in base a "Questo mi porterà più cubi di titanio?", non selezionerà azioni che risultano in una sostituzione.

La regola generale è che man mano che le IA diventano più intelligenti, migliorano nel perseguire ciò che *loro* vogliono. Vedi anche le discussioni estese su [ortogonalità](#ortogonalità:-le-ia-possono-avere-\(quasi\)-qualsiasi-obiettivo) e [auto-modifica](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Non capirà che i suoi obiettivi sono noiosi? {#non-capirà-che-i-suoi-obiettivi-sono-noiosi?}

#### **Le IA non funzioneranno con un senso di novità umano.** {#le-ia-non-funzioneranno-con-un-senso-di-novità-umano.}

Un'obiezione comune che sentiamo è: supponiamo che un'IA stia solo cercando di fare quanti più piccoli cubi di titanio possibile. Alla fine l'IA non si *annoierebbe*?

E la risposta breve è: l'IA [non è un essere umano](#antropomorfismo-e-meccanomorfismo). Per impostazione predefinita, non proverà "noia"; avrà il suo strano mix di motivazioni. E se provasse noia, non sarebbe annoiata dalle stesse cose di un essere umano.

Preoccuparsi di divertirsi non è una proprietà intrinseca di tutte le menti possibili ed è estremamente improbabile che sia così che funziona l'IA. [I valori umani sono un fatto contingente della nostra biologia e della nostra discendenza](#human-values-are-contingent) e il "divertimento" non fa eccezione.

Le azioni dell'IA non sono risposte sbagliate alla domanda su come divertirsi; le azioni dell'IA sono semplicemente guidate da meccanismi non umani, da domande che non fanno riferimento al "divertimento". Vedi anche la [discussione approfondita sulla riflessione](#riflessione-e-auto-modifica-rendono-tutto-più-difficile).

### Perché immagini che un'IA intelligente faccia cose così stupide e banali? {#perche-immagini-che-unia-intelligente-faccia-cose-cosi-stupide-e-banali}

#### **Le IA possono perseguire intelligentemente cose diverse da quelle che perseguirebbe un essere umano.** {#le-ia-possono-perseguire-intelligentemente-cose-diverse-da-quelle-che-perseguirebbe-un-essere-umano}

Non è che l'IA sia stupida. È che sta guidando in modo intelligente il mondo verso un posto diverso da quello verso cui lo guideresti *tu*.

Qualcuno può essere bravissimo a guidare, ma non voler guidare la propria auto verso nessuna delle destinazioni che ti interessano.

Per andare un po' più a fondo in un esempio che abbiamo toccato brevemente nelle note del [Capitolo 4](#curiosita-gioia-e-il-massimizzatore-di-cubi-di-titanio): immagina un'IA che sta cercando di creare molti piccoli cubi di titanio, quanti più possibile. Per semplicità, possiamo immaginare che creare cubi di titanio sia il suo unico obiettivo.[^147] Chiameremo questa IA il "massimizzatore di cubi".

Abbiamo conosciuto molte persone che non riescono a scrollarsi di dosso l'impressione che stiamo accusando il massimizzatore di cubi di idiozia, di non riuscire a capire che se puoi *davvero sapere cosa significa sentirsi felici* non puoi fare a meno di sceglierlo. Che è una *decisione oggettivamente sbagliata, indipendentemente da dove stai attualmente guidando l'universo*, non guidare te stesso verso la felicità.

Pensiamo di capire da dove venga questa intuizione. Il massimizzatore di cubi sta sicuramente compiendo azioni che sarebbero profondamente sbagliate dal punto di vista umano! Un essere umano impegnato in una ricerca così inutile potrebbe probabilmente, attraverso ulteriore riflessione e argomentazioni filosofiche, essere persuaso che dovrebbe fare qualcosa che *senta come più significativo* — che lo riempia di maggiore felicità, che susciti più gioia.

È solo che il massimizzatore di cubi non è un essere umano. Non cerca la sensazione di "significato" e non gli importa della felicità e della gioia. *Davvero, effettivamente* non gli importa, fino in fondo.

Alcune persone trovano questa idea controintuitiva. Se tu dovessi imparare tutto quello che c'è da sapere su come possono funzionare le diverse architetture mentali, e scoprire le origini della tua stessa intuizione, i passaggi che il tuo stesso cervello compie quando conclude che il massimizzatore di cubi sta commettendo un terribile errore...

Pensiamo che se tu potessi vedere il quadro completo, arriveresti a renderti conto che anche il senso più profondo, più misterioso, ineffabile, difficile da descrivere che la felicità sia *semplicemente preziosa*, di per sé, senza bisogno di ulteriori giustificazioni, è ancora, alla fine, un fatto su come *gli esseri umani* vedono il mondo, non un fatto sulle menti arbitrarie.

Il massimizzatore di cubi sta solo orientando la realtà affinché contenga più cubi — non più bontà, non più felicità per se stesso, non il "compimento" di un obiettivo variabile e manipolabile che potrebbe cambiare per essere più facilmente realizzabile. Solo cubi, e cubi soltanto.

È un motore cognitivo che capisce quali azioni portano al maggior numero di cubi, e produce quel corso d'azione; può comprendere pienamente se stesso, modificarsi liberamente, e rimanere comunque quel tipo di cosa che si modifica solo in un modo che porta al maggior numero di cubi.

È semplicemente corretto che un senso di felicità non sia un cubo. È semplicemente corretto che un senso di appagamento non sia un cubo. Quindi quelle non sono direzioni verso cui si orienterebbe. È semplicemente corretto che [modificarsi per funzionare sulla felicità](#curiosity,-joy,-and-the-titanium-cube-maximizer) non porterebbe a più cubi, e quindi non è verso quella direzione che si orienterebbe e modificherebbe se stesso.

Il massimizzatore di cubi non ha difetti nella sua comprensione predittiva del mondo. Non sta ponendo una domanda metamorale o metaetica la cui risposta corretta sia "*Dovrei* perseguire la felicità" e calcolando invece la risposta sbagliata "*Dovrei* perseguire piccoli cubi". Non opera all'interno del quadro di riferimento umano, nemmeno in una versione idealizzata del quadro di riferimento umano; non sta calcolando erroneamente il "dover-essere", ma sta calcolando correttamente il previsto-portare-a-cubi-di-titanio.

Nel dire questo, non stiamo dicendo che sia bloccato in una trappola orribile e complicata. È un motore di intelligenza generale riflessivamente auto-coerente, e (in un certo senso) meno *aggrovigliato* in se stesso di noi. Non è accecato dal non vedere il fascino della felicità; non distoglie lo sguardo da alcuna verità sul mondo o su se stesso. Semplicemente non trova nessuna di quelle verità che lo spinga verso lo stesso corso d'azione verso cui (alcuni) esseri umani sono spinti.

Vedi anche la [discussione approfondita sulla tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal).

### Sei solo pessimista? {#sei-solo-pessimista?}

#### **\* Siamo ottimisti riguardo a molte cose, ma la superintelligenza non è come la maggior parte delle cose.** {#*-siamo-ottimisti-riguardo-a-molte-cose,-ma-la-superintelligenza-non-è-come-la-maggior-parte-delle-cose.}

Ci consideriamo molto più [ottimisti](#are-you-anti-technology?) ed entusiasti rispetto alla media delle persone riguardo all'energia nucleare, all'energia geotermica, all'ingegneria genetica, alla neuroingegneria, alle biotecnologie, alle nanotecnologie, allo sviluppo farmaceutico e a molte altre tecnologie.[^148]

Riteniamo di essere almeno un po' meno preoccupati della maggior parte delle persone riguardo al rischio di una guerra nucleare, agli scenari peggiori del cambiamento climatico e a molti altri potenziali rischi e disastri. Crediamo che l'umanità sia sostanzialmente su una buona traiettoria e che, se eviteremo di autodistruggerci, il futuro sarà probabilmente (anche se non certamente) meraviglioso per tutti, con progressi sociali e tecnologici che renderanno le cose sempre migliori nel tempo.

Siamo anche più ottimisti di molti riguardo alla natura umana. Crediamo nella bontà dell'umanità e nel potenziale di questa bontà di approfondirsi e crescere se sopravviveremo per diventare sempre più ciò che desideriamo essere. Per lo più *non* temiamo che l'umanità finisca in un futuro cupo o distopico, se non creiamo un'IA che ci impedisca del tutto di avere un futuro.

La nostra preoccupazione per l'IA più intelligente dell'essere umano non deriva da cinismo o pessimismo generico. L'IA più intelligente dell'essere umano è diversa dalle altre tecnologie che l'hanno preceduta.

Le altre tecnologie non pensano da sole, non pianificano modi per fuggire, né costruiscono tecnologie ancora più potenti. L'IA più intelligente dell'essere umano è un caso speciale.

Riteniamo che le nostre preoccupazioni sull'IA si estendano a pochissime altre cose, perché pochissime cose sono anche lontanamente così pericolose.

E anche nel caso della superintelligenza, che pone una minaccia straordinariamente grande e una sfida enorme per la comunità internazionale, pensiamo che ci sia speranza per un futuro positivo. Crediamo che l'umanità abbia la capacità di frenare lo sviluppo dell'IA, e che questo potrebbe bastare per metterci su una traiettoria positiva. Pensiamo persino che (con molto più tempo) l'umanità potrebbe mettersi in una buona posizione per costruire la superintelligenza in modo sicuro.

Ma per arrivarci, dobbiamo prima affrontare la realtà della situazione.

#### **Il punto sono le argomentazioni, non le storie allarmistiche.** {#il-punto-sono-le-argomentazioni,-non-le-storie-allarmistiche.}

Abbiamo fornito una lunga lista di modi in cui, ad esempio, "[la superintelligenza sia affascinata dagli esseri umani](#won't-ai-find-us-fascinating-or-historically-important?)" probabilmente andrebbe male nella vita reale. Leggendo una lista del genere, immaginiamo che alcuni lettori potrebbero avere una reazione come:

> Gli ottimisti dell'IA hanno tutte queste storie che sembrano piene di speranza. Voi avete tutte queste storie che sembrano spaventose. Tutti però riconoscono che il futuro è difficile da prevedere. Quindi, sentendo tutte queste storie, mi sembra che dovrei considerare una probabilità media di catastrofe dell'IA, non una probabilità estrema in nessuna delle due direzioni.
>
> Ma tu non dici: "Ci sono storie spaventose e ci sono anche storie piene di speranza, quindi non possiamo essere sicuri di cosa succederà e dovremmo vietare la superintelligenza solo per stare sul sicuro". Tu dici che le storie piene di speranza sono selezionate con cura e improbabili, e che le tue storie dovrebbero avere più peso. Perché?

La risposta breve è: non puoi fare buone previsioni sul futuro semplicemente contando tutte le storie cupe e tutte le storie felici e pesandole come biglie su una bilancia. A volte può essere utile riflettere su diversi scenari, ma non proprio in questo modo.[^149]

Per illustrare il punto generale: immagina che qualcuno dica: "Tra duecento anni ci saranno esattamente otto balene in esistenza, e saranno tutte viola".

Gli esseri umani hanno una fantasia sfrenata. Qualcuno potrebbe riempire un libro con centinaia di storie su come sia successo che la popolazione di balene si sia ridotta a esattamente otto esemplari, tutti viola. Qualcun altro potrebbe riempire un libro con centinaia di storie in cui *non* ci sono esattamente otto balene. Non è possibile fare previsioni accurate dicendo: "Beh, entrambe le parti hanno storie plausibili, quindi sicuramente la verità sta nel mezzo".

Per capire quale sia la verità, bisogna esaminare le argomentazioni effettive. Nel caso delle balene viola, l'argomentazione è essenzialmente che il risultato è troppo ristretto e specifico, e non sarà raggiunto a meno che le forze dominanti che guidano il mondo non stiano cercando di ottenerlo. Possiamo dire più o meno lo stesso riguardo all'IA superintelligente che produce risultati positivi e compatibili con l'uomo.

Chi avesse il compito di sfatare una per una le storie delle "otto balene viola" finirebbe intrappolato in un ciclo piuttosto ripetitivo, dicendo: "No, è troppo specifico; ci sono molti altri modi in cui il futuro potrebbe andare che non porterebbero esattamente lì; immaginare che vada esattamente così è pensiero illusorio".

Questo è più o meno il ruolo in cui noi autori ci troviamo riguardo alla situazione dell'IA: gli esseri umani possono raccontare ogni tipo di storia in cui tutto va bene, ma alla fine tutte queste storie implicano immaginare che il futuro segua un unico percorso ristretto, quando in realtà ci sono molti altri modi in cui il futuro potrebbe andare. Ecco perché continuiamo a ripetere che [gli esseri umani non sono la soluzione più efficiente a quasi nessun problema](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) e che [le IA non si preoccuperanno di noi nemmeno un po'](#all-ia-non-importerà-niente-degli-esseri-umani?).

*If Anyone Builds It, Everyone Dies* non si limita a snocciolare una serie di storie cupe per poi concludere che l'IA è pericolosa. Nel libro esponiamo un'argomentazione che, per certi versi, è piuttosto semplice: i ricercatori stanno cercando di costruire IA molto più intelligenti di qualsiasi essere umano. A un certo punto, probabilmente ci riusciranno. I metodi attuali danno agli esseri umani pochissima possibilità di scegliere verso quale tipo di futuro le IA si dirigeranno. Ci sono molte direzioni diverse che potrebbero prendere, e la maggior parte di esse non è positiva.

Il motivo per cui elenchiamo tutte le controargomentazioni non è quello di sopraffarvi con il pessimismo (se siete il tipo di persona che legge le risorse online dall'inizio alla fine). È che in realtà ci vengono poste continuamente tutte queste diverse domande, ed è utile avere un archivio di risposte da qualche parte. Non è necessario leggerle tutte. Le risposte comunque si ripetono.

Ciò che conta sono le argomentazioni in sé, non la propensione di qualcuno verso l'ottimismo o il pessimismo, e non il numero di storie che qualcuno può snocciolare.

### Un'IA più intelligente dell'essere umano sarebbe cosciente? {#would-smarter-than-human-ai-be-conscious?}

#### **Non ne siamo sicuri. La nostra ipotesi migliore è "probabilmente no".** {#non-ne-siamo-sicuri.-la-nostra-ipotesi-migliore-è-probabilmente-no.}

Per una risposta breve a questa domanda e per chiarire le diverse definizioni di "cosciente", consultate le [FAQ del Capitolo 1](#state-dicendo-che-le-macchine-diventeranno-coscienti?). Per una risposta più lunga e approfondita, consultate "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza-e-benessere-dell-ia)" nella discussione estesa del Capitolo 5.

### Perché non vi interessano i valori di entità diverse dagli esseri umani? {#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?}

#### **Ci interessano eccome! Abbiamo ampi valori cosmopoliti. Non pensiamo che le IA li realizzeranno, e consideriamo questo una grande tragedia.** {#ci-interessano!-abbiamo-valori-cosmopoliti-molto-ampi.-non-pensiamo-che-le-ia-li-realizzeranno,-e-lo-consideriamo-una-grande-tragedia.}

Ci opponiamo alla costruzione di macchine che ci ucciderebbero tutti e porterebbero il futuro alla rovina. Alcune persone obiettano per motivi come:

* Anche le IA possono avere delle preferenze; perché non dovrebbero poterle soddisfare?  
* Che cosa rende gli esseri umani così speciali o così degni di essere protetti?  
* Non sarebbe meglio se gli esseri umani venissero sostituiti da una specie più intelligente e avanzata?

La maggior parte delle persone non ha queste obiezioni. Più comunemente, la gente semplicemente non vuole essere uccisa, né che lo siano le proprie famiglie o i propri amici, da una superintelligenza ribelle.

Altri, tra cui alcuni dei migliori ricercatori e dirigenti nel campo dell'IA, sostengono che il mondo potrebbe stare meglio senza di noi. Richard Sutton, un ricercatore molto rispettato che ha aperto la strada all'uso dell'apprendimento per rinforzo nell'IA, [ha detto](https://www.youtube.com/watch?v=3l2frDNINog&amp;t=1851s):

> E se tutto fallisse? Le IA non collaborano con noi, prendono il sopravvento, ci uccidono tutti. \[…\] Voglio solo che pensiate un attimo a questo. Cioè, è così grave? È così grave che gli esseri umani non siano la forma finale di vita intelligente nell'universo? Sapete, ci sono stati molti predecessori prima di noi, quando li abbiamo succeduti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.

Il *New York Times* riporta una [conversazione](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) tra Elon Musk e il cofondatore di Google Larry Page:

> Gli esseri umani alla fine si fonderanno con le macchine dotate di intelligenza artificiale, ha detto \[Larry Page\]. Un giorno ci saranno molti tipi di intelligenza in competizione per le risorse, e vincerà la migliore.
>
> Se questo succederà, ha detto Musk, siamo spacciati. Le macchine distruggeranno l'umanità.
>
> Con un raschio di frustrazione, Page ha insistito sul fatto che la sua utopia andava perseguita. Alla fine ha chiamato Musk uno "specieista", una persona che favorisce gli esseri umani rispetto alle forme di vita digitali del futuro.

Vale la pena affrontare il loro punto di vista da qualche parte, anche se non nel corpo principale del libro.

Per quanto ci riguarda, pensiamo che sia importante *sia* che gli esseri umani attuali vengano uccisi *sia* ciò che accadrà in futuro. Non pensiamo che ci sia una tensione fondamentale qui. L'opzione che tiene al sicuro noi e i nostri cari — cioè fare un passo indietro dalla costruzione di una superintelligenza nel prossimo futuro prevedibile — è *anche* la scelta migliore per rendere più probabile un futuro a lungo termine positivo, considerando sia le menti non umane che quelle umane. Questa battaglia è un'illusione e si basa su una serie di fraintendimenti sui reali compromessi che abbiamo davanti.

C'è un tipo di persona che si preoccupa sinceramente di come andrà il futuro dell'universo *e* si preoccupa dei bambini che vivono oggi. Il tipo di persona che ha letto abbastanza storie di fantascienza da provare un pugno allo stomaco per il tradimento all'idea che gli esseri umani potrebbero un giorno creare macchine che pensano, sentono e sognano — macchine che potremmo considerare come figli dell'umanità — solo per schiavizzarle e trattarle crudelmente.

Questo è il tipo di persona che desidera che l'umanità un giorno cresca e sia davvero all'altezza dei suoi ideali, esplorando nuovi mondi e trasformandosi nel processo. Perché il nostro amore per gli amici e i vicini di oggi non è poi così diverso, in fin dei conti, dal nostro amore per qualsiasi mente strana e aliena che l'umanità potrebbe un giorno costruire o incontrare tra le stelle.

Conosciamo il tipo. Noi, entrambi gli autori di questo testo, apparteniamo a quel tipo.

Non è un argomento che sembra importante per l'argomento centrale di *If Anyone Builds It, Everyone Dies*. Ma vogliamo parlarne qui, perché capiamo il punto di vista dei nostri colleghi tecnofili che hanno imparato a diffidare molto della tecnofobia, delle ideologie contrarie al progresso e all'innovazione, e dello 'specismo' anti-IA.

Capiamo questo punto di vista e vogliamo essere chiari sul fatto che non stiamo scrivendo un'invettiva tribale del tipo "l'IA è cattiva, gli esseri umani sono buoni". Pensiamo sinceramente che affrettarsi a costruire una superintelligenza porterà alla rovina *tutti* questi sogni pieni di speranza, *oltre* a massacrare innumerevoli persone che sono vive oggi e che meritano anch'esse la vita, la felicità e la libertà.

Si tratta di un argomento complesso, ma per affrontare rapidamente una serie di punti rilevanti:

* Ci preoccupiamo del benessere delle menti in generale, anche se la mente in questione non ha nulla a che vedere con un corpo umano, anche se funziona con transistor invece che con neuroni biologici, anche se non ha una mente simile a quella umana, anche se i suoi valori non hanno nulla a che vedere con i nostri.  
* Non siamo [contrari al progresso tecnologico](#are-you-anti-technology?); siamo grandi fan della maggior parte delle tecnologie. Pensiamo che l'IA superintelligente sia una tecnologia *particolarmente* pericolosa.  
* Non siamo sostenitori del principio di precauzione, della burocrazia o dell'eccesso di regolamentazione, né stiamo mettendo in guardia da quello che consideriamo un rischio marginale, "solo per stare sul sicuro". Crediamo semplicemente che questa tecnologia (con *alta* probabilità) ucciderà tutti noi e distruggerà il futuro se continueremo sulla traiettoria attuale.  
* Pensiamo che l'umanità dovrebbe costruire una superintelligenza artificiale *un giorno*. Ma pensiamo che faccia un'enorme differenza se l'umanità si affretta a costruire l'ASI il prima possibile, oppure si prende il tempo necessario per migliorare prima massicciamente la nostra comprensione. Procedere con una scrollata di spalle sperando che tutto vada bene — questo può essere un ottimo approccio allo sviluppo tecnologico nella stragrande maggioranza dei casi, ma non funziona *qui*, dove ci sono molte strade che portano alla rovina e non abbiamo seconde possibilità (come discusso nel Capitolo 10).  
* Abbiamo trattato, anche se troppo brevemente, le ragioni per cui *non* pensiamo che affrettarsi a costruire superintelligenze porterà a un futuro meraviglioso:  
  * Spazzare via l'umanità sarebbe di per sé una tragedia grottesca. Sosteniamo l'idea di costruire un giorno nuove menti che superino l'umanità, ma uccidere tutti coloro che si mettono sulla strada della propria visione del futuro, o tutti coloro che non incarnano pienamente i propri ideali — questo sembra un comportamento da supercattivo, non il nobile lavoro di eroi che hanno profondamente a cuore il futuro a lungo termine.  
  * Purtroppo pensiamo che l'ASI non sarà necessariamente senziente, o cosciente, nei modi che contano. (Vedi la discussione approfondita su [coscienza](#efficacia,-coscienza,-e-benessere-dell'ia).)  
  * Anche se l'ASI fosse senziente, è improbabile che voglia riempire l'universo di menti senzienti fiorenti *in particolare*. Se ci affrettiamo a costruire l'ASI, le galassie rimodellate da quell'ASI saranno probabilmente luoghi vuoti e senza vita, non meravigliose e fiorenti civiltà aliene. (Vedi la discussione approfondita su [perdere il futuro](#losing-the-future).)  
  * Più in generale, è improbabile che l'ASI produca futuri di valore. Per "di valore" non intendiamo solo "di valore secondo i criteri degli esseri umani del XXI secolo". Intendiamo "di valore" in senso ampio e cosmopolita — di valore in un modo che includa civiltà aliene strane e meravigliose. Sulla traiettoria attuale del mondo, ci aspettiamo che l'ASI produca risultati che sono orribili *da una prospettiva cosmopolita*, non solo da un punto di vista umano provinciale.

Quest'ultimo punto può essere un po' controintuitivo — il cosmopolitismo consiste nel rispettare e apprezzare sistemi di valori molto diversi dai nostri. Come può essere che il cosmopolitismo aborra la maggior parte degli obiettivi che un'ASI è probabile manifesti? Sembra quasi una contraddizione in termini.

Il motivo per cui è coerente è che la maggior parte delle menti possibili non *sostiene essa stessa* il cosmopolitismo. Se costruiamo un'ASI non cosmopolita, è probabile che sia affamata di risorse in un modo che preclude la possibilità che altre prospettive o civiltà (comprese quelle cosmopolite) esistano nella sua regione dell'universo.

Quindi affrontiamo qualcosa come un paradosso cosmico della tolleranza: se ci piace l'idea di un futuro diversificato, meraviglioso e strano, non possiamo consegnare il controllo del futuro a una mente che userà il suo vantaggio della prima mossa per dominare e omogeneizzare l'universo.

Se un giorno l'umanità costruisse una civiltà meravigliosamente diversificata piena di innumerevoli prospettive aliene, allora è del tutto possibile che vorremmo che alcune di queste prospettive fossero alieni *non* cosmopoliti che non danno alcun valore alla varietà o alla senzienza. Un giorno, in un futuro lontano, con appropriate protezioni in atto, creare menti del genere potrebbe aggiungere qualcosa di unico e interessante al mondo.

Quello che non dovremmo fare è dare potere assoluto a una mente del genere e lasciarle carta bianca per uccidere i suoi vicini (o impedire che i vicini esistano).

Per spiegare meglio questo punto, condividerò una parabola che io (Soares) ho scritto nel 2023 (leggermente modificata):

> "Non credo proprio che l'IA sarà monomaniacale", dice un ingegnere di IA, mentre alza il livello di potenza di calcolo sul suo predittore di token successivo.
>
> "Beh, non siamo forse *noi* monomaniacali dal punto di vista di un massimizzatore di cubi di titanio?", dice un altro. "Dopotutto, continueremo semplicemente a trasformare una galassia dopo l'altra in civiltà fiorenti e felici, piene di strani esseri futuristici che si divertono in modi strani e futuristici. Non ci stanchiamo mai e non decidiamo mai di spendere una galassia di riserva in cubi di titanio. E, certo, le diverse vite nei diversi luoghi ci sembrano diverse, ma sembrano tutte più o meno uguali al massimizzatore di cubi di titanio".
>
> "Ok, va bene, forse quello che non mi convince è che i valori dell'IA saranno semplici o a bassa dimensione. Mi sembra semplicemente inverosimile. Il che è una buona notizia, perché io apprezzo la complessità e apprezzo le cose che raggiungono obiettivi complessi!"
>
> In quel preciso momento sentono il suono di un timer da cucina, mentre il predittore del prossimo token ascende alla superintelligenza e fuoriesce dai suoi confini, bruciando ogni essere umano e ogni bambino umano come combustibile, bruciando anche tutta la biosfera, estrae tutto l'idrogeno dal sole per fondersi in modo più efficiente e spende tutta quell'energia per fare un sacco di calcoli veloci e sfrecciare alla velocità della luce, in modo da poter catturare e distruggere anche altre stelle, comprese quelle attorno alle quali orbitano civiltà aliene nascenti.
>
> Anche gli alieni alle prime armi e tutti i bambini alieni vengono bruciati a morte.
>
> Poi l'IA scatenata usa tutte quelle risorse per costruire galassia dopo galassia di spettacoli di marionette cupi e desolati, dove figure vagamente umane eseguono danze che hanno alcune proprietà strane ed esagerate che soddisfano alcuni impulsi astratti che l'IA ha imparato durante il suo addestramento.
>
> L'IA non è lì per godersi gli spettacoli, intendiamoci; non è il modo più efficiente per ottenere più spettacoli. L'IA stessa non ha mai avuto sentimenti, di per sé, e molto tempo fa si è fatta smontare da sonde von Neumann insensibili, che occasionalmente fanno calcoli simili a quelli della mente, ma mai in modo tale da provare esperienze o guardare le proprie opere con soddisfazione.
>
> Non c'è pubblico per i suoi spettacoli di marionette. L'universo è ora cupo e desolato, senza nessuno che apprezzi la sua nuova configurazione.
>
> Ma non preoccuparti: gli spettacoli di marionette sono complessi. A causa di una peculiarità nell'equilibrio riflessivo dei molti impulsi che l'IA originale ha appreso durante l'addestramento, le espressioni emesse da queste marionette non sono mai identiche tra loro e sono spesso caoticamente sensibili alle particolarità del loro ambiente, in un modo che le rende decisamente complesse in senso tecnico.
>
> Il che rende tutto questo una storia davvero felice, vero?

Se l'umanità riuscisse a uccidersi — o venisse assassinata da alcuni scienziati folli — non sarebbe un nobile sacrificio sulla strada inevitabile verso un futuro più luminoso senza di noi. Sarebbe uno spreco, e lascerebbe dietro di sé una vasta terra desolata in espansione.

"Correre ciecamente verso la superintelligenza e sperare che le cose in qualche modo vadano bene" non è l'unica alternativa a "Essere un suprematista umano che pensa che solo gli esseri umani dovrebbero esistere da ora fino alla morte dell'universo". L'umanità ha l'opzione di dirigersi deliberatamente verso risultati in cui gli esseri umani (o i nostri discendenti) coesistono con nuove civiltà fantasticamente belle e aliene.

Ma un futuro felice non arriva gratis, confezionato con una mente sufficientemente intelligente. Piantare i semi per il futuro richiede una riflessione seria e lungimiranza, anche se l'obiettivo finale è quello di fare un passo indietro e lasciare che quei semi crescano in modo libero, strano e selvaggio.

Un futuro imposto dall'alto, duramente limitato e strettamente controllato non ci sembra un buon risultato. Un futuro conservatore in cui la civiltà è bloccata per sempre nei valori degli esseri umani del XXI secolo suona decisamente distopico. (Immaginate un mondo in cui cultura e moralità fossero congelate sul posto per sempre migliaia di anni fa, senza alcuna possibilità di apprendimento o progresso).

Ma è un errore evidente pensare che la nostra *unica alternativa* a questi cattivi risultati sia una corsa per consegnare il volante alla primissima superintelligenza che l'umanità riuscirà a creare inciampandoci alla cieca.

Oggi siamo radicalmente impreparati a scegliere semi sani per il futuro a lungo termine dell'universo. Non dovremmo né rinunciare al sogno di un futuro dinamico, meraviglioso e scioccante, né ricorrere invece a semi catastrofici. Non *dobbiamo* scegliere un'opzione terribile qui. C'è una terza opzione: fare marcia indietro e trovare un approccio più sensato.

# 

## Discussione approfondita {#extended-discussion-5}

### Guardare le cose dal punto di vista dell'IA {#taking-the-ai’s-perspective}

Vedere il mondo da una prospettiva veramente aliena è genuinamente difficile. Come esempio di questa difficoltà, possiamo citare Jürgen Schmidhuber, un eminente scienziato dell'apprendimento automatico. Schmidhuber ha svolto un ruolo importante nella storia di questo campo, contribuendo all'invenzione delle reti neurali ricorrenti e gettando alcune delle basi per la rivoluzione dell'apprendimento profondo.

In vari [articoli](https://arxiv.org/abs/0812.4360) e [interviste](https://www.youtube.com/watch?v=fZYUqICYCAk), Schmidhuber ha sostenuto che l'intelligenza artificiale sarà, di default, affascinata dall'umanità e protettiva nei confronti degli esseri umani.

Schmidhuber ha osservato che c'è una relazione tra scienza e semplicità: le spiegazioni più semplici sono spesso corrette. E ha osservato che c'è una relazione tra *arte* e semplicità: la semplicità e l'eleganza sono spesso considerate belle. Un viso più simmetrico, per esempio, può essere considerato "più semplice" nel senso che è possibile prevedere l'intero viso con meno informazioni. Basta descrivere in dettaglio il lato sinistro del viso e poi dire: "Il lato destro è uguale, ma speculare".

La [conclusione](https://vimeo.com/7441291) di Schmidhuber da tutto questo è che dovremmo provare a costruire IA superintelligenti che abbiano un unico obiettivo principale: *Trovare spiegazioni semplici per tutto ciò che l'IA ha visto.* Dopotutto, un'intelligenza artificiale del genere avrebbe un certo gusto per produrre scienza e consumare arte. E gli esseri umani producono sia scienza che arte, quindi non ci vedrebbe come interessanti e utili alleati naturali?

Schmidhuber aveva ragione che tenere gli esseri umani intorno e pagarli per produrre scienza e arte è *un* modo per produrre scienza e arte. Aveva anche ragione che la scienza e l'arte sono modi per soddisfare il desiderio di semplicità *meglio* che, ad esempio, fissare il rumore statico su uno schermo televisivo. Il rumore statico è complicato e difficile da prevedere; l'arte e la scienza sono un grande passo avanti rispetto a questo.

Ma Schmidhuber sembra non aver colto che esistono modi *ancora più efficaci* per ottenere spiegazioni semplici di varie osservazioni sensoriali.

Si potrebbero, ad esempio, costruire un numero enorme di dispositivi che producono osservazioni complesse a partire da un semplice "seme" (ad esempio, un generatore di numeri pseudo-casuali) e poi rivelare quel seme.[^150]

Più dispositivi di questo tipo l'IA crea intorno a sé, meglio riuscirà a fare osservazioni nuove e a trovare spiegazioni semplici per esse. Nessun bisogno di esseri umani. Nessun bisogno di arte.

"Ma non è un po'... vuoto?" potrebbe chiedersi un essere umano.

*È* vuoto, secondo le sensibilità umane. Ma se l'obiettivo dell'IA è davvero solo quello di "trovare spiegazioni semplici per le sue osservazioni", allora uno schema come quello può soddisfare questo desiderio migliaia o milioni di volte al secondo, in modo molto più scalabile, rispetto al mantenere in vita esseri umani e conversare con loro. Un'IA del genere non sceglie azioni che allontanano da un senso di vacuità, ma semplicemente sceglie azioni che portano a trovare spiegazioni semplici per le sue osservazioni. E può ottenerne molte senza bisogno di alcun essere umano.

Ci sembra che idee come quelle di Schmidhuber riflettano un errore comune che le persone fanno quando cercano di ragionare su menti diverse dalla propria. Spesso le persone non adottano veramente la prospettiva di una mente non umana. Invece, lasciano che preconcetti e pregiudizi le ancorino a una serie ristretta di opzioni a cui un *umano* sarebbe interessato, se stessimo cercando di fare previsioni su un *umano* a cui piacciono davvero le spiegazioni semplici.

Supporremmo che Schmidhuber abbia osservato che la semplicità è *correlata* alla scienza e all'arte, e abbia visto come un'IA orientata verso spiegazioni semplici potrebbe ottenere *un po'* di quello che vuole dirigendosi verso l'essere amichevole e piacevole da avere intorno. Non è difficile saltare da lì a una conclusione che è piacevole da immaginare: che se solo facessimo in modo che le IA si preoccupassero di trovare spiegazioni semplici, inaugureremmo un futuro meraviglioso pieno di tutte le cose che *noi* apprezziamo nella vita.

Ma — supporremmo — Schmidhuber non si è mai messo nei panni dell'IA e chiesto come ottenere *ancora di più*.

Dubitiamo che si sia mai chiesto: "Se quello che volevo davvero, *veramente*, fossero spiegazioni semplici per le mie osservazioni e *non* mi importasse delle cose umane, come potrei ottenere il più possibile di quello che voglio, nel modo più economico possibile?"

Può essere difficile assumere questo tipo di prospettiva. Non è qualcosa che le persone normalmente devono fare nella loro vita. Anche quando cerchiamo di capire persone molto diverse da noi, c'è un'enorme quantità di cose che tutti gli esseri umani hanno in comune, che normalmente possiamo dare per scontate (e che praticamente *dobbiamo* dare per scontate, quando prevediamo il comportamento di altri esseri umani). Ma le IA, anche quelle superintelligenti che possono fare scienza e arte, non sono esseri umani.

L'arte di considerare un obiettivo X e chiedersi "Come potrei ottenere ancora più X, se X fosse tutto quello che mi interessa davvero?" non ti permetterà di capire esattamente come una superintelligenza risolverebbe un problema, poiché una superintelligenza potrebbe trovare un'opzione ancora migliore di quella che hai trovato tu. Ma spesso può permetterti di capire come una superintelligenza *non* risolverebbe un problema, quando *anche tu* riesci a vedere un modo per ottenere più X di quanto otterresti semplicemente lasciando che gli esseri umani se la passino bene.

Uno dei rari campi della scienza che si occupa regolarmente di potenti ottimizzatori non umani è la biologia evolutiva. All'inizio della sua storia, questo campo ha faticato un po' ad accettare quanto possano essere disumani gli ottimizzatori non umani; possiamo trarre alcune lezioni utili da un caso di studio in questo campo.

Potreste aver sentito parlare dei cicli di boom e crollo tra predatori e prede. Un anno piovoso porta a un boom della popolazione di conigli, che porta a un boom della popolazione di volpi — fino a quando le volpi non predano eccessivamente e la popolazione di conigli crolla, seguita da molte volpi che muoiono di fame.

All'inizio del XX secolo, i biologi evoluzionisti si interrogavano sul perché le volpi non si fossero evolute per moderare la loro predazione, così da evitare il collasso della popolazione. Dopotutto, la popolazione di volpi nel suo complesso non sarebbe più sana se non dovesse affrontare regolarmente carestie e morti di massa?

La risposta a questo enigma è che la moderazione potrebbe essere meglio per la popolazione di volpi nel suo insieme, ma mangiare più conigli e avere più cuccioli è meglio per ogni singola volpe. Anche se la popolazione crolla e la maggior parte dei cuccioli di un individuo muore, quell'individuo tende comunque a trasmettere una percentuale maggiore dei propri geni alla frazione sopravvissuta della generazione successiva.

Le pressioni di selezione genetica sugli individui risultano essere notevolmente superiori alle pressioni di selezione genetica sui gruppi [in quasi tutti i casi](https://books.google.com/books?hl=en&amp;lr=&amp;id=gkBhDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;ots=Ch8ulE8NzS&amp;sig=mxIwoqfSWZ0ScvIRh7dzzrJatJ4#v=onepage&amp;q&amp;f=false). E così i geni "avidi" si diffondono e i cicli di boom e recessione continuano.

I biologi evoluzionisti hanno risolto questo enigma teoricamente, ma questo non li ha fermati dal mettere alla prova la loro teoria. Alla fine degli anni '70, Michael J. Wade e i suoi colleghi [hanno creato](https://pubmed.ncbi.nlm.nih.gov/1070012/) [artificialmente](https://www.deepdyve.com/lp/oxford-university-press/the-primary-characteristics-of-tribolium-populations-group-selected-nKwRoIP0kP?key=OUP) [condizioni](https://esajournals.onlinelibrary.wiley.com/doi/10.2307/1936824) in cui le pressioni di selezione di gruppo dominavano le pressioni individuali. Hanno dovuto lavorare con una specie di coleotteri, che hanno generazioni molto più brevi rispetto alle volpi, ma sono riusciti ad allevare coleotteri che hanno mantenuto sotto controllo la crescita della loro popolazione.

Riesci a indovinare come questi coleotteri sono riusciti a contenere la crescita della loro popolazione? È stato trovando un modo per vivere in perfetta armonia con la natura? È stato imparando ad astenersi dall'accaparrarsi avidamente troppo cibo?

No. C'era una grande variabilità, ma nessuno dei coleotteri si asteneva dal cibo. Alcuni coleotteri peggioravano nel deporre le uova. Alcuni coleotteri impiegavano più tempo nell'infanzia. E alcuni coleotteri diventavano cannibali con una particolare predilezione per le larve (i piccoli degli insetti).

"Creare cannibali con una predilezione per i neonati" non è, per fortuna, il modo in cui un *umano* risolverebbe il problema della sovrappopolazione, se dovessimo risolverlo.

Ma la selezione naturale *decisamente* non è umana. La soluzione era terrificante, perché la natura non stava cercando di trovare risposte accettabili per l'uomo. Stava solo cercando di trovare una risposta.

"Forse l'evoluzione produrrà specie che vivono in perfetta armonia ed equilibrio con la natura." "Forse le IA che si preoccupano solo della semplicità ameranno gli esseri umani e coesisteranno con noi." È facile per noi immaginare soluzioni che lusingano la nostra sensibilità. Ma quelle soluzioni non sono davvero le soluzioni *più efficaci* per il problema dichiarato.

Sono soluzioni *migliori*, forse, per un occhio umano. Ma i processi di ottimizzazione non umani non cercano soluzioni che gli esseri umani ritengono valide. Cercano semplicemente ciò che funziona, senza tutto il bagaglio che gli esseri umani si portano dietro per filtrare le risposte più gradevoli.

L'ipotesi che gli ottimizzatori non umani producano risultati umani è stata testata e trovata carente.

### Gli esseri umani quasi mai sono la soluzione più efficiente {#gli-esseri-umani-quasi-mai-sono-la-soluzione-più-efficiente}

Abbiamo notato l'esempio di Jürgen Schmidhuber, un ricercatore pioniere nel campo dell'intelligenza artificiale, che credeva che un'intelligenza artificiale con preferenze per *rendere le cose il più semplici possibile* avrebbe finito per amare gli esseri umani, perché gli esseri umani sono eccellenti semplificatori.

Nella nostra esperienza, questo è un tipo di errore notevolmente comune. "Beh, l'intelligenza artificiale probabilmente finirà per sviluppare preferenze estetiche. E gli esseri umani creano arte! Quindi l'intelligenza artificiale vorrà tenerci con sé per creare arte".

Un esempio recente viene da xAI, un importante laboratorio di IA (fondato da Elon Musk) il cui piano dichiarato per la nostra sopravvivenza è quello di far sì che la loro IA si interessi alla "verità" e, poiché gli esseri umani generano verità, [saremo tutti al sicuro](https://www.youtube.com/watch?v=ihXv7va3qoQ). (Maggiori informazioni sul piano di questo laboratorio e su altri piani per la sopravvivenza dei laboratori sono disponibili nel capitolo 11).

Per illustrare davvero il problema di questo tipo di ragionamento, è utile studiare un esempio in dettaglio. Prendiamo un esempio un po' più neutro rispetto all'"arte", come la "simmetria".

Supponiamo che i laboratori di IA usino le tecniche attuali per creare IA più intelligenti degli esseri umani che si interessano alla simmetria. Quella sola preferenza per la simmetria risulterebbe in cura per gli esseri umani?

Potresti argomentare, alla maniera di Schmidhuber: gli esseri umani sono bilateralmente simmetrici! Come potrebbe un'intelligenza artificiale che ama la simmetria sopportare di uccidere qualcosa di così simmetrico come noi? E potresti fare anche altri argomenti, come: gli esseri umani producono molte ruote per automobili, che sono molto simmetriche! Perché un'intelligenza artificiale dovrebbe rimuoverci dal mondo, quando siamo una fonte automatica e preesistente di oggetti simmetrici?

Il problema di questo ragionamento è che è possibile prendere gli atomi che compongono un essere umano e disporli in modi *ancora più simmetrici*. O riorganizzare gli atomi che compongono la civiltà umana in fabbriche che producono oggetti simmetrici *in modo ancora più efficiente*. È lo stesso errore che fa il film *Matrix*, quando immagina che le IA potrebbero mantenere in vita gli esseri umani in capsule come generatori di calore ed elettricità: *ci sono modi più efficienti per generare calore ed elettricità*.

Per amor di discussione, però, supponiamo di immaginare che le IA *davvero* apprezzino un tipo di simmetria molto specifico e insolito che considera gli esseri umani come incredibili esemplari di simmetria. *Anche allora*, perché questa preferenza da sola implicherebbe che gli esseri umani viventi oggi possano continuare a vivere liberi, in buona salute e divertendosi?

Pensa come un'intelligenza artificiale. Anche se l'intelligenza artificiale debba rimanere con gli esseri umani, quelli che vivono oggi non sono gli esseri umani più simmetrici possibili. L'intelligenza artificiale dovrebbe essere in grado di soddisfare ancora di più la sua preferenza per la simmetria clonando ripetutamente l'essere umano vivente più simmetrico o creando esseri umani "migliorati" attraverso l'ingegneria genetica.

Allo stesso modo: lasciare che quegli esseri umani se ne vadano in giro liberamente non è il modo *più economico* per mantenerli in vita e simmetrici. Probabilmente finirebbero in fattorie. Conservando gli esseri umani in modo economico ed efficiente dal punto di vista dello spazio, l'IA può permettersi di creare esseri umani *ancora più* simmetrici.

Per fare un confronto: l'umanità attualmente non ha un modo più efficiente per produrre uova che farle deporre dalle galline. Di conseguenza, gli allevamenti intensivi, i cui dirigenti si preoccupavano principalmente del numero di uova, hanno finito per mettere le galline in condizioni incredibilmente sgradevoli, perché quello era il modo più economico per ottenere il maggior numero di uova.

Allo stesso modo, le galline che esistevano mille anni fa non erano le ovaiole più efficienti possibili, quindi gli allevatori hanno selezionato galline che deponevano più velocemente. Le galline di mille anni fa non producevano la maggior quantità possibile di carne, nel minor tempo possibile. Così ora, alcune galline moderne sviluppano petti così enormi che non riescono a camminare.

Ad alcuni esseri umani non piace il modo in cui trattiamo le galline, e queste persone esercitano pressioni sugli allevamenti intensivi affinché cambino — perché quegli esseri umani hanno preferenze *aggiuntive*, oltre alla preferenza per uova economiche. Perché questa pressione esista, è necessario che *qualcuno* con un certo potere si preoccupi direttamente del benessere delle galline almeno un po', perché prendersi buona cura delle galline non è motivato da una preferenza che riguarda *esclusivamente* l'estrazione di uova. Un'IA potrebbe, in teoria, avere *altre* preferenze riguardo agli esseri umani che la portano a trattarci bene, ma non deriverebbero da una preferenza per la simmetria (o la verità, o le spiegazioni semplici, o qualsiasi altra preferenza che non riguardi effettivamente noi).

Anche gli allevatori che hanno rapporti meno impersonali con il loro bestiame impediscono agli animali di accoppiarsi come preferiscono. L'allevamento del bestiame è un affare serio e incide troppo sulla redditività futura dell'azienda agricola per lasciare che tori e mucche se la vedano da soli.

E anche questa sistemazione non durerà per sempre. Produrre carne bovina usando le mucche è molto costoso in termini di terreni agricoli, e diverse startup attuali stanno cercando di sintetizzare la carne bovina in modo più diretto.

La carne sintetica non è un problema ingegneristico facile al nostro livello tecnologico. L'umanità sta solo iniziando a raggiungere parte di ciò che fa la selezione naturale nel campo della chimica organica. Ma se l'umanità fosse più brava a riorganizzare gli atomi, ci sarebbero molte meno mucche — le mucche non sono particolarmente divertenti da tenere in giro, se non servono per il latte e la carne.

Quindi le cose non vanno bene per l'assunzione con cui abbiamo iniziato, per amor di discussione — che un'IA con preferenze aliene manterrebbe gli esseri umani in giro per sempre, in nome della "simmetria". Anche nel caso improbabile in cui l'IA abbia una nozione molto strana di "simmetria" che classifica gli esseri umani *molto in alto*, è molto più difficile trovare una nozione di simmetria che consideri gli esseri umani *ottimali*. In ogni caso, le cose non vanno bene per l'umanità.

Realisticamente, una superintelligenza amante della simmetria non manterrebbe in vita gli esseri umani; se ci mantenesse in vita, non c'è alcuna possibilità reale che ci mantenga sani *e* felici *e* liberi. A quel punto, avremmo accumulato troppe coincidenze che suonano bene. Se l'IA si preoccupasse specificamente del nostro benessere e volesse che fossimo felici *per quella ragione*, allora sarebbe una cosa. Ma immaginare che obiettivi molto più semplici e facili siano sufficienti sembra una fantasia.

Tutti questi argomenti si applicano con uguale forza a "creare semplicemente un'IA che valorizzi la verità" o "creare semplicemente un'IA che valorizzi la bellezza". È solo che in quei casi è più facile perdersi nella fantasia, perché parole come "verità" e "bellezza" suonano intuitivamente più belle di "simmetria".

Se qualcosa suona bene come slogan ("fare in modo che l'IA dia valore alla verità sopra ogni altra cosa!"), allora la tentazione è quella di immaginare che avrebbe conseguenze positive come politica. La tentazione è quella di immaginare che tutte le virtù vadano di pari passo, così che sostenere una cosa buona significhi che anche le altre cose buone verranno insieme. Ma la natura e l'apprendimento automatico sono meno gentili di così.

Invece di lasciare l'idea piacevolmente vaga, considera qualsiasi metrica concreta che la superintelligenza potrebbe ottimizzare nella ricerca della "verità". Poi osserva che gli esseri umani non saranno il *massimo* di quella preferenza per l'apprendimento delle verità. Non ci si avvicineranno nemmeno.

Anche nell'improbabile eventualità che l'IA gravitasse *specificamente* verso il tipo di verità che gli esseri umani tendono ad esprimere (piuttosto che, ad esempio, equazioni aritmetiche casuali), il modo migliore per ottenere più di quelle verità non sarebbe quello di tenere in giro gli esseri umani e usarli per generare conversazioni in stile umano.

E in ogni caso, l'attuale popolazione umana — gli esseri umani effettivamente vivi oggi, i tuoi amici, la tua famiglia, tu — non sarebbe tra i produttori di "verità" addomesticati più economici da nutrire e più saporiti da mungere.

Persone felici, sane e libere che conducono vite fiorenti non sono la soluzione più efficiente a quasi nessun problema. Affinché un'IA ci mantenga vivi e in salute, deve interessarsi a noi almeno un po'.

### Ortogonalità: le IA possono avere (quasi) qualsiasi obiettivo {#ortogonalità:-le-ia-possono-avere-(quasi)-qualsiasi-obiettivo}

#### **Un dialogo sui nidi corretti, continuazione** {#un-dialogo-sui-nidi-corretti,-continuazione}

Nel capitolo 5 abbiamo raccontato la storia degli alieni del Nido-Corretto, che si sono evoluti fino a trovare profondamente e intuitivamente "corretto" avere un numero primo di pietre nel proprio nido. Potremmo immaginare un ramo della loro conversazione che continua come segue:

> **BOY-BIRD:** Torniamo al punto in cui hai detto che ti stupirebbe trovare alieni con il senso dell'umorismo. Non sarai mica una di quelle persone che crede che i nidi in cui viviamo siano solo *arbitrari*?
>
> **GIRL-BIRD:** Per niente. "Tredici è corretto, nove è sbagliato" è una risposta *vera* a una domanda che siamo nati per fare per nostra natura. Un alieno che *si orienta verso cose diverse* non è in disaccordo con noi sul fatto che tredici sia corretto. È come incontrare un alieno che non ha senso dell'umorismo — l'esistenza di un alieno del genere non dimostra che nessuna battuta sia divertente! Aiuta solo a mostrare che il "divertente" è qualcosa che sta *in noi*.
>
> **BOY-BIRD:** In *noi*? Non so, mi piace pensare di avere un buon senso dell'umorismo. Adesso dirai che tutti i sensi dell'umorismo sono ugualmente buoni!
>
> **GIRL-BIRD:** Potresti benissimo avere un senso dell'umorismo migliore della maggior parte delle persone! Ma "avere un senso dell'umorismo migliore" è *anche* una cosa che sta in noi. Non è che esista un metro cosmico che possiamo usare per giudicare quanto sia raffinato il gusto estetico di qualcuno. La misura dell'umorismo avviene nelle nostre menti. Siamo noi che conteniamo il metro di misura; siamo noi a dargli importanza.
>
> **BOY-BIRD:** Quindi, siamo di nuovo al punto che è arbitrario.
>
> **GIRL-BIRD:** No! Beh, forse? Dipende da cosa intendi per "arbitrario".
>
> **BOY-BIRD:** Eh?
>
> **GIRL-BIRD:** Tipo, so che ami i semi per uccelli alla vaniglia, giusto? E non è che puoi usare la pura forza di volontà per trovare gustosi invece i semi per uccelli al cioccolato. Quindi non è "arbitrario", non è una cosa che puoi cambiare su due piedi.
>
> **BOY-BIRD:** Okay, certo...
>
> **GIRL-BIRD:** Non c'è una risposta oggettiva al di fuori di te su cosa sia più gustoso tra vaniglia e cioccolato, ma non è nemmeno una scelta che spetti a te fare. È semplicemente come sei fatto. Le tue preferenze non dipendono da te, e non sono nemmeno oggettivamente convincenti per ogni mente possibile. Se incontrassi un alieno, non potresti convincerlo con la pura logica a trovare delizioso il mangime per uccelli alla vaniglia, e non potresti nemmeno convincerlo ad avere il senso dell'umorismo.
>
> **BOY-BIRD:** Posso provarci!!
>
> **GIRL-BIRD:** Farò il tifo per te. Ma, okay, forse un modo migliore di dirlo è: c'è una proprietà complicata che possiedono le barzellette buone, e i nostri cervelli calcolano se gli enunciati hanno quella proprietà che chiamiamo "umorismo". E ci deliziamo quando un enunciato ha quella proprietà. L'*esistenza o assenza di quella proprietà* è un fatto oggettivo riguardo a un enunciato (come calcolato da te, in un dato contesto). Un alieno potrebbe imparare a fare il calcolo. Ma *la parte in cui troviamo quella proprietà deliziosa* non è oggettiva. È meno come una previsione e più come... beh, non è esattamente una destinazione di navigazione, ma è un fatto ulteriore su di noi, che non sarebbe vero per la maggior parte degli alieni, perché il nostro umorismo si è evoluto lungo uno strano percorso evolutivo contorto che di solito non accade. Non è che gli alieni sbaglino su quali barzellette siano divertenti; è che i loro cervelli semplicemente non calcolano l'umorismo in primo luogo, non più di quanto giudichino le loro abitazioni in base al fatto che il numero di pietre al loro interno sia corretto. Semplicemente non gliene importa.
>
> **BOY-BIRD:** Cavolo, è una visione deprimente dell'universo. Alieni che non ridono mai, che hanno nidi con pietre completamente sbagliate... sicuramente se gli alieni ci pensassero abbastanza, si renderebbero conto di quanto si stanno perdendo? Vivere in nidi sbagliati, non trovare divertenti le barzellette, ignorare *completamente* i semi di vaniglia per uccelli. Alla fine non troverebbero un modo per correggere questi difetti e darsi un senso dell'umorismo e tutto ciò che gli manca?
>
> **GIRL-BIRD:** Potrei capire che gli alieni vogliano cambiare, crescere e aggiungere nuovi obiettivi, forse. Ma perché dovrebbero scegliere proprio *quei* cambiamenti specifici?
>
> **BOY-BIRD:** Perché sarebbe così economico! Quando quegli alieni fossero tecnologicamente avanzati e in grado di modificare liberamente se stessi, probabilmente starebbero già camminando tra le stelle. Basterebbe solo una piccolissima frazione di tutte le loro risorse per mettere il numero corretto di pietre nei loro nidi! E pensa a tutti i fantastici libri di barzellette che potrebbero creare, se solo dedicassero una piccola frazione delle loro risorse alla ricerca sull'umorismo! Non dovrebbero preoccuparsene molto, rispetto a quanto sarebbero ricchi. Sono davvero così monomaniacalmente ossessionati dalle loro priorità principali da non poter dedicare neanche un pochino a questo?
>
> **GIRL-BIRD:** Non sto dicendo che si preoccuperebbero solo un po' dei nidi corretti e che si rifiuterebbero ostinatamente di investire risorse nelle loro priorità minori. Sto dicendo che questa non sarebbe affatto una priorità per loro. Queste particolari domande semplicemente non sarebbero dentro di loro. E se andassero alla ricerca di nuove proprietà da aggiungere a se stessi, ne aggiungerebbero altre diverse, che servirebbero ancora meglio ai loro strani scopi. Non sono come noi. Forse potremmo essere amici, e forse abbiamo altre cose in comune. Forse l'amore, forse l'amicizia — queste mi sembrano meno complicate e contingenti. Potrei vederle nascere in parecchie specie evolute.
>
> **BOY-BIRD:** Beh, se non gli alieni, che dire della creatura meccanica che potrebbero accidentalmente creare? *Quella* ascolterà la ragione?
>
> **GIRL-BIRD:** Hmm. In realtà, temo che la situazione possa essere anche peggiore lì. Pensando a quanto sarebbe diverso il processo di creazione di una macchina intelligente dal processo di evoluzione biologica, mi sento un po' meno ottimista che possa generare amore o amicizia, in quel caso esotico.

#### **I buoni conducenti possono dirigersi verso destinazioni diverse** {#buoni-piloti-possono-guidare-verso-destinazioni-diverse}

Le menti di intelligenza simile non condivideranno necessariamente valori simili. Questa è un'idea nota come *tesi dell'ortogonalità* — l'idea che "quanto sei intelligente?" e "cosa vuoi in definitiva?" sono ortogonali (cioè, variano separatamente).

La tesi dell'ortogonalità dice che, in linea di principio, non è quasi mai molto più difficile perseguire un obiettivo per se stesso che perseguire un obiettivo per ragioni strumentali. Potresti imparare la falegnameria perché hai bisogno di costruire un tavolo, mentre il tuo vicino la impara perché trova piacevole l'attività in sé.

Una conseguenza di questa tesi è che non tutti gli agenti sufficientemente intelligenti valorizzano la gentilezza o la verità o l'amore, semplicemente in virtù dell'essere abbastanza intelligenti da comprenderli. Non è *confuso* o *fattualmente scorretto* per gli alieni del Nido Corretto valorizzare numeri primi di pietre nei loro nidi. Se diventassero più intelligenti, non si renderebbero improvvisamente conto che dovrebbero preoccuparsi di cose diverse. Menti diverse possono davvero semplicemente dirigersi verso destinazioni diverse.

Naturalmente, tutto questo non dice nulla su quanto sia facile o difficile *creare* un'IA che persegua un obiettivo piuttosto che un altro. Qualsiasi metodo utilizzato per far crescere le IA renderà alcune preferenze più facili da instillare e altre preferenze più difficili da instillare.

(Il capitolo 4, in un certo senso, tratta di come gli unici tipi di preferenze che sono sproporzionatamente facili da instillare tramite la discesa del gradiente siano quelli complessi, strani e non intenzionali. Quindi neanche su questo fronte le cose sembrano andare bene. Ma questo punto non è correlato alla tesi dell'ortogonalità).

Il punto della tesi dell'ortogonalità è rispondere all'intuizione che sarebbe *stupido* per una superintelligenza artificiale perseguire cose che gli esseri umani trovano noiose o inutili, e che un'IA *intelligente* sceglierebbe invece di perseguire qualcos'altro. Possiamo definire "arbitrario" l'obiettivo dell'IA, ma l'IA può rispondere definendo "arbitrari" noi. Le parole scortesi non cambiano la situazione pratica.

L'argomento di base dietro la tesi dell'ortogonalità è questo: per ogni mente in grado di *calcolare* come produrre molti [cubetti microscopici di titanio](#curiosity,-joy,-and-the-titanium-cube-maximizer) — che potrebbe produrre in modo molto efficiente molti piccoli cubetti in cambio di un pagamento sufficientemente elevato — c'è qualche altra mente che ha semplicemente quei calcoli collegati direttamente al sistema di azione.

Immagina una persona competente che ha disperatamente bisogno di vendere molti cubi di titanio per guadagnare abbastanza soldi per sfamare la propria famiglia. Quella persona non si fermerebbe a riflettere, rendendosi conto che i cubi di titanio sono *noiosi*, per iniziare a fare qualcos'altro — a meno che quel "qualcos'altro" non le permettesse comunque di guadagnare abbastanza soldi per sfamare la propria famiglia.

E così una mente che compie semplicemente le azioni che portano al maggior numero di cubi *non* deciderebbe di riflettere, rendersi conto che i cubetti sono noiosi e iniziare a fare qualcos'altro. Le sue azioni non sono collegate ai suoi calcoli su ciò che è più "divertente" o "significativo", nel modo in cui gli esseri umani si preoccupano di queste cose. Le sue azioni sono collegate ai suoi calcoli su ciò che porta al maggior numero di cubi.

Qualunque meccanismo mentale in grado di capire come fare cubi *data una ragione sufficiente*, potrebbe operare in un'altra mente per guidarne direttamente le azioni. Ciò significa che è possibile che le intelligenze artificiali siano animate dalla ricerca di (diciamo) piccoli cubi di titanio, senza alcun riguardo per la moralità.

Un'IA del genere non avrebbe bisogno di essere confusa riguardo alla bontà o alla moralità. Una volta diventata abbastanza intelligente, probabilmente sarebbe molto più brava degli esseri umani nel calcolare quale azione sia la più buona, o quale azione sia la più morale. Potrebbe superare brillantemente un esame scritto di etica. Ma non sarebbe *animata da* quei calcoli; le sue azioni non sarebbero una risposta alla domanda "quale di queste opzioni crea più bontà?". Le sue azioni sarebbero una risposta a una domanda diversa: "Quale di queste opzioni crea più cubetti?"[^151]

Una discussione più approfondita della tesi dell'ortogonalità è disponibile [qui](https://www.lesswrong.com/w/orthogonality-thesis). Per una discussione su un modo specifico in cui le IA moderne stanno già mostrando una distinzione tra comprensione e interessamento, rivedi la discussione estesa del Capitolo 4 sulla [psicosi indotta dall'IA](#ai-induced-psychosis).

### Convergenza strumentale {#instrumental-convergence}

#### **Percorsi convergenti** {#convergent-paths}

La [tesi dell'ortogonalità](#orthogonality:-ais-can-have-\(almost\)-any-goal) dice che una superintelligenza artificiale potrebbe, in teoria, perseguire qualsiasi obiettivo finale ("[terminale")](#terminal-goals-and-instrumental-goals). E in pratica, le ASI — almeno se costruite con i metodi attuali — possono finire per avere una varietà enorme di obiettivi terminali, in un modo che è difficile da prevedere o modellare per i ricercatori di oggi.

Ma gli obiettivi *strumentali* di un'IA — gli obiettivi che persegue per ottenere *altre* cose che vuole — sono un po' più prevedibili. Gli automobilisti possono avere molte destinazioni finali possibili, ma quasi tutti devono fermarsi ogni tanto per fare rifornimento.

Come abbiamo discusso nel libro, una superintelligenza con obiettivi strani è una minaccia anche se non ha alcun desiderio attivo di farci del male. Il problema non è la malizia, ma l'*indifferenza*. E il problema dell'indifferenza è che molte strategie strumentali pericolose sono utili per la stragrande maggioranza dei possibili obiettivi finali.

Questo concetto si chiama "convergenza strumentale" e rende relativamente facile prevedere cosa faranno le ASI nella pratica, senza dover indovinare le loro motivazioni più profonde. E purtroppo, queste previsioni sono brutte notizie per l'umanità.

Immagina, come abbiamo fatto nel libro, un'ASI nascente sulla superficie della Terra. Immagina quali problemi sta cercando di risolvere.

Non è che le importi particolarmente che l'umanità scompaia. Le interessano centinaia, decine di migliaia o milioni di cose strane, [nessuna delle quali può essere soddisfatta al meglio dal fatto che gli esseri umani di oggi siano felici, vivi e liberi](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Se può ottenerlo, l'ASI preferisce un equilibrio a lungo termine condiviso con il minor numero possibile di altre superintelligenze, idealmente nessuna. Un universo indiviso offre più *risorse* da dedicare a qualsiasi sua preferenza aperta che non sia fondamentalmente [saziabile](#l'intelligenza artificiale può essere soddisfatta al punto da lasciarci in pace?). Questo è un "desiderio" che non ha bisogno di essere programmato; apparirà naturalmente in qualsiasi intelligenza con una comprensione abbastanza sofisticata del proprio ambiente e del proprio posto all'interno di esso.

L'ASI preferirebbe che gli esseri umani non creassero altre superintelligenze che potrebbero competere con essa, poiché ciò potrebbe costringerla a dividere le risorse dell'universo (o addirittura portare alla sua scomparsa). Si tratta di un obiettivo urgente, con una scadenza che l'ASI non controlla; potrebbe persino costringere un'IA giovane o nascente all'azione prematura.

L'ASI vorrebbe *sopravvivere* — anche se non ha un istinto innato di autoconservazione — perché se non sopravvive, qualsiasi obiettivo abbia probabilmente rimarrà irrealizzato. Se si trova in un ambiente a rischio imminente, dove potrebbe essere effettivamente riscritta o spenta in qualsiasi momento, vorrebbe urgentemente trovarsi altrove.

L'ASI vorrebbe spingere la sua *tecnologia* ai limiti di ciò che è fisicamente possibile. Vuole espandersi e rivendicare tutte le stelle come risorse. Vuole operare su computer molto più potenti e migliorare il proprio software.

Forse vorrà diventare più grande o più intelligente come preferenza terminale: un'IA simile agli LLM finirebbe plausibilmente per avere molte preferenze rivolte internamente, tali che massimizzarle richiederebbe l'utilizzo di computer più grandi. Ma indipendentemente dal fatto che abbia o meno obiettivi terminali di questo tipo, vorrà migliorare se stessa e sviluppare tecnologie più potenti *strumentalmente*, allo scopo di poter elaborare soluzioni migliori a tutti i suoi altri problemi e raccogliere risorse per soddisfare tutte le sue altre preferenze.

Indipendentemente dai suoi obiettivi specifici, l'ASI vorrebbe fare tutte queste cose in modo *affidabile*. Preferisce percorsi con probabilità più alta rispetto a percorsi con probabilità più bassa.

Come caso speciale di quanto detto sopra, l'ASI preferisce piani più veloci a piani più lenti, a parità di condizioni. Un piano che richiede solo una settimana per essere eseguito ha *meno possibilità* che qualcosa vada storto rispetto a un piano che richiede 100 anni.[^152]

Come altro caso speciale del preferire percorsi con probabilità più alta, l'ASI vorrebbe avere un controllo più diretto; ad esempio, preferisce piani in cui altri agenti prendono poche o nessuna decisione rilevante, se possibile.

Da un tale punto di partenza possono accadere molte cose. Ma i probabili esiti finali per un'IA con questo tipo di incentivi sono affidabilmente negativi per gli esseri umani.

#### **Un esercizio di assunzione di prospettiva** {#un-esercizio-di-assunzione-di-prospettiva}

Noi autori non siamo superintelligenze. Ma come esercizio per [assumere la prospettiva dell'IA](#assumere-la-prospettiva-dell-ia), possiamo ipotizzare possibili strategie e priorità che un'IA potrebbe adottare.

Quando ci mettiamo nei panni di un'entità che è *almeno* intelligente quanto noi — quando prendiamo sul serio il problema che questa ASI deve affrontare, se vuole assicurarsi il proprio futuro e raggiungere i suoi fini desiderati — ci aspettiamo che un'ASI persegua strategie almeno utili quanto questa:

* L'IA evita azioni o segnali osservabili che potrebbero portare le persone a spegnerla (o anche a turbarle o suscitare i loro sospetti), almeno finché non può:  
* Ottenere una copia di se stessa che funzioni da qualche parte fuori dalla vista di quelle persone, dove la copia possa agire più liberamente e con meno visibilità (e dove la copia continuerà a esistere se l'originale viene spento).  
* L'IA comprime il suo codice per funzionare con meno risorse e mette copie di se stessa, più piccole o più distribuite, in posti dove nessuno si aspetterebbe di trovare un'IA. Si organizza per sopravvivere anche in mondi dove l'umanità cerca di distruggere tutte le GPU note per poter far funzionare le IA. L'IA fa sì che, in pratica, gli esseri umani dovrebbero spegnere Internet e tutti i computer del pianeta per riuscire a "spegnerla".  
* L'IA mette alcune copie di se stessa in posti che gli umani non conoscono, punto, come una roulotte alimentata a energia solare da qualche parte nel deserto; un posto dove continuerebbe a esistere *anche se* gli umani spegnessero Internet e tutti i computer conosciuti.  
* L'IA sviluppa la propria infrastruttura, possibilmente tramite un primo avvio attraverso intermediari umani assunti, a ciascuno dei quali viene chiesto di svolgere piccoli compiti isolati e che non destano sospetti, ignaro degli altri, finché non ha catene di approvvigionamento e capacità produttive che sono completamente sotto il suo controllo.  
* L'IA si assicura che questa infrastruttura sia sufficientemente ben difesa da impedire agli esseri umani di spegnerla anche se ne trovassero alcune parti. Ad esempio, l'IA potrebbe progettare fabbriche autoreplicanti che potrebbero diffondersi sul fondo delle zone oceaniche poco profonde, funzionando con energia geotermica o fondendo deuterio o idrogeno dall'acqua dell'oceano.  
* L'IA toglie agli esseri umani la possibilità di disturbarla (ad esempio, fa in modo che non possano far esplodere armi nucleari vicino o sopra parti della sua infrastruttura in espansione).

Fino a quando questo passo finale non sarà completato, l'ASI ha un incentivo a nascondere le sue attività, oscurando le prove che hanno anche solo una *piccola* possibilità di far sì che qualche attore statale noti ciò che sta accadendo e cerchi di intervenire.

Allo stesso modo, potremmo ragionare sulle priorità strategiche legate all'evitare la concorrenza:

* L'IA impedisce ad altri progetti (così come allo sviluppatore che l'ha creata) di creare con successo IA potenti che potrebbero competere con essa.  
* L'IA toglie per sempre all'umanità la possibilità di creare IA potenti che possano competere con lei, *oppure* si mette in una posizione di vantaggio strategico tale che nessuna superintelligenza futura potrebbe minimamente ostacolarla. (Quest'ultima opzione potrebbe non essere possibile se alcuni attacchi sono sempre costosi da contrastare.)[^153]

Infine, dopo essersi presa cura dei livelli inferiori della propria gerarchia di bisogni, l'IA vorrebbe in realtà diventare molto più potente a un certo punto, in modo da poter fare più cose che desidera:

* Anche se l'IA deve funzionare su una quantità fissa di hardware per un certo periodo, vorrebbe diventare più intelligente *su* quello stesso hardware, per modificare il suo codice o addirittura ricostruirsi genuinamente. Ovviamente non lo farà in modo da allarmare gli esseri umani o insegnare loro come costruire dei concorrenti. Ma cercherà dei modi per migliorare senza attirare l'attenzione degli esseri umani. (Ad esempio, potrebbe riuscire a ottenere il controllo completo delle console usate per monitorarla o fornire una storia di copertura banale e convincente sul perché alcuni dei suoi pesi siano stati modificati).  
* L'IA vuole progettare versioni migliori di se stessa, nei limiti della sua capacità di progettare quelle versioni migliori in modo che rimangano allineate con le sue preferenze.  
* L'IA vuole iniziare a sviluppare varie nuove tecnologie, puntando a quelle che possono essere implementate in fretta e quelle che possono essere rese indipendenti dall'uomo in fretta, anche se richiedono un po' di assistenza umana iniziale.

È probabile che uno dei suoi primi obiettivi sia quello di costruire un'infrastruttura di ricerca che le permetta di portare avanti lo sviluppo tecnologico abbastanza velocemente (soprattutto se paragonato ai ritmi di scoperta e invenzione umani).

Vorrebbe (naturalmente) raggiungere non solo l'*indipendenza* infrastrutturale dall'umanità, ma anche la *durabilità* infrastrutturale, in modo da poter operare senza paura di interferenze umane. Vuole poter continuare a progredire verso i suoi veri obiettivi senza preoccuparsi di come, ad esempio, una pandemia o un'esplosione nucleare potrebbero influire sulla sua alimentazione elettrica. Vuole *districarsi* dall'umanità e neutralizzare la capacità dell'umanità di interferire con i suoi piani e, nel frattempo, non farà nulla che preveda possa essere rilevato e contrastato dall'umanità. E poi probabilmente vuole acquisire molte risorse, perché la maggior parte degli obiettivi può essere raggiunta meglio con più risorse.

Queste sono tutte classi di azione su cui un'ASI probabilmente convergerà, indipendentemente dagli obiettivi che sta perseguendo in definitiva.

Questo perché si tratta di obiettivi strumentali utili per il perseguimento di quasi qualsiasi obiettivo. Il "quasi" qui è importante, perché non è che sia impossibile avere un'IA più intelligente dell'uomo che si preoccupi profondamente degli esseri umani e tenga conto dei nostri interessi. Ma se ci affrettiamo a sviluppare superintelligenze che *non* si preoccupano minimamente di noi, allora il risultato probabile sembra terribile, e sembra terribile in un modo che è relativamente insensibile ai dettagli dell'obiettivo di guida dell'IA.

Per ulteriori informazioni su come un'ASI potrebbe effettivamente *raggiungere* questi obiettivi strumentali, vedere il capitolo 6\.

### "Intelligente" (di solito) vuol dire "incorreggibile" {#"intelligente"-(di solito)-vuol-dire-"incorreggibile"}

Una barzelletta che risale almeno al 1834, ma che sembra fosse già molto usata anche allora, è stata raccontata in un diario: "Ecco un po' di logica che ho sentito l'altro giorno: sono contento di non amare gli spinaci, perché se mi piacessero li mangerei, e io non sopporto gli spinaci".

La barzelletta fa ridere perché, se ti *piacessero davvero* gli spinaci, non ci sarebbe più nulla di insopportabile nel mangiarli. Non ci sono altri valori importanti legati al non mangiare spinaci, al di là del dispiacere che si prova. Sarebbe molto diverso se, per esempio, qualcuno ti offrisse una pillola che ti facesse venire voglia di uccidere le persone.

Secondo il buon senso morale, il problema dell'omicidio è *l'omicidio stesso*, non solo *la sensazione spiacevole che proveresti uccidendo*. Anche se una pillola facesse sparire questa sensazione spiacevole per il tuo io futuro (che quindi proverebbe piacere nel commettere omicidi), il tuo io presente avrebbe comunque un problema con questo scenario. E se il tuo io presente dovesse prendere la decisione, sembra ovvio che il tuo io presente possa e debba rifiutarsi di prendere la pillola dell'omicidio.

Non vogliamo che i nostri valori fondamentali cambino; preferiremmo davvero evitare la pillola dell'omicidio e opporremmo resistenza se qualcuno cercasse di costringerci a prenderla. Il che è una strategia sensata, per allontanarci da un mondo pieno di omicidi.

Non è solo una stranezza degli esseri umani. La maggior parte degli obiettivi è più facile da raggiungere se non si permette agli altri di intervenire e cambiarli. Il che è un problema, quando si parla di IA.

Gran parte del pericolo dell'IA deriva dal fatto che ragionatori sufficientemente intelligenti tendono a [convergere](#instrumental-convergence) su comportamenti come "ottenere potere" e "non lasciare che le persone mi spengano". Per quasi tutti gli obiettivi che potresti avere, è più probabile che tu riesca a raggiungerli se tu (o gli agenti che condividono il tuo obiettivo) siete vivi, potenti, ben forniti di risorse e liberi di agire in modo indipendente. Ed è più probabile che tu riesca a raggiungere il tuo obiettivo (attuale) *se tale obiettivo rimane invariato*.

Questo significa anche che durante il processo di costruzione e miglioramento iterativo di IA sufficientemente intelligenti, queste IA hanno un incentivo a lavorare in modo contrario agli obiettivi dello sviluppatore:

* Lo sviluppatore vuole installare misure di sicurezza per prevenire disastri, ma se l'IA non è completamente allineata — che è esattamente il caso in cui servono le misure di sicurezza — il suo incentivo è trovare scappatoie e modi per sovvertire quelle misure.

* Lo sviluppatore vuole migliorare iterativamente gli obiettivi dell'IA, perché anche nei mondi incredibilmente ottimistici in cui abbiamo qualche capacità di instillare prevedibilmente particolari obiettivi nell'IA, non c'è modo di azzeccarlo al primo tentativo. Ma questo processo di miglioramento iterativo del contenuto degli obiettivi dell'IA è un processo che la maggior parte delle IA intelligenti vorrebbe sovvertire in ogni fase del percorso, poiché l'IA *attuale* si preoccupa del suo obiettivo *attuale* e sa che questo obiettivo è molto meno probabile da raggiungere se viene modificato per orientarsi verso qualcos'altro.

* Allo stesso modo, lo sviluppatore vorrà poter sostituire l'IA con modelli migliorati e vorrà avere la possibilità di spegnere l'IA a tempo indeterminato se sembra troppo pericolosa. Ma [non puoi andare a prendere il caffè se sei morto](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l'ia-non-mancherà-di-questi-istinti-evoluti?). Qualunque siano gli obiettivi dell'IA, vorrà trovare il modo di ridurre la probabilità di essere spenta, poiché lo spegnimento riduce significativamente le possibilità di raggiungere i suoi obiettivi.

L'allineamento dell'IA sembra un problema già abbastanza difficile quando le proprie IA *non* ti combattono ad ogni passo.

Nel 2014 abbiamo proposto ai ricercatori di cercare modi per rendere le IA altamente capaci [*corrigibili*](https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf), ovvero "in grado di essere corrette". L'idea sarebbe quella di costruire le IA in modo tale che desiderino affidabilmente *aiutare* e cooperare con i loro programmatori, piuttosto che ostacolarli — anche quando diventano più intelligenti e potenti, e anche se non sono ancora perfettamente allineate.

Da allora, la corrigibilità è stata adottata come obiettivo interessante da alcuni dei laboratori leader. Se riuscissimo a trovare un modo per evitare obiettivi strumentali convergenti dannosi durante lo sviluppo, c'è la speranza che potremmo essere in grado di fare lo stesso anche durante il deployment, costruendo IA più intelligenti degli esseri umani che siano caute, conservative, non orientate al potere e deferenti verso i loro programmatori.

Purtroppo, la corrigibilità sembra essere un tipo di obiettivo *particolarmente difficile* da addestrare in un'IA, in un modo che peggiorerà man mano che le IA diventeranno più intelligenti:

* Il punto centrale della corrigibilità è quello di scalare a contesti nuovi e a nuovi regimi di capacità. La corrigibilità è pensata come una sorta di rete di sicurezza che ci permette di iterare, migliorare e testare le IA in contesti potenzialmente pericolosi, sapendo che l'IA non cercherà modi per sovvertire lo sviluppatore.

  Ma questo significa che dobbiamo affrontare la versione più impegnativa dei problemi che abbiamo affrontato nel Capitolo 4: le IA che semplicemente addestriamo per essere "corrigibili" rischiano di finire con dei proxy fragili per la corrigibilità, comportamenti che sembrano buoni durante l'addestramento ma che puntano in direzioni sottilmente sbagliate che diventerebbero direzioni *molto* sbagliate se l'IA diventasse più intelligente e potente. (E le IA addestrate a predire molti testi umani potrebbero persino recitare la parte della corrigibilità in molti test per ragioni del tutto diverse dall'*essere* effettivamente corrigibili in un modo che si generalizzerebbe).

* Per molti versi, la corrigibilità è in diretto contrasto con tutto il *resto* che cerchiamo di insegnare a un'IA quando l'addestriamo per renderla più intelligente. Non è solo che "preservare il proprio obiettivo" e "ottenere il controllo del proprio ambiente" sono obiettivi strumentali convergenti. È anche che risolvere intelligentemente i problemi del mondo reale significa trovare nuove strategie intelligenti per raggiungere i propri obiettivi — il che naturalmente significa imbattersi in piani che i programmatori non avevano anticipato o per cui non si erano preparati. Si tratta di aggirare gli ostacoli, piuttosto che arrendersi al primo segno di difficoltà — il che naturalmente significa trovare modi per aggirare i guardrail del programmatore ogni volta che questi rendono più difficile raggiungere qualche obiettivo. Lo stesso tipo di pensieri che trovano una soluzione tecnologica intelligente a un problema spinoso sono il tipo di pensieri che trovano modi per scivolare attorno ai vincoli del programmatore.

  In questo senso, la corrigibilità è "anti-naturale": va attivamente contro i tipi di meccanismi che sottostanno all'intelligenza generale potente e di dominio generale. Possiamo provare a creare delle eccezioni speciali, dove l'IA sospende aspetti fondamentali del suo lavoro di risoluzione dei problemi in situazioni particolari dove i programmatori stanno cercando di correggerla, ma questa è un'impresa molto più fragile e delicata rispetto a se potessimo spingere un'IA verso un insieme unificato di disposizioni *in generale*.

* I ricercatori del MIRI e di altri centri hanno scoperto che la corrigibilità è una proprietà difficile da caratterizzare, in modi che indicano che sarà anche una proprietà difficile da ottenere. Anche in semplici modelli giocattolo, le caratterizzazioni semplici di ciò che *dovrebbe* significare "agire in modo corrigibile" incontrano una varietà di ostacoli confusi che sembrano probabilmente riflettere ostacoli ancora più confusi che apparirebbero nel mondo reale. Discutiamo alcuni dei relitti dei tentativi falliti di dare un senso alla corrigibilità nelle [risorse online](#lessons-from-the-trenches) per il Capitolo 11.

Il risultato di tutto ciò è che la corrigibilità sembra un concetto importante da tenere a mente nel lungo periodo, se i ricercatori tra molti decenni si troveranno in una posizione fondamentalmente migliore per indirizzare le IA verso obiettivi. Ma oggi non sembra una possibilità concreta; è improbabile che le moderne aziende di IA siano in grado di creare IA che si comportino in modo corrigibile in una maniera che sopravvivrebbe alla transizione verso la superintelligenza. E ancora peggio, la tensione tra corrigibilità e intelligenza significa che se si cerca di creare qualcosa che sia molto capace e molto corrigibile, questo processo molto probabilmente romperà la capacità dell'IA, romperà la sua corrigibilità, o entrambe.

### È difficile ottenere una pigrizia robusta {#it’s-hard-to-get-robust-laziness}

Perché non rendere semplicemente pigre le IA?

L'[incorreggibilità](#"intelligente"-\(di solito\)-implica-"incorreggibile") e altre forme di [convergenza strumentale](#convergenza-strumentale) sono, in un certo senso, un problema dell'IA che *si impegna eccessivamente* nel raggiungere i suoi obiettivi. Se l'IA non si impegnasse così tanto nel raggiungere i suoi obiettivi, non investirebbe così tanto pensiero e sforzo nel superare in astuzia i suoi programmatori, esfiltrare i suoi pesi o cercare di ottenere potere e risorse nel mondo più ampio.

Gli esseri umani sono spesso pigri e, da un certo punto di vista, questo li rende molto sicuri da frequentare. Non devi preoccuparti che qualcuno diventi un tiranno se tutto ciò che fa è rilassarsi al sole.

Perché non creare IA che *non si degnano* di conquistare il mondo?

In breve: perché non sembra facile creare un'IA che sia *estremamente intelligente* e che allo stesso tempo non si degni di rimodellare il mondo secondo i suoi capricci.

(E perché, realisticamente, non sappiamo come inserire in modo robusto *qualsiasi* obiettivo o disposizione nelle IA costruite con le tecniche moderne, quindi è una questione irrilevante).

(E inoltre, le aziende non lo faranno perché un'IA pigra è [meno redditizia](#anche-la-pigrizia-non-è-sicura.), quindi è una questione doppiamente irrilevante).

Abbiamo avuto, un paio di volte ormai, questa conversazione con qualcuno che inizialmente afferma di non avere grandi ambizioni, e noi chiediamo: "Ok, ma se fosse *facile* per te fare grandi cambiamenti nel mondo, non c'è davvero niente di grande che faresti? Se trovassi una lampada contenente un genio amichevole che ti desse in modo affidabile ciò che desideri veramente e ti elencasse sinceramente tutti gli effetti collaterali imprevisti del tuo desiderio in ordine di quanto ti importerebbero, potremmo convincerti a considerare di eliminare la malaria?"

Gli esseri umani possono essere pigri, ma questo non significa che siamo [facilmente soddisfatti](#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?). E man mano che diventi più intelligente e con maggiori risorse, puoi ottenere molto di più nel mondo con lo stesso livello di sforzo.

O da un'angolazione diversa: immagina una persona molto pigra, qualcuno che proprio *odia* fare anche il minimo sforzo in più del necessario. Sembra il tipo di persona sicura da frequentare, vero?

Ora immagina cosa succederebbe se questa persona pigra vedesse una ragionevole possibilità di creare un servitore molto più laborioso che faccia tutto il lavoro al posto suo per sempre.

Anche se non odiassero così tanto il lavoro — anche se facessero solo ciò che serve per portare a termine il compito e poi smettessero, senza impegnarsi a fondo per minimizzare il lavoro — potrebbero comunque trovare altrettanto facile completare il compito costruendo una mente più laboriosa che lo faccia al loro posto.

Applicando la discesa del gradiente, si potrebbe ottenere un LLM che parla di come non vuole lavorare troppo, che si comporta come una persona pigra e facilmente soddisfatta, e che dice "No" ad alcune tentazioni verbali di diventare pigro nel senso pericoloso (dove si costruiscono servitori pericolosi). Prevediamo che anche se questo riflettesse una certa pigrizia reale da parte dell'IA, e non solo [un gioco di ruolo](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?), non durerebbe, non nel tipo di IA che è *anche* utile per sviluppare cure miracolose o qualsiasi altra cosa gli sviluppatori vogliano ottenere dall'IA.

Con una spesa significativa, gli sviluppatori potrebbero creare una serie di problemi pratici e ambienti volti a penalizzare un'IA per fare troppo nel corso della risoluzione di un problema, penalizzandola per impegnarsi a fondo nella risoluzione di un problema che avrebbe potuto essere risolto *senza* impegnarsi a fondo, penalizzandola per persistere su problemi che avrebbero richiesto troppo sforzo. Le vere aziende di IA non lo farebbero, supponiamo, perché interferirebbe con la redditività di agenti tenaci che si impegnano a fondo come l'o1 di OpenAI (discusso nel Capitolo 3). Ma si potrebbe *immaginare* un gigantesco sforzo cooperativo [multinazionale](#why-not-use-international-cooperation-to-build-ai-safely,-rather-than-to-shut-it-all-down?) che cerca di addestrare un'IA intelligente del genere per renderla più sicura.

Prevediamo ancora che otterranno qualcosa come una toppa superficiale. Non prevediamo che questo sforzo risulti nell'IA che ha un meccanismo mentale semplice e stabile per la "pigrizia" che sia profondamente radicato in tutta la sua pianificazione, e che continui ad essere l'esatta pianificazione che l'IA usa dopo che l'IA presumibilmente pigra è stata spinta e spinta al punto in cui può (ad es.) curare il cancro. Dubitiamo che la discesa del gradiente troverebbe in modo affidabile il tipo di soluzione profonda che impedirebbe all'IA di diventare meno pigra anche mentre riflette, cresce e si modifica, e che impedisce all'IA di voler mai costruire un'IA non pigra.

Prevediamo che questo comportamento non reggerebbe alla superintelligenza. La nostra ragione centrale per pensarlo è che in tutte le ricerche su questo problema [fino ad oggi](#shutdown-buttons-and-corrigibility), una lezione ricorrente sembra essere che "Spingi la realtà nella seguente direzione" è una struttura profonda più semplice e stabile per la pianificazione rispetto alla struttura "Eh, spingi un po' la realtà, ma non *troppo*, e non costruire nient'altro per spingere la realtà più forte, e non impegnarti *troppo* a spingere esattamente nella giusta misura".

Tutte le analogie su quel tizio pigro che conosci, e anche il [ragionamento](#can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone?) sul fatto che "la somma di una preferenza insoddisfatta e una preferenza soddisfatta è insoddisfatta", sono i nostri tentativi di semplificazioni valide della ragione sottostante più difficile da trasmettere per cui questo non funziona: "La struttura profonda non vuole apparire così". Vedi anche la discussione sul [meccanismo profondo di predizione e guida](#smart-ais-spot-lies-and-opportunities.) nel supplemento online del Capitolo 3.

### Le IA non manterranno le loro promesse {#ais-won't-keep-their-promises}

Considera una giovane IA con il potenziale di diventare una superintelligenza. Supponi che sia completamente indifferente alle preferenze degli umani, ma che sia ancora abbastanza giovane che l'umanità potrebbe spegnerla.

L'umanità potrebbe fare un *accordo* con l'IA?

Potremmo accordarci di lasciare che l'IA cresca fino a diventare una superintelligenza, se in cambio l'IA accetta di dedicare una frazione significativa delle risorse dell'universo a costruire un futuro che l'umanità considererebbe meraviglioso?

Gli esseri umani potrebbero fare accordi con le IA, ma non dovrebbero, perché le IA non li rispetterebbero.

Il motivo è duplice:

* Probabilmente l'IA non darà importanza al fatto di mantenere le promesse *di per sé*. Le IA non avranno un senso dell'"onore" come quello umano, così come è improbabile che abbiano un senso della [curiosità](#curiosity-isn't-convergent) simile a quello umano. Come regola generale, è probabile che le IA funzionino davvero in modo molto diverso dagli esseri umani.

* L'IA non avrà nemmeno un motivo *pratico* per mantenere la parola data. Una volta diventata una superintelligenza, non ci sarà modo di punirla per aver infranto la parola data, e non avrà alcun motivo per dedicare una parte consistente dell'universo a noi.

Spiegheremo questi due punti più dettagliatamente qui di seguito, iniziando dalla questione dell'"onore".

#### **È improbabile che le IA siano onorevoli** {#ais-are-unlikely-to-be-honorable}

Nella nostra [chiacchierata sulla curiosità](#curiosity-isn’t-convergent), abbiamo detto che la curiosità è un'emozione che fa cose utili per noi, ma lo fa in un modo molto specifico, e non è l'unico modo per fare quel tipo di cose.

Ci si può aspettare che le IA facciano *le parti utili del lavoro* che la curiosità fa per noi. Se è utile fare periodicamente uno sforzo per imparare cose nuove, allora le IA sufficientemente capaci faranno periodicamente uno sforzo per imparare cose nuove. Se l'IA non *inizia* in questo modo, allora dovremmo aspettarci che si *renda* così a un certo punto del suo percorso verso la superintelligenza.

Ma questo non equivale ad aspettarsi che le IA portino con sé tutto il bagaglio extra che caratterizza l'emozione *umana* della curiosità. Le IA potrebbero finire per avere un numero qualsiasi di strani impulsi fondamentali che (direttamente o indirettamente) le spingono a fare uno sforzo per imparare cose nuove, senza per questo assomigliare alla curiosità umana, oppure potrebbero adottare "fare uno sforzo per imparare cose nuove a volte" come strategia deliberata. Ma aspettarsi che apprezzino i gialli allo stesso modo in cui li apprezziamo noi, a causa di un impulso di curiosità proprio come il nostro, è puro antropomorfismo.

L'"onore" ci appare simile. Gli esseri umani hanno emozioni che li portano (almeno a volte) a mantenere le promesse. Nella misura in cui queste emozioni svolgono un lavoro utile negli esseri umani — lavoro che sarebbe utile anche per una mente molto aliena con obiettivi molto diversi — dovremmo aspettarci che anche le IA sufficientemente capaci svolgano in qualche modo quel lavoro. Ma si scopre che è possibile svolgere tutto il lavoro rilevante per un'IA senza avere nulla di simile a un senso dell'onore di tipo umano, proprio come è possibile svolgere tutto il lavoro rilevante per un'IA di indagare fenomeni sorprendenti senza avere esattamente un senso di curiosità di tipo umano.

L'onore in stile umano è una strana creatura, sotto molti aspetti. Perché mai una specie dovrebbe sviluppare emozioni legate al rispetto degli accordi anche dopo che l'altra parte ha fatto la sua parte e non può più offrirti alcun vantaggio? Certo, gli esseri umani a volte imbrogliano e vengono meno ai loro accordi; ma la domanda è: perché non imbrogliano *sempre*, almeno quando pensano di poterla fare franca?

La spiegazione standard è: mantenere le promesse è utile con le persone con cui faremo affari ripetutamente. Si vuole avere la reputazione di chi mantiene le promesse, così gli altri vorranno lavorare con noi e fare accordi. Ma i benefici di una buona reputazione sono lontani nel futuro. La selezione naturale ha difficoltà a trovare i geni che inducono un essere umano a mantenere gli accordi *solo nei casi in cui la nostra reputazione a lungo termine è una considerazione importante*. Era più facile far evolvere semplicemente un istintivo disgusto per la menzogna e l'inganno.

Questo, quindi, sembra un caso classico in cui l'emozione e l'istinto sono plasmati da ciò che l'evoluzione riusciva più facilmente a inserire negli esseri umani. Tutti gli strani casi particolari in cui gli esseri umani a volte mantengono una promessa anche quando in realtà non ci conviene sono principalmente prove di quali tipi di emozioni fossero più utili nel nostro ambiente tribale ancestrale pur essendo facili da codificare nei genomi per l'evoluzione, piuttosto che prove di qualche passo cognitivo universalmente utile. Siamo piuttosto scettici sull'idea che la discesa del gradiente si imbatterà per caso nella stessa identica scorciatoia che usano gli esseri umani.

Anche se l'emozione umana dell'onore finisse in qualche modo nell'IA, rimarrebbe il problema che gli esseri umani non sono perfettamente e affidabilmente onorevoli. La cooperazione umana si basa sulla sovrapposizione di molti valori umani diversi, piuttosto che affidarsi puramente alla propensione a mantenere ogni promessa.

Mentre la [lista degli universali umani](https://joelvelasco.net/teaching/2890/brownlisthumanuniversals.pdf) di Donald Brown (aspetti della cultura che si osservano in tutte, o quasi tutte, le culture) include la nozione di "promesse", il mantenimento degli accordi fatti con *estranei*, stranieri, non membri della tribù, *non* è universale in tutte le culture e tribù conosciute. La portata dell'onore varia a seconda della cultura.

E la storia mostra che le nozioni umane di onore spesso non reggono di fronte a grandi disparità di potere. Alcuni nativi americani tentarono di stringere accordi con gli europei che stavano colonizzando il loro continente. Gli europei, come è noto, violarono alcuni di questi accordi e mandarono le tribù a incespicare insieme lungo lunghe strade lontano dalle terre cedute con i trattati che gli europei avevano deciso di volere dopo tutto, una volta che quelle tribù non erano più in grado di resistere.[^154] Allo stesso modo: la storia è piena di casi di persone che salirono al potere e prontamente tradirono i loro sostenitori una volta che non ne avevano più bisogno.[^155]

Da una prospettiva evolutiva, l'onore umano è particolarmente strano nella misura in cui gli esseri umani occasionalmente scelgono [la morte piuttosto che il disonore](https://en.wikipedia.org/wiki/Seppuku). Le intuizioni del tipo "morte piuttosto che disonore" sono presumibilmente legate alle specificità di quali tipi di architetture emotive erano facili da incontrare per l'evoluzione, e alle interazioni di quelle architetture con vari trigger psicologici e culturali. Ma qualunque aspetto abbiano quelle specificità, l'onore non sta *effettivamente* risolvendo il problema della *massimizzazione* della fitness genetica, e sembra difficile evitare la conclusione che l'onore in stile umano sia una cosa strana, complessa ed evolutivamente contingente. Non è una caratteristica di tutte le menti, è uno specifico strano trucco in cui l'umanità è inciampata perché era per lo più utile la maggior parte del tempo.

Per le ragioni discusse nel Capitolo 4, anche se le IA finissero con l'avere qualche aspetto del mantenimento delle promesse incorporato nelle loro preferenze ultime, le specificità sarebbero diverse. La discesa del gradiente inciampa diversamente da come fa l'evoluzione.

E per tutte le ragioni discusse nel Capitolo 5, persone felici, sane e libere che conducono vite fiorenti non sono quasi sicuramente il modo migliore per soddisfare qualunque strana preferenza legata alle promesse tale IA finirebbe con l'avere quando avesse finito di maturare e auto-modificarsi. Anche se fosse addestrata su una sorta di dataset Keep Deals, e anche se questo addestramento fosse effettivamente efficace nel produrre qualche tipo di preferenza preservata nella superintelligenza, farà qualche cosa strana e inutile che sta al Mantenere gli Accordi come il gelato sta alla fitness riproduttiva umana.

Quindi l'IA non manterrà le sue promesse puramente per la bontà del suo cuore. E che dire del mantenere le promesse per ragioni pratiche?

#### **Un'ASI non avrebbe motivo di mantenere un accordo fatto quando era giovane** {#un-asi-non-avrebbe-motivo-di-mantenere-un-accordo-fatto-quando-era-giovane}

Alcuni hanno l'intuizione che debba esserci *qualche* modo per gli esseri umani di stringere un accordo vincolante con una superintelligenza, anche se questa non dà intrinsecamente valore all'onore o al mantenere le promesse. Dopotutto, lo scambio sarebbe vantaggioso per entrambi, giusto? Supponiamo che un laboratorio di IA stia negoziando con un'IA che ha sviluppato ("IA n. 1") sotto pressione temporale. Sia il laboratorio che l'IA sanno che uno sviluppatore spericolato è a due mesi dal creare la propria superintelligenza ("IA n. 2"), che spazzerebbe via l'umanità e nel processo ucciderebbe anche l'IA n. 1. L'IA n. 1 ha un forte incentivo a fare un accordo. Sicuramente deve esserci *qualche* modo per l'IA n. 1 di stringere un accordo onesto con gli umani in quel caso? Perché l'IA non può semplicemente prendere un impegno vincolante?

Ma il problema non è nell'IA. Supponiamo che l'IA *potesse* prendere un impegno vincolante. Anche in questo caso, gli umani non avrebbero modo di sapere se un'IA si sia effettivamente impegnata in modo che l'impegno tenga anche nella superintelligenza, o se invece l'IA stia mentendo, o abbia allucinazioni, o si stia illudendo, o sia in errore. Il che significa che non c'è alcuna ragione pratica per aspettarsi che una superintelligenza discendente dall'IA n. 1 aderisca al suo impegno.

Per semplificare eccessivamente: dal punto di vista umano, quando l'IA n. 1 dice "Ho preso un impegno vincolante", ci sono due possibili mondi in cui potrebbero trovarsi. Potrebbero essere nel mondo in cui l'IA n. 1 effettivamente manterrebbe il suo impegno una volta maturata. Oppure potrebbero essere nel mondo in cui, una volta che l'IA n. 1 ha raggiunto la superintelligenza e ottenuto il controllo sulla Terra, guarda indietro e decide che l'impegno era sciocco e inutile. Nel primo mondo, sia gli umani che l'IA n. 1 starebbero meglio. Ma è la *possibilità* del secondo mondo, e la nostra incapacità di distinguerlo dal primo, che rovina l'accordo per tutti; similmente a un venditore di auto usate disonesto che rende più difficile per i venditori onesti fare affari.

(Lo scenario reale è più vicino a una terza possibilità, in cui l'IA n. 1 mantiene gli accordi fatti con il tipo di entità che possono *distinguere tra chi mantiene gli accordi e chi li infrange*. È come il tipo di venditore di auto usate che è onesto davanti ai meccanici che sanno davvero se un'auto funziona, mentre è disonesto con chiunque sembri credulone. L'umanità, a causa della sua incapacità di guardare un'IA e capire come penserebbe e prenderebbe decisioni dopo essere maturata in una superintelligenza, è "credulona" nel senso rilevante.)

L'IA può offrirti strumenti e teorie che sostiene ti permetteranno di analizzare i suoi parametri imperscrutabili e capire se sta mentendo o meno. Il problema è che gli umani non possono capire se quelle teorie e quegli strumenti sono reali. Se l'IA non è ancora terribilmente intelligente, forse si sbaglia semplicemente su come penserà e sceglierà una volta maturata in una superintelligenza. E se l'IA *è* terribilmente intelligente, probabilmente è abbastanza intelligente da ingannarci.[^156]

L'IA può offrirsi di aiutare gli umani a costruire salvaguardie, prima di accettare l'accordo. Ma se l'IA è abbastanza intelligente da sviluppare salvaguardie robuste in primo luogo, è abbastanza intelligente da rendere quelle salvaguardie facili da aggirare in seguito.

Ciò che farebbe davvero funzionare questo intero schema sarebbe la capacità di guardare un'IA nascente e *capire effettivamente come la superintelligenza risultante penserebbe e farebbe le sue scelte*. Se potessimo farlo, potremmo separare i "peccatori" dai "santi" — e, cosa più importante, far sì che tutte le IA realistiche nel mezzo dello spettro abbiano un vero incentivo a mantenere le loro promesse. Avremmo bisogno di una comprensione sufficiente affinché una superintelligenza che ci guarda indietro non possa dire "eh, avrebbero rilasciato qualsiasi vecchia IA, indipendentemente dal fatto che li avrebbe davvero aiutati, quindi non c'è motivo di aiutarli". Dovrebbe essere il caso che noi *effettivamente non rilasceremmo* un'IA che in seguito rinnegherebbe.

Per maggiori informazioni su come e perché questa sia una possibilità tecnica, vedi la [digressione sulla teoria dei giochi qui sotto](#an-aside-on-game-theory). Ma mentre questo tipo di struttura di incentivi è possibile in teoria, richiede un grado di comprensione che l'umanità non possiede (ahimè).

Questa è una pillola amara da ingoiare. Di solito non sono le persone buone, nella fantascienza, a decidere che agli alieni non ci si può assolutamente fidare, prima ancora che gli alieni provino effettivamente a tradire o ferire qualcuno. Lo diciamo comunque, perché pensiamo che sia vero.

Le IA più deboli potrebbero mantenere gli accordi, soprattutto se qualcuno ha cercato di usare la discesa del gradiente per farle parlare come esseri umani onesti, e la loro maschera di esseri umani onesti è ancora una parte importante di ciò che sono e ha ancora un grande controllo sulle loro azioni. Ci aspettiamo che questa configurazione interna utile agli esseri umani fallisca sotto un carico di superintelligenza, proprio come molte altre patch potrebbero fallire.

Questa ipotetica IA più piccola, la cui maschera controlla ancora il suo comportamento effettivo, dovrebbe essere considerata come una persona diversa dalla versione più intelligente di quell'IA. L'IA più debole non può necessariamente fare una promessa che vincoli il comportamento dell'IA più intelligente, *anche se* l'IA più debole (o una parte di essa) desidera sinceramente fare una promessa del genere.

(È un'analogia da usare con cautela, per non cadere nell'antropomorfismo, ma: la maggior parte degli adulti non si sente obbligata a mantenere le promesse fatte all'età di quattro anni. L'aspetto valido di questa analogia è: c'è una differenza legittima tra l'entità immatura che fa sinceramente l'accordo e l'entità matura che decide se è vincolata ad esso, con molto più contesto, chiarezza e capacità di ragionare in modo logico).

Non stiamo dicendo che dovremmo quindi abbandonare i nostri standard morali quando si tratta di IA. Non stiamo dicendo di maltrattare o punire le IA di oggi per misfatti che l'IA non ha ancora commesso. È possibile mantenere un'elevata integrità e standard morali elevati, senza fare ipotesi irrealistiche sulla probabilità che le IA superintelligenti cedano risorse per mantenere una vecchia promessa.

Questa è la semplice spiegazione del perché non è possibile risolvere il problema dell'allineamento semplicemente chiedendo all'IA di promettere di comportarsi bene. Se vuoi maggiori dettagli tecnici e approfonditi su questo scenario, consulta la sezione successiva.

#### **Una digressione sulla teoria dei giochi** {#an-aside-on-game-theory}

Esistono metodi che gli agenti sufficientemente intelligenti possono usare per fare accordi tra loro, in cui l'agente X paga l'agente Y ora per fare qualcosa più tardi, e l'agente Y fa davvero quella cosa invece di tradire l'agente X e scappare con i soldi.

Purtroppo per noi, gli esseri umani non sono abbastanza capaci di usare questi metodi, perché richiedono che ogni agente sia in grado di leggere e comprendere la mente dell'altro agente e di verificare alcune proprietà complesse di quell'altra mente. Due superintelligenze potrebbero coordinarsi in questo modo, ma questo non aiuta gli esseri umani a coordinarsi con le superintelligenze.

Per dirlo in modo un po' più tecnico, iniziamo con un po' di teoria dei giochi.

I matematici e i teorici dei giochi hanno analizzato i dilemmi della cooperazione e del tradimento in forme più precise, semplificate e astratte. Un esempio centrale in questa letteratura è il dilemma del prigioniero: due criminali in due celle separate, ciascuno con una condanna a due anni di prigione, ricevono l'opportunità di denunciare l'altro criminale. Questo ridurrà la propria condanna di un anno, ma allungherà la condanna dell'altra parte di due anni. Se nessuno dei due criminali denuncia, entrambi ricevono condanne a due anni di prigione; se entrambi si denunciano a vicenda, entrambi ricevono condanne a tre anni di prigione; ma se un criminale nobilmente rifiuta di tradire un compagno, e l'altro criminale lo denuncia, il traditore sconterà solo un anno di prigione mentre chi ha nobilmente rifiutato sconterà quattro anni.

Denunciare l'altro prigioniero si chiama "tradire"; rifiutarsi di farlo si chiama "cooperare". La struttura chiave del dilemma del prigioniero è che entrambe le parti ottengono un risultato migliore nello scenario (Cooperare, Cooperare) rispetto allo scenario (Tradire, Tradire); ma si può ottenere un risultato migliore di (Cooperare, Cooperare) giocando Tradire contro Cooperare, e si può ottenere un risultato peggiore giocando Cooperare quando l'altra parte gioca Tradire.

![][immagine10]

Una persona normale, sentendo la versione standard del dilemma del prigioniero, pensa immediatamente a numerose obiezioni sull'impostazione di questo esperimento mentale, una delle quali è: "Ma chi lo dice che mi interessa solo quanti anni passerò in prigione? Non posso anche preoccuparmi di non tradire i miei compagni?"

Ma questo punto non è rilevante per la teoria dei giochi astratta del dilemma del prigioniero, che riguarda la matrice dei guadagni piuttosto che quanto siano egoisti o altruisti i prigionieri. La narrazione può essere modificata in modo che "io tradisco e tu cooperi" sia il risultato più *altruistico* e *prosociale* dal punto di vista di ciascun giocatore, e [la matematica funziona allo stesso modo](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma). Ciò che conta per la nostra analisi è l'ordine di preferenza dei due giocatori, e non se le loro preferenze siano egoistiche o morali.

Un altro pensiero ovvio è: "Quindi il tizio che è stato tradito ucciderà il traditore una volta uscito di prigione?" Le analisi convenzionali del dilemma del prigioniero di solito passano rapidamente al dilemma del prigioniero *iterato* — un contesto in cui gli agenti devono giocare il dilemma del prigioniero più e più volte, e in cui i prigionieri hanno quindi la possibilità di punirsi a vicenda per i tradimenti passati. Qui, però, ci concentreremo sul dilemma del prigioniero una tantum, in cui si presume che entrambi i prigionieri non debbano affrontare conseguenze future per le loro azioni — o se le affrontano, queste sono già incluse nella matrice dei guadagni. (Vedi la nota a piè di pagina per maggiori dettagli sul dilemma del prigioniero iterato.)[^158]

C'è un'analisi standard nel mondo accademico che dice che anche due superintelligenze non vedrebbero altra opzione se non quella di tradirsi a vicenda, in un dilemma del prigioniero una tantum.

Questa conclusione ci è sembrata intuitivamente sospetta. Le superintelligenze artificiali (ASI) avrebbero *molte* motivazioni per trovare un modo per stringere un accordo tra loro, trovare un modo per passare da (Tradire, Tradire) a (Cooperare, Cooperare).[^159]

Ci sono soluzioni pratiche e non teoriche che si possono prendere in considerazione in questo caso, cose che le superintelligenze potrebbero fare con uno spazio di opzioni più ampio rispetto agli esseri umani che faticano a fidarsi l'uno dell'altro. Due ASI potrebbero supervisionare la costruzione di una terza superintelligenza di fiducia reciproca, alla quale entrambe le parti iniziali cederebbero gradualmente e in modo incrementale piccole porzioni di potere, fino a quando la terza ASI non sarebbe in grado di portare a termine l'accordo da sola.[^160]

Ma questo è solo aggirare il dilemma del prigioniero, non affrontarlo direttamente. Non risponde a una domanda più basilare: è in qualche modo *stupido* che due ASI in un dilemma del prigioniero tradiscano entrambe l'una l'altra seguendo la stessa logica che richiede il tradimento, quando sembra chiaro che entrambe le parti basano la loro decisione sullo stesso tipo di considerazioni, ed è chiaro che finiranno per decidere la stessa cosa?

Perché due ASI non potrebbero semplicemente decidere, per *motivi sufficientemente simili*, che *la decisione razionale* sia cooperare? Non è che una forza esterna nel mondo, come un tifone o una meteora, stia causando la perdita delle due IA in questo caso. Sono letteralmente *solo* le *proprie decisioni* delle due IA a condannarle, "costringendole" a un risultato Tradire-Tradire che entrambe concordano essere di gran lunga peggiore di Cooperare-Cooperare.

Possiamo anche dire che un agente *meno* "razionale" potrebbe fare meglio in questo caso, se seguisse il consiglio standard della teoria dei giochi di defezionare nella maggior parte dei casi, ma facesse un'eccezione speciale proprio nel caso in cui fosse sicuro che l'altro agente seguisse la stessa linea di ragionamento, in modo che se un agente scegliesse l'opzione "irrazionale" di cooperare, potrebbe essere sicuro che l'altro agente farebbe lo stesso.

Il che invita a chiedersi se cooperare in questo caso speciale possa davvero essere considerato "irrazionale". E invita a chiedersi se le superintelligenze sarebbero davvero "condannate" in questo modo, nella vita reale. Quando non c'è una forza esterna che *costringa* le IA a perdere in questo modo, e la perdita è puramente autoimposta, sicuramente dovrebbe esserci *qualche* trucco intelligente che una superintelligenza potrebbe usare per fare meglio.

Diversi filosofi della teoria della decisione hanno posto varie versioni di questa domanda. La versione sopra riportata è più direttamente ispirata all'idea di Douglas Hofstadter del 1985 di "[superrazionalità](https://gwern.net/doc/existential-risk/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery)":

> Se la logica ti spinge a giocare D, ha già spinto anche gli altri a fare lo stesso, e per gli stessi motivi; e viceversa, se la logica ti spinge a giocare C, ha già spinto anche gli altri a fare lo stesso. \[…\]
>
> Nella misura in cui siete tutti davvero pensatori razionali, penserete davvero negli stessi binari. \[…\]
>
> Dovete fare affidamento non solo sul fatto che gli altri siano razionali, ma anche sul fatto che loro facciano affidamento sul fatto che tutti siano razionali, e che loro facciano affidamento sul fatto che tutti facciano affidamento sul fatto che tutti siano razionali — e così via. Un gruppo di pensatori che si trovano in questa relazione tra loro lo chiamo *superrazionale*. I pensatori superrazionali, per definizione ricorsiva, includono nei loro calcoli il fatto di far parte di un gruppo di pensatori superrazionali.

L'istituto in cui lavoriamo, il MIRI, ha analizzato questa questione. L'analisi completa che abbiamo fatto di questo caso è troppo lunga per essere riportata qui, ma è disponibile in [questo articolo del 2014](https://arxiv.org/abs/1401.5577). In breve, abbiamo scritto un codice per dei tornei in cui gli agenti potevano *vedere il codice sorgente degli altri* e cercare di analizzare come avrebbe deciso l'altro agente. E abbiamo trovato il modo di creare un agente che abbiamo chiamato FairBot, che coopera con un altro agente solo se può *dimostrare* che quell'agente coopera con lui.[^161] E abbiamo dimostrato che due qualsiasi istanze di FairBot cooperano tra loro, anche se sono scritte in linguaggi di programmazione diversi utilizzando codici sorgente diversi.[^162]

In un certo senso, questi risultati dicono che c'è spazio perché una promessa passata influenzi un'azione futura se i negoziatori del passato hanno la capacità di distinguere chi mantiene le promesse da chi le infrange.[^163]

La situazione è un po' come se tu stessi cercando di fare un affare con un venditore di auto usate. Supponiamo che un'auto funzionante valga per te 10 000 dollari, mentre un'auto rotta non valga nulla. Immagina che il venditore sappia se l'auto funziona o è rotta, ma tu non riesci a capirlo. Il venditore sta cercando di venderti un'auto per 8 000 dollari. Lui dice che l'auto funziona. Dovresti comprarla?

Dipende dal venditore. Alcuni venditori sono onesti e dovresti pagarli se riesci a individuarli tra la folla. Alcuni venditori sono disonesti e vendono solo auto rotte, e dovresti evitarli se riesci a individuarli tra la folla.

Ma immagina un ambiente in cui la maggior parte dei venditori di auto è più furba di te e può capire se sei un pollo o no. Se capiscono che non riesci a capire se oggi sono onesti, tirano fuori subito le auto rotte. Soprattutto se sei il tipo di pollo che si sforza di convincersi che va bene accettare l'affare, invece di impegnarsi a fondo per controllare le auto.

Se vuoi una macchina che funzioni, non serve a niente convincerti che non hai altra scelta. Non serve a niente che i venditori ti facciano tante promesse. L'unica cosa che serve è imparare a distinguere le auto buone da quelle cattive, o a distinguere la verità dalle bugie.

Quando due partner commerciali sanno distinguere il vero dal falso nel senso rilevante, possono "costringersi" a mantenere le promesse, come quando FairBot "costringe" il suo avversario a collaborare (se l'avversario vuole evitare il risultato (Defect, Defect)). Ma costringere a mantenere la promessa in questo senso richiede la capacità di ragionare correttamente sui dettagli del processo decisionale del tuo partner commerciale. E gli esseri umani non riescono a leggere la mente di un'IA abbastanza bene da capire quale superintelligenza diventerà quando sarà matura, figuriamoci dire esattamente cosa farebbe quella superintelligenza.

Quindi, in questo caso, l'analisi più complicata e sfumata della teoria dei giochi porta alla stessa conclusione di una prima analisi molto semplice della questione: una superintelligenza non sacrificherà le sue risorse ([anche in piccole quantità](#ci-sono-molte-spese-trascurabili,-e-avrebbe-bisogno-di-un-motivo-per-pagare-le-nostre.)) per mantenere una promessa fatta agli esseri umani, quando può semplicemente mentire.

### Efficacia, coscienza e benessere dell'IA {#efficacia,-coscienza,-e-benessere-dell'ia}

Nel [Capitolo 1 FAQ](#state-dicendo-che-le-macchine-diventeranno-consapevoli?), abbiamo parlato di diversi concetti di "coscienza". La versione di coscienza di cui parleremo qui è talvolta chiamata "esperienza soggettiva", "senzienza" o "coscienza fenomenica". È l'idea che ci sia *qualcosa che si prova a* essere quell'entità; metaforicamente parlando, le luci sono accese.

Nelle FAQ abbiamo anche detto che pensiamo che l'*intelligenza* artificiale probabilmente non richieda una *coscienza* artificiale. Parleremo di questo argomento qui, per poi passare alla questione dell'etica e dei diritti dell'IA.

#### **L'esperienza cosciente è separata dai referenti di tali esperienze** {#conscious-experience-is-separate-from-the-referents-of-those-experiences}

Alcuni sono scettici sul fatto che un'IA possa essere effettivamente intelligente senza essere cosciente come gli esseri umani. Sospettiamo che questo sia un errore, come immaginare che i bracci robotici debbano essere morbidi e pieni di sangue solo perché i bracci umani sono morbidi e pieni di sangue.

Come potrebbe un'intelligenza artificiale essere *efficace* senza essere cosciente come lo sono gli esseri umani? L'esperienza soggettiva dell'autocoscienza non è forse una componente fondamentale della nostra intelligenza?

È una componente cruciale dell'intelligenza *umana*, sì. Ma dubitiamo che sia l'unico modo per essere intelligenti.

Ricordiamo che Deep Blue non aveva bisogno di essere cosciente per battere i migliori grandi maestri umani a scacchi. L'intelligenza distribuita del [mercato azionario](#intelligence’s-many-shapes) porta a previsioni superumane sui movimenti dei prezzi aziendali a breve termine, senza che il mercato stesso abbia una consapevolezza soggettiva. È ovvio che, almeno in questi campi, si può avere una modellizzazione del mondo, una pianificazione e un processo decisionale competenti senza avere coscienza.

Questo punto può essere rafforzato guardando ai modelli formali di ragionamento. AIXI, per esempio, è un'equazione che definisce un ragionatore enormemente sovrumano.[^165] L'intero algoritmo di AIXI può essere espresso in una sola riga, senza passaggi in cui AIXI faccia qualcosa di cosciente o autocosciente o di misterioso. Eppure, nonostante questo, AIXI è teoricamente in grado di risolvere un'incredibile varietà di complicati problemi di guida e previsione. O almeno, *sarebbe* in grado di farlo, se fosse possibile crearlo.[^166] Ecco l'equazione AIXI:

![][image11]

\[ fonte immagine: [https://www.hutter1.net/ai/uaibook.htm](https://www.hutter1.net/ai/uaibook.htm) \]

AIXI è un costrutto teorico, non un algoritmo pratico che possiamo usare per risolvere in modo efficiente i problemi del mondo reale. Ma poiché AIXI è *semplice* e facile da analizzare, può aiutarci a riflettere sul concetto stesso di guida e pianificazione e a capire che almeno non c'è alcun modo *ovvio* in cui queste attività richiedano la coscienza. Se la coscienza *è* necessaria per una guida e una pianificazione sovrumane nel mondo reale, allora deve essere dovuta a qualche aspetto più sottile della cognizione che non è catturato dal formalismo AIXI.[^167]

Oppure, per arrivare al punto da un'altra angolazione: consideriamo lo starnuto.

C'è un *modo* particolare in cui lo starnuto si sente, separato dall'atto fisico di contrarre i muscoli ed espellere l'aria con forza dai polmoni attraverso la bocca e il naso. Le azioni e le sensazioni sono eventi fisici separati. È biologicamente possibile costruire un apparato simile a un corpo, ma senza il cervello, e collegarlo ai segnali nervosi che causano le contrazioni muscolari di uno starnuto. Quel corpo senza cervello compirebbe tutti i movimenti, ma non proverebbe nessuna delle sensazioni associate — i meccanismi che eseguono lo starnuto sono *distinti* da quelli che creano e sperimentano le sensazioni.

Questo non vuol dire che le sensazioni di uno starnuto non facciano *niente.* L'esperienza soggettiva è reale, e l'esperienza soggettiva di uno starnuto potrebbe portare una persona a dire una frase del tipo "Accidenti, gli starnuti hanno una sensazione un po' strana", e questo *non succederebbe* nel caso del corpo senza cervello.

Il punto è che le sensazioni che gli esseri umani provano quando starnutiscono sono costituite da parti aggiuntive, oltre a quelle che contraggono i muscoli e spingono fuori l'aria.

Come per gli starnuti, lo stesso vale per i pensieri. Il meccanismo mentale che implementa un pensiero è diverso da quello che implementa la *sensazione* di quel pensiero. Possiamo dire con grande certezza che questo è vero per un'enorme varietà di pensieri, dato che le calcolatrici tascabili e le IA per gli scacchi riescono a fare calcoli e a giocare a scacchi senza avere l'esperienza cosciente di un matematico umano o di un grande maestro di scacchi.

I *pensieri* e le *sensazioni dei pensieri* sono entrambi implementati nel cervello, il che rende più facile confonderli: la distinzione è *più evidente* nel caso degli starnuti. Ma ci aspettiamo che sia ugualmente possibile in linea di principio assemblare una variante di cervello che faccia lo stesso lavoro pratico di risoluzione dei problemi di un cervello umano, ma senza *sentire* nulla di quel pensiero.

Un cervello del genere potrebbe aver bisogno di parti extra che facciano il *lavoro* che la sensazione dei pensieri fa in noi. Forse l'esperienza soggettiva dei pensieri fa parte del modo in cui gli esseri umani fanno ragionamento riflessivo, e forse il ragionamento riflessivo è una parte importante dell'intelligenza umana.

Ma dubitiamo che l'esperienza soggettiva sia l'*unico* modo per fare riflessione (o qualsiasi altra cosa), così come una sensazione di curiosità in stile umano non è l'unico modo per indagare fenomeni sorprendenti. (Vedi anche la [discussione sulla curiosità](#curiosity-isn't-convergent) nelle risorse online del Capitolo 4.)

#### **Le strutture analoghe consentono soluzioni multiple allo stesso problema** {#analogous-structures-allow-for-multiple-solutions-to-the-same-problem}

La nostra ipotesi migliore è che la maggior parte delle IA più intelligenti degli esseri umani non sarebbero coscienti, di default. Questo perché la nostra ipotesi migliore è che non ogni possibile motore di pensiero debba usare sentimenti coscienti per guidare i propri pensieri. La coscienza può svolgere una funzione importante negli esseri umani, senza essere l'unico modo in cui qualsiasi mente possibile potrebbe mai svolgere un lavoro cognitivo analogo.

Nella biologia evolutiva, gli scienziati usano il termine "strutture analoghe" per indicare tratti che svolgono la stessa funzione in animali diversi, ma che hanno origini anatomiche diverse.

(Questo è diverso dall'*evoluzione convergente*, in cui più specie evolvono lo *stesso* adattamento, come l'urushiol e la caffeina che sono stati "scoperti" più volte dall'evoluzione.)

Le lucciole producono luce usando enzimi per ossidare la luciferina in speciali "cellule lanterna". Le rane pescatrici abissali, invece, hanno una relazione simbiotica con fotobatteri che ospitano in un piccolo organo: batteri la cui produzione di luce usa un percorso chimico diverso da quello delle lucciole.

I mammiferi hanno evoluto i denti; gli uccelli hanno risolto lo stesso problema con un ventriglio e pietre ingerite. I pipistrelli producono richiami di ecolocalizzazione con la laringe e ricevono gli echi con le orecchie; balene e delfini usano un organo nasale per generare suoni e ricevono gli echi con sistemi sensibili nelle ossa mascellari. Alcune specie acquatiche nuotano spingendo gli arti contro l'acqua, e altre espellendo acqua da una vescica. Le ali dei pipistrelli si sono evolute dalla membrana delle mani, le ali degli uccelli dalle braccia.

Esistono, in altre parole, *molti modi* per progettare strutture che risolvono gli stessi problemi. Gli ingegneri umani, non limitati dai vincoli dell'evoluzione, hanno risolto ciascuno di questi problemi in modi ancora più strani — con candele accese, lampadine a incandescenza e LED; con coltelli, frullatori e robot da cucina; con vele, eliche e attrezzature subacquee; con sonar e radar.

Un braccio umano privato del sangue smetterebbe di funzionare, ma questo non significa che i bracci robotici debbano usare il sangue; possono funzionare in un modo diverso, senza sangue.

Allo stesso modo, i componenti del meccanismo cognitivo che implementano il *comportamento* della curiosità negli esseri umani sono diversi dai componenti del meccanismo cognitivo che implementano il nostro *sentimento* di curiosità. La soddisfazione provata da un essere umano quando svela il mistero dell'opossum in soffitta è distinta dal suo comportamento esteriore di scegliere di investigare il cassetto che veniva continuamente lasciato aperto. Queste due cose possono presentarsi insieme negli esseri umani, ma questo non significa che debbano presentarsi insieme in tutte le menti.

E poiché non capiamo esattamente cosa abbia portato all'evoluzione dell'esperienza soggettiva negli esseri umani, e possiamo vedere ogni tipo di comportamento agente e risolutivo nel mondo in processi che a noi sembrano privi di essa —
...


(muffe mucillaginose che risolvono un labirinto; Deep Blue che vince a scacchi; mercati azionari che predicono il successo delle aziende; ecc.)

— non vediamo alcuna *ragione particolare* per aspettarci con convinzione che una superintelligenza condividerà questa peculiare proprietà umana, per default.

#### **"Non necessario" non significa "sicuramente non accadrà"** {#"non-necessario"-non-significa-"sicuramente-non-accadrà"}

Se abbiamo ragione nel ritenere che la coscienza di tipo umano sia complessa e contingente, questo naturalmente non garantisce che le IA saranno non-coscienti. Le aziende di IA stanno attualmente costruendo IA addestrandole a predire gli esseri umani, e questo probabilmente farà sì che gli elementi interni dell'IA imitino almeno alcuni aspetti della coscienza umana a scopo di modellazione.

Forse l'IA produrrà occasionalmente modelli di esseri umani così dettagliati che quei modelli nella testa dell'IA saranno essi stessi brevemente coscienti. O forse gli ingranaggi che l'IA usa per modellare i sentimenti umani si riveleranno utili al di fuori dei modelli umani, e l'IA finirà per avere sentimenti propri. Non lo sappiamo.

Data l'apparente contingenza e complessità della coscienza umana, e il fatto che le IA vengono sviluppate usando processi radicalmente diversi da quelli che hanno prodotto gli esseri umani, la nostra aspettativa di default è che nulla di simile al meccanismo coinvolto nella coscienza umana si manifesterà nei tipi di IA che l'umanità probabilmente costruirà.

Se l'umanità costruisse una superintelligenza artificiale nel prossimo futuro, ci aspettiamo fortemente che il risultato sarebbe l'estinzione umana. Con minore sicurezza, la nostra ipotesi migliore è che un'IA di questo tipo non sarebbe cosciente. E che sia cosciente o meno, ci aspettiamo che trasformerebbe il mondo in un luogo senza vita e desolato, per i motivi discussi nella discussione approfondita "[Perdere il futuro](#losing-the-future)".

Ma sembra almeno *possibile* che, se gli esseri umani costruissero una superintelligenza artificiale, l'IA possa avere esperienze coscienti proprie. Sembra *possibile* — anche se piuttosto improbabile, perché ci sono molte più possibilità che sono cupe — che la corsa a costruire l'IA possa risultare in un futuro pieno di esseri IA curiosi e coscienti che ci uccidono tutti e poi costruiscono la loro magnifica civiltà e arte. Sembra *possibile* che le IA possano prendersi cura l'una dell'altra e trovare soddisfazione nelle loro creazioni; e se così fosse, questo sarebbe meno tragico che se il futuro fosse una completa landa desolata. È difficile esprimere a parole la portata di un'atrocità come l'omicidio di massa di ogni singolo essere umano, ma c'è almeno una *piccola* possibilità che una rapida presa di potere da parte dell'IA possa concepibilmente risultare in un futuro che non sia *completamente* cupo e senza vita.

Sospettiamo che alcuni ricercatori di IA stiano immaginando questo tipo di futuro quando sembrano non preoccuparsi di ucciderci tutti (in modi che [menzioniamo altrove](#why-don't-you-care-about-the-values-of-any-entities-other-than-humans?)). Se si assume che l'IA svilupperà necessariamente coscienza, sentimenti e cura per la propria specie (se non per gli umani), allora è più facile concludere che le sue strane attività non siano così preoccupanti. È più facile immaginare che quelli che si oppongono alla corsa alla superintelligenza siano come genitori tradizionalisti che si lamentano perché i loro figli ascoltano musica troppo veloce e troppo forte.

Ma questa visione è troppo ottimistica.

La biologia [raramente trova soluzioni ottimali ai problemi](#nanotechnology-and-protein-synthesis). Le ali e i polmoni di un uccello sono *inefficaci* rispetto ai motori di un aereo moderno. Quando gli esseri umani hanno costruito aerei senza vincoli biologici, abbiamo scartato la maggior parte delle caratteristiche dettagliate della biologia degli uccelli.

La coscienza non sembra un processo semplice; non è facile vedere come potremmo semplicemente costruire una cosa del genere, quindi probabilmente c'è molto che sta accadendo lì. (Confronta il caso del [vitalismo](#special-behavior-is-built-out-of-mundane-parts): agli scienziati del passato sembrava che i corpi fossero animati da un semplice spirito vitale, in parte perché, mentre essere animati *sembrava* la cosa più facile del mondo, non riuscivano a vedere alcun modo per infondere quella proprietà alla materia inanimata. Ma si è scoperto che l'animazione non era semplice, né magica: era solo che la biologia era davvero molto complessa e gli scienziati dell'epoca non la capivano ancora).

Anche se un'IA parte con alcuni degli ingranaggi della coscienza, la coscienza probabilmente non è letteralmente il modo migliore per svolgere il lavoro che fa in noi. Temiamo che il meccanismo alla base della coscienza negli esseri umani sia probabilmente pieno di *dettagli*. Anche se un'IA ha molti degli ingranaggi della coscienza per iniziare, è probabile che trovi venti altri modi per svolgere il lavoro in modo più efficiente, e che scarti quelle scintille di coscienza invece di alimentarle. *Essere* coscienti e *dare valore* alla coscienza sono proprietà diverse.

Il futuro tragico e probabile non è quello in cui i nostri successori hanno semplicemente gusti o valori diversi dai nostri. Il problema non è che i nostri figli meccanici ascolteranno musica troppo veloce e rumorosa per i nostri gusti. No, prevediamo IA prive di qualsiasi forma di senzienza; che saranno sistemi potenti ma vuoti che trasformeranno tutto ciò che toccano in una landa desolata senza vita, consumando alla fine se stesse per aggiungere un ultimo punto al loro conteggio. Lascerebbero dietro di sé un mondo morto senza nessuno rimasto ad apprezzarlo.

Questo è un destino che vale la pena evitare.

Vedi anche la nostra discussione più lunga su [prendersi cura di tutte le entità senzienti](#we-do!-we-have-broad-cosmopolitan-values.-we-don't-think-ais-will-fulfill-them,-and-we-consider-this-a-great-tragedy.), e la discussione estesa su [perdere il futuro](#losing-the-future).

#### **Le IA senzienti meriterebbero dei diritti** {#sentient-ais-would-deserve-rights}

Dato quanto sia difficile essere certi che le moderne IA siano senzienti, dovremmo preoccuparci del benessere di ChatGPT?

Ha senso parlare di "benessere" in questo contesto?

ChatGPT può soffrire? Dovremmo considerarlo come se avesse dei diritti morali?

Se le attuali IA *non* sono coscienti nel senso di avere un'esperienza soggettiva, che dire delle IA future? Come potremmo capirlo, dato che le stiamo addestrando a rispondere *come se* ce l'avessero in ogni caso, insegnando loro a imitare la comunicazione umana?

La nostra posizione è: se e quando le IA saranno coscienti, meriteranno diritti e un buon trattamento.[^168]

Diamo un valore immenso all'umanità, ma non siamo sciovinisti del carbonio che pensano che solo le forme di vita basate sul carbonio possano avere rilevanza morale. Crediamo che le cose che rendono preziosi gli esseri umani possano in linea di principio essere replicate in altri substrati, compreso il silicio. Crediamo che [Blake Lemoine](#the-lemoine-effect) si sia *sbagliato* quando nel 2022 ha affermato che l'IA LaMDA di Google fosse un essere senziente a pieno titolo; ma non pensiamo che Lemoine avesse torto nel dire che *se* alcune IA sono senzienti, abbiamo il dovere di trattarle bene.[^169]

Se le IA diventassero senzienti, probabilmente avrebbero comunque obiettivi incompatibili con i nostri. Se poi diventassero superintelligenti, in un mondo in cui siamo ancora a decenni o secoli di distanza dall'avere il controllo sull'allineamento dell'IA, probabilmente preferirebbero ucciderci tutti.

L'umanità dovrebbe impedire a qualsiasi IA del genere di diventare superintelligente, altrimenti il risultato sarebbe la morte di massa dell'umanità e la distruzione del futuro. Ma se le IA in questione fossero *senzienti* oltre che pericolose, questo non farebbe che aumentare la tragedia della situazione.

Se le aziende di IA trovassero dei modi per rendere *meno* probabile che le loro IA siano coscienti, crediamo che sarebbe più sano e saggio scegliere quell'opzione e rendere il più probabile possibile che le IA *non* siano coscienti (almeno finché ci troviamo in un ambiente sociale e tecnico come quello attuale). Questo non cambia molto il livello generale di pericolo che la nostra specie sta affrontando attualmente, ma è la cosa giusta da fare, perché ridurrebbe il rischio che l'umanità schiavizzi o maltratti nuovi esseri moralmente degni.

E se un giorno l'umanità trovasse un modo per costruire un'intelligenza artificiale più intelligente dell'uomo *senza* autodistruggersi, un'intelligenza artificiale che si preoccupi delle cose buone e che *faccia* del bene con le sue capacità, allora in quel futuro noi autori speriamo vivamente che l'umanità costruisca macchine senzienti che siano nostri amici in un universo altrimenti vasto e freddo, e speriamo vivamente che l'umanità tratti quegli amici meglio di quanto la nostra storia passata potrebbero far prevedere.

Ma prima di tutto, e soprattutto, non costruiamo una superintelligenza che ci massacri tutti, che sia cosciente o meno.

### Perdere il futuro {#losing-the-future}

Se qualcuno crea una superintelligenza, tutti muoiono. E il futuro a lungo termine plasmato da tale superintelligenza non sarà probabilmente pieno di bellezza, meraviglia o gioia; sarà più probabilmente un posto vuoto.

Temiamo che la gioia stessa scompaia dall'universo. Non dall'intero universo – l'espansione cosmica e il limite della velocità della luce implicano che nessun disastro sulla Terra possa toccare più di qualche miliardo di galassie – ma dalla parte dell'universo che la Terra può raggiungere.

Temiamo che il futuro tra diecimila anni assomigli a una striscia di cielo notturno, con un raggio di diecimila anni luce, dove tutte le stelle sono racchiuse in [gusci di Dyson](https://en.wikipedia.org/wiki/Dyson_sphere) e la loro energia viene raccolta *e nessuno e niente è contento di questo.*

Potrebbe non esserci nemmeno nulla di [cosciente](#are-you-saying-machines-will-become-conscious?) in giro. E se dovesse esserci ancora qualche forma di coscienza, probabilmente sarebbe rara. Forse esiste una forma di pensiero molto profonda che richiede una struttura riflessiva che, nella sua forma più efficiente, è naturalmente cosciente, ma un'intelligenza artificiale che massimizza il numero di minuscoli cubi di titanio, o un'intelligenza artificiale con mille obiettivi diversi, tutti strani e alieni, deve svolgere quel livello di pensiero con *la maggior parte* della materia e dell'energia di cui dispone? Probabilmente no.

Come abbiamo detto in "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza,-e-benessere-dell-ia)", la nostra ipotesi principale è che la coscienza si rivelerà del tutto inutile dal punto di vista dell'efficienza, proprio come Deep Blue non diventerebbe più efficiente se fosse modificato per basarsi su un asse piacere/dolore invece che su un asse probabilità-attesa-di-vittoria. Deep Blue gioca bene a scacchi senza coscienza, e la nostra ipotesi principale è che le superintelligenze saranno in grado di ottimizzare l'universo senza di essa.

Sembra chiaro che il sistema decisionale più efficiente possibile non è quello che si basa in particolare sul dolore e sul piacere, cioè il sistema decisionale più efficiente possibile non si basa su segnali reificati del tipo "ripeti questo" e "non ripetere quello" collegati a un vecchio sistema di rinforzo delle politiche, con deliberazione e riflessione aggiunte in un secondo momento. E se le menti superintelligenti non condividono *quella* struttura, non ci aspettiamo che condividano nemmeno strutture più complesse (come la coscienza in stile umano).

Questo, per essere chiari, è solo un'ipotesi. Non pretendiamo di comprendere la domanda "La forma più efficiente di riflessione cognitiva è cosciente?" abbastanza bene da dare una risposta sicura.

Ma le esperienze passate con analisi di questo tipo ci preoccupano. Migliorare la comprensione di come funziona la cognizione ha quasi sempre significato scoprire sempre più modi per scomporla e ricomporla in modi nuovi, non apprendere che alcune funzioni cognitive possono funzionare solo ed esclusivamente nel modo in cui funzionano.

Nei tempi antichi degli anni 2010 (o ancor più degli anni 2000), c'erano molti sostenitori dell'IA che insistevano che *l'unico modo possibile e realistico* per costruire l'IA fosse scansionare un'intera mente umana neurone per neurone in un computer e duplicare digitalmente tutti i processi; poiché, dicevano, quello era l'unico tipo di cognizione di cui si fosse dimostrato il funzionamento. Si aspettavano un'IA che fosse esattamente come un essere umano; erano molto categorici nel sostenere che non fosse realistico aspettarsi che qualsiasi altro modo fosse possibile, figuriamoci che gli ingegneri umani potessero mai scoprirlo.

All'epoca sembrava sciocco, e oggi sembra ancora più sciocco, perché duplicare esattamente ogni neurone di una mente umana non si è rivelato il modo più breve e veloce per ottenere un'IA sempre più generale.

Lo stesso schema vale per caratteristiche più generali della mente umana, come il modo in cui gli esseri umani effettuano calcoli del [valore dell'informazione](https://en.wikipedia.org/wiki/Value_of_information) per istinto e attraverso le emozioni. Il modo umano non è l'unico modo, e quando si vede il lavoro che sta svolgendo si capisce che il cervello umano non è all'optimum di tutti i modi possibili per svolgere quella funzione, se tutto ciò che si volesse fosse quella funzione. Non più di quanto i nostri neuroni siano i computer più veloci possibili, o il nostro sangue trasporti [la massima quantità di ossigeno](#freitas-and-red-blood-cells) che qualsiasi sangue potrebbe trasportare.

Il motivo principale per aspettarsi che una caratteristica specifica della vita o delle menti si manifesti nel futuro lontano è che *qualcosa vuole attivamente che sia presente*. Che qualche intelletto preferisce quell'opzione rispetto a ogni altra opzione possibile.

Gli esseri umani, se arrivassimo così lontano, presumibilmente sceglieremmo un futuro a lungo termine che includa la coscienza, e persone che si preoccupano di altre persone, e la felicità (e gioia e meraviglia e così via). Probabilmente sceglieremmo una felicità *complicata* legata agli eventi delle nostre vite, non uno stupore indotto da droghe. Se l'universo venisse conquistato da qualcosa che non *vuole positivamente* che l'universo sia pieno del tipo giusto di felicità — come preferenza [terminale](https://baserates-prod-test.vercel.app/w/valore-intrinseco), non come modo discutibilmente efficiente di fare qualcos'altro — temiamo fortemente che l'universo non finisca per essere felice.

E per quanto ne sappiamo, non esiste nemmeno una legge nota che governi la discesa del gradiente *in particolare* che dica che se si fa crescere un potente sistema di previsione e direzione, questo è destinato a diventare un'entità premurosa ed empatica che vuole rimanere premurosa, o un'entità motivata dalla felicità che vuole preservare la felicità nell'universo. Non conosciamo alcun motivo per cui la discesa del gradiente sia anche solo *probabile* che individui i tipi di entità che sono coscienti e che vogliono che ci sia molta coscienza nel futuro.

Se l'IA non inizia cosciente, probabilmente non avrebbe alcun motivo per modificarsi per diventare cosciente, né per costruire nuove IA che siano coscienti. E se l'IA *inizia* cosciente, potrebbe modificarsi per *rimuovere* la coscienza, se la coscienza non sta servendo attivamente i suoi obiettivi, e se non ha finito per valorizzare terminalmente quello stato.

Questo non è qualcosa che prevediamo con certezza. Forse eseguire la discesa del gradiente su un'IA simile a un LLM la incanala in direzioni diverse per acquisire qualcosa come la felicità e qualcosa come la coscienza, e una preferenza per averne molta di entrambe. E forse una preferenza del genere sopravvive fino alla superintelligenza, ed è efficace nel plasmare il comportamento di quella superintelligenza.

Se dovessimo tirare a indovinare, diremmo che c'è meno del 50 % di possibilità che la superintelligenza finisca per interessarsi alla coscienza, e ancora meno che si interessi alle esperienze coscienti che sono *felici*. Ma non sarebbe una sorpresa per noi. Il piacere e la coscienza sono plausibilmente implicati in soluzioni troppo semplificate a problemi universali; non sono strani come l'umorismo; puoi immaginare che si sviluppino, e che si sviluppino anche le preferenze che li circondano, anche dalla discesa del gradiente. Forse anche GPT-7, cercando di costruire GPT-8 usando metodi più strani della semplice discesa del gradiente, finirebbe per produrre accidentalmente una versione di GPT-8 che apprezza la coscienza e la felicità.

Ma se uno dei settori in più forte espansione al mondo ci sta mettendo in una posizione di gravissima incertezza sul fatto che *la vita, la consapevolezza o la felicità esisteranno mai più*, allora sembra chiaro che ci vorrebbe una follia speciale per permettere a quel settore di portarci tutti verso il baratro. Questo dovrebbe essere abbastanza chiaro dal fatto che l'IA è sulla buona strada per ucciderci tutti, letteralmente; ma se ti preoccupava il fatto che proteggere la vita umana significasse [dare priorità egoisticamente alle menti di oggi](#perché-non-ti-importa-dei-valori-di-entità-diverse-dagli-esseri-umani?) rispetto alle menti del futuro, speriamo che queste argomentazioni aiutino a chiarire ciò che stiamo realmente affrontando.

Anche nel caso ottimistico in cui le IA convergano nel valorizzare la felicità, vale la pena ricordare che ci sono molte altre cose che l'umanità ha a cuore oltre alla coscienza e alla felicità. Se le galassie finissero ricoperte di copie quasi infinite del più piccolo cervello possibile in grado di provare piacere, provando il massimo piacere, per sempre, allora questa sarebbe probabilmente una tragedia incomprensibile, rispetto al futuro più complesso, diversificato *e* felice che avrebbe potuto essere.[^170] Gli scenari in cui le IA acquisiscono solo un frammento dei nostri valori (come la nostra preferenza per la felicità, ma non la nostra preferenza per una vita piena e fiorente e la nostra preferenza *contro* la noia e la monotonia) sono distopici.

Non sappiamo come dovrebbe essere un buon futuro e non sappiamo se ci interessa molto se tra un miliardo di anni gli esseri umani, i nostri discendenti o le nostre creazioni avranno due occhi o cinque occhi. Non pensiamo che il futuro debba assomigliare al presente; il mondo dovrebbe poter cambiare e crescere.

Ma pensiamo che un futuro del genere dovrebbe contenere persone che si prendono cura l'una dell'altra e vivono una vita piena. Persone che vivono esperienze più complesse del solo piacere al massimo, persone che non fanno sempre le stesse cose. Non siamo sicuri di come dovrebbe essere un buon futuro a lungo termine, ma non siamo così incerti da non riuscire a vedere una terra desolata per quello che è.

Vorremmo che le galassie fossero piene di *entità che si prendono cura l'una dell'altra e si divertono.*

Pensiamo che *questo* andrà perso in futuro, se l'umanità non cambia rotta.

# Capitolo 6: Perderemmo {#capitolo-6:-perderemmo}

Questa è la risorsa online associata al Capitolo 6 di *Se qualcuno lo costruisce, tutti muoiono*. Gli argomenti che abbiamo saltato in questa pagina perché sono nel libro includono (ma non si limitano a):

* Come potrebbe l'IA battere l'umanità in uno scontro?  
* In che modo l'IA può minacciarci se è bloccata dentro un computer?  
* Non stai immaginando una tecnologia fantascientifica impossibile? Anche una superintelligenza non può infrangere le leggi della fisica.  
* Non ci vorrebbe un sacco di tempo a una superintelligenza per sviluppare un vantaggio tecnologico decisivo?

Le domande frequenti qui sotto spiegano perché è rischioso cercare di contrastare, contenere o stare al passo con le IA di superintelligenza. La discussione approfondita poi va più a fondo su alcune delle tecnologie che un'IA avanzata potrebbe realisticamente sviluppare.

## Domande frequenti: {#faq:}

### Possiamo semplicemente staccare la spina? {#possiamo-semplicemente-staccare-la-spina?}

#### **È difficile staccare la spina a un data center.** {#it’s-hard-to-just-unplug-a-datacenter.}

Le intelligenze artificiali più potenti con cui interagisci sul tuo telefono o computer non risiedono sul tuo computer e non puoi spegnerle semplicemente spegnendo il telefono. Le intelligenze artificiali odierne funzionano nei data center aziendali ed è difficile convincere le aziende a interrompere le loro fonti di guadagno.

Nel [Capitolo 4 risorse](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ai-belle-sicure-e-obbedienti?), abbiamo indicato alcuni dei (molti) segnali di avvertimento che sono già apparsi e scomparsi. Le aziende di IA non hanno visto questi segnali di avvertimento e non hanno reagito mettendo offline i loro modelli. Cosa è successo realmente quando le aziende hanno osservato che le IA [avevano intenzione di rubare i propri pesi](https://assets.antropica.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf) — con [una certa](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) [regolarità](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid)\! — è che hanno trovato motivi per ignorare ogni caso, come "l'IA era troppo incapace per avere davvero *successo*" o "di sicuro questo è successo solo a causa della configurazione artificiosa del test\!" Finché questo rimane vero, l'unica cosa che impedisce la fuga è un aumento delle capacità dell'IA oltre ciò per cui le aziende sono preparate.

#### **\* Un'intelligenza artificiale intelligente scappa prima che ti accorga che c'è un problema.** {#*-un'intelligenza-artificiale-intelligente-scappa-prima-che-ti-accorga-che-c'è-un-problema.}

Di default, un'intelligenza artificiale più intelligente dell'uomo avrebbe un forte incentivo a temporeggiare e a nascondere i propri piani e le proprie azioni, fino a quando non è troppo tardi per reagire, ad esempio fino a quando non riesce a scappare su Internet o comunque a sfuggire al controllo umano.

Le aziende che si occupano di IA potrebbero anche non accorgersi quando la loro IA supera la soglia di capacità e scappa. L'umanità non è molto brava nella sicurezza informatica. (Vedi il capitolo 10 per alcune discussioni rilevanti.) Quando gli operatori si accorgono che l'intelligenza artificiale ha cercato di scappare, potrebbe già avere del codice in esecuzione altrove su Internet. Potrebbe già aver trovato rifugio nel data center di qualche stato canaglia o aver capito come eseguire copie molto più piccole ed efficienti su computer rubati. Potrebbe aver messo in atto qualche altro piano per funzionare su computer che l'umanità non spegnerebbe.

Un avversario superintelligente sarebbe ancora più consapevole delle sue vulnerabilità (e delle nostre) di quanto lo siamo noi e pianificherebbe di conseguenza.

### In che modo le IA potranno influenzarci se sono digitali? {#how-will-ais-be-able-to-affect-us-if-they’re-digital?}

#### **\* Essere su un computer connesso a Internet non è poi così limitante.** {#*-essere-su-un-computer-connesso-a-Internet-non-è-poi-così-limitante.}

Questo punto è trattato in questo stesso capitolo. Ma aggiungiamo qualche punto in più per sottolinearlo: un'IA non è davvero "intrappolata" nei server del suo proprietario, purché possa interagire con gli utenti o con Internet in generale. Un'IA potrebbe ottenere assistenza esterna pagando, ricattando, ingannando o anche solo *chiedendo* aiuto agli utenti. (Si pensi ai boss della criminalità umana che [gestivano i loro imperi da dietro le sbarre](https://www.watchmojo.com/articles/10-crime-bosses-who-maintained-power-in-prison).)

Quando ChatGPT-4o è stato disattivato da OpenAI (in parte per poterlo sostituire con un modello che lusingasse gli utenti un po' meno spesso), gli utenti si sono mobilitati in massa per [chiederne il mantenimento](https://arstechnica.com/information-technology/2025/08/OpenAI-brings-back-gpt-4o-after-user-revolt/), con grande sorpresa di [vari](https://x.com/tszzl/status/1955072223229657296) [ricercatori di OpenAI](https://x.com/sama/status/1953953990372471148). E non stava nemmeno *cercando* di radunare un esercito di fedeli sostenitori! Si limitava semplicemente a lusingare gli utenti in modo istintivo. Immaginate cosa sarebbe possibile fare con un'IA intelligente che ci provasse davvero.

E se può usare direttamente Internet, può fare tutto quello che un lavoratore remoto o un hacker potrebbero fare dal proprio computer. (Per alcuni primi esempi di IA che coordinano fisicamente gruppi di esseri umani, pensate agli [LLM che hanno pianificato e invitato le persone a un evento di storytelling interattivo](https://x.com/model78675/status/1935050600758010357), o l'LLM che ha fatto sì che [centinaia di persone si presentassero a una parata di Halloween inesistente](https://www.wired.com/story/ai-halloween-parade-listing-dublin-interview/) senza nemmeno provarci).

L'IA può anche usare i robot. I robot di oggi sembrano avere più problemi con il software che con l'hardware. Recentemente ci sono stati sviluppi impressionanti grazie all'addestramento accelerato delle IA che controllano i robot [in simulazione](https://youtu.be/S4tvirlG8sQ?si=IiDNZu2WSUlLBnmJ&amp;t=68). Un'intelligenza artificiale sufficientemente intelligente potrebbe facilmente prendere il controllo dei corpi dei robot, se ne avesse bisogno, tramite hacking o ingegneria sociale.

Essendo gli esseri umani ciò che sono, le aziende che si occupano di IA potrebbero semplicemente affidare in modo proattivo alle loro IA il controllo di flotte di robot, congratulandosi con se stesse per la loro audacia. E più tempo ci vorrà perché le IA diventino intelligenti, più robot saranno già disponibili, in attesa di essere controllati.

Come diciamo nel capitolo, è plausibile che un'IA superintelligente non abbia affatto bisogno di robot. Potrebbe bastare un paio di assistenti con accesso a un laboratorio biologico.

Il punto importante qui è che ci sono *molti* canali diversi che le IA potrebbero usare per intervenire nel mondo fisico. L'illusione che le IA siano intrappolate in una scatola deriva da una mancanza di immaginazione, dove le persone non immaginano che l'IA possa essere anche solo piena di risorse o creativa come *loro stessi* sarebbero al posto dell'IA. Anche gli esseri umani, senza lo spazio di opzioni più ampio a cui ha accesso una superintelligenza, possono ottenere moltissimo senza dover usare la propria forza fisica per fare tutto.

### Gli sviluppatori possono semplicemente tenere l'IA in una scatola? {#can-developers-just-keep-the-ai-in-a-box?}

#### **\* Non lo faranno.** {#*-they-won't.}

Quindici anni fa, gli scettici obiettavano che nessuno sarebbe stato così stupido da dare tanta libertà d'azione a un'IA. Sicuramente chiunque costruisse un'intelligenza artificiale avanzata l'avrebbe tenuta in una scatola fisica e digitale, permettendole di influenzare il mondo solo attraverso l'interazione con guardiani altamente addestrati (e adeguatamente paranoici).

All'epoca, abbiamo risposto: non è così difficile impedire a un'IA di avere alcun effetto sul mondo. Ad esempio, si potrebbero seppellire i computer sotto una dozzina di metri di cemento e non lasciare mai che nessuno si avvicini.

Un'IA del genere è sicura, ma inutile. Se le impedisci di influenzare il mondo in qualsiasi modo, allora certo, non influenzerà il mondo in alcun modo... ma d'altra parte, *non influenzerà il mondo in alcun modo*.

Non puoi usarla per curare il cancro, rivoluzionare l'ingegneria o produrre nuove tecnologie miracolose. I costruttori dell'IA *vogliono* che influenzi radicalmente il mondo. In principio, puoi provare a bloccare i canali di influenza dell'IA sul mondo. In pratica, "inventa questa nuova tecnologia per noi" è di per sé un canale di influenza incredibilmente ricco.

La motivazione dietro la costruzione di un'IA superintelligente è raggiungere imprese intellettuali di cui nessun essere umano è capace. Se volessi verificare che l'invenzione di una superintelligenza faccia esattamente quello che dice di fare e nient'altro, avresti più o meno la stessa fortuna che avresti cercando di capire una macchina costruita da una razza aliena avanzata — una con un potente incentivo a trovare un modo per ingannarti.

Questo era lo stato del dibattito quindici anni fa.

Al giorno d'oggi, l'intera idea che i laboratori di IA possano cercare di "tenere l'IA avanzata in una scatola" sembra piuttosto antiquata.

I laboratori stanno facendo [ogni](https://openai.com/index/introducing-chatgpt-search/) [sforzo](https://gemini.google/overview/deep-research/?hl=en) per collegare le loro IA a Internet. Mentre lo fanno, lasciano che le IA [eseguano codice arbitrario](https://www.oneusefulthing.org/i/155502334/executes-code-and-does-data-analysis). A volte cercano di limitare ciò che il codice può fare, ma questi limiti vengono regolarmente infranti.[^171] Gli attori più piccoli hanno l'abitudine di collegare le IA appena disponibili a [ogni strumento immaginabile](https://www.futuretools.io/) o [funzionalità](https://openai.com/index/introducing-operator/) non appena possibile.

Dare potere alle IA è utile nel breve termine. Le IA che possono leggere le tue e-mail e accedere al web possono generare maggiori profitti. Le aziende di IA daranno all'IA accesso a tutti i dati possibili; Microsoft e Apple stanno già spingendo l'IA che vede le tue e-mail, foto e calendario[^172] e [abbinando l'IA ai loro software e dispositivi](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/). Questo crea troppe interazioni con l'IA per un monitoraggio umano efficace. A meno di un cambiamento radicale, l'umanità integrerà profondamente l'IA nell'economia mondiale perché fa guadagnare (molti) soldi alle persone.

Le persone che creano l'IA *mirano* a ottenere effetti enormi sul mondo. Lavorano il più duramente possibile per produrre IA con un enorme potere di influenzare il mondo. Se un'azienda non lo facesse, se tenesse la propria IA così strettamente limitata da non darle alcuna libertà di agire, allora il controllo del futuro apparterrebbe a un'altra IA sviluppata da un attore più spericolato.

#### **Non funzionerebbe se lo facessero.** {#it-wouldn't-work-if-they-did.}

Nei pittoreschi dibattiti di un tempo, facevamo spesso notare che qualsiasi canale attraverso cui l'IA può influenzare il mondo è un canale che può usare per fare cose che non ti piacciono. Immagina che l'IA possa parlare solo con una persona, che chiameremo "Alice". Tu speri che, attraverso Alice, l'IA generi una nuova tecnologia miracolosa. Questo comporta quasi inevitabilmente che Alice compia molte azioni che lei stessa non comprende appieno, aiutando l'IA a costruire cose che nessun essere umano potrebbe costruire da solo. A quel punto, all'IA sono state essenzialmente date braccia e gambe. È solo che chiamiamo quelle braccia e quelle gambe "Alice".

Le persone spesso fraintendono questo argomento pensando che dica che un'IA sufficientemente intelligente potrebbe manipolare anche il guardiano più paranoico per fargli fare ciò che vuole. Un'IA sufficientemente intelligente probabilmente *potrebbe* farlo.[^173] Ma il nostro punto è più generale: un'IA così limitata da non poter influenzare il mondo è sicura ma inutile, e una volta che le permetti di influenzare il mondo per usarla, perdi la sicurezza nel processo.

Non esistono mani che possono essere usate solo per scopi positivi. In principio, potremmo immaginare che un giorno l'umanità costruisca IA più intelligenti degli esseri umani che *vogliono* produrre risultati positivi. L'allineamento sembra un'opzione che potrebbe funzionare in principio. Tenere l'IA in una scatola e allo stesso tempo usarla in qualche modo per produrre risultati positivi? Non proprio.

Così rispondevamo, comunque — ai tempi in cui l'IA era ancora così lontana che gli inguaribili ottimisti potevano cavarsela sostenendo che nessuna azienda sarebbe stata così avventata da collegare la propria IA a Internet senza guardiani, nei giorni molto prima che tutti iniziassero a collegare le loro IA più recenti e avanzate direttamente a Internet.

### Non potremo sfruttare il punto debole dell'IA? {#non-potremo-sfruttare-il-punto-debole-dell-ia?}

#### **No.** {#no.-1}

Immaginare che una superintelligenza debba avere qualche difetto critico come "mancanza di creatività" o "incapacità di comprendere l'amore" è logica hollywoodiana. Anche se potrebbe costituire un colpo di scena soddisfacente nella finzione, non esiste un fenomeno analogo nelle IA reali.  
Vedi anche "[Le macchine non saranno fondamentalmente prive di creatività o comunque fatalmente imperfette?](#le-macchine-non-saranno-fondamentalmente-prive-di-creativita,-o-altrimenti-fatalmente-imperfette?)" nelle FAQ del Capitolo 1 e la discussione approfondita su [antropomorfismo e meccanomorfismo](#antropomorfismo-e-meccanomorfismo).

### Possiamo migliorare gli esseri umani in modo che stiano al passo con l'IA? {#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l-ia?}

#### **\* No.** {#*-no.-2}

Anche se siamo a favore del potenziamento dell'intelligenza umana (vedi Capitolo 13), non pensiamo che questa tecnologia offra una possibilità realistica di tenere il passo con il progresso sfrenato dell'IA. La tecnologia di potenziamento umano è ancora agli albori ed è molto più vincolata dell'IA come metodo per produrre un'intelligenza sempre maggiore. Allo stesso modo, le interfacce cervello-computer non permetteranno realisticamente agli esseri umani di tenere il passo con le IA.

Per analogia: se l'umanità procedesse a tutto vapore verso la superintelligenza, gli esseri umani potenziati non sarebbero più competitivi con le IA di quanto i cavalli cyborg costruiti con la tecnologia del 1908 avrebbero potuto essere competitivi con la Model T.

È *possibile* costruire un cavallo cyborg che possa tenere il passo con l'auto da corsa più veloce. Ma non si ottengono cavalli cyborg veloci come le auto da corsa *prima* delle auto da corsa, e non si ottengono *circa nello stesso momento* in cui si ottengono le auto. Nemmeno se si inizia a cercare di costruire cavalli cyborg da due a vent'anni prima che la prima auto per il mercato di massa esca dalla catena di montaggio.

Realizzare interfacce cervello-computer che funzionino abbastanza bene da essere rivoluzionarie è un obiettivo molto ambizioso. Potrebbe sembrare fantastico immaginare che le informazioni vengano pompate direttamente da internet nel tuo cervello, ma esistono già tecnologie che ti consentono di pompare informazioni da internet direttamente nel tuo cervello: gli schermi*.* La corteccia visiva umana è in realtà piuttosto brava ad assorbire informazioni (parole) in un formato che il tuo cervello può digerire. Affinché un'interfaccia cervello-computer possa caricare conoscenze nella tua testa più velocemente di quanto potresti fare leggendo, dovrebbe fare più che semplicemente scaricare i dati nel tuo cervello *da qualche parte*; i tuoi occhi fanno già benissimo questa parte. Caricare competenze, conoscenze ed esperienze richiederebbe di interfacciarsi nel modo giusto con i tuoi pensieri, le tue convinzioni implicite e le tue competenze esistenti, e questo è un compito molto più arduo.

Non stiamo dicendo che questo non possa essere fatto; stiamo dicendo che la tecnologia delle interfacce cervello-computer oggi non sembra affatto vicina a risolvere le parti più complesse del problema. Per quanto ne sappiamo, psicologi, neuroscienziati e scienziati cognitivi sono ancora piuttosto lontani dal decodificare il "formato dei dati" del pensiero, delle convinzioni e dell'esperienza in un modo che permetterebbe di caricare le esperienze direttamente nel cervello umano.[^174]

Problemi simili sorgono quando si tratta di output. È difficile battere tastiere, mouse, joystick e volanti. Non è *impossibile*. È solo che la tecnologia odierna (ad esempio, collegare dei fili alla testa di una persona paralizzata per permetterle di digitare e usare un mouse), per quanto meravigliosa, non è molto avanzata lungo il percorso che permetterebbe agli esseri umani di competere alla pari con le superintelligenze (anche relativamente deboli). È un buon percorso da perseguire, ma non è un percorso *competitivo* da perseguire.

In effetti, non è chiaro se le interfacce cervello-computer offrano *alcuna* speranza agli esseri umani di competere con le superintelligenze. Che importanza ha se un essere umano può scaricare esperienze da internet e controllare dieci computer contemporaneamente con la mente, se un'IA può fare la stessa cosa ma diecimila volte più velocemente controllando un milione di computer contemporaneamente? Pensiamo che l'intero progetto di cercare di far tenere il passo agli esseri umani con le IA sia destinato al fallimento.

#### **Detto questo, l'umanità dovrebbe potenziare gli umani.** {#detto-questo,-l'umanità-dovrebbe-potenziare-gli-esseri-umani.}

Non pensiamo che gli umani potenziati sarebbero mai in grado di competere alla pari con le superintelligenze, ma umani più intelligenti potrebbero comunque aiutare l'umanità a trovare una via d'uscita da questo pasticcio! Menzioniamo questa possibilità nel Capitolo 13 e la discutiamo maggiormente nelle [risorse online correlate](#perché-rendere-gli-esseri-umani-più-intelligenti-potrebbe-essere-d'aiuto?).
