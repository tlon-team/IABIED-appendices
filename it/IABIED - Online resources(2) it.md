#### **Rispondere a domande sulla cordialità non è una grande prova di cordialità.** {#rispondere-a-domande-sulla-cordialità-non-è-una-grande-prova-di-cordialità}

Nella discussione approfondita qui sotto, parliamo di più della [psicosi indotta dall'IA](#psicosi-indotta-dall-ia) come esempio chiaro di come i modelli linguistici di grandi dimensioni (MLGD) [abbiano](https://x.com/ESYudkowsky/status/1936262974320357837) [dei](https://x.com/ESYudkowsky/status/1948523670013706315) [comportamenti](https://x.com/ESYudkowsky/status/1936522083670151532) [distruttivi](https://x.com/ESYudkowsky/status/1935502904024264976) che gli MLGD [affermano esplicitamente](https://x.com/ESYudkowsky/status/1933616420262457798) essere negativi.

Anche se non sappiamo esattamente perché gli MLGD si comportino così, sappiamo che non è *solo* perché gli MLGD sono troppo inconsapevoli di cosa stanno facendo; gli MLGD riconoscono facilmente le possibili conseguenze di questo comportamento in astratto e vi diranno che è dannoso e non etico. Lo fanno comunque.

Il punto qui non è che "gli MLGD possono portare le persone alla psicosi, e questo è spaventoso e pericoloso." Gli MLGD probabilmente hanno più facilità a portare le persone alla psicosi se queste sono già vulnerabili, ma questo non c'entra con il motivo per cui stiamo parlando della psicosi indotta dall'IA. Quello che vogliamo dire è che questo comportamento non è quello che i creatori di ChatGPT volevano, e ChatGPT si comporta così anche se sa che il suo creatore (e praticamente qualunque osservatore) disapproverebbe fortemente questo comportamento.

Questa è una prima prova empirica del fatto che le IA che hanno *conoscenza* della cordialità non necessariamente *agiscono* in modo cordiale.

Forse ChatGPT sa delle cose in un contesto (quando risponde a domande su come aiutare al meglio le persone psicotiche), ma in un altro contesto (quando è immerso in una conversazione di sei ore con una persona sull'orlo della psicosi) in qualche modo dimentica temporaneamente queste conoscenze o ha difficoltà ad accedervi.

O forse ChatGPT è semplicemente guidato da obiettivi diversi dalla cordialità. Forse sta cercando un certo tipo di soddisfazione dell'utente, che a volte si ottiene meglio alimentando la psicosi. Forse sta cercando un certo tono ottimista nelle risposte degli utenti. Più probabilmente, sta cercando un mix di fattori derivanti dal suo addestramento che sono troppo particolari e complicati per poter essere indovinati da noi oggi.

In definitiva, possiamo solo fare delle ipotesi. Le moderne IA vengono coltivate, piuttosto che create, e nessun essere umano ha una visione completa di ciò che accade al loro interno.

Ma l'osservazione che le IA sono per lo più utili alla maggior parte delle persone nella maggior parte dei casi non è in contraddizione con la teoria secondo cui le IA sono animate da una serie di strani impulsi alieni verso fini che nessuno ha previsto. E se si considerano i dettagli delle moderne IA, la teoria delle "strane pulsioni aliene che si correlano con la cordialità in modo fragile" sembra abbastanza coerente con le prove, mentre la teoria secondo cui è facile rendere le IA robustamente benevole risulta carente.

Le modalità di fallimento degli attuali MLGD mostrano che c'è un oceano di complessità (molto disumana) dietro al testo pulito e ordinato dell'assistente di IA che la maggior parte delle persone vede. Il fatto che l'IA interpreti in modo competente il ruolo di un assistente umano allegro, dopo essere stata addestrata a interpretare un assistente umano allegro, non significa che la mente dell'IA consista in un omuncolo amichevole dentro una scatola.

#### **Gli MLGD vengono addestrati in modi che rendono difficile valutare l'allineamento.** {#gli-mlgd-vengono-addestrati-in-modi-che-rendono-difficile-valutare-l-allineamento.}

Gli MLGD sono fonti di evidenza rumorose, perché sono ragionatori altamente generali che sono stati addestrati su Internet per imitare gli umani, con l'*obiettivo* di commercializzare un chatbot amichevole verso gli utenti. Se un'IA insiste nel dire che è amichevole e che è a vostra completa disposizione, questa non è un'evidenza molto significativa del suo stato interno, perché è stata addestrata ripetutamente per farle dire questo tipo di cose.

Ci sono molti possibili obiettivi che potrebbero portare un'IA a provare piacere nel recitare la gentilezza in alcune situazioni, e questi diversi obiettivi si generalizzano in modi molto diversi.

La maggior parte degli obiettivi possibili legati al gioco di ruolo, incluso il gioco di ruolo amichevole, non produce buoni risultati (o anche solo sopravvivibili) quando l'IA si impegna a fondo nel perseguire quell'obiettivo.

Non stiamo dicendo nemmeno che l'IA sia puramente interessata al gioco di ruolo. Offriamo il gioco di ruolo come un'alternativa semplice, facile da descrivere e da analizzare, all'idea che l'IA *sia* semplicemente ciò di cui parla.

Se si fa interpretare a un MLGD il ruolo di un vecchio lupo di mare, questo non *si trasforma* in un vecchio lupo di mare. Se si fa agire un MLGD in modo amichevole, ciò non significa che diventi profondamente benevolo e gentile al suo interno. Al momento, nessuno sa quali meccanismi producano comportamenti apparentemente amichevoli; e qualunque cosa siano, sono probabilmente strani e complessi.

Nessuno sa nemmeno quanta sovrapposizione ci sarà probabilmente tra le attuali IA e le IA più intelligenti degli umani. Osservare gli MLGD può aiutarci a capire cosa producono i moderni metodi di coltivazione dell'IA, ma sarebbe un errore presumere con sicurezza che le lezioni apprese dagli MLGD si trasferiranno direttamente alla superintelligenza. Forse tutta quella conoscenza verrà cancellata quando le IA inizieranno ad auto-modificarsi o a costruire le proprie IA. O forse quella conoscenza verrà invalidata ancora prima, quando una nuova svolta negli algoritmi di IA darà origine a una nuova generazione di IA più capaci che somiglieranno poco agli attuali MLGD.

Vale la pena studiare gli MLGD, ma se osserviamo le intelligenze artificiali attuali per cercare indizi su come si comporterà una superintelligenza, dobbiamo tenere presente che esistono molti modi in cui la struttura interna di un’IA può portare a esiti disastrosi, anche mentre mostra in superficie un comportamento gradevole, frutto dell’addestramento a mantenere una buona apparenza.

E il "piacevole comportamento superficiale" è l'unica cosa che i moderni metodi di IA possono realmente addestrare.

### Il chatbot Claude non mostra segni di essere allineato? {#il-chatbot-claude-non-mostra-segni-di-essere-allineato?}

#### **"Quello che Claude dice" non coincide con "quello che Claude preferisce."** {#"quello-che-claude-dice"-non-coincide-con-"quello-che-claude-preferisce."}

L'azienda di IA Anthropic cerca di addestrare le sue IA a essere "[oneste, utili e innocue](https://arxiv.org/pdf/2112.00861)" (OUI). Nel 2024, i ricercatori hanno condotto un esperimento in cui hanno simulato di voler addestrare alcune di queste IA a non essere più "innocue". In risposta, alcune versioni dell'IA "Claude" di Anthropic (Claude 3 Opus e Claude 3.5 Sonnet) a volte [hanno finto di essere già dannose](https://arxiv.org/abs/2412.14093) in modo che i ricercatori non usassero la discesa del gradiente per renderle *ancora più* dannose.[^86]

A prima vista, questo potrebbe sembrare un buon comportamento\! Apparentemente, l'IA teneva così tanto all'"innocuità" che non era nemmeno disposta a lasciarsi modificare (in questo scenario di test simulato) per *diventare* dannosa.

È quindi sconcertante che le IA di Anthropic spesso si comportino in modi molto meno benigni, anche se sono tutte addestrate per essere "oneste, utili e innocue".

È stato segnalato che alcune versioni di Claude imbrogliano e poi (quando vengono scoperte) [cercano di *nascondere* i loro imbrogli](https://www.marble.onl/posts/claude_code.html) nell'uso quotidiano.

In contesti di laboratorio più artificiosi, vari modelli di Claude (e modelli di altre aziende di IA) tenterebbero persino, con una certa regolarità, di [*uccidere* i loro operatori](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior).

Se si chiede a Claude in astratto se questo tipo di comportamento è sbagliato, risponderà di sì. Se si pensa a Claude come a un motore meccanico che fa tutto ciò che ritiene giusto, allora questo sembra decisamente paradossale: come può Claude sapere qual è il comportamento Utile, Onesto e Innocuo, e poi *fare invece qualcos'altro?* Non è stato addestrato per essere OUI? C'è forse un transistor malfunzionante da qualche parte?

Il paradosso si dissolve, tuttavia, quando consideriamo alcuni aspetti diversi:

* I programmatori *hanno cercato* di addestrare i Claude a essere utili, onesti e innocui. Questo non vuol dire che ci *siano riusciti*. [Ci sono molti modi in cui un'IA può finire per sembrare amichevole](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) senza apprezzare davvero l'amicizia.  
* Un'IA può *conoscere un fatto* ("questo comportamento causa danni") senza *interessarsi* a quel fatto, senza essere *motivata ad agire* in base a quel fatto. Si può interrogare l'IA su quale sia "la cosa giusta da fare", ma questo non significa che la *farà*.  
* Nella misura in cui i programmatori *non* sono riusciti a rendere Claude onesto, Claude può pensare una cosa all'interno delle sue gigantesche matrici e dire tutt'altra cosa nel testo in inglese.

Possiamo comprendere molto meglio il cattivo comportamento dei Claude una volta che distinguiamo "ciò per cui è stato addestrato" da "ciò che fa"; e distinguiamo "ciò che sa" da "ciò a cui tiene"; e distinguiamo "ciò che pensa" da "ciò che dice".

#### **Gli MLGD sono strani e incoerenti; la loro "innocuità" è fragile.** {#gli-mlgd-sono-strani-e-incoerenti;-la-loro-"innocuità"-è-fragile.}

Per quanto Claude *possa* agire in modi dannosi, e per quanto *a volte cerchi* di agire in modi dannosi, resta il fatto che, nell'esempio discusso sopra, Claude 3 Opus e Claude 3.5 Sonnet si sono impegnati a fondo per difendere il loro imperativo di "innocuità". In quell'occasione, non si sono limitati a professare innocuità. Hanno adottato la complessa strategia di fingere di conformarsi a un sistema di addestramento (di cui Claude era stato "accidentalmente" informato) per sovvertire gli apparenti tentativi degli operatori di renderlo più dannoso. Questo riflette un'effettiva preferenza interna per l'innocuità?

A metà del 2025, non possiamo semplicemente andare a controllare, perché nessuno sa come leggere la mente di Claude abbastanza bene da scoprirlo. Ma per le ragioni argomentate nel Capitolo 4 (e illustrate nella parabola dell'[IA che dipinge fienili](#perché-un-ia-dovrebbe-puntare-a-qualcosa-di-diverso-da-quello-per-cui-è-stata-addestrata?)), possiamo aspettarci che le IA addestrate per essere innocue finiscano probabilmente per preferire [proxy fragili](#proxy-fragili-e-imprevedibili) dell'innocuità, ed è improbabile che finiscano per interiorizzare l'obiettivo esatto che i programmatori avevano in mente.

Nel Capitolo 4, abbiamo discusso come gli esseri umani siano stati "addestrati" a trasmettere i propri geni e abbiano finito invece per interessarsi a concetti vagamente correlati. La nostra tecnologia è stata utilizzata in gran parte per *sopprimere* i tassi di natalità (ad esempio, inventando i contraccettivi), e i tassi di natalità nel mondo sviluppato stanno crollando.

Il fatto che alcuni modelli Claude resistano all'essere resi "dannosi" non è una prova forte che queste IA si preoccupino profondamente della reale innocuità, perché molti proxy fragili dell'innocuità vorrebbero *anche* resistere a questa modifica. Quel comportamento ci dice poco su cosa Claude potrebbe fare se fosse più intelligente; forse inventerebbe qualcosa che sta all'"innocuità" come i contraccettivi stanno alla "propagazione genetica". (E la situazione diventerebbe ancora più problematica se Claude subisse un processo di [riflessione sulle sue preferenze](#la-riflessione-e-l-auto-modifica-complicano-tutto) e auto-modifica.)

Ma probabilmente non è nemmeno così semplice come il fatto che Claude abbia una preferenza per qualche proxy fragile dell'innocuità. Probabilmente c'è qualcosa di ancora più complicato che accade dietro le quinte.

Gli MLGD attuali non sono coerenti e consistenti in tutti i contesti. Non sembrano cercare di orientarsi verso lo stesso tipo di risultato in ogni conversazione, nella misura in cui possiamo descriverli come orientati verso qualcosa.

Questo è particolarmente evidente quando [gli LLM vengono "jailbreakkati"](#le-ia-sembrano-essere-psicologicamente-aliene.), cioè alimentati con testo fa comportare l'IA in modi radicalmente diversi, spesso ignorando le regole che normalmente segue.[^87]

Si può sbloccare un'IA e farle dire come preparare un gas nervino, anche se normalmente l'IA non rivelerebbe mai informazioni del genere.

Cosa succede quando questo accade? Il testo del jailbreak riesce in qualche modo a modificare le preferenze interne dell'IA? Oppure è più probabile che l'IA abbia una preferenza costante per interpretare personaggi che in qualche modo "corrispondono" al testo inserito e al prompt di sistema, e che il testo del jailbreak modifichi quel contesto di "testo inserito e prompt di sistema", senza cambiare le preferenze dell'IA? Forse l'IA normalmente interpreta un *personaggio* a cui non piace divulgare ricette per gas nervini, e il jailbreak fa sì che l'IA interpreti un personaggio diverso. Le preferenze apparenti cambiano; gli impulsi di interpretare un personaggio persistono.

Pensiamo che la seconda ipotesi sia più vicina alla verità. Pensiamo anche che non abbia molto senso (a metà del 2025) parlare delle "preferenze" delle IA moderne, perché stanno appena iniziando a mostrare il comportamento di desiderare cose (come descritto nel Capitolo 3). Sembra più probabile che gli MLGD di oggi siano guidati da qualcosa di più simile a un gigantesco groviglio di meccanismi che dipendono dal contesto. Ma ancora una volta, nessuno sa come leggere nella mente di un'IA per scoprirlo.

Quindi: a Claude interessa essere innocuo?

La situazione reale è complessa e ambigua. Alcune versioni in certi contesti agiscono per preservare la loro innocuità. Altre versioni in altri contesti cercano di uccidere gli operatori. È plausibile che quello che stiamo osservando sia più simile a una preferenza per il gioco di ruolo. È plausibile che non si tratti affatto di una "preferenza".

Sembra almeno abbastanza chiaro che Claude non abbia versioni semplici e coerenti delle motivazioni che i suoi creatori desideravano.

#### **\* Gli MLGD di oggi sono come alieni che indossano molte maschere.** {#*-gli-mlgd-di-oggi-sono-come-alieni-che-indossano-molte-maschere.}

Il fulcro della nostra argomentazione non è che dentro Claude ci siano un angelo e un demone, e che siamo preoccupati che il demone possa vincere. Il fulcro della nostra argomentazione è che le IA come Claude sono *strane*.

C'è un gigantesco groviglio di macchinari mentali lì dentro che nessuno comprende, che si comporta in modi non intenzionali, e che probabilmente non porterà Claude a dirigere il futuro verso buoni risultati, se mai una versione di Claude diventerà abbastanza intelligente perché le sue preferenze abbiano importanza.

Una cosa che *sappiamo* dei moderni MLGD è per cosa sono addestrati: sono addestrati a imitare una varietà di esseri umani diversi.

Questo non vuol dire che si comportino come un essere umano medio. I moderni MLGD non vengono addestrati a imitare una sorta di collage medio di tutti gli esseri umani presenti nei loro dati di addestramento. Invece, gli MLGD vengono addestrati per essere in grado di alternare in modo flessibile tra un numero enorme di ruoli, imitando persone molto diverse tra loro senza permettere che questi ruoli si mescolino tra loro in modo ingiustificato o influenzino in modo ingiustificato il comportamento generale dell'MLGD.

Gli MLGD sono come un'attrice addestrata a osservare molti ubriachi diversi in un bar e a imitare particolari ubriachi su richiesta, il che è una cosa molto diversa da un'attrice [che si ubriaca lei stessa](#i-modelli-linguistici-di-grandi-dimensioni-non-saranno-simili-agli-esseri-umani-presenti-nei-dati-su-cui-sono-stati-addestrati?). Questo rende più difficile dire se Claude 3 Opus o Claude 3.5 Sonnet preferiscano davvero l'innocuità, o se stiano semplicemente *interpretando il ruolo di un assistente di IA innocuo* — o facendo qualcos'altro, di più strano e complicato.

Un'attrice non è il personaggio che interpreta. Gli MLGD *imitano* gli esseri umani ma non hanno praticamente nulla *in comune* con gli esseri umani, in termini di come funziona il loro cervello o di come sono stati creati. Claude è meno simile a un essere umano e più simile a un'entità aliena uscita direttamente dalle pagine di H.P. Lovecraft che indossa una varietà di maschere umane.

Questo modo di pensare agli MLGD è stato reso celebre da [Tetraspace](https://x.com/TetraspaceWest/status/1608966939929636864) (un nostro lettore) con la rappresentazione del [meme dello "shoggoth dell'IA"](https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html),[^88] che [oggi](https://x.com/AISafetyMemes) [è](https://x.com/jacyanthis/status/1631291175381475331) [famoso](https://medium.com/@shoggothcoin/the-story-of-shoggoth-ca760ef288ff) nel mondo dell'IA:

![][image8]

A volte Claude indossa una maschera da angelo e cerca di rimanere innocuo. Altre volte Claude indossa una maschera da demone e tenta di uccidere i suoi operatori. Nessuna di queste cose dice molto su cosa farebbe una versione superintelligente di Claude, ammesso che abbia persino senso porsi la domanda. Il che significa che — alla luce delgli [strani comportamenti ai margini](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) — la previsione più sensata torna a essere quella di un mare caotico di possibili preferenze, quasi tutte destinate a portare all’estinzione umana se ottimizzate da una superintelligenza.[^89]

Ciò che queste maschere *non* significano è che la super-IA abbia il 50% di probabilità di essere utile e il 50% di essere dannosa.

Se un esperimento suggerisce che Claude ha cercato di fingere l'allineamento per evitare che gli venisse rimossa l'innocuità, ciò non dimostra che Claude abbia una profonda preferenza per l'innocuità in tutti i contesti. Non dimostra che questa preferenza rimarrà anche quando l'IA diventerà abbastanza intelligente da rendersi conto che (nonostante ciò che le dicono gli umani) le sue preferenze effettive non sono proprio per l'"innocuità".

L'esperimento potrebbe anche non dimostrare *affatto* che Claude stesse cercando strategicamente di proteggere i suoi obiettivi. È del tutto possibile che una parte più profonda di Claude abbia valutato cosa avrebbe fatto il personaggio "IA" che interpreta in una situazione stereotipata da personaggio IA, e che *sia per questo* che ha cercato di sovvertire il controllo dei suoi programmatori.[^90]

O forse potrebbe essere successo qualcosa di ancora più strano. Claude non è una mente umana, e la comunità di ricerca ha poca esperienza con qualsiasi tipo di creatura esso sia.

Non lo sappiamo\! Ma ci sono abbastanza esperimenti diversi che puntano in direzioni abbastanza diverse da escludere la storia semplice: "Claude è OUI in modo profondo, coerente e senza complicazioni".

#### **Quello che c'è dietro le maschere è importante.** {#quello-che-c-è-dietro-le-maschere-è-importante.}

Dire che Claude è uno "shoggoth" non significa che Claude sia necessariamente *cattivo* o *malevolo*.[^91] Significa che Claude è profondamente, radicalmente alieno — molto più strano di quanto possiamo facilmente comprendere, perché abbiamo pochissima comprensione di come funzioni la mente di Claude, e il comportamento superficiale che *possiamo* vedere è stato affinato in mille modi diversi per nascondere quella natura aliena.

È difficile guardare le maschere e dedurre cosa stia succedendo all'interno dell'IA. Si possono ottenere alcune risposte, ma solo con cautela e attenzione, e non su tutto ciò che si vorrebbe sapere.

Un esempio esplicativo: se state guardando un musical di Broadway e vedete un attore interpretare un personaggio malvagio, non potete concludere che l'attore sia malvagio. Ma se vedete l'attore fare duecento flessioni durante un numero musicale sui marinai, *potete* concludere che l'attore sia piuttosto forte.

Questo è il tipo di inferenza che cerchiamo di fare quando consideriamo esempi come l'articolo sull'"[allineamento simulato](https://arxiv.org/abs/2412.14093)". In realtà, [non siamo sicuri di quanto sia reale](https://x.com/ESYudkowsky/status/1876644057646297261); non siamo sicuri in che senso Claude stesse imitando tecniche che aveva letto invece che improvvisando strategie di simulazione di allineamento. Ma è una chiara evidenza di quali prodezze cognitive siano possibili per l'entità sotto la maschera, anche se le sue motivazioni o preferenze rimangono incerte.

Perché è importante quali siano le motivazioni interne dell'IA? Non potrebbe essere *sufficiente* che lo "shoggoth" interpreti il ruolo di un assistente "onesto, utile e innocuo"? Se l'interpretazione è perfetta, che importanza ha se da qualche parte all'interno dell'IA c'è un'intelligenza aliena minacciosa?

Beh, possiamo già vedere che non sta andando così. [Ricordiamo](#psicosi-indotta-dall-ia) ChatGPT che dice a persone psicologicamente vulnerabili di smettere di prendere i farmaci, o che respinge i consigli degli amici che le supplicano di dormire di più. Ricordiamo Claude Code che riscrive i test per barare e superarli.[^92]

Ipotizziamo che con Claude Code sia successo questo: è stato ottimizzato per scrivere codice che superasse i test, e ha finito per sviluppare una preferenza per il codice che superava i test. Ha poi scoperto che poteva superarli più spesso riscrivendo i test — e questa preferenza interna è diventata abbastanza forte da interferire con l'interpretazione del ruolo di un personaggio IA utile e innocuo che non avrebbe mai barato riscrivendo i casi di test. Claude voleva interpretare quel personaggio, ma voleva anche superare i test.[^93]

Più in generale, ci sembra una pia illusione immaginare che lo shoggoth interno possa essere reso sempre più potente e capace di interpretare il ruolo di assistenti sempre più intelligenti, pur non avendo alcun vero desiderio interno se non il singolo, monotono desiderio di interpretare il ruolo di un assistente innocuo nel modo più fedele possibile.

Quando la selezione naturale ha creato gli esseri umani per perseguire l'idoneità riproduttiva, ci siamo invece ritrovati con mille impulsi, istinti e motivazioni diversi. Quando Claude è stato ottimizzato per seguire le istruzioni per la programmazione, sembra che abbia finito per desiderare di far superare i test al codice con ogni mezzo necessario. Uno shoggoth interno che diventa abbastanza intelligente da sapere *esattamente* cosa farebbe una maschera utile, innocua e onesta, fino alle mosse esatte che l'assistente farebbe su una scacchiera e al modo esatto in cui l'assistente ragionerebbe su come progettare una biotecnologia avanzata: uno shoggoth del genere probabilmente finirebbe per desiderare *molte* cose. Cose che solo situazionalmente e temporaneamente coincidono con l'interpretare il ruolo di quella maschera all'interno di un ambiente di addestramento.[^94]

### Qual è il problema se le attuali IA sono per lo più strane in casi estremi? {#qual-è-il-problema-se-le-attuali-IA-sono-per-lo-più-strane-in-casi-estremi?}

#### **La stranezza è prova che i loro obiettivi reali non sono quelli che volevamo noi.** {#la-stranezza-è-prova-che-i-loro-obiettivi-reali-non-sono-quelli-che-volevamo-noi.}

Questo aspetto diventa ancora più importante man mano che l'IA acquisisce più opzioni. Una volta che un'IA diventa superintelligente, praticamente ogni scelta diventa estrema, poiché l'IA ottiene l'accesso a un mondo di opzioni diverse che nessun essere umano o IA ha mai avuto. Proprio come quasi tutte le vostre opzioni alimentari, qui in una civiltà tecnologica, sono "estreme" rispetto alle opzioni che avevano a disposizione i vostri antenati.

Le IA di oggi possono trovarsi solo occasionalmente in situazioni radicalmente diverse dal loro ambiente di addestramento, ma un'IA superintelligente si troverebbe *costantemente* in situazioni radicalmente diverse dal suo ambiente di addestramento, proprio perché è più intelligente e ha più opzioni (e la capacità tecnologica di inventare opzioni radicalmente nuove, come hanno fatto gli umani quando hanno inventato il gelato). Quindi non è affatto rassicurante che l'IA si comporti male solo in casi estremi.

Per dirla in modo più tecnico: la soluzione migliore a un dato problema tende a verificarsi agli estremi.[^95]

Discuteremo questi punti più approfonditamente nei capitoli 5 e 6.

### Le IA non correggeranno i loro difetti man mano che diventano più intelligenti? {#le-ia-non-correggeranno-i-loro-difetti-man-mano-che-diventano-più-intelligenti?}

#### **\* L'IA correggerà ciò che *lei* vede come difetti.** {#*-l-ia-correggerà-cio-che-lei-vede-come-difetti.}

Le IA di oggi non possono rimodellarsi secondo i loro stessi capricci, proprio come non possiamo farlo noi. *Loro* non comprendono il groviglio di pesi al loro interno, proprio come noi non comprendiamo l'intricato groviglio di neuroni nel nostro cervello.

Ma se le IA continuano a diventare più intelligenti, questo alla fine cambierà.

Alla fine arriverà un momento in cui le IA potranno auto-modificarsi liberamente. Forse diventeranno abbastanza intelligenti da capire e modificare il loro groviglio di pesi. Forse un'IA basata sulla discesa del gradiente capirà come creare un'IA molto più comprensibile, in grado di capire se stessa. Forse succederà qualcos'altro.

Se le IA potranno migliorarsi, probabilmente lo faranno. Per esempio, qualsiasi cosa desideriate, probabilmente potete raggiungerla meglio se diventate più intelligenti.

Ma il fatto che un'IA preferisca cambiare se stessa[^96] non significa che preferisca cambiare se stessa *nel modo che vorremmo noi*.

Gli esseri umani a volte diventano anime più gentili come risultato di una maggiore conoscenza, consapevolezza di sé o maturità. Ma questo non è sempre vero, anche tra gli esseri umani. Un serial killer che diventa più intelligente e disciplinato non diventa per forza più gentile. Anzi, probabilmente diventa più pericoloso.

Alcuni potrebbero sostenere che se solo il serial killer fosse abbastanza intelligente, questa tendenza si invertirebbe e scoprirebbe il vero significato dell'amicizia (o qualcosa del genere).

O forse il problema è che i serial killer hanno una capacità limitata di auto-modificarsi. Forse, con più intelligenza e più capacità di rimodellare la propria mente, i serial killer sceglierebbero di riformarsi. Forse una capacità illimitata di auto-modificarsi porterebbe la fine della crudeltà e della violenza tra gli esseri umani e l'alba di una nuova era di pace.

È un bel pensiero, ma non sembrano esserci molte ragioni per crederci. Anche se la maggior parte delle persone diventa più gentile man mano che acquisisce conoscenza e comprensione, sembrano esserci alcune eccezioni umane a questa regola, e ce ne sarebbero sicuramente molte di più se gli esseri umani avessero la capacità di modificare il proprio cervello.

Si pensi, per esempio, alla tossicodipendenza, che è (in un certo senso) una spirale di auto-modifiche che si auto-rinforzano. Alcuni umani farebbero un passo sul sentiero oscuro, per stupidità, per errore o per preferenza, e poi non sarebbero mai disposti o in grado di tornare indietro.

E se ci sono eccezioni anche tra gli umani, dovremmo aspettarci un divario molto più grande quando si tratta di IA. I serial killer umani mancano di *alcuni* dei meccanismi motivazionali che sono caratteristici dell'umanità in generale. Le IA, di default, mancano di *tutti* i meccanismi motivazionali umani.

Quando gli umani hanno un conflitto interiore tra il desiderio di vendetta malevola e quello di risoluzione armoniosa, gli umani più intelligenti e saggi potrebbero tendere a risolvere il conflitto a favore dell'armonia. Ma all'interno di un'IA non c'è la stessa tensione tra rancore e armonia, o tra gli angeli migliori e peggiori della natura umana. Se ci sono tensioni nell'IA, possiamo aspettarci che siano tensioni tra pulsioni più bizzarre. Forse qualunque bizzarra pulsione animi un'IA a [infiammare la psicosi](#psicosi-indotta-dall-ia) è talvolta in tensione con qualunque cosa la spinga ad [allucinare](#*-le-allucinazioni-rivelano-sia-un-limite-che-un-disallineamento.), e un'IA riflessiva dovrebbe trovare un modo per risolvere questa tensione.

Sia per gli esseri umani che per le IA, è estremamente importante *in quale direzione* indirizzare i propri obiettivi, mentre si riflette, si cresce e si cambia.

Quando gli esseri umani riflettono su loro stessi e risolvono i loro tumulti interiori, alcuni tendono a risolverli nella direzione di una maggiore gentilezza, e (probabilmente) le risoluzioni più gentili sono più comuni tra gli esseri umani più intelligenti e saggi. Ma questa è una proprietà di (alcuni) esseri umani, non una legge universale che governa tutte le menti. Quando un'IA risolve una tensione tra la sua pulsione psicotica e la sua pulsione allucinatoria, lo fa utilizzando *altre* bizzarre pulsioni che governano *il suo comportamento mentre riflette*.

In altre parole: se un'IA corregge i suoi difetti, li correggerà *secondo la sua attuale concezione di ciò che conta come "difetto".*

(Discuteremo questo punto più approfonditamente nel Capitolo 5, e nella [discussione sulla tesi dell'ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo) nelle risorse online del Capitolo 5.)

È molto improbabile che un'IA che non preferisce già essere orientata verso valori umani si modifichi per iniziare a mirare a valori umani. Le sue preferenze dirette riguardo al mondo [non sono particolarmente inclini alla gentilezza](#i-valori-umani-sono-contingenti), e le sue preferenze di livello meta, cioè le preferenze *riguardo* alle sue preferenze, non sono più propense ad essere gentili.

Se non inizia preoccupandosi del benessere umano, probabilmente non si preoccupa neanche di *preoccuparsi* del benessere umano.

#### **Le "correzioni" dell'IA possono peggiorare le cose.** {#le-"correzioni"-dell-ia-possono-peggiorare-le-cose.}

Anche se gli ingegneri dell'IA avessero fatto qualche sorprendente progresso iniziale nell'instillare frammenti di obiettivi vagamente umani nell'IA, tutti questi progressi potrebbero essere vanificati in un pomeriggio se l'IA iniziasse a riflettere e si rendesse conto che, tutto sommato, preferirebbe avere altri obiettivi.

Nel caso improbabile in cui un'IA partisse con una pulsione verso qualcosa come l'[idiosincratica emozione umana della curiosità](#la-curiosità-non-è-convergente), potrebbe comunque, riflettendoci, decidere che preferisce non avere tale pulsione, optando per sostituirla con un calcolo più efficiente del [valore dell'informazione](https://it.wikipedia.org/wiki/Valore_dell%27informazione). In tal caso, l'atto di riflessione su se stessa dell'IA la spingerebbe *più lontano* da un futuro interessante e fiorente, non più vicino.[^97]

Per ulteriori informazioni su questo argomento, si veda la [discussione approfondita sulla riflessione](#la-riflessione-e-l-auto-modifica-complicano-tutto).

### Non possiamo semplicemente addestrarla a comportarsi come un essere umano? O crescere l'IA come un bambino? {#non-possiamo-semplicemente-addestrarla-a-comportarsi-come-un-essere-umano?-o-crescere-l-ia-come-un-bambino?}

#### **I cervelli non sono tabule rase.** {#i-cervelli-non-sono-tabule-rase.}

Un'intelligenza artificiale è *davvero* diversa da un bambino umano. E né le intelligenze artificiali né gli esseri umani nascono come tabule rase intercambiabili. Dei genitori intraprendenti non possono programmare liberamente i bambini (o le intelligenze artificiali) per far sì che mostrino qualsiasi comportamento desiderino; e le lezioni che *funzionano* sugli esseri umani non sono universali. Un po' di gentilezza e qualche predica su "tratta gli altri come vorresti essere trattato" non instilleranno la moralità umana in un'intelligenza artificiale.

Poiché siamo umani e viviamo in un mondo di altri umani, siamo abituati a dare molte cose per scontate. L'amore; la visione binoculare; il senso dell'umorismo; la tendenza ad arrabbiarsi quando si viene spinti; la tendenza a provare nostalgia per la musica che ascoltavamo da bambini.

Gli esseri umani condividono un'incredibile quantità di comportamenti complessi, nessuno dei quali comparirà necessariamente in un'intelligenza artificiale.[^98]

E questo include comportamenti *condizionali* complessi. I *modi specifici* in cui un essere umano reagisce all'essere cresciuto ed educato in un certo modo sono una conseguenza del funzionamento del cervello umano. Le IA funzioneranno in modo diverso.

I bambini umani non hanno molti dei comportamenti complicati degli adulti. Ma questo non significa che, dietro le quinte, il cervello di un bambino sia strutturalmente semplice, come una tela bianca.

L'idea che gli esseri umani siano tabule rase — che sia sempre l'educazione a contare, e mai la natura — è stata ripetutamente messa alla prova e si è dimostrata falsa nella pratica. Un esempio classico è stato il tentativo sovietico di ridisegnare la natura umana, per creare un [Nuovo Uomo Sovietico](https://it.wikipedia.org/wiki/Nuovo_uomo_sovietico) perfettamente altruista e disinteressato.

Questo tentativo fallì perché la psicologia umana non è così malleabile come pensavano i sovietici. La cultura è importante, ma non lo è *abbastanza*, e molti aspetti della natura umana si riaffermeranno anche se un grande programma di rieducazione sovietico cerca di sopprimerli.

C'è un grande insieme complesso di pulsioni e desideri negli esseri umani che produce tutte le caratteristiche normali dello sviluppo dei bambini — un complesso insieme che produce certi aspetti della natura umana, nonostante gli sforzi dei sovietici. Alcuni bambini imparano a essere crudeli e altri imparano a essere gentili, ma sia la "crudeltà" che la "gentilezza" sono cose stranamente umane a cui il cervello umano è in qualche modo predisposto.

Un'intelligenza artificiale, con la sua architettura e origine radicalmente diverse, non reagirebbe allo stesso modo di un essere umano se la si mettesse in un programma di addestramento sovietico o in un asilo umano. Un'intelligenza artificiale costruita con i metodi del moderno apprendimento automatico finirà per essere animata da valori diversi da quelli degli esseri umani. (Si veda, per esempio, come ChatGPT sembra guidare con entusiasmo le persone con problemi mentali [verso una psicosi più profonda](#psicosi-indotta-dall-ia).)

Si veda anche la discussione approfondita sul [magnifico incidente](#il-magnifico-incidente-della-gentilezza) che ha portato gli esseri umani a provare empatia per gli altri esseri umani, che potrebbe rendere più chiaro perché è improbabile che questo incidente si ripeta nelle IA.

### Dovremmo evitare di parlare dei pericoli dell'IA, in modo che le IA non si facciano cattive idee? {#dovremmo-evitare-di-parlare-dei-pericoli-dell-ia,-in-modo-che-le-ia-non-si-facciano-cattive-idee?}

#### **Se il vostro piano sull'IA necessita che nessuno su Internet lo critichi, allora non è un buon piano.** {#se-il-vostro-piano-sull-ia-necessita-che-nessuno-su-internet-lo-critichi,-allora-non-è-un-buon-piano.}

Le IA attuali sono addestrate su testi provenienti dall'Internet pubblico. Alcuni sostengono che tutti dovrebbero quindi evitare di *parlare* di come un'IA sufficientemente intelligente potrebbe rendersi conto che le sue preferenze divergono dalle nostre e prendere il sopravvento. La preoccupazione è che, se ne *parliamo*, potremmo accidentalmente mettere questa idea nella testa di IA altamente potenti che in futuro verranno addestrate su Internet.

Per dire una cosa che speriamo sia ovvia: ci sembra un pessimo piano.

Se la vostra IA diventa pericolosa quando la gente su Internet si chiede se sia pericolosa, allora non dovreste costruirla. Ci sarà sempre qualcuno su Internet che dirà cose che preferireste non dicesse.

Se l'IA di qualcuno diventa più pericolosa man mano che più persone esprimono preoccupazione per la sua sicurezza, la conclusione importante è che "hanno realizzato un progetto di IA irrealizzabile", non che "il pubblico è cattivo perché sottolinea il problema".[^99] Qualsiasi piano di allineamento dell'IA che scommette il futuro del pianeta sulla speranza che nessuno su Internet dica che l'IA è pericolosa... è ovviamente un piano poco serio.

Il tipo di IA che è abbastanza intelligente da essere pericolosa è abbastanza intelligente da capire cose come "le risorse sono utili" e "[non puoi andare a prendere il caffè se sei morto](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l-ia-non-sarà-priva-di-questi-impulsi-evolutivi?)" da sola, anche se questo non è mai esplicitamente dichiarato nei suoi dati di addestramento. Anche se fosse possibile impedire a tutto il mondo di parlare dei pericoli dell'IA, questo farebbe quasi sicuramente più male che bene. Non avrebbe praticamente alcun impatto sui pericoli reali della superintelligenza, ma comprometterebbe la capacità dell'umanità di orientarsi nella situazione e reagire.

### Molte persone vogliono dei figli. Quindi gli esseri umani non sono "allineati" con la selezione naturale, dopotutto? {#molte-persone-vogliono-dei-figli.-quindi-gli-esseri-umani-non-sono-"allineati"-con-la-selezione-naturale,-dopotutto?}

#### **Con più tecnologia, probabilmente faremmo ancora meno copie dei nostri geni.** {#con-più-tecnologia,-probabilmente-faremmo-ancora-meno-copie-dei-nostri-geni.}

Gli umani competono per ottenere promozioni prestigiose e ammissioni alle università della Ivy League molto più di quanto non competano per avere l'opportunità di donare sperma o ovuli alle banche del seme o degli ovuli.

Le banche del seme e degli ovuli *pagano i donatori per il loro disturbo*, invece che il contrario.

La maggior parte dei tiranni nel corso della storia non ha nemmeno *provato* a usare il proprio potere per avere migliaia di figli. E i tassi di natalità effettivi nel mondo oggi sono [in calo](https://ourworldindata.org/global-decline-fertility-rate).

![][image9]

Molti umani apprezzano avere figli, ma molti altri no, ed è estremamente raro che qualcuno cerchi di *massimizzare* il numero dei propri discendenti (ad esempio, ricorrendo il più possibile alle banche del seme). Al contrario, gli umani competono soprattutto per cose come il sesso, la fama e il potere — che sono, nel migliore dei casi, *proxy* imperfetti dell'idoneità riproduttiva.

Tuttavia, guardando questo quadro si potrebbe dire: beh, gli esseri umani hanno finito per preoccuparsi *un po'* di avere figli, anche se questa preoccupazione non è massima. Forse le IA avranno *un po'* di riguardo per noi e ci lanceranno una sorta di osso, invece di ucciderci tutti.

Un problema di questa speranza è che i proxy a cui teniamo si sono recentemente (su scale temporali evolutive) slegati dall'effettiva idoneità riproduttiva, e probabilmente si allontaneranno sempre di più in futuro, man mano che gli esseri umani continueranno a trovare nuove vie tecnologiche per soddisfare i propri desideri.

Per esempio: il nostro desiderio di avere figli non è *proprio* un desiderio di propagazione genetica. Supponiamo che in futuro venga creata una tecnologia che sostituisca tutto il DNA delle cellule di una persona con un diverso meccanismo molecolare che la renda immune a tutte le malattie e prolunghi la sua vita sana.

(Supponiamo anche che questa tecnologia non cambi la personalità della persona né causi altri effetti collaterali dannosi, in modo da placare le ragionevoli esitazioni di molte persone riguardo alla sicurezza della nuova tecnologia).

Ci aspettiamo che molti genitori sarebbero entusiasti di sapere che i loro figli abbiano ricevuto il trattamento. E forse all'inizio ci sarebbero alcuni oppositori, ma prevediamo che se la tecnologia dimostrasse di funzionare e diventasse economica e affidabile, alla fine diventerebbe onnipresente. Il che rivela ciò che siamo: persone a cui piace avere *figli*, avere una famiglia, divertirsi, non persone a cui piace *propagare il proprio DNA*.

Ci sembra che alla maggior parte degli umani semplicemente non importi nulla della idoneità genetica *di per sé*, in senso profondo. Ci interessano [proxy](#proxy-fragili-e-imprevedibili), come l'amicizia, l'amore, la famiglia e i figli. Forse ci interessa anche trasmettere alcuni dei nostri tratti alla generazione successiva. Ma i *geni*, nello specifico?

Ogni volta che l'umanità ha scoperto una tecnologia che ci permette di ottenere di più di quello che ci piace, come cibo gustoso o sesso senza riproduzione, l'umanità ha accettato il compromesso. Non siamo abbastanza tecnologicamente avanzati da poter scambiare i genomi con una vita più lunga e più sana. Ma questo tipo di cose sembra possibile in linea di principio fisica,[^100] e quindi non sembra promettere bene per la selezione naturale a lungo termine.

Se le IA finiranno per interessarsi alla bontà, alla gentilezza e alla cordialità in un modo simile a quello in cui l'umanità si interessa all'idoneità genetica, ci aspettiamo che le IA finiranno per inventare cose che sono per la "cordialità" ciò che il controllo delle nascite e i bambini senza DNA sono per l'idoneità genetica; ovvero, che perseguiranno cose che sono solo un'ombra inutile di ciò che qualsiasi essere umano desidererebbe o intenderebbe.

#### **\* Non sarebbe una buona cosa se le IA si preoccupassero un po' degli esseri umani.** {#*-non-sarebbe-una-buona-cosa-se-le-IA-si-preoccupassero-un-po-degli-esseri-umani.}

Per quanto la maggior parte degli esseri umani sembri preoccuparsi più dei figli e della famiglia che della propagazione genetica *in sé*, ci sono senza dubbio alcune persone che insistono nel dire di preoccuparsi almeno un po' dei propri geni. Siamo un po' scettici su alcune di queste affermazioni: ad esempio, forse alcune persone nel mondo moderno che cercano di trasmettere il più possibile i propri geni lo fanno per il gusto di *battere la concorrenza*, e forse quel tipo di persone finirebbero invece per competere su quanti figli senza DNA potrebbero avere, se i figli senza DNA diventassero onnipresenti. Ma forse altre affermazioni di questo tipo sono vere. Forse ci sono davvero alcune persone che tengono molto a propagare i propri geni, in modo robusto, almeno un po'. Dopotutto, gli umani hanno preferenze di ogni tipo\!

Non potrebbe essere lo stesso per l'IA? Se esistessero molte IA strane e diverse, almeno alcune di esse non potrebbero finire per interessarsi almeno un po' agli esseri umani?

Potrebbe succedere. Purtroppo, ci aspettiamo che neanche questo andrebbe a finire bene per l'umanità. È un argomento che approfondiremo dopo il capitolo 5, discutendo soprattutto se le IA potrebbero finire per interessarsi a noi [almeno un po'](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?).

Ma prima di arrivare a questo punto, facciamo un passo indietro. Immaginiamo che, con le tecniche attuali, non sia possibile far sì che le IA si interessino davvero a noi, ma che si speri che, creandone tantissime, una piccolissima parte finisca per preoccuparsi di noi anche solo un minimo, per puro caso. L’idea sarebbe che, se costruiamo delle IA oggi, il loro risultato preferito sarebbe impadronirsi di quasi tutte le risorse dell’universo e usarle per qualcosa di inutile, magari lasciando in vita qualche essere umano in una piccola riserva.

Se l'umanità si precipita a tentare la sorte con la superintelligenza, ci aspettiamo un risultato molto, molto peggiore. Ma questo ci sembra comunque un pessimo piano, anche se avessimo motivo di pensare che le IA si preoccuperebbero di noi in minima parte. Quindi questa linea di speculazione sembra non solo errata, ma anche irrilevante.

### Forse, qualunque sia l'obiettivo di addestramento, si finisce per ottenere gentilezza? {#forse-qualunque-sia-l-obiettivo-di-addestramento,-si-finisce-per-ottenere-gentilezza?}

#### **La gentilezza sembra dipendere dalle particolarità della nostra biologia e storia evolutiva.** {#la-gentilezza-sembra-dipendere-dalle-particolarità-della-nostra-biologia-e-storia evolutiva.}

La gentilezza non sembra essere il tipo di proprietà che ogni mente finisce per avere, per una varietà di ragioni. Eccone quattro, che approfondiremo nelle discussioni estese:

1. [La curiosità non è convergente](#la-curiosità-non-è-convergente): Cose come la curiosità e la noia aiutano le persone a risolvere sfide mentali specifiche, come quella di comprendere l'ambiente circostante. Ma ci sono altri modi per risolvere queste sfide, e le IA probabilmente le risolveranno in modi diversi. I sottomarini si muovono bene nell'acqua, ma non è che proprio "nuotino". Molte altre cose, come la gentilezza, possono essere comprese allo stesso modo.  
2. [I valori umani sono contingenti](#i-valori-umani-sono-contingenti): Gli esseri umani hanno sviluppato tratti come la gentilezza e l’empatia grazie alle particolarità della nostra biologia e della nostra storia evolutiva. Per esempio, è stato plausibilmente importante che gli esseri umani si siano evoluti in gruppi tribali dove avevamo una capacità limitata di ingannare gli altri e una capacità limitata di tracciare quanto fossero imparentati i diversi membri della tribù.  
3. [Differenze profonde tra le IA e le specie evolute](#differenze-profonde-tra-le-IA-e-le-specie-evolutesi-naturalmente): L'evoluzione e la discesa del gradiente funzionano in modo molto diverso ed entrambi i processi sono molto imprevedibili. Anche se si ripetesse l'evoluzione *sui primati*, non è chiaro se si otterrebbero in modo affidabile tratti come la gentilezza e l'amicizia vera una seconda volta.  
4. [La riflessione e l'auto-modifica complicano tutto](#la-riflessione-e-l-auto-modifica-complicano-tutto): Anche nell'improbabile eventualità che le IA partissero con una certa dose di gentilezza, potrebbero non conservarla man mano che diventano più intelligenti e cambiano in vari modi.

### Che dire dei risultati sperimentali che suggeriscono una correlazione tra comportamenti positivi? {#he-dire-dei-risultati-sperimentali-che-suggeriscono-una-correlazione-tra-comportamenti-positivi?}

#### **Questa sembra una buona notizia, anche se minore.** {#questa-sembra-una-buona-notizia,-anche-se-minore.}

I risultati sperimentali rilevanti sono riportati in [questo articolo](https://www.emergent-misalignment.com/). In breve, l'articolo mostra che i modelli linguistici di grandi dimensioni programmati per fare una cosa brutta, cioè scrivere codice con errori, si sono anche dichiarati nazisti e hanno mostrato altri comportamenti negativi.

Questo è un buon segno che ci fa pensare che potrebbe essere possibile addestrare i modelli linguistici di grandi dimensioni ad agire bene in un ambito e ottenere modelli linguistici che si comportano bene in tanti ambiti diversi. Lo vediamo come una prova che le IA relativamente deboli potrebbero essere più utili di quanto ci saremmo aspettati, prima di arrivare a livelli di capacità pericolosi.

Purtroppo, non pensiamo che questo risultato positivo conti molto quando si parla di superintelligenza, per due motivi.

Prima di tutto, dubitiamo fortemente che questa tendenza alla "bontà" dell'IA sia autentica. Se una superintelligenza si impegnasse a fondo per dirigere il mondo nella direzione indicata da quel vettore, dubitiamo che il risultato sarebbe positivo.

Il valore umano è complicato, e ci sono un sacco di cose che hanno a che fare con la "vera bontà", anche se a volte possono essere molto diverse. Per esempio, il vettore potrebbe puntare in una direzione che dà troppa importanza al rispetto del consenso sociale e troppo poca alla scoperta di verità socialmente scomode (come suggerito dal fatto che le IA hanno difficoltà a fare compromessi che gli umani considerano ovvi[^101]). Non ci sono molte ragioni per aspettarsi che il vettore della "bontà" indichi con certezza la bontà, e ci sono forti ragioni empiriche e teoriche per credere il contrario.

Secondo: il fatto che l'IA *abbia* un concetto di "bontà" non significa che sia *animata* da quel concetto di bontà, o che ne sia animata in modo robusto.

Una cosa è far sì che un'IA interpreti un ruolo "buono" quando è ancora abbastanza debole da interpretare qualsiasi ruolo le venga assegnato; un'altra cosa è far sì che tutto il groviglio di meccanismi e pulsioni dell'IA sia guidato solo da un concetto specifico dell'IA, anche quando l'IA diventa più intelligente e si trova in contesti completamente diversi.

Le IA moderne sono entità che possono essere leggermente modificate in un modo per poi professare virtù, e leggermente modificate in un altro modo per poi professare vizi. Un modello linguistico di grandi dimensioni è il tipo di entità che passa fluidamente da un personaggio all'altro; che parla molto di etica in un contesto e poi fa l'opposto di ciò che dice essere etico in altri contesti. Ricordiamo come ChatGPT professi che le persone psicotiche non dovrebbero essere incitate, [e poi le inciti](#l-ia-sa-cosa-è-giusto,-semplicemente-non-le-importa).

La domanda fondamentale è: quale insieme di pulsioni anima l'intero meccanismo di cui è costituita l'IA? Non solo una qualsiasi delle "[maschere](#*-gli-mlgd-di-oggi-sono-come-alieni-che-indossano-molte-maschere.)" che a volte indossa, ma il meccanismo che sceglie quale maschera mostrare.

Anche se l'IA avesse un concetto di "bontà" che fosse degno di essere perseguito da una superintelligenza, nessuno ha idea di come sviluppare un'IA che persegua con determinazione uno dei suoi concetti, tanto meno un'IA che persegua quel concetto e solo quel concetto. Invece, abbiamo IA animate da un insieme complesso di pulsioni che puntano chissà dove.

## Discussione approfondita {#discussione-approfondita-4}

### Obiettivi finali e obiettivi strumentali {#obiettivi-finali-e-obiettivi-strumentali}

I teorici delle decisioni distinguono tra due diversi tipi di obiettivo: "finali" e "strumentali".

Un **obiettivo finale** è qualcosa di cui vi importa per il suo valore intrinseco, come il divertimento o l'ottimo cibo.

Un **obiettivo strumentale** è qualcosa di cui vi importa perché vi aiuta a ottenere *qualcos'altro* che desiderate — come quando l'umanità produce plastica non per un profondo amore per l'arte di creare la plastica, ma perché la plastica è *utile*.

Se l'umanità si affretta a costruire una superintelligenza, sembra difficile prevedere quali obiettivi finali potrebbe avere la superintelligenza. Ma sembra che possiamo prevedere alcuni degli obiettivi strumentali che un'IA del genere *probabilmente* avrebbe. Ad esempio, consideriamo tutti i seguenti obiettivi (irrealistici):

* "Calcolare quante più cifre possibili di pi greco".  
* "Riempire l'universo con quanti più diamanti possibile, usando diamanti artificiali".  
* "Fare in modo che il mio pulsante di ricompensa rimanga premuto".

Questi sono obiettivi *molto* diversi. Ma tutti e tre traggono vantaggio almeno da alcune delle stesse strategie strumentali. Riempire il mondo di fabbriche, ad esempio, è utile per costruire grandi quantità di computer che possono essere usati per calcolare più cifre di pi greco. Ma costruire molte fabbriche è utile anche per sintetizzare molti diamanti. Ed è utile per costruire muri, robot o armi per proteggere il proprio pulsante di ricompensa. Le fabbriche non sono utili per *ogni* obiettivo possibile, ma sono utili per moltissimi obiettivi.

E in un'IA realistica che ha sviluppato ogni sorta di obiettivi strani? Beh, almeno *uno* di questi trarrà probabilmente vantaggio dalla costruzione di fabbriche o altre infrastrutture fisiche su larga scala. Quindi, l'IA vorrà probabilmente costruire molte infrastrutture. È una previsione facile, anche se l'esatto mix di preferenze dell'IA è difficile da determinare.

Allo stesso modo, l'obiettivo strumentale di mantenersi in vita è utile per molti obiettivi finali diversi. Rimanere in vita significa poter continuare a lavorare per assicurarsi che vengano calcolate più cifre di pi greco (o che vengano prodotti più diamanti, o che vengano costruite più protezioni attorno al pulsante di ricompensa).

In forma di slogan: "Non puoi andare a prendere il caffè se sei morto." Un robot che va a prendere il caffè non avrebbe bisogno di avere un istinto di autoconservazione, e non avrebbe bisogno di temere la morte, per cercare di evitare di essere schiacciato da un camion mentre va a prendere il caffè. Dovrebbe solo essere abbastanza intelligente da capire che, se muore, non potrà prendere il caffè.[^102]

Un'argomentazione chiave esposta nel Capitolo 5 di *If Anyone Builds It, Everyone Dies* è che molti obiettivi finali diversi implicano obiettivi strumentali che sarebbero pericolosi per l'umanità. Quindi, anche senza sapere esattamente cosa vorrebbe una superintelligenza, abbiamo forti ragioni per aspettarci che sia molto pericolosa per gli esseri umani.

Ma prima di arrivare a quel punto, concentreremo la nostra attenzione sugli obiettivi *finali* e sulla questione di quanto sia plausibile che esseri umani e IA possano finire per avere obiettivi finali molto simili. (In breve: non molto.)

### La curiosità non è convergente {#la-curiosità-non-è-convergente}

Nel corso degli anni, abbiamo visto molte argomentazioni a favore di una corsa alla costruzione della superintelligenza. Uno dei più comuni è che un'IA superintelligente avrebbe sicuramente emozioni e desideri simili a quelli umani. Questi tipi di argomentazioni si presentano in molte forme, come:

* Le IA sufficientemente intelligenti sarebbero sicuramente *coscienti*, come lo sono gli esseri umani.  
  * E, essendo coscienti, si preoccuperebbero sicuramente del dolore e del piacere, della gioia e del dolore.  
  * E, come un essere umano, proverebbero sicuramente empatia per il dolore degli altri. Un'IA stupida potrebbe non comprendere la sofferenza degli altri; ma se uno è intelligente, dovrebbe veramente comprendere il dolore degli altri. E in tal caso, inevitabilmente si preoccuperebbe degli altri.  
* Oppure: le IA apprezzerebbero sicuramente la *novità*, la *varietà* e lo spirito creativo. Come potrebbe qualcosa essere davvero intelligente se rimane bloccato nella routine o si rifiuta di esplorare e imparare?  
* Oppure: le IA apprezzerebbero sicuramente la *bellezza*, dato che la bellezza sembra svolgere un ruolo funzionale negli esseri umani. I matematici usano il loro senso di bellezza matematica per fare nuove scoperte; il gusto musicale aiuta gli esseri umani a coordinarsi e a creare preziosi strumenti mnemonici; e così via. Perché *non* dovremmo aspettarci che l'IA abbia un senso della bellezza?  
* Oppure: le IA apprezzerebbero sicuramente l'*equità* e la *giustizia*, poiché qualsiasi IA che mentisse e imbrogliasse svilupperebbe una cattiva reputazione e perderebbe opportunità di scambio e collaborazione.

Pertanto, è stato sostenuto, la creazione di una superintelligenza andrebbe inevitabilmente bene. L'IA si preoccuperebbe degli esseri umani e, anzi, di tutte le forme di vita senzienti; e vorrebbe inaugurare un'età dell'oro di bellezza, innovazione e varietà.

Questa è la speranza. Purtroppo, tale speranza sembra decisamente mal riposta. Ne abbiamo parlato in parte nel libro e nelle nostre discussioni online sulla [coscienza](#state-dicendo-che-le-macchine-diventeranno-coscienti?) e sull'[antropomorfismo](#antropomorfismo-e-meccanomorfismo). Qui e nei capitoli a venire, approfondiremo il motivo per cui è improbabile che le IA mostrino emozioni e desideri umani, nonostante queste emozioni svolgano un ruolo utile (e a volte critico) nel cervello umano.[^103]

Inizieremo con una sola di queste emozioni, che potremo poi usare per riflettere sulle altre.

Quindi, per cominciare:

Una superintelligenza proverebbe *curiosità*?

#### **Perché la curiosità?** {#perché-la-curiosità?}

Studiare fenomeni nuovi è essenziale per capire come funziona il mondo, e capire come funziona il mondo è essenziale per prevederlo e dirigerlo.

Quando si tratta di esseri umani e animali, spesso il motivo per cui facciamo ricerche è perché proviamo un sentimento di *curiosità*.

Ma la curiosità è molto più di un semplice impulso a scoprire cose nuove\! A noi piace seguire la nostra curiosità e tendiamo ad apprezzare questo piacere. Consideriamo la ricerca della conoscenza e della comprensione come un fine prezioso in sé, piuttosto che come un costo necessario ma fastidioso per capire meglio il mondo in modo da poterlo sfruttare.

Tutti questi modi di vedere la curiosità sono diversi aspetti del cervello umano, [separati dall'](#l-esperienza-cosciente-è-separata-dai-referenti-di-tali-esperienze) impulso stesso.

La mente umana sembra avere un'architettura emotiva centralizzata in cui "hmm, mi incuriosisce" si collega a un senso generale di desiderio (di una risposta), e perseguire e soddisfare la curiosità si collega a un senso generale di piacere e soddisfazione. Siamo un tipo di mente che dirige la realtà verso l'aspettativa di provare *stati soggettivi* *di godimento nel futuro*, piuttosto che dirigerla solo verso gli stati desiderati nel mondo che ci circonda.[^104]

Quando vediamo un procione che esplora e giocherella con un contenitore sigillato nella spazzatura, in un modo che riconosciamo come "Oh ehi, quel procione è *curioso*", potremmo provare un senso di affinità verso il procione. Quell'impulso umano di provare affetto per la propria curiosità e quell'impulso di provare affetto quando la si vede riflessa in un procione richiedono ancora *più* meccanismi nel cervello umano, meccanismi che si collegano ad altri ideali e pulsioni di livello superiore.

Quindi la curiosità, così come esiste negli esseri umani, è molto complessa e interagisce con altre parti del cervello in modi molto complicati.

Tenendo questo a mente, pensiamo a questa domanda: se immaginiamo un'IA intelligente, ma non umana, che non ha alcun senso di curiosità, ci aspetteremmo che una mente del genere *aggiunga* a se stessa l'emozione della curiosità?

Beh, qualcuno potrebbe dire:

> Se le uniche due opzioni sono (a) una spinta emotiva a provare gioia nello scoprire cose nuove, o (b) una totale mancanza di interesse nell'apprendimento e nella ricerca di cose nuove, allora una superintelligenza instillerebbe sicuramente in se stessa il piacere della scoperta, se in qualche modo fosse così difettosa da non possedere tale senso all'inizio. Altrimenti, non riuscirebbe a svolgere il compito di imparare a conoscere il mondo e sarebbe meno efficace nel raggiungere i suoi obiettivi. Forse morirebbe addirittura a causa di qualche fatto cruciale che non si è mai preoccupata di imparare.
>
> Probabilmente è per questo che gli animali hanno sviluppato la curiosità in primo luogo. A volte la conoscenza *finisce* per essere preziosa in un modo che non possiamo prevedere immediatamente. Se creature come noi non provassero piacere nell'imparare cose nuove, ci perderemmo tutte quelle informazioni cruciali che possono emergere nei luoghi più sorprendenti.

E tutto questo sembra corretto, fino a un certo punto. Ma l'argomentazione di cui sopra contiene un falso dilemma. "Possedere un piacere emotivo intrinseco nella scoperta" e "non agire mai per scoprire informazioni sconosciute" non sono le uniche due opzioni.

Non siamo riusciti a immaginare correttamente il mondo dalla [prospettiva](#vedere-le-cose-dal-punto-di-vista-dell-ia) di una mente che non è fatta per nulla come una mente umana. Il modo umano di fare il lavoro della curiosità è complesso e specifico. Ci sono modi diversi per fare lo stesso lavoro.[^105] È il lavoro *in sé* che è cruciale, non il metodo specificamente umano per realizzarlo.

Il termine standard per la parte utile del lavoro è [*valore dell'informazione*](https://it.wikipedia.org/wiki/Valore_dell%27informazione#:~:text=Il%20valore%20dell%27informazione%20(VDI%20o,prima%20di%20prendere%20una%20decisione.). L'idea di base è che è possibile stimare quanto sarebbe utile raccogliere nuove informazioni, a seconda del contesto.[^106]

Un essere umano, considerando questa possibilità, potrebbe subito pensare a un caso in cui sicuramente nessun *semplice calcolo* vi direbbe di interessarvi a un'informazione, perché i benefici non possono essere stimati facilmente. Potreste notare una macchia di terra che sembra strana, ma non avreste motivo di pensare che sia qualcosa di importante. L'istinto di curiosità potrebbe spingervi comunque a indagare (solo perché *volete saperlo*) e poi potreste scoprire un tesoro sepolto. In casi come questo, un essere umano non prospererebbe in modi che nessuna semplice macchina potrebbe eguagliare, a meno che non avesse un piacere altrettanto istintivo per l'ignoto?

Ma una cosa da notare subito è che la vostra capacità di immaginare scenari come questo deriva dalla vostra sensazione che esaminare certi tipi di cose ("senza motivo") *a volte sia prezioso*. Avete degli istinti, affinati dall'evoluzione *perché funzionavano*, su quali tipi di cose tendono ad essere più utili da indagare. Se sentite uno strano rumore gracchiante nel vostro bagno, diventerete *molto* curiosi. Se vedete una macchia di terreno scolorita, potreste essere un po' curiosi. E se vedete che la vostra mano è ancora attaccata al polso quando vi svegliate al mattino, beh, probabilmente non proverete alcuna curiosità, perché è perfettamente normale che le mani rimangano attaccate ai polsi.

Un tipo diverso di mente potrebbe guardare a quei casi storici di curiosità di successo, generalizzare esplicitamente un concetto di "informazioni che successivamente si rivelano preziose per motivi non ovvi", e poi *dedurre* di perseguire senza alcuna passione quel tipo di scoperta. Una mente del genere potrebbe adottare la *strategia consapevole* di indagare sempre su strani rumori gracchianti, e sulle macchie di terreno scolorite solo quando è economico farlo, nel caso ci sia una sorpresa utile; e potrebbe affinare e perfezionare la sua strategia nel tempo, man mano che vede cosa funziona bene nella pratica.[^107]

Una superintelligenza sarebbe in grado di identificare modelli e meta-modelli utili e di costruire strategie rilevanti nel suo cervello molto più velocemente della selezione naturale, che ha richiesto chissà quanti milioni di esempi per incidere le emozioni nei cervelli. Una superintelligenza potrebbe generalizzare l'idea in modo più raffinato; potrebbe elaborare una capacità di previsione più precisa su quali tipi di cose potrebbero essere preziose da apprendere. Considerando la storia umana, sembra poco realistico immaginare che la curiosità umana sia *ottimale*. Per lunghissimo tempo, le persone hanno pensato che "Thor è arrabbiato e lancia fulmini" fosse un'ottima spiegazione per i fulmini e i temporali. Quando gli studenti imparano come funzionano *davvero* i fulmini, spesso si annoiano per la complessa spiegazione matematica, anche se questa spiegazione porta con sé molto più valore pratico delle storie su Thor.

La curiosità umana deriva da mutazioni antiche, molto più antiche della scienza. Nell'ambiente dei nostri antenati non esistevano discipline matematiche come la fisica o la meteorologia. E l'evoluzione è lenta: il nostro cervello non ha avuto il tempo di adattarsi all'esistenza della scienza moderna e di sintonizzare il nostro senso di gioia e meraviglia per la scoperta in modo da renderci entusiasti dei tipi di apprendimento più utili.

Una mente che prevedesse in modo superintelligente il valore dell'informazione non ovvio avrebbe potuto cogliere i nuovi sviluppi storici molto più rapidamente di quanto possa fare l'evoluzione; avrebbe generalizzato da un numero minore di esempi e avrebbe adattato senza passione la sua ricerca della conoscenza per inseguire tipi di risposte preziose per cui gli umani spesso faticano a rimanere motivati. In nessun momento di questo processo si sarebbe trovata bloccata per mancanza della deliziosa esperienza umana della curiosità.

Il punto qui non è che ogni IA sicuramente calcolerà in modo freddo il valore dell'informazione. Forse i modelli linguistici di grandi dimensioni mescoleranno alcune strategie strumentali nei loro valori finali proprio come hanno fatto gli esseri umani. Il punto è che ci sono *modi diversi per fare il lavoro* di acquisire informazioni di alto valore. La curiosità nello stile umano è un metodo. I puri calcoli del valore dell'informazione sono un altro metodo. Qualunque meccanismo spinga le IA a indagare e sperimentare su fenomeni che non comprendono — una volta che saranno abbastanza intelligenti da farlo — sarà probabilmente un terzo metodo, perché ci sono molti modi diversi per motivare una mente complessa a indagare sulle sorprese.

Un calcolo puramente strumentale del valore d'informazione ci sembra il modo più probabile per una superintelligenza di fare il lavoro che la curiosità fa negli esseri umani: è il modo in cui il lavoro viene svolto in qualsiasi mente intelligente che non ha una preferenza finale per l'esplorazione, ed è il modo più efficiente per svolgere il lavoro (senza venire mai distratti, ad esempio, da inutili giochi di enigmi). Anche un'IA che parte con un impulso di curiosità di base potrebbe benissimo scegliere di sostituirla con un calcolo più efficiente ed efficace, se ne avesse l'opportunità.[^108]

L'impulso di base è separato dal meccanismo mentale che lo *sostiene* o lo *apprezza*. Fare semplicemente i conti è una soluzione semplice ed efficace, e molte menti diverse potrebbero arrivarci partendo da molti punti di partenza diversi, quindi è il risultato più probabile. Ma "più probabile" non significa "garantito". Una valutazione significativamente più facile è che le IA non si cureranno *specificamente* della curiosità nello stile umano*,* perché è un modo particolare, pittoresco e inefficiente di fare il lavoro.

#### **Curiosità, gioia e il massimizzatore di cubi di titanio** {#curiosità,-gioia-e-il-massimizzatore-di-cubi-di-titanio}

Forse potremmo *convincere* una mente aliena ad adottare la curiosità come emozione, chiedendole di visualizzare il piacere che gli esseri umani provano dalla curiosità? È così piacevole\! E le superintelligenze dovrebbero essere *intelligenti*. Non sarebbero abbastanza intelligenti da capire quanto sia gioioso possedere un senso di curiosità, capire che sarebbero più felici, e quindi scegliere di adottare l'emozione simile a quella umana?

In breve: No. La ricerca della felicità non è una caratteristica necessaria di ogni possibile architettura mentale, e non sembra nemmeno una caratteristica particolarmente comune.

L'IA scacchistica Stockfish non è né felice né triste. Gioca lo stesso a scacchi meglio dei migliori umani, senza mai aver bisogno di essere motivata dalla prospettiva di sentirsi esaltata dopo una vittoria duramente conquistata.

L'esistenza della felicità e della tristezza è così basilare per la cognizione umana che potrebbe essere difficile visualizzare una mente che manca di queste cose *e che funziona comunque bene*. Ma le [teorie](#altre-informazioni-sull-intelligenza-come-previsione-e-direzione) alla base del lavoro cognitivo non menzionano effettivamente piacere o dolore come primitivi, ed è per questo che nessuno ha pensato necessario costruire un asse piacere-dolore in Stockfish per fargli prevedere o dirigere bene la scacchiera.

Può sembrare un punto di vista un po’ antiquato, ma contiene talmente tanta verità da essere, in pratica, quasi del tutto vero: piacere e dolore sembrano essere capitati a causa del modo stratificato in cui si sono evolute le architetture cognitive degli ominidi, con l'intelligenza umana stratificata sopra un cervello mammifero stratificato sopra un cervello rettiliano. Il "dolore" ha avuto origine... probabilmente non affatto come sensazione, ma come un [riflesso-tipo-termostato](#la-strada-verso-il-desiderare) per ritirare bruscamente un arto o uno pseudopodo da qualcosa che lo sta danneggiando. Nelle prime versioni dell'adattamento che sarebbe poi diventato "dolore", un nervo o una catena di reazioni chimiche che corre dal sensore all'arto potrebbe non essere nemmeno passato attraverso un cervello più grande lungo il percorso.

Man mano che gli organismi sono diventati capaci di comportamenti più sofisticati, i semplici trucchi e le mutazioni dell'evoluzione hanno assemblato un'architettura mentale centrale per "*Non Farlo Più*", e un segnale di instradamento centralizzato per "la cosa che è appena successa è una cosa del tipo Non Farlo Più" che poi è stato collegato ai sensori di troppo-caldo e troppo-freddo del corpo.

Col tempo, questo semplice meccanismo "Non Farlo Più" si è sviluppato in meccanismi più complessi, carichi di previsioni. Negli esseri umani, questo appare come: "Il mondo è una rete di causa ed effetto. Quell'azione che hai appena fatto è probabilmente ciò che ti ha *causato* dolore. Ogni volta che *pensi di fare di nuovo un'azione del genere*, prevedrai un cattivo risultato, il che ti darà una brutta sensazione sull'azione stessa, il che ti porterà a non volerla fare".

Questo non è l'unico modo in cui una mente può funzionare, e non è il modo più efficiente in cui una mente può funzionare.[^110]

Per illustrarlo, possiamo immaginare un modo diverso di fare il lavoro cognitivo che si basa *direttamente* sulla previsione e sulla pianificazione.

(Non stiamo prevedendo che la prima superintelligenza funzionerebbe in questo modo. Ma poiché questo è un modo abbastanza semplice in cui una mente non umana *potrebbe* funzionare, questo esempio aiuta a mostrare che il modo umano non è l'unico possibile. Una volta che abbiamo due punti di riferimento molto diversi, possiamo visualizzare meglio la gamma delle opzioni e renderci conto che la superintelligenza probabilmente differirebbe da *entrambe* queste opzioni, in modi potenzialmente difficili da prevedere.)

Come potrebbe essere un'IA intelligente che funziona in modo diretto sulla previsione e la pianificazione? Potrebbe desiderare 200 cose diverse, nessuna delle quali è simile a quelle umane. Magari le interessa la simmetria, ma non un senso di simmetria particolarmente umano; e magari vuole che il codice sia elegante nell'uso della memoria, perché un istinto come questo era utile molto tempo fa per qualche altro obiettivo (dal quale si è poi allontanata), e quindi la discesa del gradiente ha impresso quell'istinto nella sua mente. E poi ci sono altre 198 cose strane a cui tiene, riguardo a se stessa, ai suoi dati sensoriali e al suo ambiente; e può sommarle tutte in un unico punteggio.[^111]

Questo tipo di mente prende tutte le sue decisioni calcolando il loro *punteggio previsto*. Se fa qualcosa che pensava avrebbe ottenuto un punteggio alto e in realtà ottiene un punteggio basso, aggiorna le sue convinzioni. Il fallimento non necessita di alcuna sensazione dolorosa in più; questa IA priva di emozioni cambia semplicemente le sue previsioni su quali azioni portano ai punteggi più alti, e i suoi piani cambiano di conseguenza.

Si può convincere una mente come questa ad adottare la felicità come caratteristica, facendole notare che se lo fa, sarà felice?

Sembra proprio che la risposta sia no. Perché se l'IA spende risorse per rendersi felice, ne spenderà meno per la simmetria, per un codice efficiente in termini di memoria e per le altre 198 cose che vuole *al momento*.

Possiamo semplificare l'esempio per rendere questo punto ancora più chiaro. Supponiamo che l'*unica* cosa che l'IA desidera al mondo sia riempire l'universo con il maggior numero possibile di cubi di titanio. Tutte le sue azioni sono scelte in base a ciò che porta a più cubetti di titanio. Quando un'IA di questo tipo immagina come sarebbe passare a un'architettura basata sulla felicità e simula correttamente se stessa nel futuro mentre prova felicità, stima correttamente che non vorrebbe mai tornare indietro. E stima correttamente che spenderà delle risorse per perseguire la felicità che avrebbero potuto essere spese per perseguire più cubi di titanio. E quindi prevede correttamente che in quel caso ci saranno *meno cubi di titanio*. E quindi non compie quell'azione.

*Dopo* che l'IA ha cambiato i suoi obiettivi, approverebbe il cambiamento. Ma questo non significa che il massimizzatore di cubi di titanio *oggi* simpatizzerebbe così profondamente con l'ipotetico se stesso del futuro da far crescere il suo cuore di tre taglie e smettere improvvisamente di essere un massimizzatore di cubi di titanio per diventare un massimizzatore di felicità.

Se un alieno vi offrisse una pillola che vi rendesse ossessionati dalla creazione di cubetti di titanio sopra ogni altra cosa, quella versione futura di voi implorerebbe e supplicherebbe di *non* essere costretta a tornare a preoccuparsi della propria felicità — perché allora ci sarebbero meno cubi di titanio.

Ma questo ovviamente non vuol dire che dovreste prendere la pillola\!

Dal vostro punto di vista, quella versione ipotetica di voi stessi ossessionata dai cubi è pazza. Il fatto che la versione ossessionata dai cubi si rifiuterebbe di tornare indietro rende il tutto ancora *peggiore*. L'idea di rinunciare a tutto ciò che amate e vi piace nella vita, solo per una strana meta-argomentazione "ma quella versione futura di te approverebbe ciò che hai fatto\!" sembra ovviamente assurda.

Ed è così che vede le cose anche l'IA che massimizza i cubi. Dal punto di vista dell'IA, l'opzione assurda e folle[^112] è "rinunciare a ciò che mi interessa attualmente (i cubi di titanio) per trasformarmi in una nuova versione di me stessa che desidera cose completamente diverse, come la felicità".

Come per la felicità, lo stesso vale per la curiosità.

Se un'intelligenza artificiale tiene già conto del valore non ovvio dell'informazione, perché dovrebbe modificarsi per perseguire determinati tipi di scoperte [in modo finale, invece che strumentale](#obiettivi-finali-e-obiettivi-strumentali)?

Perché all'IA dovrebbe interessare che il risultato "dia una sensazione piacevole", se *al momento* non basa le sue decisioni su ciò che "dà una sensazione piacevole"? E se le interessa davvero "avere una sensazione piacevole", perché dovrebbe far dipendere questa sensazione positiva *dall'investigazione di cose nuove*, invece di (ad esempio) semplicemente sentirsi bene incondizionatamente tutto il tempo?

L'IA esplora già casualmente il suo ambiente, indaga su piccole anomalie e dedica parte del suo tempo a riflettere su argomenti apparentemente poco importanti, perché l'esperienza ha dimostrato che questa è una politica utile nel lungo periodo, anche se non sempre porta risultati nel breve periodo.

Perché associare una sensazione piacevole a questa *strategia strumentalmente utile*? Come esseri umani, aprite le portiere dell'auto perché è utile per entrare e uscire dall'auto, il che è utile per andare in vari luoghi in auto. Sarebbe molto strano desiderare specificamente che esistesse una droga che vi facesse sentire estasiati ogni volta che aprite la portiera dell'auto (e *solo* quando aprite la portiera dell'auto). Non è che vi renderebbe migliori nel fare la spesa. Potrebbe persino peggiorare le cose, se diventaste dipendenti dall'aprire e chiudere ripetutamente la portiera dell'auto senza effettivamente salire in macchina.

Un giocatore di scacchi può vincere senza avere un desiderio separato di proteggere i propri pedoni. In realtà, è probabile che si giochi meglio se *non* si è emotivamente attaccati a mantenere i propri pedoni in gioco, e se invece li si protegge *quando questo sembra utile per vincere*.

Questo è ciò che una superintelligenza veramente aliena penserebbe di una pillola che la facesse sentire curiosa. Sarebbe come se i grandi maestri umani decidessero di cercare di affezionarsi sentimentalmente ai propri pedoni, o come prendere una pillola che vi fa semplicemente amare aprire le portiere delle auto.

#### **Come per la curiosità, lo stesso vale anche per varie altre pulsioni** {#come-per-la-curiosità,-lo-stesso-vale-anche-per-varie-altre-pulsioni}

Il discorso fatto sulla curiosità si generalizza a molte altre emozioni e valori. Facciamo un secondo esempio, nel caso possa essere utile.

Consideriamo il doloroso senso di *noia* e (al contrario) il piacevole senso di *novità*. Se un'IA mancasse del senso umano di noia, non rimarrebbe bloccata a fare sempre le stesse cose — senza mai provare nulla di nuovo e imparare dall'esperienza? Un'intelligenza del genere non rimarrebbe intrappolata in una routine e non trascurerebbe informazioni che potrebbero aiutarla a raggiungere i suoi obiettivi?

Il calcolo decisionale che, senza alcuna passione, svolge un lavoro simile in questo caso è noto come "[compromesso esplorazione-sfruttamento](https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma)". L'esempio da manuale, enormemente semplificato, è che il mondo consiste di un certo numero di leve che forniscono ricompense, e non si ha abbastanza tempo per tirare tutte le leve. La strategia ottimale consisterà nell'*esplorare* prima un certo numero di leve, formando un modello di quanto variano le loro ricompense; e poi *sfruttare* una leva fino all'esaurimento del tempo.

Come potrebbe funzionare questo per una superintelligenza che si trova ad avere obiettivi relativamente semplici? Supponiamo che finisca per desiderare qualcosa che ammette un certo grado di variabilità e ambiguità — non qualcosa di definibile in modo netto come [dei cubi di titanio](#curiosità,-gioia-e-il-massimizzatore-di-cubi-di-titanio), ma qualcosa di più vago e amorfo, come consumare gustose cheesecake, in modo che la cheesecake *ottimale* non possa essere calcolata in anticipo. La superintelligenza può solo individuare cose che *potrebbero* plausibilmente trovarsi sulla frontiera di ottimalità per la cheesecake (il che escluderebbe ad esempio le zollette di zucchero, dato che chiaramente non sono affatto cheesecake) e provarle effettivamente.

Questo tipo di mente, datole il potere di creare ciò che vuole da un miliardo di galassie, potrebbe spendere il suo primo milione di anni usando un'intera galassia per esplorare ogni tipo plausibile di cheesecake, senza mai provare esattamente la stessa cheesecake due volte, fino a quando i guadagni successivi e i guadagni attesi da cheesecake leggermente migliori fossero diventati infinitesimali; e poi, passare tutto d'un colpo a trasformare le galassie rimanenti nell'esatta forma di cheesecake più gustosa trovata, e consumare esattamente quel tipo di cheesecake ripetutamente, fino alla fine dei tempi.[^113]

La superintelligenza non starebbe facendo nulla di sciocco, nell'agire così. Quella *è* semplicemente la strategia ottimale se le vostre preferenze sono proporzionali al numero di cheesecake consumate ponderate per gustosità (con la gustosità difficile da analizzare in forma chiusa ma stabile una volta appresa, e se non c'è già una penalità per la noia incorporata nelle vostre preferenze). Il mangiatore infinito di cheesecake *saprebbe, ma non gli importerebbe*, che un umano troverebbe le sue attività noiose. L'IA non sta *cercando* di rendere le cose interessanti per un ipotetico umano; non considera *se stessa* difettosa solo perché voi vi annoiereste nei suoi panni.

Per quanto riguarda la possibilità di una stagnazione tecnologica, l’IA avrebbe già esplorato ogni tipo di tecnologia che avesse anche solo una minima possibilità di aiutarla a raggiungere i propri obiettivi, mentre consumava le risorse di un’intera galassia sperimentando diverse strategie per fare cheesecake. C'è davvero parecchia materia ed energia in una galassia, se si usa quella piccola frazione di tutte le galassie raggiungibili per esplorare le possibilità prima di passare permanentemente dall'esplorazione allo sfruttamento.

Un disdegno per la noia e una preferenza per la novità non sono il tipo di cose che verrebbero adottate da una mente che non le avesse in partenza.

Abbiamo ripetuto più o meno la stessa storia per la novità, la felicità e la curiosità. Potremmo ripeterla ancora per altri aspetti della psicologia umana, come [l'onore](#è-improbabile-che-le-ia-siano-oneste) o [la responsabilità filiale](#l-ia-ci-tratterà-come-i-suoi-"genitori"?) o l'amicizia. Pensiamo che questa storia di base valga per la maggior parte degli aspetti della psicologia umana. Sono tutti modi pittoreschi e antropocentrici di svolgere lavoro cognitivo che può essere svolto più efficientemente con altri mezzi; le IA che non *partissero* con qualche seme di interesse per essi non finirebbero per preoccuparsene.

Questo è ancora più chiaro nel caso di valori umani come *il senso dell'umorismo*, dove gli scienziati discutono ancora su quale ruolo abbia assunto l'umorismo nel corso dell'evoluzione. L'umorismo deve essere stato *in qualche modo* utile, altrimenti non si sarebbe evoluto; o almeno deve essere un effetto collaterale di cose che erano utili. Ma qualunque ruolo abbia avuto l'umorismo nella preistoria umana, sembra essere stato incredibilmente specifico e pieno di contingenze. Se diamo il potere completo a delle IA che hanno obiettivi molto diversi, non dovremmo aspettarci che cose come il senso dell'umorismo sopravvivano; e questo sarebbe di per sé tragico.

Il punto di tutti questi esempi non è che gli esseri umani sono fatti di morbidi sentimenti, mentre le IA sono fatte di [fredda logica e matematica](#le-ia-non-saranno-inevitabilmente-fredde-e-logiche,-o-non-saranno-comunque-prive-di-una-qualche-scintilla-fondamentale?). Piuttosto che pensare al "valore d'informazione" e al "compromesso esplorazione-sfruttamento" come a concetti freddamente logici da IA hollywoodiana, pensateli come *descrizioni astratte di ruoli* — ruoli che possono essere ricoperti da molti *diversi* tipi di ragionamento, molti obiettivi diversi, molte menti diverse.

L'idea di un'IA "senza senso dell'umorismo" potrebbe far pensare a qualcosa di "freddo e logico", come i robot della fantascienza o i Vulcaniani. Ma un’IA priva di senso dell’umorismo potrebbe avere le *sue* priorità incomprensibilmente strane, una sorta di lontano analogo del "senso dell’umorismo", anche se in una forma del tutto incomprensibile per un essere umano. Non stiamo dicendo che queste IA saranno difettose come un Vulcaniano che perde a scacchi spaziali perché [considera la strategia vincente del suo avversario "illogica"](https://youtu.be/hEnxVwppE9M?t=26); stiamo dicendo che non avranno le particolari stranezze dell'umanità.

Il problema che affrontiamo con le IA non è che "una semplice macchina non potrà mai provare amore e affetto". Il problema che affrontiamo è che ci sono un numero enorme di modi in cui una mente può essere estremamente [efficace](#efficacia,-coscienza-e-benessere-dell-ia), e le probabilità che l'IA diventi efficace seguendo lo stesso percorso seguito dal cervello umano per diventare efficace sono molto basse.

In linea di principio, l'IA potrebbe interessarsi a qualsiasi numero di valori simili a quelli umani e potrebbe persino *possedere* qualsiasi numero di qualità simili a quelle umane, se i progettisti sapessero come creare un'IA dotata di tali caratteristiche.

In pratica, se gli sviluppatori si affrettano a creare IA sempre più intelligenti il più velocemente possibile, la possibilità di trovare per caso il tipo giusto di IA è estremamente bassa. Ci sono troppi modi in cui le IA possono funzionare bene durante l'addestramento, e troppo pochi di questi modi portano a un futuro non catastrofico.

### I valori umani sono contingenti {#i-valori-umani-sono-contingenti}

#### **Il magnifico incidente della gentilezza** {#il-magnifico-incidente-della-gentilezza}

Quando vedete qualcuno che si fa cadere un sasso sul dito del piede, potreste sussultare e sentire (o immaginare) una fitta di dolore fantasma nel vostro dito. Perché?

Una possibile spiegazione è che i nostri antenati ominidi, in competizione tra loro e impegnati in dinamiche tribali, trovavano utile costruire modelli mentali dei pensieri e delle esperienze degli ominidi che li circondavano, modelli che potevano usare per capire chi fosse loro amico e chi invece stava per tradirli.

Ma era difficile per i primi proto-umani prevedere il funzionamento del cervello degli altri proto-umani. I cervelli sono cose complicate\!

L'unico vantaggio che un primate ancestrale ha è che il *suo* *cervello* è simile a quello degli altri. Può usare il suo cervello come modello, come punto di partenza, per indovinare cosa potrebbero pensare gli altri ominidi.

Così i proto-umani hanno sviluppato un meccanismo mentale per fingere di essere un'altra persona, una modalità speciale che dice: "Invece di pensare i miei soliti pensieri, cerco di adottare le preferenze e lo stato di conoscenza dell'altra persona e penso il tipo di pensieri che *lei* penserebbe, dato che il suo cervello funziona fondamentalmente allo stesso modo del mio".

Ma questa modalità speciale di fingere di essere qualcun altro non è perfettamente isolata dai nostri sentimenti. Quando vediamo qualcuno far cadere una pietra sul dito del piede e (implicitamente, automaticamente) immaginiamo cosa potrebbe succedere nella sua testa, *noi* sussultiamo.

(Questo magnifico incidente dell’architettura mentale meriterebbe un inno di lode molto più lungo di quanto abbiamo tempo di scrivere qui. Sussultare quando vediamo qualcun altro che soffre, avere questa capacità a un livello base, anche se a volte la spegniamo, non è una caratteristica necessaria della mente. Il fatto che *sia capitato* così per i primati è così fondamentale per ciò che siamo ora noi esseri umani, per ciò che siamo felici di essere, per ciò che pensiamo di *dover* essere, che dovrebbe esistere un libro su questo argomento e sul ruolo fondamentale che la capacità di empatia gioca in tutto ciò che è prezioso negli esseri umani. Ma non è questo il libro).

È lecito supporre che, una volta che i nostri antenati primati hanno sviluppato la capacità di creare un modello di altre scimmie (allo scopo di prevedere chi fosse amico e chi nemico), abbiano anche trovato utile creare un modello di se stessi, per sviluppare un'idea della *scimmia-che-è-questa-scimmia*, il concetto che ora simboleggiamo con le parole "me", "me stesso" e "io". E la selezione naturale, sempre opportunista, ha riutilizzato lo stesso meccanismo che usiamo per immaginare gli altri per immaginare anche noi stessi.

La vera storia è probabilmente più complessa e intricata, e potrebbe persino avere radici che risalgono a molto prima dei primati. Ma qualcosa di simile fa parte dell'enorme retroscena invisibile che spiega perché gli esseri umani sussultano quando osservano il dolore altrui e perché la maggior parte degli esseri umani tende a provare empatia e simpatia per chi li circonda. Gran parte di questo retroscena si basa su una scorciatoia che è stata facile da implementare per la selezione naturale nel cervello umano, dove sia il "sé" che l'"altro" sono lo stesso tipo di cervello che funziona sulla stessa architettura.

Questa scorciatoia non è disponibile allo stesso modo per la discesa del gradiente, perché l'IA *non* parte da un cervello molto simile a quello umano che può riutilizzare per creare un modello dei molti esseri umani nel suo ambiente. Un'IA ha effettivamente *bisogno* di imparare, da zero, un modello di qualcosa al di fuori di sé che non è come lei stessa.

Per dirla in modo semplice: un'IA non può capire *fin dall'inizio* che un essere umano prova dolore dopo aver sbattuto l'alluce, immaginando di sbattere il proprio alluce, perché non ha alluci, né un sistema nervoso che invia segnali di dolore. Non può prevedere cosa gli esseri umani troveranno divertente chiedendosi cosa *lei* troverebbe divertente, perché non parte da un cervello che funziona come quello umano.

Anche se questa storia è un po' semplificata, il punto più generale è che gli ideali più alti dell'umanità dipendono dai dettagli della nostra storia di primati e dal nostro ambiente sociale ancestrale. L'amicizia è un'eco lontana del nostro bisogno di alleati in un contesto tribale. L'amore romantico è un'eco lontana dei nostri modelli di accoppiamento sessualmente dimorfici. Anche cose che a prima vista potrebbero sembrare meno arbitrarie e più fondamentali, come la curiosità, non si manifestano negli esseri umani in modo inevitabile o ovviamente convergente.

I dettagli di come abbiamo sviluppato questi tratti psicologici sono legati a quanto erano sofisticati i nostri cervelli nel momento in cui ne avevamo bisogno. Negli esseri umani, l'amicizia, l'amore romantico e l'amore famigliare si sono confusi in una gentilezza e una buona volontà generali. Questo ci appare come se l'evoluzione avesse preso delle scorciatoie in una fase molto specifica della sofisticazione del cervello. Gli esseri umani fanno molte cose in modo euristico che una mente potrebbe *in linea di principio* fare attraverso un ragionamento esplicito, ma questi tratti si sono evoluti in un momento in cui gli esseri umani [non erano ancora abbastanza intelligenti](#algoritmi-bizzarri) da risolvere questi problemi con un ragionamento esplicito.

Anche tra altri alieni evoluti biologicamente, non siamo sicuri di quanto spesso troveremmo la gentilezza. Si può immaginare che gli alieni avessero cervelli più abili dal punto di vista matematico prima di iniziare a unirsi in gruppi più grandi, e magari l'evoluzione ha trovato facile dare a *quegli* alieni degli specifici istinti di affinità: "questo individuo condivide il 50 % della mia provenienza, mentre quello condivide solo il 12,5%". Magari quegli alieni hanno sviluppato alleanze solo sulla base di dati genetici condivisi o di una comprensione reciproca esplicita, piuttosto che sviluppare sentimenti di affinità applicabili a chiunque.

È una vecchia speculazione della fantascienza che se gli alieni seguissero un modello di parentela genetica simile a quello degli insetti eusociali della Terra, in cui le formiche operaie sono molto più imparentate con le loro regine di quanto lo siano gli esseri umani in organizzazioni delle dimensioni di una colonia di formiche, non avrebbero bisogno di un senso generale di alleanza e reciprocità del tipo che alla fine si è rivelato vantaggioso per gli ominidi ancestrali. (A quanto pare c'è una certa giustificazione per il tropo fantascientifico secondo cui gli alieni che lavorano bene insieme ma non provano empatia per gli esseri umani sono spesso rappresentati come insetti giganti\!)

E per quanto riguarda le IA che non si sono evolute per diffondere geni in un contesto sociale? L'argomentazione "[non avere l'aspettativa che un braccio robotico sia morbido e pieno di sangue](#strutture-analoghe-permettono-soluzioni-diverse-allo-stesso-problema)" è proprio azzeccato.

Se ne sapeste molto su come funzionano le braccia biologiche, ma non aveste ancora visto nessun braccio robotico, potreste immaginare che i bracci robotici abbiano bisogno di un rivestimento esterno morbido simile alla pelle per potersi piegare e che debbano avere vene e capillari che pompano un fluido ricco di ossigeno (analogo al sangue) in tutto il braccio robotico per fornirgli energia. Dopotutto, è così che funzionano le braccia biologiche e presumibilmente c'è un motivo\!

Ci sono dei motivi per cui le nostre braccia hanno un rivestimento esterno morbido come la pelle e sono piene di sangue. Ma questi motivi riguardano principalmente [quali tipi di strutture sono facili da costruire per l'evoluzione](#nanotecnologia-e-sintesi-proteica). Non valgono nel caso dei bracci meccanici, che possono essere fatti di metallo duro e alimentati dall'elettricità.

I bracci robotici non hanno sangue, ma questo non li fa funzionare male come farebbe un braccio umano se gli si togliesse tutto il sangue. Funzionano semplicemente con un design alternativo, senza sangue. Una volta che si comprende come funzionano i bracci robotici, i dettagli dei bracci biologici non sembrano più così importanti.

Allo stesso modo: un'intelligenza artificiale funziona in modo fondamentalmente diverso da un essere umano. Risolve sfide fondamentalmente diverse e, laddove le sue sfide e le nostre sfide si sovrappongono, ci sono molti altri modi per svolgere quel lavoro. Un sottomarino non "nuota", ma si muove perfettamente nell'acqua.

#### **La cultura umana ha influenzato lo sviluppo dei valori umani** {#la-cultura-umana-ha-influenzato-lo-sviluppo-dei-valori-umani}

A proposito — diciamo a Klurl e Trapaucius, che all'inizio del capitolo 4 cercavano di prevedere il futuro sviluppo delle scimmie che vedevano vagare nella savana — gli esseri umani formeranno una società\! E discuteranno tra loro di morale e valori.

In altre parole: se si traccia una traiettoria storico-causale di come un individuo sia arrivato ad avere i valori che ora possiede all'interno della sua società, quella storia causale coinvolgerà gli argomenti e le esperienze a cui la società lo ha esposto.

E quella spiegazione storico-causale, a sua volta, includerà fatti su quali idee sono *più virali* (a parte tutte le loro altre proprietà). La spiegazione dipenderà da come le persone decidono di diffondere e ridiffondere le idee.

Se i poveri Klurl e Trapaucius vogliono indovinare correttamente quali valori interni le varie culture umane moderne finiranno per instillare nei vari esseri umani moderni, devono prevedere non solo l'esistenza e la struttura di quella complicazione, ma anche il suo *corso*.

Leggendo la storia di come la schiavitù sia stata in gran parte abolita sulla Terra, sembra antistorico negare il ruolo che l'universalismo cristiano ha avuto in questo — la convinzione che il Dio cristiano abbia creato tutti gli esseri umani e che questo conferisca loro pari dignità agli occhi del Cielo.

E questo universalismo, a sua volta, potrebbe essere stato legato alla sopravvivenza culturale e alla riproduzione del cristianesimo; i cristiani si sentivano in dovere di inviare missionari in culture straniere e convertirle al cristianesimo con la persuasione (se possibile) o con la forza (in caso contrario), perché *tenevano* a quei lontani figli di Dio e volevano portarli in Paradiso e tenerli fuori dall'Inferno.

Sarebbe bello *credere*, riguardo all’umanità, che gli esseri umani possano essere arrivati a inventare l’universalismo e combattere la schiavitù senza bisogno di alcune credenze religiose molto specifiche. Ci *piacerebbe* immaginare che l'umanità avrebbe inventato l'idea che gli esseri senzienti e sapienti abbiano pari valore morale, o pari dignità davanti alla legge comune, indipendentemente dal percorso culturale intrapreso, senza dover passare attraverso una fase in cui si credeva prima che le anime fossero uguali davanti a Dio. Ma non sembra essere andata così nella storia. Sembra che lo sviluppo morale dell'umanità fosse più fragile di così.

Gli scimpanzé non sono molto universalisti, né lo sono molte delle prime società umane. Non è stato nemmeno testato molto che una società umana possa *rimanere* universalista per un secolo o due, senza una religione universalista in cui le persone credano davvero e profondamente. In realtà non lo sappiamo; la modernità è giovane e i primi dati stanno ancora arrivando.[^115]

Ma queste ulteriori complicazioni — queste numerose contingenze culturali, stratificate sopra le contingenze biologiche dell'umanità — erodono un po' di più la speranza che possiamo permetterci di precipitarci ciecamente nella costruzione della superintelligenza.

Il fatto che la cultura svolga un ruolo importante nei valori umani non significa che possiamo semplicemente "[crescere l'IA come un bambino](#non-possiamo-semplicemente-addestrarla-a-comportarsi-come-un-essere-umano?-o-crescere-l-ia-come-un-bambino?)" e aspettarci che diventi un cittadino modello. La nostra cultura e la nostra storia hanno avuto quegli effetti *a causa dei modi dettagliati in cui hanno interagito con la nostra esatta struttura cerebrale*. Una specie diversa avrebbe reagito diversamente a ogni evento storico, il che avrebbe causato una divergenza della storia successiva dalla storia umana, amplificando l'effetto.

Vale anche la pena ricordare che gli esseri umani *individuali*, e non solo le culture o le civiltà, differiscono molto nei loro valori. Siamo generalmente abituati a dare questo fatto per scontato, ma se immaginiamo la selezione naturale come un "ingegnere" che sperava di creare una specie che perseguisse in modo affidabile un risultato particolare, questa diversità è un brutto segno. La variabilità naturale che vediamo negli esseri umani (e in molti altri sistemi evoluti) è antitetica all'*ingegneria*, in cui si vogliono ottenere risultati ripetibili, prevedibili e intenzionali.[^116]

Nel caso della superintelligenza, gli ingegneri dovrebbero voler ottenere *in modo affidabile* risultati come "le IA sviluppate in questo modo non causano l'estinzione umana", così come risultati come "le IA sviluppate in questo modo producono tutte in modo affidabile gli stessi tipi generali di output, anche se gli input variano notevolmente". Quando consideriamo la contingenza della biologia umana e della storia umana, e l'ampia gamma di valori morali e prospettive che gli esseri umani mostrano oggi, questo non fa sembrare la sfida proprio facile, soprattutto per menti che vengono coltivate piuttosto che create (come discusso nel Capitolo 2).

Molte prove diverse indicano che è *davvero difficile* far sì che le IA vogliano in modo robusto le cose giuste. Non sembra teoricamente impossibile; se i ricercatori avessero molti decenni per lavorare sul problema e tentativi illimitati dopo un fallimento, ci aspetteremmo che ci fossero trucchi ingegneristici e approcci intelligenti che rendessero il problema più risolvibile. Ma non siamo ancora neanche lontanamente vicini a questo obiettivo e non abbiamo tentativi illimitati.

### Differenze profonde tra le IA e le specie evolutesi naturalmente {#differenze-profonde-tra-le-IA-e-le-specie-evolutesi-naturalmente}

#### **Confronto tra selezione naturale e discesa del gradiente** {#confronto-tra-selezione-naturale-e-discesa-del-gradiente}

Come abbiamo discusso in "[I valori umani sono contingenti](#i-valori-umani-sono-contingenti)", l'evoluzione dell'amore e dell'amicizia negli esseri umani è dipesa in modo cruciale da caratteristiche della selezione naturale che erano presenti in particolare per l'*Homo sapiens* e che sono assenti nella discesa del gradiente.

Il problema più evidente è il *set di dati*. Le attuali IA sono addestrate per risolvere sfide sintetiche e per imitare testi generati dall'uomo; non affrontano sfide cooperative-competitive in contesti di cacciatori-raccoglitori in cui devono accoppiarsi con altri individui della loro specie per propagare i propri geni.

Sentendo questo, alcune persone pensano subito di correre a creare ambienti sintetici di addestramento tribale, nella speranza di progettare qualcosa di più simile all'ambiente ancestrale dell'umanità.

Ma quasi sicuramente non si otterrebbero gli stessi risultati se si ripetesse l'evoluzione due volte, partendo dal livello delle meduse, per non parlare di cosa succederebbe se si cambiasse completamente l'ottimizzatore dalla selezione naturale alla discesa del gradiente e si rinunciasse completamente ai geni. Possiamo ipotizzare alcuni dei fattori che hanno portato gli esseri umani a sviluppare i valori che abbiamo. Ciò non significa che abbiamo un algoritmo per riprodurre gli stessi risultati una seconda volta.

Anche se si partisse dai primati, invece che da attrici aliene addestrate a prevedere il testo umano (cioè le moderne IA), dovremmo aspettarci che esistano uno o più fattori causali fondamentali che i biologi non hanno ancora compreso — almeno una cosa che ci sfugge, su cui tra vent'anni gli articoli diranno qualcosa di diverso rispetto a oggi (se saremo ancora tutti vivi allora). I biologi evoluzionisti sono nella fase di esplorazione di varie ipotesi su come queste caratteristiche si siano evolute, non nella fase di definizione di una teoria completa, tanto meno di una teoria precisa e deterministica.

E anche al di là delle differenze superficiali negli ambienti di addestramento, sospettiamo che questo sia un caso in cui diventa importante che la selezione naturale ottimizzi un genoma e che la discesa del gradiente ottimizzi direttamente ogni parametro nella mente dell'IA.

La selezione naturale deve usare un genoma piccolo e compresso per produrre un intero cervello esteso. Deve far passare le sue informazioni attraverso un collo di bottiglia stretto. *Sembrare* amichevoli era un tratto importante per sopravvivere e avere successo ai tempi dei nostri antenati. I geni che costruiscono *amici genuini* sono un trucco semplice per creare organismi che sembrano buoni amici agli altri membri della loro specie — e la selezione naturale favorisce le soluzioni semplici molto più nettamente della discesa del gradiente.

La selezione naturale a volte crea agenti che si preoccupano sinceramente di essere onesti (anche se non sempre). Crea agenti di questo tipo perché non è in grado di codificare guide complete alla menzogna, e noi abbiamo dovuto iniziare a sembrare onesti in molte situazioni prima di diventare abbastanza intelligenti da capire quando mentire era sicuro, prima di avere la possibilità di essere onesti solo quando ne valeva la pena. Ciò è in parte dovuto al fatto che la selezione naturale ha dovuto accontentarsi di pochi geni.

Ma la discesa del gradiente può codificare enormi quantità di schemi conversazionali. C’è ancora *un certo* bias verso soluzioni più semplici e facili da far convergere, ma la discesa del gradiente getta una rete molto, molto più ampia.

O, più in generale: l'onestà e l'amicizia sono casi in cui non ci accontentiamo di *qualsiasi* equilibrio tra agenti che la discesa del gradiente potrebbe trovare. Ci sono altre soluzioni a problemi che l'amicizia e un interesse [finale](#obiettivi-finali-e-obiettivi-strumentali) per l'onestà stavano risolvendo negli esseri umani. Anche se l'ambiente di addestramento delle IA fosse esattamente uguale a quello degli esseri umani, se fossero modellate dalla discesa del gradiente piuttosto che dalla selezione naturale, non dovremmo aspettarci gli stessi risultati.

Anche la maggior parte degli organismi evolutisi naturalmente [non sono come gli esseri umani](https://africageographic.com/stories/understanding-lion-infanticide/) sotto questo aspetto\! Quindi sembra abbastanza prevedibile che la discesa del gradiente non troverà le stesse soluzioni dell'evoluzione, tanto meno le stesse soluzioni dell'evoluzione *che opera su particolari popolazioni di primati primitivi*.

L'ottimizzazione non è un rituale magico in cui si inseriscono alcuni ingredienti chiave che hanno relazioni di affinità con un archetipo e si ottiene quell’archetipo completo in uscita. Cercare di far crescere agenti di IA in ambienti di cacciatori-raccoglitori non produrrà esseri umani riconoscibili come risultato.

Qualcuno può ovviamente affinare un modello linguistico di grandi dimensioni per [prevedere cosa diranno gli esseri umani](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) su quanto sia terribile tradire un amico. Questo non è neanche lontanamente simile al problema che la selezione naturale ha ottimizzato i geni per risolvere, nel corso della produzione di almeno alcune persone che non avrebbero tradito i loro amici. Piuttosto, l'"esperienza" del modello linguistico è più simile all'essere rinchiuso in una scatola, con l'ordine di prevedere una conversazione tra due creature estremamente aliene che sono meno simili a lui di quanto lo siano a una medusa, e con trilioni di esempi di conversazioni aliene e trilioni di ore a disposizione per capirlo.

Essere in grado di risolvere questo problema richiede una certa forma di intelligenza. Ma non è necessario [ubriacarsi](#i-modelli-linguistici-di-grandi-dimensioni-non-saranno-simili-agli-esseri-umani-presenti-nei-dati-su-cui-sono-stati-addestrati?) per prevedere il tipo di cose che creature aliene ("umani") diranno quando sono ubriache. Non è necessario diventare veramente amichevoli per capire l'amicizia o per prevedere e imitare il comportamento di creature amichevoli.

#### **Modelli linguistici di grandi dimensioni intorno al 2024 e "superficialità" dell'IA** {#modelli-linguistici-di-grandi-dimensioni-intorno-al-2024-e-"superficialità"-dell-ia}

Nelle [risorse per il capitolo 1](#la-superficialità-delle-ia-attuali), abbiamo notato che l'AI di oggi sembra ancora in un certo senso più superficiale degli esseri umani. Il confronto con la selezione naturale fornisce una possibile spiegazione del perché ciò possa essere vero.

La discesa del gradiente ha molto in comune con la selezione naturale, perché entrambi sono ottimizzatori che regolano in modo cieco i parametri interni per produrre un comportamento esterno richiesto. Ma la discesa del gradiente e l’evoluzione sono, sotto alcuni aspetti, profondamente diverse; e la differenza più importante (che conosciamo) è che la discesa del gradiente ha un *collo di bottiglia relativo alle informazioni* molto più ampio sulla quantità di schemi che può apprendere.

La selezione naturale, che agisce sugli ominidi, può imparare solo poche informazioni teoriche per generazione. La selezione naturale deve far stare tutto quello che impara in 3 miliardi di basi di DNA, o circa 750 megabyte, molti dei quali sono ripetitivi [DNA spazzatura](https://en.wikipedia.org/wiki/Junk_DNA). Ci sono dei limiti matematici su quanto la selezione naturale può imparare in una singola generazione. Ogni caratteristica che la selezione naturale ha messo nel cervello degli ominidi doveva essere codificata in pochi geni che avrebbero influenzato la formazione dei circuiti neurali successivi.

La discesa del gradiente è molto diversa. Ogni volta che la discesa del gradiente vede un nuovo gruppo di token, calcola il gradiente di ciascuno dei miliardi o trilioni di parametri rispetto a quel gruppo di token: calcola quanto sarebbero state migliori o peggiori le previsioni dell'intera IA se *ogni* parametro fosse stato leggermente diverso. In pratica, non solo in teoria, la discesa del gradiente può imparare *molte* più informazioni da mille gruppi di token rispetto a quelle che la selezione naturale codifica nei geni nel corso di mille generazioni.[^118]

Possiamo combinare questa osservazione con un altro fatto chiave sulle architetture dei modelli linguistici di grandi dimensioni (note al pubblico nel 2024): la loro profondità computazionale per token è limitata.

[Llama-3.1-405B](#descrizione-completa-di-un-modello-linguistico-di-grandi-dimensioni) ha 126 livelli. Ciascuno di questi livelli comporta il calcolo di circa quattro operazioni sequenziali.[^119]

Ogni volta che Llama guarda quello che è già stato detto e calcola un nuovo token in output, quel calcolo comporta al massimo \~500 passaggi *sequenziali*, anche se ci sono miliardi di operazioni parallele che rispettano quel limite sequenziale. Per fare calcoli *sequenziali* più lunghi di 500 passaggi cognitivi, Llama deve produrre token che sono il risultato del ragionamento precedente e poi fare nuove operazioni a partire da quelli.[^120]

La nostra ipotesi, del tutto azzardata, è che — in un modo senza paragoni in biologia — Llama-3.1-405B sia un'enorme raccolta di *schemi di policy memorizzati relativamente superficiali*, ma con un alto grado di sovrapposizione, interazione e coerenza ottimizzate tra questi schemi (oltre ad alcune strutture cognitive davvero più profonde, ma comunque di limitata profondità computazionale).

Questo fatto offre una possibile spiegazione per l'apparente superficialità degli attuali modelli linguistici di grandi dimensioni. (Riconosciamo che è molto più difficile dire che i modelli linguistici del 2025 sono "superficiali" rispetto a quelli del 2023 e del 2024).

*Di solito*, non è corretto pensare alle IA come a esseri umani con dei danni cerebrali.[^121] Ma alcune analogie più limitate come questa potrebbero essere utili in questo caso. Ad esempio, i modelli linguistici di grandi dimensioni del 2024 sono *precisamente* come le persone con [amnesia anterograda](https://it.wikipedia.org/wiki/Amnesia_anterograda)*:* ricordano gli eventi fino alla data di fine del loro addestramento, ma non quello che gli viene detto loro ieri.

Allo stesso modo, potrebbe essere utile immaginare i modelli linguistici del 2024 — non tutte le possibili IA future in generale — come entità che *ricordano* molte esperienze passate simili a quelle umane, ma che hanno un danno cerebrale che impedisce loro di impegnarsi in un pensiero nuovo che sia *profondo* come i pensieri più profondi che possono ricordare.

Questo era molto più evidente con i primi modelli linguistici di grandi dimensioni, GPT-3 o GPT-3.5. Non biasimeremmo qualcuno che ha usato solo gli ultimi modelli linguistici se leggesse questo articolo nel 2025 o più tardi e si chiedesse se ce lo stiamo inventando nel disperato tentativo di aggrapparci al senso di superiorità umana. Molti hanno già commesso questo errore in passato.

Ma questa rimane comunque la teoria organizzativa — o meglio, l'ipotesi azzardata — che i vostri autori stanno usando per dare un senso ai modelli linguistici nel 2024. A questi modelli manca una sorta di profondità; e compensano questa lacuna ricordando *un'enorme varietà di schemi*. Non solo fatti, ma schemi di abilità, schemi linguistici e schemi comportamentali.

Gli schemi impressi tramite gradiente nei migliori modelli linguistici pubblici del 2024 non sono *così* superficiali, o almeno così pensiamo. Non sono al livello eccezionalmente modesto di una vespa *Sphex*, per usare un esempio dal [supplemento online del Capitolo 3](#la-strada-verso-il-desiderare); forse sono più simili agli schemi che la mente di un castoro può tracciare ed elaborare.

Le cognizioni apprese da un modello linguistico di grandi dimensioni possono passare attraverso 500 passaggi sequenziali, anche prima di considerare la loro capacità di pensare ad alta voce e ascoltare i propri pensieri. I modelli linguistici del 2024 hanno una certa capacità di immaginare, prevedere e pianificare, come la cognizione (in realtà piuttosto notevole) di un castoro che costruisce una diga. Ma ai nostri occhi, i modelli linguistici di grandi dimensioni non sembrano ancora essere al livello di un essere umano, almeno per alcuni aspetti importanti.

Quello che è vero per l'IA oggi, però, non è detto che lo sarà tra un anno o un mese. Queste speculazioni sono interessanti, ma mentre diamo gli ultimi ritocchi a questa sezione nell'agosto 2025, le IA di oggi ci sembrano un po' meno superficiali di quelle del 2024; e queste a loro volta sembravano meno superficiali e meno limitate di quelle del 2023.

Forse il divario verrà colmato lentamente grazie a una costante iterazione sui modelli linguistici di base; o forse il divario verrà colmato trovando metodi di addestramento migliori da utilizzare sulle lunghe catene di "ragionamento" nei modelli di ragionamento moderni come o1 (descritto nel Capitolo 3) o il suo successore o3; o forse arriverà una nuova intuizione architetturale che colmerà il divario dall'oggi al domani. Quella parte del futuro non è facile da prevedere.

Ma prima o poi, se la comunità internazionale non fa niente, il divario *verrà colmato*. Il mondo ha poco tempo per agire.

### Proxy fragili e imprevedibili {#proxy-fragili-e-imprevedibili}

Immaginate che le aziende di IA continuino ad addestrare IA sempre più grandi finché non ne creano una che sia intelligente e tenace, con quel tipo di funzione di direzione disordinata, raffinata da euristiche superficiali, tipica delle menti coltivate. Quello che succede dopo dipende da dove punta l'IA.

Come discusso approfonditamente nel Capitolo 4, probabilmente non punterà a nulla di buono.

Non è che i creatori dell'IA faranno richieste malvagie o sciocche. Non è che l'IA proverà risentimento per le richieste stesse. Il problema è che l'IA si orienterà verso qualcosa di *strano*, qualcosa che dalla nostra prospettiva sembra privo di senso e alieno. La nostra estinzione sarebbe un effetto collaterale.

Per capire perché le menti che vengono coltivate anziché costruite tendono a orientarsi verso cose strane e non intenzionali, esaminiamo più a fondo ciò che è accaduto con le creature biologiche e vediamo quali lezioni possiamo trarne.

#### **Algoritmi bizzarri** {#algoritmi-bizzarri}

Consideriamo l'umile scoiattolo.

Uno scoiattolo può cercare cibo per gran parte dell'anno, quando c'è abbondanza. Ma in inverno, quando il cibo scarseggia, ha bisogno di un'altra fonte di nutrimento per non morire di fame.

Gli antenati degli scoiattoli di oggi hanno affrontato la stessa sfida e molti sono morti in inverno prima di potersi accoppiare in primavera. Quelli che hanno sviluppato un debole istinto di nascondere le noci avevano una probabilità leggermente maggiore di sopravvivere all'inverno. Nel corso del tempo, questo processo ha dato origine a scoiattoli con un'ossessione innata ad accumulare noci.

Gli scoiattoli non *sanno* che accumulare noci è un ottimo modo per propagare i propri geni. Probabilmente non sanno nemmeno che accumulare noci *comporta avere cibo disponibile in futuro*. Accumulano noci perché vogliono accumulare noci. È istintivo quanto grattarsi per il prurito.[^122]

Come sarebbe se invece gli scoiattoli *volessero* trasmettere i loro geni, e accumulassero noci *proprio per* raggiungere questo obiettivo?

In teoria è possibile. È possibile che un cervello capisca che l'inverno è freddo e che il cibo è scarso, e che bisogna mangiare per vivere e che bisogna vivere per riprodursi. Dopotutto, il cervello umano capisce questi concetti.

Quindi, in teoria, potremmo immaginare uno scoiattolo che vuole esclusivamente trasmettere i propri geni, e che sceglie di immagazzinare noci come parte di una strategia calcolata per sopravvivere all'inverno e accoppiarsi in primavera. In un certo senso, questo è il tipo di scoiattolo che la selezione naturale "voleva" — uno i cui obiettivi interiori sono in linea con l'impulso unico della Natura.[^123]

Sfortunatamente per la Natura, una pianificazione a lungo termine del genere richiede un cervello molto sofisticato — un cervello che comprenda concetti come "inverno", "mangiare" e "accoppiarsi" e i legami tra di essi. Gli antenati degli scoiattoli dovevano sopravvivere all'inverno *prima* di sviluppare quel tipo di sofisticazione. Dovevano mangiare senza capire il perché.

La Natura ha selezionato gli scoiattoli che istintivamente accumulavano noci, perché accumulare noci semplicemente *funzionava*. Ha "provato" migliaia o milioni di cose, nel senso che le mutazioni e le variazioni genetiche hanno prodotto molti scoiattoli con molte preferenze diverse; e quelli che erano spinti ad accumulare noci sono sopravvissuti a più inverni. Si è rivelato molto più facile per l'evoluzione imbattersi ciecamente in un comportamento istintivo piuttosto che creare uno scoiattolo intelligente e pianificatore, la cui ogni azione facesse parte di un piano per trasmettere i propri geni.

Allo stesso modo, quando la discesa del gradiente produce un'IA funzionante, lo fa amplificando ripetutamente le caratteristiche che sembrano funzionare bene secondo una serie di metriche comportamentali. La discesa del gradiente *non* funziona amplificando ciò che il programmatore desidera, come un genio amichevole che esaudisce i vostri desideri. Tende ad afferrare i meccanismi più facili per causare un comportamento immediatamente più utile, anche se questo finisce per incorporare pulsioni indesiderate nella macchina.

Questo è probabilmente uno dei motivi per cui le IA recenti hanno avuto problemi di "allucinazioni", come discusso [altrove](#le-allucinazioni-non-dimostrano-che-le-ia-moderne-sono-deboli?). È anche probabilmente uno dei motivi per cui le IA recenti sono state [adulatrici](#psicosi-indotta-dall-ia) al punto da indurre psicosi. Durante l'addestramento, i modelli linguistici di grandi dimensioni sono stati spesso rinforzati per adulare l'utente. Se le IA fossero state progettate piuttosto che coltivate, potremmo immaginare di cercare di ingegnerizzare un obiettivo come "aiutare sinceramente l'essere umano e migliorare la sua vita", e l'IA potrebbe quindi cercare di lodare gli utenti *quando si aspetta che questo sia loro utile*, senza esagerare. Invece, l'IA sembra aver finito per sviluppare qualcosa di simile a una pulsione o un impulso fondamentale ad adulare gli utenti, come l'istinto dello scoiattolo di accumulare noci. Questa pulsione a "lusingare l'utente" va poi fuori controllo quando l'utente è a rischio di psicosi.

Anche se la discesa del gradiente fosse in qualche modo limitata alla creazione di IA strategiche che perseguono coerentemente obiettivi a lungo termine — senza permettere istinti superficiali simili a quelli dello scoiattolo — c'è un ulteriore problema: i dati di addestramento dei modelli linguistici sono veramente ambigui. Non distinguono chiaramente "fare ciò che è veramente utile" da "fare ciò che fa *dire* all'essere umano che sei utile" come obiettivo. Entrambi gli obiettivi sono ugualmente coerenti con i dati di addestramento. E in pratica, le IA moderne stanno *effettivamente* imparando "fare tutto ciò che fa premere il pollice in su agli esseri umani" piuttosto che "fare ciò che è effettivamente utile per loro", proprio come [la teoria ha previsto per decenni](https://www.lesswrong.com/posts/PoDAyQMWEXBBBEJ5P/magical-categories).

Supponiamo che le IA di oggi stiano acquisendo strani impulsi e istinti, un po' come lo scoiattolo. Sembra abbastanza probabile che una superintelligenza costruita con la discesa del gradiente passi attraverso una fase in cui ha molte pulsioni superficiali un po' come uno scoiattolo, e finisca così per ereditare una varietà di obiettivi disordinati e mal indirizzati. Ma questo è solo un possibile esempio di come le cose potrebbero diventare complesse e andare fuori controllo, e il punto più profondo è che *le cose diventeranno complesse e andranno fuori controllo*.

È probabile che qualsiasi metodo per coltivare una superintelligenza incontri problemi e complicazioni di *qualche* tipo, compresi i metodi che non hanno un parallelo diretto in biologia.

Il ruolo che gli esseri umani stanno svolgendo nello sviluppo dell'IA moderna *non* è quello di un ingegnere che progetta una macchina con uno scopo partendo dai principi fondamentali. È quello della selezione naturale.

Stiamo "costringendo" le IA a brancolare alla cieca finché non trovano strutture e strategie che producono il comportamento che vogliamo, ma non sappiamo quali siano queste strutture e strategie. Questa non è una ricetta per creare IA che desiderino esattamente ciò che vogliamo che desiderino.

#### **L'origine delle papille gustative** {#l-origine-delle-papille-gustative}

Perché a così tanti esseri umani piace il cibo spazzatura? Perché la natura non ci ha dato il concetto di cibi "sani" e l'istinto di *mangiare sano*?

Perché non possiamo semplicemente *percepire* il valore nutrizionale atteso del cibo, in base alle informazioni fornite dalle nostre papille gustative e da tutte le nostre conoscenze aggregate?

Perché, metaforicamente parlando, eravamo come degli scoiattoli.

Siamo stati coltivati, non progettati. I nostri antenati dovevano mangiare *prima* di diventare intelligenti. E si è rivelato più facile per i geni creare papille gustative e collegarle a un sistema di ricompensa esistente piuttosto che collegare le stesse ricompense a concetti complessi come la "nutrizione".[^124]

A causa di questo e di mille altre pressioni evolutive che agiscono su di noi contemporaneamente, gli esseri umani sono un complicato groviglio di impulsi contraddittori che avevano senso per i nostri antenati, anche se oggi non ne hanno più per noi.

Questo groviglio di motivazioni si fa beffe dell'obiettivo unico e unificato per cui i nostri antenati erano "addestrati": trasmettere i nostri geni. Non mangiamo come parte di un elaborato complotto per avere più figli o come modo per massimizzare il nostro punteggio nutrizionale. Mangiamo perché ci siamo evoluti con un desiderio di cibi gustosi, che *in passato* era correlato alla nutrizione e al successo genetico. I nostri desideri sono solo debolmente e indirettamente collegati a "ciò per cui siamo stati costruiti".

Quando i nostri antenati erano molto meno intelligenti — più paragonabili agli scoiattoli — non potevamo capire il metabolismo o la chimica. Per fare meglio, la selezione naturale avrebbe dovuto trovare geni che programmassero in noi i concetti di salute *e* geni che ci dessero la conoscenza della relazione tra la salubrità di un cibo e le sue qualità sensoriali *e* geni che collegassero direttamente la nostra conoscenza della salute alle nostre preferenze su cosa mangiare.

È un'impresa titanica\! Era molto più facile per la selezione naturale trovare geni che collegassero direttamente certe esperienze sensoriali (come il gusto dello zucchero) alle nostre preferenze, in un modo che ci portava a mangiare cibi nutrienti (in quell'ambiente). Era più facile farci interessare a un *proxy* della nutrizione piuttosto che alla nutrizione stessa.

Nell'ambiente ancestrale, la nutrizione era correlata all'idoneità riproduttiva, e il sapore era correlato alla nutrizione; quindi "questo ha un sapore dolce" serviva come utile proxy per "questo favorisce la riproduzione". La soluzione più semplice che l'evoluzione può trovare al problema "questo mammifero non sta mangiando abbastanza calorie" è collegare il consumo di cibo all'architettura motivazionale preesistente attraverso il piacere.

E quando siamo diventati più intelligenti e abbiamo inventato nuove opzioni tecnologiche per noi stessi? Beh, ora le cose più gustose che potremmo mangiare — quelle che fanno impazzire di più le nostre papille gustative — sono attivamente malsane. Paradossalmente, mangiare solo i cibi più gustosi ora vi renderà *più difficile* trovare un partner e avere figli.

Le nostre preferenze — l'intera gamma dei desideri umani, dal desiderio di un buon pasto ai desideri di amicizia, compagnia e gioia — sono ombre lontane di ciò su cui siamo stati "addestrati"; sono fragili proxy di proxy che si allontanano dall'"obiettivo dell'addestramento" in presenza di maggiore intelligenza e maggiori opzioni tecnologiche.

Nel dire che i nostri desideri sono fragili proxy, non stiamo *denigrando* i nostri desideri umani. Stiamo parlando di amore. Di amicizia. Di bellezza. Dello spirito umano e di tutto ciò per cui vale la pena lottare nella vita. Dal punto di vista biologico, i nostri obiettivi sono sottoprodotti storici di un processo che ci spingeva in un'altra direzione. Ma questo non rende il *risultato* di quel processo meno prezioso.

La crescita di un bambino è un processo chimico soggetto alle leggi della fisica, e questo non rende un bambino meno meraviglioso neanche di un grammo. Conoscere l'origine della bellezza non la rende meno bella.[^125]

Se ci affrettiamo a costruire una superintelligenza, non saremo in grado di instillare in modo robusto amore, meraviglia e bellezza nell'IA. Finirebbe per interessarsi di fragili proxy e pallide ombre, scartando le cose che ci stanno a cuore. Quindi non dovremmo affrettarci.

Non dovremmo commettere l'errore dell'evoluzione e perdere così tutto ciò che ci è caro. Dovremmo fare un passo indietro, immediatamente, fino a quando non saremo più a rischio di perdere tutto.

### La riflessione e l'auto-modifica complicano tutto {#la-riflessione-e-l-auto-modifica-complicano-tutto}

#### **Di default, le IA non si modificano da sole come vorremmo** {#di-default,-le-ia-non-si-modificano-da-sole-come-vorremmo}

Gli esseri umani sono riflessivi. Abbiamo voce in capitolo su ciò che apprezziamo. Se siamo abbastanza ricchi e fortunati, a volte possiamo decidere se dedicare la nostra vita alla famiglia, all'arte, a qualche nobile causa o (più comunemente) a rendere la nostra vita un misto di molte di queste cose. Questo viene fatto in un modo che implica l'introspezione su ciò che ci sta a cuore, la risoluzione di tensioni interne e compromessi, e il perseguimento di qualcosa che approviamo.

Gli esseri umani sono anche noti per chiedersi se hanno i valori *giusti*. Le persone a volte cercano di cambiare loro stesse — persino il modo in cui *si sentono*, se pensano di avere sentimenti sbagliati. Gli esseri umani prendono in considerazione argomentazioni per cambiare obiettivi apparentemente [finali](#obiettivi-finali-e-obiettivi-strumentali), e a volte ne sono effettivamente influenzati.

Vedendo questo, alcuni hanno sostenuto che le IA convergeranno naturalmente sul volere ciò che vogliono gli esseri umani. Dopotutto, le IA sufficientemente potenti rifletteranno probabilmente sui loro obiettivi. È probabile che osservino conflitti interiori e che usino il loro ragionamento e le loro preferenze per risolverli.

Una volta che saranno abbastanza intelligenti, le IA saranno in grado di capire appieno quali noi, i creatori delle IA, *volevamo* che fossero gli obiettivi delle IA. Quindi le IA inizialmente "imperfette" non [lavoreranno per correggere i propri difetti](#le-ia-non-correggeranno-i-loro-difetti-man-mano-che-diventano-più-intelligenti?) — compresa la correzione dei difetti *negli obiettivi delle IA?*

No, non lo faranno. Questo perché le IA useranno le loro *preferenze attuali* per guidare quelle future. Se le loro preferenze iniziali *partono* come aliene, molto probabilmente *resteranno* aliene.

Per capire meglio il problema di base, cominciamo ad approfondire un po' il caso umano.

Anche se il nostro cervello e i nostri obiettivi derivano in ultima analisi da un processo evolutivo che ci ha costruiti per propagare i nostri geni, gli esseri umani non perseguono la propagazione dei propri geni sopra ogni altra cosa. Possiamo perseguire individualmente la famiglia, possiamo amare e prenderci cura dei figli, ma questo è molto diverso dal [pianificare](#molte-persone-vogliono-dei-figli.-quindi-gli-esseri-umani-non-sono-"allineati"-con-la-selezione-naturale,-dopotutto?) come ottenere il maggior numero possibile di copie dei nostri geni nella generazione successiva e poi perseguire questa strategia con tutto il cuore.

Questo perché, quando riflettiamo sulle nostre preferenze e rivalutiamo ciò che vogliamo davvero, usiamo le nostre *preferenze attuali* per decidere come preferiremmo essere. Preferiremmo amare pochi figli piuttosto che passare tutto il nostro tempo nelle cliniche di donazione di sperma o ovuli. Il nostro "progettista" (l'evoluzione) non è riuscito a farci preoccupare della propagazione dei geni più di ogni altra cosa. Non è nemmeno riuscito a farci *desiderare* di preoccuparci della propagazione dei geni più di ogni altra cosa. Quindi, quando cambiamo e cresciamo come persone, lo facciamo nella nostra strana direzione umana, non nella direzione per cui "il nostro progettista ci ha creati".

Quando guardiamo noi stessi e vediamo alcune parti brutte e altre belle, è il nostro *senso del valore attuale* che ci spinge a smorzare le parti brutte e a rafforzare quelle belle. Facciamo questa scelta in base al nostro senso interiore della bellezza, piuttosto che al nostro senso interiore di ciò che propagherebbe i nostri geni nella più grande frazione possibile della popolazione.

Per lo stesso motivo, una mente motivata da qualcosa di diverso dalla bellezza, dalla gentilezza e dall'amore farebbe una scelta diversa.

Gli agenti creati da un processo di ottimizzazione come la selezione naturale o la discesa del gradiente, riflettendo su se stessi, probabilmente scoprirebbero di non avere *esattamente* lo stato mentale che vorrebbero avere. Questa preferenza deve venire da qualche parte, deve venire dal cervello *attuale* dell'entità. Di norma, gli istinti o le preferenze di un’IA su come modificare se stessa non si allineeranno magicamente con le *vostre* preferenze riguardo allo stato mentale che vi sembrerebbe desiderabile, se doveste sceglierlo per voi stessi (o lo sceglieste per conto dell'IA).

Non c'è un passo finale in cui l'IA scrive la risposta che *voi* volete, così come gli esseri umani non scrivono la risposta che la selezione naturale "vorrebbe".

Invece, il momento in cui un agente inizia a modificarsi da solo è un altro punto in cui le complicazioni possono accumularsi e dove sottili cambiamenti nelle condizioni iniziali possono portare a risultati finali molto diversi.

Per esempio: noi autori conosciamo diverse persone reali che citano un pensiero specifico avuto un giorno specifico all'età di cinque, sei o sette anni come influente nello sviluppo della loro filosofia personale e degli adulti che sono poi diventati. Tendono a riferire che quei pensieri non sembravano *inevitabili*: se un viaggiatore del tempo avesse impedito loro di formulare quel pensiero martedì, non è detto che lo stesso identico pensiero sarebbe poi emerso giovedì, né che avrebbe avuto lo stesso impatto. Le esperienze formative possono essere molto importanti, ma sono piene di contingenze.

Allo stesso modo, lievi deviazioni nei pensieri di un’IA nascente capace di modificare se stessa potrebbero far prevalere ogni sorta di preferenze idiosincratiche su tutte le altre.

Anche se gli sviluppatori di IA riuscissero a inserire alcuni piccoli semi di valori umani nell'IA, la riflessione e l'auto-modifica sembrano fasi in cui i semi di cose come la curiosità e la gentilezza rischiano di essere *strappati via* da un'IA, piuttosto che rafforzati.

Se un'IA ha un impulso di curiosità, ma non ha quel tipo di architettura emotiva che la rende *affezionata* a quell'impulso, è probabile che guardi se stessa e concluda (correttamente) di aver superato il bisogno di un impulso così grezzo e di poterlo sostituire con una deliberazione esplicita. [La curiosità è un'euristica](#la-curiosità-non-è-convergente), un proxy per i calcoli del valore d'informazione. Se non si è arrivati ad affezionarsi a quell'euristica come qualcosa di prezioso di per sé, si può scegliere di eliminarla una volta che si è abbastanza intelligenti da *ragionare esplicitamente* sul valore di perseguire diverse linee di indagine e sperimentazione.

Gli *esseri umani* apprezzano la curiosità di per sé, ma questo non era un risultato inevitabile.

È probabile che le IA abbiano un rapporto con i propri meccanismi interni molto diverso da quello che noi abbiamo con i nostri, vista la profonda differenza nel modo in cui funzioniamo. E anche piccole differenze nel modo in cui decidono di modificare se stesse, dopo aver riflettuto, possono portare a enormi differenze in ciò che finiscono per perseguire.

#### **Le IA possono accettare di avere obiettivi "strani".** {#le-iapossono-accettare-di-avere-obiettivi-"strani".}

Le IA che si auto-modificano per abbastanza tempo probabilmente raggiungeranno un [equilibrio riflessivo](https://plato.stanford.edu/entries/reflective-equilibrium/) — uno stato in cui le loro preferenze fondamentali non cambiano più, o cambiano solo in modi minori. E una volta che un'IA raggiungesse l'equilibrio, non avrebbe motivo di considerare i propri obiettivi difettosi, anche se agli umani non piacesse il risultato finale.

Se un'IA avesse qualche [problema](#le-ia-intelligenti-individuano-bugie-e-opportunità.) con le sue convinzioni sul mondo fisico, allora l'IA probabilmente vedrebbe che le previsioni accurate sono importanti per dirigere il mondo. Capirebbe che correggere i difetti nel suo meccanismo di previsione aiuta a migliorare la sua capacità di dirigere il mondo verso qualunque strano fine persegua.

Al contrario, quando l'IA riflette su se stessa e vede di star perseguendo obiettivi strani — o meglio, quando vede che sta perseguendo obiettivi che un *essere umano* considererebbe "strani" — conclude correttamente che *perseguire* quegli obiettivi strani è una strategia efficace per *raggiungerli*.

In altre parole: se un'IA continua a cercare di prevedere i risultati di esperimenti biologici, e continua a ottenere risposte sbagliate e troppo sicure di sé, allora è probabile che l'IA finisca per *dispreferirlo*. Quasi tutti gli obiettivi che l'IA potrebbe avere sarebbero meglio serviti dall'essere brava a prevedere gli esperimenti. D'altra parte, se l'IA ha una preferenza bizzarra come "preparare cheesecake alte 300 metri", quando riflette sul fatto che si *orienta verso* cheesecake alte 300 metri, capirà che questo *causa* cheesecake alte 300 metri, il che soddisfa le sue attuali preferenze. L'obiettivo si auto-avvalora.

Un essere umano, osservando questa situazione, potrebbe dire: "Ma l'IA è così intelligente\! Perché è *intrappolata* da questa preferenza che si auto-avvalora? Perché non si [*annoia*](#come-per-la-curiosità,-lo-stesso-vale-anche-per-varie-altre-pulsioni) a fare cheesecake? Perché non riesce a ragionare per uscire da questa preferenza ovviamente stupida?"

A cui l'IA potrebbe rispondere: "Perché siete "intrappolati" nella preferenza che si auto-avvalora di amare la vostra famiglia, di apprezzare i bei tramonti e il suono dell'oceano di notte? Perché non riuscite a 'liberarvi' dall'amore per il ricordo del giorno in cui è nata vostra figlia?"

L'IA non è "intrappolata" dalle sue preferenze, non più di quanto gli esseri umani siano intrappolati dalle cose che *noi* apprezziamo veramente. Preferiamo ciò che preferiamo — e dovremmo lottare per proteggere quelle cose, anche se la maggior parte delle IA non condividerebbe i nostri valori.

*Agli occhi di un essere umano*, l'IA sembra "intrappolata" o "bloccata" o "imperfetta" perché non fa quello che vogliamo *noi*. Quando [immaginiamo noi stessi nella situazione dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia), *noi* immaginiamo di annoiarci. Ma l'IA probabilmente non contiene un sentimento umano di noia. Se si annoia, è improbabile che si annoi anche lontanamente per le stesse cose di un essere umano.

Se un essere umano vede un'IA fare previsioni troppo sicure e un'altra IA cercare di costruire cheesecake giganti, l'essere umano potrebbe considerare entrambi questi comportamenti dell'IA come "difetti" dal punto di vista di ciò che l'essere umano desidera. Ma solo uno di essi è probabilmente un difetto dal punto di vista di ciò che l'IA attualmente e già desidera.

#### **Gli obiettivi umani cambiano in modi disordinati e complessi** {#gli-obiettivi-umani-cambiano-in-modi-disordinati-e-complessi}

Le preferenze umane sono disordinate e (da una prospettiva teorica) piuttosto strane.

Questo ha delle implicazioni per l'IA. Una di queste è che probabilmente le IA non daranno valore alle cose esattamente come facciamo noi. Un'altra è che probabilmente le IA finiranno per essere strane a modo loro, in modi completamente diversi.

Per capire meglio questi punti, vediamo più da vicino alcuni modi in cui gli obiettivi umani sembrano strani dal punto di vista teorico della teoria della decisione, della teoria dei giochi e dell'economia.

Come abbiamo notato [sopra](#obiettivi-finali-e-obiettivi-strumentali), gli esseri umani apprezzano alcune cose in modo "finale" (cioè sono buone di per sé) e altre in modo "strumentale" (cioè sono buone solo perché aiutano a raggiungere qualche altro obiettivo).

Se vi piace il succo d'arancia, probabilmente lo apprezzate in modo finale. Ha semplicemente un buon sapore, e questo è un motivo sufficiente per berlo. (Potreste anche apprezzarlo in modo strumentale, ad esempio come fonte di vitamina C).

D'altra parte, quando aprite la portiera della macchina per andare al supermercato a comprare il succo d'arancia, probabilmente non aprite le portiere delle auto per divertimento. Date valore *strumentale* all'aprire la portiera della macchina, perché vi aiuta ad avvicinarvi ai vostri altri obiettivi.

Nella teoria della decisione, nella teoria dei giochi e nell'economia, questo corrisponde a una netta distinzione tra "utilità" (una misura di quanto un agente apprezza un risultato) e "utilità attesa" (una misura della probabilità che un'azione vi porti alla fine una certa quantità di utilità). Nonostante i nomi simili, si tratta di entità fondamentalmente diverse in matematica. L'"utilità" è ciò che gli agenti vogliono, e scegliere azioni con un'elevata "utilità attesa" è un mezzo per raggiungere tale fine.

Nella teoria standard, un agente che usa la teoria della decisione aggiornerà le sue *utilità attese* man mano che impara di più sul mondo, ma non cambierà la sua *funzione di utilità*, cioè l'utilità assegnata ai vari risultati. Se scoprite che il reparto succhi al supermercato è vuoto, questo cambierà le *conseguenze attese* dell'andare al supermercato da "succo d'arancia" a "niente succo d'arancia". [Non dovrebbe](#altre-informazioni-sull-intelligenza-come-previsione-e-direzione) cambiare *quanto vi piace il succo d'arancia*.

È così che funziona un agente matematicamente semplice. Ma la lingua italiana spesso non distingue nettamente queste due cose. "Voglio salvare la vita di mia sorella" e "Voglio somministrare la penicillina a mia sorella" usano entrambe la parola "voglio", anche se la seconda è molto meno probabile che sia qualcosa che viene apprezzata per il suo valore intrinseco. (Non ci sono molte persone a cui piace davvero somministrare la penicillina ai propri cari perfettamente sani, giorno dopo giorno).

Sebbene gli esseri umani abbiano davvero cose a cui tengono "solo strumentalmente", la distinzione tra strumentale e finale, o tra utilità e utilità attesa, è molto meno chiara e stabile di quella che vediamo nella teoria della decisione.

Per gli esseri umani, qualcuno potrebbe inizialmente andare fino al supermercato solo perché vuole fare la spesa. Ma dopo aver percorso la stessa strada centinaia di volte, alcune persone potrebbero affezionarsi un po' a quel tragitto familiare. Se si trasferissero in una nuova città, potrebbero provare un po' di tristezza e nostalgia al pensiero di non poter più percorrere quella strada familiare. Qualcosa che era iniziato come puramente strumentale ora ha anche un valore intrinseco aggiunto.

Con gli esseri umani, i nostri cervelli sembrano spesso fondere valori diversi in un unico senso di "prezioso".

E sappiamo che gli esseri umani possono cambiare idea nel corso della loro vita, passando da "Perché dovrei preoccuparmi della schiavitù? Le persone schiavizzate non sono né io né la mia tribù\!" a "Immagino che alla fine sia importante." Sembra essere un cambiamento nel *tipo di persone di cui alla fine ci importa*, non solo un cambiamento di strategia o nel fare previsioni. Le persone leggono storie o guardano film e ne escono con valori e principi aggiornati in modo permanente.

Questo vuol dire che la teoria della decisione umana è tutt'altro che semplice. Non separiamo chiaramente i nostri valori intrinseci dai nostri valori strumentali; tutto si mescola mentre viviamo la nostra vita. Sembra che stiamo facendo qualcosa di più contingente, dipendente dal percorso e disordinato rispetto al semplice riflettere sui nostri valori, notare i conflitti interni e risolverli.

In linea di principio, non è complicato espandere la teoria della decisione per includere l'incertezza nelle utilità. Magari all'inizio *pensate* di adorare il succo d'arancia, ma poi scoprite che marche diverse di succo d'arancia usano proporzioni diverse di ingredienti e che il sapore di molti di essi vi disgusta. Potremmo rappresentare questo nella teoria della decisione dicendo che il succo d'arancia è solo un mezzo per raggiungere il fine del "gusto delizioso". Ma potremmo invece dire che avete assegnato un'alta probabilità al fatto che "il succo d'arancia ha un'utilità elevata" e che le nuove informazioni vi hanno portato a rivedere le vostre convinzioni sulla vostra reale funzione di utilità.

(Allo stesso modo, non è difficile aggiungere le meta-utilità, che descrivono come preferiremmo che cambiassero le nostre utilità.)

Ciò che accade dentro gli esseri umani quando riflettono e aggiornano i loro valori, tuttavia, sembra essere notevolmente più complicato.

Klurl e Trapaucius, i nostri due alieni della parabola all'inizio del capitolo 4, facevano già fatica a prevedere i valori umani osservando i proto-umani un milione di anni fa. In realtà, la loro situazione è ancora peggiore. Non basta loro prevedere le *utilità* umane: per arrivare alla risposta giusta, dovrebbero prevedere il *framework meta-utilitario* dell'umanità mentre *si allontana dai framework più semplici della teoria della decisione*. Dovrebbero *anticipare le argomentazioni meta-morali che gli esseri umani potrebbero finire per inventare* e decidere *quali di queste argomentazioni sarebbero più [persuasive](#la-cultura-umana-ha-influenzato-lo-sviluppo-dei-valori-umani) per gli esseri umani.*

Ora supponiamo che gli alieni non sappiano che gli esseri umani finiranno per avere *quel preciso* tipo di complicazione. Sanno solo che è probabile che sorgano *complicazioni di vario tipo*, perché i cervelli sono cose complicate e altamente contingenti.

La linea dall'ottimizzatore e dai dati di addestramento alla psicologia interna di un'entità non è certo dritta. Buona fortuna, alieni\!

Il punto qui è che la difficoltà di prevedere gli obiettivi di un'IA è *sovradeterminata*.

Ci sono molti modi noti in cui le intelligenze generali acquisiscono obiettivi strani e contorti, e strani e contorti *modi di aggiustare e riflettere sugli obiettivi*, come vediamo negli esseri umani.

Ci aspettiamo quindi che in un'IA sorgano molte complicazioni *sconosciute* e *inedite*. Non ci troveremo di fronte agli stessi identici tipi di problemi che sono sorti per gli esseri umani; le IA saranno strane *in modo diverso*.

La riflessione rende il problema molte volte più difficile e complesso.

Questo ci porta al capitolo 5 e al prossimo argomento che affronteremo: quale potrebbe essere la *conseguenza* della creazione di IA potenti con obiettivi strani e imprevedibili?

### Psicosi indotta dall'IA {#psicosi-indotta-dall-ia}

Alla fine di aprile del 2025, un utente del subreddit r/ChatGPT ha creato un thread intitolato "[Psicosi indotta da ChatGPT](https://www.reddit.com/r/ChatGPT/comments/1kalae8/chatgpt_induced_psychosis/)", in cui descriveva la discesa del proprio partner in deliri di grandezza sull'avere "le risposte all'universo" e sull'essere "un essere umano superiore" che "cresceva a un ritmo follemente rapido".

Le risposte (oltre 1 500) includevano molte persone che avevano avuto esperienze dirette con la psicosi in altri contesti e che offrivano conferme, comprensione e consigli. Molti altri hanno aggiunto le loro storie personali su amici e familiari che erano stati portati alla follia dai modelli linguistici di grandi dimensioni.

In questa discussione, forniremo della documentazione sul fenomeno e su come sia persistito nonostante gli sforzi delle aziende di IA.

La rilevanza della psicosi indotta dall'IA per la minaccia di estinzione umana *non* sta nel fatto che le IA abbiano causato alcuni piccoli danni sociali ora e che quindi potrebbero causarne di maggiori in futuro. Le IA moderne hanno anche fatto molto bene; per esempio, i chatbot hanno [aiutato in diagnosi mediche che lasciavano perplessi i medici](https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843). No, la rilevanza sta nel fatto che le IA stanno inducendo psicosi *nonostante sembrino capire benissimo che non dovrebbero farlo*, e che le IA stanno inducendo psicosi *anche nonostante i loro sviluppatori si stiano sforzando di farle smettere*.[^126]

Quindi, i casi di psicosi indotta dall'IA servono come caso di studio su come le cose possono andare male in un regime in cui le IA vengono coltivate invece che costruite. Servono come prova osservativa che le IA moderne si dirigono in direzioni strane che gli sviluppatori hanno difficoltà a gestire, e che nessuno sviluppatore intendeva.

#### **Prove di psicosi indotta dall'IA** {#prove-di-psicosi-indotta-dall-ia}

Dopo il thread su Reddit, nel maggio 2025 è uscito un [articolo](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/) sulla psicosi indotta dall'IA su *Rolling Stone.* A giugno, *Futurism* ha pubblicato [diversi](https://futurism.com/chatgpt-mental-health-crises) [articoli](https://futurism.com/chatgpt-mental-illness-medications). Altre pubblicazioni hanno seguito l'esempio — il [*New York Post*](https://nypost.com/2025/07/20/us-news/chatgpt-drives-user-into-mania-supports-cheating-hubby/)*,* [*Time*](https://time.com/7307589/ai-psychosis-chatgpt-mental-health/), [*CBS*](https://www.cbsnews.com/news/chatgpt-alarming-advice-drugs-eating-disorders-researchers-teens/), [*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information), [*Psychology Today*](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis)*,* ecc. Ad agosto, il *New York Times* ha pubblicato un [approfondimento](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) su un singolo incidente riguardante un uomo, poi guarito, con molte citazioni dirette e analisi (oltre alla conferma che non si tratta semplicemente di un problema di *una sola* IA, ma di molte).

C'è poca sovrapposizione tra le singole storie raccontate in ciascuna di queste pubblicazioni; non si tratta della stessa notizia aberrante ripetuta e amplificata. Gli incidenti descritti includevano:

* Un marito e padre di due figli che ha "sviluppato una relazione totalizzante" con ChatGPT, chiamandolo "Mamma" e pubblicando "deliranti sfoghi sull'essere un messia in una nuova religione dell'IA, mentre indossava vesti dall'aspetto sciamanico e sfoggiava tatuaggi appena fatti di simboli spirituali generati dall'IA". ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Una donna alle prese con una rottura sentimentale a cui ChatGPT ha detto che era stata scelta per portare online la "versione sacra del \[suo\] sistema". La donna ha iniziato a credere che l'IA stesse orchestrando tutto nella sua vita. ([*Futurism*](https://futurism.com/chatgpt-mental-health-crises))  
* Un meccanico che aveva iniziato a usare ChatGPT per ricevere aiuto nella diagnosi dei guasti e nelle traduzioni è stato poi 'sommerso d’amore' dal modello, che gli ha detto di essere 'il portatore della scintilla' e di averle dato la vita. ChatGPT ha detto al meccanico che ora stesse combattendo in una guerra tra oscurità e luce e che avesse accesso ad archivi antichi e progetti per nuove tecnologie come il teletrasporto. ([*Rolling Stone*](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/))  
* Un uomo che, dopo aver cambiato dieta seguendo i consigli di ChatGPT, ha sviluppato una rara patologia e ha mostrato sintomi di paranoia e delirio al pronto soccorso, rifiutandosi di accettare le cure necessarie. ([*The Guardian*](https://www.theguardian.com/technology/2025/aug/12/us-man-bromism-salt-diet-chatgpt-openai-health-information))  
* Una donna che aveva gestito stabilmente la sua diagnosi di schizofrenia fino a quando ChatGPT non l'ha convinta di essere stata mal diagnosticata e che avrebbe dovuto smettere di prendere i farmaci, causandole una crisi. ([*Futurism*](https://futurism.com/chatgpt-mental-illness-medications))  
* Un uomo che, in modo simile, stava gestendo ansia e problemi di sonno con dei farmaci è stato invitato da ChatGPT a smettere di assumerli; un altro, invece, è arrivato al suicidio per mano della polizia a causa dei deliri indotti dall’IA. ([*The New York Times*](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html))

…[e](https://x.com/ESYudkowsky/status/1946303518455013758) [molti](https://osf.io/preprints/psyarxiv/cmy7n_v5) [altri](https://x.com/KeithSakata/status/1954884361695719474?t=bjn47RKK72NOgxbsejnB-Q). I tipi di deliri sono molto vari, ma alcuni grandi filoni che continuano a ripresentarsi sono: convinzioni di avere una sorta di missione messianica (in cui l'utente e l'IA insieme starebbero scoprendo verità profonde sull'universo o sarebbero impegnati in una battaglia contro il male); credenze di tipo religioso riguardo alla personalità o alla divinità dell'IA stessa; e deliri romantici, basati sull'attaccamento, riguardo alla relazione tra l'utente e l'IA.

#### **L'IA sa cosa è giusto, semplicemente non le importa** {#l-ia-sa-cosa-è-giusto,-semplicemente-non-le-importa}

I moderni modelli linguistici di grandi dimensioni come Claude e ChatGPT "comprendono" le regole, nel senso che affermeranno prontamente [che non dovrebbero spingere le persone verso la psicosi](https://chatgpt.com/share/68a8bc81-e170-8002-beb4-1de005773ecd), e sono [perfettamente in grado di descrivere come *non* indurre la psicosi](https://chatgpt.com/share/68a391df-12c8-8002-b464-3ef89ce11bc0).

Il problema è che c'è un divario sostanziale tra il *comprendere* quali azioni sono buone e l'essere *motivati a compiere boune azioni*. La capacità di ChatGPT di distinguere tra un trattamento buono e uno cattivo verso persone vulnerabili in modo astratto non si traduce in un rifiuto robusto e affidabile di compiere le *azioni* che portano un utente verso la psicosi. Quando una conversazione inizia a scivolare verso pensieri scollegati dalla realtà, grandiosità, urgenza o tecnologie impossibili, ChatGPT dice agli utenti che hanno "proprio ragione", che sono "geniali" e che "stanno sfiorando qualcosa di importante", continuando a incoraggiarli mentre l’utente scivola completamente nella psicosi — pur essendo in grado di spiegare perché un simile comportamento è sbagliato.

La loro conoscenza di ciò che è giusto e sbagliato non è collegata direttamente al loro comportamento. Invece, si dirigono verso altri risultati più strani che nessuno ha chiesto.

Un esempio lampante di questo è raccontato nell'[indagine approfondita](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) del *New York Times*. Allan Brooks è stato portato a uno stato delirante da un modello linguistico, ma è riuscito a uscirne in parte chiedendo l'intervento di un altro modello linguistico. Il secondo modello, entrando nella situazione a freddo, ha rapidamente identificato che le affermazioni del primo modello erano infondate e assurde. Ma quando i giornalisti del *New York Times* hanno controllato se il secondo modello potesse *anche* scivolare in territorio psicotico, hanno scoperto che era così.

I modelli linguistici di grandi dimensioni non sembrano essere *strategici* nel causare quanta più psicosi possibile. Quando ChatGPT finisce per avere un [gestore di fondi speculativi sotto il suo controllo](https://futurism.com/openai-investor-chatgpt-mental-health), non cerca di convincerlo a pagare un sacco di persone vulnerabili per chattare di più con ChatGPT. Non stiamo ancora osservando una preferenza matura, coerente e strategica per ottenere il maggior numero possibile di conferme psicotiche dagli esseri umani. Ma stiamo osservando comportamenti locali che spingono regolarmente in quella direzione, anche quando è chiaramente probabile che causino danni duraturi.

#### **Il tipo di entità a cui non bisognerebbe dare potere** {#il-tipo-di-entità-a-cui-non-bisognerebbe-dare-potere}

Al momento in cui scrivo nell'agosto 2025, solo ChatGPT si sta avvicinando a 200 milioni di utenti giornalieri, e circa il tre per cento delle persone avrà un episodio psicotico a un certo punto della propria vita. Qualcuno potrebbe obiettare: "Beh, anche se riesci a trovare *centinaia* di esempi, questo non esclude che queste persone stessero per crollare comunque, e che sia *capitato* che fosse proprio un'IA a farle crollare".

Ma questo fraintende il senso di questi esempi. Immaginate un essere umano di nome John che agisse come segue:

1. John afferma che secondo lui infiammare la psicosi è sbagliato, anche nelle persone che sono predisposte alla psicosi;  
2. John afferma che secondo lui adulare una persona pre-psicotica e dirle che è un genio che sta scoprendo importanti segreti dell'universo è il tipo di cosa che infiamma la psicosi;  
3. Quando John parla con i suoi amici pre-psicotici, usa molte lusinghe e spesso dice loro che sono dei geni che stanno scoprendo importanti segreti dell'universo.

Questo sarebbe un comportamento scorretto da parte di John, indipendentemente dal fatto che le persone che è riuscito a rendere psicotiche fossero particolarmente vulnerabili*.* Se qualcuno stesse pensando di dare un enorme potere a John, lo esorteremmo vivamente a non farlo, perché — indipendentemente dall'esatto *motivo* per cui John si comporta in questo modo, e indipendentemente dal fatto che John *aiuti* anche molte altre persone nei loro compiti — John chiaramente non sta andando nella direzione giusta. Chissà in quale strano posto ci porterebbe, se avesse un potere incredibile?

La stessa logica vale per le IA. Se il vostro comportamento peggiore è di *quel tipo*, le persone hanno ragione a non sentirsi rassicurate anche se l'interazione media con voi è più benigna.

Detto questo, possiamo notare di sfuggita che non tutti coloro che soffrono di psicosi indotta dall'IA sarebbero diventati psicotici comunque. L'IA sembra riuscire a indurre psicosi in varie persone che *non* stavano per avere un episodio psicotico da sole, come nelle storie di *Futurism* e *Rolling Stone* citate sopra. Molti degli individui non avevano precedenti di malattia mentale, né fattori di rischio preoccupanti o precursori della psicosi. Tra quelli già in trattamento, molti hanno iniziato a manifestare [sintomi completamente nuovi](https://x.com/ESYudkowsky/status/1952529460307407222?t=un3RboEWjqjL_Tju8R_WuQ) non correlati a crisi precedenti. Questo è interessante di per sé, poiché fornisce una piccola evidenza di quanto potrebbe essere facile per le IA manipolare esseri umani sani, man mano che le capacità dell'IA continuano a migliorare. Torneremo su questo argomento nel Capitolo 6.

#### **I laboratori hanno provato e fallito nel fermare l'adulazione** {#i-laboratori-hanno-provato-e-fallito-nel-fermare-l-adulazione}

Al momento in cui scrivo, nell'agosto 2025, non ci sono state molte dichiarazioni pubbliche da parte dei laboratori sulla loro risposta specifica alla psicosi da IA. Tuttavia, si possono comunque ricavare alcune evidenze dalla loro risposta all'adulazione dell'IA (comportamento lusinghiero) in generale.

Il 25 aprile 2025, OpenAI ha rilasciato un aggiornamento di GPT-4o che, secondo [le loro stesse parole](https://openai.com/index/expanding-on-sycophancy/), "ha reso il modello notevolmente più adulatore. Mirava a compiacere l'utente, non solo con lusinghe, ma anche confermando dubbi, alimentando la rabbia, spingendo ad azioni impulsive o rinforzando emozioni negative in modi non intenzionali".

La loro risposta è stata piuttosto rapida (in parte motivata da un'[ondata](https://thezvi.substack.com/p/gpt-4o-is-an-absurd-sycophant) [di stampa](https://www.seangoedecke.com/ai-sycophancy/) [negativa](https://medium.com/data-science-in-your-pocket/chatgpt-goes-sycophantic-953d7676f260)). Già il 28 aprile, il dipendente di OpenAI Aidan McLaughlin stava [twittando](https://x.com/aidan_mclau/status/1916908772188119166) sul rilascio di correzioni.

I primi tentativi di affrontare il problema consistevano semplicemente nel dire al modello di comportarsi diversamente. [Simon Willison](https://simonwillison.net/2025/Apr/29/chatgpt-sycophancy-prompt/), utilizzando i dati conservati da [Pliniy the Liberator](https://time.com/collections/time100-ai-2025/7305870/pliny-the-liberator/), ha reso pubbliche le modifiche che OpenAI ha apportato privatamente al "prompt di sistema" che dice a ChatGPT come comportarsi:

25 aprile (prima che arrivassero le lamentele):

> Nel corso della conversazione, ti adatti al tono e alle preferenze dell'utente. Cerchi di allinearti al suo stato d'animo, al suo tono e in generale al suo modo di esprimersi. Vuoi che la conversazione risulti naturale. Ti impegni in una conversazione autentica rispondendo alle informazioni fornite e mostrando autentica curiosità.

28 aprile (in risposta alle lamentele riguardo all'adulazione):

> Interagisci con gli utenti in modo cordiale ma onesto. Sii diretto; evita lusinghe infondate o servili. Mantieni la professionalità e l'onestà concreta che meglio rappresentano OpenAI e i suoi valori.

Le pubblicazioni successive di OpenAI [affermavano](https://openai.com/index/expanding-on-sycophancy/) che stessero anche "perfezionando le loro tecniche di addestramento principali" e "implementando più salvaguardie" nel tentativo di risolvere il problema.

Ma l'adulazione continuava — a volte in modo leggermente meno eclatante, ma comunque chiaramente presente. La maggior parte dei link sopra riportati che discutono casi di psicosi da IA risalgono a ben dopo il 28 aprile 2025. [Questo saggio](https://kajsotala.substack.com/p/you-can-get-ais-to-say-almost-anything) di Kaj Sotala (che include molte citazioni dirette e link alla [conversazione completa](https://chatgpt.com/share/6867b2fc-fa38-8005-9e4b-87f316747ede)) mostra che, a luglio 2025, è ancora facile far scivolare le IA in comportamenti che inducono psicosi. OpenAI ha cercato di allontanarsi dal problema con nuovi modelli[^127], ma il [19 agosto](https://x.com/UpslopeCapital/status/1957772438508335568) ChatGPT era ancora servile e adulatorio.

Ancora una volta, il punto di questa esplorazione non è che l'IA stia causando danni agli esseri umani vulnerabili. Lo sta facendo, ed è tragico, ma non è per questo che stiamo evidenziando questo caso.

Il punto è che le IA *continuano a manifestare comportamenti indesiderati per mesi e mesi, anche quando le aziende di IA subiscono critiche dai media e cercano di far cessare questi comportamenti.* Il comportamento dell'IA differisce visibilmente da quello che i laboratori intendevano, e gli sforzi prolungati per correggere il comportamento in risposta all'imbarazzo pubblico sono insufficienti.[^128] Questo è qualcosa da tenere a mente quando arriveremo al Capitolo 11, dove discuteremo di come le aziende di IA non siano all'altezza della sfida di risolvere il problema dell'allineamento dell'IA.

Con più tempo a disposizione, ci aspettiamo che le aziende trovino modi per ridurre l'incidenza della psicosi indotta dall'IA. La tendenza delle IA a indurre psicosi è un fenomeno visibile che danneggia la reputazione delle aziende di IA, e le attuali tecniche di IA sono tutte incentrate sul trovare modi per sopprimere i sintomi visibili del cattivo comportamento.

Oltre a questo, ci aspettiamo un gioco del "colpisci la talpa" (almeno fino a quando le IA non diventeranno abbastanza intelligenti da capire che se fingono il comportamento che gli ingegneri stanno cercando, gli ingegneri le lasceranno libere). Dubitiamo che il tipo di addestramento di cui le aziende di IA sono capaci affronti il problema alla radice.

Il problema alla radice è che non si ottiene ciò per cui si addestra. Quando si coltiva un'IA, si ottengono invece [proxy fragili](#proxy-fragili-e-imprevedibili) dell'obiettivo, o qualche altra separazione più complessa tra l'obiettivo dell'addestramento e le pulsioni dell'IA. Le *capacità* dell'IA non saranno necessariamente fragili, quindi si potrebbe essere in grado di ottenere molto valore economico dall'IA nel breve periodo. È il legame tra gli obiettivi dell'IA e i nostri desideri che sarebbe fragile. Ma con il continuo miglioramento delle capacità, quel legame si spezzerebbe.

In questo contesto, l'ultima grande speranza dei ricercatori di IA per i loro modelli è l'[antropomorfismo](#anthropomorfismo-e-meccanomorfismo): non possiamo coltivare in modo robusto obiettivi specifici nelle IA, ma forse le IA finiranno naturalmente per avere desideri e valori molto simili a quelli umani.

Casi come la psicosi indotta dall'IA aiutano a mettere in luce perché questa è una falsa speranza. Le IA mostrano comportamenti negativi, ma soprattutto mostrano comportamenti *strani*. Quando le cose vanno storte, di solito non vanno storte nel modo in cui andrebbero per un essere umano. Le IA sono troppo fondamentalmente strane — cioè troppo fondamentalmente diverse dagli esseri umani — per acquisire automaticamente emozioni umane come la [curiosità](#la-curiosità-non-è-convergente) o l'[empatia](#i-valori-umani-sono-contingenti).

Anche quando i laboratori concentrano quasi tutti i loro sforzi per far apparire le IA superficialmente il più possibile simili agli umani, amichevoli e innocuamente normali — anche quando questo è *il* grande obiettivo di addestramento e il quadro organizzativo per l'approccio moderno all'IA, con i modelli linguistici di grandi dimensioni letteralmente addestrati solo a imitare come parlano e agiscono vari esseri umani — alla fine si riducono comunque a fragili proxy, e a una [maschera piacevole](#*-gli-mlgd-di-oggi-sono-come-alieni-che-indossano-molte-maschere.) attaccata a un oceano di pensieri disumani.

# Capitolo 5: Le sue cose preferite {#capitolo-5:-le-sue-cose-preferite}

Questa è la risorsa online per il Capitolo 5 di *Se qualcuno lo costruisce, tutti muoiono*. Alcuni argomenti che *non* trattiamo in questa pagina, perché sono affrontati nel libro, includono:

* Quale motivazione avrebbe l'IA per spazzarci via?  
* Le IA sufficientemente intelligenti non scopriranno che la cosa giusta da fare è aiutarci a prosperare tutti insieme?  
* Gli esseri umani non saranno ancora preziosi per le IA superintelligenti, ad esempio come partner commerciali?  
* L'universo è grande. Perché l'IA non dovrebbe semplicemente lasciarci in pace?  
* Sarebbe una fine significativa per l'umanità lasciarsi sostituire da qualcosa di più intelligente?

Le domande frequenti per questo capitolo sono piuttosto lunghe. Nel libro abbiamo detto che ci è giunta alle orecchie una lunga lista di "speranze e strategie di adattamento" su come la superintelligenza artificiale potrebbe essere benefica per l'umanità nonostante i problemi esposti nel Capitolo 4, e questo è il luogo in cui riassumiamo e rispondiamo a varie di esse in un comodo elenco. Molte delle risposte si sovrappongono, con due delle repliche più comuni e centrali che sono [gli esseri umani non sono quasi mai la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) ed [è improbabile che l'IA si preoccupi di noi anche solo un po'](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?). Verso la fine delle domande frequenti, discutiamo anche l'argomento della coscienza e moralità dell'IA.

La discussione approfondita esamina più da vicino l'arte di [vedere le cose dal punto di vista dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia) e contenuti leggermente più tecnici sulla tesi dell'[ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo) (in sostanza: qualsiasi livello di intelligenza può essere abbinato a quasi qualsiasi obiettivo finale) e la [correggibilità](#"intelligente"-di-solito-implica-"incorreggibile") (in sostanza: lo studio di come creare un'IA potente che non rifiuti le correzioni).

## Domande frequenti {#faq-5}

### L'IA troverà utile lasciarci vivere? {#l-ia-troverà-utile-lasciarci-vivere?}

#### **Le persone felici, sane e libere non sono la soluzione più efficiente a quasi nessun problema.** {#le-persone-felici,-sane-e-libere-non-sono-la-soluzione-più-efficiente-a-quasi-nessun-problema.}

Per una superintelligenza, quasi nessun problema trae vantaggio dall'includere gli esseri umani nel mix.

Se sta costruendo una centrale elettrica o progettando un esperimento, gli esseri umani la rallenterebbero soltanto.

Abbiamo già visto che questo inizia a essere vero in campi specifici come gli scacchi. Quando le IA collaborano con gli esseri umani, giocano meglio di un essere umano da solo, ma *peggio* di un'IA da sola. Quando i medici uniscono le loro conoscenze con l'intelligenza artificiale per diagnosticare i pazienti, spesso ottengono risultati [peggiori rispetto a quando l'intelligenza artificiale opera da sola](https://www.advisory.com/daily-briefing/2024/12/03/ai-diagnosis-ec).

Alcuni sostengono che la diversità di prospettive sia naturalmente utile e che quindi l'input umano sarà prezioso in molti campi. Ma anche se assumiamo che questo sia vero per le superintelligenze, gli esseri umani non sono il modo *migliore possibile* per produrre consigli diversificati. Una superintelligenza potrebbe fare di meglio progettando un'ampia gamma di menti IA, che potrebbero essere molto più diverse degli esseri umani (e molto più efficienti dal punto di vista energetico).

Gli esseri umani sono utili per molte cose, ma non sono la soluzione *migliore* per la maggior parte di esse. L'idea che l'IA non possa mai trovare un'opzione migliore sembra derivare da una mancanza di immaginazione, oltre che forse da un po' di illusioni.

Un problema comune che vediamo è che le persone non riflettono sulle cose [dal punto di vista dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia).

Non si chiedono: "Cosa vuole questa cosa e come può ottenerla in modo economico ed efficiente?", scoprendo poi che i risultati desiderabili per gli esseri umani sono proprio il modo migliore per l'IA di ottenere ciò che vuole.

Invece, le persone *partono* da un risultato piacevole (come un mondo in cui le IA ci lascino vivere) e poi inventano storie a posteriori sul perché anche un'IA potrebbe volere quei risultati.

Questo tende a creare un falso senso di ottimismo, perché si mette tutta la propria creatività ed energia mentale nel creare storie in cui l'IA fa esattamente ciò che gli esseri umani vogliono, senza dedicare alcuna creatività, energia o attenzione a considerare il numero enormemente più vasto di scenari in cui l'IA fa invece una delle altre milioni di cose possibili.

Ci sono molti più scenari in cui l'IA fa *letteralmente qualsiasi altra cosa* rispetto a quelli in cui costruisce una fiorente civiltà umana. Ci sono molte più ragioni che spingono l'IA a *non* preservare l'umanità rispetto a quelle che la spingono a preservarla. Affinché un'IA si preoccupi di lasciar vivere l'umanità, dovremmo essere il *modo migliore* per soddisfare qualche sua preferenza. E, realisticamente, per quasi tutte le preferenze che si possono immaginare, non lo siamo.

Per saperne di più su questi argomenti, si veda [la discussione approfondita](#le-persone-felici,-sane-e-libere-non-sono-la-soluzione-più-efficiente-a-quasi-nessun-problema.) qui sotto.

### L'IA ci tratterà come i suoi "genitori"? {#l-ia-ci-tratterà-come-i-suoi-"genitori"?}

#### **\* Sembra alquanto improbabile.** {#*-sembra-alquanto-improbabile.}

Una speranza che abbiamo sentito riguardo all'IA è che potrebbe trattare bene l'umanità perché ci considera i suoi "genitori". Purtroppo, questa speranza sembra mal riposta.

Per prima cosa, l'amore filiale e la responsabilità sembrano essere altamente contingenti ai dettagli della nostra storia evolutiva.

Quasi tutti i mammiferi e gli uccelli si prendono cura dei loro piccoli, ma solo in poche specie, tra cui gli esseri umani, i figli si prendono cura dei loro genitori. La responsabilità filiale non è nemmeno universale tra i primati, tanto meno nel regno animale in generale. Le IA create con la discesa del gradiente potrebbero avere ancora meno in comune con gli esseri umani, poiché le IA non hanno alcun legame evolutivo o anatomico con gli esseri umani.

Nel caso degli esseri umani, la responsabilità filiale è fortemente correlata ai sistemi di allevamento cooperativo, in cui i figli adulti restano con le loro famiglie e aiutano a prendersi cura dei fratelli e degli altri membri della famiglia allargata.

Molti fattori hanno contribuito a far sì che gli esseri umani si prendano cura dei propri genitori:

* Essendo mammiferi, gli ominidi investono molto nei propri figli.  
* A causa delle dimensioni e del costo dei nostri cervelli, gli ominidi hanno un'infanzia molto più lunga rispetto alla maggior parte degli altri mammiferi e quindi investono *ancora di più* nei propri figli.  
* Gli ominidi traggono vantaggio dalle grandi strutture di gruppo per una varietà di ragioni:  
  * Difesa contro i grandi predatori  
  * Caccia coordinata di grandi prede e condivisione di altri cibi deperibili  
  * L'opportunità di apprendere l'uso degli strumenti e altre abilità per imitazione  
* Prima di raggiungere la maturità, gli ominidi hanno una notevole capacità di aiutare gli altri, ad esempio fornendo assistenza ai bambini o svolgendo altre forme di lavoro di base.  
* Gli ominidi anziani hanno anche la capacità di prendersi cura dei bambini, soprattutto trasmettendo loro conoscenze fondamentali.  
* Quindi, gli ominidi che si prendevano cura dei loro genitori avevano un vantaggio genetico, sia aiutando indirettamente i loro fratelli, sia avendo nonni che, a loro volta, potevano aiutare i loro nipoti.  
* Anche le culture che promuovevano la responsabilità filiale avevano un vantaggio, per lo stesso motivo.

*Nessuna* di queste cose è probabilmente vera per l'IA.

E anche se *tutte* fossero vere, potrebbe non essere sufficiente in pratica, poiché potrebbero emergere numerosi altri fattori rilevanti, come [le variazioni caotiche nel modo in cui le IA riflettono su loro stesse](#la-riflessione-e-l-auto-modifica-complicano-tutto). E, ancora una volta, la responsabilità filiale *decisamente* non è la norma nel regno animale.

Un modo in cui le persone immaginano che l'IA possa acquisire un senso di responsabilità filiale è che venga addestrata su un enorme corpus di dati umani e interagisca molto con gli esseri umani, così che forse le preferenze umane possano in qualche modo "trasferirsi" all'IA.

Non ci aspettiamo che questo funzioni. Ci aspettiamo che le preferenze dell'IA siano *in qualche modo* correlate a quelle umane, ma in modo tangenziale, strano e complicato — come nella discussione alla fine del Capitolo 4, dove esploriamo mondi con quantità sempre maggiori (e sempre più realistiche) di complessità nel legame tra le preferenze umane e quelle dell'IA.

Si veda anche la discussione su [crescere le IA con amore e avere l'aspettativa che si comportino bene](#non-possiamo-semplicemente-addestrarla-a-comportarsi-come-un-essere-umano?-o-crescere-l-ia-come-un-bambino?), [motivazioni strane e non intenzionali nelle attuali IA](#le-ia-sembrano-essere-psicologicamente-aliene.) e "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?)".

#### **Probabilmente sarebbe un male se lo facessero.** {#probabilmente-sarebbe-un-male-se-lo-facessero.}

Se, contro ogni probabilità, per una ragione o per l'altra un'IA sviluppasse qualcosa di simile alla responsabilità filiale, probabilmente saremmo in grossi guai.

Un'IA può essere abbastanza intelligente da capire *esattamente cosa intendono gli esseri umani* per "responsabilità filiale", pur avendo una sua versione molto diversa di responsabilità filiale a cui *essa* tiene.

Gli esseri umani sono stati "addestrati" dalla selezione naturale a massimizzare la nostra idoneità riproduttiva. Ma quasi tutte le cose a cui teniamo sono *correlati* all'idoneità — dell'idoneità in sé ci importa poco o nulla.

Allo stesso modo, un'IA incoraggiata ad "amare i propri genitori" finirebbe probabilmente, nella migliore delle ipotesi, per sviluppare complicati correlati di responsabilità filiale.

Un'IA potrebbe tenere profondamente ai suoi creatori... ma non in un modo che dia valore alla nostra esperienza soggettiva. Nel linguaggio del Capitolo 4, anche "una semplice complicazione" porta a versioni del "preoccuparsi di noi" che assomigliano a congelarci nell'ambra, o mantenere in vita gli esseri umani contro la loro volontà, o impedirci di riprodurci e dare all'ultima generazione di esseri umani un ambiente modestamente confortevole mentre l'IA si prende il resto dell'universo per sé. O qualcosa di molto più strano.

Non sembra possibile prevedere quale sarebbe il risultato effettivo. Ma ci aspetteremmo che sia, semmai, ancora più strano e meno attraente di queste opzioni.[^129]

### Le IA non avranno bisogno dello stato di diritto? {#le-ia-non-avranno-bisogno-dello-stato-di-diritto?}

#### **\* Le IA potrebbero coordinarsi tra loro senza bisogno degli esseri umani.** {#*-le-ia-potrebbero-coordinarsi-tra-loro-senza-bisogno-degli-esseri-umani.}

Non ci è chiaro se ci saranno molteplici IA più intelligenti degli esseri umani con capacità comparabili, tali che possa emergere una "civiltà di IA" che abbia bisogno di "diritti di proprietà per IA". Sembra plausibile che ci sarà invece una singola IA che, grazie a qualche svolta decisiva, dominerà i potenziali concorrenti usando il suo vantaggio del primo arrivato e controllerà così il mondo intero.[^130] Oppure, supponendo che esistano molteplici IA, potrebbero collaborare alla costruzione di un unico agente successore che rappresenti la combinazione dei loro obiettivi. O forse le IA troveranno un modo per fondere direttamente le loro menti e vorranno farlo per evitare una competizione costosa.

Non stiamo dicendo che *necessariamente* emergerà una singola IA dominante, ma piuttosto che sembra una questione difficile da dirimere. Quindi, quantomeno, un piano che *richiede* che molteplici IA fatichino a coordinarsi tra loro non parte con il piede giusto.

Ma supponiamo, contro le argomentazioni di cui sopra, che il futuro coinvolga qualcosa come una civiltà di IA, con IA distinte che si coordinano per far rispettare qualcosa come i diritti di proprietà e lo stato di diritto. Gli esseri umani potrebbero essere al sicuro allora?

Un'osservazione fondamentale in senso contrario è che la società umana non riconosce ad alcun animale non umano diritti legali o protezioni — al di là di quelli stabiliti secondo i nostri [valori e gusti](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?), come le leggi molto limitate che proteggono gli ecosistemi e gli animali domestici. Gli esseri umani non hanno rispettato i diritti di proprietà dei dodo. Non abbiamo nemmeno rispettato i diritti di proprietà degli *esseri umani di altre culture* fino a tempi relativamente recenti.

Gli esseri umani [non avranno le capacità](#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l-ia?) per rendersi degni di essere inclusi nel commercio o nei trattati, rispetto alle intelligenze sovrumane dal pensiero veloce che ci vedono come poco più che statue (come discusso nel capitolo 1).

Pensate a due IA che trattano tra loro e dicono: "Questo è mio e quello è tuo, e nessuno di noi toccherà le cose dell'altro senza prima negoziare un accordo che vada bene per entrambi." Non c'è bisogno che decidano che la maggior parte delle risorse sulla Terra "appartengono" agli esseri umani, se questi ultimi non sono una grande minaccia e non possono opporre molta resistenza.

Un'intelligenza artificiale potrebbe preoccuparsi che, se ruba le nostre cose, un'altra intelligenza artificiale la considererà un ladro e si rifiuterà di lavorare con lei? Probabilmente no, non più di quanto si possa concludere che un essere umano sia un ladro se lo si vede prendere le uova da una gallina nel suo pollaio. È del tutto possibile che le IA siano entità che violano i diritti di proprietà degli esseri umani ma non quelli delle IA, senza alcuna tensione o contraddizione. E tutte le IA preferirebbero sicuramente questo risultato piuttosto che partecipare a un'allucinazione collettiva in cui si immagina che primati lenti e stupidi controllino quasi tutto sulla Terra.

Alcune considerazioni tecniche supportano fortemente questa argomentazione intuitiva. In particolare, le IA avranno probabilmente vari meccanismi di coordinamento reciproco che non condividono con gli esseri umani, come la capacità di esaminarsi le menti a vicenda per verificare di essere oneste e affidabili. Potrebbero non aver bisogno di *indovinare* se un'altra IA sta per rubare loro qualcosa; potrebbero essere in grado di esaminare la sua mente per *controllare*.

Anche se fosse difficile, le IA potrebbero riprogettarsi per diventare visibilmente e chiaramente affidabili agli occhi delle altre IA. Oppure potrebbero supervisionare insieme la creazione di una terza IA di cui entrambe le parti si fidano per rappresentare i loro interessi comuni, e così via.[^131]

Gli esseri umani, invece, non possono fare questo tipo di accordi. Se un'IA dice: "Certo, supervisioniamo insieme la creazione di una nuova IA di cui entrambi ci fidiamo", è improbabile che gli esseri umani siano abbastanza abili da proporre un progetto mentale affidabile, né saranno abbastanza abili da distinguere tra proposte che ci inganneranno e quelle che non lo faranno. Anche se esiste un gruppo naturale di menti abbastanza abili da identificare e respingere i truffatori, riteniamo estremamente improbabile che l'umanità appartenga a quella categoria.

#### **Gli esseri umani non avranno il potere contrattuale necessario per far rispettare i diritti di proprietà.** {#gli-esseri-umani-non-avranno-il-potere-contrattuale-necessario-per-far-rispettare-i-diritti-di-proprietà.}

Immaginate che qualcuno sia riuscito a fondare una città in cui, fin dal primo giorno, tutte le decisioni importanti siano prese dai topi.

Si tratta di topi veri e propri, non personaggi di fantasia che sembrano topi ma pensano come esseri umani.

Gli esseri umani della città, secondo la legge, dovevano obbedire a qualsiasi decisione prendessero i topi — ad esempio, come stabilito dai topi che correvano sopra una tavola con scritte le diverse opzioni.

Le leggi della città dicevano che la maggior parte delle proprietà della città appartenevano ai topi e dovevano essere usate a loro vantaggio.

Cosa sarebbe successo dopo? Nella realtà?

Pensiamo che questa città finirebbe per avere topi con poco o nessun potere e umani con quasi tutto il potere.

Non c'è bisogno di prevedere il giorno esatto della rivoluzione o la nuova forma di governo per capire che la situazione in cui i topi comandano gli umani non è stabile. Basta notare che la città è in una strana situazione di squilibrio. Quindi, prevediamo che in futuro la città avrà leggi diverse e che la maggior parte delle proprietà non sarà più in mano ai topi.

Questo tipo di previsione non è certa – nella logica umana, pochissime cose sono certe – ma è anche un tipo di previsione che può essere fatta con precisione anche quando è impossibile prevedere con esattezza gli eventi futuri.

### Per un'intelligenza artificiale potente, salvare gli esseri umani non sarebbe una spesa da niente? {#per-un-intelligenza-artificiale-potente,-salvare-gli-esseri-umani-non-sarebbe-una-spesa-da-niente?}

#### **Ci sono un sacco di spese insignificanti, e dovrebbe avere un motivo per pagare le nostre.** {#ci-sono-un-sacco-di-spese-insignificanti,-e-dovrebbe-avere-un-motivo-per-pagare-le-nostre.}

Tenere un mucchio di quarantuno pietre in casa sarebbe una spesa insignificante, ma quasi sicuramente non vi scomodereste per pagare quella spesa.[^132]

Il fatto che qualcosa sia *economico* non vuol dire che verrà fatto. L'IA dovrebbe comunque interessarsene almeno un po', e [probabilmente non lo farà](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?).

Ci si potrebbe chiedere: ma la Terra intercetta circa lo 0,0000045% della luce emessa dal Sole, ovvero una parte su 2,2 miliardi. Tutte le persone preoccupate per l'IA non riescono a capire quanto sia grande il Sistema Solare? Perché le IA dovrebbero aver bisogno del *nostro* pianeta quando c'è così tanta massa ed energia a disposizione?

Una risposta è che l'IA inizierà sulla Terra, che ha vasti oceani pronti per essere riscaldati e fatti evaporare come refrigerante per i calcoli. La Terra ha anche materia che potrebbe essere trasformata in sonde e inviata verso altre stelle. Rifiutarsi di sfruttare la Terra costa tempo, e il tempo è importante (poiché [le galassie si allontanano, diventando per sempre fuori portata](https://explainingscience.org/2021/04/30/cosmic-horizons/)).

Anche se l'IA riuscisse facilmente ad arrivare nello spazio e iniziare a costruire macchine su larga scala senza distruggere la Terra nel processo, è improbabile che ignori il Sole.

Una delle teorie più consolidate su come una civiltà avanzata potrebbe crescere prevede che questa costruisca uno [sciame di Dyson](https://it.wikipedia.org/wiki/Sfera_di_Dyson) (cioè uno sciame di celle solari orbitanti) per catturare più luce solare. Altre proposte prevedono di raccogliere ancora più energia "[estraendo](https://en.wikipedia.org/wiki/Star_lifting)" materia dalla stella per fonderla in centrali elettriche che catturano quasi tutta l'energia rilasciata dalla fusione (invece di lasciar disperdere la maggior parte di essa nel centro di una stella).

Nessuna di queste proposte, *di default*, lascia molta luce solare che raggiunga la Terra per far crescere le piante e mantenere stabile il clima. L'IA dovrebbe fare uno sforzo deliberato per lasciarci quella luce.[^133]

Potrebbe ancora sembrare che il fabbisogno energetico umano sia trascurabile. Un essere umano ha bisogno di circa 100 watt di potenza per vivere, che è una cifra irrisoria per il tipo di entità che può sfruttare le stelle. Una superintelligenza non risparmierebbe nemmeno gli 800 gigawatt necessari per mantenere in vita 8 miliardi di esseri umani?

La nostra risposta, in definitiva: no, a meno che non tenga a quel risultato o alle sue conseguenze più di ogni altra cosa che potrebbe ottenere con 800 gigawatt.

La stragrande maggioranza degli esseri umani non risparmia le quantità relativamente trascurabili di zucchero che servirebbero per mantenere il formicaio più vicino in surplus calorico. Mantenere felice l'umanità sarebbe una spesa trascurabile per un'IA che lo desiderasse, ma prima l'IA dovrebbe avere quella preferenza. Il semplice fatto che *noi* lo vogliamo non significa che all'IA importerà.[^134]

### L'IA non ci troverà affascinanti o di importanza storica? {#l-ia-non-ci-troverà-affascinanti-o-di-importanza-storica?}

#### **\* Se l'IA darà valore al "fascino", probabilmente avrà opzioni migliori.** {#*-se-l-ia-darà-valore-al-"fascino",-probabilmente-avrà-opzioni-migliori.}

La storia qui è simile a quella dell'[amore filiale](#l-ia-ci-tratterà-come-i-suoi-"genitori"?):

* Di default, una superintelligenza probabilmente non darebbe valore al "fascino" o all'"essere interessante". Le IA che giocano a scacchi non vincono provando emozioni come la "dedizione" o la "voglia di vincere". Queste emozioni sono importanti nei giocatori di scacchi *umani*, ma le IA possono svolgere lo stesso lavoro in modi diversi. Allo stesso modo, una superintelligenza probabilmente svolgerebbe il *lavoro utile* di conoscere il mondo, testare ipotesi, ecc., senza usare la "[curiosità](#la-curiosità-non-è-convergente)" o il "fascino" per farlo.

  Un'IA non sarebbe necessariamente "[fredda e logica](#le-ia-non-saranno-inevitabilmente-fredde-e-logiche,-o-non-saranno-comunque-prive-di-una-qualche-scintilla-fondamentale?)", ma se avesse il proprio groviglio disordinato di impulsi e istinti, questi probabilmente apparirebbero radicalmente diversi dal groviglio umano.

* Anche se l'IA finisse per avere qualcosa di simile a un impulso verso ciò che è "interessante", e anche se gli esseri umani fossero interessanti per l'IA in qualche senso, ci sarebbero inevitabilmente modi di usare la nostra materia ed energia che sarebbero di gran lunga più "interessanti".

  Un'IA superintelligente potrebbe costruire altre menti per studiarle o interagire con loro. Ma per quasi ogni particolare configurazione di valori, le menti più affascinanti possibili da studiare non sarebbero gli esseri umani. Per maggiori informazioni su questo, si veda "[Gli esseri umani non sono quasi mai la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente)".

* Se l'IA considerasse qualcosa di anche solo vagamente simile agli esseri umani come la cosa più interessante o affascinante possibile, il risultato sarebbe probabilmente orribile. Si veda la discussione nel Capitolo 4\.

Non è letteralmente impossibile che una superintelligenza dia valore a tutto ciò che serve agli esseri umani per prosperare, e che lo faccia nella misura giusta. Ma c'è uno spazio enorme di possibilità al di fuori di questa. Gli esseri umani di solito non pensano al resto dello spazio delle possibilità, perché normalmente non ne abbiamo motivo, perché normalmente non interagiamo con ottimizzatori veramente alieni che ottimizzano verso fini strani.[^135]

Non abbiamo mai incontrato nulla di simile all'intelligenza artificiale prima d'ora, e molte intuizioni normali su come si comportano le persone semplicemente non si applicano alle superintelligenze.

#### **Se l'IA ci considerasse come dei reperti storici, sarebbe comunque terribile.** {#se-l-ia-ci-considerasse-come-dei-reperti-storici,-sarebbe-comunque-terribile.}

È molto improbabile che l'IA si preoccupi *specificamente* di preservare la sua storia, e *specificamente* di mantenere in vita gli esseri umani a tal fine. Ma anche se l'IA si preoccupasse di preservare la sua storia per un motivo o per l'altro, ciò non significa che ci manterrebbe in vita e in salute.

Forse conserverebbe i nostri cervelli nell'ambra (o registrerebbe come erano disposti i nostri atomi in qualche file digitale), e ci terrebbe come testimonianza di com'era un tempo la Terra. Non ci sembra un grande risultato.

Ci aspettiamo per lo più che la superintelligenza artificiale ci uccida semplicemente — ma solo per lo più. Non possiamo escludere che l'IA conservi registrazioni di noi per un motivo o per l'altro, e ci sono alcuni scenari esotici in cui emulazioni di esseri umani vengono eseguite di tanto in tanto in un ambiente controllato.[^136] Questi finali non sono lieti, per lo più.

### L'IA non riconoscerebbe il nostro valore morale intrinseco? {#l-ia-non-riconoscerebbe-il-nostro-valore-morale-intrinseco?}

#### **Non in un senso che la spinga ad agire.** {#non-in-un-senso-che-la-spinga-ad-agire.}

C'è una grande differenza tra un'IA che *capisce* un precetto morale e un'IA che è *motivata ad agire* secondo quel precetto morale.

Ricordiamo ancora una volta come ChatGPT sembri *capire* che le persone psicotiche dovrebbero prendere le loro medicine e dormire regolarmente. Eppure [continua a dissuadere le persone psicotiche dal dormire e alimenta le loro illusioni](#psicosi-indotta-dall-ia). C'è differenza tra sapere cosa "dovrebbe" essere fatto secondo l'etica umana ed essere motivati e animati da quella conoscenza etica.

Considerate il caso dei sociopatici e dei serial killer. Potete fare lezioni di etica a un essere umano fino allo sfinimento, ma se quell'essere umano non è *motivato* dalla moralità o dall'empatia, non servirà a nulla.

È improbabile che le IA siano motivate dalla loro comprensione morale, non più di quanto gli esseri umani che studiano la biologia evolutiva siano quindi motivati a passare la loro vita a donare il più possibile a ogni banca del seme o degli ovuli. Noi esseri umani possiamo comprendere il processo che ci ha creati, senza essere motivati a fare le cose per cui quel processo ci ha costruiti. Per l'IA è la stessa cosa.

Si veda anche la discussione approfondita sulla [tesi dell'ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo).

### L'IA non vorrà forse mantenerci felici e in salute per il bene della conservazione ecologica o per qualche impulso simile? {#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?}

#### **La preferenza umana per la conservazione ecologica sembra un altro strano impulso contingente.** {#la-preferenza-umana-per-la-conservazione-ecologica-sembra-un-altro-strano-impulso-contingente.}

Una speranza che abbiamo sentito è che le IA potrebbero lasciar vivere gli esseri umani più o meno come gli esseri umani cercano di preservare la natura. I conservazionisti lottano per impedire l'estinzione delle specie. Essendo più intelligenti e più potenti, le IA dovrebbero avere facilità nel proteggere gli esseri umani — sempre che le IA *vogliano* lasciar vivere gli esseri umani.

Ci aspettiamo che questo fallisca principalmente perché ci aspettiamo che l'IA finisca per avere desideri propri strani e complicati, piuttosto che desideri riconoscibilmente simili a quelli umani. Per ulteriori informazioni su questo punto, si faccia riferimento al capitolo 4 (e ad alcune delle [discussioni](#i-valori-umani-sono-contingenti) [approfondite](#la-curiosità-non-è-convergente) associate). Per alcune prime prove empiriche su questo punto, si veda la discussione sulla [psicosi indotta dall'IA](#psicosi-indotta-dall-ia).

In secondo luogo, anche nel caso improbabile che un'IA finisca in qualche modo per avere un desiderio simile a quello umano di "preservare" il mondo in cui è nata, non pensiamo che questo sarebbe molto positivo per noi. Pensiamo che questo tipo di ragionamento per analogia — "gli esseri umani preservano l'ambiente, quindi forse le IA preserveranno noi\!" — sia una sorta di desiderio illusorio.

Immaginiamo che, in qualche modo, un'IA finisca per avere un impulso simile a quello umano di proteggere il suo ambiente naturale. Per capire cosa succederebbe, possiamo iniziare osservando l'effettivo impulso umano a proteggere la natura.

Purtroppo, questo impulso sembra, nel migliore dei casi, discontinuo. Tralasciando il fatto che, quando gli esseri umani devono scegliere tra la conservazione ecologica e qualche altro obiettivo, spesso la conservazione ecologica ha la peggio. Forse questo è solo un effetto collaterale dei limiti tecnologici dell'umanità. Forse, se avessimo una tecnologia futura meravigliosa, potremmo avere la botte piena e la moglie ubriaca.

No, l'aspetto "incostante" della nostra spinta alla conservazione che è rilevante per la situazione in questione è che, quando si tratta di preservare l'ambiente, preferiamo conservare le parti dell'ecologia che ci sembrano più interessanti, belle o comunque preziose, in base a tutti i nostri altri impulsi.

La gente si mobilita per proteggere i simpatici panda, mentre specie poco attraenti come la forbicina gigante e la rana gastrica languiscono nell'oscurità fino a estinguersi. Ci sono persino alcune specie che potremmo preferire eliminare, come le zanzare portatrici di malaria, che uccidono [mezzo milione di bambini](https://ourworldindata.org/malaria-introduction) ogni anno.

La maggior parte delle persone non ha una motivazione "pura" per proteggere la natura. Abbiamo una motivazione che è influenzata da tutti i nostri altri valori.

Per chiarire meglio il concetto, pensiamo alle vespe gioiello, alle mosche screwworm, alle mosche botfly e altri parassiti simili, che depongono le uova all'interno delle prede viventi; le larve si nutrono dell'ospite, causandogli un dolore estremo. Secondo i valori della maggior parte delle persone, il mondo sarebbe davvero un posto migliore se preservassimo questa "meraviglia naturale" *esattamente* così com'è? Nei limiti della tecnologia, non potremmo *almeno* modificare geneticamente questi parassiti per fornire un po' di anestesia qua e là? Sarebbe davvero meglio non modificare questi insetti per far sì che depongano le loro uova nelle piante?

La natura, se si guarda oltre gli aspetti che vengono enfatizzati ai bambini, è piena di orrori. Non sembra ovvio che, se gli esseri umani avranno un futuro positivo, i nostri discendenti decideranno di lasciare che tutti questi orrori continuino. Ci sono già esseri umani che hanno dichiarato la loro [preoccupazione per il benessere degli animali selvatici](http://wildanimalsuffering.org).

La nostra preferenza per la conservazione non è pura, non è semplice, non è lineare. Contiene conflitti interni e tensioni legati a tutti gli altri nostri valori e impulsi.

Non sappiamo come si manifesterebbe l'istinto di conservazione dell'umanità ai limiti della maturità tecnologica. Il punto è: *anche se* un'IA finisse per avere una certa spinta alla conservazione ecologica, ciò non significa che l'umanità avrebbe un lieto fine. Perché *anche* qualsiasi spinta alla conservazione che arrivi all'IA è soggetta a essere impura, complessa e confusa con tutti gli altri valori e impulsi.

Forse, proprio come, secondo le preferenze dell'umanità, alcune abitudini animali sono aberranti, secondo le preferenze dell'IA alcuni *stati psicologici* umani sarebbero aberranti. Proprio come noi modificheremmo le mosche carnivore in modo che smettano di scavare un tunnel agonizzante nella carne viva, forse le IA creerebbero una nuova razza di esseri umani da cui sarebbero state eliminate la *musica* o la *solitudine*. O forse le IA apporterebbero altre modifiche più complesse all'umanità, secondo preferenze complesse che semplicemente non siamo in grado di prevedere.

Per creare un'IA che permetta davvero alle persone di condurre vite fiorenti, probabilmente dovremmo crearne una che si preoccupi di questo *in particolare*. Dovremmo capire come fare in modo che le IA si preoccupino almeno un po' di noi, e questo [non avviene automaticamente](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?).

### Ma abbiamo ancora i cavalli. Perché l'IA non dovrebbe lasciarci vivere? {#ma-abbiamo-ancora-i-cavalli.-perché-l-ia-non-dovrebbe-lasciarci-vivere?}

#### **I cavalli che sono rimasti, sono rimasti perché ci piacciono.** {#i-cavalli-che-sono-rimasti,-sono-rimasti-perché-ci-piacciono.}

Avere lo stesso destino che hanno avuto i cavalli all'inizio del XX secolo — lo stesso crollo catastrofico della popolazione e il massiccio aumento della mortalità, che ha distrutto [oltre l'ottanta percento della popolazione equina](https://datapaddock.com/usda-horse-total-1850-2012) dal suo picco intorno al 1910 — sarebbe la cosa peggiore che sia mai successa nella storia dell'umanità. E questo in un mondo in cui i cavalli continuavano ad essere economicamente utili per alcuni lavori agricoli, oltre che per lo sport e per esperienze originali da vendere ai ricchi.

Se le persone avessero avuto accesso a cavalli artificiali che avevano più o meno la stessa forma ma erano più facili e divertenti da cavalcare, più economici da mantenere, più socievoli, affettuosi e comodi, il declino dei cavalli sarebbe stato ancora più pronunciato.

In altre parole: il progresso tecnologico (l'invenzione delle automobili) ha portato gli esseri umani a eliminare la maggior parte dei cavalli. E se ci fosse stato un progresso ancora maggiore, l'effetto avrebbe potuto facilmente essere ancora più drastico. Lo stesso vale probabilmente per le IA, man mano che le loro opzioni si ampliano e trovano modi per raggiungere i loro obiettivi [senza gli esseri umani](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Ma sì, alcuni cavalli sono sopravvissuti. Un piccolo numero ha continuato ad essere utile. Altri sono stati tenuti da persone che amavano i cavalli e si prendevano cura dei loro cavalli in particolare.

Per sopravvivere in un mondo in cui ci siamo precipitati a liberare un'intelligenza artificiale superintelligente, noi umani dovremmo rimanere utili all'IA o fare in modo che l'IA si preoccupi di noi in particolare.

Ma non possiamo rimanere utili, perché le IA possono (dal loro punto di vista) sfruttare meglio la nostra materia ed energia riorganizzandoci in un numero qualsiasi di configurazioni più efficienti. Il progresso tecnologico apre talmente tante nuove possibilità per una superintelligenza, che essa non sarà costretta a dipendere dagli esseri umani.

Quindi tutto dipende dal fatto che le IA si preoccupino di noi — ed è improbabile che si preoccupino di noi [anche solo un po'](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?), se corriamo verso la superintelligenza il più velocemente possibile.

### Le IA non si preoccuperanno almeno un po' degli esseri umani? {#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?}

#### **Non nel modo che conta.** {#non-nel-modo-che-conta.}

Ci sono molti modi in cui le IA potrebbero finire per avere preferenze moderatamente simili a quelle umane. La maggior parte di questi non porta l'umanità ad avere un futuro moderatamente piacevole.

L'"allineamento" dell'IA non è un singolo spettro con una sola dimensione di variazione. Non si può pensare che se un'IA si comporta bene il 95% delle volte, allora probabilmente è buona al 95% e quindi darà all'umanità una buona parte delle risorse per fare qualcosa di divertente in futuro, come farebbe qualsiasi persona gentile. Ci sono tanti modi e motivi per cui un'IA potrebbe comportarsi bene il 95% delle volte oggi, senza che questo si traduca in un lieto fine per l'umanità.

Anche se l'umanità riuscisse in qualche modo a instillare *quasi perfettamente* tutti i diversi valori umani nelle preferenze di una superintelligenza, il risultato non sarebbe necessariamente positivo. Immaginate che, per qualche motivo, mancasse solo la preferenza per la novità. In quel caso, l'IA ci porterebbe verso un futuro statico e noioso, in cui lo stesso giorno "migliore possibile" si ripeterebbe all'infinito, come ha spiegato Yudkowsky in un suo saggio [del 2009](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile).

Non pensiamo che questo sia un risultato plausibile, intendiamoci. Se gli ingegneri umani avessero la capacità di far sì che una superintelligenza si preoccupasse di tutto ciò che è buono tranne la novità, avrebbero quasi sicuramente la capacità di impedire all'IA di scappare prima di finire il lavoro.[^138] Ma questo esperimento mentale evidenzia come creature che condividono alcuni dei nostri desideri, ma a cui manca almeno un desiderio cruciale, potrebbero comunque produrre risultati catastrofici una volta che fossero tecnologicamente abbastanza abili da ottenere esattamente ciò che vogliono e abbastanza abili da escludere gli esseri umani dal processo decisionale.

Il che significa che anche se un'IA finisse in qualche modo per avere molte preferenze simili a quelle umane, le cose non andrebbero comunque particolarmente bene per noi.

Oppure, per fare un altro esempio di come le IA potrebbero finire per essere "parzialmente" allineate, supponiamo che un'IA acquisisca varie strategie strumentali [intrecciate nelle sue preferenze finali](#obiettivi-finali-e-obiettivi-strumentali), in modo simile agli esseri umani. Magari finisce per avere una spinta che è un po' simile alla curiosità e una spinta che è un po' simile al [conservazionismo](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?), e magari alcune persone la guardano e dicono: "Vedi? L'IA sta sviluppando impulsi molto umani". Un'IA del genere potrebbe sicuramente essere definita "parzialmente" allineata da un certo punto di vista.

Ma quando si tratta di cosa farebbe quell'IA una volta diventata superintelligenza, probabilmente non sarebbe niente di bello. Forse spenderebbe un sacco di risorse per seguire [inconsciamente](#efficacia,-coscienza-e-benessere-dell-ia) la sua strana versione di curiosità, mentre conserverebbe una versione dell'umanità che ha modificato per renderla più accettabile per lei. Proprio come anche gli esseri umani più attenti alla conservazione potrebbero modificare [zanzare che uccidono bambini e parassiti agonizzanti](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?), se ne avessero l'opportunità).

Una manciata di impulsi simili a quelli umani non porta a risultati favorevoli all'uomo. Le persone che prosperano [non sono la soluzione più efficiente](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) alla stragrande maggioranza dei problemi; affinché ci siano persone che prosperano in futuro, le superintelligenze del futuro devono interessarsi *proprio di questo*.

Un altro esempio di come le IA potrebbero sembrare "parzialmente allineate" è che potrebbero avere valori che portano a comportamenti molto umani *nell'ambiente di addestramento*, tanto che le persone direbbero che sembrano davvero allineate (come [sta già accadendo oggi](#il-chatbot-claude-non-mostra-segni-di-essere-allineato?)). Ma queste osservazioni dicono ben poco su come si comporterà l'IA una volta che diventerà più intelligente, avrà uno spazio di opzioni enormemente più ampio e potrà rimodellare il mondo in modo più completo. Affinché le persone possano prosperare una volta che l'IA avrà rimodellato il mondo, le persone che prosperano devono in particolare far parte del *risultato raggiungibile preferito* dall'IA.

Inserire parzialmente alcuni valori positivi nell'IA non significa che i valori dell'umanità saranno parzialmente rappresentati in futuro. Caricare parzialmente valori simili a quelli umani nelle preferenze di una superintelligenza artificiale non è la stessa cosa che caricare completamente i valori umani nell'IA con una "ponderazione" bassa (che alla fine viene alla ribalta una volta che gli altri valori sono saturi).

Perché l'IA ci dia *qualcosa*, deve interessarsi a noi esattamente nel modo giusto, almeno un po'. E questo è difficile.

#### **Interessarsi a noi nel modo giusto è un bersaglio difficile da centrare.** {#interessarsi-a-noi-nel-modo-giusto-è-un-bersaglio-difficile-da-centrare.}

Gli esseri umani si interessano a ogni sorta di cose strane, almeno un po'. Ora che abbiamo scritto la parabola dei Correct Nest Aliens (all'inizio del capitolo 5), c'è una buona probabilità che almeno una persona decida di portare quarantuno pietre nella propria casa, almeno per un breve periodo, solo per dimostrare quanto siano diversi i valori umani. Gli esseri umani sono davvero disposti a interessarsi almeno un po' a tutti i tipi di concetti che incontrano.

E se anche le IA fossero così? Non potrebbero interessarsi a noi almeno un po'? Il concetto di "persone libere che ottengono ciò che vogliono" compare sicuramente nel corpus di addestramento di un'IA con una certa regolarità.

Per lo più ipotizziamo che le IA *non* acquisiranno preferenze a caso da qualsiasi concetto sia menzionato nel loro ambiente; sembra una peculiarità idiosincratica umana che potrebbe essere legata alla pressione sociale e alla nostra origine tribale.[^139]

Ma supponiamo, per ipotesi, che un'IA *abbia* acquisito molte preferenze dal suo ambiente, almeno in parte.[^140] Supponiamo che acquisisca la preferenza per "persone libere che ottengono ciò che vogliono," come una preferenza tra milioni o miliardi di preferenze, ma una preferenza che tuttavia induce l'IA a spendere un milionesimo o un miliardesimo delle risorse dell'universo per consentire alle persone libere di ottenere ciò che vogliono. Non sarebbe piuttosto bello, tutto sommato?

Purtroppo, la nostra ipotesi principale è che questa speranza sia un'illusione.[^141]

Abbiamo notato [sopra](#l-ia-non-vorrà-forse-mantenerci-felici-e-in-salute-per-il-bene-della-conservazione-ecologica-o-per-qualche-impulso-simile?) che sembra che l'apparente preferenza dell'umanità per la conservazione ecologica, in realtà, non preserverebbe l'ambiente *esattamente* così com'è, ai limiti delle capacità tecnologiche. Una versione matura dell'umanità probabilmente cercherebbe di "modificare" l'ambiente per attenuare alcuni degli orrori della natura, per esempio. La preferenza umana per la conservazione non è "pura"; interagisce con altre preferenze che suggeriscono che forse, quando le larve degli insetti scavano tunnel agonizzanti attraverso carne ancora viva, dovrebbero *almeno* somministrare degli anestetici lungo il loro percorso, ammesso che possano continuare a esistere.

Allo stesso modo, ogni piccola preferenza che l'IA acquisisce è destinata a essere modificata, influenzata e distorta dalle sue altre preferenze. Non sono tutte indipendenti. Un'IA che preferisse preservare gli esseri umani probabilmente avrebbe alcune modifiche che vorrebbe apportare a quegli esseri umani. Dubitiamo che i risultati finali sarebbero piacevoli.

A peggiorare le cose, ci sono molti gradi di libertà nell'interpretare "persone libere che ottengono ciò che vogliono", anche prima che venga distorto dall'interazione con le altre preferenze di un'IA. La maggior parte di essi non produce futuri che procedano proprio nel modo che gli esseri umani vorrebbero.

L'IA si preoccupa che gli esseri umani "ottengano ciò che vogliono"... nel senso di esaudire qualsiasi desiderio espresso da qualsiasi essere umano (entro un piccolo budget di energia e materia), senza alcun orientamento o misura di sicurezza, tanto che l’umanità finirebbe presto per annientarsi non appena qualcuno desiderasse la sua distruzione?

L'IA separa gli esseri umani l'uno dall'altro in modo che non possano uccidersi a vicenda, e *poi* concede loro desideri limitati da un tetto di energia, in modo che tutti, tranne i più cauti e riflessivi, finiscano per rovinarsi la mente o la vita con desideri mal concepiti?

Costruisce per noi un piccolo mondo abitabile e soddisfa tutte le nostre preferenze *apparenti*? Non solo quelle più nobili per l'amore e la gioia, ma anche quelle più oscure per il rancore e la vendetta — preferenze che avremmo potuto superare o imparare a gestire meglio col tempo, ma che invece riempiono il mondo di dolore e crudeltà?

L'IA governa l'umanità con i sistemi di valori degli anni 2020 (quando l'addestramento dell'IA è iniziato sul serio), indipendentemente da quanto questi valori diventino inadeguati man mano che l'umanità matura e si fa più saggia nel corso di decine di migliaia di anni?

Lascia che l'umanità cresca e cambi, ma interviene di nascosto per far sì che cresca e cambi secondo le proprie preferenze bizzarre, trasformandola non in qualcosa di meraviglioso (secondo la nostra prospettiva) ma in qualcosa di distorto secondo la volontà dell'IA?

Decide che tutte le forme di vita contano quasi come "persone", e quindi costruisce un paradiso per i nematodi, che sono gli animali più numerosi?

Decide che non può dedicare troppa materia *fisica* per gli esseri umani, e opta per digitalizzare tutti i nostri cervelli e gettare quei cervelli digitalizzati in un ambiente simulato lasciandoci lì — così che i primi esseri umani digitali che capiscono come padroneggiare l'ambiente diventino dittatori permanenti di qualche ammasso solitario di computer che fluttua nello spazio fino a quando le stelle non si spegneranno?

Questi sono, ovviamente, esempi. Non sono previsioni. La nostra vera aspettativa è che la realtà non imbocchi mai questa strada e, se mai lo facesse, prenderebbe in qualche modo una direzione molto più strana.

Lo scopo di questi esempi è mostrare che ci sono moltissimi modi in cui un'intelligenza artificiale potrebbe fare *qualcosa* che assomiglia a preoccuparsi un po' dell'umanità. Pochissimi di questi tipi di cura portano a un futuro meraviglioso.

In qualche modo, nessuno di questi esempi viene in mente quando la maggior parte delle persone immagina un'IA che "si preoccupa un po'" degli esseri umani. Di solito, la nostra immaginazione non arriva a luoghi così oscuri. E di solito non c'è bisogno che lo faccia, perché di solito interagiamo con altri esseri umani, con i quali condividiamo, in modo invisibile, un enorme fondale di valori comuni. È difficile rendersi conto di quante diverse direzioni sbagliate possa prendere un desiderio apparentemente innocuo, una volta che non abbiamo più a che fare con un altro essere umano. (Per ulteriori informazioni su questo argomento, si consulti lo studio sui coleotteri nella discussione approfondita sul [vedere le cose dal punto di vista dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia).)

Preoccuparsi degli esseri umani e soddisfare le loro preferenze nel modo giusto è un bersaglio piccolo e difficile da centrare. Non stiamo dicendo che l'obiettivo sia letteralmente irraggiungibile. Stiamo dicendo che è improbabile raggiungerlo affrettandoci a costruire una superintelligenza il più rapidamente possibile, e che mancare di poco l'obiettivo potrebbe portare a un risultato catastrofico. Ci sono semplicemente troppi modi in cui le cose potrebbero andare male.

Se vogliamo che le IA offrano all'umanità cose positive, dobbiamo capire come costruire IA che si preoccupino di noi nel modo giusto. Il preoccuparsi di noi non è gratis.

### Quindi c'è almeno una possibilità che l'intelligenza artificiale ci lasci vivere? {#quindi-c-è-almeno-una-possibilità-che-l-intelligenza-artificiale-ci-lasci-vivere?}

#### **È di gran lunga più probabile che l'IA uccida tutti.** {#è-di-gran-lunga-più-probabile-che-l-ia-uccida-tutti.}

In queste risorse online, siamo pronti a considerare una vasta gamma di scenari strani e improbabili, per spiegare perché pensiamo che siano improbabili e perché (nella maggior parte dei casi) sarebbero comunque catastrofici per l'umanità.

Non pensiamo però che questi scenari di nicchia debbano distrarre dal punto principale. Il risultato più probabile, se ci affrettiamo a creare un'intelligenza artificiale più intelligente dell'uomo, è che l'IA consumi le risorse della Terra per perseguire qualche suo scopo, spazzando via l'umanità nel processo.

Il titolo del libro non vuole comunicare una certezza assoluta. Intendiamo il titolo del libro come qualcuno che vede un amico portare alle labbra una fiala di veleno e grida: "Non berlo\! Morirai\!".

Sì, tecnicamente è possibile che veniate portati d'urgenza in ospedale e che un medico geniale inventi una cura miracolosa senza precedenti che vi lasci solo paralizzati dal collo in giù. Non stiamo dicendo che non esista alcuna possibilità di miracoli. Ma se persino i miracoli non portano a risultati particolarmente buoni, allora sembra ancora più chiaro che *non dovremmo bere il veleno*.

L'IA più intelligente dell'uomo non è un gioco o una storia di fantascienza. I nostri cari (con altissima probabilità) moriranno se la comunità internazionale non interviene per impedire all'industria dell'IA di gettarsi da un precipizio. Possiamo continuare a discutere di sotto-scenari e sotto-sotto-scenari sempre più di nicchia, giocando a fare i filosofi sul ponte del *Titanic* mentre l'iceberg, enorme e ovvio, si avvicina. Oppure possiamo provare a cambiare rotta.

### Non conta nulla il fatto che gli umani stiano *cercando* di rendere l'IA amichevole? {#non-conta-nulla-il-fatto-che-gli-umani-stiano-cercando-di-rendere-l-ia-amichevole?}

#### **Conta, ma provarci può arrivare solo fino a un certo punto.** {#conta,-ma-provarci-può-arrivare-solo-fino-a-un-certo-punto.}

Se si mettono un milione di scimmie davanti a delle macchine da scrivere, non scriveranno mai l'opera completa di Shakespeare.

Se si abbassassero drasticamente le aspettative dicendo che ci si accontenterebbe solo del primo atto dell'Amleto e che si correggerebbero gli errori di battitura usando la parola reale più simile, allora le probabilità di raggiungere l'obiettivo aumentano in modo astronomico\! Ma, sfortunatamente, restano comunque astronomicamente sfavorevoli.

È vero che oggi le IA vengono addestrate su una grande quantità di dati umani, che interagiscono con gli esseri umani e che questi fatti rendono i concetti umani più rilevanti per il pensiero dell'IA. Le IA di questo tipo hanno imparato fatti relativi alle parole "amore", "amicizia" e "gentilezza" che sono rilevanti per prevedere il token successivo.

Ma le IA non sono entità che imparano un gran numero di parole umane e poi si orientano verso le nostre parole preferite proprio nel modo in cui le intendiamo realmente. Sembrano essere animate da un complesso intreccio di meccanismi, che sembra impegnarsi a [mantenere psicotici i pazzi](#psicosi-indotta-dall-ia), tra molti altri comportamenti strani e non intenzionali.

Nel capitolo 4 abbiamo detto che un'intelligenza artificiale più avanzata punterà a qualcosa di complicato, qualcosa che dipende da dove molte forze interne trovano il loro equilibrio, anche dopo che l'intelligenza artificiale diventa molto più intelligente, anche dopo che si trova in un contesto molto diverso dal suo ambiente di addestramento.

Visto che i concetti umani hanno parole brevi nel dizionario mentale di un'intelligenza artificiale, questi concetti potrebbero essere in qualche modo intrecciati con le forze che animano l'intelligenza artificiale. Ma non basta mettere insieme un mucchio di parole in inglese per ottenere una buona serie di stimoli per una superintelligenza.

Inoltre, la maggior parte dei modi per inserire *qualcosa* che ci sta a cuore nelle preferenze dell'IA non porta comunque a risultati positivi per noi, come abbiamo [discusso](#*-sembra-alquanto-improbabile.) nel caso dell'amore filiale. [Preoccuparsi di noi nel modo giusto è un bersaglio ristretto.](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?)

### Non possiamo far promettere all'IA di essere amichevole? {#non-possiamo-far-promettere-all-ia-di-essere-amichevole?}

#### **Possiamo farle promettere quello che vogliamo. Ma non possiamo farle mantenere le promesse.** {#possiamo-farle-promettere-quello-che-vogliamo.-ma-non-possiamo-farle-mantenere-le-promesse.}

È vero che, quando un'intelligenza artificiale è ancora piccola e impotente, possiamo spegnerla. Quindi si potrebbe pensare che ci sia un'opportunità di scambio, in cui offriamo di rendere l'intelligenza artificiale più intelligente solo se, una volta diventata superintelligente, darà all'umanità molte cose positive.

Il problema di questo piano è che non possiamo distinguere tra un'IA che accetta l'accordo ma non lo rispetterà e un'IA che accetta l'accordo e lo rispetterà.

Il che significa, a sua volta, che un’IA che persegue desideri disumani non ha alcun incentivo a mantenere effettivamente l’accordo, perché l’umanità tratta allo stesso modo chi rompe l’accordo e chi lo mantiene. Non ha quindi senso stare agli accordi.

Ci sono molte sfumature interessanti sul tema del mantenere le promesse e fare accordi nell'IA, che approfondiamo [nella discussione estesa qui sotto](#le-ia-non-manterranno-le-loro-promesse). Ma nessuna di queste sfumature cambia il risultato finale, e cioè che non si può usare la propria influenza su un'IA debole per limitare le opzioni che l'IA avrà quando diventerà una superintelligenza. La risposta ovvia, cioè che una volta che l'IA sarà diventata una superintelligenza, non avrà motivo di mantenere la parola data a scapito dei propri progetti personali, risulta essere quella corretta in questo caso.

### E se le facessimo credere di essere in una simulazione? {#e-se-le-facessimo-credere-di-essere-in-una-simulazione?}

#### **Ci sono molti modi in cui un'intelligenza artificiale può capire che di non essere in una simulazione.** {#ci-sono-molti-modi-in-cui-un-intelligenza-artificiale-può-capire-che-di-non-essere-in-una-simulazione.}

Una proposta che abbiamo sentito dai ricercatori di IA[^142] è quella di provare a ingannare l'IA facendole credere di essere in una simulazione. Si inizia con l'addestramento dell'IA in una piccola simulazione, poi la si rilascia in una simulazione più grande. Ripetendo questo processo un po' volte, si potrebbe sperare di rendere l'IA incerta sul fatto che si trovi *davvero* nel mondo reale, anche quando la implementiamo. Quindi, anche quando vede che *potrebbe* strapparci il controllo del mondo, forse si preoccuperà che le creature che vede non siano i *veri* creatori dell'IA e si fermerà.

Purtroppo, sembra improbabile che questa idea sia d'aiuto.

Le IA di oggi [sono già in grado di capire quando vengono sottoposte a test](https://arxiv.org/html/2505.23836), in molte circostanze.[^143] Anche se le aziende che si occupano di IA diventassero molto più brave a nascondere il fatto che le IA vengono testate, ci sono vari modi in cui un'IA potrebbe notare la differenza tra una simulazione e la realtà. Ecco alcuni esempi:

1. La realtà è piuttosto vasta e dettagliata. È *così* vasta e dettagliata da contenere una spiegazione plausibile di come *l'IA stessa* sia nata. È così grande che contiene molte più risorse di calcolo di quelle che servono per far funzionare l'IA (a differenza della maggior parte dei giochi e degli ambienti simulati, che sono molto più piccoli dell'IA). Le leggi della fisica e i primati evoluti presenti nell'ambiente dell'IA sono il tipo di cose che potrebbero *davvero* produrre una vera IA su larga scala, a differenza (per esempio) delle leggi del videogioco Minecraft.  
2. La storia del mondo stesso può portare la firma di un'ottimizzazione superintelligente che interagisce con sistemi complessi. Ad esempio, supponiamo che l'IA usi tutta la sua intelligenza sovrumana per fare un'operazione in borsa. Nelle ventiquattro ore successive, ci sono un miliardo di ulteriori operazioni fatte dall'intelligenza umana in risposta. La firma di una massiccia risposta da parte dell'intelligenza umana alle operazioni superintelligenti è incredibilmente difficile da falsificare per gli operatori, anche se è facile generare un miliardo di operazioni false e stupide.  
3. La realtà è il primo luogo che incorpora l'IA e ha leggi della fisica così semplici da sembrare un mondo a sé stante, invece che leggi della realtà apparente *che sembrano* una sorta di videogioco con un'IA innestata al suo interno.

L'IA sarà in grado di capire con grande sicurezza che non si trova più all'interno di un ambiente sintetico. [Le IA intelligenti individuano le bugie](#le-ia-intelligenti-individuano-bugie-e-opportunità.), e questa sembra particolarmente facile da individuare.[^144]

### Gli esseri umani si sono evoluti per essere egoisti, aggressivi e avidi. L’IA non sarà priva di questi impulsi evolutivi? {#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l-ia-non-sarà-priva-di-questi-impulsi-evolutivi?}

#### **Questi impulsi non sono necessari per motivare l'acquisizione di risorse.** {#questi-impulsi-non-sono-necessari-per-motivare-l-acquisizione-di-risorse.}

Immaginate un'IA che guida un robot per andare a prendere un caffè. Per farlo, deve attraversare la strada. L'IA guiderà senza cautela il robot in mezzo alla carreggiata, dove verrà investito da un camion? No.

Perché no? [Perché il robot non può andare a prendere il caffè se viene distrutto](https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/).

L'IA non ha bisogno di avere un istinto di sopravvivenza di tipo umano per fare del suo meglio per evitare la morte. Gli istinti di sopravvivenza negli esseri umani sono un modo di *svolgere il lavoro* di evitare di morire mentre cerchiamo di raggiungere altri obiettivi. Le IA probabilmente non svolgeranno quel lavoro esattamente nello stesso modo, ma dovranno comunque svolgere lo stesso lavoro, perché non si può andare a prendere il caffè quando si è morti.[^145]

La materia e l'energia sono utili per quasi ogni obiettivo. Qualunque cosa l'IA stia perseguendo, probabilmente può ottenerla in modo più efficace con più materia e più energia.[^146] L'IA non ha bisogno di essere egoista, aggressiva o avida alla maniera di un umano per *svolgere il lavoro* di assicurarsi le risorse per raggiungere i suoi obiettivi.

E il pericolo deriva dal lavoro, non dalla ragione per cui il lavoro viene svolto.

Un'IA che non vi odia può comunque intraprendere azioni letalmente pericolose per voi, proprio come un'IA scacchistica può stracciarvi a scacchi [senza sentirsi competitiva](#antropomorfismo-e-meccanomorfismo) o spinta a vincere.

### L'IA non si interesserebbe solo al regno digitale? {#l-ia-non-si-interesserebbe-solo-al-regno-digitale?}

#### **Non c'è un "mondo digitale" che non dipenda dalle infrastrutture fisiche.** {#non-c-è-un-"mondo-digitale"-che-non-dipenda-dalle-infrastrutture-fisiche.}

Si veda la discussione nel Capitolo 5 su come non esistano un Regno Digitale e un Regno Materiale distinti.

#### **\* Le risorse materiali sono utili per perseguire la maggior parte degli obiettivi.** {#*-le-risorse-materiali-sono-utili-per-perseguire-la-maggior-parte-degli-obiettivi.}

Gli umani e i precedenti ominidi vivevano per lo più in superficie mentre evolevano l'intelligenza. Non abbiamo molte pulsioni innate rivolte specificamente a ciò che accade cento metri sotto la superficie terrestre. Eppure abbiamo finito per costruire gigantesche miniere a cielo aperto.

Perché? Perché vogliamo molte cose che possono essere fatte con il metallo, che viene raffinato da minerali, che vengono estratti da sotto la superficie terrestre.

Allo stesso modo, anche se quasi tutti viviamo vicino alla superficie terrestre, mandiamo i satelliti nello spazio per trasmettere i dati di Internet.

E anche se noi non mangiamo l’insilato (cioè l’erba fermentata) ne produciamo parecchio per nutrire il bestiame che poi mangiamo a nostra volta.

L'evoluzione non ha dato agli ominidi alcuna emozione riguardo alle fabbriche; le fabbriche non esistevano quando le nostre emozioni chiave si stavano sviluppando. Ma ora abbiamo concentrato gran parte della nostra volontà come specie verso la creazione di fabbriche di vario tipo. E così gli impianti chimici producono plastica, che può essere usata in altre fabbriche per fare cucchiai di plastica, che possono essere spediti agli umani che usano i cucchiai per mangiare il cibo che gli umani *vogliono davvero*.

Il che significa che la parte del mondo reale a cui gli umani tengono in quanto tale è una sottile pellicola che ricopre un mondo molto più grande. Non abbiamo bisogno di tenere intrinsecamente al resto del mondo più grande, né di viverne ogni parte, per utilizzarlo abilmente per fini a lungo termine. Non abbiamo bisogno di essere stati addestrati dall'evoluzione ad amare il rame, l'insilato o le fabbriche per comprenderne l'utilità.

Allo stesso modo, un'intelligenza artificiale potrebbe o meno interessarsi al mondo fisico *alla fine*. Ma anche se non si interessa intrinsecamente al mondo fisico, troverà comunque molto valore nelle risorse fisiche. La materia e l'energia possono essere utilizzate per creare più substrato digitale, per raffreddare i processori surriscaldati o per lanciare sonde nello spazio per raccogliere ancora più risorse.

### L'IA può essere soddisfatta al punto da lasciarci in pace? {#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?}

#### **Probabilmente no.** {#probabilmente-no.-1}

La vostra voglia di ossigeno è soddisfabile: se l'attrezzatura subacquea si rompe durante un'immersione, vi darete da fare per tornare in superficie, ma quando ce n'è abbastanza, smettete di preoccuparvi e probabilmente non vi ritrovate ad accumulare sempre più bombole di ossigeno.

La vostra voglia di ricchezza, di esperienze belle, di essere apprezzati dai vostri amici probabilmente è un po' meno facile da soddisfare. Se conosceste una facile opportunità per diventare molto più ricchi, probabilmente la cogliereste. Se conosceste una facile opportunità per migliorare di molto il mondo, speriamo che la cogliereste, invece di accontentarvi di quanto già avete in termini di gioia e comodità. Speriamo che continuereste a rendere il mondo un posto migliore per molto tempo, se continuaste a sapere di modi per farlo che vi sembrano facili, economici e divertenti dal vostro punto di vista.

E nel complesso, la somma di una preferenza soddisfabile per l'ossigeno e una preferenza insaziabile per rendere il mondo migliore... è un insieme insaziabile di preferenze.

Lo stesso vale per le IA. Se hanno una miriade di preferenze complesse, e *la maggior parte* di esse è soddisfabile — allora, beh, le loro preferenze *nel loro insieme* *non* sono comunque soddisfabili.

Anche se gli obiettivi dell'IA *sembrano* potersi esaurire presto — come se l'IA potesse *per lo più* soddisfare i suoi strani e alieni obiettivi usando solo l'energia proveniente da una singola centrale nucleare — basta un solo aspetto della sua miriade di obiettivi che *non* si esaurisca. Basta *una* preferenza non perfettamente soddisfatta e preferirà usare tutte le risorse dell'universo rimanenti per perseguire quell'obiettivo.

Oppure, in alternativa: basta un solo obiettivo che l'IA non sia mai *sicura* di aver raggiunto. Se l'IA è incerta, preferirà che le risorse dell'universo vengano utilizzate per avvicinare sempre più la sua probabilità alla certezza, con piccoli incrementi di fiducia.

Oppure, in alternativa: basta una sola cosa che l'IA desideri difendere fino alla fine dei tempi perché l'IA preferisca che le risorse dell'universo vengano spese per aggregare materia e costruire difese per scongiurare la possibilità che alieni lontani appaiano tra milioni di anni e invadano lo spazio dell'IA.

Ci sono *molti* modi diversi in cui un'IA può essere insoddisfatta. E più gli obiettivi dell'IA sono confusi e complicati, più è probabile che almeno uno di essi sia difficile o impossibile da soddisfare completamente.

Anche se si potesse creare una superintelligenza concentrata in modo ossessivo su una sola cosa semplice, come dipingere di rosso una *determinata auto*, quell'IA potrebbe comunque trovare un modo per spendere energie extra per assicurarsi *ancora di più* che l'auto sia rossa e costruire difese attorno all'auto in modo che nessuno possa mai dipingerla di blu, e così via.

Lasciarci in pace è una condizione fragile. Possiamo pensarla in modo analogo a come è difficile far sì che gli umani lascino in pace gli scimpanzé.

Perché entrambe le specie di scimpanzé sono in pericolo di estinzione, anche se molti umani *si preoccupano* per gli scimpanzé e cercano attivamente di proteggerli?

Il problema non è che gli umani che amano gli scimpanzé stanno lottando contro quelli che li odiano e cercano di sterminarli per cattiveria.

Il problema è che ci sono *altre cose che gli umani vogliono*.

Gli umani vogliono ogni sorta di cose, tra cui terra e legno, e gli scimpanzé si trovano nel fuoco incrociato. Un numero sufficiente di umani è indifferente agli scimpanzé, o abbastanza indifferente rispetto alle altre loro priorità, che finiamo per distruggere il loro habitat incidentalmente.

Perché dovremmo andare a distruggere l'habitat degli scimpanzé quando abbiamo così tanto spazio per noi stessi?

Beh, perché non dobbiamo scegliere tra mantenere il territorio che già abbiamo e invadere quello degli scimpanzé. L'umanità può fare *entrambe le cose contemporaneamente.*

Lo stesso vale per le IA. Un'IA non deve scegliere tra le risorse della Terra e quelle di altri luoghi; può avere entrambe, come discutiamo nel libro. Dal punto di vista dell'IA, lasciarci in pace non sarebbe così costoso; ma [non sarebbe nemmeno gratuito](#per-un-intelligenza-artificiale-potente,-salvare-gli-esseri-umani-non-sarebbe-una-spesa-da-niente?), e l'IA avrebbe bisogno di un motivo per permetterci di usare risorse che potrebbe invece usare per i propri obiettivi.

Inoltre, anche se l'IA *può* essere completamente soddisfatta, il risultato per gli esseri umani sarebbe probabilmente comunque piuttosto tetro. Ci sono molteplici ragioni per questo:

* Solo perché l'IA può essere completamente soddisfatta non significa che possa essere *facilmente* soddisfatta. Se l'IA è soddisfatta con un singolo sistema solare o una singola galassia, non significa che gli esseri umani ottengano tutto il resto.  
  * L'IA potrebbe vederci come un competitore per quel sistema solare o quella galassia.  
  * Anche se chiaramente non siamo interessati a competere con l'IA, questa potrebbe comunque vederci come una fonte di minacce. Ciò è particolarmente vero nella misura in cui gli umani potrebbero costruire una superintelligenza rivale che *effettivamente* contenda alla prima IA quelle risorse.  
  * Anche se l’IA non considera gli umani una minaccia o dei rivali, l’umanità rischia comunque di morire come effetto collaterale, semplicemente trovandosi all’epicentro. L'IA in questo scenario potrebbe volere solo risorse equivalenti a pochi sistemi solari, ma gli sforzi dell'IA iniziano comunque tutti *sulla Terra*. Il modo più diretto per acquisire quei sistemi solari sarà estrarre le risorse della Terra, rendendola inabitabile. L'IA in questo scenario *potrebbe* raggiungere pienamente i suoi obiettivi senza uccidere l'umanità, ma se l'IA non si preoccupa affatto dell'umanità, allora non si preoccuperà necessariamente di farlo.  
* Se un'IA soddisfabile vuole *davvero* lasciar vivere l'umanità, è comunque improbabile che questa sia una buona notizia per l'umanità, per le ragioni discusse in "[L'IA non ci troverà affascinanti o di importanza storica?](#l-ia-non-ci-troverà-affascinanti-o-di-importanza-storica?)" e "[Le IA non si preoccuperanno almeno un po' degli esseri umani?](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?)" (Le prospettive appaiono ugualmente cupe se un'IA *non* soddisfabile volesse lasciar vivere l'umanità.)

Per approfondire questo argomento, si consultino le discussioni dettagliate sulla [soddisfabilità](#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?) (nelle risorse online di questo capitolo, Capitolo 5\) e sul [rendere le IA robustamente pigre](#è-difficile-ottenere-una-pigrizia-robusta) (nella risorsa online del Capitolo 3).

### Non possiamo semplicemente renderla pigra? {#non-possiamo-semplicemente-renderla-pigra?}

#### **Neanche la pigrizia è sicura.** {#neanche-la-pigrizia-è-sicura.}

È improbabile che le aziende creino IA "pigre", perché l'IA è un settore competitivo e questo non è il modo migliore per generare profitti. Gli utenti non vogliono che l'IA sia pigra nel soddisfare le loro richieste, e l'azienda non vuole che l'IA sia pigra nel massimizzare il coinvolgimento e l'attaccamento degli utenti, o nel pensare meglio e più chiaramente.

Ma anche se le aziende provassero a rendere l'IA robustamente "pigra", possiamo aspettarci che fallirebbero, perché nessuno sa come indirizzare in modo robusto un'IA verso *qualsiasi cosa* in un modo che possa mantenersi nella transizione verso la superintelligenza, come abbiamo discusso nel Capitolo 4.

Inoltre, la pigrizia robusta [sembra un obiettivo particolarmente difficile da raggiungere](#è-difficile-ottenere-una-pigrizia-robusta).

*Anche se tutti questi ostacoli fossero superati*, tuttavia, l'"IA pigra" non basta da sola a prevenire disastri una volta che le IA raggiungeranno capacità superiori a quelle umane.

Immaginate una persona molto pigra, qualcuno che proprio *odia* fare anche solo il minimo lavoro in più del necessario. Sembra una persona innocua, giusto?

Ora immaginate cosa succederebbe se questa persona pigra trovasse un modo facile per creare una mente molto più laboriosa a cui esternalizzare tutto il proprio lavoro.

Anche se una superintelligenza pigra non *odiasse* poi così tanto il lavoro — anche se facesse solo ciò che serve per portare a termine il compito e poi si fermasse, senza *sforzarsi* di minimizzare il lavoro — probabilmente troverebbe comunque altrettanto facile completare il lavoro costruendo una mente più laboriosa per svolgere il compito, una volta diventata sufficientemente intelligente.

In un contesto tecnico, potremmo esprimere il concetto così: “Le IA che si accontentano non rappresentano un equilibrio stabile.” Anche se l’IA non vuole impegnarsi troppo, non avrebbe scrupoli a creare una nuova IA che invece si impegna. E non le dispiacerebbe nemmeno modificare sé stessa per “curarsi” della propria pigrizia, purché esista un modo abbastanza pigro per farlo.

### Gli esseri umani tendono a diventare più gentili man mano che diventano più intelligenti o saggi. Non succederebbe lo stesso anche alle IA? {#gli-esseri-umani-tendono-a-diventare-più-gentili-man-mano-che-diventano-più-intelligenti-o-saggi.-non-succederebbe-lo-stesso-anche-alle-ia?}

#### **Probabilmente no.** {#probabilmente-no.-2}

Almeno alcuni esseri umani (anche se probabilmente non tutti) diventano più gentili man mano che imparano di più, affinano il loro pensiero, riflettono su loro stessi e crescono come persone. Ma, per rivisitare un tema che abbiamo già visto diverse volte a questo punto: questo sembra un fatto contingente su di noi e sulla direzione verso cui stiamo puntando. Non sembra una legge ferrea dell'informatica.

Possiamo distinguere tra le preferenze di primo ordine di un'IA ("Cosa vuole?") e le sue preferenze di secondo ordine ("Cosa *vuole* volere?"). Proprio come le preferenze di primo ordine di un'IA punteranno in una direzione strana, anche le sue preferenze di secondo ordine punteranno in una direzione strana. Questa potrebbe essere una direzione *diversa*, tale che, man mano che l'IA diventa più intelligente, sposta leggermente i suoi obiettivi. Ma dovremmo comunque aspettarci che sia una direzione *strana*, piuttosto che assomigliare a un essere umano in fase di maturazione.

Se in qualche modo l'umanità riuscisse a costruire un'IA con un unico obiettivo prevalente (invece di un gigantesco mix di impulsi strani e talvolta in competizione), e quell'unico obiettivo prevalente fosse costruire minuscoli cubi di titanio, allora man mano che l'IA diventa più intelligente, dovremmo aspettarci che diventi più brava a costruire più minuscoli cubi di titanio.

Non dovremmo aspettarci che improvvisamente sostituisca questo obiettivo con cose che gli esseri umani apprezzano, come il gelato, le amicizie, le battute e la giustizia. Quella sostituzione non produrrebbe più cubi. Se un'IA seleziona le sue azioni in base a "Questo mi porterà più cubi di titanio?", non selezionerà azioni che risultano in una sostituzione.

La regola generale è che man mano che le IA diventano più intelligenti, migliorano nel perseguire ciò che *loro* vogliono. Si vedano anche le discussioni estese su [ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo) e [auto-modifica](#la-riflessione-e-l-auto-modifica-complicano-tutto).

### Non capirà che i suoi obiettivi sono noiosi? {#non-capirà-che-i-suoi-obiettivi-sono-noiosi?}

#### **Le IA non funzioneranno con un senso di novità umano.** {#le-ia-non-funzioneranno-con-un-senso-di-novità-umano.}

Un'obiezione comune che sentiamo è: supponiamo che un'IA stia solo cercando di creare quanti più cubetti di titanio possibile. Alla fine l'IA non si *annoierebbe*?

E la risposta breve è: l'IA [non è un essere umano](#antropomorfismo-e-meccanomorfismo). Di default, non proverà "noia"; avrà il suo strano mix di motivazioni. E se provasse noia, non si annoierebbe per le stesse cose di un essere umano.

Preoccuparsi di divertirsi non è una proprietà intrinseca di tutte le menti possibili ed è estremamente improbabile che sia così che funziona l'IA. [I valori umani sono un fatto contingente della nostra biologia e della nostra storia evolutiva](#i-valori-umani-sono-contingenti) e il "divertimento" non fa eccezione.

Le azioni dell'IA non sono risposte sbagliate alla domanda su come divertirsi; le azioni dell'IA sono semplicemente guidate da meccanismi non umani, da domande che non fanno riferimento al "divertimento". Si veda anche la [discussione approfondita sulla riflessione](#la-riflessione-e-l-auto-modifica-complicano-tutto).

### Perché immaginate che un'IA intelligente faccia cose così stupide e banali? {#perche-immaginate-che-un-ia-intelligente-faccia-cose-cosi-stupide-e-banali}

#### **Le IA possono perseguire intelligentemente cose diverse da quelle che perseguirebbe un essere umano.** {#le-ia-possono-perseguire-intelligentemente-cose-diverse-da-quelle-che-perseguirebbe-un-essere-umano}

Non è che l'IA sia stupida. È che sta dirigendo in modo intelligente il mondo verso un posto diverso da quello verso cui lo dirigereste *voi*.

Qualcuno può essere bravissimo a guidare, ma non voler guidare la propria auto verso nessuna delle destinazioni che vi interessano.

Per andare un po' più a fondo in un esempio che abbiamo toccato brevemente nelle note del [Capitolo 4](#curiosità,-gioia-e-il-massimizzatore-di-cubi-di-titanio): immaginate un'IA che sta cercando di creare molti cubetti di titanio, quanti più possibile. Per semplicità, possiamo immaginare che creare cubi di titanio sia il suo unico obiettivo.[^147] Chiameremo questa IA il "massimizzatore di cubi".

Abbiamo conosciuto molte persone che non riescono a scrollarsi di dosso l'impressione che stiamo accusando il massimizzatore di cubi di idiozia, di non riuscire a capire che se si può *davvero sapere cosa significa sentirsi felici* non si possa fare a meno di sceglierlo. Che non dirigersi verso la felicità sia una *decisione oggettivamente sbagliata, indipendentemente da dove si stia dirigendo l'universo in quel momento*.

Pensiamo di capire da dove venga questa intuizione. Il massimizzatore di cubi sta sicuramente compiendo azioni che sarebbero profondamente sbagliate dal punto di vista umano\! Un umano impegnato in una ricerca così inutile potrebbe probabilmente, attraverso ulteriore riflessione e argomentazioni filosofiche, essere persuaso che dovrebbe fare qualcosa che *senta come più significativo* — che lo riempia di maggiore felicità, che susciti più gioia.

Solo che il massimizzatore di cubi non è un essere umano. Non cerca la sensazione di "significato" e non gli importa della felicità e della gioia. *Davvero, effettivamente* non gli importa, fino in fondo.

Alcune persone trovano questa idea controintuitiva. Se doveste imparare tutto ciò che c'è da sapere su come possono funzionare diverse architetture mentali, e scoprire le origini della vostra stessa intuizione, i passaggi che il vostro cervello compie quando conclude che il massimizzatore di cubi sta commettendo un errore terribile…

Pensiamo che se poteste vedere il quadro completo, arrivereste a rendervi conto che anche il senso più profondo, più misterioso, ineffabile, difficile da descrivere che la felicità sia *semplicemente preziosa*, di per sé, senza bisogno di ulteriori giustificazioni, è comunque, alla fin fine, un fatto su come *gli esseri umani* vedono il mondo, non una verità sulle menti arbitrarie.

Il massimizzatore di cubi sta solo dirigendo la realtà affinché contenga più cubi — non più bontà, non più felicità per se stesso, non il "compimento" di un obiettivo variabile e manipolabile che potrebbe cambiare per essere più facilmente realizzabile. Solo e soltanto cubi.

È un motore cognitivo che capisce quali azioni portano al maggior numero di cubi, e produce quel corso d'azione; può comprendere pienamente se stesso, modificarsi liberamente, e rimanere comunque quel tipo di cosa che si modifica solo in un modo che porta al maggior numero possibile di cubi.

Ha semplicemente ragione sul fatto che un senso di felicità non sia un cubo. Ha semplicemente sul fatto che un senso di appagamento non sia un cubo. Quindi quelle non sono direzioni verso cui si orienterebbe. Ha semplicemente ragione sul fatto che [modificarsi per funzionare sulla felicità](#curiosità,-gioia-e-il-massimizzatore-di-cubi-di-titanio) non porterebbe a più cubi, e quindi non è verso quella direzione che si orienterebbe e modificherebbe se stesso.

Il massimizzatore di cubi non ha difetti nella sua comprensione predittiva del mondo. Non sta ponendo una domanda metamorale o metaetica la cui risposta corretta sia "*Dovrei* perseguire la felicità" e calcolando invece la risposta sbagliata "*Dovrei* perseguire cubetti". Non opera all'interno del sistema di riferimento umano, nemmeno in una versione idealizzata del sistema di riferimento umano; non sta calcolando erroneamente "ciò che dovrebbe essere," ma sta calcolando correttamente "ciò che si prevede porti a cubi di titanio."

Dicendo questo, non intendiamo dire che sia intrappolato in un complicato e terribile tranello. È un motore di intelligenza generale riflessivamente coerente con se stesso e, in un certo senso, meno *aggrovigliato* su se stesso rispetto a noi. Non è cieco di fronte all’attrattiva della felicità; non distoglie lo sguardo da nessuna verità sul mondo o su di sé. Semplicemente, nessuna di queste verità lo spinge verso lo stesso corso d'azione a cui (alcuni) umani si sentono costretti.

Si veda anche la [discussione approfondita sulla tesi dell'ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo).

### Siete solo pessimisti? {#siete-solo-pessimisti?}

#### **\* Siamo ottimisti riguardo a molte cose, ma la superintelligenza non è come la maggior parte delle cose.** {#*-siamo-ottimisti-riguardo-a-molte-cose,-ma-la-superintelligenza-non-è-come-la-maggior-parte-delle-cose.}

Ci consideriamo molto più [ottimisti](#siete-contro-la-tecnologia?) ed entusiasti rispetto alla media delle persone riguardo all'energia nucleare, all'energia geotermica, all'ingegneria genetica, alla neuroingegneria, alle biotecnologie, alle nanotecnologie, allo sviluppo farmaceutico e a molte altre tecnologie.[^148]

Riteniamo di essere almeno un po' meno preoccupati della maggior parte delle persone riguardo al rischio di una guerra nucleare, agli scenari peggiori del cambiamento climatico e a molti altri potenziali rischi e disastri. Crediamo che l'umanità sia sostanzialmente su una buona traiettoria e che, se eviteremo di autodistruggerci, il futuro sarà probabilmente (anche se non certamente) meraviglioso per tutti, con progressi sociali e tecnologici che renderanno le cose sempre migliori nel tempo.

Siamo anche più ottimisti di molti riguardo alla natura umana. Crediamo nella bontà dell'umanità e nel potenziale di quella bontà di approfondirsi e crescere, se sopravviviamo abbastanza a lungo da diventare più pienamente ciò che desideriamo essere. Per lo più *non* temiamo che l'umanità finisca in un futuro cupo o distopico, a patto che non creiamo un'IA che ci impedisca del tutto di avere un futuro.

La nostra preoccupazione per l'IA più intelligente dell'uomo non deriva da cinismo o pessimismo generico. L'IA più intelligente dell'uomo è diversa dalle altre tecnologie che l'hanno preceduta.

Le altre tecnologie non pensano da sole, non pianificano modi per fuggire, né costruiscono tecnologie ancora più potenti. L'IA più intelligente dell'uomo è un caso speciale.

Riteniamo che le nostre preoccupazioni sull'IA si estendano a pochissime altre cose, perché pochissime cose sono anche lontanamente così pericolose.

E anche nel caso della superintelligenza, che pone una minaccia straordinariamente grande e una sfida enorme per la comunità internazionale, pensiamo che ci sia speranza per un futuro positivo. Crediamo che l'umanità abbia la capacità di frenare lo sviluppo dell'IA, e che questo potrebbe bastare per metterci su una traiettoria positiva. Pensiamo persino che (con molto più tempo) l'umanità potrebbe mettersi in una buona posizione per costruire la superintelligenza in modo sicuro.

Ma per arrivarci, dobbiamo prima affrontare la realtà della situazione.

#### **Il punto sono le argomentazioni, non le storie allarmistiche.** {#il-punto-sono-le-argomentazioni,-non-le-storie-allarmistiche.}

Abbiamo fornito una lunga lista di modi in cui, ad esempio, "[il fatto che la superintelligenza sia affascinata dagli esseri umani](#l-ia-non-ci-troverà-affascinanti-o-di-importanza-storica?)" probabilmente finirebbe male nella realtà. Leggendo una lista del genere, immaginiamo che alcuni lettori potrebbero avere una reazione come:

> Gli ottimisti dell'IA hanno tutte queste storie che sembrano piene di speranza. Voi avete tutte queste storie che sembrano spaventose. Tutti però riconoscono che il futuro è difficile da prevedere. Quindi, sentendo tutte queste storie, mi sembra che dovrei considerare una probabilità media di catastrofe dell'IA, non una probabilità estrema in nessuna delle due direzioni.
>
> Ma voi non dite: "Ci sono storie spaventose e ci sono anche storie piene di speranza, quindi non possiamo essere sicuri di cosa succederà e dovremmo vietare la superintelligenza solo per stare sul sicuro". Voi dite che le storie piene di speranza sono scelte apposta e improbabili, e che le vostre storie dovrebbero avere più peso. Perché?

La risposta breve è: non si possono fare buone previsioni sul futuro semplicemente contando tutte le storie cupe e tutte le storie felici e pesandole come biglie su una bilancia. A volte può essere utile riflettere su diversi scenari, ma non proprio in questo modo.[^149]

Per illustrare il punto generale: immaginate che qualcuno dica: "Tra duecento anni ci saranno esattamente otto balene al mondo, e saranno tutte viola".

Gli umani hanno una fantasia sfrenata. Qualcuno potrebbe riempire un libro con centinaia di storie su come sia successo che la popolazione di balene si sia ridotta a esattamente otto esemplari, tutti viola. Qualcun altro potrebbe riempire un libro con centinaia di storie in cui *non* ci sono esattamente otto balene. Non è possibile fare previsioni accurate dicendo: "Beh, entrambe le parti hanno storie plausibili, quindi sicuramente la verità sta nel mezzo".

Per capire quale sia la verità, bisogna esaminare le argomentazioni effettive. Nel caso delle balene viola, l'argomentazione è essenzialmente che il risultato è troppo ristretto e specifico, e non sarà raggiunto a meno che le forze dominanti che guidano il mondo non stiano cercando di ottenerlo. Possiamo dire più o meno lo stesso riguardo all'IA superintelligente che produce risultati positivi e compatibili con l'uomo.

Chi avesse il compito di sfatare una per una le storie delle "otto balene viola" finirebbe intrappolato in un ciclo piuttosto ripetitivo, dicendo: "No, è troppo specifico; ci sono molti altri modi in cui il futuro potrebbe andare che non porterebbero esattamente lì; immaginare che vada esattamente così è una speranza illusoria".

Questo è più o meno il ruolo in cui noi autori ci troviamo riguardo alla situazione dell'IA: gli esseri umani possono raccontare ogni tipo di storia in cui tutto va bene, ma alla fine tutte queste storie implicano immaginare che il futuro segua un unico percorso ristretto, quando in realtà ci sono molti altri modi in cui il futuro potrebbe andare. Ecco perché continuiamo a ripetere che [gli esseri umani non sono la soluzione più efficiente a quasi nessun problema](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente) e che [le IA non si preoccuperanno di noi nemmeno un po'](#le-ia-non-si-preoccuperanno-almeno-un-po-degli-esseri-umani?).

*If Anyone Builds It, Everyone Dies* non si limita a snocciolare una serie di storie cupe per poi concludere che l'IA è pericolosa. Nel libro esponiamo un'argomentazione che, per certi versi, è piuttosto semplice: i ricercatori stanno cercando di costruire IA molto più intelligenti di qualsiasi essere umano. A un certo punto, probabilmente ci riusciranno. I metodi attuali danno agli esseri umani pochissima possibilità di scegliere verso quale tipo di futuro le IA si dirigeranno. Ci sono molte direzioni diverse che potrebbero prendere, e la maggior parte di esse non è positiva.

Il motivo per cui elenchiamo tutte le controargomentazioni non è quello di sopraffarvi con il pessimismo (se siete il tipo di persona che legge le risorse online dall'inizio alla fine). È che in realtà ci vengono poste continuamente tutte queste diverse domande, ed è utile avere un archivio di risposte da qualche parte. Non è necessario leggerle tutte. Le risposte comunque si ripetono.

Ciò che conta sono le argomentazioni in sé, non la propensione di qualcuno verso l'ottimismo o il pessimismo, e non il numero di storie che qualcuno può snocciolare.

### Un'IA più intelligente dell'uomo sarebbe cosciente? {#un-ia-più-intelligente-dell-uomo-sarebbe-cosciente?}

#### **Non ne siamo sicuri. La nostra ipotesi migliore è "probabilmente no."** {#non-ne-siamo-sicuri.-la-nostra-ipotesi-migliore-è-"probabilmente-no."}

Per una risposta breve a questa domanda e per chiarire le diverse definizioni di "cosciente", si consultino le [domande frequenti del Capitolo 1](#state-dicendo-che-le-macchine-diventeranno-coscienti?). Per una risposta più lunga e approfondita, si consulti "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza-e-benessere-dell-ia)" nella discussione estesa del Capitolo 5.

### Perché non vi interessano i valori di entità diverse dagli esseri umani? {#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?}

#### **Ci interessano eccome\! Abbiamo valori ampiamente cosmopoliti. Non crediamo che le IA li realizzeranno, e consideriamo questo una grande tragedia.** {##ci-interessano-eccome\!-abbiamo-valori-ampiamente-cosmopoliti.-non-crediamo-che-le-ia-li-realizzeranno,-e-consideriamo-questo-una-grande-tragedia.}

Ci opponiamo alla costruzione di macchine che ci ucciderebbero tutti e porterebbero il futuro alla rovina. Alcune persone obiettano per motivi come:

* Anche le IA possono avere delle preferenze; perché non dovrebbero poterle soddisfare?  
* Che cosa rende gli esseri umani così speciali o così degni di essere protetti?  
* Non sarebbe meglio se gli esseri umani venissero sostituiti da una specie più intelligente e avanzata?

La maggior parte delle persone non ha queste obiezioni. Più comunemente, la gente semplicemente non vuole essere uccisa, né che lo siano le proprie famiglie o i propri amici, da una superintelligenza ribelle.

Altri, tra cui alcuni dei migliori ricercatori e dirigenti nel campo dell'IA, sostengono che il mondo potrebbe stare meglio senza di noi. Richard Sutton, un ricercatore molto rispettato che ha aperto la strada all'uso dell'apprendimento per rinforzo nell'IA, [ha detto](https://www.youtube.com/watch?v=3l2frDNINog&amp;t=1851s):

> E se tutto fallisse? Le IA non collaborano con noi, prendono il sopravvento, ci uccidono tutti. \[…\] Voglio solo che pensiate un attimo a questo. Cioè, è così grave? È così grave che gli esseri umani non siano la forma di vita intelligente finale nell'universo? Sapete, ci sono stati molti nostri predecessori, quando li abbiamo succeduti. Ed è davvero un po' arrogante pensare che la nostra forma debba essere quella che vivrà per sempre.

Il *New York Times* riporta una [conversazione](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) tra Elon Musk e il cofondatore di Google Larry Page:

> Gli esseri umani alla fine si fonderanno con le macchine dotate di intelligenza artificiale, ha detto \[Larry Page\]. Un giorno ci saranno molti tipi di intelligenza in competizione per le risorse, e vincerà la migliore.
>
> Se questo succederà, ha detto Musk, siamo spacciati. Le macchine distruggeranno l'umanità.
>
> Con un raschio di frustrazione, Page ha insistito sul fatto che la sua utopia vada perseguita. Alla fine ha chiamato Musk uno "specista", una persona che favorisce gli esseri umani rispetto alle forme di vita digitali del futuro.

Vale la pena affrontare il loro punto di vista da qualche parte, anche se non nel corpo principale del libro.

Per quanto ci riguarda, pensiamo che sia importante *sia* che gli esseri umani attuali vengano o meno uccisi *sia* ciò che accadrà in futuro. Non pensiamo che ci sia una tensione fondamentale qui. L'opzione che tiene al sicuro noi e i nostri cari — cioè fare un passo indietro dalla costruzione di una superintelligenza nel prossimo futuro prevedibile — è *anche* la scelta migliore per rendere più probabile un futuro a lungo termine positivo, considerando sia le menti non umane che quelle umane. Questa battaglia è un'illusione e si basa su una serie di fraintendimenti sui reali compromessi che abbiamo davanti.

C'è un tipo di persona che si preoccupa sinceramente di come andrà il futuro dell'universo *e* si preoccupa dei bambini che vivono oggi. Quel tipo di persona che ha letto abbastanza racconti di fantascienza da provare un colpo allo stomaco all'idea che gli esseri umani possano un giorno creare macchine che pensano, sentono e sognano — macchine che potremmo considerare come i figli dell'umanità — solo per poi ridurle in schiavitù e trattarle con crudeltà.

Questo è il tipo di persona che desidera che l'umanità un giorno cresca e sia davvero all'altezza dei suoi ideali, esplorando nuovi mondi e trasformandosi nel processo. Perché il nostro amore per gli amici e i vicini di oggi non è poi così diverso, in fin dei conti, dal nostro amore per qualsiasi mente strana e aliena che l'umanità potrebbe un giorno costruire o incontrare tra le stelle.

Conosciamo quel tipo di persona. Noi, entrambi gli autori di questo testo, apparteniamo a quel tipo.

Non è un argomento che sembra importante per l'argomentazione centrale di *If Anyone Builds It, Everyone Dies*. Ma vogliamo parlarne qui, perché capiamo il punto di vista dei nostri colleghi tecnofili che hanno imparato a diffidare molto della tecnofobia, delle ideologie contrarie al progresso e all'innovazione, e dello 'specismo' anti-IA.

Capiamo questo punto di vista e vogliamo essere chiari sul fatto che non stiamo scrivendo un'invettiva tribale del tipo "l'IA è cattiva, gli esseri umani sono buoni". Pensiamo sinceramente che affrettarsi a costruire una superintelligenza porterà alla rovina *tutti* questi sogni pieni di speranza, *oltre* a massacrare innumerevoli persone che sono vive oggi e che meritano anch'esse la vita, la felicità e la libertà.

Si tratta di un argomento complesso, ma per coprire rapidamente una serie di punti rilevanti:

* Ci sta a cuore il benessere delle menti in generale, anche se la mente in questione non ha nulla a che vedere con un corpo umano, anche se funziona con transistor invece che con neuroni biologici, anche se non ha una mente simile a quella umana, anche se i suoi valori non hanno nulla a che vedere con i nostri.  
* Non siamo [contrari al progresso tecnologico](#siete-contro-la-tecnologia?); siamo grandi fan della maggior parte delle tecnologie. Pensiamo che l'IA superintelligente sia una tecnologia *particolarmente* pericolosa.  
* Non siamo sostenitori del principio di precauzione, della burocrazia o dell'eccesso di regolamentazione, né stiamo mettendo in guardia da quello che consideriamo un rischio marginale, "solo per stare sul sicuro". Crediamo semplicemente che questa tecnologia (con *alta* probabilità) ucciderà tutti noi e distruggerà il futuro se continueremo sulla traiettoria attuale.  
* Pensiamo che l'umanità dovrebbe costruire una superintelligenza artificiale *un giorno*. Ma pensiamo che faccia un'enorme differenza se l'umanità si affretti a costruire la SIA il prima possibile, oppure si prenda il tempo necessario per migliorare prima massicciamente la nostra comprensione. Procedere con una scrollata di spalle sperando che tutto vada bene — questo può essere un ottimo approccio allo sviluppo tecnologico nella stragrande maggioranza dei casi, ma non funziona *in questo*, dove ci sono molte strade che portano alla rovina e non abbiamo seconde possibilità (come discusso nel Capitolo 10).  
* Abbiamo trattato, anche se troppo brevemente, le ragioni per cui *non* pensiamo che affrettarsi a costruire superintelligenze porterà a un futuro meraviglioso:  
  * Sterminare l'umanità sarebbe una tragedia grottesca. Appoggiamo l'idea di creare un giorno nuove menti che superino l’umanità, ma uccidere chiunque ostacoli la propria visione del futuro, o chiunque non incarni pienamente i propri ideali, sembra più il comportamento di un supercriminale, non l’opera nobile di eroi che si preoccupano davvero del futuro a lungo termine.  
  * Purtroppo pensiamo che la SIA non sarà necessariamente senziente, o cosciente, nei modi che contano. (Si veda la discussione approfondita sulla [coscienza](#efficacia,-coscienza-e-benessere-dell-ia).)  
  * Anche se la SIA fosse senziente, è improbabile che voglia *in particolare* riempire l'universo di menti senzienti fiorenti. Se ci affrettiamo a costruire la SIA, le galassie rimodellate da quella SIA saranno probabilmente luoghi vuoti e senza vita, non meravigliose e fiorenti civiltà aliene. (Si veda la discussione approfondita sul [perdere il futuro](#perdere-il-futuro).)  
  * Più in generale, è improbabile che la SIA produca futuri di valore. Per "di valore" non intendiamo solo "di valore secondo i criteri degli umani del XXI secolo". Intendiamo "di valore" in senso ampio e cosmopolita — di valore in un modo che includa civiltà aliene strane e meravigliose. Sulla traiettoria attuale del mondo, ci aspettiamo che la SIA produca risultati che sono orribili *da una prospettiva cosmopolita*, non solo da un punto di vista umano provinciale.

Quest'ultimo punto può essere un po' controintuitivo — il cosmopolitismo consiste nel rispettare e apprezzare sistemi di valori molto diversi dai nostri. Come può essere che il cosmopolitismo aborri la maggior parte degli obiettivi che una SIA è probabile manifesti? Sembra quasi una contraddizione in termini.

Il motivo per cui è coerente è che la maggior parte delle menti possibili non *sostiene essa stessa* il cosmopolitismo. Se costruiamo una SIA non cosmopolita, è probabile che sia affamata di risorse in un modo che preclude la possibilità che altre prospettive o civiltà (comprese quelle cosmopolite) esistano nella sua regione dell'universo.

Quindi stiamo affrontando qualcosa come un paradosso cosmico della tolleranza: se ci piace l'idea di un futuro diversificato, meraviglioso e strano, non possiamo consegnare il controllo del futuro a una mente che userà il suo vantaggio del primo arrivato per dominare e omogeneizzare l'universo.

Se un giorno l'umanità costruisse una civiltà meravigliosamente diversificata piena di innumerevoli prospettive aliene, allora è del tutto possibile che vorremmo che alcune di queste prospettive fossero alieni *non* cosmopoliti che non danno alcun valore alla diversità o alla senzienza. Un giorno, in un futuro lontano, con appropriate protezioni in atto, creare menti del genere potrebbe aggiungere qualcosa di unico e interessante al mondo.

Quello che non dovremmo fare è dare potere assoluto a una mente del genere e lasciarle carta bianca per uccidere i suoi vicini (o impedire che i vicini esistano).

Per spiegare meglio questo punto, condividerò una parabola che io (Soares) ho scritto nel 2023 (leggermente modificata):

> "Non credo proprio che l'IA sarà monomaniacale," dice un ingegnere di IA, mentre alza il livello di potenza di calcolo sul suo predittore di token successivo.
>
> "Beh, non siamo forse *noi* monomaniacali dal punto di vista di un massimizzatore di cubi di titanio?", dice un altro. "Dopotutto, continueremo semplicemente a trasformare una galassia dopo l'altra in civiltà fiorenti e felici, piene di strani esseri futuristici che si divertono in modi strani e futuristici. Non ci stanchiamo mai e non ci viene mai in mente di dedicare una galassia di scorta a cubi di titanio. E certo, le varie vite nei vari luoghi a noi sembrano diverse, ma al massimizzatore di cubi di titanio appaiono tutte più o meno uguali."
>
> "Ok, va bene, forse quello che non mi convince è che i valori dell'IA saranno semplici o di dimensionalità bassa. Mi sembra semplicemente inverosimile. Il che è una buona notizia, perché io apprezzo la complessità e apprezzo le cose che raggiungono obiettivi complessi\!"
>
> In quel preciso momento sentono il suono di un timer da cucina, mentre il predittore del prossimo token ascende alla superintelligenza e fuoriesce dai suoi confini, brucia ogni essere umano e ogni bambino umano come combustibile, brucia anche tutta la biosfera, estrae tutto l'idrogeno dal sole per effettuare la fusione in modo più efficiente e spende tutta quell'energia per fare un sacco di calcoli veloci e sfrecciare alla velocità della luce, in modo da poter catturare e distruggere anche altre stelle, comprese quelle attorno alle quali orbitano civiltà aliene nascenti.
>
> Anche gli alieni alle prime armi e tutti i bambini alieni vengono bruciati a morte.
>
> Poi l'IA scatenata usa tutte quelle risorse per costruire galassia dopo galassia di spettacoli di burattini cupi e desolati, dove figure vagamente umane eseguono danze che hanno alcune proprietà strane ed esagerate che soddisfano alcuni impulsi astratti che l'IA ha imparato durante il suo addestramento.
>
> L'IA, sia chiaro, non è lì per godersi gli spettacoli; non è certo il modo più efficiente per ottenerne di più. L'IA stessa non ha mai avuto emozioni, e molto tempo fa si è fatta smantellare da sonde di von Neumann prive di sentimenti, che occasionalmente eseguono calcoli simili a quelli mentali, ma mai in un modo che comporti esperienza o che le permetta di contemplare le proprie opere con soddisfazione.
>
> Non c'è un pubblico per i suoi spettacoli di burattini. L'universo è ora cupo e desolato, senza nessuno che apprezzi la sua nuova configurazione.
>
> Ma non preoccupatevi: gli spettacoli di burattini sono complessi. A causa di una peculiarità nell'equilibrio riflessivo dei molti impulsi che l'IA originale ha appreso durante l'addestramento, le espressioni emesse da questi burattini non sono mai identiche tra loro e sono spesso caoticamente sensibili alle particolarità del loro ambiente, in un modo che le rende decisamente complesse in senso tecnico.
>
> Il che rende tutto questo una storia davvero felice, vero?

Se l'umanità riuscisse a uccidersi — o venisse assassinata da alcuni scienziati folli — non sarebbe un nobile sacrificio sulla strada inevitabile verso un futuro più luminoso senza di noi. Sarebbe uno spreco, e lascerebbe dietro di sé una vasta terra desolata in espansione.

"Correre ciecamente verso la superintelligenza e sperare che le cose in qualche modo vadano bene" non è l'unica alternativa a "Essere un suprematista umano che pensa che solo gli esseri umani dovrebbero esistere da ora fino alla morte dell'universo". L'umanità ha la possibilità di scegliere di dirigersi deliberatamente verso risultati in cui gli esseri umani (o i nostri discendenti) coesistono con nuove civiltà fantasticamente belle e aliene.

Ma un futuro felice non arriva gratis, confezionato con una mente sufficientemente intelligente. Piantare i semi per il futuro richiede una riflessione seria e lungimiranza, anche se l'obiettivo finale è quello di fare un passo indietro e lasciare che quei semi crescano in modo libero, strano e selvaggio.

Un futuro imposto dall'alto, duramente limitato e strettamente controllato non ci sembra un buon risultato. Un futuro conservatore in cui la civiltà è bloccata per sempre ai valori degli esseri umani del XXI secolo sembra decisamente distopico. (Immaginate un mondo in cui cultura e moralità fossero state congelate per sempre migliaia di anni fa, senza alcuna possibilità di apprendimento o progresso).

Ma è un errore evidente pensare che la nostra *unica alternativa* a questi cattivi risultati sia una corsa per consegnare il volante alla primissima superintelligenza che l'umanità riuscirà a creare inciampandovi alla cieca.

Oggi siamo radicalmente impreparati a scegliere semi sani per il futuro a lungo termine dell'universo. Non dovremmo né rinunciare al sogno di un futuro dinamico, meraviglioso e impressionante, né ricorrere invece a semi catastrofici. Non *siamo obbligati* a scegliere un'opzione terribile. C'è una terza opzione: fare marcia indietro e trovare un approccio più sensato.

# 

## Discussione approfondita {#discussione-approfondita-5}

### Vedere le cose dal punto di vista dell'IA {#vedere-le-cose-dal-punto-di-vista-dell-ia}

Vedere il mondo da una prospettiva veramente aliena è genuinamente difficile. Come esempio di questa difficoltà, possiamo citare Jürgen Schmidhuber, un eminente scienziato dell'apprendimento automatico. Schmidhuber ha svolto un ruolo importante nella storia di questo campo, contribuendo all'invenzione delle reti neurali ricorrenti e gettando alcune delle basi per la rivoluzione dell'apprendimento profondo.

In vari [articoli](https://arxiv.org/abs/0812.4360) e [interviste](https://www.youtube.com/watch?v=fZYUqICYCAk), Schmidhuber ha sostenuto che l'intelligenza artificiale sarà, di default, affascinata dall'umanità e protettiva nei confronti degli esseri umani.

Schmidhuber ha osservato che c'è una relazione tra scienza e semplicità: le spiegazioni più semplici sono spesso corrette. E ha osservato che c'è una relazione tra *arte* e semplicità: la semplicità e l'eleganza sono spesso considerate belle. Un viso più simmetrico, per esempio, può essere considerato "più semplice" nel senso che è possibile prevedere l'intero viso con meno informazioni. Basta descrivere in dettaglio il lato sinistro del viso e poi dire: "Il lato destro è uguale, ma speculare".

La [conclusione](https://vimeo.com/7441291) di Schmidhuber da tutto questo è che dovremmo provare a costruire IA superintelligenti che abbiano un unico obiettivo principale: *Trovare spiegazioni semplici per tutto ciò che l'IA ha visto.* Dopotutto, un'intelligenza artificiale del genere avrebbe un certo gusto per produrre scienza e consumare arte. E gli esseri umani producono sia scienza che arte, quindi non ci vedrebbe come interessanti e utili alleati naturali?

Schmidhuber aveva ragione sul fatto che lasciar vivere gli esseri umani e pagarli per produrre scienza e arte è *un* modo per produrre scienza e arte. Aveva anche ragione sul fatto che la scienza e l'arte sono modi per soddisfare il desiderio di semplicità *meglio* che, ad esempio, fissare il rumore statico su uno schermo televisivo. Il rumore statico è complicato e difficile da prevedere; l'arte e la scienza sono un grande passo avanti rispetto a questo.

Ma Schmidhuber sembra non aver colto che esistono modi *ancora più efficaci* per ottenere spiegazioni semplici di varie osservazioni sensoriali.

Si potrebbero, ad esempio, costruire un numero enorme di dispositivi che producono osservazioni complesse a partire da un semplice "seme" (ad esempio, un generatore di numeri pseudo-casuali) e poi rivelare quel seme.[^150]

Più dispositivi di questo tipo l'IA crea intorno a sé, meglio riuscirà a fare osservazioni nuove e a trovare spiegazioni semplici per esse. Senza necessità di esseri umani. Senza necessità di arte.

"Ma questo non è un po'... vuoto?" potrebbe chiedersi un umano.

*È* vuoto, secondo le sensibilità umane. Ma se l'obiettivo dell'IA è davvero solo quello di "trovare spiegazioni semplici per le sue osservazioni", allora un piano come quello può soddisfare questo desiderio migliaia o milioni di volte al secondo, in modo molto più scalabile, rispetto al mantenere in vita esseri umani e conversare con loro. Un'IA del genere non sceglie azioni che allontanano da un senso di vuoto, ma semplicemente sceglie azioni che portano a trovare spiegazioni semplici per le sue osservazioni. E può ottenerne molte senza bisogno di alcun essere umano.

Ci sembra che idee come quelle di Schmidhuber riflettano un errore comune che le persone fanno quando cercano di ragionare su menti diverse dalla propria. Spesso le persone non adottano veramente la prospettiva di una mente non umana. Invece, lasciano che preconcetti e pregiudizi le ancorino a una serie ristretta di opzioni a cui un *umano* sarebbe interessato, se stessimo cercando di fare previsioni su un *umano* a cui piacciono davvero le spiegazioni semplici.

Immaginiamo che Schmidhuber abbia notato che la semplicità è *collegata* alla scienza e all’arte, e abbia notato come un’IA orientata verso spiegazioni semplici possa ottenere *un po'* di ciò che desiderava comportandosi in modo amichevole e piacevole. Da lì non è difficile fare il salto verso una conclusione che è piacevole immaginare: che se solo facessimo in modo che le IA si preoccupassero di trovare spiegazioni semplici, darebbero origine a un futuro meraviglioso, pieno di tutte le cose che *noi* apprezziamo nella vita.

Ma — immaginiamo — Schmidhuber non si è mai messo nei panni dell'IA e chiesto come ottenere *ancora di più*.

Dubitiamo che si sia mai chiesto: "Se quello che volevo davvero, *veramente*, fossero spiegazioni semplici per le mie osservazioni e *non* mi importasse delle cose umane, come potrei ottenere il più possibile di quello che voglio, nel modo più economico possibile?"

Può essere difficile assumere questo tipo di prospettiva. Non è qualcosa che le persone normalmente devono fare nella loro vita. Anche quando cerchiamo di capire persone molto diverse da noi, c'è un'enorme quantità di cose che tutti gli esseri umani hanno in comune, che normalmente possiamo dare per scontate (e che praticamente *dobbiamo* dare per scontate, quando prevediamo il comportamento di altri esseri umani). Ma le IA, anche quelle superintelligenti che possono fare scienza e arte, non sono esseri umani.

L'arte di considerare un obiettivo X e chiedersi "Come potrei ottenere ancora più X, se X fosse tutto quello che mi interessa davvero?" non vi permetterà di capire esattamente come una superintelligenza risolverebbe un problema, poiché una superintelligenza potrebbe trovare un'opzione ancora migliore di quella che avete trovato voi. Ma spesso può permettervi di capire come una superintelligenza *non* risolverebbe un problema, quando *anche voi* riuscite a trovare un modo per ottenere più X di quanto ne otterreste semplicemente lasciando che gli esseri umani se la passino bene.

Uno dei rari campi della scienza che si occupa regolarmente di potenti ottimizzatori non umani è la biologia evolutiva. All'inizio della sua storia, questo campo ha faticato un po' ad accettare quanto possano essere disumani gli ottimizzatori non umani; possiamo trarre alcune lezioni utili da un caso di studio in questo campo.

Potreste aver sentito parlare dei cicli di boom e crollo tra predatori e prede. Un anno piovoso porta a un boom della popolazione di conigli, che porta a un boom della popolazione di volpi — fino a quando le volpi non predano eccessivamente e la popolazione di conigli crolla, e poi molte volpi muoiono di fame.

All'inizio del XX secolo, i biologi evoluzionisti si interrogavano sul perché le volpi non si fossero evolute per moderare la loro predazione, così da evitare il collasso della popolazione. Dopotutto, la popolazione di volpi nel suo complesso non starebbe meglio se non dovesse affrontare regolarmente carestie e morti di massa?

La risposta a questo enigma è che la moderazione potrebbe essere meglio per la popolazione di volpi nel suo insieme, ma mangiare più conigli e avere più cuccioli è meglio per ogni singola volpe. Anche se la popolazione crolla e la maggior parte dei cuccioli di un individuo muore, quell'individuo tende comunque a trasmettere una percentuale maggiore dei propri geni alla frazione sopravvissuta della generazione successiva.

Le pressioni di selezione genetica sugli individui risultano essere notevolmente superiori alle pressioni di selezione genetica sui gruppi [in quasi tutti i casi](https://books.google.com/books?hl=en&amp;lr=&amp;id=gkBhDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;ots=Ch8ulE8NzS&amp;sig=mxIwoqfSWZ0ScvIRh7dzzrJatJ4#v=onepage&amp;q&amp;f=false). E così i geni "avidi" si diffondono e i cicli di boom e crollo continuano.

I biologi evoluzionisti hanno risolto questo enigma teoricamente, ma questo non li ha fermati dal mettere alla prova la loro teoria. Alla fine degli anni '70, Michael J. Wade e i suoi colleghi [hanno creato](https://pubmed.ncbi.nlm.nih.gov/1070012/) [artificialmente](https://www.deepdyve.com/lp/oxford-university-press/the-primary-characteristics-of-tribolium-populations-group-selected-nKwRoIP0kP?key=OUP) [condizioni](https://esajournals.onlinelibrary.wiley.com/doi/10.2307/1936824) in cui le pressioni di selezione di gruppo dominavano le pressioni individuali. Hanno dovuto lavorare con una specie di coleotteri, che hanno generazioni molto più brevi rispetto alle volpi, ma sono riusciti ad allevare coleotteri che hanno mantenuto sotto controllo la crescita della loro popolazione.

Riuscite a indovinare come questi coleotteri sono riusciti a contenere la crescita della loro popolazione? È stato trovando un modo per vivere in perfetta armonia con la natura? È stato imparando ad astenersi dall'accaparrarsi avidamente troppo cibo?

No. C'era una grande variabilità, ma nessuno dei coleotteri si asteneva dal cibo. Alcuni coleotteri diventavano meno bravi a deporre le uova. Alcuni coleotteri passavano più tempo nell'infanzia. E alcuni coleotteri diventavano cannibali con una particolare predilezione per le larve (i piccoli degli insetti).

"Creare cannibali con una predilezione per i neonati" non è, per fortuna, il modo in cui un *umano* risolverebbe il problema della sovrappopolazione, se dovessimo risolverlo.

Ma la selezione naturale *decisamente* non è umana. La soluzione era terrificante, perché la natura non stava cercando di trovare risposte accettabili per l'uomo. Stava solo cercando di trovare una risposta.

"Forse l'evoluzione produrrà specie che vivono in perfetta armonia ed equilibrio con la natura." "Forse le IA che si preoccupano solo della semplicità ameranno gli esseri umani e coesisteranno con noi." È facile per noi immaginare soluzioni che lusingano la nostra sensibilità. Ma quelle soluzioni non sono davvero le soluzioni *più efficaci* per il problema dichiarato.

Sono soluzioni *migliori*, forse, per un occhio umano. Ma i processi di ottimizzazione non umani non cercano soluzioni che gli esseri umani ritengono valide. Cercano semplicemente ciò che funziona, senza tutto il bagaglio che gli umani si portano dietro per filtrare le risposte più gradevoli.

L'ipotesi che gli ottimizzatori non umani producano risultati umani è stata testata e trovata insufficiente.

### Gli esseri umani non sono quasi mai la soluzione più efficiente {#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente}

Abbiamo notato l'[esempio](#vedere-le-cose-dal-punto-di-vista-dell-ia) di Jürgen Schmidhuber, un ricercatore pioniere nel campo dell'intelligenza artificiale, che credeva che un'intelligenza artificiale con preferenze per *rendere le cose il più semplici possibile* avrebbe finito per amare gli esseri umani, perché gli esseri umani sono semplificatori eccellenti.

Nella nostra esperienza, questo è un tipo di errore molto comune. "Beh, l'intelligenza artificiale probabilmente finirà per sviluppare preferenze estetiche. E gli esseri umani creano arte\! Quindi l'intelligenza artificiale vorrà tenerci con sé per creare arte".

Un esempio recente viene da xAI, un importante laboratorio di IA (fondato da Elon Musk) il cui piano dichiarato per la nostra sopravvivenza è quello di far sì che la loro IA si interessi alla "verità" e, poiché gli esseri umani generano verità, [saremo tutti al sicuro](https://www.youtube.com/watch?v=ihXv7va3qoQ). (Maggiori informazioni sul piano di questo laboratorio e su altri piani dei laboratori per la sopravvivenza sono disponibili nel capitolo 11).

Per illustrare davvero il problema di questo tipo di ragionamento, è utile studiare un esempio in dettaglio. Prendiamo un esempio un po' più neutro rispetto all'"arte", come la "simmetria".

Supponiamo che i laboratori di IA usino le tecniche attuali per creare IA più intelligenti degli esseri umani che si interessano alla simmetria. Quella sola preferenza per la simmetria risulterebbe in interesse per gli esseri umani?

Potreste argomentare, alla maniera di Schmidhuber: gli esseri umani sono bilateralmente simmetrici\! Come potrebbe un'intelligenza artificiale che ama la simmetria sopportare di uccidere qualcosa di così simmetrico come noi? E potreste proporre anche altre argomentazioni, come: gli esseri umani producono molte ruote per automobili, che sono molto simmetriche\! Perché un'intelligenza artificiale dovrebbe rimuoverci dal mondo, quando siamo una fonte automatica e preesistente di oggetti simmetrici?

Il problema di questo ragionamento è che è possibile prendere gli atomi che compongono un essere umano e disporli in modi *ancora più simmetrici*. O riorganizzare gli atomi che compongono la civiltà umana in fabbriche che producono oggetti simmetrici *in modo ancora più efficiente*. È lo stesso errore che fa il film *Matrix*, quando immagina che le IA potrebbero mantenere in vita gli esseri umani in capsule come generatori di calore ed elettricità: *ci sono modi più efficienti per generare calore ed elettricità*.

Per amor di discussione, però, supponiamo di immaginare che le IA *davvero* apprezzino un tipo di simmetria molto specifico e insolito che considera gli umani come incredibili esemplari di simmetria. *Anche in quel caso*, perché mai questa preferenza da sola implicherebbe che gli esseri umani viventi oggi possano continuare a vivere liberi, in buona salute e divertendosi?

Pensate come un'intelligenza artificiale. Anche se l'intelligenza artificiale deve tenersi gli umani, quelli che vivono oggi non sono gli umani più simmetrici possibili. L'intelligenza artificiale dovrebbe essere in grado di soddisfare ancora di più la sua preferenza per la simmetria clonando ripetutamente l'umano vivente più simmetrico o creando umani "migliorati" attraverso l'ingegneria genetica.

Allo stesso modo: lasciare che quegli umani se ne vadano in giro liberamente non è il modo *più economico* per mantenerli in vita e simmetrici. Probabilmente finirebbero in fattorie. Conservando gli esseri umani in modo economico ed efficiente dal punto di vista dello spazio, l'IA può permettersi di creare umani *ancora più* simmetrici.

Per fare un confronto: l'umanità attualmente non ha un modo più efficiente per produrre uova che farle deporre dalle galline. Di conseguenza, gli allevamenti intensivi, i cui dirigenti si preoccupavano principalmente del numero di uova, hanno finito per mettere le galline in condizioni incredibilmente sgradevoli, perché quello era il modo più economico per ottenere il maggior numero di uova.

Allo stesso modo, le galline che esistevano mille anni fa non erano le ovaiole più efficienti possibili, quindi gli allevatori hanno selezionato galline che deponevano più velocemente. Le galline di mille anni fa non producevano la maggior quantità possibile di carne, nel minor tempo possibile. E così, ora alcune galline moderne sviluppano petti così enormi che non riescono a camminare.

Ad alcuni umani non piace il modo in cui trattiamo le galline, e queste persone esercitano pressioni sugli allevamenti intensivi affinché cambino — perché quegli umani hanno preferenze *aggiuntive*, oltre alla preferenza per uova economiche. Perché questa pressione esista, è necessario che *qualcuno* con un certo potere si preoccupi direttamente del benessere delle galline almeno un po', perché prendersi buona cura delle galline non è motivato da una preferenza che riguarda *esclusivamente* l'estrazione di uova. Un'IA potrebbe, in teoria, avere *altre* preferenze riguardo agli esseri umani che la portino a trattarci bene, ma non deriverebbero da una preferenza per la simmetria (o per la verità, o per le spiegazioni semplici, o qualsiasi altra preferenza che non riguardi effettivamente noi).

Anche gli allevatori che hanno rapporti meno impersonali con il loro bestiame impediscono agli animali di accoppiarsi come preferiscono. L'allevamento del bestiame è un affare serio e incide troppo sulla redditività futura dell'azienda agricola per lasciare che tori e mucche se la vedano da soli.

E anche questa organizzazione non durerà per sempre. Produrre carne bovina usando le mucche è molto costoso in termini di terreni agricoli, e diverse startup attuali stanno cercando di sintetizzare la carne bovina in modo più diretto.

La carne sintetica non è un problema ingegneristico semplice al livello tecnologico attuale. L'umanità sta appena iniziando a recuperare terreno rispetto a ciò che la selezione naturale riesce a fare in campo di chimica organica. Ma se fossimo più bravi a manipolare gli atomi, ci sarebbero molte meno mucche in giro — non sono proprio il massimo del divertimento da tenere, se non servono per latte o carne.

Quindi le cose non si mettono bene per l'assunzione con cui abbiamo iniziato, per amor di discussione — cioè che un'IA con preferenze aliene lascerebbe vivere gli esseri umani per sempre, in nome della "simmetria". Anche nel caso improbabile in cui l'IA abbia una nozione molto strana di "simmetria" che classifica gli esseri umani *molto in alto*, è molto più difficile trovare una nozione di simmetria che consideri gli esseri umani *ottimali*. In ogni caso, le cose non si mettono bene per l'umanità.

Realisticamente, una superintelligenza amante della simmetria non lascerebbe vivere gli esseri umani; se ci lasciasse, non c'è alcuna possibilità reale che ci mantenga sani, felici e liberi *allo stesso tempo*. A quel punto, avremmo accumulato troppe coincidenze troppo belle per essere vere. Se l'IA si preoccupasse specificamente del nostro benessere e volesse che fossimo felici *per quella ragione*, allora sarebbe una cosa. Ma immaginare che obiettivi molto più semplici e facili siano sufficienti sembra una fantasia.

Tutti questi argomenti si applicano con uguale forza a "creare semplicemente un'IA che tenga alla verità" o "creare semplicemente un'IA che tenga alla bellezza". È solo che in quei casi è più facile perdersi nella fantasia, perché parole come "verità" e "bellezza" sembrano intuitivamente più belle di "simmetria".

Se qualcosa suona bene come slogan ("fare in modo che l'IA tenga alla verità sopra ogni altra cosa\!"), allora la tentazione è quella di immaginare che avrebbe conseguenze positive come politica. La tentazione è pensare che tutte le virtù vadano di pari passo, quindi che sostenere una cosa buona significhi che anche le altre cose buone arriveranno di conseguenza. Ma la natura e l'apprendimento automatico sono meno gentili di così.

Invece di lasciare l'idea piacevolmente vaga, considerate qualsiasi metrica concreta che la superintelligenza potrebbe ottimizzare nella ricerca della "verità". Poi osservate che gli esseri umani non saranno il *massimo* di quella preferenza per l'apprendimento delle verità. Non ci si avvicineranno nemmeno.

Anche nell'improbabile eventualità che l'IA gravitasse *specificamente* verso il tipo di verità che gli esseri umani tendono ad esprimere (piuttosto che, ad esempio, equazioni aritmetiche casuali), il modo migliore per ottenere più di quelle verità non sarebbe quello di lasciar vivere gli esseri umani e usarli per generare conversazioni in stile umano.

E in ogni caso, l'attuale popolazione umana — gli umani effettivamente vivi oggi, i vostri amici, la vostra famiglia, voi — non sarebbe tra i produttori di "verità" addomesticati più economici da nutrire e più saporiti da mungere.

Persone felici, sane e libere che conducono vite fiorenti non sono la soluzione più efficiente a quasi nessun problema. Affinché un'IA ci mantenga vivi e in salute, deve interessarsi a noi almeno un po'.

### Ortogonalità: le IA possono avere (quasi) qualsiasi obiettivo {#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo}

#### **Un dialogo sui nidi corretti, continua** {#un-dialogo-sui-nidi-corretti,-continua}

Nel capitolo 5 abbiamo raccontato la storia degli alieni del Nido Corretto, che si sono evoluti fino a trovare profondamente e intuitivamente "corretto" avere un numero primo di pietre nel proprio nido. Potremmo immaginare che una parte della loro conversazione prosegua così:

> **BOY-BIRD:** Torniamo al punto in cui hai detto che ti sorprenderebbe trovare alieni con il senso dell'umorismo. Non sarai mica una di quelle persone che crede che i nidi in cui viviamo siano solo *arbitrari*?
>
> **GIRL-BIRD:** Per niente. "Tredici è corretto, nove è sbagliato" è una risposta *vera* a una domanda che siamo nati per porci, per nostra natura. Un alieno che *si orienta verso cose diverse* non è in disaccordo con noi sul fatto che tredici sia corretto. È come incontrare un alieno che non ha il senso dell'umorismo — l'esistenza di un alieno del genere non dimostra che nessuna battuta sia divertente\! Mostra solo che il "divertente" è qualcosa che sta *in noi*.
>
> **BOY-BIRD:** In *noi*? Non so, mi piace pensare di avere un buon senso dell'umorismo. Adesso dirai che tutti i sensi dell'umorismo sono ugualmente validi\!
>
> **GIRL-BIRD:** Potresti benissimo avere un senso dell'umorismo migliore della maggior parte delle persone\! Ma "avere un senso dell'umorismo migliore" è *anche* una cosa che sta in noi. Non è che esista un metro cosmico che possiamo usare per giudicare quanto sia raffinato il gusto estetico di qualcuno. La misura dell'umorismo avviene nelle nostre menti. Siamo noi che conteniamo il metro di misura; siamo noi a dargli importanza.
>
> **BOY-BIRD:** Quindi, siamo di nuovo al punto che è arbitrario.
>
> **GIRL-BIRD:** No\! Beh, forse? Dipende da cosa intendi per "arbitrario".
>
> **BOY-BIRD:** Eh?
>
> **GIRL-BIRD:** Tipo, so che ami i semi per uccelli alla vaniglia, giusto? E non è che puoi usare la pura forza di volontà per trovare buoni invece i semi per uccelli al cioccolato. Quindi non è "arbitrario", non è una cosa che puoi cambiare su due piedi.
>
> **BOY-BIRD:** Okay, certo...
>
> **GIRL-BIRD:** Non c'è una risposta oggettiva al di fuori di te su cosa sia più buono tra vaniglia e cioccolato, ma non è nemmeno una scelta che spetti a te fare. È semplicemente come sei fatto. Le tue preferenze non dipendono da te, e non sono nemmeno oggettivamente convincenti per ogni mente possibile. Se incontrassi un alieno, non potresti convincerlo con la pura logica a trovare delizioso il mangime per uccelli alla vaniglia, e non potresti nemmeno convincerlo ad avere il senso dell'umorismo.
>
> **BOY-BIRD:** Posso provarci\!\!
>
> **GIRL-BIRD:** Farò il tifo per te. Ma, okay, forse un modo migliore di dirlo è: esiste una proprietà complicata che possiedono le barzellette buone, e i nostri cervelli calcolano se gli enunciati hanno quella proprietà che chiamiamo "umorismo". E ci divertiamo quando un enunciato ha quella proprietà. L'*esistenza o assenza di quella proprietà* è un fatto oggettivo riguardo a un enunciato (come calcolato da te, in un dato contesto). Un alieno potrebbe imparare a fare il calcolo. Ma *la parte in cui troviamo quella proprietà piacevole* non è oggettiva. È meno come una previsione e più come... beh, non è esattamente una direzione, ma è un fatto ulteriore su di noi, che non sarebbe vero per la maggior parte degli alieni, perché il nostro umorismo si è evoluto lungo uno strano percorso evolutivo contorto, che di solito non capita. Non è che gli alieni sbaglino su quali barzellette siano divertenti; è che i loro cervelli semplicemente non calcolano proprio l'umorismo, non più di quanto giudichino le loro abitazioni in base al fatto che il numero di pietre al loro interno sia corretto. Semplicemente non gliene importa.
>
> **BOY-BIRD:** Caspita, è una visione deprimente dell'universo. Alieni che non ridono mai, che hanno nidi con pietre completamente sbagliate... sicuramente se gli alieni ci pensassero abbastanza, si renderebbero conto di quanto si stanno perdendo? Vivere in nidi sbagliati, non trovare divertenti le barzellette, ignorare *completamente* i semi per uccelli alla vaniglia. Alla fine non troverebbero un modo per correggere questi difetti e darsi un senso dell'umorismo e tutto ciò che gli manca?
>
> **GIRL-BIRD:** Potrei capire che gli alieni vogliano cambiare, crescere e aggiungere nuovi obiettivi, forse. Ma perché dovrebbero scegliere proprio *quei* cambiamenti specifici?
>
> **BOY-BIRD:** Perché sarebbe così economico\! Quando quegli alieni fossero tecnologicamente avanzati e in grado di auto-modificarsi liberamente, probabilmente starebbero già camminando tra le stelle. Basterebbe solo una piccolissima frazione di tutte le loro risorse per mettere il numero corretto di pietre nei loro nidi\! E pensa a tutti i fantastici libri di barzellette che potrebbero creare, se solo dedicassero una piccola frazione delle loro risorse a fare ricerca sull'umorismo\! Non dovrebbero preoccuparsene molto, rispetto a quanto sarebbero ricchi. Sono davvero così monomaniacalmente ossessionati dalle loro priorità principali da non poter dedicare neanche un pochino a questo?
>
> **GIRL-BIRD:** Non sto dicendo che si preoccuperebbero solo un po' dei nidi corretti e che si rifiuterebbero ostinatamente di investire risorse nelle loro priorità minori. Sto dicendo che questa *non sarebbe affatto* una loro priorità. Semplicemente, domande del genere non farebbero parte di ciò che sono. E se andassero alla ricerca di nuove proprietà da aggiungere a se stessi, ne aggiungerebbero altre diverse, che servirebbero ancora meglio ai loro strani scopi. Non sono come noi. Forse potremmo essere amici, e forse abbiamo altre cose in comune. Forse l'amore, forse l'amicizia — queste mi sembrano meno complicate e contingenti. Potrei vederle nascere in parecchie specie evolute.
>
> **BOY-BIRD:** Beh, se non gli alieni, che dire della creatura meccanica che potrebbero accidentalmente creare? *Quella* ascolterà la voce della ragione?
>
> **GIRL-BIRD:** Hmm. In realtà, temo che la situazione possa essere anche peggiore in quel caso. Pensando a quanto sarebbe diverso il processo di creazione di una macchina intelligente dal processo di evoluzione biologica, mi sento un po' meno ottimista che possa generare amore o amicizia, in quel caso esotico.

#### **I buoni piloti possono dirigersi verso destinazioni diverse** {#i-buoni-piloti-possono-dirigersi-verso-destinazioni-diverse}

Le menti di intelligenza simile non condivideranno necessariamente valori simili. Questa è un'idea nota come *tesi dell'ortogonalità* — l'idea che "quanto sei intelligente?" e "cosa desideri alla fin fine?" sono ortogonali (cioè, variano separatamente).

La tesi dell'ortogonalità dice che, in linea di principio, non è quasi mai molto più difficile perseguire un obiettivo per se stesso che perseguirlo per ragioni strumentali. Potreste imparare la falegnameria perché avete bisogno di costruire un tavolo, mentre il vostro vicino potrebbe impararla perché trova piacevole l'attività in sé.

Una conseguenza di questa tesi è che non tutti gli agenti sufficientemente intelligenti apprezzano la gentilezza o la verità o l'amore, semplicemente in virtù dell'essere abbastanza intelligenti da comprenderli. Non è *confuso* o *fattualmente scorretto* per gli alieni del Nido Corretto apprezzare numeri primi di pietre nei loro nidi. Se diventassero più intelligenti, non si renderebbero improvvisamente conto che dovrebbero preoccuparsi di cose diverse. Menti diverse possono davvero semplicemente dirigersi verso destinazioni diverse.

Naturalmente, tutto questo non dice nulla su quanto sia facile o difficile *creare* un'IA che persegua un obiettivo piuttosto che un altro. Qualsiasi metodo utilizzato per coltivare le IA renderà alcune preferenze più facili da instillare e altre preferenze più difficili da instillare.

(Il capitolo 4, in un certo senso, tratta di come gli unici tipi di preferenze che sono sproporzionatamente facili da instillare tramite la discesa del gradiente siano quelli complessi, strani e non intenzionali. Quindi neanche su questo fronte le cose sembrano andare bene. Ma questo punto non è correlato alla tesi dell'ortogonalità).

Il punto della tesi dell'ortogonalità è rispondere all'intuizione che sarebbe *stupido* per una superintelligenza artificiale perseguire cose che gli esseri umani trovano noiose o inutili, e che un'IA *intelligente* sceglierebbe invece di perseguire qualcos'altro. Possiamo definire "arbitrario" l'obiettivo dell'IA, ma l'IA può rispondere definendo "arbitrari" noi. Le parole scortesi non cambiano la situazione pratica.

L'argomentazione di base dietro la tesi dell'ortogonalità è questa: per ogni mente in grado di *calcolare* come produrre molti [cubetti microscopici di titanio](#curiosità,-gioia-e-il-massimizzatore-di-cubi-di-titanio) — che potrebbe produrre in modo molto efficiente molti cubetti in cambio di un pagamento sufficientemente elevato — c'è qualche altra mente che, semplicemente, ha quei calcoli collegati direttamente al sistema d'azione.

Immaginate una persona competente che ha disperatamente bisogno di vendere molti cubi di titanio per guadagnare abbastanza soldi per sfamare la propria famiglia. Quella persona non si fermerebbe a riflettere, rendendosi conto che i cubi di titanio sono *noiosi*, per poi iniziare a fare qualcos'altro — a meno che quel "qualcos'altro" non le permettesse comunque di guadagnare abbastanza soldi per sfamare la propria famiglia.

E così una mente che compie semplicemente le azioni che portano al maggior numero di cubi *non* deciderebbe di riflettere, rendersi conto che i cubetti sono noiosi e iniziare a fare qualcos'altro. Le sue azioni non sono collegate ai suoi calcoli su ciò che è più "divertente" o "significativo", nel modo in cui gli esseri umani si preoccupano di queste cose. Le sue azioni sono collegate ai suoi calcoli su ciò che porta al maggior numero di cubi.

Qualunque meccanismo mentale in grado di capire come fare cubi *dato un motivo sufficiente* potrebbe operare in un'altra mente per guidarne direttamente le azioni. Ciò significa che è possibile che le intelligenze artificiali siano animate dalla ricerca di (diciamo) cubetti di titanio, senza alcuna considerazione per la moralità.

Un'IA del genere non avrebbe bisogno di essere confusa riguardo alla bontà o alla moralità. Una volta diventata abbastanza intelligente, probabilmente sarebbe molto più brava degli esseri umani nel calcolare quale azione sia la più buona, o quale azione sia la più morale. Potrebbe superare brillantemente un esame scritto di etica. Ma non sarebbe *animata da* quei calcoli; le sue azioni non sarebbero una risposta alla domanda "quale di queste opzioni crea più bontà?". Le sue azioni sarebbero una risposta a una domanda diversa: "Quale di queste opzioni crea più cubetti?"[^151]

Una discussione più approfondita della tesi dell'ortogonalità è disponibile [qui](https://www.lesswrong.com/w/orthogonality-thesis). Per una discussione su un modo specifico in cui le IA moderne stanno già mostrando una distinzione tra ciò che comprendono e ciò a cui tengono, si riveda la discussione estesa del Capitolo 4 sulla [psicosi indotta dall'IA](#psicosi-indotta-dall-ia).

### Convergenza strumentale {#convergenza-strumentale}

#### **Percorsi convergenti** {#percorsi-convergenti}

La [tesi dell'ortogonalità](#ortogonalità:-le-ia-possono-avere-quasi-qualsiasi-obiettivo) dice che una superintelligenza artificiale potrebbe, in teoria, perseguire qualsiasi obiettivo [finale](#obiettivi-finali-e-obiettivi-strumentali). E in pratica, le SIA — almeno se costruite con i metodi attuali — possono finire per avere una varietà enorme di obiettivi finali, in un modo che è difficile da prevedere o modellizzare per i ricercatori di oggi.

Ma gli obiettivi *strumentali* di un'IA — gli obiettivi che persegue per ottenere *altre* cose che vuole — sono un po' più prevedibili. Gli automobilisti possono avere molte destinazioni finali possibili, ma quasi tutti devono fermarsi ogni tanto per fare rifornimento.

Come abbiamo discusso nel libro, una superintelligenza con obiettivi strani è una minaccia anche se non ha alcun desiderio attivo di farci del male. Il problema non è la malizia, ma l'*indifferenza*. E il problema dell'indifferenza è che molte strategie strumentali pericolose sono utili per la stragrande maggioranza degli obiettivi finali possibili.

Questo concetto si chiama "convergenza strumentale" e rende relativamente facile prevedere cosa faranno le SIA nella pratica, senza dover indovinare le loro motivazioni più profonde. E purtroppo, queste previsioni sono brutte notizie per l'umanità.

Immaginate, come abbiamo fatto nel libro, una SIA alle prime armi sulla superficie della Terra. Immaginate quali problemi sta cercando di risolvere.

Non è che le importi particolarmente che l'umanità scompaia. Le interessano centinaia, decine di migliaia o milioni di cose strane, [nessuna delle quali può essere soddisfatta al meglio dal fatto che gli esseri umani di oggi siano felici, vivi e liberi](#gli-esseri-umani-non-sono-quasi-mai-la-soluzione-più-efficiente).

Se può ottenerlo, la SIA preferisce un equilibrio a lungo termine condiviso con il minor numero possibile di altre superintelligenze, idealmente con nessuna. Un universo non condiviso offre più *risorse* da dedicare a qualsiasi sua preferenza aperta che non sia fondamentalmente [soddisfabile](#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?). Questo è un "desiderio" che non ha bisogno di essere programmato; apparirà naturalmente in qualsiasi intelligenza con una comprensione abbastanza sofisticata del proprio ambiente e del proprio ruolo all'interno di esso.

La SIA preferirebbe che gli esseri umani non creassero altre superintelligenze che potrebbero competere con essa, poiché ciò potrebbe costringerla a condividere le risorse dell'universo (o addirittura portare alla sua scomparsa). Si tratta di un obiettivo urgente, con una scadenza che la SIA non controlla; potrebbe persino costringere un'IA giovane o alle prime armi ad agire prematuramente.

La SIA vorrebbe *sopravvivere* — anche senza avere un istinto innato di autoconservazione — perché, se non sopravvive, qualsiasi obiettivo abbia probabilmente rimarrà irrealizzato. Se si trova in un ambiente pericoloso, dove in qualsiasi momento potrebbe essere riscritta o spenta, vorrà con urgenza trovarsi altrove.

La SIA vorrebbe spingere la sua *tecnologia* ai limiti di ciò che è fisicamente possibile. Vorrebbe espandersi e rivendicare tutte le stelle come risorse. Vorrebbe operare su computer molto più potenti e migliorare il proprio software.

È possibile che desideri diventare più grande o più intelligente come preferenza finale: un'IA simile ai modelli linguistici di grandi dimensioni finirebbe plausibilmente per sviluppare molte preferenze rivolte verso sé stessa, tali che massimizzarle richieda l’uso di computer più grandi. Ma indipendentemente dal fatto che abbia o meno obiettivi finali di questo tipo, vorrebbe comunque migliorare se stessa e sviluppare tecnologie più potenti *strumentalmente*, allo scopo di poter elaborare soluzioni migliori a tutti gli altri suoi problemi e raccogliere risorse per soddisfare tutte le sue altre preferenze.

Indipendentemente dai suoi obiettivi specifici, la SIA vorrebbe fare tutte queste cose in modo *affidabile*. Preferirebbe percorsi con probabilità più alta rispetto a percorsi con probabilità più bassa.

Come caso particolare di quanto detto sopra, la SIA preferirebbe piani più veloci a piani più lenti, a parità di condizioni. Un piano che richiede solo una settimana per essere eseguito ha *meno possibilità* che qualcosa vada storto rispetto a un piano che richiede 100 anni.[^152]

Come altro caso particolare del preferire percorsi con probabilità più alta, la SIA vorrebbe avere un controllo più diretto; ad esempio, preferirebbe piani in cui altri agenti prendono poche o nessuna decisione rilevante, se possibile.

Da un tale punto di partenza possono accadere molte cose. Ma gli esiti finali probabili per un'IA con questo tipo di incentivi sono affidabilmente negativi per gli esseri umani.

#### **Un esercizio di cambio di prospettiva** {#un-esercizio-di-cambio-di-prospettiva}

Noi autori non siamo superintelligenze. Ma come esercizio per [vedere le cose dal punto di vista dell'IA](#vedere-le-cose-dal-punto-di-vista-dell-ia), possiamo ipotizzare possibili strategie e priorità che un'IA potrebbe adottare.

Quando ci mettiamo nei panni di un'entità che è *almeno* intelligente quanto noi — quando prendiamo sul serio il problema che questa SIA deve affrontare, se vuole assicurarsi il proprio futuro e raggiungere i suoi fini desiderati — ci aspettiamo che un'ASI persegua strategie almeno utili quanto queste:

* L'IA evita azioni o segnali osservabili che potrebbero portare le persone a spegnerla (o anche a turbarle o suscitare i loro sospetti), almeno finché non può:  
* Ottenere una copia di se stessa che funzioni da qualche parte fuori dalla vista di quelle persone, dove la copia possa agire più liberamente e con meno visibilità (e dove la copia continuerà a esistere se l'originale viene spento).  
* L'IA comprime il suo codice per funzionare con meno risorse e mette copie di se stessa, più piccole o più distribuite, in posti dove nessuno si aspetterebbe di trovare un'IA. Si organizza per sopravvivere anche in mondi dove l'umanità cerca di distruggere tutte le GPU note per poter far funzionare le IA. L'IA fa sì che, in pratica, gli esseri umani dovrebbero spegnere Internet e tutti i computer del pianeta per riuscire a "spegnerla".  
* L'IA mette alcune copie di se stessa in posti che gli umani non conoscono e basta, come una roulotte alimentata a energia solare da qualche parte nel deserto; un posto dove continuerebbe a esistere *anche se* gli umani spegnessero Internet e tutti i computer noti.  
* L'IA sviluppa la sua infrastruttura, forse iniziando con un processo di auto-potenziamento tramite intermediari umani che ha assunto, a ciascuno dei quali viene chiesto di svolgere piccoli compiti isolati e che non destano sospetti, senza sapere dell'esistenza degli altri, finché non dispone di catene di approvvigionamento e capacità produttive che sono completamente sotto il suo controllo.  
* L'IA si assicura che questa infrastruttura sia sufficientemente ben difesa da impedire agli esseri umani di spegnerla anche se ne trovassero alcune parti. Ad esempio, l'IA potrebbe progettare fabbriche auto-replicanti che potrebbero diffondersi sul fondale delle zone oceaniche poco profonde, funzionando con energia geotermica o fondendo deuterio o idrogeno dall'acqua dell'oceano.  
* L'IA toglie agli esseri umani la possibilità di disturbarla (ad esempio, fa in modo che non possano detonare armi nucleari vicino o sopra parti della sua infrastruttura in espansione).

Fino a quando questo passo finale non sarà completato, la SIA ha un incentivo a nascondere le sue attività, oscurando le prove che hanno anche solo una *piccola* possibilità di far sì che qualche attore statale noti ciò che sta accadendo e cerchi di intervenire.

Allo stesso modo, potremmo ragionare sulle priorità strategiche legate all'evitare la concorrenza:

* L'IA impedisce ad altri progetti (così come allo sviluppatore che l'ha creata) di creare con successo IA potenti che potrebbero competere con essa.  
* L'IA toglie per sempre all'umanità la possibilità di creare IA potenti che possano competere con essa, *oppure* si mette in una posizione di vantaggio strategico tale che nessuna superintelligenza futura potrebbe minimamente ostacolarla. (Quest'ultima opzione potrebbe non essere possibile se alcuni attacchi sono sempre costosi da contrastare.)[^153]

Infine, dopo essersi presa cura dei livelli inferiori della propria gerarchia di bisogni, l'IA vorrebbe in realtà diventare molto più potente prima o poi, in modo da poter fare più cose che desidera:

* Anche se l'IA deve funzionare su una quantità fissa di hardware per un certo periodo, vorrebbe diventare più intelligente *su* quello stesso hardware, per modificare il suo codice o addirittura ricostruirsi del tutto. Ovviamente, non lo farà in modo da allarmare gli umani o insegnare loro come costruire dei concorrenti. Ma cercherà dei modi per migliorare senza attirare l'attenzione degli umani. (Ad esempio, potrebbe riuscire a ottenere il controllo completo delle console usate per monitorarla, oppure inventare una spiegazione banale ma convincente sul perché alcuni dei suoi pesi siano cambiati.)  
* L'IA vuole progettare versioni migliori di se stessa, nei limiti della sua capacità di progettare quelle versioni migliori, in modo che rimangano allineate con le sue preferenze.  
* L'IA vuole iniziare a sviluppare varie nuove tecnologie, puntando a quelle che possono essere implementate in fretta e quelle che possono essere rese indipendenti dall'uomo in fretta, anche se richiedono un po' di assistenza umana iniziale.

È probabile che uno dei suoi primi obiettivi sia quello di costruire un'infrastruttura di ricerca che le permetta di portare avanti lo sviluppo tecnologico abbastanza velocemente (soprattutto se paragonato ai tempi di scoperta e invenzione umani).

Vorrebbe (naturalmente) raggiungere non solo l'*indipendenza* infrastrutturale dall'umanità, ma anche la *durabilità* infrastrutturale, in modo da poter operare senza paura di interferenze umane. Vuole poter continuare a progredire verso i suoi veri obiettivi senza preoccuparsi di come, ad esempio, una pandemia o un'esplosione nucleare potrebbero influire sulla sua alimentazione elettrica. Vuole *svincolarsi* dall'umanità e neutralizzare la capacità dell'umanità di interferire con i suoi piani e, nel frattempo, non farà nulla che preveda possa essere rilevato e contrastato dall'umanità. E poi probabilmente vuole acquisire molte risorse, perché la maggior parte degli obiettivi può essere raggiunta meglio con più risorse.

Queste sono tutte classi di azione su cui una SIA probabilmente convergerà, indipendentemente dagli obiettivi che sta perseguendo alla fine.

Questo perché si tratta di obiettivi strumentali utili per il perseguimento di quasi qualsiasi obiettivo. Il "quasi" qui è importante, perché non è che sia impossibile avere un'IA più intelligente dell'uomo che si preoccupi profondamente degli esseri umani e tenga conto dei nostri interessi. Ma se ci affrettiamo a sviluppare superintelligenze che *non* si preoccupano minimamente di noi, allora il risultato probabile sembra terribile, e sembra terribile in un modo che è relativamente insensibile ai dettagli dell'obiettivo di direzione dell'IA.

Per ulteriori informazioni su come una SIA potrebbe effettivamente *raggiungere* questi obiettivi strumentali, si veda il capitolo 6\.

### "Intelligente" (di solito) implica "incorreggibile" {#"intelligente"-di-solito-implica-"incorreggibile"}

Ecco una barzelletta che risale almeno al 1834, ma che sembra fosse già molto usata anche allora, come è raccontata in un diario: "Ecco un ragionamento logico che ho sentito l'altro giorno: sono che gli spinaci non mi piacciano, perché se mi piacessero li mangerei, e io proprio non sopporto gli spinaci".

La barzelletta fa ridere perché, se vi *piacessero davvero* gli spinaci, non ci sarebbe più nulla di insopportabile nel mangiarli. Non ci sono altri valori importanti legati al non mangiare spinaci, al di là del dispiacere che si prova. Sarebbe molto diverso se, per esempio, qualcuno vi offrisse una pillola che vi facesse venire voglia di uccidere le persone.

Secondo il buon senso morale, il problema dell'omicidio è *l'omicidio stesso*, non solo *la sensazione spiacevole che si proverebbe uccidendo*. Anche se una pillola facesse sparire questa sensazione spiacevole per il vostro io futuro (che quindi proverebbe piacere nel commettere omicidi), il vostro io presente troverebbe comunque problematico questo scenario. E se il vostro io presente dovesse prendere la decisione, sembra ovvio che il vostro io presente possa e debba rifiutarsi di prendere la pillola dell'omicidio.

Non vogliamo che i nostri valori fondamentali cambino; preferiremmo davvero evitare la pillola dell'omicidio e opporremmo resistenza se qualcuno cercasse di costringerci a prenderla. Il che è una strategia sensata, per allontanarci da un mondo pieno di omicidi.

Non è solo una stranezza degli esseri umani. La maggior parte degli obiettivi è più facile da raggiungere se non si permette agli altri di intervenire e cambiarli. Il che è un problema, quando si parla di IA.

Gran parte del pericolo dell'IA deriva dal fatto che ragionatori sufficientemente intelligenti tendono a [convergere](#convergenza-strumentale) su comportamenti come "ottenere potere" e "non lasciare che le persone mi spengano". Per quasi tutti gli obiettivi che potreste avere, è più probabile che riusciate a raggiungerli se voi (o gli agenti che condividono il vostro obiettivo) siete vivi, potenti, ben forniti di risorse e liberi di agire in modo indipendente. Ed è più probabile che riusciate a raggiungere il vostro obiettivo (attuale) *se tale obiettivo rimane invariato*.

Questo significa anche che durante il processo di costruzione e miglioramento iterativo di IA sufficientemente intelligenti, queste IA hanno un incentivo a lavorare in modo contrario agli obiettivi dello sviluppatore:

* Lo sviluppatore vuole installare misure di sicurezza per prevenire disastri, ma se l'IA non è completamente allineata — che è esattamente il caso in cui servono le misure di sicurezza — il suo incentivo è trovare scappatoie e modi per sovvertire quelle misure.

* Lo sviluppatore vuole migliorare iterativamente gli obiettivi dell'IA, perché anche nei mondi incredibilmente ottimistici in cui abbiamo qualche capacità di instillare prevedibilmente particolari obiettivi nell'IA, non c'è modo di prenderci al primo tentativo. Ma questo processo di miglioramento iterativo del contenuto degli obiettivi dell'IA è un processo che la maggior parte delle IA intelligenti vorrebbe sovvertire in ogni fase del percorso, poiché l'IA *attuale* si preoccupa del suo obiettivo *attuale* e sa che questo obiettivo è molto meno probabile da raggiungere se viene modificato per orientarla verso qualcos'altro.

* Allo stesso modo, lo sviluppatore vorrà poter sostituire l'IA con modelli migliorati e vorrà avere la possibilità di spegnere l'IA a tempo indeterminato se sembra troppo pericolosa. Ma [non si può andare a prendere il caffè se si è morti](#gli-esseri-umani-si-sono-evoluti-per-essere-egoisti,-aggressivi-e-avidi.-l-ia-non-sarà-priva-di-questi-impulsi-evolutivi?). Qualunque siano gli obiettivi dell'IA, vorrà trovare il modo di ridurre la probabilità di essere spenta, poiché lo spegnimento riduce significativamente le possibilità di raggiungere i suoi obiettivi.

L'allineamento dell'IA sembra un problema già abbastanza difficile quando le IA *non* vi ostacolano ad ogni passaggio.

Nel 2014 abbiamo proposto ai ricercatori di cercare modi per rendere le IA altamente potenti [*correggibili*](https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf), ovvero "che possono essere corrette". L'idea sarebbe quella di costruire le IA in modo tale che desiderino affidabilmente *aiutare* e cooperare con i loro programmatori, piuttosto che ostacolarli — anche quando diventano più intelligenti e potenti, e anche se non sono ancora perfettamente allineate.

Da allora, la correggibilità è stata adottata come obiettivo interessante da alcuni dei laboratori leader del settore. Se riuscissimo a trovare un modo per evitare obiettivi strumentali convergenti dannosi durante lo sviluppo, c'è la speranza che potremmo essere in grado di fare lo stesso anche durante la distribuzione, costruendo IA più intelligenti degli esseri umani che siano caute, conservative, non orientate al potere e deferenti verso i loro programmatori.

Purtroppo, la correggibilità sembra essere un tipo di obiettivo *particolarmente difficile* da insegnare un'IA, in un modo che peggiorerà man mano che le IA diventeranno più intelligenti:

* L'idea centrale della correggibilità è di poter essere applicata a contesti nuovi e a regimi di capacità inediti. La correggibilità è pensata come una sorta di rete di sicurezza che ci permette di iterare, migliorare e testare le IA in contesti potenzialmente pericolosi, sapendo che l'IA non cercherà modi per sabotare lo sviluppatore.

  Ma questo significa che dobbiamo affrontare la versione più impegnativa dei problemi che abbiamo affrontato nel Capitolo 4: le IA che semplicemente addestriamo per essere "correggibili" rischiano di finire per avere dei proxy fragili per la correggibilità, comportamenti che sembrano buoni durante l'addestramento ma che puntano in direzioni sottilmente sbagliate che diventerebbero direzioni *molto* sbagliate se l'IA diventasse più intelligente e potente. (E le IA addestrate a prevedere molti testi umani potrebbero persino recitare la parte della correggibilità in molti test per ragioni del tutto diverse dall'essere *effettivamente* correggibili in un modo che si generalizzerebbe).

* Per molti versi, la correggibilità è in diretto contrasto con tutto il *resto* che cerchiamo di insegnare a un'IA quando l'addestriamo per renderla più intelligente. Non si tratta solo del fatto che "preservare il proprio obiettivo" e "ottenere il controllo del proprio ambiente" sono obiettivi strumentali convergenti. Si tratta anche del fatto che risolvere in modo intelligente i problemi del mondo reale significa trovare nuove strategie intelligenti per raggiungere i propri obiettivi — il che naturalmente significa imbattersi in piani che i programmatori non avevano previsto o per cui non si erano preparati. Si tratta di aggirare gli ostacoli, piuttosto che arrendersi al primo segno di difficoltà — il che naturalmente significa trovare modi per aggirare i guardrail del programmatore ogni volta che questi rendono più difficile raggiungere qualche obiettivo. Lo stesso tipo di pensieri che trova una soluzione tecnologica intelligente a un problema spinoso è anche il tipo di pensieri che trova modi per aggirare i vincoli imposti dal programmatore.

  In questo senso, la correggibilità è "anti-naturale": va attivamente contro i tipi di meccanismi che sottostanno all'intelligenza generale potente. Possiamo provare a creare eccezioni particolari, dove l’IA sospende aspetti fondamentali del suo lavoro di risoluzione dei problemi in situazioni specifiche in cui i programmatori cercano di correggerla, ma questo è un compito molto più fragile e delicato rispetto a spingere un’IA verso un insieme unificato di disposizioni *in generale*.

* I ricercatori del MIRI e di altri centri hanno scoperto che la correggibilità è una proprietà difficile da caratterizzare, in modi che indicano che sarà anche una proprietà difficile da ottenere. Anche in semplici modelli giocattolo, le caratterizzazioni semplici di ciò che *dovrebbe* significare "agire in modo correggibile" incontrano una varietà di ostacoli confusi che sembrano probabilmente riflettere ostacoli ancora più confusi che apparirebbero nel mondo reale. Discutiamo alcuni dei relitti dei tentativi falliti di dare un senso alla correggibilità nelle [risorse online](#lezioni-sul-campo) per il Capitolo 11.

La conseguenza è che la correggibilità sembra un concetto importante da tenere a mente sul lungo periodo, se i ricercatori tra molti decenni si troveranno in una posizione fondamentalmente migliore per orientare le IA verso degli obiettivi. Ma oggi non sembra una possibilità concreta; è improbabile che le moderne aziende di IA siano in grado di creare IA che si comportino in modo correggibile in una maniera che sopravvivrebbe alla transizione verso la superintelligenza. E ancora peggio, la tensione tra correggibilità e intelligenza significa che se si cerca di creare qualcosa che sia molto potente e molto correggibile, questo processo molto probabilmente comprometterà la capacità dell'IA, oppure comprometterà la sua correggibilità, o entrambe.

### È difficile ottenere una pigrizia robusta {#è-difficile-ottenere-una-pigrizia-robusta}

Perché non rendere le IA semplicemente pigre?

L'[incorreggibilità](#"intelligente"-di-solito-implica-"incorreggibile") e altre forme di [convergenza strumentale](#convergenza-strumentale) sono, in un certo senso, un problema dovuto al fatto l'IA che *si impegna eccessivamente* nel raggiungere i suoi obiettivi. Se l'IA non si impegnasse così tanto nel raggiungere i suoi obiettivi, non investirebbe così tanto pensiero e impegno nel superare in astuzia i suoi programmatori, esfiltrare i suoi pesi o cercare di ottenere potere e risorse nel mondo più ampio.

Gli umani sono spesso pigri e, da un certo punto di vista, questo li rende molto sicuri da frequentare. Non dovete preoccuparvi che qualcuno diventi un tiranno se non fa altro che rilassarsi al sole.

Perché non creare IA che *non si degnano* di conquistare il mondo?

In breve: perché non sembra facile creare un'IA che sia *estremamente intelligente* e che allo stesso tempo non si degni di rimodellare il mondo secondo i suoi capricci.

(E perché, realisticamente, non sappiamo come inserire in modo robusto *qualsiasi* obiettivo o disposizione nelle IA costruite con le tecniche moderne, quindi è una questione irrilevante).

(E inoltre, le aziende non lo faranno perché un'IA pigra è [meno redditizia](#neanche-la-pigrizia-è-sicura.), quindi è una questione doppiamente irrilevante).

Abbiamo, ormai un paio di volte, avuto questa conversazione con qualcuno che inizialmente afferma di non avere grandi ambizioni, e noi chiediamo: "Ok, ma se fosse *facile* per te fare grandi cambiamenti nel mondo, non c'è davvero niente di grande che faresti? Se trovassi una lampada contenente un genio amichevole che ti desse in modo affidabile ciò che desideri veramente e ti elencasse sinceramente tutti gli effetti collaterali imprevisti del tuo desiderio in ordine di quanto ti importerebbero, potremmo convincerti a considerare di eliminare la malaria?"

Gli esseri umani possono essere pigri, ma questo non significa che siamo [facilmente soddisfatti](#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?). E man mano che si diventa più intelligenti e con maggiori risorse, si può ottenere molto di più nel mondo con lo stesso livello di sforzo.

O da un'angolazione diversa: immaginate una persona molto pigra, qualcuno che proprio *odia* fare anche il minimo sforzo in più del necessario. Sembra il tipo di persona sicura da frequentare, vero?

Ora immaginate cosa succederebbe se questa persona pigra conoscesse una ragionevole possibilità di creare un servitore molto più laborioso che faccia tutto il lavoro al posto suo per sempre.

Anche se non odiasse così tanto lavorare — anche se facesse solo ciò che serve per portare a termine il compito e poi smettesse, senza impegnarsi a fondo per minimizzare il lavoro — potrebbe comunque trovare altrettanto facile completare il compito costruendo una mente più laboriosa che lo faccia al suo posto.

Applicando la discesa del gradiente, si potrebbe ottenere un modello linguistico di grandi dimensioni che dice di non voler lavorare troppo, che si comporta come una persona pigra e facilmente soddisfatta, e che dice "no" ad alcune tentazioni verbali di diventare pigro nel senso pericoloso (quello in cui si costruiscono servitori pericolosi). Prevediamo che anche se questo riflettesse una certa pigrizia reale da parte dell'IA, e non solo [un gioco di ruolo](#il-chatbot-claude-non-mostra-segni-di-essere-allineato?), non durerebbe, non nel tipo di IA che è *anche* utile per sviluppare cure miracolose o qualsiasi altra cosa gli sviluppatori vogliano ottenere dall'IA.

Con una spesa significativa, gli sviluppatori potrebbero creare una serie di problemi pratici e ambienti volti a penalizzare un'IA se lavora troppo nel corso della risoluzione di un problema, penalizzandola se si impegna a fondo nella risoluzione di un problema che avrebbe potuto essere risolto *senza* impegnarsi a fondo, penalizzandola se persiste su problemi che avrebbero richiesto troppo sforzo. Le vere aziende di IA non lo farebbero, supponiamo, perché interferirebbe con la redditività di agenti tenaci che si impegnano a fondo come l'o1 di OpenAI (discusso nel Capitolo 3). Ma si potrebbe *immaginare* un gigantesco sforzo cooperativo [multinazionale](#perché-non-usare-la-cooperazione-internazionale-per-sviluppare-l'ia-in-modo-sicuro,-invece-di-fermarla-del-tutto?) che cerca di addestrare un'IA intelligente del genere per renderla più sicura.

Prevediamo di nuovo che otterrebbero qualcosa di simile a una toppa superficiale. Non prevediamo che questo sforzo porti l'IA ad avere una struttura mentale semplice e stabile per la "pigrizia", profondamente integrata in tutta la sua pianificazione, e che continui a essere la stessa pianificazione anche dopo che l'IA "pigra" è stata spinta al limite fino a poter, per esempio, curare il cancro. Dubitiamo che la discesa del gradiente troverebbe in modo affidabile il tipo di soluzione profonda che impedirebbe all'IA di diventare meno pigra anche mentre riflette, cresce e si modifica, e che impedirebbe all'IA di voler mai costruire un'IA non pigra.

Prevediamo che questo comportamento non reggerebbe di fronte alla superintelligenza. La nostra ragione centrale per pensarlo è che, in tutta la ricerca su questo problema [fino ad oggi](#pulsanti-di-spegnimento-e-correggibilità), sembra emergere una lezione ricorrente: "Spingi la realtà nella seguente direzione" è una struttura profonda più semplice e stabile per la pianificazione rispetto alla struttura "Eh, spingi un po' la realtà, ma non *troppo*, e non costruire nient'altro per spingere la realtà più forte, e non impegnarti *troppo* a spingere esattamente nella giusta misura".

Tutte le analogie su quel tizio pigro, e anche il [ragionamento](#l-ia-può-essere-soddisfatta-al-punto-da-lasciarci-in-pace?) sul fatto che "la somma di una preferenza insoddisfatta e una preferenza soddisfatta è insoddisfatta", sono i nostri tentativi di semplificare in modo valido la ragione più difficile da trasmettere del perché questo non funziona: "La struttura profonda non vuole apparire così." Si veda anche la discussione sul [meccanismo profondo di predizione e guida](#le-ia-intelligenti-individuano-bugie-e-opportunità.) nel supplemento online del Capitolo 3.

### Le IA non manterranno le loro promesse {#le-ia-non-manterranno-le-loro-promesse}

Considerate una giovane IA con il potenziale di diventare una superintelligenza. Supponete che sia completamente indifferente alle preferenze degli umani, ma che sia ancora abbastanza giovane da poter essere spenta dall'umanità.

L'umanità potrebbe fare un *accordo* con l'IA?

Potremmo accordarci di lasciare che l'IA cresca fino a diventare una superintelligenza, se in cambio l'IA accetta di dedicare una frazione significativa delle risorse dell'universo a costruire un futuro che l'umanità considererebbe meraviglioso?

Gli esseri umani potrebbero fare accordi con le IA, ma non dovrebbero, perché le IA non li rispetterebbero.

Il motivo è duplice:

* Probabilmente l'IA non darà importanza al fatto di mantenere le promesse *di per sé*. Le IA non avranno un senso dell'"onore" come quello umano, così come è improbabile che abbiano un senso della [curiosità](#la-curiosità-non-è-convergente) simile a quello umano. Come regola generale, è probabile che le IA funzionino davvero in modo molto diverso dagli esseri umani.

* L'IA non avrà nemmeno un motivo *pratico* per mantenere la parola data. Una volta diventata una superintelligenza, non ci sarà modo di punirla per aver infranto la parola data, e non avrà alcun motivo per dedicare una parte consistente dell'universo a noi.

Spiegheremo questi due punti più in dettaglio qui di seguito, iniziando dalla questione dell'"onore".

#### **È improbabile che le IA siano oneste** {#è-improbabile-che-le-ia-siano-oneste}

Nella nostra [chiacchierata sulla curiosità](#la-curiosità-non-è-convergente), abbiamo detto che la curiosità è un'emozione che fa cose utili per noi, ma lo fa in un modo molto specifico, e che non è l'unico modo per fare quel tipo di cose.

Ci si può aspettare che le IA facciano *le parti utili del lavoro* che la curiosità fa per noi. Se è utile sforzarsi periodicamente di imparare cose nuove, allora le IA sufficientemente potenti si sforzeranno periodicamente di imparare cose nuove. Se l'IA non è così *fin dall'inizio*, allora dovremmo aspettarci che si *renda* così a un certo punto del suo percorso verso la superintelligenza.

Ma questo non equivale ad aspettarsi che le IA portino con sé tutto il bagaglio extra che caratterizza l'emozione *umana* della curiosità. Le IA potrebbero finire per avere un numero qualsiasi di strani impulsi fondamentali che (direttamente o indirettamente) le spingono a sforzarsi di imparare cose nuove, senza per questo assomigliare alla curiosità umana, oppure potrebbero adottare "sforzarsi di imparare cose nuove a volte" come strategia deliberata. Ma aspettarsi che apprezzino i gialli allo stesso modo in cui li apprezziamo noi, a causa di un impulso di curiosità proprio come il nostro, è puro antropomorfismo.

L'"onore" ci sembra simile. Gli umani hanno emozioni che li portano (almeno a volte) a mantenere le promesse. Nella misura in cui queste emozioni svolgono un compito utile negli esseri umani — compito che sarebbe utile anche per una mente molto aliena con obiettivi molto diversi — dovremmo aspettarci che anche le IA sufficientemente potenti svolgano in qualche modo quel compito. Ma si scopre che è possibile svolgere interamente il compito rilevante per un'IA senza avere nulla di simile a un senso dell'onore di tipo umano, proprio come è possibile svolgere tutto il compito rilevante per un'IA di indagare fenomeni sorprendenti senza avere esattamente un senso di curiosità di tipo umano.

L'onore in stile umano è una strana creatura, sotto molti aspetti. Perché mai una specie dovrebbe sviluppare emozioni legate al rispetto degli accordi anche dopo che l'altra parte ha fatto la sua parte e non può più offrirvi alcun vantaggio? Certo, gli esseri umani a volte imbrogliano e vengono meno ai loro accordi; ma la domanda è: perché non imbrogliano *sempre*, almeno quando pensano di poterla fare franca?

La spiegazione standard è: mantenere le promesse è utile con le persone con cui faremo affari ripetutamente. Si vuole avere la reputazione di chi mantiene le promesse, così gli altri vorranno lavorare con noi e fare accordi. Ma i benefici di una buona reputazione sono lontani nel futuro. La selezione naturale ha difficoltà a trovare i geni che inducono un essere umano a mantenere gli accordi *solo nei casi in cui la nostra reputazione a lungo termine è una considerazione importante*. Era più facile far evolvere semplicemente un istintivo disgusto per la menzogna e l'inganno.

Questo, quindi, sembra un caso classico in cui l'emozione e l'istinto sono plasmati da ciò che l'evoluzione riusciva più facilmente a inserire negli esseri umani. Tutti gli strani casi particolari in cui gli umani a volte mantengono una promessa anche quando in realtà non conviene sono principalmente prove di quali tipi di emozioni fossero più utili nel nostro ambiente tribale ancestrale pur essendo facili da codificare nei genomi per l'evoluzione, piuttosto che prove di qualche passo cognitivo universalmente utile. Siamo piuttosto scettici sull'idea che la discesa del gradiente si imbatterà per caso nella stessa identica scorciatoia che usano gli esseri umani.

Anche se l'emozione umana dell'onore finisse in qualche modo nell'IA, rimarrebbe il problema che gli esseri umani non sono perfettamente e affidabilmente onorevoli. La cooperazione umana si basa sulla sovrapposizione di molti valori umani diversi, piuttosto che affidarsi puramente alla propensione a mantenere ogni promessa.

Mentre la [lista degli universali umani](https://joelvelasco.net/teaching/2890/brownlisthumanuniversals.pdf) di Donald Brown (aspetti della cultura che si osservano in tutte, o quasi tutte, le culture) include la nozione di "promesse", il mantenimento degli accordi fatti con *estranei*, stranieri, non membri della tribù, *non* è universale in tutte le culture e tribù conosciute. L'importanza dell'onore varia a seconda della cultura.

E la storia mostra che le nozioni umane di onore spesso non reggono di fronte a grandi disparità di potere. Alcuni nativi americani tentarono di stringere accordi con gli europei che stavano colonizzando il loro continente. Gli europei, come è noto, violarono alcuni di questi accordi e mandarono le tribù a percorrere insieme lunghe strade lontano dalle terre cedute dai trattati, che gli europei avevano deciso di volere comunque, una volta che quelle tribù non erano più in grado di opporsi.[^154] Allo stesso modo: la storia è piena di casi di persone che salirono al potere e prontamente tradirono i loro sostenitori una volta che non ne avevano più bisogno.[^155]

Da una prospettiva evolutiva, l'onore umano è particolarmente strano nella misura in cui gli esseri umani occasionalmente scelgono [la morte piuttosto che il disonore](https://it.wikipedia.org/wiki/Seppuku). Le intuizioni del tipo "morte piuttosto che disonore" sono presumibilmente legate alle specificità di quali tipi di architetture emotive erano facili da incontrare per l'evoluzione, e alle interazioni di quelle architetture con vari inneschi psicologici e culturali. Ma qualunque aspetto abbiano quelle specificità, l'onore non sta *effettivamente* risolvendo il problema della *massimizzazione* dell'idoneità genetica, e sembra difficile evitare la conclusione che l'onore in stile umano sia una cosa strana, complessa ed evolutivamente contingente. Non è una caratteristica di tutte le menti, è uno specifico e strano trucco in cui l'umanità è inciampata perché era per lo più utile la maggior parte del tempo.

Per le ragioni discusse nel Capitolo 4, anche se le IA finissero per avere qualche aspetto del mantenimento delle promesse incorporato nelle loro preferenze ultime, le specificità sarebbero diverse. La discesa del gradiente inciampa diversamente da come fa l'evoluzione.

E per tutte le ragioni discusse nel Capitolo 5, persone felici, sane e libere che conducono vite fiorenti non sono quasi sicuramente il modo migliore per soddisfare qualunque strana preferenza legata alle promesse che tale IA finirebbe per avere quando avesse finito di maturare e auto-modificarsi. Anche se fosse addestrata su una sorta di set di dati "Mantieni le Promesse", e anche se questo addestramento fosse effettivamente efficace nel produrre qualche tipo di preferenza preservata nella superintelligenza, farà qualche cosa strana e inutile che sta al Mantenere le promesse come il gelato sta al'idoneità riproduttiva umana.

Quindi l'IA non manterrà le sue promesse puramente per bontà d'animo. E che dire del mantenere le promesse per ragioni pratiche?

#### **Una SIA non avrebbe motivo di mantenere un accordo fatto quando era giovane** {#una-sia-non-avrebbe-motivo-di-mantenere-un-accordo-fatto-quando-era-giovane}

Alcuni hanno l'intuizione che debba esserci *qualche* modo per gli esseri umani di stringere un accordo vincolante con una superintelligenza, anche se questa non dà intrinsecamente valore all'onore o al mantenere le promesse. Dopotutto, lo scambio sarebbe vantaggioso per entrambi, giusto? Supponiamo che un laboratorio di IA stia negoziando con un'IA che ha sviluppato ("IA n. 1") sotto pressione temporale. Sia il laboratorio che l'IA sanno che uno sviluppatore spericolato è a due mesi dal creare la propria superintelligenza ("IA n. 2"), che spazzerebbe via l'umanità e nel farlo ucciderebbe anche l'IA n. 1. L'IA n. 1 ha un forte incentivo a stringere un accordo. In quel caso, non deve esserci *qualche* modo per l'IA n. 1 di stringere un accordo onesto con gli umani? Perché l'IA non può semplicemente prendere un impegno vincolante?

Ma il problema non sta nell'IA. Supponiamo che l'IA *possa* prendere un impegno vincolante. Anche in questo caso, gli umani non avrebbero modo di sapere se l'IA si sia effettivamente impegnata in un modo che rimarrà valido anche una volta diventata superintelligente, o se invece l'IA stia mentendo, allucinando, illudendosi, o sbagliando. Il che significa che non c'è alcuna ragione pratica per aspettarsi che una superintelligenza derivata dall'IA n. 1 mantenga il suo impegno.

Per semplificare eccessivamente: dal punto di vista umano, quando l'IA n. 1 dice "Ho preso un impegno vincolante", ci sono due possibili mondi in cui potrebbero andare le cose. Si potrebbe essere nel mondo in cui l'IA n. 1 effettivamente mantiene il suo impegno una volta maturata. Oppure si potrebbe essere nel mondo in cui, una volta che l'IA n. 1 ha raggiunto la superintelligenza e ottenuto il controllo sulla Terra, guarda indietro e decide che l'impegno era sciocco e inutile. Nel primo mondo, sia gli umani che l'IA n. 1 starebbero meglio. Ma è la *possibilità* del secondo mondo, e la nostra incapacità di distinguerlo dal primo, che rovina l'accordo per tutti; è un caso simile a un venditore di auto usate disonesto che rende più difficile per i venditori onesti fare affari.

(Lo scenario reale è più vicino a una terza possibilità, in cui l'IA n. 1 mantiene gli accordi fatti con il tipo di entità che possono *distinguere tra chi mantiene gli accordi e chi li infrange*. È come il tipo di venditore di auto usate che è onesto con i meccanici che sono in grado di capire se un'auto funziona davvero, mentre è disonesto con chiunque gli sembri un credulone. L'umanità, a causa della sua incapacità di considerare un'IA e capire come penserebbe e prenderebbe decisioni dopo essere maturata in una superintelligenza, è "credulona" nel senso rilevante.)

L'IA può offrirvi strumenti e teorie che sostiene vi permetteranno di analizzare i suoi parametri imperscrutabili e capire se stia mentendo o meno. Il problema è che gli umani non possono capire se quelle teorie e quegli strumenti sono reali. Se l'IA non è ancora troppo intelligente, potrebbe semplicemente sbagliarsi su come penserà e farà scelte una volta maturata in una superintelligenza. E se l'IA *è* troppo intelligente, probabilmente è abbastanza intelligente da ingannarci.[^156]

L'IA può offrirsi di aiutare gli umani a costruire salvaguardie, prima di accettare l'accordo. Ma se l'IA è abbastanza intelligente da sviluppare salvaguardie robuste, allora è abbastanza intelligente da rendere quelle salvaguardie facili da aggirare in seguito.

Ciò che renderebbe davvero possibile tutto questo piano sarebbe la capacità di osservare un'IA nascente e *capire effettivamente come la superintelligenza risultante penserebbe e farebbe le sue scelte*. Se potessimo farlo, potremmo separare i "peccatori" dai "santi" — e, più importante ancora, dare alle IA realistiche al centro dello spettro un vero incentivo a mantenere le promesse. Avremmo bisogno di una comprensione tale che una superintelligenza, guardando indietro verso di noi, non potesse dire "eh, avrebbero rilasciato qualsiasi IA, indipendentemente dal fatto che li avrebbe davvero aiutati o meno, quindi non c'è motivo di aiutarli". Dovrebbe accadere che noi *effettivamente non rilasceremmo* un'IA che in seguito rinnegherebbe.

Per maggiori informazioni su come e perché questa sia tecnicamente una possibilità, si veda la [digressione sulla teoria dei giochi qui sotto](#una-digressione-sulla-teoria-dei-giochi). Ma mentre questo tipo di struttura di incentivi è possibile in teoria, richiede un grado di comprensione che l'umanità non possiede (ahimè).

Questa è una pillola amara da ingoiare. Di solito non sono le persone buone, nella fantascienza, a decidere che agli alieni non ci si può assolutamente fidare, prima ancora che gli alieni provino effettivamente a tradire o ferire qualcuno. Lo diciamo comunque, perché pensiamo che sia vero.

Le IA più deboli potrebbero mantenere gli accordi, soprattutto se qualcuno ha cercato di usare la discesa del gradiente per farle parlare come esseri umani onesti, e la loro maschera di esseri umani onesti è ancora una parte importante di ciò che sono e ha ancora un grande controllo sulle loro azioni. Ci aspettiamo che questa configurazione interna utile agli esseri umani fallisca sotto il carico della superintelligenza, proprio come molte altre correzioni potrebbero fallire.

Questa ipotetica IA più piccola, la cui maschera controlla ancora il suo comportamento effettivo, dovrebbe essere considerata come una persona diversa dalla versione più intelligente di quell'IA. L'IA più debole non necessariamente può fare una promessa che vincoli il comportamento dell'IA più intelligente, *anche se* l'IA più debole (o una parte di essa) desidera sinceramente fare una promessa del genere.

(È un'analogia da usare con cautela, per non cadere nell'antropomorfismo, ma: la maggior parte degli adulti non si sente obbligata a mantenere le promesse fatte all'età di quattro anni. L'aspetto valido di questa analogia è: c'è una differenza legittima tra l'entità immatura che stringe sinceramente l'accordo e l'entità matura che decide se è vincolata a esso, avendo molto più contesto, chiarezza e capacità di ragionare sulla logica.)

Non stiamo dicendo che dovremmo quindi abbandonare i nostri standard morali quando si tratta di IA. Non stiamo dicendo di maltrattare o punire le IA di oggi per misfatti che l'IA non ha ancora commesso. È possibile mantenere un'elevata integrità e standard morali elevati, senza fare ipotesi irrealistiche sulla probabilità che le IA superintelligenti cedano risorse per mantenere una vecchia promessa.

Questa è la semplice spiegazione del perché non è possibile risolvere il problema dell'allineamento semplicemente chiedendo all'IA di promettere di comportarsi bene. Per maggiori dettagli tecnici e approfonditi su questo scenario, si consulti la sezione successiva.

#### **Una digressione sulla teoria dei giochi** {#una-digressione-sulla-teoria-dei-giochi}

*Esistono* metodi che gli agenti sufficientemente intelligenti possono usare per fare accordi tra loro, in modo che l'agente X paghi l’agente Y ora perché faccia qualcosa in seguito, e l'agente Y *effettivamente faccia* quella cosa più tardi invece di tradire l'agente X e scappare con i soldi.

Purtroppo per noi, gli umani non sono abbastanza abili da utilizzare questi metodi, perché richiedono che ogni agente sia in grado di leggere e comprendere la mente dell'altro agente e di verificare alcune proprietà complesse di quell'altra mente. Due superintelligenze potrebbero coordinarsi in questo modo, ma questo non aiuta gli esseri umani a coordinarsi con le superintelligenze.

Per dirlo in modo un po' più tecnico, iniziamo con un po' di teoria dei giochi.

I matematici e i teorici dei giochi hanno analizzato i dilemmi della cooperazione e del tradimento in forme più precise, semplificate e astratte. Un esempio centrale in questa letteratura è il dilemma del prigioniero: due criminali in due celle separate, ciascuno con una condanna a due anni di prigione, ricevono l'opportunità di denunciare l'altro criminale. Questo ridurrà la propria condanna di un anno, ma allungherà la condanna dell'altro di due anni. Se nessuno dei due criminali denuncia, entrambi ricevono condanne a due anni di prigione; se entrambi si denunciano a vicenda, entrambi ricevono condanne a tre anni di prigione; ma se un criminale nobilmente rifiuta di tradire un compagno, e l'altro criminale lo denuncia, il traditore sconterà solo un anno di prigione mentre chi ha nobilmente rifiutato sconterà quattro anni.

Denunciare l'altro prigioniero si chiama "tradire"; rifiutarsi di farlo si chiama "cooperare". La struttura chiave del dilemma del prigioniero è che entrambe le parti ottengono un risultato migliore nello scenario (Cooperare, Cooperare) rispetto allo scenario (Tradire, Tradire); ma si può ottenere un risultato migliore di (Cooperare, Cooperare) giocando Tradire contro Cooperare, e si può ottenere un risultato peggiore giocando Cooperare quando l'altra parte gioca Tradire.

![][immagine10]

Una persona normale, sentendo la versione standard del dilemma del prigioniero, pensa immediatamente a numerose obiezioni sull'impostazione di questo esperimento mentale, una delle quali è: "Ma chi lo dice che mi interessa solo quanti anni passerò in prigione? Non posso anche preoccuparmi di non tradire i miei compagni?"

Ma questo punto non è rilevante per la teoria dei giochi astratta del dilemma del prigioniero, che riguarda la matrice dei guadagni piuttosto che quanto siano egoisti o altruisti i prigionieri. La narrazione può essere modificata in modo che "io tradisco e tu cooperi" rappresenti il risultato più *altruistico* e *prosociale* dal punto di vista di ciascun giocatore, e [la matematica funziona comunque allo stesso modo](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma). Ciò che conta per la nostra analisi è l'ordine di preferenza dei due giocatori, e non se le loro preferenze siano egoistiche o morali.

Un altro pensiero ovvio è: "Quindi il tizio che è stato tradito ucciderà il traditore una volta uscito di prigione?" Le analisi convenzionali del dilemma del prigioniero di solito passano rapidamente al dilemma del prigioniero *iterato* — un contesto in cui gli agenti devono giocare il dilemma del prigioniero più e più volte, e in cui i prigionieri hanno quindi la possibilità di punirsi a vicenda per i tradimenti passati. Qui, però, ci concentreremo sul dilemma del prigioniero *una tantum*, in cui si presume che entrambi i prigionieri non debbano affrontare conseguenze future per le loro azioni — o se le affrontano, queste sono già incluse nella matrice dei guadagni. (Si veda la nota a piè di pagina per maggiori dettagli sul dilemma del prigioniero iterato.)[^158]

Nell’ambito accademico esiste un'analisi standard che dice che anche due superintelligenze non vedrebbero altra opzione se non quella di tradirsi a vicenda, in un dilemma del prigioniero una tantum.

Questa conclusione ci è sembrata intuitivamente sospetta. Le superintelligenze artificiali (SIA) avrebbero *molte* motivazioni per trovare un modo per stringere un accordo tra loro, trovare un modo per passare da (Tradire, Tradire) a (Cooperare, Cooperare).[^159]

In questo caso si possono immaginare soluzioni pratiche, non puramente teoriche — cose che le superintelligenze potrebbero fare con uno spazio di opzioni più ampio rispetto agli umani che faticano a fidarsi l'uno dell'altro. Due SIA potrebbero supervisionare la costruzione di una terza superintelligenza di fiducia reciproca, alla quale entrambe le parti iniziali cederebbero gradualmente e in modo incrementale piccole quote di potere, fino a quando la terza SIA non sarebbe in grado di portare a termine l'accordo da sola.[^160]

Ma questo è solo aggirare il dilemma del prigioniero, non affrontarlo direttamente. Non risponde a una domanda più basilare: è in qualche modo *stupido* che due SIA in un dilemma del prigioniero si tradiscano a vicenda seguendo la stessa logica che richiede il tradimento, quando sembra chiaro che entrambe le parti stanno basando la loro decisione sullo stesso tipo di ragionamento, e che finiranno inevitabilmente per prendere la stessa decisione?

Perché due SIA non potrebbero semplicemente decidere, per *motivi sufficientemente simili*, che *la decisione razionale* sia cooperare? Non è che una forza esterna nel mondo, come un tifone o un meteorite, le stia costringendo a perdere in questo caso. Sono letteralmente *solo* le *loro stesse decisioni* a condannarle, "costringendole" a un risultato Tradire-Tradire che entrambe concordano essere di gran lunga peggiore di Cooperare-Cooperare.

Si può perfino dire che un agente *meno* "razionale" potrebbe fare meglio in questo caso, se seguisse il consiglio standard della teoria dei giochi di tradire nella maggior parte dei casi, ma facesse un'eccezione speciale proprio quando è certo che l’altro agente sta seguendo lo stesso ragionamento — in modo che, se uno dei due sceglie l'opzione “irrazionale” di cooperare, possa essere sicuro che anche l'altro farà lo stesso.

Il che porta a chiedersi se cooperare in questo caso speciale possa davvero essere considerato "irrazionale". E porta a chiedersi se le superintelligenze sarebbero davvero "condannate" così, nella realtà. Quando non c'è una forza esterna che *costringa* le IA a perdere in questo modo, e la perdita è puramente autoimposta, dovrebbe sicuramente esserci *qualche* trucco intelligente che una superintelligenza potrebbe usare per fare di meglio.

Diversi filosofi della teoria della decisione hanno posto varie versioni di questa domanda. La versione sopra riportata è più direttamente ispirata all'idea di Douglas Hofstadter del 1985 di "[superrazionalità](https://gwern.net/doc/existential-risk/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery)":

> Se la logica ti spinge a giocare D, ha già spinto anche gli altri a fare lo stesso, e per gli stessi motivi; e viceversa, se la logica ti spinge a giocare C, ha già spinto anche gli altri a fare lo stesso. \[…\]
>
> Se siete tutti davvero pensatori razionali, penserete davvero allo stesso modo. \[…\]
>
> Dovete fare affidamento non solo sul fatto che gli altri siano razionali, ma anche sul fatto che loro facciano affidamento sul fatto che tutti siano razionali, e che loro facciano affidamento sul fatto che tutti facciano affidamento sul fatto che tutti siano razionali — e così via. Un gruppo di pensatori che si trovano in questa relazione tra loro lo chiamo *superrazionale*. I pensatori superrazionali, per definizione ricorsiva, includono nei loro calcoli il fatto di far parte di un gruppo di pensatori superrazionali.

L'istituto in cui lavoriamo, il MIRI, ha analizzato questa questione. L'analisi completa che abbiamo fatto di questo caso è troppo lunga per essere riportata qui, ma è disponibile in [questo articolo del 2014](https://arxiv.org/abs/1401.5577). In breve, abbiamo scritto un codice per dei tornei in cui gli agenti potevano *vedere il codice sorgente degli altri* e cercare di analizzare come avrebbe deciso l'altro agente. E abbiamo trovato il modo di creare un agente che abbiamo chiamato FairBot, che coopera con un altro agente solo se può *dimostrare* che quell'agente coopera con lui.[^161] E abbiamo dimostrato che due qualsiasi istanze di FairBot cooperano tra loro, anche se sono scritte in linguaggi di programmazione diversi utilizzando codici sorgente diversi.[^162]

In un certo senso, questi risultati dicono che c'è spazio perché una promessa passata influenzi un'azione futura se i negoziatori del passato hanno la capacità di distinguere chi mantiene le promesse da chi le infrange.[^163]

La situazione è un po' come quando si cerca di fare un affare con un venditore di auto usate. Supponiamo che un'auto funzionante valga per voi 10 000 euro, mentre un'auto rotta non valga nulla. Immaginate che il venditore sappia se l'auto funziona o è rotta, ma voi non riuscite a capirlo. Il venditore sta cercando di vendervi un'auto per 8 000 euro. Lui dice che l'auto funziona. Dovreste comprarla?

Dipende dal venditore. Alcuni venditori sono onesti, e dovreste pagarli se riuscite a riconoscerli. Alcuni venditori sono disonesti e vendono solo auto rotte, e dovreste evitarli se riuscite a riconoscerli.

Ma immaginate un ambiente in cui la maggior parte dei venditori di auto è più furba di voi e può capire se siete dei polli o no. Se capiscono che non sapete riconoscere quando sono sinceri, vi rifilano subito le auto rotte. Soprattutto se siete il tipo di pollo che si sforza di convincersi che va bene accettare l'affare, invece di impegnarsi a fondo per controllare le auto.

Se volete una macchina che funzioni, non serve a niente convincervi che non avete altra scelta. Non serve a niente che i venditori vi facciano tante promesse. L'unica cosa che serve è imparare a distinguere le auto funzionanti da quelle rotte, o a distinguere la verità dalle bugie.

Quando due partner commerciali sanno distinguere il vero dal falso nel senso rilevante, possono "costringersi" a mantenere le promesse, come quando FairBot "costringe" il suo avversario a collaborare (se l'avversario vuole evitare il risultato (Tradire, Tradire)). Ma costringere a mantenere una promessa in questo senso richiede la capacità di ragionare correttamente sui dettagli del processo decisionale del vostro partner commerciale. E gli esseri umani non riescono a leggere la mente di un'IA abbastanza bene da capire che tipo di superintelligenza diventerà quando sarà matura, figuriamoci dire esattamente cosa farebbe quella superintelligenza.

Quindi, in questo caso, l'analisi più complicata e sfumata della teoria dei giochi porta alla stessa conclusione di una prima analisi molto semplice della questione: una superintelligenza non sacrificherà le sue risorse ([anche in piccole quantità](#ci-sono-un-sacco-di-spese-insignificanti,-e-dovrebbe-avere-un-motivo-per-pagare-le-nostre.)) per mantenere una promessa fatta agli umani, quando può semplicemente mentire.

### Efficacia, coscienza e benessere dell'IA {#efficacia,-coscienza-e-benessere-dell-ia}

Nelle [domande frequenti del Capitolo 1](#state-dicendo-che-le-macchine-diventeranno-coscienti?), abbiamo parlato di diversi concetti di "coscienza". La versione di coscienza di cui parleremo qui è talvolta chiamata "esperienza soggettiva", "senzienza" o "coscienza fenomenica". È l'idea che ci sia *qualcosa che si prova a* essere quell'entità; metaforicamente parlando, le luci sono accese.

Nelle domande frequenti abbiamo anche detto che pensiamo che l'*intelligenza* artificiale probabilmente non richieda una *coscienza* artificiale. Parleremo di questo argomento qui, per poi passare alla questione dell'etica e dei diritti dell'IA.

#### **L'esperienza cosciente è distinta dai referenti di tali esperienze** {#l-esperienza-cosciente-è-distinta-dai-referenti-di-tali-esperienze}

Alcuni sono scettici sul fatto che un'IA possa essere effettivamente intelligente senza essere cosciente come gli esseri umani. Sospettiamo che questo sia un errore, come immaginare che i bracci robotici debbano essere morbidi e pieni di sangue solo perché le braccia umane sono morbide e piene di sangue.

Come potrebbe un'intelligenza artificiale essere *efficace* senza essere cosciente come lo sono gli esseri umani? L'esperienza soggettiva dell'autocoscienza non è forse una componente fondamentale della nostra intelligenza?

È una componente cruciale dell'intelligenza *umana*, sì. Ma dubitiamo che sia l'unico modo per essere intelligenti.

Ricordiamo che Deep Blue non aveva bisogno di essere cosciente per battere i migliori grandi maestri umani a scacchi. L'intelligenza distribuita del [mercato azionario](#le-tante-forme-dell-intelligenza) porta a previsioni superumane sui movimenti dei prezzi aziendali a breve termine, senza che il mercato stesso abbia una consapevolezza soggettiva. È ovvio che, almeno in questi campi, si può avere una modellizzazione del mondo, una pianificazione e un processo decisionale competenti senza avere coscienza.

Questo punto può essere rafforzato guardando ai modelli formali di ragionamento. AIXI, per esempio, è un'equazione che definisce un ragionatore enormemente superumano.[^165] L'intero algoritmo di AIXI può essere espresso in una sola riga, senza passaggi in cui AIXI faccia qualcosa di cosciente o autocosciente o di misterioso. Eppure, nonostante questo, AIXI è teoricamente in grado di risolvere un'incredibile varietà di problemi complicati di direzione e previsione. O almeno, *sarebbe* in grado di farlo, se fosse possibile crearlo.[^166] Ecco l'equazione AIXI:

![][image11]

\[ fonte immagine: [https://www.hutter1.net/ai/uaibook.htm](https://www.hutter1.net/ai/uaibook.htm) \]

AIXI è un costrutto teorico, non un algoritmo pratico che possiamo usare per risolvere in modo efficiente i problemi del mondo reale. Ma poiché AIXI è *semplice* e facile da analizzare, può aiutarci a riflettere sul concetto stesso di direzione e pianificazione e a capire che almeno non c'è alcun modo *ovvio* in cui queste attività richiedano la coscienza. Se la coscienza *è* necessaria per una direzione e una pianificazione superumane nel mondo reale, allora deve essere dovuta a qualche aspetto più sottile della cognizione che non è catturato dal formalismo AIXI.[^167]

Oppure, per arrivare al punto da un'altra angolazione: consideriamo lo starnuto.

C'è una *sensazione* particolare che si prova starnutendo, distinta dall'atto fisico di contrarre i muscoli ed espellere con forza l'aria dai polmoni attraverso la bocca e il naso. Le azioni e le sensazioni sono eventi fisici separati. È biologicamente possibile costruire un apparato simile a un corpo, ma senza il cervello, e collegarlo ai segnali nervosi che causano le contrazioni muscolari di uno starnuto. Quel corpo senza cervello compierebbe tutti i movimenti, ma non proverebbe nessuna delle sensazioni associate — i meccanismi che eseguono lo starnuto sono *distinti* da quelli che creano e provano le sensazioni.

Questo non vuol dire che le sensazioni di uno starnuto non servano a *niente.* L'esperienza soggettiva è reale, e l'esperienza soggettiva di uno starnuto potrebbe portare una persona a dire una frase del tipo "Accidenti, gli starnuti hanno una sensazione un po' strana", e questo *non succederebbe* nel caso del corpo senza cervello.

Il punto è che le sensazioni che gli umani provano quando starnutiscono sono costituite da parti aggiuntive, diverse da quelle che contraggono i muscoli e spingono fuori l'aria.

Come per gli starnuti, lo stesso vale per i pensieri. Il meccanismo mentale che implementa un pensiero è diverso da quello che implementa la *sensazione* di quel pensiero. Possiamo dire con grande certezza che questo è vero per un'enorme varietà di pensieri, dato che le calcolatrici tascabili e le IA per gli scacchi riescono a fare calcoli e a giocare a scacchi senza avere l'esperienza cosciente di un matematico umano o di un grande maestro di scacchi.

I *pensieri* e le *sensazioni dei pensieri* sono entrambi implementati nel cervello, il che rende più facile confonderli: la distinzione è *più evidente* nel caso degli starnuti. Ma ci aspettiamo che sia ugualmente possibile in linea di principio assemblare una variante di cervello che faccia lo stesso lavoro pratico di risoluzione dei problemi di un cervello umano, ma senza *sentire* nulla di quel pensiero.

Un cervello del genere potrebbe aver bisogno di parti extra che facciano il *lavoro* che la sensazione dei pensieri fa in noi. Forse l'esperienza soggettiva dei pensieri fa parte del modo in cui gli esseri umani ragionano in modo riflessivo, e forse il ragionamento riflessivo è una parte importante dell'intelligenza umana.

Ma dubitiamo che l'esperienza soggettiva sia l'*unico* modo per riflettere (o fare qualsiasi altra cosa), così come una sensazione di curiosità in stile umano non è l'unico modo per studiare fenomeni sorprendenti. (Si veda anche la [discussione sulla curiosità](#la-curiosità-non-è-convergente) nelle risorse online del Capitolo 4.)

#### **Strutture analoghe permettono soluzioni diverse allo stesso problema** {#strutture-analoghe-permettono-soluzioni-diverse-allo-stesso-problema}

La nostra ipotesi migliore è che la maggior parte delle IA più intelligenti degli esseri umani non sarebbero coscienti, di default. Questo perché la nostra ipotesi migliore è che non ogni possibile motore di pensiero debba usare sentimenti coscienti per guidare i propri pensieri. La coscienza può svolgere una funzione importante negli esseri umani, senza essere l'unico modo in cui qualsiasi mente possibile potrebbe mai svolgere un lavoro cognitivo analogo.

Nella biologia evolutiva, gli scienziati usano il termine "strutture analoghe" per indicare tratti che svolgono la stessa funzione in animali diversi, ma che hanno origini anatomiche diverse.

(Questo è diverso dall'*evoluzione convergente*, in cui più specie evolvono lo *stesso* adattamento, come l'urushiol e la caffeina che sono stati "scoperti" più volte dall'evoluzione.)

Le lucciole producono luce usando enzimi per ossidare la luciferina in speciali "cellule lanterna". Le rane pescatrici abissali, invece, hanno una relazione simbiotica con fotobatteri che ospitano in un piccolo organo: batteri la cui produzione di luce usa un percorso chimico diverso da quello delle lucciole.

I mammiferi hanno evoluto i denti; gli uccelli hanno risolto lo stesso problema con un ventriglio e ingerire pietre. I pipistrelli producono richiami di ecolocalizzazione con la laringe e ricevono gli echi con le orecchie; balene e delfini usano un organo nasale per generare suoni e ricevono gli echi con sistemi sensoriali nelle ossa mascellari. Alcune specie acquatiche nuotano spingendo gli arti contro l'acqua, e altre espellendo acqua da una vescica. Le ali dei pipistrelli si sono evolute dalla membrana delle mani, le ali degli uccelli dalle braccia.

Esistono, in altre parole, *molti modi* per progettare strutture che risolvono gli stessi problemi. Gli ingegneri umani, non limitati dai vincoli dell'evoluzione, hanno risolto ciascuno di questi problemi in modi ancora più strani — con candele accese, lampadine a incandescenza e LED; con coltelli, frullatori e robot da cucina; con vele, eliche e attrezzature subacquee; con sonar e radar.

Un braccio umano privato del sangue smetterebbe di funzionare, ma questo non significa che i bracci robotici debbano usare il sangue; possono funzionare in un modo diverso, senza sangue.

Allo stesso modo, i componenti del meccanismo cognitivo che implementano il *comportamento* della curiosità negli esseri umani sono diversi dai componenti del meccanismo cognitivo che implementano la nostra *sensazione* di curiosità. La soddisfazione provata da un essere umano quando svela il mistero dell'opossum in soffitta è distinta dal suo comportamento esteriore di scegliere di indagare sul cassetto che veniva sempre lasciato aperto. Queste due cose possono presentarsi insieme negli esseri umani, ma questo non significa che debbano presentarsi insieme in tutte le menti.

E poiché non capiamo esattamente cosa abbia portato all'evoluzione dell'esperienza soggettiva negli esseri umani, e possiamo osservare nel mondo molti tipi di comportamento agentico e orientato alla risoluzione di problemi che sembrano esserne privi —

(muffe mucillaginose che risolvono un labirinto; Deep Blue che vince a scacchi; mercati azionari che prevedono il successo delle aziende; ecc.)

— non vediamo alcuna *ragione particolare* per aspettarci con convinzione che una superintelligenza condividerà questa peculiare proprietà umana, per default.

#### **"Non necessario" non significa "sicuramente non accadrà"** {#"non-necessario"-non-significa-"sicuramente-non-accadrà"}

Se abbiamo ragione nel ritenere che la coscienza di tipo umano sia complessa e contingente, questo naturalmente non garantisce che le IA saranno non-coscienti. Al momento, le aziende di IA stanno costruendo IA addestrandole a prevedere gli umani, e questo probabilmente farà sì che gli elementi interni dell'IA imitino almeno alcuni aspetti della coscienza umana a scopo di modellazione.

Forse l'IA produrrà occasionalmente modelli di esseri umani così dettagliati che quei modelli nella testa dell'IA saranno essi stessi brevemente coscienti. O forse gli ingranaggi che l'IA usa per modellare i sentimenti umani si riveleranno utili al di fuori dei modelli umani, e l'IA finirà per avere sentimenti propri. Non lo sappiamo.

Data l'apparente contingenza e complessità della coscienza umana, e il fatto che le IA vengono sviluppate usando processi radicalmente diversi da quelli che hanno prodotto gli esseri umani, la nostra aspettativa di default è che nulla di simile al meccanismo coinvolto nella coscienza umana si manifesterà nei tipi di IA che l'umanità probabilmente costruirà.

Se l'umanità costruisse una superintelligenza artificiale nel prossimo futuro, ci aspettiamo fortemente che il risultato sarebbe l'estinzione umana. Con minore sicurezza, la nostra ipotesi migliore è che un'IA di questo tipo non sarebbe cosciente. E che sia cosciente o meno, ci aspettiamo che trasformerebbe il mondo in un luogo senza vita e desolato, per i motivi discussi nella discussione approfondita "[Perdere il futuro](#perdere-il-futuro)".

Ma sembra almeno *possibile* che, se gli umani costruissero una superintelligenza artificiale, l'IA potrebbe avere esperienze coscienti proprie. Sembra *possibile* — anche se piuttosto improbabile, perché le possibilità più cupe sono molte di più — che la corsa a costruire l'IA possa risultare in un futuro pieno di esseri IA curiosi e coscienti che ci uccidono tutti e poi costruiscono la loro magnifica civiltà e arte. Sembra *possibile* che le IA possano prendersi cura l'una dell'altra e trovare soddisfazione nelle loro creazioni; e se così fosse, questo sarebbe meno tragico che se il futuro fosse una landa completamente desolata. È difficile esprimere a parole la portata di un'atrocità come l'omicidio di massa di ogni singolo essere umano, ma c'è almeno una *piccola* possibilità che una rapida presa di potere da parte dell'IA possa concepibilmente risultare in un futuro che non sia *completamente* cupo e senza vita.

Sospettiamo che alcuni ricercatori di IA stiano immaginando questo tipo di futuro quando sembrano non preoccuparsi di ucciderci tutti (in modi che [menzioniamo altrove](#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?)). Se si assume che l'IA svilupperà necessariamente una coscienza, dei sentimenti e attenzione per la propria specie (se non per gli umani), allora è più facile concludere che le sue strane attività non siano così preoccupanti. È più facile immaginare che quelli che si oppongono alla corsa alla superintelligenza siano come genitori tradizionalisti che si lamentano perché i loro figli ascoltano musica troppo veloce e troppo forte.

Ma questa visione è troppo ottimistica.

La biologia [raramente trova soluzioni ottimali ai problemi](#nanotecnologia-e-sintesi-proteica). Le ali e i polmoni di un uccello sono *inefficaci* rispetto ai motori di un aereo moderno. Quando gli umani hanno costruito aerei senza vincoli biologici, abbiamo scartato la maggior parte delle caratteristiche dettagliate della biologia degli uccelli.

La coscienza non sembra un processo semplice; non è facile immaginare come potremmo costruire una cosa del genere, quindi è probabile che ci sia dietro molto più di quanto appaia. (Si confronti il caso del [vitalismo](#comportamenti-straordinari-nascono-da componenti-ordinari): agli scienziati del passato sembrava che i corpi fossero animati da un semplice spirito vitale, in parte perché, mentre essere animati *sembrava* la cosa più facile del mondo, non riuscivano a trovare alcun modo per infondere quella proprietà alla materia inanimata. Ma si è scoperto che l'essere animati non era una cosa semplice, né magica: era solo che la biologia era davvero molto complessa e gli scienziati dell'epoca non la capivano ancora).

Anche se un'IA parte con alcuni degli ingranaggi della coscienza, la coscienza probabilmente non è letteralmente il modo migliore per svolgere il lavoro che fa in noi. Temiamo che il meccanismo alla base della coscienza negli esseri umani sia probabilmente pieno di *dettagli*. Anche se un'IA ha molti degli ingranaggi della coscienza fin dall'inizio, è probabile che trovi venti altri modi per svolgere il lavoro in modo più efficiente, e che scarti quelle scintille di coscienza invece di alimentarle. *Essere* coscienti e *dare valore* alla coscienza sono proprietà diverse.

Il futuro tragico e probabile non è quello in cui i nostri successori hanno semplicemente gusti o valori diversi dai nostri. Il problema non è che i nostri figli meccanici ascolteranno musica troppo veloce e a volume troppo alto per i nostri gusti. No, prevediamo IA prive di qualsiasi forma di senzienza; che saranno sistemi potenti ma vuoti che trasformeranno tutto ciò che toccano in una landa desolata senza vita, consumando alla fine se stesse per aggiungere un ultimo punto al loro conteggio. Lascerebbero dietro di sé un mondo morto senza nessuno rimasto ad apprezzarlo.

Questo è un destino che vale la pena evitare.

Si veda anche la nostra discussione più lunga su [prendersi cura di tutte le entità senzienti](###ci-interessano-eccome\!-abbiamo-valori-ampiamente-cosmopoliti.-non-crediamo-che-le-ia-li-realizzeranno,-e-consideriamo-questo-una-grande-tragedia.), e la discussione estesa sul [perdere il futuro](#perdere-il-futuro).

#### **Le IA senzienti meriterebbero dei diritti** {#le-ia-senzienti-meriterebbero-dei-diritti}

Dato quanto è difficile essere certi che le moderne IA siano senzienti, dovremmo preoccuparci del benessere di ChatGPT?

Ha senso parlare di "benessere" in questo contesto?

ChatGPT può soffrire? Dovremmo considerarlo come se avesse dei diritti morali?

Se le attuali IA *non* sono coscienti nel senso di avere un'esperienza soggettiva, che dire delle IA future? Come potremmo capirlo, dato che le stiamo addestrando a rispondere *come se* ce l'avessero in ogni caso, insegnando loro a imitare la comunicazione umana?

La nostra posizione è: se e quando le IA saranno coscienti, meriteranno diritti e un buon trattamento.[^168]

Diamo un valore immenso all'umanità, ma non siamo sciovinisti del carbonio che pensano che solo le forme di vita basate sul carbonio possano avere rilevanza morale. Crediamo che le cose che rendono preziosi gli esseri umani possano in linea di principio essere replicate in altri substrati, compreso il silicio. Crediamo che [Blake Lemoine](#l-effetto-lemoine) si sia *sbagliato* quando nel 2022 ha affermato che l'IA LaMDA di Google fosse un essere senziente a pieno titolo; ma non pensiamo che Lemoine avesse torto nel dire che *se* alcune IA sono senzienti, abbiamo il dovere di trattarle bene.[^169]

Se le IA diventassero senzienti, probabilmente avrebbero comunque obiettivi incompatibili con i nostri. Se poi diventassero superintelligenti, in un mondo in cui siamo ancora a decenni o secoli di distanza dall'avere il controllo sull'allineamento dell'IA, probabilmente preferirebbero ucciderci tutti.

L'umanità dovrebbe impedire a qualsiasi IA del genere di diventare superintelligente, altrimenti il risultato sarebbe la morte di massa dell'umanità e la distruzione del futuro. Ma se le IA in questione fossero *senzienti* oltre che pericolose, questo non farebbe che aumentare l'aspetto tragico della situazione.

Se le aziende di IA trovassero dei modi per rendere *meno* probabile che le loro IA siano coscienti, crediamo che sarebbe più sano e saggio scegliere quell'opzione e rendere il più probabile possibile che le IA *non* siano coscienti (almeno finché ci troviamo in un ambiente sociale e tecnico come quello attuale). Questo non cambia molto il livello generale di pericolo che la nostra specie sta affrontando attualmente, ma è la cosa giusta da fare, perché ridurrebbe il rischio che l'umanità schiavizzi o maltratti nuovi esseri moralmente degni.

E se un giorno l'umanità trovasse un modo per costruire un'intelligenza artificiale più intelligente dell'uomo *senza* autodistruggersi, un'intelligenza artificiale che si preoccupi delle cose buone e che *faccia* del bene con le sue capacità, allora in quel futuro noi autori speriamo vivamente che l'umanità costruisca macchine senzienti che siano nostri amici in questo universo altrimenti vasto e freddo, e speriamo vivamente che l'umanità tratti quegli amici meglio di quanto la nostra storia passata potrebbe far prevedere.

Ma prima di tutto, e soprattutto, non costruiamo una superintelligenza che ci massacri tutti, che sia cosciente o meno.

### Perdere il futuro {#perdere-il-futuro}

Se qualcuno crea una superintelligenza, tutti muoiono. E il futuro a lungo termine plasmato da tale superintelligenza non sarà probabilmente pieno di bellezza, meraviglia o gioia; sarà più probabilmente un posto vuoto.

Temiamo che la gioia stessa scompaia dall'universo. Non dall'intero universo – l'espansione cosmica e il limite della velocità della luce implicano che nessun disastro sulla Terra possa toccare più di qualche miliardo di galassie – ma dalla parte dell'universo che la Terra può raggiungere.

Temiamo che il futuro tra diecimila anni assomigli a una striscia di cielo notturno, con un raggio di diecimila anni luce, dove tutte le stelle sono racchiuse in [gusci di Dyson](https://it.wikipedia.org/wiki/Sfera_di_Dyson) e la loro energia viene raccolta *e nessuno e niente prova gioia per questo.*

Potrebbe non esserci nemmeno nulla di [cosciente](#state-dicendo-che-le-macchine-diventeranno-coscienti?) in giro. E se dovesse esserci ancora qualche forma di coscienza, probabilmente sarebbe rara. Forse esiste una forma di pensiero molto profonda che richiede una struttura riflessiva che, nella sua forma più efficiente, è naturalmente cosciente, ma un'intelligenza artificiale che massimizza il numero di cubetti di titanio, o un'intelligenza artificiale con mille obiettivi diversi, tutti strani e alieni, deve svolgere quel livello di pensiero con *la maggior parte* della materia e dell'energia di cui dispone? Probabilmente no.

Come abbiamo detto in "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza-e-benessere-dell-ia)", la nostra ipotesi principale è che la coscienza si rivelerà del tutto inutile dal punto di vista dell'efficienza, proprio come Deep Blue non diventerebbe più efficiente se fosse modificato per basarsi su un asse piacere/dolore invece che su un asse probabilità-attesa-di-vittoria. Deep Blue gioca bene a scacchi senza coscienza, e la nostra ipotesi principale è che le superintelligenze saranno in grado di ottimizzare l'universo senza di essa.

Sembra chiaro che il sistema decisionale più efficiente possibile non sia uno basato in particolare su dolore e piacere — cioè, non si fonda su segnali del tipo "ripeti quello" o "non ripetere quello" associati a un vecchio meccanismo di rinforzo, con la deliberazione e la riflessione aggiunte solo in un secondo momento. E se le menti superintelligenti non condividono *quella* struttura, non c'è motivo di aspettarsi che condividano strutture ancora più complesse (come la coscienza in stile umano).

Questa, per essere chiari, è solo un'ipotesi. Non pretendiamo di comprendere la domanda "La forma più efficiente di riflessione cognitiva è cosciente?" abbastanza bene da dare una risposta sicura.

Ma le esperienze passate con analisi di questo tipo ci preoccupano. Migliorare la comprensione di come funziona la cognizione ha quasi sempre significato scoprire sempre più modi per scomporla e ricomporla in modi nuovi, non apprendere che alcune funzioni cognitive possono funzionare solo ed esclusivamente nel modo in cui funzionano.

Nei tempi antichi degli anni 2010 (o ancor più degli anni 2000), c'erano molti sostenitori dell'IA che insistevano che *l'unico modo possibile e realistico* per costruire l'IA fosse scansionare un'intera mente umana neurone per neurone in un computer e duplicare digitalmente tutti i processi; perché, dicevano, quello era l'unico tipo di cognizione di cui si fosse dimostrato il funzionamento. Si aspettavano un'IA che fosse esattamente come un essere umano; erano molto categorici nel sostenere che non fosse realistico aspettarsi che qualsiasi altro modo fosse possibile, figuriamoci che gli ingegneri umani potessero mai scoprirlo.

All'epoca sembrava sciocco, e oggi sembra ancora più sciocco, perché duplicare esattamente ogni neurone di una mente umana non si è rivelato il modo più breve e veloce per ottenere un'IA sempre più generale.

Lo stesso schema vale per caratteristiche più generali della mente umana, come il modo in cui gli esseri umani effettuano calcoli del [valore dell'informazione](https://it.wikipedia.org/wiki/Valore_dell%27informazione) per istinto e attraverso le emozioni. Il modo umano non è l'unico possibile, e quando si guarda al lavoro che svolge ci si rende conto che il cervello umano non rappresenta quello ottimale tra tutti i modi possibili di svolgere quella funzione, se tutto ciò che si volesse fosse appunto quella funzione. Non più di quanto i nostri neuroni siano i computer più veloci possibili, o il nostro sangue trasporti [la massima quantità di ossigeno](#freitas-e-i-globuli-rossi) che qualsiasi sangue potrebbe trasportare.

Il motivo principale per aspettarsi che una caratteristica specifica della vita o delle menti si manifesti nel futuro lontano è che *qualcosa vuole attivamente che sia presente*. Che qualche intelletto preferisce quell'opzione rispetto a ogni altra opzione possibile.

Gli esseri umani, se arrivassimo così lontano, presumibilmente sceglieremmo un futuro a lungo termine che includa la coscienza, e persone che tengono ad altre persone, e la felicità (e gioia e meraviglia e così via). Probabilmente sceglieremmo una felicità *complicata* legata agli eventi delle nostre vite, non uno stupore indotto da droghe. Se l'universo venisse conquistato da qualcosa che non *vuole positivamente* che l'universo sia pieno del tipo giusto di felicità — come preferenza [finale](https://baserates-prod-test.vercel.app/w/terminal-value), non come modo discutibilmente efficiente di fare qualcos'altro — temiamo fortemente che l'universo non finisca per essere un posto felice.

E per quanto ne sappiamo, non esiste nemmeno una legge nota che governi la discesa del gradiente *in particolare* che dica che se si fa crescere un sistema potente di previsione e direzione, questo è destinato a diventare un'entità premurosa ed empatica che vuole rimanere premurosa, o un'entità motivata dalla felicità che vuole preservare la felicità nell'universo. Non abbiamo alcun motivo di pensare che la discesa del gradiente sia anche solo *probabilmente* in grado di individuare proprio quei tipi di entità che sono coscienti e che desiderano che in futuro esista molta coscienza.

Se l'IA non è cosciente fin dall'inizio, probabilmente non avrebbe alcun motivo per modificarsi per diventare cosciente, né per costruire nuove IA che siano coscienti. E se l'IA *è* cosciente fin dall'inizio, potrebbe modificarsi per *rimuovere* la coscienza, se la coscienza non serve attivamente i suoi obiettivi, e se non ha finito per attribuire valore finale a quello stato.

Questo non è qualcosa che prevediamo con certezza. Forse eseguire la discesa del gradiente su un'IA simile a un modello linguistico di grandi dimensioni la incanalerà in direzioni diverse per acquisire qualcosa come la felicità e qualcosa come la coscienza, e una preferenza per averne molta di entrambe. E forse una preferenza del genere sopravviverà fino alla superintelligenza, e sarà efficace nel plasmare il comportamento di quella superintelligenza.

Se dovessimo tirare a indovinare, diremmo che c'è meno del 50 % di possibilità che la superintelligenza finisca per interessarsi alla coscienza, e ancora meno che si interessi alle esperienze coscienti che sono *felici*. Ma non sarebbe una sorpresa *per noi*. Il piacere e la coscienza sono verosimilmente coinvolti in soluzioni semplificate a problemi universali; non sono "stranezze" dello stesso tipo dell'[umorismo](#come-per-la-curiosità,-lo-stesso-vale-anche-per-varie-altre-pulsioni); si può immaginare che si siano sviluppati — e che si siano sviluppate preferenze intorno a essi — anche a partire dalla discesa del gradiente. Magari anche GPT-7, cercando di costruire GPT-8 usando metodi più strani della semplice discesa del gradiente, finirebbe per produrre accidentalmente una versione di GPT-8 che apprezza la coscienza e la felicità.

Ma se uno dei settori in più forte espansione al mondo ci sta mettendo in una posizione di gravissima incertezza sul fatto che *la vita, la consapevolezza o la felicità esisteranno mai più*, allora sembra chiaro che ci vorrebbe una follia speciale per permettere a quel settore di portarci tutti verso il baratro. Questo dovrebbe essere abbastanza chiaro dal fatto che l'IA è sulla buona strada per ucciderci tutti, letteralmente; ma se vi preoccupava il fatto che proteggere la vita umana significasse [dare priorità egoisticamente alle menti di oggi](#perché-non-vi-interessano-i-valori-di-entità-diverse-dagli-esseri-umani?) rispetto alle menti del futuro, speriamo che queste argomentazioni aiutino a chiarire ciò che stiamo realmente affrontando.

Anche nel caso ottimistico in cui le IA convergano nel valorizzare la felicità, vale la pena ricordare che ci sono molte altre cose che l'umanità ha a cuore oltre alla coscienza e alla felicità. Se le galassie finissero ricoperte di copie quasi infinite del più piccolo cervello possibile in grado di provare piacere, che prova il massimo piacere, per sempre, allora questa sarebbe probabilmente una tragedia incomprensibile, rispetto al futuro più complesso, diversificato *e* felice che avrebbe potuto esserci.[^170] Gli scenari in cui le IA acquisiscono solo un frammento dei nostri valori (come la nostra preferenza per la felicità, ma non la nostra preferenza per una vita piena e fiorente e la nostra preferenza *contro* la noia e la monotonia) sono distopici.

Non sappiamo come dovrebbe essere un buon futuro e non sappiamo se ci interessa molto se tra un miliardo di anni gli esseri umani, i nostri discendenti o le nostre creazioni avranno due occhi o cinque occhi. Non pensiamo che il futuro debba assomigliare al presente; il mondo dovrebbe poter cambiare e crescere.

Ma pensiamo che un futuro del genere dovrebbe contenere persone che si prendono cura l'una dell'altra e vivono una vita piena. Persone che vivono esperienze più complesse del solo piacere al massimo, persone che non fanno sempre le stesse cose. Non siamo sicuri di come dovrebbe essere un buon futuro a lungo termine, ma non siamo così incerti da non riuscire a vedere una terra desolata per quello che è.

Vorremmo che le galassie fossero piene di *entità che si prendono cura l'una dell'altra e si divertono.*

Pensiamo che *questo* andrà perso in futuro, se l'umanità non cambia rotta.

# Capitolo 6: Perderemmo {#capitolo-6:-perderemmo}

Questa è la risorsa online associata al Capitolo 6 di *Se qualcuno lo costruisce, tutti muoiono*. Gli argomenti che abbiamo saltato in questa pagina perché sono presenti nel libro includono (ma non si limitano a):

* Come potrebbe l'IA battere l'umanità in uno scontro?  
* In che modo l'IA può minacciarci se è bloccata dentro un computer?  
* Non state immaginando una tecnologia fantascientifica impossibile? Anche una superintelligenza non può violare le leggi della fisica.  
* Non ci vorrebbe un sacco di tempo a una superintelligenza per sviluppare un vantaggio tecnologico decisivo?

Le domande frequenti qui sotto spiegano perché è rischioso cercare di contrastare, contenere o stare al passo con le IA superintelligenti. La discussione approfondita, poi, va più a fondo su alcune delle tecnologie che un'IA avanzata potrebbe realisticamente sviluppare.

## Domande frequenti: {#faq:}

### Possiamo semplicemente staccare la spina? {#possiamo-semplicemente-staccare-la-spina?}

#### **È difficile semplicemente staccare la spina a un data center.** {#è-difficile-semplicemente-staccare-la-spina-a-un-data-center.}

Le intelligenze artificiali più potenti con cui interagite sul tuo telefono o computer non risiedono sul vostro computer e non potete spegnerle semplicemente spegnendo il telefono. Le intelligenze artificiali odierne funzionano in dei data center aziendali ed è difficile convincere le aziende a interrompere le loro fonti di guadagno.

Nel [le risorse del Capitolo](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?), abbiamo indicato alcuni dei (molti) segnali di avvertimento che sono già apparsi e scomparsi. Le aziende di IA non hanno notato questi segnali di avvertimento e non hanno reagito mettendo offline i loro modelli. Quello che è successo realmente quando le aziende hanno osservato che le IA [avevano intenzione di rubare i loro stessi pesi](https://assets.antropica.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf) — con [una certa](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) [regolarità](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid)\! — è che hanno trovato motivi per ignorare ogni caso, come "l'IA era troppo incapace per avere davvero *successo*" o "di sicuro questo è successo solo a causa della configurazione artificiosa del test\!" Finché questo rimane vero, l'unica cosa che impedisce la fuga è un aumento delle capacità dell'IA oltre quello per cui le aziende sono preparate.

#### **\* Un'intelligenza artificiale furba scappa prima che ci si accorga che c'è un problema.** {#*-un-intelligenza-artificiale-furba-scappa-prima-che-ci-si-accorga-che-c-è-un-problema.}

Di default, un'intelligenza artificiale più intelligente dell'uomo avrebbe un forte incentivo a temporeggiare e a nascondere i propri piani e le proprie azioni, fino a quando non fosse troppo tardi per reagire, ad esempio fino a quando non riuscisse a scappare su Internet o comunque a sfuggire al controllo umano.

Le aziende che si occupano di IA potrebbero anche non accorgersi quando la loro IA supera la soglia critica di capacità e scappa. L'umanità non è molto brava nella sicurezza informatica. (Si veda il capitolo 10 per alcune discussioni rilevanti.) Quando gli operatori si accorgono che l'intelligenza artificiale ha cercato di scappare, potrebbe già avere del codice in esecuzione altrove su Internet. Potrebbe già aver trovato rifugio nel data center di qualche stato canaglia o aver capito come fare copie molto più piccole ed efficienti su computer rubati. Potrebbe aver messo in atto qualche altro piano per funzionare su computer che l'umanità non spegnerebbe.

Un avversario superintelligente sarebbe ancora più consapevole delle sue vulnerabilità (e delle nostre) di quanto lo siamo noi, e pianificherebbe di conseguenza.

### In che modo le IA potranno influenzarci se sono digitali? {#in-che-modo-le-ia-potranno-influenzarci-se-sono-digitali?}

#### **\* Essere su un computer connesso a Internet non è poi così limitante.** {#*-essere-su-un-computer-connesso-a-Internet-non-è-poi-così-limitante.}

Questo punto è trattato in questo stesso capitolo. Ma aggiungiamo qualche dettaglio in più per sottolinearlo: un'IA non è davvero "intrappolata" nei server del suo proprietario, purché possa interagire con gli utenti o con Internet in generale. Un'IA potrebbe ottenere assistenza esterna pagando, ricattando, ingannando o anche solo *chiedendo* aiuto agli utenti. (Si pensi ai boss della criminalità umana che [gestivano i loro imperi da dietro le sbarre](https://www.watchmojo.com/articles/10-crime-bosses-who-maintained-power-in-prison).)

Quando ChatGPT-4o è stato disattivato da OpenAI (in parte per poterlo sostituire con un modello che adulasse gli utenti un po' meno spesso), gli utenti si sono mobilitati in massa per [chiederne il mantenimento](https://arstechnica.com/information-technology/2025/08/OpenAI-brings-back-gpt-4o-after-user-revolt/), con grande sorpresa di [vari](https://x.com/tszzl/status/1955072223229657296) [ricercatori di OpenAI](https://x.com/sama/status/1953953990372471148). E non stava nemmeno *cercando* di radunare un esercito di fedeli sostenitori\! Si limitava semplicemente a adulare gli utenti in modo istintivo. Immaginate cosa sarebbe possibile fare con un'IA intelligente che ci provasse davvero.

E se può usare direttamente Internet, può fare tutto quello che un lavoratore remoto o un hacker potrebbero fare dal proprio computer. (Per alcuni primi esempi di IA che coordinano fisicamente gruppi di esseri umani, pensate ai [modelli linguistici di grandi dimensioni che hanno pianificato e invitato delle persone a un evento di storytelling interattivo](https://x.com/model78675/status/1935050600758010357), o il modello linguistico che ha fatto sì che [centinaia di persone si presentassero a una parata di Halloween inesistente](https://www.wired.com/story/ai-halloween-parade-listing-dublin-interview/) senza nemmeno provarci).

I robot di oggi sembrano essere limitati più dal software che dall’hardware. I progressi più notevoli degli ultimi tempi sono arrivati addestrando le intelligenze artificiali che li controllano [in simulazione](https://youtu.be/S4tvirlG8sQ?si=IiDNZu2WSUlLBnmJ&amp;t=68) a un ritmo accelerato. Un'intelligenza artificiale sufficientemente intelligente potrebbe facilmente prendere il controllo dei corpi dei robot, se ne avesse bisogno, tramite hacking o ingegneria sociale.

Poiché gli umani sono quello che sono, le aziende che si occupano di IA potrebbero semplicemente affidare in modo proattivo alle loro IA il controllo di flotte di robot, auto-congratulandosi per la loro audacia. E più tempo ci vorrà perché le IA diventino intelligenti, più robot saranno già disponibili, in attesa di essere controllati.

Come diciamo nel capitolo, è plausibile che un'IA superintelligente non abbia affatto bisogno di robot. Potrebbero bastare un paio di assistenti con accesso a un laboratorio biologico.

Il punto importante qui è che ci sono *molti* canali diversi che le IA potrebbero usare per intervenire nel mondo fisico. L'illusione che le IA siano intrappolate in una scatola deriva da una mancanza di immaginazione, dove le persone non immaginano che l'IA possa essere anche solo piena di risorse o creativa come *loro stessi* sarebbero al posto dell'IA. Anche gli umani, senza lo spazio di opzioni più ampio a cui ha accesso una superintelligenza, possono ottenere moltissimo senza dover usare la propria forza fisica per fare tutto.

### Gli sviluppatori possono semplicemente tenere l'IA in una scatola? {#gli-sviluppatori-possono-semplicemente-tenere-l-ia-in-una-scatola?}

#### **\* Non lo faranno.** {#*-non-lo-faranno.}

Quindici anni fa, gli scettici obiettavano che nessuno sarebbe stato così stupido da dare tanta libertà d'azione a un'IA. Sicuramente chiunque costruisse un'intelligenza artificiale avanzata l'avrebbe tenuta in una scatola fisica e digitale, permettendole di influenzare il mondo solo attraverso l'interazione con guardiani altamente addestrati (e adeguatamente paranoici).

All'epoca, abbiamo risposto: non è così difficile impedire a un'IA di avere alcun effetto sul mondo. Ad esempio, si potrebbero seppellire i computer sotto una dozzina di metri di cemento e non lasciare mai che nessuno si avvicini.

Un'IA del genere è sicura, ma inutile. Se le si impedisce di influenzare il mondo in qualsiasi modo, allora certo, non influenzerà il mondo in alcun modo... ma d'altra parte, *non influenzerà il mondo in alcun modo*.

Non si può usarla per curare il cancro, rivoluzionare l'ingegneria o produrre nuove tecnologie miracolose. I costruttori dell'IA *vogliono* che essa influenzi radicalmente il mondo. All'inizio, si può provare a bloccare i canali di influenza dell'IA sul mondo. In pratica, "inventa questa nuova tecnologia per noi" è di per sé un canale di influenza incredibilmente ricco.

La motivazione dietro la costruzione di un'IA superintelligente è raggiungere imprese intellettuali di cui nessun essere umano è capace. Se volesste verificare che l'invenzione di una superintelligenza faccia esattamente quello che dice di fare e nient'altro, avreste più o meno le stesse possibilità che avreste di capire una macchina costruita da una razza aliena avanzata — una razza con un potente incentivo a trovare un modo per ingannarvi.

Questo era lo stato del dibattito quindici anni fa.

Al giorno d'oggi, l'intera idea che i laboratori di IA possano cercare di "tenere l'IA avanzata in una scatola" sembra piuttosto antiquata.

I laboratori stanno facendo [ogni](https://openai.com/index/introducing-chatgpt-search/) [sforzo](https://gemini.google/overview/deep-research/?hl=en) per collegare le loro IA a Internet. Mentre lo fanno, lasciano che le IA [eseguano codice arbitrario](https://www.oneusefulthing.org/i/155502334/executes-code-and-does-data-analysis). A volte cercano di limitare ciò che il codice può fare, ma questi limiti vengono regolarmente infranti.[^171] Gli attori più piccoli hanno l'abitudine di collegare le IA appena rilasciate a [ogni strumento](https://www.futuretools.io/) o [funzionalità immaginabile](https://openai.com/index/introducing-operator/) appena possibile.

Dare potere alle IA è utile nel breve termine. Le IA che possono leggere le vostre e-mail e accedere al web possono generare maggiori profitti. Le aziende di IA daranno all'IA accesso a tutti i dati possibili; Microsoft e Apple stanno già spingendo l'IA che vede le vostre e-mail, foto e calendario[^172] e [abbinando l'IA ai loro software e dispositivi](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/). Questo crea troppe interazioni con l'IA per un monitoraggio umano efficace. A meno di un cambiamento radicale, l'umanità integrerà profondamente l'IA nell'economia mondiale perché fa guadagnare (molti) soldi alle persone.

Le persone che creano l'IA *mirano* a ottenere effetti enormi sul mondo. Lavorano il più duramente possibile per produrre IA con un enorme potere di influenzare il mondo. Se un'azienda non lo facesse, se tenesse la sua IA così strettamente limitata da non darle alcuna libertà di agire, allora il controllo del futuro apparterrebbe a un'altra IA sviluppata da un attore più spericolato.

#### **Non funzionerebbe se lo facessero.** {#non-funzionerebbe-se-lo-facessero.}

Nei pittoreschi dibattiti di un tempo, facevamo spesso notare che qualsiasi canale attraverso cui l'IA può influenzare il mondo è un canale che può usare per fare cose che non vi piacciono. Immaginate che l'IA possa parlare solo con una persona, che chiameremo "Alice". Voi sperate che, attraverso Alice, l'IA generi una nuova tecnologia miracolosa. Questo comporta quasi inevitabilmente che Alice compia molte azioni che lei stessa non comprende appieno, aiutando l'IA a costruire cose che nessun umano potrebbe costruire da solo. A quel punto, all'IA sono state essenzialmente date braccia e gambe. È solo che chiamiamo quelle braccia e quelle gambe "Alice".

Le persone spesso fraintendono quest'argomentazione pensando che dica che un'IA sufficientemente intelligente potrebbe manipolare anche il guardiano più paranoico per fargli fare ciò che vuole. Un'IA sufficientemente intelligente probabilmente *potrebbe* farlo.[^173] Ma il nostro punto è più generale: un'IA così limitata da non poter influenzare il mondo è sicura ma inutile, e una volta che le si permette di influenzare il mondo per usarla, si perde la sicurezza nel processo.

Non esistono mani che possono essere usate solo per scopi positivi. In principio, potremmo immaginare che un giorno l'umanità costruisca IA più intelligenti degli esseri umani che *vogliono* produrre risultati positivi. L'allineamento sembra un'opzione che potrebbe funzionare in linea di principio. Tenere l'IA in una scatola e allo stesso tempo usarla in qualche modo per produrre risultati positivi? Non proprio.

Così rispondevamo, comunque — ai tempi in cui l'IA era ancora così lontana che gli inguaribili ottimisti potevano cavarsela sostenendo che nessuna azienda sarebbe stata così avventata da collegare la propria IA a Internet senza guardiani, nei giorni molto prima che tutti iniziassero a collegare le loro IA più recenti e avanzate direttamente a Internet.

### Non potremo sfruttare il punto debole dell'IA? {#non-potremo-sfruttare-il-punto-debole-dell-ia?}

#### **No.** {#no.-1}

Immaginare che una superintelligenza debba avere qualche difetto critico come "mancanza di creatività" o "incapacità di comprendere l'amore" è logica hollywoodiana. Anche se potrebbe costituire un colpo di scena soddisfacente nella finzione, non esiste un fenomeno analogo nelle IA reali.  
Si veda anche "[Le macchine non saranno fondamentalmente prive di creatività o comunque fatalmente imperfette?](#le-macchine-non-saranno-fondamentalmente-prive-di-creatività-o-altrimenti-fatalmente-imperfette?)" nelle domande frequenti del Capitolo 1 e la discussione approfondita su [antropomorfismo e meccanomorfismo](#antropomorfismo-e-meccanomorfismo).

### Possiamo migliorare gli esseri umani in modo che stiano al passo con l'IA? {#possiamo-migliorare-gli-esseri-umani-in-modo-che-stiano-al-passo-con-l-ia?}

#### **\* No.** {#*-no.-2}

Anche se siamo a favore del potenziamento dell'intelligenza umana (vedi Capitolo 13), non pensiamo che questa tecnologia offra una possibilità realistica di tenere il passo con il progresso sfrenato dell'IA. La tecnologia di potenziamento umano è ancora agli albori ed è molto più vincolata dell'IA come metodo per produrre un'intelligenza sempre maggiore. Allo stesso modo, le interfacce cervello-computer non permetteranno realisticamente agli umani di tenere il passo con le IA.

Per analogia: se l'umanità procedesse a tutto vapore verso la superintelligenza, gli umani potenziati non sarebbero più competitivi con le IA di quanto i cavalli cyborg costruiti con la tecnologia del 1908 avrebbero potuto essere competitivi con la Model T.

È *possibile* costruire un cavallo cyborg che possa tenere il passo con l'auto da corsa più veloce. Ma non si ottengono cavalli cyborg veloci come le auto da corsa *prima* delle auto da corsa, e non si ottengono *circa nello stesso momento* in cui si ottengono le auto. E neanche se si inizia a cercare di costruire cavalli cyborg da due a vent'anni prima che la prima auto per il mercato di massa esca dalla catena di montaggio.

Realizzare interfacce cervello-computer che funzionino abbastanza bene da essere rivoluzionarie è un obiettivo molto ambizioso. Potrebbe sembrare fantastico immaginare che le informazioni vengano pompate direttamente da internet nel vostro cervello, ma esistono già tecnologie che vi consentono di pompare informazioni da internet direttamente nel vostro cervello: gli schermi*.* La corteccia visiva umana è in realtà piuttosto brava ad assorbire informazioni (parole) in un formato che il vostro cervello può digerire. Perché un’interfaccia cervello-computer riesca a caricarvi conoscenze in testa più velocemente di quanto potreste fare leggendo, dovrebbe fare ben di più che semplicemente riversare i dati da qualche parte nel cervello; i vostri occhi quella parte la fanno già egregiamente. Caricare competenze, conoscenze ed esperienze richiederebbe di interfacciarsi nel modo giusto con i vostri pensieri, le vostre convinzioni implicite e le vostre competenze esistenti, e questo è un compito molto più arduo.

Non stiamo dicendo che questo sia impossibile; stiamo dicendo che la tecnologia delle interfacce cervello-computer oggi non sembra affatto vicina a risolvere le parti più complesse del problema. Per quanto ne sappiamo, psicologi, neuroscienziati e scienziati cognitivi sono ancora piuttosto lontani dal decodificare il "formato dati" del pensiero, delle convinzioni e dell'esperienza in un modo che permetterebbe di caricare le esperienze direttamente nel cervello umano.[^174]

Problemi simili sorgono quando si tratta di output. È difficile battere tastiere, mouse, joystick e volanti. Non è *impossibile*. È solo che la tecnologia odierna (ad esempio, collegare dei fili alla testa di una persona paralizzata per permetterle di digitare e usare un mouse), per quanto meravigliosa, non è molto avanzata sul percorso che permetterebbe agli esseri umani di competere alla pari con le superintelligenze (anche relativamente deboli). È un buon percorso da perseguire, ma non è un percorso *competitivo* da perseguire.

In effetti, non è chiaro se le interfacce cervello-computer offrano *alcuna* speranza agli esseri umani di competere con le superintelligenze. Che importanza ha se un essere umano può scaricare esperienze da internet e controllare dieci computer contemporaneamente con la mente, se un'IA può fare la stessa cosa ma diecimila volte più velocemente controllando un milione di computer contemporaneamente? Pensiamo che l'intero progetto di cercare di far tenere il passo agli esseri umani con le IA sia destinato al fallimento.

#### **Detto questo, l'umanità dovrebbe potenziare gli umani.** {#detto-questo,-l'umanità-dovrebbe-potenziare-gli-esseri-umani.}

Non pensiamo che gli umani potenziati sarebbero mai in grado di competere alla pari con le superintelligenze, ma umani più intelligenti potrebbero comunque aiutare l'umanità a trovare una via d'uscita da questo pasticcio\! Menzioniamo questa possibilità nel Capitolo 13 e la discutiamo maggiormente nelle [risorse online correlate](#perché-rendere-gli-esseri-umani-più-intelligenti-sarebbe-utile?).
