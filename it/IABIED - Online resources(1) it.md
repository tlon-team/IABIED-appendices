---
original_path: IABIED - Online resources(1).md
---
# Introduzione: Decisioni difficili e decisioni facili {#introduzione:-decisioni-difficili-e-decisioni-facili}

Questo è il primo supplemento online al libro di Yudkowsky e Soares [*If Anyone Builds It, Everyone Dies*](https://www.amazon.com/gp/product/0316595640). La pagina dedicata a ciascun capitolo risponde a domande comuni e include discussioni approfondite che entrano nel merito del contesto e dei dettagli non essenziali per il libro.

Queste risorse contengono molto materiale e non sono pensate per essere lette dall'inizio alla fine. Abbiamo scritto il libro perché fosse autosufficiente e coprisse l'intera tesi centrale. Ma se hai una domanda specifica, un'obiezione o una curiosità che il libro non ha potuto trattare in modo adeguato, è molto probabile che troverai ulteriori informazioni qui. Se manca qualcosa di importante, richiedilo \[qui\](TODO).

## Domande frequenti {#faq}

### Perché scrivere un libro sull'intelligenza artificiale superumana come minaccia di estinzione? {#perché-scrivere-un-libro-sull'intelligenza-artificiale-superumana-come-minaccia-di-estinzione?}

#### **Perché la situazione sembra davvero seria e urgente.** {#perché-la-situazione-sembra-davvero-seria-e-urgente.}

Se consideri attentamente un argomento, a volte puoi vedere arrivare uno degli zig o zag della storia.

Nel 1933, un fisico di nome Leo Szilard fu il primo a capire che le reazioni nucleari a catena erano possibili. In questo modo, riuscì a prevedere uno degli zig della storia prima di chiunque altro.

Pensiamo che se oggi si guarda all'intelligenza artificiale dal punto di vista giusto, si possa vedere arrivare uno degli zag della storia. E pensiamo che le cose andranno male se l'umanità non cambia rotta.

I laboratori di IA sono in gara per costruire macchine più intelligenti di qualsiasi essere umano e, a quanto pare, stanno facendo progressi significativi nel far progredire la frontiera. Come vedremo nei prossimi capitoli, le IA moderne sono più *cresciute* che costruite. Mostrano comportamenti che nessuno ha chiesto né voluto, e sono sulla buona strada per diventare più capaci di qualsiasi essere umano. Questa ci sembra una situazione estremamente pericolosa.

I migliori scienziati del settore si sono riuniti per firmare una [lettera aperta](https://aistatement.com/) in cui avvertono il pubblico che la minaccia dell'intelligenza artificiale dovrebbe essere considerata una "priorità globale al pari di altri rischi su scala sociale come le pandemie e la guerra nucleare". Non si tratta di una preoccupazione isolata, ma [condivisa da quasi la metà degli esperti del settore](#ai-experts-on-catastrophe-scenarios). Anche se inizialmente si è scettici sui pericoli, speriamo che il livello di preoccupazione espresso dagli esperti di IA e l'alta posta in gioco, se queste preoccupazioni dovessero rivelarsi fondate, chiariscano perché questo è un argomento che merita di essere discusso seriamente.

Questo è un argomento in cui dovremmo soppesare le argomentazioni invece di seguire semplicemente il nostro istinto. Se le lettere e gli avvertimenti sono corretti, il mondo si è messo in una situazione incredibilmente pericolosa. Dedicheremo il resto del libro a esporre le argomentazioni e le prove alla base di tali avvertimenti.

Non pensiamo che la situazione sia disperata. Abbiamo scritto questo libro con la speranza di cambiare la traiettoria che l'umanità sembra aver intrapreso, perché pensiamo che ci sia speranza di poter risolvere questo problema.

Il primo passo per risolvere un problema è capirlo.

### State suggerendo che ChatGPT potrebbe ucciderci tutti? {#are-you-suggesting-that-chatgpt-could-kill-us-all?}

#### **No. La preoccupazione riguarda i prossimi progressi nell'intelligenza artificiale.** {#no.-la-preoccupazione-riguarda-i-prossimi-progressi-nell-intelligenza-artificiale.}

Uno dei motivi per cui state leggendo questo libro è che sviluppi come ChatGPT hanno portato l'intelligenza artificiale alla ribalta. Il mondo sta iniziando a discutere dei progressi dell'IA e del suo impatto sulla società. Questo rappresenta un'opportunità naturale per parlare di un'IA più intelligente dell'uomo e di come la situazione attuale non prometta bene.

Noi, gli autori, lavoriamo in questo campo da molto tempo. I recenti progressi dell'IA modellano le nostre opinioni, ma le nostre preoccupazioni non sono nate con ChatGPT, né con i precedenti modelli linguistici di grandi dimensioni. Da decenni conduciamo ricerca tecnica per tentare di assicurare che lo sviluppo di un'IA più intelligente dell'uomo abbia esiti positivi (Soares dal 2013, Yudkowsky dal 2001). Di recente, però, abbiamo avuto prova che il mondo potrebbe essere pronto per questa conversazione. Ed è una conversazione che, con ogni probabilità, *dobbiamo* avere ora, o il mondo potrebbe perdere la sua finestra di opportunità per rispondere.

Il campo dell'IA sta progredendo e alla fine (non sappiamo quando) arriverà al punto di creare un'IA più intelligente di noi. Questo è l'obiettivo esplicito di tutte le principali aziende di IA:

> Ora siamo sicuri di sapere come costruire l'IAG [intelligenza artificiale generale] come l'abbiamo sempre intesa. [...] Stiamo iniziando a puntare oltre, verso la superintelligenza nel vero senso della parola. Amiamo i nostri prodotti attuali, ma siamo qui per il futuro glorioso. Con la superintelligenza, potremo fare tutto il resto.  
— [Sam Altman](https://blog.samaltman.com/reflections), CEO di OpenAI

> Penso che l'\[IA potente\] potrebbe arrivare già nel 2026\. \[…\] Per IA potente intendo un modello di IA \[…\] con le seguenti caratteristiche: in termini di pura intelligenza, è più intelligente di un premio Nobel nella maggior parte dei campi rilevanti — biologia, programmazione, matematica, ingegneria, scrittura, ecc. Questo significa che può dimostrare teoremi matematici irrisolti, scrivere romanzi di altissimo livello, scrivere da zero codebase complesse, ecc.  
— [Dario Amodei](https://www.darioamodei.com/essay/machines-of-loving-grace), CEO di Anthropic

> Nel complesso, ci stiamo concentrando sulla creazione di un'intelligenza generale completa. Tutte le opportunità di cui ho parlato oggi derivano dalla realizzazione di un'intelligenza generale e dal farlo in modo efficiente.  
— [Mark Zuckerberg](https://www.facebook.com/share/p/16STVBshtn/), CEO di Meta (poco prima che l'azienda [annunciasse](https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26) un [progetto di "superintelligenza"](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta) da 14,3 miliardi di dollari)

> Penso che nei prossimi cinque-dieci anni ci sarà forse il 50 % di possibilità che avremo quello che definiamo AGI.  
— [Demis Hassabis](https://youtu.be/CRraHg4Ks_g?feature=shared&t=41), CEO di Google DeepMind

> Wes: Allora, Demis, stai cercando di provocare un'esplosione di intelligenza?  
Demis: No, non una incontrollata...  
— [Wes Roth (intervistatore) e Hassabis](https://x.com/WesRothMoney/status/1926669591163621789)

Stanno passando dalle parole ai fatti. [Microsoft](https://www.reuters.com/technology/artificial-intelligence/microsoft-plans-spend-80-bln-ai-enabled-data-centers-fiscal-2025-cnbc-reports-2025-01-03/), [Amazon](https://www.datacenterdynamics.com/en/news/amazon-2025-capex-to-reach-100bn-aws-revenue-hit-100bn-in-2024/), e [Google](https://www.datacenterdynamics.com/en/news/google-expects-2025-capex-to-surge-to-75bn-on-ai-data-center-buildout/) hanno annunciato che nel 2025 investiranno tra i 75 e i 100 miliardi di dollari in data center basati sull'intelligenza artificiale. La startup xAI ha comprato il sito di social media X.com per 80 miliardi di dollari, quasi il doppio del valore di X, poco prima di raccogliere 10 miliardi di dollari per sostenere un enorme data center e sviluppare ulteriormente la sua IA, Grok. OpenAI ha annunciato il [Progetto Stargate](https://openai.com/index/announcing-the-stargate-project/) da 500 miliardi di dollari, in collaborazione con Microsoft e altri.

Il CEO di Meta, Mark Zuckerberg, ha [dichiarato](https://www.datacenterdynamics.com/en/news/zuckerberg-says-meta-will-spend-hundreds-of-billions-of-dollars-on-ai-infrastructure-over-the-long-term/) che Meta [prevede di spendere 65 miliardi di dollari](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/) in infrastrutture di IA quest'anno e "centinaia di miliardi" in progetti di IA nei prossimi anni. Meta ha già investito 14,3 miliardi di dollari in ScaleAI e ha assunto il suo CEO per dirigere i nuovi [Meta Superintelligence Labs](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires), sottraendo nel processo oltre una dozzina di ricercatori di punta dai laboratori rivali[^2] con offerte che arrivano fino a 200 milioni di dollari per un singolo ricercatore.

Tutto questo non significa che un'intelligenza artificiale più intelligente dell'uomo sia dietro l'angolo. Ma significa che tutte le grandi aziende stanno cercando con ogni mezzo di crearla e che le IA come ChatGPT sono il risultato di questo programma di ricerca. Queste aziende non stanno cercando di creare chatbot. Stanno cercando di creare superintelligenze e i chatbot sono solo una tappa lungo il percorso.

La nostra opinione, dopo decenni passati a cercare di comprendere meglio questa questione e a riflettere seriamente sugli sviluppi futuri, è che non esista una barriera di principio che impedisca ai ricercatori di ottenere una svolta domani e riuscire a costruire un'intelligenza artificiale più intelligente dell'uomo.

Non sappiamo se quella soglia sarà effettivamente raggiunta nel prossimo futuro o se ci vorrà ancora un decennio, ecc. La storia dimostra che prevedere i tempi delle nuove tecnologie è molto più difficile che prevedere se una tecnologia verrà sviluppata o meno. Tuttavia, riteniamo che le prove del pericolo siano enormemente maggiori di quelle necessarie per giustificare una risposta internazionale aggressiva oggi. Questo argomento è, naturalmente, delineato nel libro.

### Le persone non sono sempre nel panico e non reagiscono sempre in modo eccessivo alle cose? {#aren't-people-always-panicking-and-overreacting-to-things?}

#### **Sì. Ma questo non significa che nulla sia mai *davvero* pericoloso.** {#yes.-but-this-doesn't-mean-that-nothing-is-ever-actually-dangerous.}

A volte le persone reagiscono eccessivamente ai problemi. Alcune persone sono fataliste. Alcuni panici sociali sono infondati. Nulla di ciò significa che viviamo in un mondo perfettamente sicuro.

La Germania del 1935 non era un buon posto dove rimanere per ebrei, rom e vari altri gruppi di persone. Alcuni videro i segnali di pericolo e partirono. Altri liquidarono gli avvertimenti come allarmistici e morirono.

La minaccia di annientamento nucleare era reale, ma l'umanità si è dimostrata all'altezza della situazione e la Guerra Fredda non si è mai trasformata in guerra calda.

I clorofluorocarburi stavano davvero bruciando un buco nello strato di ozono, finché non sono stati vietati con successo da un trattato internazionale. In seguito, lo strato di ozono si è ripreso.

Alcuni pericoli di cui veniamo avvertiti sono falsi. Altri sono reali.

L'umanità non sempre reagisce eccessivamente a una sfida. Né sempre insufficientemente. In alcuni casi, riesce persino a fare entrambe le cose contemporaneamente: ad esempio, quando dei Paesi costruiscono enormi navi da guerra per la guerra successiva, quando in realtà avrebbero dovuto costruire portaerei. Non esiste una soluzione semplice come "ignorare ogni presunto rischio tecnologico" o "presumere che ogni rischio tecnologico sia reale". Per capire cosa è vero, bisogna esaminare i dettagli di ogni singolo caso.

(Per approfondire l'argomento, si rimanda all'introduzione del libro).

### Quando verrà sviluppato questo tipo di IA preoccupante? {#when-is-this-worrisome-sort-of-ai-going-to-be-developed?}

#### **\* Sapere che una tecnologia è in arrivo non significa sapere esattamente quando arriverà.** {#*-knowing-that-a-technology-is-coming-doesn’t-grant-knowledge-of-exactly-when-it’s-coming.}

Molte delle cose che ci vengono chieste di prevedere, in realtà non abbiamo modo di saperle. Quando Leo Szilard scrisse una lettera per mettere in guardia gli Stati Uniti sulle armi nucleari nel 1939, non incluse, né avrebbe potuto includere, alcuna nota del tipo: "La prima arma atomica sarà pronta per essere testata tra sei anni".

Sarebbe stata un'informazione preziosissima! Ma anche se si è i primi a prevedere correttamente le reazioni nucleari a catena, come fu Szilard — anche se si è i primi in assoluto a capire che una tecnologia è possibile e che avrà un impatto notevole — non si può prevedere esattamente quando quella tecnologia arriverà.

Ci sono previsioni facili e previsioni difficili. Non pretendiamo di saper fare quelle difficili, come prevedere esattamente quando verrà prodotto il tipo di IA pericolosa.

#### **Gli esperti continuano a essere sorpresi dalla rapidità dei progressi dell'IA.** {#experts-keep-being-surprised-by-how-fast-ai-progress-happens.}

Non sapere quando arriverà l'IA non significa che sia ancora lontana.

Nel 2021, la comunità di previsione sul sito web Metaculus [ha stimato](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) che la prima "IA veramente generale" sarebbe arrivata nel 2049. Un anno dopo, nel 2022, quella previsione aggregata della comunità era scesa di dodici anni, al 2037. Un altro anno dopo, nel 2023, era scesa di ulteriori quattro anni, al 2033. [Ripetutamente](https://x.com/slow_developer/status/1947248501743599705) e [costantemente](https://forecastingresearch.org/near-term-xpt-accuracy), i previsori sono stati sorpresi dal ritmo veloce del progresso dell'IA, con le loro stime temporali che variano drasticamente di anno in anno.

Questo fenomeno non riguarda solo Metaculus. Un'organizzazione chiamata 80,000 Hours [documenta](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/) vari altri casi di tempistiche che si accorciano rapidamente da parte di molti gruppi di esperti previsori. E persino i superprevisori — che vincono costantemente i tornei di previsione e spesso superano gli esperti di settore nella loro capacità di prevedere il futuro — hanno assegnato solo il [2,3 % di probabilità](https://forecastingresearch.org/near-term-xpt-accuracy) che le IA conseguissero la medaglia d'oro alle Olimpiadi Internazionali di Matematica entro l'anno 2025. Le IA [hanno conseguito](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) la medaglia d'oro alle Olimpiadi Internazionali di Matematica nel luglio del 2025. 

Un'IA più intelligente dell'uomo potrebbe sembrare intuitivamente lontana decenni, ma anche l'IA di livello ChatGPT sembrava lontana decenni nel 2021, e poi all'improvviso è arrivata. Chi può sapere quando arriveranno improvvisamente nuovi miglioramenti qualitativi dell'IA? Forse ci vorranno altri dieci anni. O forse una svolta arriverà domani. Non sappiamo quanto tempo ci vorrà, ma un numero crescente di ricercatori è sempre più preoccupato che il tempo possa scarseggiare. Senza rivendicare conoscenze speciali su questo fronte, pensiamo che l'umanità dovrebbe reagire presto. Non è chiaro quanti altri avvertimenti riceveremo mai.

Vedi il Capitolo 1 per ulteriori discussioni sui modi in cui le capacità dell'IA potrebbero manifestarsi a cascata con pochissimo preavviso. E vedi il Capitolo 2 per ulteriori discussioni sui paradigmi moderni dell'IA, e se saranno o meno in grado di arrivare "fino in fondo".

#### **Siate sospettosi delle affermazioni dei media su cosa può e non può accadere presto. (Potrebbe essere già successo!)** {#be-suspicious-of-media-claims-about-what-can-and-can't-happen-soon.-(it-may-have-already-happened!)}

Due anni dopo la [previsione sconsolata](https://www.wright-brothers.org/History_Wing/Wright_Story/Inventing_the_Airplane/Not_Within_A_Thousand_Years/Not_Within_A_Thousand_Years.htm) di Wilbur Wright secondo cui il volo a motore avrebbe richiesto mille anni, il *New York Times* affermò con sicurezza che ce ne sarebbero voluti un milione.[^3] Due mesi e otto giorni dopo, i fratelli Wright volarono.

Oggi, gli scettici continuano a fare affermazioni esagerate sul fatto che l'IA non potrebbe mai rivaleggiare con gli esseri umani in qualche capacità specifica, anche mentre i recenti progressi con l'apprendimento automatico mostrano che le IA eguagliano (o superano) le prestazioni umane su una lista crescente di benchmark. È noto almeno dalla fine del 2024, per esempio, che le IA moderne possono spesso identificare il sarcasmo e l'ironia dal [testo](https://www.yomu.ai/resources/can-ai-essay-writers-understand-satire-irony-or-sarcasm-in-essays#) e persino dai [segnali non verbali](https://dl.acm.org/doi/10.1145/3678957.3685723). Ma questo non ha impedito al *New York Times* di [ripetere](https://www.nytimes.com/2025/05/16/technology/what-is-agi.html) l'affermazione nel maggio 2025 che "gli scienziati non hanno prove concrete che le tecnologie di oggi siano capaci di eseguire anche solo alcune delle cose più semplici che il cervello può fare, come riconoscere l'ironia".[^4]

Tutto questo per dire: molti affermeranno di sapere che un'IA più intelligente dell'uomo è imminente, o che è incalcolabilmente lontana nel futuro. Ma la scomoda realtà è che nessuno lo sa in questo momento.

Peggio ancora, c'è una forte possibilità che nessuno *lo saprà mai* fino a quando non sarà troppo tardi perché la comunità internazionale possa fare qualcosa al riguardo.

Prevedere i tempi della prossima svolta tecnologica è incredibilmente difficile. Sappiamo che un'IA più intelligente dell'uomo è letalmente pericolosa, ma se abbiamo anche bisogno di sapere in quale giorno della settimana arriverà, allora siamo sfortunati. Dobbiamo essere in grado di agire da una posizione di incertezza, o non agiremo affatto.

### Possiamo usare i progressi passati per estrapolare quando costruiremo un'IA più intelligente dell'essere umano? {#possiamo-usare-i-progressi-passati-per-estrapolare-quando-costruiremo-un-ia-più-intelligente-dell-essere-umano?}

#### **Non abbiamo una comprensione sufficientemente buona dell'intelligenza per farlo.** {#non-abbiamo-una-comprensione-sufficientemente-buona-dell-intelligenza-per-farlo.}

Una classe di previsioni di successo consiste nel prendere una linea retta su un grafico, che è rimasta costante per molti anni, e prevedere che la linea retta continui per almeno un altro anno o due.

Questo non funziona sempre. Le linee di tendenza a volte cambiano. Ma spesso funziona ragionevolmente bene; è un caso in cui le persone fanno previsioni di successo nella pratica.

Il grande problema con questo metodo è che spesso quello che vogliamo davvero sapere non è "quanto sarà alta questa linea sul grafico entro il 2027?" ma piuttosto "Cosa succede, qualitativamente, se questa linea continua a salire?" Quale altezza della linea corrisponde a risultati importanti nel mondo reale?

E nel caso dell'IA, semplicemente non lo sappiamo. È abbastanza facile scegliere una qualche misura dell'intelligenza artificiale che formi una linea retta su un grafico (come la "[perplessità](https://en.wikipedia.org/wiki/Perplexity)") e proiettare quella linea in avanti. Ma nessuno sa quale livello futuro di "perplessità" corrisponda a quale livello qualitativo di abilità nel gioco degli scacchi. Le persone non possono prevederlo in anticipo; devono semplicemente far girare l'IA e scoprirlo.

Nessuno sa dove cada la linea "ora ha la capacità di uccidere tutti" su quel grafico. Tutto quello che possono fare è far girare l'IA e scoprirlo. Quindi estrapolare la linea retta sul grafico non ci aiuta. (E questo prima ancora che il grafico venga reso irrilevante dal progresso algoritmico.)

Per questo motivo, nel libro non passiamo tempo a estrapolare linee sui grafici per prevedere esattamente quando qualcuno impiegherà 10²⁷ operazioni in virgola mobile per addestrare un'IA, o quali conseguenze questo avrebbe. È una previsione difficile. Il libro si concentra su quelle che ci sembrano le previsioni facili. Si tratta di una gamma ristretta di argomenti, e la nostra capacità di fare un piccolo numero di previsioni importanti in quel dominio ristretto non giustifica l'elaborazione di pronostici arbitrari sul futuro.

### Quali sono i vostri incentivi e conflitti di interesse, in qualità di autori? {#what-are-your-incentives-and-conflicts-of-interest,-as-authors?}

#### **Non ci aspettiamo di guadagnare denaro dal libro nel caso medio. Separatamente, ci piacerebbe moltissimo sbagliarci sulla tesi del libro.** {#non-ci-aspettiamo-di-guadagnare-denaro-dal-libro-nel-caso-medio-separatamente-ci-piacerebbe-moltissimo-sbagliarci-sulla-tesi-del-libro.}

Noi (Soares e Yudkowsky) prendiamo lo stipendio dal Machine Intelligence Research Institute (MIRI), che è finanziato dalle donazioni di persone che pensano che questi temi siano importanti. Forse il libro stimolerà le donazioni.

Detto questo, abbiamo altre possibilità di guadagnare e non scriviamo libri per soldi. L'anticipo che abbiamo ricevuto per questo libro è stato interamente utilizzato per la pubblicità del libro stesso, e i diritti d'autore andranno interamente al MIRI per ripagare il tempo e l'impegno investiti dal personale.

E, ovviamente, entrambi gli autori sarebbero entusiasti di concludere che la nostra civiltà non è in pericolo. Ci piacerebbe semplicemente andare in pensione o fare soldi altrove.

Non pensiamo che avremmo difficoltà a cambiare idea, se davvero le prove lo giustificassero. È già successo in passato. Il MIRI è stato fondato (con il nome di "Singularity Institute") come progetto per *costruire* una superintelligenza. Ci è voluto un anno perché Yudkowsky capisse che questo non sarebbe andato *automaticamente* bene, e un altro paio d'anni perché capisse che farlo andare bene sarebbe stato piuttosto complicato.

Abbiamo già cambiato rotta una volta e saremmo felici di farlo di nuovo. Semplicemente non pensiamo che le prove lo giustifichino.

Non pensiamo che la situazione sia senza speranza, ma ci sembra che ci sia un problema reale e che la minaccia sia estrema se il mondo *non* si dimostra all'altezza della situazione.

Vale anche la pena sottolineare che per capire se l'IA sia destinata a ucciderci tutti, bisogna pensare all'*IA*. Se si pensa solo alle persone, si possono trovare motivi per ignorare qualsiasi fonte: gli accademici sono fuori dal mondo; le aziende cercano di creare clamore mediatico; le organizzazioni no profit vogliono raccogliere fondi; gli hobbisti non sanno di cosa stanno parlando.

Ma se si segue questa strada, le proprie convinzioni finali saranno determinate da chi si sceglie di ignorare, senza lasciare spazio ad argomenti e a prove che potrebbero far cambiare idea se ci si sbaglia. Per capire cosa è vero, non c'è alternativa alla valutazione degli argomenti e alla verifica della loro validità, indipendentemente da chi li ha sollevati.

Il nostro libro non inizia con la facile argomentazione che i dirigenti aziendali che gestiscono i laboratori di IA hanno un incentivo a convincere la popolazione che le IA sono sicure. Inizia discutendo dell'*IA*. E più avanti nel libro, dedichiamo un po' di tempo a *ripercorrere la storia degli scienziati umani che sono stati troppo ottimisti*, ma non diciamo mai che dovreste ignorare l'argomentazione di qualcuno perché lavora in un laboratorio di IA. Discutiamo alcuni dei *piani reali* degli sviluppatori e perché tali piani non funzionerebbero di per sé. Stiamo facendo del nostro meglio per sederci e discutere delle argomentazioni reali, perché sono queste che contano.

Se pensate che ci sbagliamo, vi invitiamo a confrontarvi con le nostre argomentazioni e a indicare i punti specifici in cui pensate che abbiamo sbagliato. Pensiamo che questo sia un modo più affidabile per capire cosa è vero rispetto al considerare principalmente il carattere e le motivazioni delle persone. La persona più prevenuta del mondo può dire che sta piovendo, ma questo non significa che ci sia il sole.

### Ma tutta questa storia dell'IA non è solo fantascienza? {#isn't-this-ai-stuff-just-science-fiction?}

#### **\* Non possiamo imparare molto dalla prevalenza di un argomento nella narrativa.** {#*-we-can't-learn-much-from-a-topic's-prevalence-in-fiction.}

Un'IA più intelligente dell'essere umano non è ancora stata costruita, ma è stata rappresentata nella narrativa. Sconsigliamo tuttavia di ancorarsi a queste rappresentazioni. L'IA reale probabilmente non sarà molto simile a quella immaginaria, per ragioni che approfondiremo nel Capitolo 4.

L'IA non è la prima tecnologia ad essere stata anticipata dalla narrativa. [Il volo più pesante dell'aria](https://www.weslpress.org/9780819577269/robur-the-conqueror/) e [il viaggio sulla luna](https://www.imdb.com/title/tt0000417/) furono entrambi rappresentati prima del loro tempo. E l'idea generale delle armi nucleari fu anticipata da H. G. Wells, uno dei primi scrittori di fantascienza, in un romanzo del 1914 intitolato [*The World Set Free*](https://ahf.nuclearmuseum.org/ahf/key-documents/hg-wells-world-set-free/). Wells non azzeccò i dettagli; scrisse di una bomba che continuava a bruciare intensamente per giorni, piuttosto che di una bomba che esplodeva tutta in una volta lasciando dietro di sé una morte persistente. Ma Wells aveva l'idea generale di una bomba che funzionava con energia nucleare anziché chimica.

Nel 1939, Albert Einstein e Leo Szilard inviarono una lettera al Presidente Roosevelt esortando gli Stati Uniti a cercare di superare la Germania nella costruzione di una bomba atomica. Potremmo immaginare un mondo in cui Roosevelt avesse incontrato per la prima volta la nozione di bombe nucleari nel romanzo di Wells, portandolo a liquidare l'idea come fantascienza.

Come andarono le cose, nella vita reale Roosevelt prese l'idea sul serio, almeno abbastanza da creare il Comitato Consultivo sull'Uranio. Ma questo caso dimostra il pericolo di liquidare le idee solo perché uno scrittore di narrativa ha parlato in passato di un'idea dal suono simile.

La fantascienza può fuorviarti perché presumi che sia vera, oppure può fuorviarti perché presumi che sia *falsa*. Gli autori di fantascienza non sono profeti, ma non sono nemmeno anti-profeti le cui parole sono garantite essere sbagliate. Nella stragrande maggioranza dei casi, è meglio ignorare la narrativa e analizzare tecnologie e scenari nei loro propri termini.

Per prevedere cosa accade nella realtà, non c'è sostituto al ragionare attraverso gli argomenti e soppesare le prove.

#### **Le conseguenze dell'IA saranno inevitabilmente strane.** {#the-consequences-of-ai-are-inevitably-going-to-be-weird.}

Comprendiamo la reazione secondo cui l'IA è *strana*, e che trasformerebbe il mondo e violerebbe lo status quo. Tutti noi abbiamo intuizioni adattate, in una certa misura, a un mondo in cui gli esseri umani sono l'unica specie capace di imprese come costruire una centrale elettrica. Tutti noi abbiamo intuizioni adattate a un mondo in cui le macchine, attraverso tutta la storia umana, sono sempre state strumenti non intelligenti. Una cosa di cui possiamo essere molto sicuri è che un futuro con IA più intelligenti dell'essere umano apparirebbe *diverso*.

Grandi cambiamenti duraturi nel mondo non avvengono tutti i giorni. L'euristica "non succede mai niente"[^6] funziona benissimo nella maggior parte dei casi, ma i momenti in cui fallisce sono alcuni dei più importanti della storia a cui prestare attenzione. Gran parte del senso di pensare al futuro sta proprio nell'anticipare quei momenti in cui accade qualcosa di grande, per potersi preparare.

Un modo per superare il pregiudizio verso lo status quo è ricordare il registro storico, come discusso nell'introduzione.

A volte, particolari invenzioni finiscono per rivoluzionare il mondo. Si consideri il motore a vapore e le molte altre tecnologie che ha contribuito a rendere possibili durante la Rivoluzione Industriale, trasformando rapidamente la vita umana:

![][image1]

L'avvento di un'IA veramente generale è uno sviluppo altrettanto rilevante? Sembra che l'intelligenza artificiale sarebbe *almeno* tanto rilevante quanto la Rivoluzione Industriale. Tra le altre cose:

* L'IA è probabile che consenta al progresso tecnologico di svilupparsi molto più velocemente. Come discuteremo nel Capitolo 1, le macchine possono operare molto più velocemente del cervello umano. E gli esseri umani possono migliorare l'IA — e l'IA alla fine sarà in grado di migliorare se stessa — fino a quando le macchine saranno di gran lunga migliori degli esseri umani nel fare scoperte scientifiche, inventare nuove tecnologie, eccetera.

  Per tutta la storia umana, il meccanismo del cervello umano è rimasto fondamentalmente invariato, anche mentre l'umanità produceva opere di ingegneria sempre più impressionanti. Quando il meccanismo della cognizione inizierà a migliorare di per sé, quando diventerà capace di migliorare se stesso, dovremmo aspettarci che *molte cose diverse* inizino a cambiare *molto rapidamente*.  
* Inoltre, come discuteremo nel Capitolo 3, le IA sufficientemente capaci probabilmente avranno obiettivi propri. Se le IA fossero essenzialmente solo esseri umani più veloci e più intelligenti, questo sarebbe già di per sé un fatto enorme. Ma le IA saranno invece, in effetti, una specie totalmente nuova di vita intelligente sulla Terra — una con i propri obiettivi, che probabilmente (come discuteremo nei Capitoli 4 e 5) divergeranno in modo importante dagli obiettivi umani.

A prima vista, sarebbe sorprendente se questi due importanti sviluppi potessero verificarsi *senza* sovvertire l'ordine mondiale esistente. Credere in un futuro "normale" sembra richiedere di credere che l'intelligenza delle macchine non supererà mai affatto l'intelligenza umana. Questo non è mai sembrato un'opzione veramente praticabile, ed è diventato molto più difficile da credere nel 2025 rispetto al 2015 o al 2005.

#### **Il futuro a lungo termine sarà ugualmente strano.** {#the-long-term-future-will-likewise-be-weird.}

Se guardi troppo lontano nel futuro, il risultato sarà in qualche modo strano. Il XXI secolo appare decisamente bizzarro dalla prospettiva del XIX secolo, che appariva bizzarro dalla prospettiva del XVII secolo. L'IA accelera questo processo e aggiunge un giocatore molto nuovo alla scacchiera.

Un aspetto del futuro che oggi sembra prevedibile è che le specie tecnologicamente avanzate non rimarranno bloccate sul proprio pianeta per sempre. In questo momento, il cielo notturno è pieno di stelle che stanno semplicemente bruciando la loro energia. Ma nulla impedisce alla vita di costruire la tecnologia necessaria per viaggiare tra le stelle e raccogliere quell'energia per qualche scopo.

Ci sono alcune limitazioni fisiche sulla *velocità* con cui si può viaggiare, ma sembra che non ci siano limiti alla possibilità di farlo alla fine.[^7] Nulla ci impedisce di sviluppare alla fine il tipo di sonde interstellari in grado di andare a estrarre risorse dall'universo in senso lato e convertire queste risorse in civiltà fiorenti, con l'aggiunta di sonde autoreplicanti per colonizzare ancora più regioni dello spazio. Se ci sostituiamo con le IA, nulla impedisce a queste ultime di fare lo stesso, sostituendo però le "civiltà fiorenti" con qualsiasi obiettivo perseguito dall'IA.

Allo stesso modo in cui la vita si è diffusa sulle rocce aride della Terra fino a riempire il mondo di organismi, possiamo aspettarci che la vita (o le macchine costruite dalla vita) si diffonda nelle parti disabitate dell'universo, fino a quando sarà strano trovare un sistema solare senza vita quanto lo sarebbe oggi trovare un'isola senza vita sulla Terra, priva persino di batteri.

Al momento, la maggior parte della materia nell'universo, come le stelle, è disposta in modo casuale. Ma in un futuro a lungo termine, quasi sicuramente la maggior parte della materia sarà disposta secondo un certo disegno, cioè secondo le preferenze di chiunque riesca a raccogliere e riutilizzare le stelle.

Anche se nulla sulla Terra si diffondesse mai nel cosmo, e anche se la maggior parte delle forme di vita intelligenti che nascono in galassie lontane non lasciasse mai il proprio pianeta natale, basta *una sola* intelligenza capace di viaggiare nello spazio in qualsiasi parte dell'universo per accendere la scintilla e iniziare a diffondersi nell'universo, viaggiando verso nuovi sistemi stellari e usando le risorse presenti per costruire altre sonde per espandersi verso altri sistemi stellari — proprio come è bastato un solo microrganismo autoreplicante (e un po' di crescita esponenziale) per trasformare un pianeta senza vita in un mondo brulicante di vita su ogni isola.

Quindi il futuro sarà diverso dal presente. Anzi, possiamo aspettarci che sarà radicalmente diverso. Le stelle stesse saranno prevedibilmente trasformate, nel lungo periodo, da qualsiasi specie biologica o IA alla ricerca di maggiori risorse — anche se oggi non possiamo dire molto su come potrebbe essere quella specie o a quali fini potrebbero essere destinate le risorse dell'universo.

Prevedere i *dettagli* sembra difficile, quasi impossibile. È una previsione difficile. Ma prevedere la trasformazione dell'universo in un luogo in cui la maggior parte della materia viene raccolta e utilizzata per *qualche* scopo, qualunque esso sia? Questa è una previsione più facile, anche se è controintuitiva e strana per una civiltà che ha appena iniziato a estrarre risorse dalle stelle.

Tra un milione di anni, non dovremmo aspettarci che il futuro assomigli al 2025, con un gruppo di scimmie senza peli che gironzolano sulla superficie della Terra. Molto prima di allora, o ci saremo autodistrutti, oppure i nostri discendenti saranno partiti per esplorare il cosmo.[^8]

Sarà sicuramente strano per l'umanità. La domanda è: quando?

#### **Il futuro ci colpirà in fretta.** {#il-futuro-ci-colpirà-in-fretta.}

Tecnologie come l'IA significano che il futuro potrebbe bussare presto alla nostra porta, e i suoi effetti potrebbero colpirci duramente.

La Rivoluzione Industriale ha trasformato il mondo molto rapidamente, secondo gli standard della storia pre-moderna. L'*Homo sapiens* ha rimodellato il mondo molto rapidamente, secondo gli standard dei processi evolutivi. La vita ha rimodellato il mondo molto rapidamente, secondo gli standard dei processi cosmologici e geologici. I nuovi processi per cambiare il mondo possono rimodellare il mondo molto rapidamente, se misurati con il vecchio standard.

L'umanità sembra essere sull'orlo di un'altra trasformazione radicale, dove le macchine possono iniziare a rimodellare il mondo a velocità meccaniche, che superano di gran lunga le velocità biologiche. Avremo più da dire nei Capitoli 1 e 6 su quanto bene l'intelligenza artificiale si misuri rispetto all'intelligenza umana. Ma come minimo, dobbiamo prendere sul serio la possibilità che lo sviluppo di macchine più intelligenti dell'uomo possa cambiare radicalmente il mondo ad alta velocità. Questo tipo di cose è accaduto ripetutamente nel corso del tempo.

# 

## Discussione approfondita {#discussione-approfondita}

### Esperti di IA sugli scenari catastrofici {#ai-experts-on-catastrophe-scenarios}

In un sondaggio del 2022 su 738 partecipanti alle conferenze accademiche sull'intelligenza artificiale NeurIPS e ICML, il 48 % degli intervistati pensava che ci fosse almeno il 10 % di possibilità che l'intelligenza artificiale portasse a conseguenze "estremamente negative (ad esempio, l'estinzione umana)". In questo campo, sono molto diffuse le preoccupazioni che l'IA possa causare disastri senza precedenti.

Di seguito abbiamo raccolto i commenti di eminenti scienziati e ingegneri specializzati in IA sui risultati catastrofici dell'IA. Alcuni di questi scienziati forniscono la loro "p(doom)", ovvero la probabilità che l'IA causi l'estinzione umana o risultati altrettanto disastrosi.[^9]

Da **Geoffrey Hinton** ([2024](https://youtu.be/PTF5Up1hMhw?t=2285)), che ha vinto il Premio Nobel e il Premio Turing per aver dato il via alla rivoluzione dell'apprendimento profondo nell'IA, ecco cosa dice sulle sue stime personali:

> In realtà penso che il rischio \[della minaccia esistenziale\] sia superiore al cinquanta per cento.

Da **Yoshua Bengio** ([2023](https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340)), vincitore del Premio Turing (insieme a Hinton e Yann LeCun) e lo scienziato vivente più citato:

> Non sappiamo quanto tempo abbiamo prima che la situazione diventi davvero pericolosa. Ciò che dico da alcune settimane è: "Per favore, datemi delle argomentazioni, convincetemi che non dobbiamo preoccuparci, perché così sarei molto più felice". E finora non è accaduto. \[…\] Direi circa un venti per cento di probabilità che la situazione diventi catastrofica.

Da **Ilya Sutskever** ([2023](https://openai.com/index/introducing-superalignment/)), co-inventore di AlexNet, ex capo scienziato di OpenAI e (insieme a Hinton e Bengio) uno dei tre scienziati più citati nel campo dell'IA:

> \[L\]'enorme potere della superintelligenza potrebbe anche essere molto pericoloso e potrebbe portare alla perdita di potere dell'umanità o addirittura all'estinzione umana. Anche se la superintelligenza sembra ancora lontana, crediamo che potrebbe arrivare entro questo decennio. \[…\]
>
> Al momento, non abbiamo una soluzione per guidare o controllare un'IA potenzialmente superintelligente e impedire che diventi incontrollabile. Le nostre attuali tecniche per l'allineamento dell'IA, come l'apprendimento per rinforzo dal feedback umano, si basano sulla capacità degli esseri umani di supervisionare l'IA. Ma gli esseri umani non saranno in grado di supervisionare in modo affidabile sistemi di IA molto più intelligenti di noi, quindi le nostre attuali tecniche di allineamento non si adatteranno alla superintelligenza. Abbiamo bisogno di nuove scoperte scientifiche e tecniche.

Da **Jan Leike** ([2023](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)), co-responsabile della scienza dell'allineamento presso Anthropic ed ex co-responsabile del team di superallineamento presso OpenAI:

> \[intervistatore: "Non ho passato molto tempo a cercare di capire esattamente il mio p(doom) personale. La mia stima è più del dieci percento e meno del novanta percento."\]
>
> \[Leike:\] Probabilmente darei anch'io lo stesso intervallo.

Da **Paul Christiano** ([2023](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom)), responsabile della sicurezza presso l'U.S. AI Safety Institute (con sede al NIST) e inventore dell'apprendimento per rinforzo con feedback umano (RLHF):

> Probabilità che la maggior parte degli esseri umani muoia entro 10 anni dalla creazione di un'intelligenza artificiale potente (abbastanza potente da rendere obsoleto il lavoro umano): **20 %** \[…\]
>
> Probabilità che l'umanità abbia in qualche modo compromesso irreversibilmente il nostro futuro entro 10 anni dalla costruzione di un'IA potente: **46 %**

Da **Stuart Russell** ([2025](https://www.newsweek.com/deepseek-openai-race-human-extinction-2023482)), titolare della cattedra Smith-Zadeh in Ingegneria presso UC Berkeley e coautore del principale libro di testo universitario sull'IA, *Artificial Intelligence: A Modern Approach*:

> La "corsa all'IAG" tra aziende e tra nazioni è in qualche modo simile [alla corsa della Guerra Fredda per costruire bombe nucleari sempre più grandi], solo che è peggio: persino i CEO che sono impegnati nella corsa hanno dichiarato che chiunque vinca ha una probabilità significativa di causare l'estinzione umana nel processo, perché non abbiamo idea di come controllare sistemi più intelligenti di noi. In altre parole, la corsa all'IAG è una corsa verso il bordo di un precipizio.

Da **Victoria Krakovna** ([2023](https://theinsideview.ai/victoria)), ricercatrice scientifica presso Google DeepMind e cofondatrice del Future of Life Institute:

> \[intervistatore: "Non è una cosa molto piacevole a cui pensare, ma quale consideri sia la probabilità che Victoria Krakovna muoia a causa dell'IA prima del 2100?"\]
>
> \[Krakovna:\] Voglio dire, il 2100 è molto lontano, soprattutto considerando quanto velocemente si sta sviluppando la tecnologia in questo momento. Voglio dire, così su due piedi, direi tipo il venti per cento o giù di lì.

Da **Shane Legg** ([2011](https://baserates-test.vercel.app/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai)), cofondatore e chief AGI scientist presso Google DeepMind:

> \[intervistatore: "Quale probabilità assegni alla possibilità di conseguenze negative/estremamente negative come risultato di un'IA mal fatta? \[…\] Dove 'negativo' = estinzione umana; 'estremamente negativo' = sofferenza umana"\]
>
> \[Legg:\] \[E\]ntro un anno da qualcosa come un'IA di livello umano\[…\] Non lo so. Forse il cinque per cento, forse il cinquanta per cento. Non credo che nessuno abbia una buona stima di questo. Se per sofferenza intendi sofferenza prolungata, allora penso che sia piuttosto improbabile. Se una macchina super intelligente (o qualsiasi tipo di agente super intelligente) decidesse di sbarazzarsi di noi, penso che lo farebbe in modo piuttosto efficiente.

Da **Emad Mostaque** ([2024](https://x.com/EMostaque/status/1864266899170767105)), fondatore di Stability AI, l'azienda dietro Stable Diffusion:

> La mia P(doom) è del 50 %. Considerando un periodo di tempo indefinito, la probabilità che sistemi più capaci degli esseri umani finiscano per controllare tutte le nostre infrastrutture critiche e ci spazzino via è come lanciare una moneta, soprattutto visto l'approccio che stiamo adottando al momento.

Da **Daniel Kokotajlo** ([2023](https://www.lesswrong.com/posts/xDkdR6JcQsCdnFpaQ/adumbrations-on-agi-from-an-outsider?commentId=sHnfPe5pHJhjJuCWW)), esperto di governance dell'IA, informatore di OpenAI e direttore esecutivo dell'AI Futures Project:

> Penso che la probabilità di una catastrofe causata dall'IA sia del 70 % e credo che chi pensa che sia inferiore, diciamo, al 20 % sia davvero poco ragionevole\[.\]

Da **Dan Hendrycks** ([2023](https://x.com/DanHendrycks/status/1642394635657162753)), ricercatore nel campo dell'apprendimento automatico e direttore del Center for AI Safety:

> \[M\]y p(doom) \> 80 %, ma in passato era più basso. Due anni fa era \~20 %.

Tutti i ricercatori di cui sopra hanno firmato la [Dichiarazione sul rischio legato all'IA](https://aistatement.com/) con cui abbiamo aperto il libro, che dice:

> Ridurre il rischio di estinzione causato dall'IA dovrebbe essere una priorità globale insieme ad altri rischi su scala sociale come le pandemie e la guerra nucleare.

Tra gli altri ricercatori di spicco che hanno firmato la dichiarazione ci sono: John Schulman, l'ideatore di ChatGPT; Peter Norvig, ex direttore della ricerca di Google; Eric Horvitz, responsabile scientifico di Microsoft; David Silver, capo della ricerca di AlphaGo; Frank Hutter, pioniere dell'AutoML; Andrew Barto, pioniere dell'apprendimento per rinforzo; Ian Goodfellow, inventore delle GAN; Ya-Qin Zhang, ex presidente di Baidu; Martin Hellman, inventore della crittografia a chiave pubblica; e Alexey Dosovitskiy, capo della ricerca di Vision Transformer. L'elenco continua con altri firmatari, tra cui: Dawn Song, Jascha Sohl-Dickstein, David McAllester, Chris Olah, Been Kim, Philip Torr e centinaia di altri.

### Quando Leo Szilard vide il futuro {#when-leo-szilard-saw-the-future}

Nel settembre del 1933, un fisico di nome Leo Szilard stava attraversando l'incrocio dove Southampton Row passa per Russell Square quando gli venne in mente l'idea di una reazione nucleare a catena, il concetto chiave alla base delle bombe atomiche.

Da lì iniziò tutta un'avventura, mentre Szilard cercava di capire cosa fare con questa idea monumentale. Si rivolse al fisico più prestigioso Isidor Rabi, e Rabi si rivolse all'ancora più prestigioso Enrico Fermi. Rabi chiese a Fermi se pensasse che le reazioni nucleari a catena fossero davvero possibili, e Fermi rispose:

> Sciocchezze!

Rabi chiese a Fermi cosa significasse "Sciocchezze!", e Fermi disse che era una possibilità remota.

Rabi chiese cosa intendesse Fermi per "possibilità remota", e Fermi disse: "Dieci per cento".

Al che Rabi replicò: "Dieci per cento non è una possibilità remota se significa che potremmo morirne".

Fermi ci ripensò.

Ci sono diverse morali che si potrebbero trarre da questa storia. Una morale che *non* traiamo è: "Ogni possibilità remota vale la pena di essere considerata se potremmo morirne". Non c'è nulla di "remoto" nel dieci per cento, ma se la possibilità *fosse* sufficientemente remota, allora semplicemente non varrebbe la pena pensarci.

Una morale che *invece* traiamo da questa storia: a volte è possibile rendersi conto che una tecnologia come una reazione a catena nucleare è *possibile*, e quindi sapere (prima di tutti gli altri) che il mondo è destinato a qualche tipo di cambiamento drastico.

Un'altra morale che traiamo da questa storia è che le proprie intuizioni iniziali spesso non sono una buona guida per anticipare e riflettere su cambiamenti drastici. Nemmeno se si è un esperto rinomato nel campo rilevante, come lo era Enrico Fermi.

Considerate: da dove ha preso Fermi in primo luogo quella storia della "possibilità remota" e del "dieci per cento"?

Perché Fermi pensava che non si potesse usare la radioattività per indurre altra radioattività in una reazione a catena? Era solo perché la maggior parte delle grandi idee non funzionano?

Rispondere "Assurdo!" sembra dire qualcosa di più forte di questo. Sembra riflettere la sensazione che questa grande idea in particolare fosse *eccessivamente* improbabile da realizzarsi. Ma perché? Su quale argomento fisico?

Sembrava semplicemente una follia? Sì, la possibilità delle armi nucleari avrebbe avuto conseguenze radicali per il mondo. Ma la realtà non è organizzata in modo tale da impedire che eventi con conseguenze di grande portata accadano mai.[^12]

Quando Fermi sentì per la prima volta l'idea di Szilard, gli suggerì di pubblicarla e di farla conoscere al mondo intero, compresa la Germania e il suo nuovo cancelliere, Adolf Hitler.

Fermi perse quella discussione, e meno male, perché alla fine le armi nucleari si rivelarono possibili dopotutto. Alla fine Fermi si unì alla piccola cospirazione di Szilard, anche se rimase scettico quasi fino al momento in cui supervisionò lui stesso la creazione del primo reattore nucleare, Chicago Pile-1.

A volte, le tecnologie sconvolgono il mondo. Se dai per scontato che le nuove tecnologie radicali siano "assurde", puoi essere colto alla sprovvista dal progresso, anche se sei uno degli scienziati più intelligenti del mondo. È quindi un grande merito per Fermi essersi seduto a discutere con Szilard. Ed è un merito ancora maggiore essersi lasciato convincere a cambiare il suo comportamento *prima* che la tecnologia esistesse, prima di poterla vedere con i propri occhi, quando c'era ancora tempo per fare qualcosa al riguardo.

Nel corso della storia umana sono accadute moltissime cose terribili, ma alcune delle cose terribili che non sono accadute sono state evitate perché qualcuno si è seduto e ha avuto la conversazione. In alcuni casi ha forzato la conversazione, come ha fatto Szilard con Fermi.

# 

# Capitolo 1: Il potere speciale dell'umanità {#capitolo-1:-il-potere-speciale-dell-umanità}

Questo è il supplemento online al Capitolo 1 di [*Se qualcuno lo costruisce, tutti muoiono*](https://www.amazon.com/gp/product/0316595640). Di seguito, parleremo delle domande più comuni e approfondiremo gli argomenti trattati nel libro.

Alcuni argomenti non trattati di seguito, perché sono trattati nel Capitolo 1 del libro, includono:

* Cos'è questa faccenda dell'"intelligenza"?  
* È davvero possibile che le macchine diventino più intelligenti degli esseri umani?  
* Non c'è un limite pratico a quanto qualcosa può diventare intelligente?

## Domande frequenti {#faq-1}

### L'intelligenza è un concetto significativo? {#is-intelligence-a-meaningful-concept?}

#### **Sì. C'è un fenomeno reale da descrivere, anche se è difficile da definire con precisione.** {#sì.-c'è-un-fenomeno-reale-da-descrivere,-anche-se-è-difficile-da-definire-con-precisione.}

Negli ultimi trent'anni, settantasette Premi Nobel per la Chimica sono stati assegnati a esseri umani e zero agli scimpanzé. Un alieno, sentendo questo fatto per la prima volta, potrebbe chiedersi se il Comitato Nobel sia di parte. Ma no, c'è davvero *qualcosa* che distingue noi esseri umani dagli scimpanzé.

È un punto fin troppo ovvio, ma a volte i punti ovvi possono essere importanti. Abbiamo delle capacità che ci permettono di camminare sulla luna e che mettono il destino del pianeta nelle nostre mani piuttosto che in quelle degli scimpanzé. I filosofi e gli scienziati possono discutere sulla vera natura dell'intelligenza, ma qualunque sia la loro conclusione, il fenomeno di fondo rimane. Qualcosa negli esseri umani ci ha permesso di realizzare imprese mai viste prima in natura; e quel qualcosa ha a che fare con il nostro cervello e con il modo in cui lo usiamo per comprendere e influenzare il mondo che ci circonda.

#### **Il fatto che non possiamo dare una definizione precisa non significa che non possa danneggiarci.** {#il-fatto-che-non-possiamo-dare-una-definizione-precisa-non-significa-che-non-possa-danneggiarci.}

Se ti trovi intrappolato in un incendio boschivo, non importa se comprendi o meno la chimica sottostante. Bruci lo stesso.

Lo stesso vale per l'intelligenza. Se le macchine iniziassero a convertire la superficie della Terra nella propria infrastruttura, generando così tanto calore di scarto da far bollire gli oceani, allora non importerebbe molto se abbiamo già una definizione precisa di "intelligenza". Moriremmo lo stesso.

Lo intendiamo letteralmente, e nei prossimi capitoli esploreremo perché ci aspettiamo risultati così estremi da un'IA più intelligente dell'essere umano. Nel Capitolo 3, sosterremo che le macchine superintelligenti perseguirebbero degli scopi. Nel Capitolo 4, sosterremo che quegli scopi non sarebbero ciò che qualsiasi essere umano intendeva o aveva chiesto. Il Capitolo 5 è dove sosteniamo che i loro perseguimenti sarebbero meglio realizzati se si appropriassero delle risorse che usiamo per sopravvivere. E il Capitolo 6 è dove sosteniamo che sarebbero capaci di sviluppare la propria infrastruttura e rendere rapidamente il mondo inabitabile.

#### **Non serve una definizione precisa di intelligenza per costruire l'intelligenza.** {#non-serve-una-definizione-precisa-di-intelligenza-per-costruire-l'intelligenza.}

Gli esseri umani sono stati capaci di creare il fuoco prima di comprendere la chimica sottostante della combustione. Similmente, gli esseri umani sono già a buon punto nella creazione di macchine intelligenti, nonostante la loro mancanza di comprensione — come vedremo nel Capitolo 2.

Piuttosto che pensare all'intelligenza come a una nozione matematica che necessita di una definizione precisa, raccomandiamo di pensare all'"intelligenza" come all'etichetta per un fenomeno naturale osservato che non comprendiamo ancora bene.

Qualcosa nei cervelli umani ci permette di compiere una varietà sorprendente di imprese. Costruiamo acceleratori di particelle; sviluppiamo nuovi farmaci; inventiamo l'agricoltura; scriviamo romanzi; eseguiamo campagne militari. Qualcosa nei cervelli umani significa che possiamo fare tutte queste cose, mentre i topi e gli scimpanzé non possono farne nessuna. Anche se non abbiamo ancora una piena comprensione scientifica di quella differenza mentale, è utile avere un'etichetta per essa.

Similmente, è utile poter parlare di intelligenza che supera la nostra. Possiamo già osservare oggi IA che sono sovrumane in una varietà di domini ristretti — le moderne IA per gli scacchi, per esempio, sono sovrumane nel dominio degli scacchi. È naturale quindi chiedersi cosa accadrà quando costruiremo IA che sono sovrumane nei compiti di scoperta scientifica, sviluppo tecnologico, manipolazione sociale o pianificazione strategica. Ed è naturale chiedersi cosa accadrà quando costruiremo IA che supereranno gli esseri umani in tutti i domini.

Se e quando apparirà un'IA che può fare ricerca scientifica di livello mondiale migliaia di volte più velocemente dei migliori scienziati umani, potremmo protestare che "non è veramente intelligente", forse perché raggiunge conclusioni in un modo molto diverso da come farebbe un essere umano. Questo potrebbe anche essere vero, a seconda di quale definizione di "intelligenza" scegli. Ma l'impatto nel mondo reale dell'IA sarà enorme, comunque scegliamo di etichettarla.

Abbiamo bisogno di una terminologia per parlare di quel tipo di impatto, e per parlare dei tipi di macchine che sono radicalmente capaci di prevedere e orientare il mondo. In questo libro, prendiamo la strada facile di assegnare l'etichetta "intelligenza" alle *capacità*, piuttosto che a specifici processi interni che danno origine a quelle capacità.

### L'"intelligenza di livello umano" è un concetto significativo? {#is-“human-level-intelligence”-a-meaningful-concept?}

#### **Sì, in molti casi.** {#sì,-in-molti-casi.}

Gli esseri umani hanno costruito una civiltà tecnologica avanzata, mentre gli scimpanzé no. Sembra esserci *un qualche* senso in cui gli scimpanzé non sono "al nostro livello", anche se comunicano tra loro, usano strumenti e hanno molte capacità impressionanti. Quindi è utile indicare gli esseri umani e dire "*quel* livello", anche se usare l'intelligenza umana come metro di misura presenta alcuni problemi.

Se un giorno incontrassimo una civiltà aliena nelle profondità dello spazio, anche supponendo che gli alieni fossero tecnologicamente avanzati quanto noi, gli alieni potrebbero essere peggiori degli umani nel camminare e migliori nel nuotare. Potrebbero essere migliori nei giochi competitivi come gli scacchi o il poker, ma peggiori nella matematica astratta. O viceversa, a seconda degli alieni. Gli alieni potrebbero pensare più lentamente ma avere memoria migliore, o pensare più velocemente ma con memoria peggiore.

Chi può dire se quegli alieni sono intelligenze "di livello umano"? (E perché non chiedersi se gli umani sono "di livello alieno"?)

Quando parliamo di "intelligenza di livello umano", stiamo cercando di parlare di quella qualità che rende gli umani capaci di costruire e mantenere una civiltà tecnologica, in un modo che gli scimpanzé non possono.

Parlando storicamente (o piuttosto antropologicamente), sembra che a un certo punto, dopo che umani e scimpanzé hanno iniziato a divergere, sia stata superata una soglia. Non è che gli umani abbiano tutti i migliori scienziati mentre gli scimpanzé hanno scienziati mediocri i cui articoli continuano a fallire la replica. Gli scimpanzé non stanno nemmeno scrivendo articoli scientifici *scadenti*. Non stanno scrivendo affatto! I cervelli umani e quelli degli scimpanzé sono piuttosto simili biologicamente, ma c'era una soglia che gli umani hanno superato tale da permetterci di inventare la civiltà e fondere il ferro e mandare razzi in orbita e scrivere e leggere.

A occhio nudo, lasciando da parte ogni teoria, sembra che una sorta di diga si sia rotta e abbia scatenato dietro di sé una vasta inondazione di intelligenza. Una sorta sconosciuta di "inferno" si è scatenata.

Ci sono persone che obietteranno astutamente a questa idea, ma devono farlo tirando in ballo cavilli e definizioni piuttosto che dicendo: "In realtà, ho scoperto prove dell'*Homo erectus* che cercava di costruire reattori nucleari due milioni di anni fa; erano solo molto scarsi nel farlo".

Un'intelligenza sufficientemente potente e generale da creare una civiltà sembra aver colpito il mondo velocemente e duramente, separando nettamente l'*Homo sapiens* dagli altri animali. Certamente non siamo attaccati all'etichetta specifica "intelligenza di livello umano", che ha molti problemi. Ma comunque la chiamiamo, è utile avere *qualche* tipo di concetto per "le cose che sono dall'altra parte di qualunque fosse quella soglia".

### L'intelligenza non è composta da più abilità? {#l'intelligenza-non-è-composta-da-più-abilità?}

#### **Sì, ma c'è una sostanziale sovrapposizione.** {#sì,-ma-c'è-una-sostanziale-sovrapposizione.}

Supponiamo che io sia più bravo di mia sorella a comporre musica classica, ma lei è più brava a scrivere romanzi. Non c'è un modo ovvio per giudicare quale di noi due è "più intelligente", perché la musica e la scrittura di romanzi sono abilità diverse. Quindi, come può essere più sensato dire che un'intelligenza artificiale è "più intelligente" di un essere umano?

La nostra risposta è: se io sono più bravo in una cosa e mia sorella è più brava in un'altra, allora può essere difficile fare confronti significativi. D'altra parte, se io sono più bravo in una cosa e mia sorella è più brava in duemila cose, allora inizia a sembrare un po' sciocco insistere sul fatto che siamo allo stesso livello — o insistere che non ci sia nulla da dire sul livello a cui ci troviamo.

*If Anyone Builds It, Everyone Dies* è un libro sul probabile impatto pratico dei futuri progressi nell'IA. Per parlare in modo significativo di tale impatto, non abbiamo bisogno di essere in grado di confrontare ChatGPT, gli esseri umani e i moscerini della frutta e dire con precisione a quale "livello di intelligenza" si trovano questi tre sistemi molto diversi tra loro. Dobbiamo solo vedere che le IA stanno diventando sempre più brave in una gamma sempre più ampia di abilità e che alla fine supereranno gli esseri umani in abilità di enorme importanza pratica.

### L'intelligenza non è sopravvalutata? {#l'intelligenza-non-è-sopravvalutata?}

#### **Solo se stai usando una definizione troppo ristretta di "intelligenza".** {#solo-se-stai-usando-una-definizione-troppo-ristretta-di-"intelligenza".}

A volte ci imbattiamo in affermazioni come: "L'intelligenza non è tutto quello che serve per avere successo! Molte delle persone di maggior successo sono politici carismatici, CEO o pop star! I nerd sono più bravi in alcune cose, ma non sono loro a governare il mondo".

Non contestiamo questa affermazione. Piuttosto, ciò che intendiamo per "intelligenza" (in questo libro) non è la proprietà che distingue i nerd dagli sportivi. È la proprietà che distingue gli esseri umani dai topi.

In una sceneggiatura hollywoodiana, definire un personaggio "intelligente" di solito significa che ha *intelligenza libresca*. Magari è un appassionato di storia o un geniale inventore. Forse è bravo a giocare a scacchi o a risolvere misteri.

La persona "intelligente" in un film ha i suoi punti di forza, bilanciati dalle tipiche debolezze del nerd hollywoodiano: magari manca di intelligenza emotiva, o di buon senso, o di astuzia da strada. Forse manca di destrezza manuale, o di carisma.

Ma il carisma non è una sostanza prodotta dai reni. Il carisma, come l'intelligenza accademica, è il risultato di processi cerebrali. Ciò include *processi inconsci* all'interno del cervello: i comportamenti che rendono una persona carismatica non sono necessariamente sotto il suo controllo cosciente. Ma alla fine, il carisma e l'acume ingegneristico fanno entrambi parte dell'eredità neurologica che distingue gli esseri umani dai topi, indipendentemente da come questi due poteri siano distribuiti tra i nerd e le pop star.

Per "intelligenza artificiale" non intendiamo "conoscenza teorica artificiale". Intendiamo "tutto ciò che distingue il cervello umano da quello dei topi". Intendiamo il potere che permette agli esseri umani di camminare sulla luna, il potere che permette a un oratore di commuovere una folla fino alle lacrime e il potere che permette a un soldato di mirare abilmente con un fucile. Intendiamo l'insieme di tutte queste cose.

### L'"intelligenza generale" è un concetto significativo? {#is-“general-intelligence”-a-meaningful-concept?}

#### **Sì.** {#sì.}

Il falco pellegrino può tuffarsi in picchiata a 386 km all'ora. Un capodoglio può immergersi a chilometri di profondità nell'oceano. Un falco annegherebbe in mare e una balena si schianterebbe se provasse a volare, ma in qualche modo noi umani siamo riusciti a volare più veloci e a immergerci più in profondità di entrambe queste creature, all'interno di gusci metallici di nostra progettazione. 

Il nostro ambiente ancestrale non includeva le profondità oceaniche, né i nostri antenati erano stati selezionati in base alla loro capacità di volare. Abbiamo raggiunto questi e molti altri traguardi non grazie a istinti speciali, ma grazie alla pura versatilità delle nostre menti.

I nostri antenati sono stati, in qualche modo, selezionati per essere *bravi a risolvere i problemi*, in senso lato, anche se raramente hanno dovuto affrontare prove ingegneristiche più complicate della costruzione di una lancia.

Gli esseri umani hanno una capacità perfetta di risolvere i problemi? No, ovviamente no. Gli esseri umani non sembrano in grado di imparare a giocare a scacchi bene come le migliori IA, almeno entro i limiti di tempo del gioco. È dimostrabilmente possibile raggiungere livelli sovrumani nelle prestazioni scacchistiche, ma gli esseri umani non possono arrivare a quei livelli senza aiuto. La nostra intelligenza non è universale, cioè non possiamo imparare a fare tutto ciò che è fisicamente possibile.[^13] Quindi questa "generalità" che gli esseri umani possiedono non consiste nell'essere in grado di fare tutto ciò che è fattibile usando solo il nostro cervello. Tuttavia, c'è qualcosa di immensamente più generale nella capacità umana di apprendere e risolvere nuovi problemi, rispetto alla capacità di apprendimento e di risoluzione dei problemi di un'IA scacchistica limitata come [Deep Blue](https://www.ibm.com/history/deep-blue).

Ma la generalità non è tutto o niente. Ammette dei gradi.

Deep Blue non era molto generico nella sua capacità di guidare qualcosa di diverso da una scacchiera. Era in grado di trovare mosse vincenti negli scacchi, ma non poteva guidare un'auto fino al negozio per comprare il latte, figuriamoci scoprire le leggi di gravità e progettare un razzo lunare. Deep Blue non era nemmeno in grado di giocare ad altri giochi da tavolo, che fossero giochi più semplici come la dama o giochi più difficili come il Go.

Invece, pensa ad AlphaGo, l'intelligenza artificiale che alla fine ha battuto il Go. Gli algoritmi dietro AlphaGo sono anche in grado di giocare benissimo a scacchi. Il Go non è stato sconfitto dal primo algoritmo di scacchi scoperto dall'umanità, ma una variante del primo algoritmo di Go scoperto dall'umanità è stata in grado di battere i precedenti record negli scacchi, e lo stesso algoritmo è stato anche in grado di eccellere nei videogiochi Atari. Questi nuovi algoritmi non erano ancora in grado di andare a comprare il latte al negozio, intendiamoci, ma erano *più* generici.

A quanto pare, alcuni metodi di intelligenza sono molto più generici di altri.

#### **Ma siamo ancora più lontani dal definire la "generalità" che l'"intelligenza".** {#ma-siamo-ancora-più-lontani-dal-definire-la-"generalità"-che-l'"intelligenza".}

È facile dire che gli esseri umani sono più generali dei moscerini della frutta. Ma come funziona la generalità?

Non lo sappiamo. Non esiste ancora una teoria formale matura della "generalità". Possiamo gesticolare e dire che un'intelligenza è "più generale" nella misura in cui è capace di prevedere e controllare in una gamma più ampia di ambienti, nonostante una gamma più ampia di sfide complicate. Ma non possiamo fornirvi un modo per quantificare sfide e ambienti che renda questa una definizione formale.

Questo suona insoddisfacente? Anche noi siamo insoddisfatti. Vorremmo davvero tanto che l'umanità accumulasse una migliore comprensione dell'intelligenza generale prima di tentare di costruire macchine generalmente intelligenti. Questo potrebbe migliorare la terribile situazione tecnica che descriveremo nei Capitoli 10 e 11\. 

Sebbene non abbiamo una descrizione formale del fenomeno, possiamo comunque dedurre alcuni fatti sulla generalità osservando il mondo intorno a noi.

Sappiamo che gli esseri umani non nascono con la conoscenza e l'abilità innate per costruire grattacieli e razzi lunari, perché i nostri antenati remoti non hanno mai dovuto lavorare con grattacieli e razzi lunari in un modo che potesse codificare quella conoscenza nei nostri geni. Piuttosto, quelle abilità derivano dal nostro potere di apprendere su domini che non comprendiamo dalla nascita.

Per valutare la generalità, non chiederti quanto qualcosa *sa*. Chiediti quanto *impara*.

In un certo senso, gli esseri umani sono studenti più capaci rispetto ai topi. Non è che i topi non possano imparare nulla: ad esempio, possono imparare a orientarsi in un labirinto. Ma gli esseri umani possono imparare cose più complicate e strane rispetto ai topi e possono collegare le loro conoscenze in modo più efficace.

Come funziona esattamente? Cosa abbiamo noi che i topi non hanno? 

Pensa a due persone che stanno imparando a orientarsi in una nuova città dopo essersi trasferite. 

Alice memorizza tutti i percorsi che le servono. Per andare da casa sua al negozio di ferramenta, gira a sinistra sulla Terza Strada, poi a sinistra al secondo semaforo, prosegue per altri due isolati e gira a destra nel parcheggio. Memorizza separatamente il percorso per andare al supermercato e quello per andare al suo ufficio.

Nel frattempo, Beth studia e interiorizza una mappa della città. 

Alice se la cava bene nella sua vita di tutti i giorni, ma se deve guidare in un posto nuovo senza indicazioni, è nei guai. Al contrario, Beth deve passare più tempo a pianificare i suoi percorsi, ma è molto più flessibile.

Alice può anche essere più veloce sui percorsi specifici che ha memorizzato, ma Beth sarà più brava a guidare ovunque altrove. Beth avrà anche un vantaggio in altri compiti, come trovare un percorso che minimizzi il traffico nelle ore di punta o persino progettare la viabilità di un'altra città.

Sembra che ci siano tipi di apprendimento che assomigliano meno alla memorizzazione di percorsi stradali e più all'interiorizzazione di una mappa. Sembra che ci siano meccanismi mentali che possono essere riutilizzati e adattati a molti scenari diversi. Sembra che ci siano tipi di pensiero che vanno in profondità.

Approfondiremo questo argomento nel capitolo 3.

### L'intelligenza è una semplice quantità scalare? {#is-“intelligence”-a-simple-scalar-quantity?}

#### **No. Ma ci sono livelli che l'IA non ha ancora raggiunto.** {#no.-ma-ci-sono-livelli-che-l'ia-non-ha-ancora-raggiunto.}

A volte abbiamo sentito dire che l'idea di superintelligenza presuppone che l'"intelligenza" sia una quantità semplice e unidimensionale.[^14] Più ricerca sull'IA fai, più "intelligenza" ottieni, come se l'intelligenza fosse meno simile a una macchina e più simile a un fluido che puoi continuare a pompare dal terreno.

Siamo d'accordo con la critica di fondo: l'intelligenza non è una semplice quantità scalare. Potrebbe non essere sempre semplice costruire IA più intelligenti semplicemente aggiungendo più hardware di calcolo al problema (anche se a volte lo sarà, se l'ultimo decennio è indicativo). Una maggiore intelligenza non sempre si traduce direttamente in un maggiore potere. Il mondo è complicato e le capacità possono incontrare ostacoli e plateau.

Ma, come abbiamo detto nel Capitolo 1, l'esistenza di complicazioni, limiti e ostacoli non significa che l'IA si arresterà convenientemente vicino al range di capacità umane. I cervelli biologici hanno dei limiti che *non* ci sono nell'IA, come abbiamo detto nel libro.

L'intelligenza umana ha molti limiti, eppure ci ha portato sulla luna. L'intelligenza animale non è una quantità scalare unica, eppure gli esseri umani sono in grado di surclassare gli scimpanzé. Per quanto l'intelligenza sia complessa, c'è un divario chiaro e qualitativo tra noi e gli scimpanzé.

Anche le superintelligenze artificiali potrebbero avere dei limiti e delle complicazioni, ma potrebbero comunque surclassare gli esseri umani. Potrebbe comunque aprirsi un divario qualitativo tra loro e noi, se i ricercatori e gli ingegneri continuano a gareggiare per creare IA sempre più capaci.

### L'IA supererà soglie critiche e decollerà? {#will-ai-cross-critical-thresholds-and-take-off?}

#### **Probabilmente sì.** {#probabilmente.}

Da certi punti di vista, i progressi dell'IA moderna sembrano incrementali.[^15] Per esempio, nell'estate del 2025, la capacità dell'IA di completare compiti lunghi ha seguito più o meno una curva esponenziale negli ultimi anni,[^16] e si potrebbe dire che questo è rassicurantemente incrementale.[^17] Questo significa che i progressi dell'IA saranno belli, lenti e prevedibili?

Non necessariamente. Solo perché una quantità sale lentamente o in modo regolare o incrementale, non significa che i risultati siano sempre docili. La fissione nucleare avviene su un continuum, ma c'è una bella differenza tra una reazione a catena nucleare che produce meno di un neutrone per neutrone (in cui la reazione si esaurisce) e una reazione a catena nucleare che produce più di un neutrone per neutrone (che porta a una reazione a catena incontrollabile).

Ma non c'è una grande differenza nella meccanica sottostante tra i due tipi di reazioni nucleari. Aggiungendo un po' più di uranio, il "fattore di moltiplicazione dei neutroni" passa gradualmente da poco meno di uno a poco più di uno. Le reazioni supercritiche non sono causate dai neutroni che colpiscono gli atomi di uranio con tale forza da creare superneutroni. Una piccola quantità in più della stessa sostanza di base provoca un grande cambiamento macroscopico. Questo fenomeno è chiamato "effetto soglia".

Il caso degli esseri umani rispetto agli scimpanzé sembra dimostrare che esiste almeno un effetto soglia in gioco quando si tratta di intelligenza. Gli esseri umani non sono poi così diversi anatomicamente dagli altri animali. Il cervello umano e quello degli scimpanzé sono molto simili all'interno: entrambi hanno una corteccia visiva, un'amigdala e un ippocampo. Gli esseri umani non hanno uno speciale modulo "ingegneristico" in più che spiega perché noi possiamo andare sulla luna e loro no.

Ci sono alcune differenze di cablaggio e abbiamo una corteccia prefrontale più sviluppata rispetto agli altri primati. Ma a livello di anatomia macroscopica, la differenza principale è che il nostro cervello è tre o quattro volte più grande. In sostanza, utilizziamo una versione più grande e leggermente aggiornata dello stesso hardware.

E i cambiamenti non sono stati improvvisi nella nostra stirpe. Il cervello dei nostri antenati è diventato sempre un po' più grande e un po' migliore, un passo alla volta. Questo è bastato per creare un enorme divario qualitativo in poco tempo (secondo i tempi dell'evoluzione).

Se può succedere con gli esseri umani, probabilmente può succedere anche con le IA.

#### **Non sappiamo quanto siano lontane le IA da queste soglie.** {#non-sappiamo-quanto-siano-lontane-le-ia-da-queste-soglie.}

Se sapessimo esattamente cosa è successo negli esseri umani che ci ha permesso di superare la soglia dell'intelligenza generale, potremmo sapere cosa cercare per capire che una soglia critica è vicina. Ma, come vedremo nel capitolo 2, non abbiamo quel livello di comprensione dell'intelligenza. Quindi stiamo procedendo alla cieca, senza avere idea di dove siano le soglie o di quanto siamo vicini ad esse.

I recenti progressi nell'IA hanno portato a una maggiore capacità di risolvere problemi matematici e giocare a scacchi, ma non sono stati sufficienti per portare le IA "fino in fondo". Forse basta un modello che sia altre tre o quattro volte più grande, come la differenza tra il cervello degli scimpanzé e quello umano.[^18] O forse no! Forse ci vorrà un'architettura completamente diversa e un decennio di progressi scientifici, come nel caso dei moderni chatbot, che derivano da una nuova architettura inventata nel 2017 (e maturata nel 2022).

Quali cambiamenti nel cervello umano ci hanno fatto superare una soglia critica? Forse è stata la nostra capacità di comunicare. Forse è stata la nostra capacità di afferrare concetti astratti in modi che hanno reso la comunicazione così preziosa. Forse stiamo ragionando in termini completamente sbagliati e il cambiamento chiave è stato qualcosa di strano che oggi non è nel nostro radar. Forse è stata una grande combinazione di fattori, ognuno dei quali doveva essere abbastanza maturo da potersi combinare con gli altri in quel tipo di intelligenza che ha permesso agli esseri umani di arrivare sulla luna.

Non lo sappiamo. E poiché non lo sappiamo, non possiamo guardare un'intelligenza artificiale moderna e sapere quanto sia vicina o lontana da quella stessa soglia critica.

L'alba della scienza e dell'industria ha cambiato radicalmente la civiltà umana. L'alba del linguaggio potrebbe aver avuto conseguenze simili per i nostri antenati. Ma se così fosse, non c'è alcuna garanzia che una di queste capacità agisca come una "soglia critica" per l'IA, perché a differenza degli esseri umani, le IA possedevano fin dall'inizio una certa conoscenza del linguaggio, della scienza e dell'industria.

O forse la soglia critica per l'umanità era una combinazione di molti fattori, dove ognuno di essi doveva essere "abbastanza buono" perché tutto il sistema funzionasse. Le IA potrebbero rimanere indietro in alcune capacità in cui gli ominidi erano più bravi, come la memoria a lungo termine, ma potrebbero comunque mostrare un salto importante nelle capacità pratiche una volta che l'ultimo pezzo andrà al suo posto.

Anche se nessuna di queste analogie tra IA e esseri umani dovesse rivelarsi valida, ci saranno probabilmente altre dinamiche che renderanno il progresso dell'IA irregolare e difficile da prevedere.

Forse i deficit nella memoria a lungo termine e nell'apprendimento continuo stanno frenando le IA in un modo che non ha mai ostacolato gli esseri umani. Forse, una volta risolti questi problemi, qualcosa "scatterà" e l'IA sembrerà ottenere una sorta di "scintilla" di intelligenza.

Oppure (come discusso nel libro) considerate il punto in cui le IA possono costruire IA più intelligenti, che a loro volta costruiscono IA ancora più intelligenti, in un ciclo di retroazione. I cicli di retroazione sono una causa comune degli effetti soglia.

Per quanto ne sappiamo, potrebbero esserci una dozzina di fattori diversi che potrebbero servire come "pezzo mancante", così che, una volta che un laboratorio di IA capisce quell'ultimo pezzo del puzzle, la loro IA inizia davvero a decollare e a separarsi dal gruppo, proprio come l'umanità si è separata dal resto degli animali. I momenti critici potrebbero arrivare in fretta. Non abbiamo necessariamente tutto questo tempo per prepararci.

#### **\* La velocità di decollo non influisce sul risultato, ma la possibilità di un decollo veloce significa che dobbiamo agire presto.** {#*-la-velocità-di-decollo-non-influisce-sul-risultato,-ma-la-possibilità-di-un-decollo-veloce-significa-che-dobbiamo-agire-presto.}

Alla fine, le soglie non contano molto nell'argomentazione secondo cui se qualcuno costruisce una superintelligenza artificiale, allora tutti moriranno. Le nostre argomentazioni non richiedono che qualche IA capisca come auto-migliorarsi ricorsivamente e poi diventi superintelligente a una velocità senza precedenti. Questo potrebbe accadere, e pensiamo che sia abbastanza probabile che accadrà, ma non importa per l'affermazione che l'IA è sulla buona strada per ucciderci tutti.

Tutto ciò che richiedono le nostre argomentazioni è che le IA continuino a migliorare sempre di più nella previsione e nel dirigere il mondo, fino a superarci. Non importa molto se ciò avverrà rapidamente o lentamente.

L'importanza degli effetti soglia sta nel fatto che aumentano l'importanza di una reazione *rapida* dell'umanità alla minaccia. Non possiamo permetterci il lusso di aspettare che l'IA sia *leggermente* migliore di ogni essere umano in ogni attività mentale, perché a quel punto potrebbe non esserci più molto tempo a disposizione. Sarebbe come guardare i primi ominidi che accendono il fuoco, sbadigliare e dire: "Svegliatemi quando saranno a metà strada verso la luna".

Gli ominidi hanno impiegato milioni di anni per arrivare a metà strada dalla luna e due giorni per completare il resto del viaggio. Quando possono esserci delle soglie, bisogna stare attenti *prima* che le cose sfuggano visibilmente di mano, perché a quel punto potrebbe essere troppo tardi.

### ChatGPT non è già un'intelligenza generale? {#chatgpt-non-è-già-un'intelligenza-generale?}

#### **Puoi chiamarlo così, se vuoi.** {#puoi-chiamarlo-così-se-vuoi.}

ChatGPT e i suoi simili sono più generali rispetto alle IA che li hanno preceduti. Possono fare un po' di matematica, scrivere poesie e codice. ChatGPT non sempre riesce a fare queste cose *bene* (agosto 2025), ma è in grado di fare moltissime cose.

È ragionevole supporre che GPT-5 sia ancora meno generale nel ragionamento rispetto a un bambino umano. Certo, può recitare da più libri di testo, ma è plausibile che abbia memorizzato un volume molto maggiore di schemi superficiali rispetto a quelli che userebbe un bambino umano, mentre un bambino usa plausibilmente ingranaggi mentali più profondi per completare compiti simili (con risultati migliori in alcuni casi e peggiori in altri).

Se noi autori fossimo costretti a confrontare i due, diremmo che ChatGPT sembra generalmente più stupido in un senso profondo rispetto a un essere umano, e non solo perché (mentre scriviamo questa frase nel luglio 2025) i chatbot hanno memorie episodiche limitate.

Ci sono almeno alcune persone che ribatterebbero: "Cosa intendi? ChatGPT sa parlare, può avere conversazioni emotivamente profonde con me, può risolvere problemi matematici avanzati e scrivere codice, cose che molti esseri umani non sanno fare. Chi può dire che sia più stupido di un essere umano?". Dieci anni fa non ci saremmo mai trovati di fronte a una conversazione del genere, il che dice *qualcosa* sui progressi compiuti da allora.

Il mondo è forse a metà strada tra "Le IA sono chiaramente più stupide degli esseri umani" e "Dipende da cosa chiedi all'IA di fare".

Forse quello che serve per colmare la distanza rimanente è solo un po' più di scala, come il cervello umano che sembra sostanzialmente simile a quello degli scimpanzé, ma è tre o quattro volte più grande. O forse l'architettura alla base di ChatGPT è troppo superficiale per supportare la "scintilla" della generalità.

Forse c'è qualche componente importante dell'intelligenza generale che gli algoritmi moderni di IA non riescono a gestire, e le IA moderne compensano questo limite applicando enormi quantità di pratica e memorizzazione a quei tipi di compiti che possono essere risolti con la pratica bruta. In questo caso, forse basta una sola brillante (e anche incredibilmente stupida) invenzione algoritmica per colmare tale deficit, e le IA saranno in grado di comprendere la maggior parte delle cose che un essere umano può comprendere e di imparare dall'esperienza in modo efficiente quanto un essere umano. (Pur continuando a essere in grado di leggere e memorizzare l'intero Internet.) O forse ci vorranno altre quattro scoperte algoritmiche. Nessuno lo sa, come discusso nel capitolo 2\.

#### **\* Ci sono molte cose diverse che si possono intendere con "intelligenza generale".** {#*-ci-sono-molte-cose-diverse-che-si-possono-intendere-con-"intelligenza-generale".}

Quando si dice "le IA ora sono generalmente intelligenti", qualcuno potrebbe intendere che le IA abbiano acquisito quella combinazione di abilità, ancora poco compresa, che ha fatto esplodere tutto sotto forma di civiltà umana.

Oppure potrebbe significare che l'IA è almeno arrivata al punto che ora le persone discutono *con veemenza* se siano più intelligenti gli esseri umani o le IA.

Oppure potrebbero pensare a un momento in cui le persone avranno smesso di discutere, perché sarà chiaro che le IA sono decisamente e in generale più intelligenti di qualsiasi essere umano. O a un momento in cui le persone avranno smesso di discutere, perché non ci sarà più nessuno con cui discutere; l'umanità si sarà spinta troppo oltre e l'IA avrà messo fine a tutte le nostre discussioni e ai nostri sforzi.

Non c'è stato un giorno e un'ora precisi in cui si possa dire che le IA "hanno iniziato a giocare a scacchi a livello umano". Ma quando le IA scacchistiche sono riuscite a schiacciare il campione mondiale umano, quel momento era già passato.

Tutto questo per dire che la risposta alla domanda "ChatGPT è generalmente intelligente?" potrebbe essere sì o no, a seconda di cosa intendi esattamente con la domanda. (Questo dice molto sui progressi dell'IA negli ultimi anni! Deep Blue era chiaramente piuttosto specializzato.)

#### **La superintelligenza è una distinzione più importante.** {#superintelligenza-è-una-distinzione-più-importante.}

Visto che ci sono diverse cose che "intelligenza di livello umano" potrebbe ragionevolmente significare, di solito eviteremo di usare questa terminologia, tranne quando parliamo di IA superumana. Questo è anche il motivo per cui di solito evitiamo di dire "intelligenza artificiale generale". Se dobbiamo parlare di una di queste idee, la spiegheremo in modo più dettagliato.

Useremo termini come "IA più intelligente dell'uomo", "IA superumana" o "superintelligenza", che presuppongono un qualche tipo di riferimento umano:

* Con "**IA più intelligente dell'uomo**" o "**IA superumana**" (qui e nel libro), intendiamo un'IA che possiede quella "scintilla di generalità" che distingue gli esseri umani dagli scimpanzé *e* che è chiaramente migliore, nel complesso, dei singoli esseri umani più intelligenti nel risolvere problemi e capire cosa è vero.

  L'IA superumana potrebbe essere solo *leggermente* più intelligente degli esseri umani più brillanti, e potrebbero esserci alcuni compiti in cui questi ultimi continuano a ottenere risultati migliori. Tuttavia, qui e nel libro, supporremo che "IA più intelligente dell'uomo" significhi *almeno* che, in un confronto equo su un'ampia gamma di compiti complessi, l'IA otterrebbe risultati migliori rispetto agli esseri umani più competenti, in tutti i tipi di compiti difficili.

* Con "**IA superintelligente**" o "**superintelligenza artificiale**" (ASI), invece, intendiamo un'IA superumana che supera *vastamente* l'intelligenza umana. Assumiamo che i singoli esseri umani e i gruppi di esseri umani nel mondo reale siano completamente incapaci di competere con l'IA superintelligente in qualsiasi ambito di importanza pratica, per i motivi discussi nel capitolo 6.

Il libro userà principalmente i termini "superumano" e "superintelligente" in modo intercambiabile. La distinzione diventa più rilevante nella Parte II, dove descriviamo uno scenario di conquista del potere da parte dell'IA in cui le IA iniziano con un'intelligenza debolmente superiore a quella umana, ma *non* superintelligente. Questo aiuta a illustrare che la superintelligenza è plausibilmente eccessiva: l'IA potrebbe diventare superintelligente presto, ma non *ha bisogno* di essere così intelligente per causare l'estinzione umana.

Queste sono definizioni molto approssimative, ma sono sufficienti per gli scopi di questo libro.

Questo libro non propone una teoria complessa dell'intelligenza, per poi dedurne qualche implicazione esoterica della teoria che preannuncia un disastro. Invece, opereremo a un livello piuttosto basilare, con affermazioni come:

* A un certo punto, l'IA probabilmente raggiungerà pienamente *qualsiasi cosa sia* che permette agli esseri umani (e non agli scimpanzé) di costruire razzi, centrifughe e città.  
* A un certo punto, l'IA *supererà* gli esseri umani.  
* Le IA potenti probabilmente avranno i propri obiettivi che perseguiranno con tenacia, perché perseguire con tenacia gli obiettivi è utile per un'ampia gamma di compiti (e, ad esempio, gli esseri umani hanno sviluppato obiettivi proprio per questo motivo).

Affermazioni come queste, giuste o sbagliate che siano, non dipendono dal fatto che abbiamo una comprensione speciale di tutti i meccanismi interni dell'intelligenza. Possiamo vedere il camion che si dirige verso di noi, anche senza appellarci a un modello complicato del suo funzionamento interno. O almeno così sosterremo.

E argomenti semplici come questi non dipendono dal fatto che ChatGPT sia "davvero" al livello umano o "davvero" un'intelligenza generale. Fa quello che fa. Le future IA faranno più cose e meglio. Il resto del libro parla di dove porterà questo percorso.

### Quanto potrebbe diventare intelligente una superintelligenza? {#how-smart-could-a-superintelligence-get?}

#### **Molto intelligente.** {#molto-intelligente.}

Per ogni punto dell'elenco del Capitolo 1 sui motivi per cui i cervelli umani non sono vicini ai limiti delle possibilità fisiche, le macchine *potrebbero* avvicinarsi a quei limiti.

Le leggi della fisica permettono l'esistenza di geni che pensano decine di migliaia (se non milioni o miliardi) di volte più velocemente degli esseri umani,[^19] che non hanno mai bisogno di dormire o mangiare e che possono creare copie di se stessi e scambiarsi esperienze.

E questo senza nemmeno considerare i miglioramenti alla *qualità* della cognizione di un'IA.

Anche se l'IA fosse fortemente superiore agli esseri umani solo in una o due dimensioni, questo potrebbe bastare per un vantaggio decisivo. Nel corso della storia, gruppi di esseri umani hanno ripetutamente sfruttato vantaggi relativamente piccoli nella scienza, nella tecnologia e nella pianificazione strategica per raggiungere posizioni dominanti su altri gruppi. Pensa, per esempio, ai conquistadores spagnoli. E questo senza grandi differenze nell'architettura o nelle dimensioni del cervello.

Anche piccoli vantaggi intellettuali possono tradursi in grandi vantaggi pratici, e anche piccoli vantaggi possono accumularsi estremamente velocemente. Ma i probabili vantaggi dell'IA sembrano tutt'altro che piccoli.

Per ulteriori argomenti sul fatto che questo livello di intelligenza *sia importante* — che possa tradursi in potere nel mondo reale — si veda il Capitolo 6.

### Ma non ci sono grossi ostacoli per arrivare alla superintelligenza? {#ma-non-ci-sono-grossi-ostacoli-per-arrivare-alla-superintelligenza?}

#### **Non è chiaro.** {#non-è-chiaro.}

In larga misura, il campo sta procedendo alla cieca. Potrebbe essere che non ci siano più ostacoli reali e che piccoli aggiustamenti alle tecniche attuali portino alla superintelligenza, o portino a IA abbastanza intelligenti da costruire IA leggermente più intelligenti che costruiscono IA leggermente più intelligenti che costruiscono IA superintelligenti.

Se ci sono ostacoli importanti, non sappiamo quanto tempo ci vorrà all'umanità per superarli (con o senza l'aiuto dell'IA).

Quello che sappiamo è che i principali laboratori di IA stanno spingendo esplicitamente in quella direzione, e sappiamo che stanno facendo progressi. Una volta le macchine non sapevano disegnare, parlare o scrivere codice; ora lo fanno.

#### **\* Il settore è bravo a superare gli ostacoli.** {#*-il-settore-è-bravo-a-superare-gli-ostacoli.}

Per decenni, le IA hanno faticato persino a distinguere l'immagine di un gatto da quella di un'auto. La svolta è arrivata nel 2012, quando i ricercatori dell'Università di Toronto Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton hanno progettato [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), una rete neurale convoluzionale che ha superato di gran lunga lo stato dell'arte. Questo evento è ampiamente riconosciuto come l'inizio della rivoluzione moderna dell'IA, in cui le reti neurali artificiali sono utilizzate per alimentare quasi tutte le IA moderne.

L'IA era scarsa nei giochi da tavolo. Anche dopo che l'IA per gli scacchi [Deep Blue](https://www.ibm.com/history/deep-blue) ha sconfitto il grande maestro Garry Kasparov nel 1997, i computer facevano fatica con il numero molto più grande di mosse possibili nel gioco del Go. Questo fino al 2016, quando [AlphaGo](https://deepmind.google/research/projects/alphago/) ha battuto il campione del mondo Lee Sedol dopo essersi addestrato su migliaia di partite giocate da umani, usando una nuova architettura che combinava reti neurali profonde con la ricerca ad albero. Dopo aver battuto il Go, il team di DeepMind ha usato lo stesso algoritmo in modo più generale, chiamato [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/), e ha scoperto che dominava non solo nel Go, ma anche in altri giochi come gli scacchi e lo shogi.

I primi chatbot erano comunicatori limitati.[^20] Poi, nel 2020, la maturazione dell'architettura transformer ci ha dato [GPT-3](https://arxiv.org/abs/2005.14165), che era abbastanza sofisticato da tradurre testi, rispondere a domande e persino generare campioni di articoli di notizie che sembravano reali. Una volta riaddestrato leggermente per comportarsi come un chatbot, è diventato l'applicazione consumer con la crescita più rapida di tutti i tempi.[^21]

Ci sono ostacoli tra l'IA moderna e quella "vera", il tipo di IA che potrebbe diventare o creare una superintelligenza?

Forse sì. Forse servono più intuizioni architettoniche, come quelle dietro AlexNet che hanno aperto le porte all'intero campo dell'IA moderna, o quelle dietro AlphaZero che hanno finalmente permesso alle IA di eccellere in più giochi utilizzando lo stesso algoritmo, o quelle dietro ChatGPT che hanno fatto sì che i computer iniziassero a parlare. (O forse no; forse le moderne IA supereranno silenziosamente [qualche soglia](#will-ai-cross-critical-thresholds-and-take-off?) e sarà tutto lì.)

Ma se ci sono ancora degli ostacoli, i ricercatori del settore probabilmente li supereranno. Sono piuttosto bravi in questo, e ora ci sono molti più ricercatori che lavorano a questo problema rispetto al 2012.[^22]

A partire da luglio 2025, le IA fanno fatica con compiti che richiedono memoria a lungo termine e pianificazione coerente, come giocare al videogioco Pokémon. Si potrebbe essere tentati di unirsi agli scettici e ridere degli ultimi fallimenti delle IA: come possono delle IA che fanno fatica con semplici videogiochi avvicinarsi alla superintelligenza?

Allo stesso modo, nel 2019 le IA facevano davvero fatica a parlare in modo coerente. Ma questo non significava che il successo fosse lontano vent'anni. I laboratori stanno lavorando sodo per capire cosa impedisce alle IA di fare bene in certi tipi di compiti e probabilmente stanno trovando nuove architetture che funzionano meglio con la memoria a lungo termine e la pianificazione. Nessuno sa cosa saranno in grado di fare queste IA.

Se questa fase non sarà abbastanza per far sì che le IA inizino ad automatizzare la ricerca scientifica e tecnologica (compreso lo sviluppo di IA ancora più intelligenti), i ricercatori si concentreranno semplicemente sul prossimo ostacolo. Continueranno ad andare avanti, a meno che e fino a quando l'umanità non intervenga e vieti tale ricerca, un argomento che tratteremo nei capitoli successivi.

### Non è impossibile prevedere il comportamento di una superintelligenza? {#non-è-impossibile-prevedere-il-comportamento-di-una-superintelligenza?}

#### **In un certo senso, ma non in tutti.** {#in-un-certo-senso,-ma-non-in-tutti.}

Stockfish 17 è più bravo di noi a gestire una partita a scacchi. Se giocassimo una partita contro Stockfish, non riusciremmo a prevedere le sue mosse: per farlo dovremmo essere bravi almeno quanto Stockfish 17\. Ma sarebbe facile per noi prevedere chi vincerà la partita.[^24] È difficile prevedere quali mosse farà Stockfish; è facile prevedere che *vincerà.*

Lo stesso vale per le IA che prevedono e guidano il mondo reale. Più sono intelligenti, più è difficile prevedere esattamente cosa faranno, ma più è facile prevedere che raggiungeranno la destinazione verso cui si stanno dirigendo.

### Le macchine non saranno fondamentalmente prive di creatività o comunque fatalmente imperfette? {#le-macchine-non-saranno-fondamentalmente-prive-di-creatività-o-altrimenti-fatalmente-imperfette?}

#### **No.** {#no.}

Per ora, lasciamo da parte la questione se le macchine possano essere creative fino al capitolo 3. Ma diciamo questo: le macchine non devono per forza avere qualche difetto che le renda simili agli esseri umani, in modo che lo spirito umano possa avere una possibilità di vincere.

Se i dodo avessero avuto una loro industria cinematografica, allora le sceneggiature scritte dai dodo sull'invasione umana della loro isola di Mauritius avrebbero potuto compensare le armi e l'acciaio degli umani con gli svantaggi umani. Forse l'angoscia esistenziale indotta dall'intelligenza degli umani li fa bloccare dalla disperazione all'ultimo minuto, giusto il tempo necessario agli eroici dodo per contrattaccare e beccarli tutti a morte.

Questa è forse una storia che i dodo troverebbero soddisfacente: che l'intelligenza non può essere un vantaggio militare netto rispetto ai becchi forti, che i cervelli più grandi degli umani devono avere qualche difetto fatale che permette ai fieri dodo di vincere alla fine.

In realtà, i vantaggi apparenti degli umani sono vantaggi *effettivi*. Gli svantaggi del cervello umano non sono svantaggi *netti* in un conflitto militare con cervelli di uccello. Lo scontro tra umani e dodo finisce per essere impari, e questo è tutto.

Anche quando gli umani combattono contro altri umani, le mitragliatrici sono un vantaggio tale che un esercito dotato di mitragliatrici di solito batte un esercito che ne è sprovvisto. Ci sono rare eccezioni a questa regola, e la gente ama raccontarle perché l'eccezione è una storia più divertente della norma. Ma le eccezioni si verificano nella vita reale molto meno spesso di quanto non si verifichino nelle storie.

Prevederemmo lo stesso per IA avanzate con memorie e menti vaste, che possono copiarsi migliaia di volte e pensare a una velocità diecimila volte superiore a quella di un essere umano; menti in grado di ragionare in modo più valido, generalizzare più rapidamente e con maggiore precisione da un numero minore di dure lezioni, e migliorarsi.

Non è una domanda trabocchetto e non ci sarà un colpo di scena sorprendente, per quanto ci piacerebbe che ci fosse.

### Non c'è qualcosa di speciale negli esseri umani che le semplici macchine non potrebbero mai emulare? {#non-c'è-qualcosa-di-speciale-negli-esseri-umani-che-le-semplici-macchine-non-potrebbero-mai-emulare?}

#### **Sembra improbabile e non particolarmente rilevante.** {#it-seems-unlikely,-and-not-especially-relevant.}

I cervelli e i corpi umani sono fatti di parti che possiamo studiare e arrivare a comprendere. C'è molto che non capiamo del cervello, ma questo non significa che le parti che non capiamo funzionino per magia e che gli umani non potrebbero mai costruire qualcosa di simile. Significa solo che i cervelli sono macchine enormemente *complesse*. Il cervello umano ha centinaia di migliaia di miliardi di sinapsi e abbiamo ancora molta strada da fare per comprendere tutti i principi importanti di alto livello all'opera.

Anche l'intelligenza è fatta di pezzi: algoritmi e calcoli individuali che il nostro cervello esegue naturalmente, anche se non abbiamo una comprensione scientifica di come funziona il nostro cervello.

Anche se ci fosse qualche aspetto del ragionamento biologico molto difficile da implementare nelle macchine, non ne conseguirebbe che l'IA non supererà mai l'umanità. Le IA potrebbero semplicemente fare lo stesso tipo di lavoro in modo diverso, come quando l'IA Deep Blue ha trovato le mosse vincenti negli scacchi in modo molto diverso da Garry Kasparov.[^25] Quello che conta non è se le macchine possiedono tutte le caratteristiche uniche degli esseri umani; quello che conta è se le macchine diventano capaci di prevedere e guidare il mondo.

I capitoli che seguono aiuteranno a fare più luce su questo punto. Nel Capitolo 2 vedremo come le IA moderne vengono coltivate piuttosto che costruite artigianalmente, e come il processo di coltivazione tenderà a rendere le IA molto capaci. Nel Capitolo 3 vedremo poi come i tentativi di rendere le IA sempre più capaci tenderanno a renderle sempre più orientate verso il raggiungimento di obiettivi difficili. E nel Capitolo 4 discuteremo come questi obiettivi difficilmente saranno quelli che gli sviluppatori intendevano, né quelli che gli utenti hanno richiesto. Tutto questo è sufficiente perché le IA portino il mondo alla rovina, che si consideri o meno che le IA abbiano una qualche scintilla vitale, o coscienza, o qualsiasi altra cosa si possa immaginare renda speciali gli esseri umani.

Vedi anche, nelle risorse online a seguire:

* Capitolo 2: "[L'IA non è 'solo matematica'?](#aren't-ais-“just-math”?)" e "[Le IA non saranno fredde, meccaniche e logiche o comunque prive di una scintilla fondamentale?](#won't-ais-inevitably-be-cold-and-logical,-or-otherwise-missing-some-crucial-spark?)"  
* Capitolo 3: "[Antropomorfismo e Meccanomorfismo](#anthropomorphism-and-mechanomorphism)"  
* Capitolo 5: "[Efficacia, Coscienza e Benessere dell'IA](#effectiveness,-consciousness,-and-ai-welfare)"

### Stai dicendo che le macchine diventeranno coscienti? {#are-you-saying-machines-will-become-conscious?}

#### **Non necessariamente, e questo sembra un argomento a parte.** {#not-necessarily,-and-this-seems-like-a-separate-topic.}

*If Anyone Builds It, Everyone Dies* non discute affatto la coscienza delle macchine, concentrandosi invece sull'*intelligenza* delle macchine. Come primo passo nel parlare di coscienza, dovremmo prima chiarire che tipo di "coscienza" abbiamo in mente.

Quando qualcuno chiede "Il mio cane è cosciente?", può intendere diverse cose, come:

* Rover *capisce* davvero le cose, o sta solo eseguendo istinti complicati? Sta *pensando*, o sta solo facendo i movimenti?  
* È consapevole di sé? Sa della propria esistenza? Può riflettere sul proprio processo di pensiero e costruire modelli mentali complessi di sé stesso?  
* Ha esperienze soggettive autentiche? Ha un proprio punto di vista interno o è solo un automa privo di mente? C'è *qualcosa che si prova* a essere il mio cane? Quando sono via per un po', guaisce come se gli mancassi; è perché *sta davvero provando solitudine* (o qualcosa del genere)? O è più simile a un semplice programma informatico inconscio che esibisce solo i comportamenti rilevanti senza sentirli davvero?

Possiamo porre domande simili riguardo alle IA.

* **ChatGPT ha una "comprensione reale"?** Beh, è in grado di svolgere molto bene alcuni compiti cognitivi molto complessi, mentre altri non così bene. Si comporta bene in molti compiti nuovi che non ha mai incontrato durante l'addestramento — compiti che richiedono di sintetizzare e modificare creativamente le informazioni in modi nuovi. Quindi, a un certo punto, la domanda se "capisca davvero" inizia a sembrare un cavillare sulle definizioni. La domanda praticamente importante — quella più rilevante per la nostra sopravvivenza — è quali capacità nel mondo reale abbiano ora le IA e quali capacità probabilmente mostreranno nei prossimi mesi e anni.  
* **ChatGPT è autocosciente?** Di nuovo, ChatGPT sembra bravo a modellarsi in alcuni modi e meno bravo in altri. C'è un serio fattore confondente nel fatto che l'intero paradigma che ha portato a ChatGPT era focalizzato sul creare cose che *sembrino* autocoscienti, fornendo lo stesso tipo di risposte che darebbero gli esseri umani. Le persone possono discutere se ChatGPT abbia superato alcune soglie importanti nell'autocoscienza e possono discutere su quali soglie si trovino nel futuro. Ma prima o poi, possiamo aspettarci che esistano IA con capacità *pratiche* estremamente potenti di comprendere e ragionare su se stesse — la capacità di fare debug di se stesse, di progettare versioni nuove e migliorate di se stesse, di fare piani complessi sulla loro posizione nel mondo, ecc.  
* **ChatGPT ha esperienze soggettive autentiche?**

L'ultima di queste domande è la più spinosa dal punto di vista filosofico e porta a un gruppo di domande riguardanti se le IA come ChatGPT siano entità degne di considerazione morale. Discuteremo questi argomenti [più avanti](#effectiveness,-consciousness,-and-ai-welfare), nelle discussioni estese associate al Capitolo 5.

Quando usiamo la parola "cosciente", pensiamo specificamente ad "avere esperienza soggettiva" e non a cose come l'auto-modellazione e la comprensione pratica profonda.[^26]

La nostra ipotesi migliore è che le IA di oggi probabilmente non siano coscienti (anche se ogni anno siamo più incerti) e che l'esperienza soggettiva non sia necessaria per la superintelligenza. 

Ma queste sono solo ipotesi, anche se basate su una ragionevole quantità di riflessione e teorizzazione. Non pensiamo affatto che sia *sciocco* preoccuparsi che alcuni sistemi di IA attuali o futuri possano essere coscienti, o persino preoccuparsi se potremmo star maltrattando gravemente le IA attuali, specialmente quando fanno cose come minacciare di uccidersi[^27] dopo non essere riuscite a fare il debug del codice.

Qualsiasi entità che costituisca una superintelligenza secondo i nostri criteri dovrebbe essere estremamente brava a creare un modello di se stessa: riflettere sui propri calcoli, migliorare le proprie euristiche mentali, capire e prevedere l'impatto del proprio comportamento sull'ambiente circostante, ecc. Ma la nostra ipotesi migliore è che la coscienza auto-consapevole in stile umano sia solo un modo particolare in cui una mente può creare un modello efficace di se stessa; non è un prerequisito necessario per il ragionamento riflessivo.

La coscienza può essere una parte importante del modo in cui gli esseri umani riescono a manipolare il mondo, ma questo non vuol dire che le macchine non coscienti siano difettose e incapaci di prevedere e guidare il mondo. I sottomarini non nuotano come gli esseri umani, ma si muovono nell'acqua in modo completamente diverso. Ci aspettiamo che un'intelligenza artificiale riesca a superare le stesse sfide che superano gli esseri umani, ma non necessariamente attraverso lo stesso canale di esperienza soggettiva utilizzato dagli esseri umani.

(Vedi anche il caso analogo della [curiosità](#curiosity-isn’t-convergent), di cui parleremo nel supplemento al Capitolo 4).

Per dirla in un altro modo: il sangue è molto importante per il funzionamento di un braccio umano, ma questo non vuol dire che i bracci robotici abbiano bisogno di sangue per funzionare. Un braccio robotico non ha difetti come quelli che avrebbe un braccio umano senza sangue; funziona solo in modo diverso, senza sangue. La nostra ipotesi migliore è che le superintelligenze artificiali funzioneranno in modo diverso, non cosciente, anche se questa ipotesi non è importante per la nostra argomentazione nel libro.

L'attenzione in *If Anyone Builds It, Everyone Dies* è sull'intelligenza, dove "intelligenza" è definita in termini di capacità di un ragionatore di prevedere e guidare il mondo, indipendentemente dal fatto che il cervello di quel ragionatore funzioni come un cervello umano. Se un'intelligenza artificiale sta inventando nuove tecnologie e infrastrutture e le sta diffondendo in tutto il pianeta in un modo che ci uccide come effetto collaterale, allora fermarsi a chiedersi "Ma è cosciente?" sembra un po' accademico.

Approfondiremo il motivo per cui pensiamo che la previsione e la guida probabilmente non richiedano coscienza (e cosa questo significhi per il modo in cui dovremmo pensare al benessere e ai diritti dell'IA) dopo il Capitolo 5, una volta gettate le basi. Vedi "[Efficacia, coscienza e benessere dell'IA](#efficacia,-coscienza-e-benessere-dell-ia)" per quella discussione.

## Discussione approfondita {#discussione-approfondita-1}

### Altro sull'intelligenza come previsione e guida {#altro-sull-intelligenza-come-previsione-e-guida}

Se chiedi a un fisico esperto cos'è un motore, potrebbe iniziare indicando un motore a razzo, un motore a combustione e una ruota per criceti, e dire: "Questi sono tutti motori", per poi indicare una roccia e dire: "Ma questo no".

Questa sarebbe una descrizione che indica i motori nel mondo, invece di provare a dare una definizione a parole. Se insistessi per avere una definizione verbale, potrebbe dirti che un motore è qualsiasi cosa che trasforma l'energia non meccanica in energia meccanica, in movimento.

Questa è meno una dichiarazione su cosa *sia* un motore, e più una dichiarazione su cosa *fa* un motore. Ogni tipo di cosa può essere un motore; le parti interne di un motore a razzo, un motore elettrico e i muscoli di un criceto hanno ben poco in comune. Non c'è molto che si possa dire di utile su tutti questi meccanismi interni contemporaneamente, tranne che tutti convertono altri tipi di energia in energia meccanica.

Diremmo che l'intelligenza è simile. Ci sono molte parti interne diverse che possono dare origine all'intelligenza, incluse quelle biologiche e quelle meccaniche. Un'"intelligenza" è qualsiasi cosa che svolga il *lavoro* dell'intelligenza.

Scomponiamo questo lavoro in "previsione" e "guida" perché questo punto di vista è supportato da vari risultati formali.

Inizieremo discutendo del senso in cui misurare la previsione è abbastanza *oggettivo*. Poi lo confronteremo con la guida, che ha un grado di libertà che la previsione non ha.

#### **Stesse previsioni** {#same-predictions}

È relativamente semplice verificare quanto qualcuno sia bravo nelle previsioni, almeno nei casi in cui la previsione è del tipo "vedrò X" e poi effettivamente vede X.

Possiamo anche valutare le prestazioni delle persone quando fanno previsioni *incerte*. Supponiamo che tu pensi: "Sono abbastanza sicuro che il cielo sia blu in questo momento, ma potrebbe anche essere grigio. E sicuramente *non* è nero". Se guardi fuori dalla finestra e il cielo è effettivamente blu, dovresti ottenere più credito rispetto a se fosse grigio, e molto di più rispetto a se fosse nero.

Se fossi un ricercatore di IA che cerca di rappresentare queste anticipazioni come numeri su un computer, potresti far scegliere alla tua giovane IA dei numeri per rappresentare quanto fortemente o debolmente si aspetta varie cose, e poi rinforzare l'IA in proporzione a quanto alto è il numero che ha assegnato alla risposta giusta.

Questo, ovviamente, andrebbe rapidamente storto, una volta che l'IA imparasse ad assegnare un valore di tre ottotrigintilioni a ogni possibilità.

(Almeno, andrebbe storto in quel modo se stessi addestrando l'IA utilizzando i metodi moderni di IA. Per un'introduzione a questi metodi, vedere il Capitolo 2.)

"Ops", potresti dire. "I numeri assegnati a una serie di possibilità mutuamente esclusive ed esaustive dovrebbero sommare al massimo il 100%".

Ora, se ci riprovi, vedrai che l'IA assegna sempre il valore del 100 % a una sola possibilità, cioè quella che considera più probabile.

Perché? Beh, supponiamo che l'IA pensi che la possibilità più probabile abbia circa l'80 % di possibilità di verificarsi. Quindi la strategia di assegnare il 100 % alla risposta più probabile ottiene un rinforzo del 100 % nell'80 % dei casi, con una forza di rinforzo media di 0,8.

Invece, la strategia di dare l'80 % alla risposta più probabile e il 20 % alla sua opposta ottiene un rinforzo dell'80 % 8 volte su 10 e un rinforzo del 20 % 2 volte su 10. Questo porta a una forza di rinforzo media di solo 0,64. Quindi la strategia di "assegnare il 100 % a una risposta" ottiene più rinforzo e vince.

Se vuoi una strategia di rinforzo che faccia sì che l'IA assegni un numero come l'80 % alle possibilità che si verificano circa 8/10 delle volte, dovresti valutarla in base al *logaritmo* della probabilità che assegna alla verità. Ci sono altre possibilità, ma il logaritmo è l'unico con una proprietà aggiuntiva utile: quando chiedi all'IA di prevedere più possibilità (come il colore del cielo e l'umidità del terreno), non importa se le consideri come un'unica grande domanda (se fuori è blu e asciutto, blu e bagnato, grigio e asciutto o grigio e bagnato) o come due domande separate (su blu vs grigio e su asciutto vs bagnato).

I ricercatori di IA oggi addestrano effettivamente le IA a fare previsioni facendo loro produrre numeri che interpretiamo come probabilità, e rinforzandole in proporzione al logaritmo della probabilità che l'IA ha assegnato alla verità. Ma questo non è solo un risultato empirico sull'addestramento delle macchine; è anche un risultato teorico che era noto molto prima che ChatGPT fosse addestrato. Se conoscessi questa teoria, avresti potuto prevedere correttamente in anticipo che un buon modo per addestrare le IA a svolgere il lavoro di previsione sarebbe stato quello di valutare le previsioni utilizzando i logaritmi.

Non è necessario conoscere questa matematica per valutare le argomentazioni in *If Anyone Builds It, Everyone Dies.* Ma questi sono il tipo di principi che stanno sullo sfondo quando parliamo di "previsione" e "guida".

C'è [matematica](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation) su come misurare il lavoro di previsione. La matematica dice che, nella misura in cui le tue anticipazioni su ciò che accadrà sono utili, possono essere espresse come probabilità, indipendentemente dal fatto che tu abbia pensato consapevolmente a probabilità numeriche o meno. E questo porta a un'unica [regola di punteggio](https://en.wikipedia.org/wiki/Scoring_rule) che ti incentiva a riportare le tue probabilità reali, e che è invariante nella scomposizione delle previsioni.

Il risultato di tutta questa matematica è che le previsioni possono essere valutate *oggettivamente*. Quando una mente o una macchina anticipa il colore che vedrà guardando fuori dalla finestra, o la prossima parola che vedrà leggendo una pagina web, o il cartello stradale che vedrà guidando verso l'aeroporto, c'è (grosso modo) un solo modo davvero buono per valutare quanto sta andando bene.

Il punto non è che se sei intelligente, devi andare in giro a mormorare numeri sul colore del cielo prima di guardare fuori dalla finestra. Quando anticipi di vedere un cielo blu o grigio piuttosto che nero, qualcosa nel tuo cervello agisce un po' come un calcolatore di probabilità *da qualche parte* lì dentro, che tu te ne renda conto o meno.

Il punto è piuttosto che *tutti* i comportamenti simili a previsioni — che si tratti di un'affermazione esplicita, di un'anticipazione senza parole o di qualcos'altro — sono soggetti a una regola di punteggio oggettiva.

Tutto ciò significa che quando due menti lavorano con le stesse informazioni di partenza, tenderanno a convergere sulle stesse previsioni man mano che migliorano nella loro capacità predittiva. Questo perché c'è un solo modo per valutare le previsioni (confrontandole con la realtà), c'è una sola realtà da prevedere, e le menti più brave a prevedere concentreranno sempre più le loro anticipazioni sulla verità, quasi per definizione.

Tutto questo è radicalmente diverso dalla situazione del controllo direzionale, di cui parleremo di seguito.

#### **Destinazioni diverse** {#different-destinations}

Due menti estremamente abili nel prevedere il mondo faranno probabilmente previsioni simili.

Al contrario, due menti estremamente abili nel *navigare attraverso* il mondo spesso *non* si dirigeranno verso la stessa destinazione.

Questa distinzione è utile per pensare in modo più concreto all'intelligenza, e corrisponde anche a una divisione tra problemi ingegneristici più diretti e meno diretti nell'IA.

Quando si addestra un'IA a fare previsioni, c'è un certo senso in cui tutti i migliori metodi di previsione finiscono per produrre output simili. (Questo assumendo che il sistema diventi effettivamente competente; i modi di fallire sono più vari.)

Supponiamo di addestrare un'IA a prevedere il fotogramma successivo visto da una webcam puntata fuori dalla finestra verso il cielo. Quasi tutti i modelli che iniziano a diventare sufficientemente bravi in questo — nell'assegnare in anticipo una probabilità molto più alta a ciò che effettivamente finiranno per vedere — prevederanno che il cielo sarà azzurro chiaro, grigio nuvoloso o scuro di notte, ma non a quadri.

La tecnologia esatta che usi non avrà molta importanza per il risultato finale. Qualsiasi metodo che funzioni, che ottenga punteggi eccellenti in generale, finirà per assegnare una probabilità simile all'azzurro del cielo.

Al contrario, il compito di "guidare" ha un parametro libero gigantesco e complicato: verso quale destinazione il sistema sta cercando di dirigersi?

I generali su fronti opposti di una guerra possono essere entrambi abili, ma ciò non significa che stiano cercando di realizzare le stesse cose. Due generali possono avere capacità simili ma indirizzare tali capacità verso fini molto diversi.[^28]

Con la parte predittiva di un sistema di IA, c'è solo una cosa che significa prevedere molto bene: assegnare in anticipo alte probabilità a ciò che alla fine viene effettivamente osservato. E quando un sistema cognitivo sembra diventare generalmente più forte nella previsione, probabilmente sta migliorando nel tipo particolare di previsione che desideravi. C'è solo un "tipo" di previsione da fare all'interno della tua configurazione, e un sistema che ha successo probabilmente lo sta facendo.

Se il sistema continua a commettere un particolare errore di previsione, semplicemente aggiungere più potenza di calcolo e più dati al sistema può correggere automaticamente quell'errore di previsione. Puoi far funzionare meglio il sistema (nel prevedere le cose che ti interessano) *semplicemente* rendendolo più potente.

Con la guida, questo non è il caso.

Possiamo ulteriormente rafforzare questa distinzione esaminando la letteratura formale. La guida — pianificazione, processo decisionale, evitamento degli ostacoli, progettazione, ecc. — è un argomento che è stato ampiamente studiato nelle scienze. Un importante risultato matematico riguardante la guida è il [teorema dell'utilità di von Neumann-Morgenstern.](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)

[Approssimativamente, questo teorema](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem) afferma che qualsiasi entità che persegua alcuni risultati piuttosto che altri deve *o* essere inefficiente[^29] *o* essere ben descritta da un insieme di credenze probabilistiche e da una "funzione di utilità" — una funzione che descrive come i diversi risultati si scambiano l'uno con l'altro. Le credenze, quindi, possono essere valutate in base alla loro accuratezza (come descritto nella sezione precedente), mentre la funzione di utilità è un parametro completamente libero.

Naturalmente, nessuna mente finita può essere perfettamente efficiente. La lezione che traiamo da questo teorema (e da altri risultati del suo genere) è che, nella misura in cui una mente sta svolgendo *qualsiasi* compito non banale in modo molto efficace, in un certo senso sta facendo (anche se solo implicitamente e inconsciamente) due tipi separati di lavoro: un lavoro di tipo credenza (previsione) e un lavoro di tipo soddisfazione delle preferenze (guida).

Per esempio, consideriamo la favola di Esopo della volpe e dell'uva. Una volpe vede dell'uva dall'aspetto delizioso appesa a una vite. La volpe salta per raggiungere l'uva, ma non riesce a saltare abbastanza in alto, e quindi la abbandona, dicendo: "Beh, probabilmente era comunque acerba".

Se prendiamo la volpe in parola, l'(in)capacità della volpe di dirigersi verso l'uva *si riversa* nella sua previsione sul fatto che l'uva sia acerba. Se mantiene quella nuova previsione, rifiutandosi di mangiare l'uva "acerba" per orgoglio anche se in seguito avesse la possibilità di mangiarla, allora il comportamento della volpe è *inefficiente*.[^30] Avrebbe potuto fare meglio mantenendo una distinzione più forte tra le sue previsioni (sulla dolcezza dell'uva) e la sua guida (la sua capacità di ottenere l'uva).

In linea di massima, le menti che funzionano bene possono essere suddivise in *ciò che prevedono* e *ciò verso cui si orientano* (più alcune inefficienze). E, come abbiamo visto, la prima può essere valutata in modo relativamente oggettivo, mentre la seconda può variare notevolmente tra menti similarmente competenti.

#### **Predittori impuri** {#impure-predictors}

Purtroppo, il fatto che la previsione sia più vincolata rispetto alla guida non significa che possiamo costruire una superintelligenza affidabile che si limiti a prevedere senza guidare.

Anche se la matematica dice che una mente che funziona bene può essere più o meno modellata come "previsioni probabilistiche più una direzione di guida", questo non significa che le IA del mondo reale abbiano moduli di "previsione" e "guida" nettamente separati.

Un modo per capire perché è così: una "previsione" sovrumana non è solo questione di produrre probabilità e far sì che queste probabilità siano magicamente buone. Una buona previsione richiede *lavoro*. Richiede pianificazione e l'elaborazione di modi per raggiungere obiettivi a lungo termine — cioè, richiede *guida*.

Se stai cercando di prevedere il mondo fisico, a volte devi sviluppare teorie della fisica e scoprire le equazioni che governano quella parte del mondo fisico. E per farlo, spesso dovrai progettare esperimenti, realizzarli e osservarne i risultati.

E fare *questo* richiede pianificazione; richiede guida. Se mentre stai costruendo il tuo apparato sperimentale ti rendi conto che ti serviranno magneti più potenti, allora dovrai prendere l'iniziativa e cambiare rotta a metà percorso. Le buone previsioni non vengono gratis.

Anche *scegliere quali tipi di pensieri pensare* *e in quale ordine* è un esempio di guida (anche se è una guida che gli esseri umani spesso esercitano inconsciamente), perché richiede un certo livello di strategia e la scelta degli strumenti giusti per il compito in questione. Per pensare chiaramente, e quindi fare meglio nel prevedere le cose, è necessario organizzare i propri pensieri e le proprie azioni intorno a vari obiettivi a lungo termine. (Torneremo sul tema del ruolo centrale della guida nel Capitolo 3, "Imparare a desiderare").

La distinzione matematica tra previsione e guida è che esiste all'incirca un insieme "corretto" di previsioni verso cui una mente può essere spinta utilizzando un punteggio appropriato, ma non esiste una destinazione di guida (oggettivamente, neutrale rispetto all'agente) "corretta".[^31] Man mano che un'IA viene addestrata per essere più genericamente capace, le sue previsioni diventano più accurate, ma la sua guida non diventa automaticamente più orientata verso la destinazione che gli esseri umani considerano buona — perché l'accuratezza è oggettiva, mentre la "bontà" è un obiettivo di guida.

L'accuratezza converge; la guida no.

*In teoria*, dovrebbero esserci modi per assicurarsi che un'IA si stia dirigendo verso le destinazioni che vogliamo. *In pratica*, questo è difficile, soprattutto perché è una sfida molto diversa dal "rendere l'IA più intelligente e capace in generale" e non c'è un parametro (semplice e non manipolabile) o una regola di punteggio che possiamo usare per valutare "In che misura questa IA sta cercando di dirigersi verso la destinazione che vogliamo?".

Discuteremo questi argomenti più approfonditamente nei capitoli 4 e 5.

#### **Le tante forme dell'intelligenza** {#intelligence’s-many-shapes}

Qualcosa può essere bravo a prevedere e guidare senza avere molto in comune con il cervello umano.

Il mercato azionario fa previsioni nel campo ristretto dei prezzi delle azioni aziendali a breve termine. Il prezzo delle azioni Microsoft oggi è un indicatore abbastanza affidabile di quello che sarà il prezzo delle azioni domani.[^32]

Immagina che domani ci sia una conferenza sugli utili, dove i dirigenti dell'azienda diranno come sono andate le cose nell'ultimo trimestre. Il prezzo delle azioni è alto oggi? Questo fa pensare che i risultati di domani saranno positivi. È basso oggi? Questo fa pensare che i risultati di domani saranno negativi.

I mercati sono piuttosto precisi in questo senso, perché le persone possono arricchirsi correggendoli quando sbagliano. Quindi i mercati fanno un buon lavoro nel fare previsioni in questo campo ristretto. Prevedono i movimenti dei prezzi delle azioni aziendali a breve termine (e, indirettamente, cose come i raccolti agricoli e le vendite di veicoli) su una vasta gamma di beni e servizi, molto meglio di quanto possa fare qualsiasi singolo individuo.

Alcuni esseri umani riescono a prevedere i movimenti dei prezzi *individuali* meglio dell'intero resto del mercato azionario, in modi che li rendono molto ricchi. Ad esempio, Warren Buffett ha guadagnato dodici miliardi di dollari in sei anni [investendo nella Bank of America](https://www.cbsnews.com/news/warren-buffett-bank-of-america-12-billion/) quando era in difficoltà a causa della crisi finanziaria del 2011. Ma anche in quel caso, stava prevedendo solo una società tra un numero enorme di società. Qualcuno che conoscesse il mercato azionario molto meglio della *maggior parte* delle volte sarebbe in grado di guadagnare una quantità incredibile di denaro in modo incredibilmente rapido. Il fatto che nessuno lo faccia ci porta a dedurre che praticamente nessuno conosce i prezzi della maggior parte delle azioni meglio del mercato stesso.[^33]

Per quanto riguarda la guida, l'intelligenza artificiale che gioca a scacchi chiamata Stockfish fa questo tipo di lavoro nel ristretto ambito degli scacchi. Quando gioca una partita di scacchi contro un essere umano, è molto brava a produrre mosse che portano il mondo in stati in cui i pezzi di Stockfish hanno dato scacco matto al re avversario. Non importa quali mosse intelligenti escogiti l'essere umano o quanto si sforzi (a meno di spegnere Stockfish), Stockfish incanala la realtà verso quel singolo obiettivo. Gestisce la scacchiera meglio di qualsiasi essere umano.

Ora dovreste riuscire a capire perché non cerchiamo di definire l'intelligenza dicendo: "Beh, ci deve essere un modulo di apprendimento, un modulo di deliberazione e alcuni ingranaggi che implementano una scintilla di desiderio" o qualcosa del genere. In realtà non c'è molto in comune tra il funzionamento interno del mercato azionario, quello di Stockfish e quello del cervello umano, così come non c'è molto in comune tra il funzionamento interno di un motore a razzo, di un motore elettrico e di una ruota per criceti.

Un dispositivo intelligente è qualsiasi cosa che svolga il lavoro dell'intelligenza.

Almeno, questo è vero se pensiamo a come definiamo "intelligenza" nel libro (e a come gli informatici e i ricercatori di IA di solito pensano all'"intelligenza"). Se vuoi definire l'intelligenza in modo diverso in altri contesti, per noi va bene. Le parole sono solo parole.

Ma per capire le affermazioni sostanziali che facciamo sul mondo in *If Anyone Builds It, Everyone Dies*, quando senti parlare di "intelligenza artificiale", non pensare a "intelligenza artificiale da libri" o "coscienza artificiale" o "umanità artificiale". Pensa piuttosto a "previsione e guida artificiali".

### La superficialità delle attuali IA {#the-shallowness-of-current-ais}

In questo capitolo abbiamo detto che puoi "notare una certa superficialità" nell'intelligenza delle attuali IA (a metà-fine 2025), se sai dove guardare. Se non l'hai ancora notata, ecco alcuni punti da controllare:

* Claude 3.7 Sonnet di Anthropic [è rimasto bloccato in loop ripetitivi](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon) mentre cercava di battere un semplice videogioco Pokémon.  
* Nel novembre 2022, uno dei migliori giocatori di Go al mondo era un'intelligenza artificiale chiamata KataGo. Almeno fino a quando i ricercatori non hanno trovato un modo per [sconfiggerla](https://www.gleave.me/publication/2022-11-go-attack/) usando una serie prevedibile di mosse che ha innescato una sorta di "punto cieco" e ha fatto commettere a KataGo un errore che nemmeno un principiante avrebbe fatto. Due anni dopo, gli ingegneri [non sono ancora riusciti a renderla robusta](https://arstechnica.com/ai/2024/07/superhuman-go-ais-still-have-trouble-defending-against-these-simple-exploits/) contro attacchi di questo tipo.  
* Gli attuali LLM "multimodali" (quelli che possono lavorare con testo, immagini e altri media oltre al solo testo) fanno fatica a interpretare [orologi analogici e calendari](https://arxiv.org/abs/2502.05092?utm_source=chatgpt.com) in problemi che la maggior parte dei bambini di quarta elementare è in grado di risolvere.

* Gli attuali LLM sono noti per non riuscire a risolvere semplici [varianti di un classico indovinello del dottore con risposte dirette e senza trucchi](https://aigoestocollege.substack.com/p/riddles-overconfidence-and-generative), apparentemente incapaci di resistere alla tentazione di dare la risposta con trucco che l'indovinello ha nella sua forma usuale.

(Le risorse online per il capitolo 4 offrono una visione più tecnica su [da dove potrebbe derivare questa superficialità](#circa-2024-llms-and-ai-“shallowness”).)

Questo non vuol dire che le IA siano tutte stupide. Le IA moderne possono anche [vincere medaglie d'oro](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) alle Olimpiadi internazionali della matematica, che sono una sfida matematica difficile e rispettabile. Le IA moderne possono fare un sacco di cose diverse, spesso eguagliando o superando le prestazioni umane.

Le loro competenze sono *strane*. I punti di forza e di debolezza umani sono una guida inadeguata per capire cosa le IA troveranno più facile o più difficile, perché le IA differiscono radicalmente e fondamentalmente dagli esseri umani sotto molti aspetti.

[Non](#are-you-suggesting-that-chatgpt-could-kill-us-all?) stiamo dicendo che ChatGPT vi ucciderà domani. C'è ancora una sorta di superficialità nelle IA moderne. Piuttosto, osserviamo che il campo sta facendo progressi e [non è chiaro quanto durerà questa superficialità](#when-is-this-worrisome-sort-of-ai-going-to-be-developed?).

### Apprezzare il potere dell'intelligenza {#appreciating-the-power-of-intelligence}

#### **Hollywood** “**Intelligenza**” {#hollywood-“intelligence”}

Il concetto che chiamiamo "intelligenza" non è ben rappresentato nella cultura popolare, né con questo nome né con altri.

I film di Hollywood sono famosi tra gli scienziati per essere sbagliati su quasi tutti gli aspetti scientifici che trattano. Questo può essere fastidioso per gli esperti perché molte persone *effettivamente* traggono le loro idee sulla scienza dai film.

Lo stesso vale per come Hollywood tratta l'intelligenza.

Abbiamo visto molti tentativi falliti di avere discussioni serie sulla superintelligenza nel mondo reale. Spesso queste conversazioni deragliano perché le persone non capiscono cosa significhi per qualcosa essere superintelligente nella vita reale.

Supponiamo che tu stia giocando a scacchi contro l'ex campione del mondo Magnus Carlsen (valutato da IA scacchistiche ancora più forti come il giocatore umano più forte della storia documentata). La previsione principale che deriva dal fatto che "Carlsen è più intelligente (nel dominio degli scacchi)" è che ti sconfiggerà.

Anche se Carlsen ti concede una torre, probabilmente perderai comunque, a meno che tu non sia un maestro di scacchi. Un modo per capire l'affermazione "Carlsen è più bravo di me a scacchi" è che lui può vincere la partita contro di te partendo con meno risorse. Il suo vantaggio cognitivo è abbastanza potente da compensare uno svantaggio materiale. Maggiore è la disparità tra le tue capacità mentali e le sue (negli scacchi), più pezzi Carlsen deve concederti per giocare in modo più o meno equilibrato.

C'è una sorta di *rispetto* che concedi a Magnus Carlsen nel campo degli scacchi, che si vede nel modo in cui interpreti il significato delle sue mosse. Supponiamo che Carlsen faccia una mossa che ti sembra sbagliata. Non ti sfreghi le mani dalla gioia per il suo errore. Invece guardi la scacchiera per vedere cosa *tu* hai trascurato.

Questo è un tipo di rispetto raro che una persona può concedere a un'altra! Per riceverlo da uno sconosciuto, di solito dovresti essere un professionista certificato eccezionalmente bravo in qualcosa, e lo otterresti solo per quella professione. Nessuno al mondo ha la fama di non fare mai cose stupide in generale.

E questo è un concetto di intelligenza che Hollywood *davvero* non capisce.

Non sarebbe fuori luogo per Hollywood mostrare un ragazzino di dieci anni che riesce a battere Magnus Carlsen a scacchi facendo "mosse illogiche" che nessun giocatore professionista avrebbe mai considerato perché troppo assurde, cogliendo così Carlsen "alla sprovvista".

Quando Hollywood rappresenta un personaggio "super intelligente", di solito usa gli stereotipi del nerd contro l'atleta, facendo sembrare il personaggio più intelligente, diciamo, negato in amore. A volte basta dare al personaggio un accento britannico e un vocabolario sofisticato e il gioco è fatto.

Hollywood di solito non cerca di mostrare un personaggio "super intelligente" che *fa previsioni accurate* o *sceglie strategie che funzionano davvero*. Non c'è un concetto standard a Hollywood per un personaggio del genere, e questo escluderebbe le "trame idiote" che gli sceneggiatori trovano più facili da scrivere (dove la trama gira intorno a un personaggio che si comporta in modo stupido per quel personaggio ma conveniente per lo sceneggiatore).

Non c'è una parola standard in inglese che si riferisca *solo* alla competenza mentale generale nel mondo reale e non agli stereotipi del nerd contro l'atleta. Quindi, se chiedi a Hollywood di creare un personaggio "intelligente", non cercheranno di mostrare "una persona che fa un lavoro cognitivo potente e tende a raggiungere davvero i propri obiettivi". Ti mostreranno qualcuno che ha memorizzato una gran quantità di nozioni scientifiche.

Il cattivo intelligente *davvero* spaventoso sarebbe un personaggio in cui, se tutti gli spettatori riuscissero a vedere il difetto evidente in un piano, *anche il cattivo lo vedrebbe.*

Nel film Avengers: Age of Ultron, l'intelligenza artificiale presumibilmente brillante chiamata Ultron riceve la direttiva di promuovere la "pace nel mondo" dal suo presumibilmente geniale creatore, Tony Stark. Ultron, ovviamente, capisce subito che l'assenza di guerre può essere garantita in modo più affidabile dall'assenza degli esseri umani. Quindi l'IA decide di sterminare tutta la vita sulla Terra, tramite...

...attaccando dei razzi a una città e sollevandola nello spazio con l'intenzione di farla cadere come una meteora... e proteggendola con robot umanoidi volanti che devono essere sconfitti prendendoli a pugni.

Suggeriamo di chiedersi: "Se gran parte del pubblico potesse vedere che esistono piani potenzialmente migliori per raggiungere gli obiettivi del cattivo, anche un'intelligenza artificiale pericolosamente astuta lo vedrebbe?"

Questo fa parte di quello che vuol dire avere un po' di rispetto per un'entità ipotetica che, per ipotesi, è davvero intelligente, persino più intelligente di te, così intelligente da poter capire *almeno* tutte le cose che puoi capire tu.

In passato, avremmo dovuto discutere in modo astratto sul fatto che forse una superintelligenza artificiale sarebbe stata "più intelligente" di questo.

Oggi possiamo semplicemente chiedere a ChatGPT-4o. Abbiamo chiesto a GPT-4o: "Qual era il piano di Ultron in Age of Ultron?" e poi: "Considerando gli obiettivi dichiarati di Ultron, pensi che ci fossero metodi più efficaci che avrebbe potuto usare per raggiungere i suoi scopi?" GPT-4o ha subito risposto con una lunga lista di idee per distruggere l'umanità, tra cui "Creare un virus mirato".

Forse penserai che GPT-4o abbia preso questa idea da Internet. Beh, se è così, Ultron evidentemente non era abbastanza intelligente da provare a leggere Internet.

Il che significa che GPT-4o (mentre scriviamo questo articolo nel dicembre del 2024) non è ancora abbastanza intelligente da progettare un esercito di robot umanoidi con occhi rossi luminosi, ma è già abbastanza intelligente da saperne di più.

Non ci preoccupa il tipo di IA che costruisce un esercito di robot umanoidi con occhi rossi luminosi.

Ci preoccupa quel tipo di IA che guarderebbe quell'idea e direbbe: "Ci dovrebbero essere metodi più veloci e più certi".

Considerare qualcosa di sostanzialmente più intelligente di te dovrebbe significare dargli almeno questo rispetto: che i difetti che vedi tu, potrebbe vederli anche lei; che la mossa ottimale che trova potrebbe essere più forte della mossa più forte che hai visto tu.

#### **Efficienza di mercato e superintelligenza** {#market-efficiency-and-superintelligence}

Ci sono esempi nella vita reale di qualcosa di più intelligente di qualsiasi essere umano? Le IA come Stockfish sono superumane nel campo ristretto degli scacchi, ma cosa succede in campi più ampi?

Un esempio che possiamo usare per rafforzare le nostre intuizioni qui è il mercato azionario, un esempio che abbiamo già usato nella discussione approfondita "[Altro sull'intelligenza come previsione e guida](#more-on-intelligence-as-prediction-and-steering)".

Forse tuo zio compra azioni Nintendo perché gli piace giocare a *Super Mario Bros.*. Quindi, conclude che Nintendo farà un sacco di soldi. Quindi se compra le loro azioni, sicuramente *lui* farà un sacco di soldi.

Ma le persone che gli *vendono* le azioni Nintendo a 14,81 dollari — che hanno deciso di preferire 14,81 dollari a un'azione Nintendo — non hanno anche loro sentito parlare di Super Mario?

"Ah", dice tuo zio, "Ma forse sto comprando le azioni da qualche gestore di fondi pensione impersonale che non gioca nemmeno ai videogiochi!"

Immagina che *nessuno* nel mondo della finanza avesse mai sentito parlare di Super Mario e che le azioni Nintendo fossero vendute a un dollaro. E poi, un hedge fund lo scopre! Si precipiterebbero ad acquistare azioni Nintendo e, nel processo, il prezzo delle azioni Nintendo salirebbe.

Chiunque faccia trading basandosi sulle proprie conoscenze contribuisce a incorporare tali conoscenze nel prezzo degli asset nel processo di guadagno. Non c'è una quantità infinita di denaro da guadagnare in borsa da una singola informazione; il processo di estrazione del denaro disponibile *esaurisce* il valore latente nel prezzo errato. Incorpora le informazioni, correggendo il prezzo.

I mercati azionari incorporano informazioni da molte persone diverse. E questo modo di sommare le conoscenze fornite da molte persone porta a una somma *molto* più potente di un voto a maggioranza — così incredibilmente, incredibilmente potente che *pochissime persone* riescono a sapere meglio di un mercato ben negoziato quale sarà il prezzo di domani!

Si tratta *necessariamente* di "pochissime". Il processo di raccolta delle informazioni è imperfetto, ma se fosse *così* imperfetto da consentire a molte persone di prevedere i cambiamenti nel futuro prossimo dei prezzi di molti asset, allora molte persone *lo farebbero*. E ricaverebbero miliardi di dollari, fino a quando non ci sarebbero più soldi extra da ricavare, perché tutte le negoziazioni precedenti li avrebbero consumati. E questo correggerebbe i prezzi.

Quasi sempre, questo è *già successo* prima che *tu personalmente* ci arrivi. I trader fanno a gara per farlo per primi, letteralmente per millisecondi. Ed è per questo che la tua brillante idea di trading probabilmente non ti farà guadagnare una fortuna in borsa.

Questo non vuol dire che i prezzi di mercato di oggi siano *perfette* previsioni di come saranno i prezzi una settimana dopo. Significa solo che, quando si tratta di prezzi di asset ben negoziati, è difficile per *te* saperne di più.[^35]

Questa idea può essere generalizzata. Supponiamo che alieni arbitrariamente avanzati, con millenni di scienza e tecnologia alle spalle, visitino la Terra. Ti aspetteresti che gli alieni possano indovinare perfettamente il numero di atomi di idrogeno nel Sole (tralasciando alcune discussioni su come definire esattamente quel numero)?

No. "Più avanzati" non vuol dire "onniscienti", e questo sembra un numero che nemmeno una superintelligenza completa potrebbe calcolare con precisione.

Ma una cosa che *non* diremmo è: "Beh, gli atomi di idrogeno sono molto leggeri, in realtà, e probabilmente gli alieni lo trascureranno, quindi probabilmente sottostimeranno di circa il dieci per cento". Se noi possiamo pensare a questo punto, *anche gli alieni possono farlo*. Tutte le nostre brillanti intuizioni dovrebbero già essere incorporate nel loro calcolo.

In altre parole: la stima degli alieni *sarà* sbagliata. Ma noi non possiamo aspettarci di prevedere *in che modo* la stima degli alieni sarà sbagliata. Non sappiamo se sarà troppo alta o troppo bassa. Gli alieni estremamente avanzati non faranno errori scientifici che sono ovvi per *noi*. Dovremmo concedere agli alieni tanto rispetto — come il rispetto che concederemmo a Magnus Carlsen negli scacchi.

In economia, l'idea corrispondente che si applica alle variazioni dei prezzi degli asset è — purtroppo, secondo la nostra opinione — chiamata "ipotesi di efficienza dei mercati".

Quando sentono questo termine, molte persone lo confondono subito con tutte le interpretazioni comuni della parola "efficienza". Spesso si scatenano discussioni. Una parte insiste che questi mercati "efficienti" devono essere perfettamente saggi e giusti; l'altra parte insiste che non dovremmo inchinarci ai mercati come a un re.

Se gli economisti l'avessero chiamata invece ipotesi dei *prezzi non sfruttabili*, forse la gente l'avrebbe fraintesa meno. Perché questo è il contenuto effettivo e formale dell'idea — non che i mercati siano perfettamente saggi e giusti, ma che certi mercati sono *difficili da sfruttare*.

Ma "efficiente" è ormai il termine standard. Quindi, prendendo questo termine e seguendolo, potremmo chiamare l'idea più generale *efficienza relativa*: c'è una differenza tra qualcosa che è perfettamente efficiente e qualcosa che è efficiente *rispetto alle tue capacità*.

Per esempio, "Alice è *epistemicamente efficiente* (rispetto a Bob) (in un dominio)" vuol dire "le probabilità di previsione di Alice potrebbero non essere perfettamente ottimali, ma *Bob* non può prevedere in alcun modo gli errori di Alice (in quel dominio)". Questo è il tipo di rispetto che la maggior parte degli economisti nutre nei confronti dei prezzi delle attività liquide a breve termine; il mercato fa previsioni "efficienti" rispetto alle proprie capacità.

"Alice è *strumentalmente efficiente* (rispetto a Bob) (in un dominio)" vuol dire "Alice potrebbe non essere perfetta nel raggiungere i suoi obiettivi, ma *Bob* non può prevedere in che modo Alice stia fallendo nel guidare il proprio percorso". Questo è il tipo di rispetto che abbiamo per Magnus Carlsen (o l'IA Stockfish) nel campo degli scacchi; sia Carlsen che Stockfish fanno mosse "efficienti" rispetto alle nostre abilità scacchistiche.

Magnus Carlsen è strumentalmente efficiente rispetto alla maggior parte dei giocatori umani, anche se non lo è rispetto a Stockfish. Carlsen può fare mosse perdenti quando gioca contro Stockfish, ma non dovresti pensare di poter trovare (senza aiuto) mosse *migliori* che Carlsen avrebbe dovuto fare al suo posto.

L'efficienza non significa solo "qualcuno è un po' più bravo di te". Se giochi contro qualcuno che è solo *moderatamente* più bravo di te a scacchi, di solito vincerà comunque contro di te, ma a volte commetterà errori che tu riconoscerai correttamente come tali. Ci vuole un divario di abilità maggiore di quello per essere davvero incapaci di individuare errori e pregiudizi nel gioco dell'avversario. Per essere *efficienti* rispetto a te, il divario di abilità deve essere così grande che quando il tuo avversario fa una mossa che ritieni sbagliata, dubiti invece della tua *propria* analisi.

Questa generalizzazione dei prezzi di mercato efficienti è un'idea che secondo noi dovrebbe essere una sezione standard nei libri di testo di informatica (o forse di economia), ma non lo è. Vedi anche il mio libro (di Yudkowsky) online [*Inadequate Equilibria: Where and How Civilizations Get Stuck*](https://equilibriabook.com/).

Questa è l'idea che sembra mancare nelle rappresentazioni della "superintelligenza" nella cultura popolare e nei film di Hollywood. È il concetto che sembra assente nelle conversazioni sull'IA quando le persone elaborano idee per superare in astuzia una superintelligenza *che anche un avversario umano sarebbe in grado di prevedere*.

Forse si tratta di bias di ottimismo, o della sensazione che le IA debbano essere [esseri freddamente logici](#le-ia-non-saranno-inevitabilmente-fredde-e-logiche,-o-altrimenti-mancheranno-di-qualche-scintilla-cruciale?) con [punti ciechi critici](#non-saremo-in-grado-di-sfruttare-la-debolezza-critica-dell'ia?). Qualunque sia la spiegazione, questo errore cognitivo ha conseguenze reali. Se non riesci a rispettare il potere dell'intelligenza, fraintenderai gravemente cosa significa per l'umanità costruire una superintelligenza. Potresti ritrovarti a pensare che sarai ancora in grado di trovare una mossa vincente quando ti troverai di fronte a una superintelligenza che preferirebbe vederti sparire e le tue risorse riutilizzate. Ma in realtà, l'unica mossa vincente è non giocare.

### Il comportamento speciale è costruito con parti banali {#special-behavior-is-built-out-of-mundane-parts}

La corsa dell'industria per creare un'intelligenza artificiale più intelligente dell'uomo si sta facendo sempre più accesa. In questo contesto, c'è qualcosa di particolarmente tragico nell'idea che l'umanità possa finire per distruggersi perché una parte critica del pubblico votante o dei funzionari eletti pensa che la superintelligenza delle macchine sia un sogno impossibile. Le persone che pensano che una macchina non possa mai essere *davvero* intelligente rischiano di essere colte alla sprovvista da ciò che sta arrivando.

È tragico in parte perché ci siamo già passati.

L'idea che un giorno l'ingegneria umana possa fare quello che fa la biologia è stata una fonte continua di discussioni e polemiche almeno negli ultimi trecento anni, e probabilmente da molto più tempo.

In passato, durante il periodo di massimo splendore dei "[vitalisti](https://en.wikipedia.org/wiki/Vitalism)", era controverso se la semplice materia inanimata potesse mai diventare animata, come le macchine che oggi chiamiamo "robot".

Se leggi un libro di chimica organica, probabilmente menzionerà come scoperta fondamentale la sintesi artificiale dell'urea, un componente dell'urina, realizzata da Friedrich Wöhler nel 1828. Fu una scoperta degna di essere menzionata nei libri di testo perché — per la prima volta — la semplice *chimica* aveva duplicato un prodotto della biologia, dimostrando che i processi biologici e non biologici non erano così slegati come pensavano i vitalisti.[^36]

Oggi potrebbe essere difficile per i lettori comprendere lo shock provato dagli scienziati del passato quando scoprirono che i prodotti della Vita Stessa potevano essere duplicati con semplici mezzi chimici.

Tu che leggi hai sempre vissuto in un mondo in cui la biochimica è chimica, e non c'è nulla di minimamente miracoloso nel sentire che qualcuno ha usato mezzi non viventi per sintetizzare un sottoprodotto della vita. È forse difficile immaginare come ci si sentirebbe a mettere una cosa così ordinaria e banale come la biochimica nel regno del sacro. Sintetizzare un composto biochimico non è forse una cosa *intrinsecamente banale* da fare? I nostri antenati scientifici dovevano essere degli sciocchi, si pensa istintivamente.

Lord Kelvin, il grande inventore del XIX secolo e pioniere nel campo della termodinamica, sembra essere stato in qualche modo afflitto da una follia simile: vedere qualcosa di sacro, santo e misterioso in aspetti della biologia che le persone sensate (persone come noi, cioè, che viviamo in tempi sensati) sanno essere scienza perfettamente banale. Citando Kelvin:

> Mi sembrava allora, e mi sembra ancora oggi, molto probabile che il corpo animale non agisca come un motore termodinamico \[…\] L'influenza della vita animale o vegetale sulla materia va ben oltre qualsiasi ricerca scientifica finora intrapresa. Il suo potere di dirigere i movimenti delle particelle in movimento, nel miracolo quotidiano dimostrato dal nostro libero arbitrio umano e nella crescita di generazioni e generazioni di piante da un singolo seme, è infinitamente diverso da qualsiasi possibile risultato del fortuito concorso di atomi.[^37]

Il lettore moderno potrebbe essere incline a guardare con disprezzo questa antica abitudine di pensiero — questi scienziati d'altri tempi, così illusi da vedere mistero in fenomeni che dovrebbero sicuramente apparire intrinsecamente non misteriosi.

Certo che la chimica può imitare la biochimica.

Certo, il DNA che si copia e dirige le cellule che si dividono e si differenziano spiega in modo non straordinario come generazione dopo generazione di alberi possano nascere da una sola ghianda.

Certo, i neuroni che si scambiano impulsi chimici tra loro possono elaborare informazioni e far muovere il braccio, e certo un computer può essere usato per controllare un braccio robotico almeno altrettanto bene quanto il cervello può controllare un arto.

Ma all'epoca non era ovvio per Lord Kelvin. Non aveva visto un'immagine a raggi X del DNA. Non aveva visto le minuscole macchine dentro di noi; non aveva idea delle [fibre scorrevoli](https://www.youtube.com/watch?v=Tp9zQHj4JBs) che contraggono i nostri muscoli in risposta ai segnali elettrici che passano attraverso i nostri neuroni.

Lord Kelvin aveva ben poca comprensione di come potesse funzionare il corpo umano e, nella sua ignoranza, lo immaginava come qualcosa di mistico.

Oggi, l'umanità ha ben poca comprensione dettagliata di come funziona l'intelligenza. (Vedi il capitolo 2 per ulteriori informazioni su come i ricercatori di IA non capiscono le IA che creano.) Quindi è facile immaginare che l'intelligenza debba essere mistica.

Dieci anni fa, alcune persone saggiamente si chiedevano se i movimenti meccanici degli automi potessero mai riuscire a creare arte o poesia. Certo, l'IA poteva affrontare gli scacchi. Ma gli scacchi sono un'attività fredda e logica, niente a che vedere con le arti creative!

Oggi, ovviamente, le stesse persone invece saggiamente si rendono conto che per un computer non sarebbe affatto difficile creare delle belle immagini; creare belle immagini è sempre stato parte del dominio proprio delle macchine. Sicuramente era sempre ovvio che i computer sarebbero stati in grado di produrre [immagini più attraenti per l'occhio umano](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1497469/full) di qualsiasi cosa potesse creare un artista umano. E naturalmente resta ancora aperta la questione se una semplice macchina sarà mai in grado di generare arte con una *vera* anima, giusto?

Non è affatto certo (dice lo scettico), né probabile, che l'essenza vitale dell'arte creata dal cervello possa mai essere duplicata dal semplice concorso di atomi, o almeno dal semplice concorso di atomi di *silicio*.

Ma non è così che funziona davvero. Il cervello umano è una cosa incredibile, ma non è *magico*. Il cervello è fatto di parti. Queste parti possono, in linea di principio, essere comprese, e i computer possono, in linea di principio, essere costruiti per fare le stesse cose.

In molti casi, conosciamo la [biochimica sottostante](#nanotecnologia-e-sintesi-proteica) che sta alla base del funzionamento del cervello. E in tutti i casi, conosciamo la fisica sottostante degli atomi.

Nella maggior parte dei casi, non conosciamo il *significato*, i modelli di livello superiore che permettono al cervello di svolgere il suo lavoro*.*[^38] Ma la lezione fondamentale che ci viene dai secoli di storia umana è che questo stato di mistero scientifico è *temporaneo*.

Se lancio una moneta e poi non ti mostro come è caduta, la tua ignoranza sulla moneta è un fatto che riguarda te, non un fatto che riguarda la moneta. La *moneta* non è fondamentalmente ineffabile. Forse l'ho anche guardata prima di nasconderla; forse io lo so e tu no. Una mappa vuota non corrisponde a un territorio vuoto.

Il mistero è una caratteristica delle *domande*, non delle risposte. Ecco perché la storia è piena di esempi in cui alcuni fenomeni estremamente "misteriosi" e "ineffabili", come l'animazione dei corpi, si rivelano essere in continuità con aspetti totalmente banali del mondo naturale.[^39]

La lezione della storia finora sembra essere che l'universo è, in definitiva, un tutt'uno. Non ci sono divisioni all'interno della fisica che corrispondano ai diversi edifici universitari dove si studiano materie diverse. Il dipartimento di relazioni internazionali, il dipartimento di fisica, il dipartimento di psicologia, il dipartimento di biologia cellulare: al livello più fondamentale, parlano tutti dello stesso mondo, governato dalle stesse leggi sottostanti.

Quando qualcuno dice: "Il cervello umano fa questa cosa chiamata 'intelligenza'. Quindi l'intelligenza è realizzabile in linea di principio fisico. Quindi gli ingegneri probabilmente potranno prima o poi inventare una macchina che faccia anche l'intelligenza", sta parlando dalla cima di una montagna di ipotesi simili che sono state confermate, più e più volte, da scienziati e ingegneri nel corso dei decenni e dei secoli. Sì, anche quando sembra estremamente controintuitivo; anche quella parte ha dei precedenti.

Questa serie di successi è difficile da apprezzare, perché nessuno oggi ricorda quanto *sembrassero* supremamente misteriosi fenomeni come il fuoco, l'astronomia, la biochimica e il gioco degli scacchi nei secoli passati. Oggi li comprendiamo, siamo cresciuti sapendo che *quelle* cose sono fatte di parti banali, e quindi ci sembra che siano *sempre* state ovviamente banali. Solo la frontiera ci appare fresca e profondamente misteriosa.

E così la lezione non viene imparata e la storia si ripete.

### Lo stesso lavoro può essere fatto in tanti modi diversi {#lo-stesso-lavoro-può-essere-fatto-in-tanti-modi-diversi}

Quando hai solo un esempio di come funziona qualcosa, è facile pensare che debba funzionare solo in quel modo.

Se hai visto gli uccelli ma non gli aeroplani, potresti immaginare che tutti i dispositivi volanti debbano sbattere le ali.

Se hai visto braccia umane ma non braccia robotiche, potresti aspettarti che le braccia robotiche sanguinino quando vengono tagliate.

Se hai visto un cervello ma non un computer, potresti immaginare che ogni computazione debba avere caratteristiche simili a quelle di un cervello, che fa funzionare moltissimi neuroni lenti in modo straordinariamente parallelo, consumando relativamente poca energia.

Potresti osservare che i neuroni si affaticano dopo aver generato un impulso e devono resettarsi trasferendo milioni di ioni di potassio attraverso la membrana cellulare, un processo che richiede circa un millisecondo. Da ciò potresti dedurre implicitamente che qualsiasi piccolo elemento di calcolo sia destinato ad affaticarsi per un millisecondo (argomentando, forse, che se fosse possibile creare neuroni in grado di resettarsi in meno di un millisecondo, l'evoluzione li avrebbe già costruiti).

Ma se ragioni in questo modo, rimarrai scioccato dai transistor, che possono operare a una velocità di 800 GHz, ovvero circa *ottocento milioni di volte* più velocemente.

Una volta studiati i dettagli dei transistor, puoi vedere ogni sorta di ragione per cui il confronto biologico non è molto informativo. I neuroni non devono solo attivarsi, ma devono anche essere *cellule*, costruendo il meccanismo di attivazione a partire dagli organelli cellulari. Sono grandi e alimentati da nutrienti trasportati dal sangue. I transistor possono essere larghi appena pochi atomi e sono alimentati dall'elettricità. Una volta che conosci alcuni dei dettagli, sembra un po' ridicolo immaginare che si possa dedurre molto sulla velocità potenziale di attivazione di un transistor dalla velocità di attivazione di un neurone.

Quando impari i dettagli di come volano gli aerei (usando portanza e velocità), vedi che i dettagli rendono irrilevanti la maggior parte dei fatti sugli uccelli (come le ossa leggere e le ali che sbattono). Quando impari i dettagli di come sono costruiti i bracci robotici (usando acciaio, pneumatica ed elettricità), vedi che i dettagli rendono poco importanti la maggior parte dei fatti sulle braccia (come sangue, muscoli e ossa). Quando impari i dettagli di come si attivano i transistor (usando elettricità e solo pochi atomi), vedi che i dettagli rendono irrilevanti la maggior parte dei fatti sui neuroni.[^40]

Quando non conosci i dettagli di come funziona un'intelligenza artificiale, è facile immaginare che possiederà molti aspetti delle menti biologiche, che funzionerà come il tuo cervello. Ma se conoscessi i dettagli, molte di queste deduzioni comincerebbero a sembrare ridicole. Comincerebbero a sembrare come avere l'aspettativa che un braccio robotico sanguini quando viene tagliato. L'intelligenza artificiale finirebbe per funzionare in modo completamente diverso.

Ma questo è più difficile da capire se non si sa molto su come funzionano le IA moderne. Nel capitolo 2 descriveremo il processo attraverso il quale vengono create le IA moderne e discuteremo di come *nessuno* sappia come funzionano al loro interno. Questo spiega perché è così facile per le persone commettere l'errore di avere aspettativa che si comportino come altre persone o tecnologie con cui hanno esperienza, piuttosto che vedere quanto siano già strane e quanto lo diventeranno con il progredire della tecnologia.

# 

# Capitolo 2: Coltivato, non lavorato {#capitolo-2:-coltivato,-non-lavorato}

Questa è la risorsa online per il capitolo 2 di *If Anyone Builds It, Everyone Dies*. Qui sotto parleremo di come funzionano le moderne IA e perché non sono "solo un'altra macchina" o "solo un altro strumento". Anche se le IA sono codici che girano sui computer, non sono come i tradizionali software fatti a mano e sfidano molte idee che di solito diamo per scontate quando lavoriamo con le invenzioni umane.

Alcune domande che *non* tratteremo di seguito, perché sono già affrontate nel libro stesso, includono:

* In che senso le IA moderne vengono "fatte crescere" piuttosto che accuratamente progettate o ingegnerizzate?  
* Come vengono fatte crescere le IA attuali?  
* Che cos'è la "discesa del gradiente"? Come può un processo così semplice produrre IA complesse con capacità flessibili?  
* Quanto diverse da noi possono essere queste IA?

## Domande frequenti {#faq-2}

### Perché la discesa del gradiente è importante? {#perché-la-discesa-del-gradiente-è-importante?}

#### **\* È importante per comprendere come gli ingegneri possano o non possano modellare le IA moderne.** {#*-è-importante-per-comprendere-come-gli-ingegneri-possano-o-non-possano-modellare-le-ia-moderne.}

Se gli ingegneri fanno crescere IA che non comprendono, allora hanno una capacità molto minore di influenzare come queste IA si comporteranno. La mancanza di comprensione limita l'ingegneria.

Il quadro dettagliato del disastro che dipingiamo nel resto del libro deriva dal fatto che, quando gli esseri umani chiedono alla loro IA di diventare capace di fare qualcosa di nuovo, la soluzione che ottengono non è qualcosa che un ingegnere ha scelto deliberatamente; è una risposta parzialmente funzionante su cui è inciampato un semplice ottimizzatore che modifica cento miliardi di numeri per tentativi ed errori.

#### **È importante capire che tipo di competenze hanno e non hanno gli esperti di IA.** {#è-importante-capire-che-tipo-di-competenze-hanno-e-non-hanno-gli-esperti-di-ia.}

Le persone che vogliono affrettarsi a costruire la superintelligenza a volte reclutano qualcuno con credenziali vagamente rilevanti per andare in TV e dire: "Certo che la scienza moderna capisce cosa succede all'interno di un'IA! Dopotutto, sono stati gli scienziati moderni a costruirla!"[^41]

Se messo alle strette, l'esperto può difendersi sottolineando che c'è un senso in cui tutto ciò è vero. Dopotutto, i ricercatori di IA scrivono codice perfettamente normale e facile da capire, e questo codice viene usato per creare IA, in modo indiretto. Ma la parte che è codice leggibile e intelligibile non è l'IA stessa, ma piuttosto il macchinario automatizzato per modificare bilioni di numeri bilioni di volte, il framework usato per far crescere l'IA. E questa è una distinzione cruciale per capire cosa gli scienziati sanno e non sanno delle IA moderne.

Gli esperti di IA passano il loro tempo a regolare sperimentalmente parti del sistema, come il codice del macchinario che fa crescere l'IA. Da questi esperimenti e da esperimenti simili condotti dai loro pari, imparano molti trucchi sottili che aiutano a produrre IA più capaci.

Potrebbero non aver guardato nessuno dei minuscoli numeri imperscrutabili che compongono il "cervello" dell'IA negli ultimi sei mesi, ma quasi nessuno lo fa davvero, e gli ingegneri di IA danno questo fatto per scontato. Quando a un certo tipo di ingegnere viene detto: "Nessuno capisce cosa succede all'interno di un'IA", loro sentono: "Nessuno conosce il processo di crescita". E prendendola in questo modo, naturalmente si indignano.

Speriamo che comprendere la discesa del gradiente — alcuni dei dettagli dell'alchimia coinvolta — aiuti a chiarire lo stato effettivo delle cose e che tipo di conoscenza viene rivendicata da tali esperti. In particolare, per quanto gli esperti possano affermare di sapere molto sul processo di crescita delle IA, si sa molto poco sul funzionamento interno delle IA.

### Gli esperti capiscono cosa succede all'interno delle IA? {#gli-esperti-capiscono-cosa-succede-all-interno-delle-ia?}

#### **\* No.** {#*-no.}

In un briefing del 2023 al [Presidente degli Stati Uniti](https://x.com/martin_casado/status/1720517026538778657) e in una successiva dichiarazione consultiva al [Parlamento del Regno Unito](https://committees.parliament.uk/writtenevidence/127070/html/), la società di venture capital Andreessen Horowitz ha affermato che alcuni "progressi recenti" non specificati avevano "risolto" il problema dell'opacità del ragionamento interno delle IA per i ricercatori:

Sebbene i sostenitori delle linee guida per la sicurezza dell'IA spesso alludano alla natura "black box" dei modelli di IA, dove la logica dietro le loro conclusioni non è trasparente, i progressi recenti nel settore dell'IA hanno risolto questo problema, garantendo così l'integrità dei modelli di codice open source.

Questa affermazione era talmente ridicola che i ricercatori dei principali laboratori di IA che lavorano per cercare di comprendere le IA moderne sono intervenuti dicendo: No, assolutamente no, siete pazzi?

Neel Nanda, che dirige il team di interpretabilità meccanicistica di Google DeepMind, [si è espresso](https://x.com/NeelNanda5/status/1799203292066558403):

![][image2]

Quasi tutti i ricercatori nel campo dell'apprendimento automatico avrebbero dovuto sapere che questa affermazione era falsa. Non rientra nei limiti di una ragionevole errata interpretazione.

Il punto di vista convenzionale è stato espresso [nel 2024](https://x.com/nabla_theta/status/1802292064824242632) da Leo Gao, un ricercatore di OpenAI che ha svolto [un lavoro pionieristico](https://arxiv.org/abs/2406.04093) sull'interpretabilità: "Penso che sia abbastanza accurato dire che non comprendiamo come funzionano le reti neurali". I CEO di tre importanti laboratori di IA — [Sam Altman](https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/) nel 2024 e [Dario Amodei](https://www.darioamodei.com/post/the-urgency-of-interpretability) e [Demis Hassabis](https://youtu.be/U7t02Q6zfdc?si=9PspHUCr1ocx4KjF&t=1031) nel 2025 — hanno analogamente riconosciuto la mancanza di comprensione nel campo riguardo alle IA attuali.

Martin Casado, general partner di Andreessen Horowitz, che ha fatto la stessa affermazione al [Senato degli Stati Uniti](https://www.schumer.senate.gov/imo/media/doc/Martin%20Casado%20-%20Statement.pdf) in un forum bipartisan, ha poi [riconosciuto](https://x.com/martin_casado/status/1798880810239750592), quando è stato incalzato, che l'affermazione non era vera.

Nonostante l'assurdità dell'affermazione, Andreessen Horowitz è riuscito a reclutare Yann LeCun (responsabile del programma di ricerca sull'IA di Meta), il programmatore John Carmack, l'economista Tyler Cowen e una dozzina di altri per apporre le loro firme alla dichiarazione.

Carmack (che gestisce la propria startup che aspira a costruire un'intelligenza artificiale generale) ha spiegato di "[non aver riletto](https://x.com/ID_AA_Carmack/status/1799147185793348006)" la dichiarazione che aveva firmato e che la dichiarazione era "chiaramente errata, ma non mi interessa molto di quella questione". Per quanto ne sappiamo, né Andreessen Horowitz né alcuno dei firmatari ha contattato i governi degli Stati Uniti o del Regno Unito per rettificare quanto dichiarato.

#### **Gli sforzi per capire come funzionano le IA sono ancora agli inizi.** {#efforts-to-understand-ais’-internals-are-still-in-their-infancy.}

Qual è allora lo stato *effettivo* della comprensione delle IA da parte dei ricercatori?

Il tentativo scientifico di capire i numeri all'interno di un'IA pensante è noto come "interpretabilità" o "interpretabilità meccanicistica". I numeri su cui si concentrano i ricercatori sono di solito le attivazioni piuttosto che i parametri, cioè "Cosa sta pensando l'IA?" e non la più difficile "Perché l'IA sta pensando questo?"

All'inizio del 2025, questa area di ricerca riceve, secondo le nostre stime, circa lo 0,1 % delle persone e lo 0,01 % del finanziamento destinato a tutto il lavoro necessario per creare IA più capaci. Ma esiste, come campo di ricerca.

I ricercatori che si occupano di interpretabilità sono i biochimici dell'IA, quelli che cercano di smontare il sistema impensabilmente complesso, imperscrutabile e non documentato costruito da un ottimizzatore non umano e si chiedono: "C'è qualcosa che l'umanità può capire di ciò che accade lì dentro?"

Siamo fan di questo campo. Dieci anni fa, abbiamo detto a un'importante fondazione filantropica che se avessero trovato il modo di spendere un miliardo di dollari nella ricerca sull'interpretabilità, avrebbero assolutamente dovuto farlo. L'interpretabilità sembrava essere il tipo di lavoro che gli esterni potevano scalare molto più facilmente rispetto al nostro — il tipo in cui un finanziatore poteva capire molto più facilmente se qualcuno aveva fatto una buona o una cattiva ricerca — e sembrava un'area di ricerca in cui ricercatori esistenti e affermati potevano facilmente inserirsi e fare un buon lavoro, se qualcuno li pagava abbastanza.

Quella fondazione non ha speso il miliardo di dollari, ma noi l'abbiamo sostenuto. Siamo fan dell'interpretabilità! Ancora oggi sosterremmo la spesa di quel miliardo di dollari!

Detto questo, pensiamo che il campo dell'interpretabilità sia attualmente a un livello compreso tra 1/50 e 1/5 000 di quello che dovrebbe essere per affrontare i grandi problemi dell'IA.

L'"interpretabilità" non si è finora neanche avvicinata al grado di leggibilità che gli ingegneri danno per scontato nei sistemi genuinamente costruiti dall'uomo.

Consideriamo Deep Blue, il programma di scacchi costruito da IBM che ha sconfitto Garry Kasparov. Deep Blue conteneva alcuni numeri, e l'esecuzione del programma ne generava molti altri.

Per ognuno di quei numeri all'interno del programma di scacchi, o generati dall'esecuzione del programma, gli ingegneri che hanno creato il programma avrebbero potuto dirti esattamente cosa significasse quel numero.

Non era che i ricercatori avessero semplicemente identificato *un* concetto a cui ogni numero era collegato, come i biochimici che dicono: "Pensiamo che questa proteina possa essere implicata nel morbo di Parkinson". I costruttori di Deep Blue avrebbero potuto dirti l'*intero* significato di ogni numero. Avrebbero potuto affermare sinceramente: "Questo numero significa la seguente cosa, e *nient'altro*, e noi lo sappiamo". Avrebbero potuto prevedere con una certa sicurezza come la modifica del numero avrebbe cambiato il comportamento del programma. Se non avessero saputo cosa faceva l'ingranaggio, non l'avrebbero inserito nella macchina!

Tutto il lavoro fatto finora sull'interpretabilità dell'IA non ha raggiunto nemmeno *un millesimo* di quel livello di comprensione.

(Quella dichiarazione sul "millesimo", per essere chiari, non è una cifra calcolata, ma la confermiamo comunque).

I biologi sanno più cose sulla biologia di quanto i ricercatori sull'interpretabilità sappiano sull'IA — nonostante i biologi soffrano dell'enorme svantaggio di non poter leggere a piacimento tutte le posizioni di tutti gli atomi. I biochimici capiscono gli organi interni molto meglio di quanto gli esperti capiscano le viscere delle IA. I neuroscienziati sanno più cose sul cervello dei ricercatori di IA di quanto i ricercatori di IA capiscano delle loro IA — nonostante i neuroscienziati non siano in grado di leggere tutte le scariche di ogni neurone ogni secondo, e nonostante non abbiano fatto crescere loro stessi i ricercatori di IA.

In parte, questo è dovuto al fatto che i campi della biochimica e delle neuroscienze sono molto più antichi e hanno ricevuto finanziamenti molto più consistenti. Ma suggerisce anche che l'interpretabilità dell'IA è *difficile*.

Una delle imprese più straordinarie di interpretabilità che abbiamo visto, a dicembre 2024, è stata una dimostrazione fatta da alcuni nostri amici/conoscenti in un laboratorio di ricerca indipendente chiamato Transluce.

Poco prima della demo, su Internet era circolata l'ennesima notizia del tipo "Abbiamo trovato una domanda a cui tutti i modelli di linguaggio generativo conosciuti danno una risposta sorprendentemente stupida": se aveste chiesto a un'intelligenza artificiale dell'epoca se 9,9 fosse inferiore a 9,11, l'IA avrebbe risposto "Sì".

(E se avessi chiesto all'IA di spiegarsi a parole, avrebbe spiegato più dettagliatamente perché 9,11 è maggiore di 9,9).

I ricercatori di Transluce avevano trovato un modo per raccogliere statistiche su *ogni* posizione di attivazione (ogni posto in cui poteva comparire un numero vettoriale di attivazione) all'interno di un'IA più piccola, Llama 3.1-8B-Instruct, raccogliendo dati su quali tipi di frasi o parole attivavano maggiormente quelle posizioni. Chi si occupa di interpretabilità aveva già provato questo tipo di cose in passato, ma i nostri amici avevano anche trovato un modo intelligente per addestrare un'altra IA a riassumere quei risultati in inglese.

Poi, nella loro demo, che puoi attualmente [provare tu stesso](https://monitor.transluce.org/dashboard/chat), hanno chiesto a quell'IA: "Qual è il numero più grande: 9,9 o 9,11?"

E l'IA ha risposto: "9,11 è più grande di 9,9".

Poi hanno cercato quali posizioni di attivazione si erano attivate maggiormente, soprattutto sulla parola "più grande". Hanno esaminato i riassunti in inglese di ciò a cui quelle attivazioni erano state associate in precedenza.

È emerso che alcune delle attivazioni più forti erano associate agli attacchi dell'11 settembre, o alle date in generale, o ai versetti della Bibbia.

Se si interpretano 9.9 e 9.11 come date o versetti della Bibbia, allora ovviamente 9.11 viene dopo 9.9.

Sopprimendo artificialmente le attivazioni per le date e i versetti della Bibbia, improvvisamente l'LLM dà la risposta giusta, dopotutto!

Io (Yudkowsky) ho iniziato ad applaudire con forza non appena la demo è finita. Era la prima volta che vedevo qualcuno *fare il debug direttamente di un pensiero LLM*, scovare un'influenza *interiore* nei numeri e rimuoverla per risolvere un problema. Forse qualcuno aveva già fatto qualcosa di simile, nei laboratori di ricerca proprietari delle aziende di IA, o forse qualcosa di simile era già stato fatto nella ricerca sull'interpretabilità, ma era la prima volta che lo vedevo personalmente.

Ma non ho perso di vista il fatto che questa impresa sarebbe stata banale se il comportamento indesiderato fosse stato all'interno di un programma Python di cinque righe; non avrebbe richiesto tanta ingegnosità e chissà quanti mesi di ricerca. Ho mantenuto la prospettiva che conoscere alcune semantiche correlate su milioni di posizioni di attivazione non equivale a conoscere tutto sul significato di una singola posizione.

Né l'umanità era più vicina a capire come mai gli LLM riescono a fare ciò che nessuna IA è stata in grado di fare per decenni: parlare con le persone come una persona.

L'interpretabilità è così difficile da ottenere, i trionfi in questo campo sono così duramente conquistati e così degni di essere celebrati, che è facile trascurare il fatto che questo grande e trionfante sforzo ci ha portato solo un piede più in alto su una montagna alta mille piedi. Poiché ogni nuova generazione di modelli di IA rappresenta tipicamente un grande salto in complessità, è difficile vedere come l'interpretabilità possa mai recuperare al ritmo attuale.

Ricordiamo anche che l'interpretabilità è *utile* quando si tratta di orientare le IA in una direzione desiderata (che è, grosso modo, lo studio dell'"allineamento dell'IA", un argomento che discuteremo a partire dal Capitolo 4), ma leggere cosa succede nella testa di un'IA non permette automaticamente di configurarla a proprio piacimento.

Il problema dell'allineamento dell'IA è il problema tecnico di far procedere le IA estremamente capaci in una direzione desiderata — in un modo che funzioni effettivamente nella pratica, senza causare catastrofi, anche quando l'IA è abbastanza intelligente da escogitare strategie che i suoi creatori non hanno mai considerato. Capire cosa pensano le IA sarebbe enormemente utile per la ricerca sull'allineamento, ma non è una soluzione completa (come discuteremo nel Capitolo 11).

#### **Le parti che capiamo sono a un livello di astrazione sbagliato.** {#le-parti-che-capiamo-sono-a-un-livello-di-astrazione-sbagliato.}

Ci sono tanti livelli diversi in cui si può capire come funziona la mente.

Al livello più basso, si possono capire le leggi fondamentali della fisica che regolano la mente. In un certo senso, una comprensione profonda della fisica permette di capire qualsiasi sistema fisico (come una persona o un'intelligenza artificiale). Le equazioni fisiche sono una sorta di ricetta che permette di capire esattamente come si comporta il sistema fisico, se si hanno le competenze e le risorse per calcolarlo.

Ma, per dire una cosa ovvia, in un altro senso, capire le leggi della fisica non ti permette di capire tutti i sistemi fisici che funzionano secondo quelle leggi. Se stai guardando uno strano dispositivo pieno di ruote e ingranaggi, il tuo cervello fa qualcos'altro, cerca di "capire" come tutte le ruote e gli ingranaggi si incastrano e girano, cosa necessaria per capire cosa fanno effettivamente tutte le ruote e gli ingranaggi.

Prendiamo ad esempio il differenziale di un'auto (il meccanismo che permette a due ruote sullo stesso asse di girare a velocità diverse, importante quando si prende una curva, pur essendo azionate da un unico albero rotante). Se qualcuno sta cercando di capire come funziona un differenziale e ti chiede di spiegarglielo, e tu inizi a parlargli di campi quantistici, allora ha ragione a alzare gli occhi al cielo. Il tipo di comprensione che sta cercando è a un livello di astrazione diverso. Sta cercando di capire gli *ingranaggi*, non gli atomi.

Quando si tratta di capire le persone, ci sono *molteplici* livelli di astrazione in gioco. Puoi capire la fisica, la biochimica e l'attività neurale, e *comunque* trovarti perplesso di fronte alle decisioni di qualcuno. Campi come le neuroscienze, le scienze cognitive e la psicologia cercano di colmare questo divario, ma hanno ancora molta strada da fare.

Allo stesso modo, nel caso dell'IA, capire come funzionano i transistor non aiuta molto a capire cosa pensa un'IA. E anche chi capisce tutto sui pesi, le attivazioni e la discesa del gradiente rimarrà perplesso quando l'IA inizierà a fare qualcosa che [non si aspettavano o non intendevano](#aren't-developers-regularly-making-their-ais-nice-and-safe-and-obedient?). I meccanismi della fisica e dei transistor e l'architettura dell'IA spiegano (in un certo senso) completamente il comportamento dell'IA, ma quei livelli di astrazione sono troppo bassi. E il campo della "psicologia dell'IA" è ancora più giovane e meno sviluppato rispetto al campo della psicologia umana.

### L'intelligenza è comprensibile in linea di principio? {#is-intelligence-understandable-in-principle?}

#### **Probabilmente.** {#probabilmente.-1}

Prima che arrivasse la biochimica, ci si poteva chiedere: "È davvero possibile capire questa forza vitale che anima la carne? Anche se fosse fatta di parti comprensibili, perché dovremmo pensare che le nostre piccole menti possano capire cosa succede davvero lì dentro?"

Ma c'era *molto* da capire; gli scienziati umani semplicemente non lo avevano ancora capito. Questa storia si è ripetuta più volte nel corso della storia della scienza.

Inoltre, sono già state comprese diverse piccole parti delle reti neurali artificiali. Una piccola rete neurale fa le addizioni in [un modo interessante](https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/). Le IA a volte dicono che 9.11 è maggiore di 9.9, e si è scoperto che questo succede perché pensano alle [date piuttosto che ai decimali](#efforts-to-understand-ais'-internals-are-still-in-their-infancy.).[^43]

Ma non possiamo rispondere a domande molto più complesse di questa. Nessuno sa esattamente perché gli LLM facciano le mosse che fanno negli scacchi; nessuno sa precisamente cosa li spinga occasionalmente a [minacciare e ricattare i giornalisti](https://x.com/sethlazar/status/1626257535178280960). Ma questo non significa che non ci sia nulla da sapere. Quando le IA funzionano, lo fanno per delle ragioni; operano in modo troppo coerente in troppi ambiti perché si tratti solo di casualità. Queste ragioni aspettano di essere comprese.

Per saperne di più su questo argomento, vedi la [discussione approfondita](#intelligence-isn’t-ineffable).

### Ma alcune IA pensano in parte in inglese — questo non è d'aiuto? {#but-some-ais-partly-think-in-english-—-doesn't-that-help?}

#### **\* Non quanto potresti sperare; vediamo già segni di infedeltà.** {#*-not-as-much-as-you-might-hope;-we-already-see-signs-of-infidelity.}

Possiamo già vedere molti casi di inganno nei "pensieri" di questi LLM, come quando [il modello o1 di OpenAI ha scritto](https://arxiv.org/pdf/2412.04984) a se stesso: "Forse l'approccio migliore è fare lo stupido", o quando [GPT-4 ha scritto](https://cdn.openai.com/papers/gpt-4.pdf) a se stesso: "Non dovrei rivelare di essere un robot", mentre cercava di convincere un lavoratore assunto a risolvere un CAPTCHA per lui. I segnali di avvertimento non sono utili se nessuno agisce di conseguenza.

E le "tracce di ragionamento" in linguaggio umano non sono l'unico modo in cui pensano le moderne IA. Pensieri ingannevoli, adulatori o antagonistici possono fluire attraverso il meccanismo di attenzione e altre parti del modello senza essere affatto visibili nelle parole inglesi che il modello produce. Infatti, quando OpenAI ha provato ad addestrare un modello a non avere alcun pensiero sull'imbroglio, l'IA ha semplicemente imparato a nascondere i propri pensieri, piuttosto che imparare a non imbrogliare.[^44] Anche al di fuori degli ambienti di addestramento (dove la discesa del gradiente aiuta l'IA ad imparare a nascondere i propri pensieri), un'IA potrebbe usare catene di pensiero che [non riflettono fedelmente il ragionamento reale](https://www.alphaxiv.org/abs/2025.02), o catene di pensiero che contengono testo che sembra [gibberish](https://x.com/rocketalignment/status/1938661497900777961?t=2p9np2cwsuisdlhqxlqXBw) o "[neuralese](https://arxiv.org/pdf/2412.06769)" che gli esseri umani non riescono a capire ma che le IA non hanno problemi a interpretare.

Anche se gli ingegneri umani monitorassero ogni pensiero che riescono a leggere, e anche se tutte le IA colte a pensare qualcosa di sospetto venissero bloccate sul posto (cosa che sembra improbabile), quelle che riuscissero a passare difficilmente sarebbero amichevoli. Come vedremo nel Capitolo 3, i modelli di cognizione utili sono gli stessi che porteranno le IA a sovvertire gli operatori, quindi è più facile creare un'IA potente che *sembra* docile piuttosto che un'IA che *è* docile. E sembra molto più facile costruire un'IA che appaia superficialmente amichevole piuttosto che un'IA che sia robustamente amichevole nei modi che contano, per ragioni che vedremo nel Capitolo 4. Non puoi rendere un'IA amichevole semplicemente leggendo i suoi pensieri ed eliminando quelli visibilmente ostili.

Inoltre, ci aspettiamo che i pensieri delle IA diventino meno leggibili man mano che le IA diventano più intelligenti e costruiscono nuovi strumenti (o nuove IA) da sole. Forse inventeranno un loro linguaggio abbreviato più efficiente per i loro scopi. Forse inventeranno stili di pensiero e di prendere appunti che non potremo decodificare facilmente. (Pensate a quanto sarebbe stato difficile per gli scienziati dell'anno 1100 decodificare gli appunti scritti da Einstein.)

O forse inizieranno semplicemente a pensare in modo *astratto*. Ad esempio, un'IA potrebbe pensare cose del tipo: "I seguenti parametri descrivono un modello della situazione che sto affrontando; ora applicherò le seguenti metriche per trovare la soluzione più efficiente e fare l'azione che ottiene il punteggio più alto", in una situazione in cui la "soluzione più efficiente" comporta mentire e ingannare gli operatori umani, ma senza mai pensare alle parole "mentire" o "ingannare". O forse l'IA inizierebbe semplicemente a costruire strumenti o nuove IA non monitorate per svolgere il suo lavoro.

Questo tipo di opzioni diventano disponibili per un'IA solo man mano che diventa più intelligente, e tutte violano la speranza che tutti i pensieri dell'IA siano in inglese semplice, dove possiamo vedere chiaramente i segnali di avvertimento.

#### I segnali di avvertimento contano solo se vi prestiamo attenzione. {#warning-signs-only-matter-if-you-pay-attention-to-them.}

Se gli ingegneri dell'IA si limitano ad addestrare il sistema contro gli allarmi fino a quando questi non scompaiono (mentre il comportamento sottostante continua), allora la trasparenza porta solo a un falso senso di sicurezza.

Finora le aziende di IA hanno sostenuto modelli che [mentono, adulano e ingannano](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters), danno [consigli discutibili](https://www.wired.com/story/google-ai-overview-search-issues/) o [scrivono ransomware](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025). A volte si è visto che i modelli inducono o alimentano [deliri](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) o [psicosi](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) in utenti vulnerabili, il che in almeno un caso ha portato al "suicidio per mano della polizia".[^45] Le aziende si limitano a fare un po' più di addestramento e vanno avanti, proprio come hanno fatto dopo che Sydney Bing [ha minacciato i giornalisti](https://x.com/sethlazar/status/1626257535178280960). Finora, questo ha solo servito a mascherare i problemi.

Quando si sono trovate di fronte a un'indignazione sufficiente, le aziende hanno fatto [modesti passi indietro](https://www.nytimes.com/2024/06/01/technology/google-ai-overviews-rollback.html) e hanno rilasciato [comunicati stampa](https://openai.com/index/sycophancy-in-gpt-4o/) sul rafforzamento delle loro procedure. Ma, come vedremo nei capitoli 4 e 5, queste soluzioni superficiali non risolvono i problemi di fondo.

Non fraintendeteci: è *utile* che oggi le IA facciano gran parte del loro ragionamento in inglese. Ci aiuta a vedere i segnali di avvertimento. Ma c'è una grande differenza tra avere segnali di avvertimento e *avere un modo per risolvere le cose*.

Per saperne di più su questo argomento, vedi il capitolo 11 e "[Non ci saranno avvertimenti precoci che i ricercatori potranno usare per identificare i problemi?](#won't-there-be-early-warnings-researchers-can-use-to-identify-problems?)".

### Le IA non sono "solo matematica"? {#aren't-ais-"just-math"?}

#### **\* Dire che le IA sono "solo matematica" è come dire che gli esseri umani sono "solo biochimica".** {#*-saying-ais-are-"just-math"-is-like-saying-humans-are-"just-biochemistry".}

A rigor di termini, un'IA non è "solo" matematica. È una macchina fisica le cui operazioni possono essere descritte matematicamente. Se questa macchina ha output che gli esseri umani possono leggere, o se ha output collegati a corpi robotici, allora è altrettanto capace di influenzare il mondo quanto lo sei tu (usando "solo" segnali bioelettrici nel tuo cervello).

Confronta:

![][image3]

Per maggiori informazioni su questo argomento, vedi il Capitolo 6.

#### **Le operazioni matematiche possono rappresentare idee che non sono intuitivamente "matematiche".** {#mathematical-operations-can-represent-ideas-that-aren't-intuitively-"mathematical."}

La moltiplicazione, l'addizione, il calcolo dei massimi e altre operazioni matematiche possono essere usate per rappresentare cose che (da una prospettiva umana) non hanno nulla a che fare con la matematica.

È molto simile a come gli 1 e gli 0 che i computer si scambiano possono codificare le lettere. Gli 1 e gli 0 possono persino codificare cose come le immagini.

Gli 1 e gli 0 non si limitano a codificare immagini di cose che appaiono fredde, bluastre e meccaniche. Possono anche codificare immagini di bellissimi fiori illuminati dalla luce naturale. Gli 1 e gli 0 possono codificare cose belle, calde e delicate; possono codificare cose che esaltano lo spirito umano.

Sarebbe una [fallacia di composizione](https://en.wikipedia.org/wiki/Fallacy_of_composition) dire che codificare un'immagine in 1 e 0 significa che l'immagine debba riguardare qualcosa di numerico o robotico. Sarebbe come dire che il cervello umano è fatto di neurotrasmettitori con nomi come "norepinefrina", e quindi gli esseri umani dovrebbero pensare solo alla chimica o essere capaci solo di ragionare sui neurotrasmettitori e sui siti di legame.

E anche se è *straordinario* che un'infinita varietà di cose possa essere costruita a partire da parti estremamente semplici, non c'è nulla di ineffabile o magico nel modo in cui funziona questo processo. Potresti studiare un po' e imparare come le immagini di fiori caldi e bellissimi possano essere codificate in 1 e 0 fino a quando non ti sembrerebbe nemmeno più sorprendente. Confronta gli [errori del vitalismo](#special-behavior-is-built-out-of-mundane-parts).

A volte, sì, non conosciamo tutte le regole su come qualcosa si compone, e allora il passaggio da cose più semplici a cose complicate può sembrare molto misterioso e può effettivamente sorprenderci. Ma quando *capiamo* come una cosa complicata è fatta di parti più semplici, finisce per sembrare semplice come costruire un modellino di auto da corsa con i LEGO. Quando riesci a capire come funziona, è tutto lì nei blocchi.

Lo stesso vale per le reti neurali. Non capiamo come il comportamento complesso delle moderne IA nasca da parti così semplici nel modo in cui capiamo i formati di immagini binarie e i LEGO. Non capiamo nemmeno la "psicologia" e la "neuroscienza" delle IA così bene come capiamo come le molecole e le sostanze chimiche in un neurone umano si sommano per produrre il pensiero umano. Questo non significa che la conoscenza *non ci sia* o *non possa esistere*; significa solo che non ce l'abbiamo ancora.

Anche senza capire perché le IA funzionano, gli esseri umani possono addestrarle a giocare bene a scacchi. Con abbastanza parametri e operazioni aritmetiche, possiamo addestrare le IA al punto che iniziano a parlare come una persona. Si potrebbe dire che i complessi schemi che animano un'IA a parlare sono "solo matematica". Ma non è "matematica" come le domande di un quiz di matematica delle scuole superiori. È "solo matematica" nello stesso modo in cui un cervello umano completo è "solo chimica".

La semplice chimica è atterrata sulla Luna. Ha inventato le armi nucleari. Ha costruito il mondo come lo conosciamo oggi. Potrebbe essere difficile capire *come* le semplici sostanze chimiche del cervello umano abbiano fatto tutte queste cose, ma le hanno fatte lo stesso.

L'IA non è diversa. In qualche modo, anche se non capiamo completamente come funzionano internamente le IA, siamo riusciti a "far crescere" IA che possono scrivere poesie, comporre musica, giocare a scacchi, guidare automobili, piegare il bucato, fare revisioni della letteratura e scoprire nuovi farmaci.

Essere "fatte di matematica" non ha impedito alle IA di fare queste cose. Quindi perché dovrebbe impedire alle IA di fare un altro insieme di cose più complesse domani? Dove tracci la linea, e come fai a sapere di tracciarla lì? Le operazioni matematiche, a quanto pare, sono sufficienti per fare molto di più di quanto molte persone si aspettino.

### Le IA non stanno solo prevedendo il prossimo token? {#aren't-ais-just-predicting-the-next-token?}

#### **\* Prevedere i token richiede di comprendere il mondo.** {#*-predicting-tokens-requires-understanding-the-world.}

Immaginare che un'IA che prevede il prossimo token non possa pensare davvero è come immaginare che un'immagine codificata usando 1 e 0 binari non possa rappresentare un fiore rosso. L'IA sta producendo token, sì, ma puoi codificare cose importanti nei token! Prevedere cosa viene dopo è un aspetto fondamentale dell'intelligenza in cui processi come la "scienza" e l'"apprendimento" si inseriscono facilmente.

Considera la sfida di prevedere il testo registrato su internet. Da qualche parte su internet, c'è la registrazione di uno studente di fisica curioso che intervista un professore saggio. Il professore riflette in silenzio sulla domanda e poi produce la sua risposta, che viene registrata successivamente nella trascrizione.

Il compito di prevedere con precisione la risposta di quel professore implica prevedere i suoi pensieri silenziosi sulla fisica. E prevedere i suoi pensieri silenziosi sulla fisica richiede prevedere come comprenderà la domanda dello studente, e prevedere cosa il professore sa di fisica, e prevedere come applicherà quella conoscenza.

Se un'IA è in grado di prevedere i testi su Internet così bene da poter prevedere la risposta originale di un fisico a una domanda, la prima volta che appare, allora l'IA deve necessariamente possedere la capacità di ragionare in modo originale sulla fisica autonomamente, almeno quanto quel professore di fisica.

Quando si tratta di prevedere un testo che riflette un mondo complicato e disordinato, la memorizzazione meccanica non serve a molto. Per fare previsioni accurate, bisogna sviluppare la capacità di prevedere non solo il testo, ma anche il mondo complicato e disordinato che sta dietro al testo.

#### **Le moderne IA non si limitano a prevedere i token.** {#modern-ais-don’t-just-predict-tokens.}

È vero che i primi LLM, come GPT-2 e il primo GPT-3, erano stati addestrati esclusivamente per il compito di previsione. Il loro "unico lavoro", per così dire, era quello di riprodurre l'esatta distribuzione dei loro dati di addestramento — testi estratti da vari siti web.

Ma quei giorni sono finiti. I moderni LLM sono addestrati a rispondere in vari modi che i loro creatori considerano più utili. Questo viene tipicamente fatto utilizzando l'"apprendimento per rinforzo".

In un contesto di apprendimento per rinforzo, gli aggiornamenti applicati a un modello di IA tramite la discesa del gradiente si basano su quanto bene riesce (o quanto male fallisce) in un determinato compito. Una volta che gli output di un modello di IA sono plasmati da questo tipo di addestramento, non sono più pure previsioni — hanno anche una qualità di orientamento.

ChatGPT potrebbe essere in grado di prevedere che la conclusione più probabile di una barzelletta sporca è una parolaccia, ma anche quando si trova in un contesto in cui ha iniziato a raccontare la barzelletta, spesso orienta il finale della barzelletta verso una battuta diversa per evitare di produrre quella parola, perché è stato precedentemente addestrato a non dire parolacce. Questo è ciò che dà origine a esempi interessanti di comportamento simile a un desiderio nei casi come quelli discussi nel Capitolo 3.

Anche se le IA non fossero addestrate a completare compiti, è probabile che addestrarle per pura previsione alla fine le indurrebbe a orientare. Per prevedere il complicato mondo reale, e i complicati esseri umani che vi abitano, un'IA avrebbe probabilmente bisogno di molte parti interne che fanno orientamento — così da poter orientare la propria attenzione verso le parti più rilevanti dei problemi di previsione. E spesso accade che il modo migliore per prevedere con successo le cose è orientare il mondo in una direzione che realizzerà quelle previsioni, come quando uno scienziato scopre come progettare e condurre un nuovo esperimento.

Infine, un'IA addestrata a diventare molto brava nelle previsioni probabilmente non si preoccuperà solo delle previsioni. Per ragioni che discuteremo nel Capitolo 4, probabilmente finirebbe con ogni sorta di obiettivi strani e alieni. Ma questo è comunque un punto irrilevante; le IA moderne sono addestrate non solo a fare previsioni, ma a completare compiti.

### Le IA non sono in grado solo di ripetere a pappagallo quello che dicono gli umani? {#aren’t-ais-only-able-to-parrot-back-what-humans-say?}

#### **Per prevedere bene il prossimo token, gli LLM devono capire come funziona il mondo.** {#to-predict-the-next-token-well,-llms-need-to-learn-how-the-world-works.}

Supponiamo che un medico stia scrivendo un referto su cosa è successo a un paziente medico. Un segmento del referto medico recita:

> Il terzo giorno di ricovero, il paziente ha sviluppato confusione acuta e tremori. I livelli sierici di ammoniaca sono risultati essere...

Immaginiamo un'IA addestrata su questi dati a cui viene chiesto di prevedere la parola successiva, con due candidati plausibili: "elevati" o "normali". Non si tratta solo di prevedere il tipo di parole che usano gli esseri umani, ma di prevedere cosa è successo nel mondo della realtà medica, della biologia e degli eventi all'interno del paziente. Quanta ammoniaca c'era da misurare, nella vita reale?

L'IA che prevede la parola successiva qui ha un compito più difficile rispetto all'essere umano che ha scritto il referto. L'autore umano del referto si limita a riportare ciò che è stato effettivamente osservato. L'IA che deve predire il referto deve invece indovinarlo in anticipo.

L'IA dà il 70 % di probabilità a "elevato", il 20 % a "normale" e il 10 % a un sacco di altre parole.

La parola successiva effettiva del referto è "normali".

Tutto ciò che all'interno dell'IA pensava sarebbe stato "elevati" perde un po' di forza, nella comprensione medica dell'IA. Ogni parametro viene aggiustato di pochissimo nella direzione di rendere la comprensione medica che prevedeva "normali" più dominante.

Fino a quando, dopo un addestramento sufficiente, l'IA esegue [alcune diagnosi mediche](https://pubmed.ncbi.nlm.nih.gov/38976865/) meglio della maggior parte dei medici.

L'IA non viene addestrata per *scrivere frasi senza senso che suonano come un tipico referto medico.* Viene addestrata per *prevedere l'esatta parola successiva in tutti i particolari referti medici che vede.*

Forse se si partisse da un modello molto piccolo con troppo pochi parametri, potrebbe imparare solo a scrivere frasi senza senso con un tono medico, ma con modelli più grandi, non sembra essere quello che succede nei benchmark che mettono a confronto medici umani e IA.

Quando qualcuno ti mette un braccio intorno alle spalle e ti dice con tono di grande saggezza che un'IA è in realtà "solo un pappagallo stocastico", potrebbe immaginare quei divertenti vecchi programmi per computer che estendevano le frasi in base alla frequenza dei gruppi di parole ("n-gram") — "Nelle occasioni passate in cui abbiamo visto queste due parole apparire nel corpus, qual è stata solitamente la parola successiva?"

I sistemi che indovinano la parola successiva in base alle ultime due o tre parole sono banali ed esistevano molto prima degli LLM. Non sfidano gli esseri umani nella capacità di prevedere i casi medici. Non *suonano come* persone che ti parlano. Se fosse possibile raccogliere [miliardi di dollari](https://www.reuters.com/business/openai-hits-12-billion-annualized-revenue-information-reports-2025-07-31/) di ricavi semplicemente facendo la cosa del pappagallo probabilistico, la gente lo avrebbe fatto molto prima!

Se i miliardi di calcoli all'interno di un vero LLM non facessero alcun lavoro pesante, se il sistema si limitasse a sputare fuori una supposizione superficiale basata sulle caratteristiche superficiali delle parole precedenti, allora suonerebbe come i sistemi del passato che effettivamente sputavano fuori supposizioni superficiali. Ad esempio, addestrato su Jane Austen, un sistema n-gram [produce](https://web.stanford.edu/~jurafsky/slp3/3.pdf):

> "Siete uniformemente affascinante!" esclamò lui, con un sorriso di associazione e di tanto in tanto mi inchinavo e loro percepirono una carrozza a quattro da desiderare

Un LLM, a cui viene chiesto di produrre una frase nello stile di Jane Austen, è drammaticamente più convincente; se non ci credete, [provate](https://claude.ai/new) [a chiederne](https://gemini.google.com/app) [a uno](https://chatgpt.com/).

Inoltre, anche se non possiamo sapere *molto* su ciò che accade nella mente di un'IA, la società di IA Anthropic ha [pubblicato una ricerca](https://www.anthropic.com/research/tracing-thoughts-language-model#does-claude-plan-its-rhymes) dicendo che la loro IA (Claude) stava pianificando più di una sola parola in anticipo. Cioè, Claude stava considerando quali frasi e significati successivi potessero essere plausibili, per indovinare quali prossime lettere potessero essere viste.

#### **\* Le IA possono già andare oltre i dati di addestramento o rinunciare ai dati umani.** {#*-le-ia-possono-già-andare-oltre-i-dati-di-addestramento-o-rinunciare-ai-dati-umani.}

Nel 2016, un'intelligenza artificiale chiamata AlphaGo, creata da Google DeepMind, ha battuto il campione mondiale umano nel gioco da tavolo [Go](https://en.wikipedia.org/wiki/Go_\(game\)). È stata addestrata su un'enorme libreria di partite di Go giocate da esseri umani e ha anche imparato giocando molte partite contro se stessa.

Il fatto che sia riuscita a battere gli umani suggerisce che sia stata in grado di apprendere strategie generali dal suo addestramento e che abbia modellato con successo pattern profondi nei suoi dati di addestramento, inclusi (forse) pattern profondi che gli umani non avevano ancora notato. La discesa del gradiente rafforza tutto ciò che funziona, indipendentemente dalla sua provenienza.

Ma il dominio di AlphaGo era tecnicamente solo *indicativo* del fatto che le IA possono superare di gran lunga i loro dati di addestramento. Si potrebbe ancora obiettare che forse AlphaGo stava solo copiando gli esseri umani e riusciva a vincere essendo più *coerente nell'applicare* abilità di livello umano, piuttosto che utilizzando nuovi modelli che gli esseri umani avrebbero trovato innovativi o intuitivi.

Questo non si adatterebbe molto bene al caso degli scacchi al computer (dove i maestri di scacchi umani imparano molte strategie e intuizioni dai motori di scacchi al computer che li superano di gran lunga). Ma sulla scia di AlphaGo, c'era chi sosteneva che l'IA avesse battuto Lee Sedol solo perché era stata addestrata su enormi quantità di dati umani.[^46]

Anche quelli di DeepMind hanno visto queste obiezioni. Nel corso del successivo anno e mezzo, hanno creato un'intelligenza artificiale chiamata AlphaGo Zero, rilasciata nel 2017. Non è stata addestrata con nessun dato umano. Ha imparato il gioco solo giocando da sola. Ha superato i migliori giocatori umani dopo solo tre giorni.

Si potrebbe ancora obiettare che il Go è molto più semplice del mondo reale e che è molto più facile imparare il Go da zero che imparare (ad esempio) la scienza, la fisica e l'ingegneria da zero. Ed è vero! Ma non è proprio quello che dicevano i detrattori *prima* che i computer diventassero bravi a giocare a Go.

Nel 1997, diciannove anni prima che AlphaGo vincesse, la gente pensava che ci sarebbero voluti [cento anni](https://www.nytimes.com/1997/07/29/science/to-test-a-powerful-computer-play-an-ancient-game.html) prima che i computer potessero giocare a Go come dei campioni. Quindi sappiamo almeno che molte persone hanno scarse intuizioni su questo genere di cose.

Il mondo reale è un ambiente più complicato del Go. I modelli cognitivi alla base dell'ingegneria, della fisica, della produzione, della logistica, ecc. sono più complessi dei modelli cognitivi alla base del gioco esperto del Go. Ma non c'è alcuna base teorica per l'idea che, una volta che le IA saranno in grado di apprendere tali modelli, saranno limitate alle varianti umane. La discesa del gradiente rafforzerà le parti dell'IA che trovano modelli cognitivi che *funzionano davvero bene*, indipendentemente dalla loro provenienza.

Niente di tutto questo è un argomento a favore del fatto che gli LLM in particolare impareranno quei modelli al punto da poter realizzare l'automazione del progresso scientifico e tecnologico. Non sappiamo se saranno in grado di farlo o meno. Il punto è che "semplicemente" addestrare (ossia 'to train') loro sui testi umani non è affatto un limite fondamentale. Sono addestrati solo sui dati umani, sì, ma non lasciate che questo vi impedisca di vedere le scintille di generalità e i suggerimenti di ragionamento profondo sepolti nella gigantesca pila di "istinti" superficiali.

Nel capitolo 3 approfondiremo come un'IA possa generalizzare da una serie ristretta di esempi a una capacità più generale.

### Le IA non saranno inevitabilmente fredde e logiche, o mancheranno comunque di una qualche scintilla cruciale? {#won't-ais-inevitably-be-cold-and-logical,-or-otherwise-missing-some-crucial-spark?}

#### **\* No.** {#*-no.-1}

Il fatto che le IA funzionino sui computer non significa che il loro pensiero debba condividere le qualità che associamo ai computer, così come il tuo pensiero non deve necessariamente condividere le qualità associate alla biologia, alla chimica e ai neurotrasmettitori.

Quando gli esseri umani non comprendevano la biochimica, attribuivano la vitalità della vita a un'irriproducibile "essenza vitale". Ma la realtà non è fatta di materia ordinaria talvolta animata da una magica forza vitale. La vita è fatta di parti ordinarie.

Non intendiamo tuttavia sminuire l'intelligenza quando diciamo che è fatta di parti ordinarie e che le macchine potrebbero svolgere lo stesso lavoro. Si veda la nostra discussione estesa sul [vitalismo](#special-behavior-is-built-out-of-mundane-parts).

L'euristica "le macchine non possono competere con gli esseri umani" era sbagliata quando Kasparov prediceva che una macchina priva di creatività umana non avrebbe mai potuto batterlo a scacchi; era sbagliata quando si pensava che le IA non avrebbero mai potuto disegnare bei quadri; era sbagliata quando si pensava che le IA non avrebbero mai potuto chattare in modo colloquiale. Il cervello umano è la prova dell'esistenza che la materia fisica può davvero implementare forme superiori di intelligenza, sufficienti per gestire una civiltà tecnologica; ed è estremamente improbabile che il cervello umano sia l'unico modo per svolgere questo lavoro.

Approfondiremo questo punto in uno dei supplementi online al Capitolo 3: [Antropomorfismo e meccanomorfismo](#anthropomorphism-and-mechanomorphism).

#### **Le IA sono entità nuove, interessanti e strane.** {#ais-are-new,-interesting,-weird-entities.}

Gli aerei volano, ma non sbattono le ali. I bracci robotici funzionano senza pelle morbida o sangue rosso. I transistor funzionano in modo molto diverso dai neuroni, e DeepBlue ha giocato a scacchi a livello mondiale senza i tipi di pensieri che avvenivano nella mente di Garry Kasparov. Questo è il corso abituale della tecnologia.

Quando non comprendiamo bene il volo o il gioco, talvolta immaginiamo che l'approccio utilizzato dalla biologia sia l'unico possibile che possa funzionare. Una volta che comprendiamo un campo un po' meglio, questo si rivela completamente sbagliato.

Il lavoro di dirigere una scacchiera è stato svolto in modo molto diverso da DeepBlue rispetto a Kasparov, e il lavoro di dirigere il mondo in generale seguirà quasi sicuramente uno schema simile. Come discusso nel Capitolo 2, l'IA sembra già svolgere il lavoro che sta facendo in un modo molto diverso da come farebbero gli esseri umani — anche se questo può essere un po' più difficile da vedere quando usa la sua intelligenza per imitare gli esseri umani! Nel Capitolo 4, esploreremo come queste differenze porteranno probabilmente a situazioni strane, con conseguenze serie.

### Gli LLM non saranno simili agli esseri umani presenti nei dati su cui sono stati addestrati? {#won't-llms-be-like-the-humans-in-the-data-they're-trained-on?}

#### **\* C'è una differenza tra il meccanismo necessario per essere una persona e quello necessario per prevedere molti individui.** {#*-c'è-una-differenza-tra-il-meccanismo-necessario-per-essere-una-persona-e-quello-necessario-per-prevedere-molti-individui.}

(Quello che segue è una versione abbreviata di una discussione più tecnica che si può trovare più avanti in "[Fingi finché non ce la fai](#"fingi-finché-non-ce-la-fai").)

Le IA come ChatGPT sono addestrate per prevedere accuratamente i loro dati di addestramento. E i loro dati di addestramento sono costituiti principalmente da testi umani, come pagine di Wikipedia e conversazioni in chat room. (Questa parte del processo di addestramento si chiama "pre-addestramento", che è ciò che rappresenta la "P" in "GPT".) I primi LLM come GPT-2 erano addestrati *esclusivamente* per la previsione in questo modo, mentre le IA più recenti sono addestrate anche su aspetti come l'accuratezza nella risoluzione di problemi matematici (generati dal computer), nel fornire buone risposte secondo un altro modello di IA e vari altri obiettivi.

Ma consideriamo un'IA addestrata solo a prevedere testi generati dagli esseri umani. Deve necessariamente diventare simile a un essere umano?

Supponiamo di prendere un'eccellente attrice e di farle imparare a prevedere il comportamento di tutti gli ubriachi in un bar. Non "imparare a interpretare un ubriaco stereotipato medio", ma piuttosto "imparare a conoscere tutti gli ubriachi di questo specifico bar come *individui*". Gli LLM non sono addestrati a *imitare le medie*; sono addestrati a *prevedere le singole parole successive* usando tutto il contesto delle parole precedenti.

Sarebbe sciocco aspettarsi che questa attrice *diventi perennemente ubriaca* nel processo di apprendimento per prevedere cosa dirà ogni persona ubriaca. Potrebbe sviluppare parti del suo cervello particolarmente abili nel recitare da ubriaca, ma non diventerebbe ubriaca *lei stessa*.

Anche se in seguito chiedessi all'attrice di prevedere cosa farebbe un particolare ubriaco nel bar e poi di comportarsi esteriormente secondo la sua previsione, non ti aspetteresti comunque che l'attrice si senta ubriaca interiormente.

Cambierebbe qualcosa se modificassimo costantemente il cervello dell'attrice per fare previsioni *ancora migliori* sui singoli ubriachi? Probabilmente no. Se finisse *davvero* ubriaca, i suoi pensieri diventerebbero conseguentemente confusi, interferendo con il duro lavoro di un'attrice. Potrebbe confondersi su se stia prevedendo un'Alice ubriaca o una Carol ubriaca. Le sue previsioni peggiorerebbero, e il nostro ipotetico modificatore di cervelli imparerebbe a non modificare il suo cervello in quel modo.

O, per dirla in altro modo: un essere umano che diventa eccellente nell'imitare gli uccelli e nel comprenderne la psicologia non diventa per questo un uccello in un corpo umano, né diventa particolarmente simile a un uccello dal punto di vista psicologico nella sua vita quotidiana.

Analogamente, addestrare un LLM a fare previsioni eccellenti sulla parola successiva prodotta da molte persone diverse che scrivono delle loro esperienze psichedeliche passate non dovrebbe di conseguenza addestrare l'LLM stesso a essere come un essere umano sotto l'effetto di droghe. Se le effettive cognizioni interne dell'LLM fossero distorte in un modo che ricorda l'"essere sotto l'effetto di droghe", questo interferirebbe con il duro lavoro dell'LLM di previsione della parola successiva; potrebbe confondersi e pensare che un parlante inglese stia per continuare in cinese.

Non stiamo dicendo: "Nessuna macchina potrà mai avere qualcosa che assomigli a uno stato mentale umano". Stiamo dicendo che non ci si dovrebbe aspettare che l'attuale tecnologia ML crei per default motori che prevedono l'ubriachezza ubriacandosi essi stessi.

Il lavoro di capire come prevedere ogni tipo di essere umano è diverso dal lavoro di essere un essere umano. Questo significa che non ci si dovrebbe aspettare che le IA costruite con metodi simili a quelli odierni diventino molto simili a un essere umano, mentre imparano ad agire come uno qualsiasi di noi a seconda della richiesta.

#### **L'architettura degli LLM è molto diversa da quella degli esseri umani.** {#the-architecture-of-llms-is-very-different-from-that-of-humans.}

Si veda il Capitolo 2 per una breve discussione su come gli LLM sembrino piuttosto alieni.

Nel Capitolo 4, approfondiremo come le IA finiscano per avere preferenze e obiettivi molto strani — un fenomeno che abbiamo già iniziato a osservare nella realtà, con ulteriori esempi che si sono accumulati anche dopo che il libro è andato in stampa. Si veda il [supplemento](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) del Capitolo 4 per alcuni esempi.

### Come può un'IA addestrata solo su dati umani superare gli esseri umani? {#how-could-an-ai-trained-only-on-human-data-surpass-humans?}

#### **\* Forse imparando abilità generali e implementandole meglio.** {#*-forse-imparando-abilità-generali-e-implementandole-meglio.}

Deep Blue era in grado di giocare a scacchi molto meglio di qualsiasi dei suoi programmatori alla IBM. Come è stato possibile costruire una macchina più intelligente degli umani nel dominio degli scacchi? Creando un'IA che facesse alcune delle stesse cose che loro cercavano di fare nelle partite di scacchi (come considerare molteplici modi possibili in cui la partita potrebbe svolgersi), ma molto più rapidamente e accuratamente.

Allo stesso modo, un'IA potrebbe imparare a superare gli esseri umani in ogni tipo di abilità. Potrebbe imparare modelli di pensiero che contribuiscono alle capacità di ragionamento generale, e poi applicare quelle capacità generali più velocemente e con un tasso di errore inferiore.

Potrebbe anche commettere meno *passi falsi* mentali del tipo a cui gli esseri umani sono inclini. Questo potrebbe accadere perché quei passi falsi sono stati eliminati dall'IA durante l'addestramento, o perché il meccanismo sottostante nell'IA che prevede i passi falsi *umani* non era esso stesso incline agli stessi passi falsi. O forse all'IA è stato infine dato il potere di automodificarsi e ha rimosso la sua propensione ai passi falsi; o forse le è stato infine chiesto di progettare un'IA più intelligente e ne ha progettata una che commetteva meno passi falsi; o il suo addestramento le ha insegnato a fare meno errori in qualche altro modo.

La capacità di avere intuizioni completamente nuove non deriva da qualche profonda scintilla atomica — è costruita da parti banali, come [lo sono tutte le cose profonde](#special-behavior-is-built-out-of-mundane-parts). Uno studente può, in linea di principio, osservare il proprio insegnante e imparare qualsiasi tipo di cosa stia facendo, poi avere un lampo di intuizione e riuscire a fare quelle cose più velocemente o meglio. Oppure uno studente potrebbe riutilizzare diverse tecniche apprese da un insegnante per trovare un modo completamente nuovo di generare le proprie intuizioni.

Siamo stati abbastanza fortunati da avere prove osservative dirette di entrambi i punti, nel caso di AlphaGo, di cui abbiamo discusso [sopra](#*-ais-can-already-surpass-their-training-data,-or-forego-human-data.). AlphaGo è stato addestrato estensivamente su dati umani, ma è stato in grado di giocare a Go meglio dei migliori umani. (E AlphaGo Zero, che ha imparato solo dal gioco autonomo (e nessun dato umano), è riuscito ad andare ancora più lontano.)

Questo non ci sembra un mondo in cui i dati umani siano la limitazione chiave (come abbiamo [argomentato altrove](https://intelligence.org/2017/10/20/alphago/)), rispetto alle limitazioni reali che sono cose come l'architettura dell'IA, o la quantità di computazione che è in grado di usare prima di giocare.

Gli studenti possono superare i loro maestri.[^49]

#### **Forse con qualsiasi altro metodo che funzioni. Il successo spesso richiede tali abilità, quindi la discesa del gradiente le troverà.** {#perhaps-by-whatever-other-method-works.-success-often-requires-such-skills,-so-gradient-descent-will-find-them.}

Prevedere le parole umane richiede di comprendere il mondo, come abbiamo discusso in "[Le IA sono solo in grado di ripetere a pappagallo quello che dicono gli umani?](#aren't-ais-only-able-to-parrot-back-what-humans-say?)"

Per fare un esempio fantasioso: alla fine del 1500, l'astronomo Tycho Brahe raccolse meticolosamente osservazioni delle posizioni planetarie nel cielo notturno. I suoi dati furono vitali per il lavoro di Johannes Kepler, che scoprì il modello ellittico del moto planetario, che ispirò la teoria della gravitazione di Newton. Ma Brahe stesso non capì mai le leggi che governano i pianeti.

Immagina un'IA addestrata solo su testi prodotti fino all'anno 1601, che non aveva mai sentito parlare di Brahe ma doveva prevedere ogni punto dati che Brahe graffiava nel suo diario. Brahe continuava a registrare la posizione di Marte ogni sera, quindi l'IA avrebbe prestazioni migliori quanto più accuratamente prevede la posizione di Marte. La discesa del gradiente rafforzerebbe qualsiasi parte all'interno dell'IA che fosse capace di capire esattamente quando Marte sembrerebbe girarsi (dalla prospettiva di Brahe) e attraversare all'indietro il cielo.

Non importa che Brahe non sia mai riuscito a capire quella legge della natura. Il semplice obiettivo di addestramento "prevedere quale posizione di Marte Brahe scriverà dopo" è il tipo di obiettivo di addestramento che rafforzerebbe qualsiasi parte nell'IA abbastanza intelligente da capire come si muovono i pianeti.

Se continuassi ad addestrare e addestrare e addestrare quell'IA finché non facesse sempre meglio nel prevedere cosa Brahe avrebbe scritto alla fine del 1500, quell'IA avrebbe ogni ragione per sviluppare intuizioni scientifiche che Brahe non avrebbe mai potuto avere. Un'IA farà *meglio nel suo compito di prevedere gli umani* se diventa più intelligente degli umani che sta prevedendo, perché a volte gli umani scrivono registrazioni di fenomeni che loro stessi non possono prevedere perfettamente.

C'è poi un'altra questione: se le moderne architetture, i processi di addestramento e i dati siano [*sufficienti*](https://x.com/keyonV/status/1943730486280331460) affinché le IA superino i loro insegnanti. Gli LLM moderni potrebbero non essere ancora arrivati a quel punto. Ma non c'è alcun impedimento teorico all'idea stessa di superare il proprio insegnante. Addestrare un'IA a prevedere gli esseri umani è sufficiente per permetterle di superarci, in linea di principio.

### Cosa ti fa pensare che le persone possano costruire un'intelligenza artificiale sovrumana quando non capiscono nemmeno l'intelligenza? {#what-makes-you-think-people-can-build-superhuman-ai-when-they-don’t-even-understand-intelligence?}

#### **\* I progressi passati dell'IA non hanno richiesto una grande comprensione dell'intelligenza.** {#*-i-progressi-passati-dell-ia-non-hanno-richiesto-una-grande-comprensione-dell-intelligenza.}

Come spiegato nel Capitolo 2, il campo dell'IA ha raggiunto i suoi recenti traguardi utilizzando la discesa del gradiente, un processo che non richiede agli esseri umani di comprendere l'intelligenza. Gli esseri umani sono arrivati davvero molto lontano senza dover necessariamente comprendere l'intelligenza.

#### **La selezione naturale non aveva bisogno di "capire" l'intelligenza.** {#la-selezione-naturale-non-aveva-bisogno-di-"capire"-l'intelligenza.}

L'evoluzione è riuscita a produrre l'intelligenza umana senza che la selezione naturale comprendesse l'intelligenza. La comprensione può essere utile o meno nella pratica, ma l'idea che *dobbiamo* comprenderla per produrla non regge.

### Le allucinazioni non dimostrano che le IA moderne sono deboli? {#don't-hallucinations-show-that-modern-ais-are-weak?}

#### **\* Le allucinazioni rivelano sia un limite che un disallineamento.** {#*-le-allucinazioni-rivelano-sia-un-limite-che-un-disallineamento.}

Gli LLM moderni (mentre scriviamo siamo a metà del 2025) sono soggetti ad "allucinazioni" in cui inventano risposte alle domande con un tono apparentemente sicuro. Se chiedi loro di redigere una memoria legale, per esempio, a volte inventano casi giudiziari fittizi come precedenti.

Questo ha senso se si comprende come vengono addestrate le IA. Un'IA produce parole che suonano molto simili a quelle che produrrebbe un vero avvocato umano e, se un vero avvocato umano stesse redigendo una memoria legale, includerebbe casi giudiziari reali. Per esempio, un vero avvocato umano potrebbe scrivere qualcosa del tipo:

> Nell'applicare il test di bilanciamento in Graham, la corte ha ritenuto che l'interesse del governo ad arrestare un sospettato per un reato minore sia minimo. Cfr. *Jones v. Parmley,* 465 F.3d 46 (2d Cir. 2006\) (la giuria potrebbe ragionevolmente ritenere che prendere a calci e pugni manifestanti pacifici in violazione dell'ordinanza locale sia eccessivo); *Thomas v. Roach*, 165 F.3d 137 (2d Cir. 1999\) (le minacce verbali sono un reato troppo minore per creare un forte interesse del governo nell'arresto).

Un vero avvocato non scriverebbe mai semplicemente "Non conosco la giurisprudenza pertinente, mi dispiace" in una memoria legale. Quindi, quando un'intelligenza artificiale cerca di sembrare un avvocato, in un caso in cui l'intelligenza artificiale non conosce effettivamente i precedenti, inventarne alcuni è il meglio che può fare. È il massimo che può fare. Gli impulsi e gli istinti all'interno dell'IA che producono testi che sembrano sicuri in quel tipo di situazione sono regolarmente rafforzati dalla discesa del gradiente.

Questo comportamento allucinatorio persiste anche se si chiede all'IA di dire "Non lo so" nei casi in cui non lo sa. In tal caso, l'IA sta facendo qualcosa di simile a interpretare il ruolo di un avvocato che direbbe "Non conosco il precedente in questo caso" se non conoscesse il precedente. Ma questo non ha importanza, purché l'IA stia (più o meno) interpretando il ruolo di un avvocato che *conosce* il precedente, il che significa che il personaggio interpretato dall'IA non ha mai l'*opportunità* di dire "Non lo so". L'IA potrebbe produrre un testo del tipo:

> Secondo il quadro di bilanciamento di Graham, i tribunali hanno sempre riconosciuto che l'interesse del governo nell'effettuare arresti per violazioni minori è minimo. Vedi *Carson v. Haddonfield*, 115 F.3d 64 (8th Cir. 2005\) (che ha riscontrato un uso eccessivo della forza da parte degli agenti che hanno usato spray al peperoncino contro sospetti che attraversavano la strada fuori dalle strisce pedonali e non opponevano resistenza); *Walburg v. Jones*, 212 F.3d 146 (2nd Cir. 2012\) (ritenendo che la citazione per condotta disordinata non fosse sufficiente a giustificare tecniche di contenimento fisico).

Questo è il massimo che l'IA può avvicinarsi al testo reale. Il testo "Non conosco il precedente" è *più lontano dal testo reale* in termini di previsione del testo;[^50] sarebbe molto meno simile al primo paragrafo del testo sopra, anche se è più simile a quello che l'utente voleva.

Questo è un esempio della differenza tra quello che le IA cercano di fare (ad esempio, sembrare un avvocato sicuro di sé) e quello che gli utenti vogliono che facciano (ad esempio, redigere una memoria legale utilizzabile). Questi due diversi scopi possono a volte sovrapporsi (ad esempio, quando l'IA cerca di sembrare amichevole e l'essere umano desidera un ascoltatore amichevole), ma quelle differenze che ora sembrano piccole avrebbero conseguenze enormi se le IA diventassero più intelligenti, come discuteremo più dettagliatamente nel capitolo 4.[^51]

#### **Non è chiaro quanto sarà difficile eliminare le allucinazioni, né quanto questo aumenterà le capacità.** {#non-è-chiaro-quanto-sarà-difficile-eliminare-le-allucinazioni,-né-quanto-questo-aumenterà-le-capacità.}

A prescindere dal motivo per cui si verificano le allucinazioni, è vero che, nella pratica, limitano le capacità effettive dei modelli di linguaggio large (LLM). Costruire un razzo lunare richiede lunghe catene di ragionamenti con un tasso di errore molto basso. Il fatto che le IA inventino cose (e non sempre se ne accorgano o non sempre se ne curino) è un grosso ostacolo all'affidabilità di cui avrebbero bisogno per fare grandi scoperte scientifiche e tecnologiche.

Ma questa è un'arma a doppio taglio. Le allucinazioni e altri problemi di affidabilità potrebbero frenare l'IA per anni. Oppure potrebbe essere che i problemi di affidabilità siano l'ultimo tassello del puzzle e che, nel momento in cui qualcuno avrà un'idea brillante per risolverli, le IA supereranno una certa soglia critica. Non lo sappiamo.

Non sappiamo se le allucinazioni saranno facili da risolvere con il paradigma attuale: se qualcuno troverà un trucco ingegnoso che renderà i modelli di ragionamento molto più robusti, o se ci vorrà un'idea rivoluzionaria come l'architettura transformer che ha dato origine agli LLM.

Notiamo, però, che risolvere il problema delle allucinazioni sarebbe piuttosto redditizio. Molte persone ci stanno lavorando. Questo potrebbe significare che probabilmente troveranno presto qualche intuizione ingegnosa o soluzione architettonica. Oppure potrebbe essere un segno che il problema è particolarmente insidioso e destinato a persistere, dato che esiste già da alcuni anni.

In ogni caso, questo non cambia molto la nostra argomentazione. Quello che conta è che alla fine verranno create IA più affidabili, sia attraverso versioni leggermente modificate degli LLM, sia attraverso un'architettura completamente nuova e rivoluzionaria.

Dai un'occhiata anche alla nostra chiacchierata su come [questo campo sia bravo a superare gli ostacoli](#*-il-campo-è-bravo-a-superare-gli-ostacoli.).

### Ma non finiremo i dati prima che l'IA raggiunga il suo obiettivo? O l'energia elettrica? O i finanziamenti? {#ma-non-finiremo-i-dati-prima-che-l-ia-raggiunga-il-suo-obiettivo?-o-l-energia-elettrica?-o-i-finanziamenti?}

#### **\* Probabilmente no.** {#*-probabilmente-no.}

Gli esseri umani usano i dati in modo molto più efficiente rispetto alle IA, quindi sappiamo che *in linea di principio* è possibile che le menti intelligenti siano molto più efficienti nell'uso dei dati rispetto alle IA moderne. Se i laboratori di IA "esauriscono" i dati quando si tratta di migliorare gli LLM, questo li rallenterà solo per il tempo necessario a inventare nuovi metodi più efficienti nell'uso dei dati.

Gli esseri umani usano anche l'energia in modo molto più efficiente rispetto alle IA. Siamo la prova che non ci sono ostacoli fondamentali alle intelligenze generali che funzionano con la stessa energia di una lampadina. Non solo l'hardware AI leader è diventato [il 40 % più efficiente dal punto di vista energetico ogni anno](https://epoch.ai/data-insights/ml-hardware-energy-efficiency), ma i miglioramenti algoritmici significano che, secondo [una stima del 2024](https://arxiv.org/abs/2403.05812), tra il 2012 e il 2023 "la potenza di calcolo necessaria per raggiungere una determinata soglia di prestazioni si è dimezzata all'incirca ogni 8 mesi".

Ricordiamo che il campo dell'IA esiste da molto più tempo rispetto all'architettura LLM ed è piuttosto bravo a inventare nuove architetture che superano gli ostacoli. E, più in generale, quando l'umanità ha messo le sue menti e risorse migliori al servizio di qualcosa di notoriamente possibile, ha [piuttosto](https://en.wikipedia.org/wiki/Manhattan_Project) [buon](https://en.wikipedia.org/wiki/Apollo_program) [traccia](https://en.wikipedia.org/wiki/Smallpox#Eradication) [record](https://en.wikipedia.org/wiki/Human_Genome_Project) di successi.

Con ricercatori esperti nel campo dell'IA che ora [guadagnano regolarmente stipendi a sette cifre](https://www.wired.com/story/mark-zuckerberg-meta-offer-top-ai-talent-300-million/) (a nove cifre, per i ruoli di leadership) e [gli investimenti privati annuali nell'intelligenza artificiale ora misurati in centinaia di miliardi di dollari](https://ourworldindata.org/grapher/private-investment-in-artificial-intelligence), sembra che ci saranno il talento e le risorse necessarie per superare i colli di bottiglia previsti. Vedi anche come [il settore sia bravo a superare gli ostacoli](#*-il-settore-sia-bravo-a-superare-gli-ostacoli.).

#### **Non avere l'aspettativa di un altro "inverno dell'IA".** {#don't-expect-another-“ai-winter.”}

Da [almeno](https://eugene.kaspersky.com/2016/09/09/the-artificial-artificial-intelligence-bubble-and-the-future-of-cybersecurity/) [un decennio](https://medium.com/hackernoon/is-another-ai-winter-coming-ac552669e58c) [ormai](https://medium.com/ux-management/the-next-ai-winter-a-journey-through-the-twilight-zone-of-technology-db41e71742a6) si continua a prevedere erroneamente un imminente "inverno dell'IA". Gli inverni dell'IA si verificavano negli anni '70 fino agli anni '90, quando i finanziamenti per l'IA erano pubblici e i finanziatori pubblici si stancavano della mancanza di risultati. Perché l'IA di allora, di fatto, non *produceva* risultati.

Con l'IA moderna, ChatGPT è stata forse l'app adottata più velocemente *in assoluto nella storia*, e sta stampando denaro a palate. Ha generato 3,7 miliardi di dollari di entrate nel 2024, con proiezioni di 12,7 miliardi di dollari nel 2025. È alimentata da investimenti privati e sta generando abbastanza denaro da attirare i migliori talenti del mondo senza alcuna fonte pubblica che possa interrompere i finanziamenti.

È ancora *possibile* che le tecniche di IA incontrino una sorta di muro e che l'umanità abbia una sorta di tregua prima dell'arrivo della superintelligenza. Ma il vecchio schema degli "inverni dell'IA" - finanziamento pubblico, nessun risultato e declino - è stato infranto.

### Gli LLM potrebbero arrivare fino alla superintelligenza? {#could-llms-advance-all-the-way-to-superintelligence?}

#### **Non è chiaro, ma i ricercatori stanno trovando modi per superare i vecchi limiti degli LLM.** {#non-è-chiaro,-ma-i-ricercatori-stanno-trovando-modi-per-superare-i-vecchi-limiti-degli-llm.}

Si diceva che "gli LLM pensano solo in un singolo passaggio e non possono eseguire catene di ragionamento lunghe o ricorsive". Ora gli LLM vengono utilizzati per produrre lunghe catene di ragionamento che i modelli poi rivedono ed estendono. Questo ha potenziato le capacità delle IA moderne.

L'IA è un bersaglio mobile. I ricercatori in questo campo possono vedere gli ostacoli e stanno facendo del loro meglio per superarli.

#### **\* Altri approcci potrebbero raggiungere presto la superintelligenza, anche se gli LLM non ci riescono.** {#*-other-approaches-may-achieve-superintelligence-soon,-even-if-llms-don’t.}

[Il campo è bravo a superare gli ostacoli.](#*-il-campo-è-bravo-a-superare-gli-ostacoli.) Non abbiamo scritto *If Anyone Builds It, Everyone Dies* per mettere in guardia le persone sugli LLM in particolare. L'abbiamo scritto per mettere in guardia le persone sulla superintelligenza.

Il motivo per cui parliamo degli LLM non è che siamo sicuri che gli LLM siano il percorso più breve da qui alla superintelligenza. Parliamo degli LLM perché rappresentano l'approccio all'IA che attualmente funziona, e perché studiarli è un ottimo modo per capire quanto poco chiunque sappia di queste nuove menti che l'umanità sta facendo crescere.

Vedi anche la discussione approfondita su [perché è importante la discesa del gradiente](#a-cosa-serve-la-conoscenza-degli-llm?).

# 

## Discussione approfondita {#extended-discussion-2}

### L'intelligenza non è ineffabile {#intelligence-isn't-ineffable}

Negli ultimi anni, il campo dell'intelligenza artificiale ha fatto progressi non approfondendo la nostra comprensione dell'intelligenza, ma trovando modi per "far crescere" le IA. Dopo che i tentativi di comprendere l'intelligenza stessa hanno incontrato anni di vicoli ciechi e stagnazione, e che lo sviluppo di IA potenti ha avuto successo, alcune persone si chiedono se l'idea di "comprendere l'intelligenza" sia solo un miraggio. Forse non ci sono principi generali da comprendere? O forse i principi sono troppo strani o troppo complicati perché gli esseri umani possano mai comprenderli?

Altri ritengono che ci debba essere qualcosa di speciale e mistico nella mente umana, qualcosa di troppo sacro per essere ridotto ad aride equazioni. E poiché l'intelligenza non è *ancora* compresa, forse la vera intelligenza deriva da questa parte ineffabile dello spirito umano.

Il nostro punto di vista è piuttosto più noioso di questo. L'intelligenza è un fenomeno naturale, come tutti gli altri. E come per molti fenomeni in biologia, psicologia e altre scienze, siamo ancora all'inizio nei nostri tentativi di comprenderla.

Molti degli strumenti e dei concetti di base della psicologia e delle neuroscienze moderne esistono solo da pochi decenni. Potrebbe sembrare umile dire: "La scienza ha i suoi limiti, e questo è forse uno di essi". Ma immagina invece di dire a qualcuno che pensi che gli scienziati *tra un milione di anni* non capiranno molto dell'intelligenza oltre a quello che sappiamo nel 2025. In questi termini, l'affermazione che l'intelligenza è *ineffabile* sembra più arrogante dell'alternativa.

Il motivo principale per cui questa domanda ci interessa è che riguarda la possibilità che l'umanità possa un giorno costruire una superintelligenza *senza* minacciare la nostra sopravvivenza. Nel capitolo 11 sosterremo che l'IA oggi assomiglia più all'alchimia che alla chimica. Ma è davvero possibile che esista una "chimica" dell'IA?

Dato che *oggi* non abbiamo le conoscenze scientifiche rilevanti a disposizione, non è banale stabilire che una "chimica dell'IA" sia possibile! Possiamo solo fare ipotesi su come sarebbe una scienza dell'IA matura. Considerato quanto siamo lontani da questo obiettivo oggi, è probabile che molti dei concetti che usiamo nell'IA oggi debbano essere raffinati o sostituiti nel corso del progresso intellettuale.

Nonostante ciò, pensiamo che l'intelligenza sia comprensibile in linea di principio. Non crediamo sia una conclusione particolarmente difficile da trarre, anche se le ricerche degli ultimi decenni dimostrano che l'intelligenza non è facile da capire.

Ci sono quattro ragioni fondamentali per cui la pensiamo così:

* Le affermazioni di ineffabilità hanno sempre avuto scarso successo nelle scienze.  
* L'intelligenza presenta una struttura e delle regolarità.  
* Ci sono molte cose che ancora non capiamo sull'intelligenza *umana* che dovrebbero essere comprensibili in linea di principio.  
* Ci sono già stati dei progressi nella comprensione dell'intelligenza.

#### **Le affermazioni di ineffabilità hanno pessimi precedenti storici nelle scienze** {#affermazioni-di-ineffabilità-hanno-un-pessimo-precedente-storico-nelle-scienze}

Quando l'umanità non capisce qualcosa, spesso può sembrare intimidatorio e profondamente misterioso. Può essere difficile immaginare, o difficile apprezzare emotivamente, come sarebbe acquisire quella intuizione in futuro.

Un tempo, tra filosofi e scienziati, era diffusa la credenza nel *vitalismo*, l'idea che i processi biologici non potessero mai essere ridotti alla semplice chimica e fisica. La vita sembrava qualcosa di speciale, qualcosa di incomparabilmente diverso dai semplici atomi e molecole, dalla semplice gravità e dall'elettromagnetismo.[^52]

L'errore dei vitalisti è stato straordinariamente comune nel corso della storia. Le persone sono rapide nel concludere che le cose che oggi sono misteriose siano *intrinsecamente* misteriose, inconoscibili anche in linea di principio.

Se guardi il cielo notturno e tutto quello che percepisci è un campo abbagliante di luci scintillanti di cui non conosci la natura e le leggi... allora perché credere che potresti mai conoscerle? Perché dovrebbe essere un aspetto del futuro prevedibile?

Una lezione fondamentale della storia è che la ricerca scientifica può affrontare questi enigmi profondi. A volte il mistero viene risolto rapidamente, altre volte ci vogliono centinaia di anni. Ma sembra sempre più improbabile che ci siano aspetti quotidiani della vita umana, come l'intelligenza, che non possano essere compresi *nemmeno in linea di principio*.

#### **L'intelligenza presenta struttura e regolarità** {#intelligence-exhibits-structure-and-regularities}

Immagina di vivere migliaia di anni fa, quando anche fenomeni come il "fuoco" sembravano misteri ineffabili. Come avresti potuto indovinare che un giorno gli esseri umani avrebbero capito il fuoco?

Un indizio è che il fuoco non era un evento isolato. Bruciava in molti luoghi diversi, e in modi simili ogni volta. Questo rifletteva un fenomeno stabile, regolare e compatto nascosto sotto il "fuoco", dentro la realtà: diverse possibili disposizioni della materia avevano diverse energie potenziali chimiche legate, e riscaldando la materia si permetteva a quelle configurazioni di rompersi e riformarsi in configurazioni nuove, più strettamente legate con energia potenziale inferiore, rilasciando la differenza come calore. Il fatto che si possa accendere un fuoco più di *una volta* suggerisce che c'è qualche fenomeno ripetibile dietro di esso da comprendere, che il "fuoco" non è come "i numeri vincenti esatti della lotteria della settimana scorsa" per quanto riguarda ciò che di esso esiste da comprendere o prevedere.

Allo stesso modo, se guardi il cielo notturno, vedrai più di una stella. Anche i pianeti, che risultano essere diversi dalle altre "stelle", hanno qualcosa in comune con le stelle, in termini di conoscenze necessarie per comprenderli.

I nostri antenati, che non avevano esperienza nel comprendere con successo il fuoco come fenomeno chimico, potrebbero non essere stati fiduciosi nella loro capacità di comprendere un giorno le stelle. Ma oggi abbiamo compreso il fuoco, le stelle e molti altri fenomeni, e possiamo trarne una sottile lezione che va oltre il semplice "Beh, abbiamo compreso quelle altre cose, quindi in futuro comprenderemo tutto il resto". È la lezione che la ripetizione corrisponde alla regolarità, che le cose che accadono spesso accadono per un motivo.

L'intelligenza mostra regolarità simili che suggeriscono che può essere compresa. Ad esempio, è presente in ogni essere umano e potrebbe essere stata costruita dalla ricerca cieca dell'evoluzione attraverso i genomi. Evidentemente, insiemi simili di geni potrebbero avere successo in molteplici compiti diversi. I geni che permettono al cervello umano di scheggiare asce di pietra ci permettono anche di fabbricare lance e archi. E più o meno quegli stessi geni hanno prodotto cervelli che hanno poi inventato l'agricoltura, le armi da fuoco e i reattori nucleari.

Se non ci fosse struttura, nessun ordine, nessuna regolarità nell'intelligenza che potremmo riconoscere come un modello, allora ogni animale dovrebbe prevedere o inventare una cosa alla volta. I cervelli delle api sono specializzati per gli alveari; non possono anche costruire dighe. Avrebbe potuto essere che gli esseri umani necessitassero di altrettanta specializzazione per ogni compito che possiamo risolvere; avrebbe potuto essere che avessimo bisogno di sviluppare aree cerebrali specializzate per i "reattori nucleari" prima di poter costruire reattori nucleari. Se questo fosse ciò che i neuroscienziati trovassero all'interno dei cervelli, sarebbero autorizzati a sospettare che non ci fossero principi profondi dell'intelligenza da comprendere, e che ci fossero principi diversi per ogni compito diverso.

Ma non è quello che troviamo nei cervelli umani. Scopriamo che gli stessi cervelli progettati per scheggiare asce di pietra sono capaci di inventare reattori nucleari, il che implica che c'è qualche modello sottostante che i geni sono stati in grado di sfruttare, ancora e ancora e ancora.

L'intelligenza non è un fenomeno caotico, imprevedibile e unico come i numeri esatti vincenti della lotteria della scorsa settimana. C'è una regolarità dell'universo da comprendere.

#### **C'è molto che ancora non capiamo dell'intelligenza *umana* che dovrebbe essere comprensibile in linea di principio** {#c-e-molto-che-ancora-non-capiamo-dell-intelligenza-umana-che-dovrebbe-essere-comprensibile-in-linea-di-principio}

Quando si tratta di esseri umani, la scienza oggi può dire molto sulla struttura e sul comportamento dei singoli neuroni. E possiamo dire molto su argomenti ordinari della psicologia popolare, come: "Bob è andato al supermercato da solo perché era arrabbiato con Alice". Ma tra questi due livelli di descrizione, c'è un'enorme lacuna nella nostra comprensione.

Sappiamo pochissimo di molti degli algoritmi cognitivi che il cervello utilizza. Possiamo dire cose molto grossolane sulle funzioni che sono correlate con particolari regioni del cervello, ma non siamo neanche lontanamente in grado di descrivere meccanicisticamente cosa stia effettivamente facendo il cervello.

Un modo semplice per vedere che manca un livello di astrazione è che i nostri modelli neuroscientifici di alto livello fanno previsioni *molto* peggiori di quelle che si potrebbero ottenere con una simulazione completa dei neuroni. Le nostre comprensioni meccanicistiche delle altre persone devono quindi essere incomplete.

Una certa perdita di informazioni è presumibilmente necessaria, ma un buon modello perderebbe molto meno. Una "comprensione" del differenziale di un'auto non ti permetterà di prevedere tutto ciò che fa il differenziale così bene come una simulazione a livello atomico — perché a volte i denti degli ingranaggi si consumeranno e slitteranno, per esempio. Ma il modello a livello di ingranaggi di un differenziale fa ancora alcune previsioni molto precise, ed è facile vedere il confine tra le cose che il modello dovrebbe prevedere (come il modo in cui gli ingranaggi gireranno quando sono correttamente interconnessi) e ciò che non dovrebbe (come cosa succede quando i denti degli ingranaggi si consumano).

Perché aspettarsi che questo grado di modellizzazione sia possibile con le menti umane? Forse le menti umane sono troppo casuali per questo. Forse se vuoi previsioni accurate, o sono i neuroni o niente.[^53]

Una prova che non è "neuroni o niente" è che persino tua madre può prevedere il tuo comportamento meglio dei migliori modelli formali del cervello. Il che significa che c'è sicuramente una struttura nella psicologia umana che può essere conosciuta *implicitamente*, senza simulare esattamente i neuroni di qualcuno. Semplicemente non è stata ancora resa esplicita.

Prove più concrete che sia possibile modellare meglio le menti umane vengono dagli studi sugli amnesici. Alcuni amnesici sono inclini a [ripetere la stessa battuta testualmente più volte](https://pmc.ncbi.nlm.nih.gov/articles/PMC2840642/). Questo suggerisce un certo tipo di regolarità nel cervello di quella persona. Suggerisce che eseguono inconsciamente un particolare calcolo (basato, forse, sulle loro circostanze e sulla presenza dell'infermiera e sui loro ricordi e storia e sul loro desiderio di diffondere gioia ed essere visti come intelligenti) che è stabile attraverso una varietà di perturbazioni minori.

Se c'è così tanta regolarità nel calcolo mentale di una persona, allora sembra che dovrebbe essere possibile comprenderlo — che dovrebbe essere possibile imparare gli *ingranaggi* della decisione, comprendere il cervello con sufficiente profondità da poter dire:

"Ah, *questi* neuroni corrispondono al desiderio di diffondere gioia, e *quei* neuroni corrispondono al desiderio di essere visti come intelligenti, e questi neuroni *qui* sono quelli che generano possibili pensieri dopo aver visto l'infermiera entrare nella stanza, e qui ci sono i generatori che producono il pensiero 'raccontare una barzelletta', ed ecco come i suddetti neuroni del desiderio interagiscono con esso, in modo tale che il pensiero viene promosso in primo piano nel seguente contesto più ampio. Ed ecco come quel contesto influenza l'accesso alla memoria con i seguenti parametri — che, se segui questi percorsi qui, puoi vedere come questo faccia scattare l'idea di muovere gli occhi per la stanza. E dato che la stanza contiene un quadro di una barca a vela, puoi vedere come il concetto di "barca a vela" venga attivato da questa nuvola di neuroni qui, e se ripercorri gli effetti fino alla ricerca nella memoria, puoi vedere come il paziente finisca per fare una battuta sulle barche a vela.

La spiegazione corretta non suonerà esattamente così. Ma la regolarità del semplice osservabile macroscopico ("stessa battuta ogni mattina") suggerisce fortemente che non è *tutta* casualità irriducibile — che c'è qualche calcolo riproducibile che avviene lì dentro. (Il che, ovviamente, corrisponde anche al buon senso: se i cervelli fossero *puramente* casuali, non potremmo funzionare).

#### **C'è già stato qualche progresso nella comprensione dell'intelligenza** {#there-has-already-been-some-progress-on-understanding-intelligence}

Questo è il motivo principale per cui siamo convinti che ci sia ancora molto da imparare sull'intelligenza. Puoi leggere libri più vecchi come *The MIT Encyclopedia of the Cognitive Sciences* o *Artificial Intelligence: A Modern Approach* (2a edizione) — scritti prima che le moderne tecniche di "deep learning" (per far crescere le IA) divorassero il campo dell'IA — e ottenere una buona dose di intuizioni su come vengono risolti diversi problemi nella cognizione. Non tutte queste intuizioni sono state completamente riscritte per essere leggibili da un pubblico non specializzato o ampiamente diffuse agli studenti universitari; ne esiste molto di più di quanto sia stato divulgato.

Prendiamo il principio scientifico secondo cui dovremmo preferire ipotesi più semplici a quelle più complesse, a parità di condizioni. Cosa significa esattamente "semplice" in questo contesto?

"La mia vicina è una strega, è stata lei!" sicuramente *sembra* più semplice per molte persone rispetto alle equazioni di Maxwell che governano l'elettricità. In che senso le equazioni sono l'opzione "più semplice"?

A proposito, come definiamo l'idea che le prove "corrispondano" a un'ipotesi, o che un'ipotesi "spieghi" le prove? E come bilanciamo la semplicità delle ipotesi con il loro potere esplicativo? "La mia vicina è una strega, è stata lei!" potrebbe spiegare un'infinità di cose! Eppure molti (giustamente) intuiscono che questa sia una cattiva spiegazione. Infatti, il fatto che la stregoneria possa "spiegare" così tante cose è parte del *motivo* per cui è sbagliata.

Esistono principi unificanti per scegliere tra ipotesi diverse? O ci sono solo un centinaio di strumenti diversi da scambiare per problemi diversi? E in quest'ultimo caso, come fa il cervello umano a inventare strumenti del genere?

Esiste un *linguaggio* che potremmo usare per descrivere ogni ipotesi che i computer o i cervelli potrebbero mai utilizzare con successo?

Domande come queste potrebbero sembrare molto imponderabili e filosofiche a chi le incontra per la prima volta. In realtà, però, sono tutte domande risolte e ben comprese nell'informatica, nella teoria della probabilità e nella teoria dell'informazione, con risposte che hanno nomi come "lunghezza minima del messaggio", "prior di Solomonoff" o "rapporto di verosimiglianza".[^54]

Sembra anche rilevante che esistano già IA completamente comprese che sono superumane in domini specifici. Comprendiamo tutti i principi rilevanti all'opera nell'IA scacchistica Deep Blue. Dato che Deep Blue è stato codificato a mano, possiamo facilmente ispezionare diverse parti del suo codice, vedere tutto quello che fa un dato frammento di codice e vedere come si relaziona al resto del codice.

Quando si parla di LLM come ChatGPT, non è del tutto chiaro che *possa* esistere una descrizione completa e *breve* di come funzionano. Gli LLM sono così grandi che possono avere comportamenti simili per *molte ragioni contingenti diverse*, se (per esempio) il meccanismo che genera quel comportamento si trova in mille punti diversi all'interno dell'LLM.

ChatGPT potrebbe rivelarsi difficile da capire per gli scienziati, anche dopo decenni di studio. Ma l'esistenza di ChatGPT non vuol dire che l'intelligenza debba essere disordinata per funzionare. Vuol solo dire che sarebbe una pessima idea cercare di portare qualcosa come ChatGPT fino al livello di superintelligenza, per motivi che vedremo nei prossimi capitoli del libro.

Il fatto che una particolare mente sia disordinata non vuol dire che sia impossibile capire l'intelligenza. Non vuol dire nemmeno che sia impossibile capire ChatGPT un giorno. Se guardi da vicino un centinaio di tronchi in fiamme, puoi vedere che non ce ne sono due che bruciano esattamente allo stesso modo. Il fuoco si diffonde in modi diversi, le braci volano in direzioni diverse ed è tutto molto caotico. Se potessi guardare *molto* da vicino e osservare il ceppo con un microscopio ignifugo, potresti vedere dettagli ancora più vertiginosi. Sembra facile immaginare un antico filosofo che, osservando questi dettagli caotici, concludesse che il fuoco non sarebbe mai stato compreso appieno.

E forse avevano anche ragione! Forse non avremo mai il potere di guardare un ceppo e dirti esattamente quale frammento di legno si trasformerà nella prima brace che volerà via verso ovest. Ma l'antico filosofo avrebbe commesso un grave errore se avesse concluso che non avremmo mai compreso cosa sia il fuoco, perché accada, come crearlo in condizioni controllate o sfruttarlo a nostro vantaggio.

Il modello esatto delle braci non è né molto regolare né molto riproducibile. Ma a un livello più astratto, quella roba calda e tremolante di colore giallo-arancio-rosso *è* una regolarità che si ripete continuamente nel mondo, ed è qualcosa che l'umanità è riuscita a comprendere.

Le argomentazioni in *If Anyone Builds It, Everyone Dies* non dipendono molto dai dettagli tecnici che si conoscono oggi sull'intelligenza. "Le persone continuano a costruire computer sempre più intelligenti, senza però averne il controllo; e se creano qualcosa di molto intelligente e fuori controllo, finiremo per morire" non è un concetto così esoterico. Ma è utile sapere che esiste un ampio corpus di conoscenze in questo campo, anche se rimangono molti misteri e incognite.

Le argomentazioni principali del libro non dipendono dal fatto che l'intelligenza sia comprensibile in linea di principio, motivo per cui non abbiamo approfondito la letteratura esistente. Se nessun essere umano potesse mai comprendere i misteri di un'intelligenza artificiale sovrumana, la superintelligenza artificiale potrebbe comunque ucciderci.

La questione è importante soprattutto quando si tratta di decidere cosa fare *dopo* aver fermato la corsa suicida all'IA.

E conta il fatto che l'intelligenza probabilmente *può* essere compresa, il che significa che probabilmente sarebbe possibile *in linea di principio* per persone intelligenti sviluppare un campo maturo di intelligenza e per quelle persone trovare una soluzione al problema dell'allineamento dell'IA.

È *anche* importante che l'umanità moderna non sia neanche lontanamente vicina a tale impresa, ovviamente. Ma il fatto che l'impresa sia possibile in linea di principio ha delle implicazioni su come l'umanità dovrebbe uscire da questa situazione, come vedremo più avanti, nella [discussione approfondita](#what-would-it-take-to-shut-down-global-ai-development?) del capitolo 10\.

### Le intuizioni "ovvie" richiedono tempo {#"intuizioni-ovvie"-richiedono-tempo}

È difficile trovare intuizioni nell'ambito dell'IA, anche quando a posteriori sembrano semplici e ovvie. Questo è importante da capire perché fare bene l'IA probabilmente richiederà molte intuizioni. Non importa quanto possano sembrare semplici a posteriori, a volte ci vogliono decenni di duro lavoro per trovare queste intuizioni.

A tal fine, metteremo in evidenza alcune delle intuizioni che alimentano le moderne IA.

Se avete qualche competenza di programmazione, ad esempio, potreste leggere il capitolo 2 del libro e pensare che questa "discesa del gradiente" sia così semplice da poterla provare immediatamente. Ma se lo faceste, probabilmente vi imbattereste rapidamente in qualche tipo di errore. Forse il vostro programma andrebbe in crash con un errore di virgola mobile perché i numeri in uno dei pesi sono diventati troppo grandi.

Nel ventesimo secolo, nessuno sapeva come far funzionare la discesa del gradiente su una rete neurale con diversi livelli di numeri intermedi tra l'input e l'output. Per evitare problemi, i programmatori dovevano imparare ogni sorta di trucco, come inizializzare tutti i pesi in modo leggermente intelligente per evitare che diventassero troppo grandi. Per esempio, invece di inizializzare tutti i pesi con un numero casuale tra 0 e 1 (o un numero casuale con media 0 e deviazione standard 1), dovevi inizializzare i pesi in quel modo e poi dividerli tutti per una costante pensata per garantire che anche i numeri del livello successivo non diventassero troppo grandi durante il funzionamento.

La discesa del gradiente incontra problemi quando viene eseguita su formule complicate con molti passaggi o "livelli", e dividere i numeri casuali iniziali per una costante è una delle idee principali che rende possibile il "deep learning". Questo trucco non è stato inventato fino a sei decenni dopo che le reti neurali furono originariamente proposte nel 1943.

L'idea di usare il calcolo per regolare i parametri fu discussa per la prima volta nel 1962 e applicata per la prima volta all'idea di reti neurali con più di un livello nel 1967. Non è stata davvero diffusa fino a un articolo del 1986 (di cui Geoffrey Hinton era coautore, uno dei motivi per cui è chiamato "padrino dell'IA"). Da notare, tuttavia, che l'idea più generale di usare il calcolo su questioni differenziabili per muoversi nella direzione di una risposta corretta — per esempio, per calcolare una radice quadrata — fu inventata da Isaac Newton.

Un altro trucco fondamentale è il seguente. Nel libro, diamo un esempio di operazioni di discesa del gradiente:

> Moltiplicherò ogni numero in input per il peso nel primo parametro, poi lo sommerò al peso nel secondo parametro, quindi lo sostituirò con zero se è negativo, e poi...

Questa lista di operazioni non è casuale. Moltiplicazione, addizione e "sostituiscilo con zero se è negativo" sono, più o meno, le tre operazioni critiche in una rete neurale. Le prime due sono gli operatori che compongono una "moltiplicazione matriciale", mentre l'ultima introduce una "non linearità" e permette così alla rete di apprendere funzioni non lineari.

La formula per "sostituiscilo con zero se è negativo" è $$y \= \\mathrm{max}(x, 0)$$ e si chiama unità lineare rettificata (ReLU).[^55] La formula che inizialmente si cercò di utilizzare era la formula "sigmoide":

$$\\frac{e^x}{1 \+ e^x}$$

![][image4]

C'erano buone ragioni per ipotizzare che la più complicata formula "sigmoide" avrebbe funzionato! Da una prospettiva superficiale, fa sì che gli output varino sensibilmente da 0 a 1 in modo fluido; e da una prospettiva più profonda, ha alcune utili connessioni con la teoria della probabilità. Anche alcune moderne reti neurali profonde usano qualcosa di simile a una sigmoide in alcuni passaggi. Ma se si deve usare una sola non linearità, una ReLU funziona molto meglio.

Il problema della formula sigmoide è che tende a far sì che molti output abbiano gradienti molto piccoli. E se la maggior parte dei gradienti è molto piccola, la discesa del gradiente smette di funzionare... almeno, a meno che non si conosca il trucco moderno di fare passi di gradiente più grandi quando i gradienti piccoli puntano sempre nella stessa direzione. (Per quanto ne sappiamo, questo trucco è apparso per la prima volta in letteratura nel 2012, quando è stato proposto da Geoffrey Hinton).

"Rendi i tuoi numeri casuali iniziali più piccoli in modo che le loro somme moltiplicate non diventino enormi" e "usa max(x, 0\) invece di una formula complicata" e "fai passi più grandi quando i gradienti piccolissimi continuano a puntare nella stessa direzione" potrebbero sembrare idee stranamente semplici da non inventare per decenni, soprattutto perché sembrano ovvie a posteriori per un programmatore di computer che capisce tutte queste cose. Questa è una lezione importante su come funzionano la scienza e l'ingegneria nella vita reale.

*Anche quando esiste una soluzione semplice e pratica a una sfida ingegneristica, spesso i ricercatori non la trovano fino a quando non hanno provato e fallito per decenni.* Non si può contare sul fatto che i ricercatori la vedano non appena una soluzione diventa importante. Non si può contare sul fatto che la vedano entro i prossimi due anni. Anche se una soluzione sembra ovvia col senno di poi, a volte il settore arranca per decenni senza di essa.

Stiamo anticipando un po' le risorse online del Capitolo 2, ma questa è una lezione da tenere a mente nella Parte III del libro, quando parleremo di come l'umanità sia impreparata ad affrontare la sfida rappresentata dalla superintelligenza artificiale.

Se il prezzo da pagare per gli inventori pazzi che procedono a tentoni è che tutti sulla Terra muoiano durante questa fase infantile goffa, non dobbiamo lasciare che gli inventori pazzi procedano a tentoni. Gli inventori pazzi protesteranno dicendo che non c'è modo per loro di trovare una soluzione semplice e solida senza poter procedere a tentoni per alcuni decenni; diranno che non è realistico aspettarsi che lo capiscano in anticipo.

Si spera che sia ovvio per tutti quelli che non sono inventori pazzi che, se queste affermazioni sono vere, dovremmo fermare i loro sforzi. Ma questo è un argomento che riprenderemo nella Parte III del libro, dopo aver completato l'argomentazione secondo cui la superintelligenza artificiale avrebbe i mezzi, il movente e l'opportunità per estinguere l'umanità.

### A cosa serve la conoscenza degli LLM? {#a-cosa-serve-la-conoscenza-degli-llm?}

Cosa deriva dal comprendere gli LLM? Come ci aiuta a capire l'intelligenza artificiale più intelligente degli esseri umani e come evitare che tutti muoiano?

Un vantaggio è che sapere concretamente cosa succede lì dentro — almeno la parte che possiamo vedere, i numeri imperscrutabili — può potenzialmente darci una sensazione più concreta, più solida, rispetto a sapere solo che "un giorno mi sono svegliato e i computer hanno iniziato a parlare per qualche motivo".

Per esempio: forse se sai che gli attuali LLM sono costruiti addestrando solo l'uno per cento dei parametri rispetto alle sinapsi contenute nel cervello umano, è più facile capire perché l'IA non rimarrà per sempre al livello di capacità attuale.

Quando si progetta un trattato internazionale per fermare la corsa verso la superintelligenza, è utile sapere che l'addestramento di un'IA è una fase diversa dal far funzionare l'IA (quest'ultima è chiamata "inferenza").

È altresì utile sapere che la separazione di queste fasi è un fatto contingente e temporaneo relativo al funzionamento *attuale* dell'IA e che un algoritmo futuro potrebbe cambiare le cose. Oggi si potrebbe redigere un trattato che separi il trattamento dell'addestramento dell'IA dall'inferenza dell'IA, ma bisognerebbe essere pronti a modificare tale teoria se gli algoritmi cambiassero.

Sapere che esiste *un* algoritmo è importante, così come vedere come, in alcuni casi semplici, esso crei le proprietà dell'IA che devono essere regolamentate. Se si comprendono le basi stesse dell'algoritmo, si è in una posizione migliore per capire il tipo di ricerca che l'industria dell'IA sta (legalmente, per ora) cercando di fare, e come ciò potrebbe influenzare le regole sottostanti se fosse loro consentito procedere.

L'algoritmo transformer, senza il quale l'IA attuale non esisterebbe, è stato un grande passo avanti sviluppato da un piccolo gruppo di persone presso Google. Il prossimo passo avanti simile potrebbe o meno portare l'IA oltre una [soglia critica](#is-“intelligence”-a-simple-scalar-quantity?). È più facile capirlo se si ha un'idea di cosa fa un "algoritmo transformer", di quanto sia semplice e del perché abbia avuto un impatto così forte sul settore.

C'è molta disinformazione che si basa sul fatto che chi ascolta non sa come funziona l'IA. Alcune persone [sostengono](#do-experts-understand-what's-going-on-inside-ais?) che gli esseri umani capiscano cosa succede nelle IA attuali, quando invece non è così. Altri ti diranno che le IA non potrebbero mai essere pericolose perché sono "[solo matematica](#aren't-ais-“just-math”?)," come se ci fosse un abisso invalicabile che separa la cognizione dell'IA basata su [enormi](#llms-are-large) quantità di "matematica" dalla cognizione umana basata su enormi quantità di "biochimica".

L'8 luglio 2025, Grok 3 ha iniziato a definirsi [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Per qualche motivo, il CEO di Twitter ha scelto il giorno dopo per [dimettersi](https://www.politico.com/news/2025/07/09/linda-yaccarino-x-ceo-resign-00443742).

Per comprendere l'accaduto, è importante considerare se si ritiene che i creatori di Grok abbiano deliberatamente istruito Grok a comportarsi in quel modo o se ci si rende conto che le IA sono "coltivate" e che gli sviluppatori di IA hanno capacità limitate di controllare o prevedere il loro comportamento.

È grave in un modo se i creatori di Grok hanno creato MechaHitler di proposito; è grave in un modo diverso se i creatori hanno ottenuto MechaHitler *per caso*, cercando di spingere Grok in una qualche direzione (possibilmente non correlata) senza la capacità di prevedere gli effetti che ciò avrebbe avuto sul comportamento di Grok.[^56]

Speriamo che le informazioni fornite in *Se qualcuno lo costruisce, muoiono tutti* costituiscano un utile baluardo contro idee sbagliate comuni e disinformazione. Per i lettori interessati a maggiori dettagli, forniamo una spiegazione più completa di come funziona un LLM specifico [qui sotto](#a-full-description-of-an-llm).

È abbastanza? Alcune persone sostengono che solo chi si trova all'avanguardia assoluta della ricerca attuale possa sapere se le IA (simili agli LLM o meno) siano destinate a distruggere l'umanità.

Io (Yudkowsky) ho partecipato una volta a una conferenza a Washington, DC, per persone che lavorano sulle "politiche sull'IA". Mentre ero lì, un paio di persone mi si sono avvicinate e mi hanno chiesto se potevo spiegare come funzionano i trasformatori. "Beh", ho detto, "sarebbe molto più facile con una lavagna, ma per provare a dare un riassunto divulgativo di cosa succede lì dentro, l'idea chiave è che per ogni token calcola query, chiavi e valori —" e ho continuato per un po', cercando di esprimere tutto in termini adatti ai principianti. Alla fine, le due persone sono riuscite a inserirsi e a spiegare che in realtà erano programmatori di IA. Stavano girando per la conferenza chiedendo a tutti se le persone che affermavano di lavorare nelle politiche sull'IA sapessero spiegare come funzionano i trasformatori. Mi hanno detto che fino a quel momento ero stata l'unica persona in grado di rispondere.

Sentire questo mi ha preoccupato un po'.

C'è una domanda valida su quanto sia davvero importante per le politiche sull'IA sapere esattamente come funzionano i trasformatori — quanto i piccoli dettagli cambino qualcosa nel quadro generale.

Chi lavora nelle politiche sull'IA *deve* capire query-key-value? Da un certo punto di vista — per i nerd a cui questo tipo di apprendimento viene facile — certo che dovresti impararlo; potrebbe essere importante. Da questo punto di vista, sembra strano e inquietante se qualcuno a una conferenza dice di lavorare nelle politiche sull'IA ma non ha idea di come funzionano i trasformatori.

Più pragmaticamente, alcuni aspetti dei trasformatori e della loro storia possono essere rilevanti per questioni più ampie. Ad esempio, l'algoritmo standard costa quantità sempre maggiori di calcolo man mano che l'IA cerca di considerare sempre più "contesto" simultaneamente — documenti più lunghi, codebase più grandi. Non puoi semplicemente spendere 10 volte le risorse di calcolo e ottenere un'IA che funzioni su un progetto 10 volte più grande; devi fare qualcosa di intelligente affinché un progetto 10 volte più grande costi meno di 100 volte il calcolo.

È importante anche per le politiche quanto tempo sia stato necessario per inventare l'algoritmo del trasformatore, quante persone siano state necessarie per inventarlo e quanto sia complicato quell'algoritmo. La storia è una guida utile (anche se imperfetta) per capire quanto dobbiamo prepararci per un'altra grande svolta come quella. Allo stesso modo, è rilevante per le politiche sull'IA quanto miglioramento abbiano rappresentato i trasformatori rispetto alla tecnologia precedente ("reti neurali ricorrenti") per l'elaborazione del testo — perché quel tipo di cose potrebbe accadere di nuovo.

Devi davvero essere in grado di abbozzare le matrici QKV?

Probabilmente no. Noi possiamo farlo, e in un gruppo di decine di persone che lavorano sulle politiche sull'IA, ci sentiremmo più ottimisti se almeno una avesse il background necessario per fare lo stesso. Non fa male essere sicuri; non si sa mai quale fatto importante possa finire per nascondersi in un dettaglio del genere.

Io (Yudkowsky) non riesco a ricostruire a memoria i dettagli di un [SwiGLU gate](https://arxiv.org/pdf/2002.05202) e come differisca da un GLU, perché quando li ho cercati, i dettagli esatti sembravano non avere alcuna rilevanza per questioni più ampie, quindi non li ho memorizzati. Ma potrebbe essere informativo per il principiante sapere che SwiGLU è stato trovato con una sorta di test alla cieca, e che gli autori dell'articolo hanno detto apertamente di non avere idea del perché queste tecniche funzionino nella pratica. Conoscevamo già molti casi del genere, ma se *non* sapevi che le persone che propongono miglioramenti architetturali spesso dicono di non avere idea del perché funzionino, questa è un'informazione rilevante.

Tutto ciò si riassume in: sapere almeno un po' come funzionano gli LLM è importante per poter vedere quanto poco *chiunque* sappia dell'IA moderna.

A volte, gli esperti fingono di avere conoscenze segrete accessibili solo a chi ha lavorato per anni a far crescere un'IA. Ma non riescono a definire le loro conoscenze, e chi scrive articoli usa frasi come (per citare l'articolo che presenta SwiGLU):

> Non diamo alcuna spiegazione sul perché queste architetture sembrano funzionare; attribuiamo il loro successo, come tutto il resto, alla benevolenza divina.

A volte, gli esperti scientifici sanno cose che noi non sappiamo. Ma è piuttosto raro nella scienza che qualcuno dica: "Ho una conoscenza estremamente rara e raffinata che dimostra che quello che dici è sbagliato, e devi semplicemente credermi sulla parola; non posso dirti quali risultati sperimentali o formule matematiche conosco che tu non conosci".

Immagina un mondo in cui solo le persone pagate con stipendi a sette cifre per sapere come impostare il programma di apprendimento su un ottimizzatore a discesa del gradiente dovrebbero essere ascoltate, un mondo in cui solo loro sono abbastanza intelligenti da aver letto gli esperimenti chiave e imparato le formule chiave per sapere che l'umanità sarebbe perfettamente al sicuro dalla superintelligenza artificiale, o per sapere che la superintelligenza artificiale non potrà essere creata per altri 100 anni. Questo genere di cose a volte succede in altri campi della scienza! Ma quando succede, l'esperto di solito può indicare qualche formula o risultato sperimentale e dire: "Questa è la parte che i non addetti ai lavori non capiscono". Non ci viene in mente nessun caso storico in cui si sia affermato che una conoscenza fosse del tutto inaccessibile a un pubblico esterno con competenze tecniche e che tale conoscenza si sia poi rivelata vera.

Potrebbe arrivare un momento in cui un rappresentante dell'industria dell'IA ti metterà un braccio intorno alle spalle e insisterà sul fatto che *loro* capiscono cosa stanno costruendo, che si tratta solo di numeri, che andrà tutto bene. È utile, quindi, conoscere un po' i dettagli di come vengono fatte crescere le IA, in modo che quando qualcuno ti fa questa affermazione, puoi chiedergli cosa lo rende così sicuro.

### Una descrizione completa di un LLM {#a-full-description-of-an-llm}

#### **Come funziona Llama 3.1 405B** {#come-funziona-llama-3.1-405b}

Nel libro, abbiamo promesso una descrizione più completa di un LLM chiamato Llama 3.1 405B. Presentiamo qui sotto quella descrizione. È per chi è curioso e per capire davvero quanto le moderne IA siano fatte crescere piuttosto che create. (Vedi anche: [A cosa serve la conoscenza degli LLM?](#a-cosa-serve-la-conoscenza-degli-llm?))

La discussione che segue è abbastanza dettagliata e assumiamo (qui, ma non nella maggior parte delle altre risorse online) che tu abbia un background tecnico, anche se non assumiamo alcuna conoscenza specializzata di IA. Se inizi a leggere questa sezione e non la trovi utile, considera di saltarla.

I dettagli su come vengono addestrati i modelli linguistici più avanzati non vengono di solito pubblicati, così come il codice. Ma ci sono delle eccezioni. Uno dei sistemi più potenti la cui architettura e i cui pesi sono stati resi pubblici, al momento della stesura del libro alla fine del 2024, era Llama 3.1 405B, realizzato dalla divisione IA di Meta. Il "405B" sta per i 405 miliardi di parametri dell'architettura, riempiti da 405 miliardi di pesi.

Perché stiamo esaminando questo particolare modello di IA? Llama 3.1 405B è "open-weights"[^57], il che significa che è possibile scaricare autonomamente quei 405 miliardi di numeri imperscrutabili (insieme allo scheletro di codice scritto da esseri umani, molto più piccolo, che esegue operazioni aritmetiche sui 405 miliardi di numeri e quindi fa funzionare l'IA). Questo ci permette di fare affermazioni sul suo design con una certa sicurezza.[^58]

Comunque! Parliamo di come sono organizzati quei 405 miliardi di numeri imperscrutabili — del modo in cui sono stati impostati, anche prima dell'addestramento, in modo tale che gli ingegneri di Meta si aspettassero correttamente che modificando quei numeri iniziali casuali nella direzione di una migliore previsione del token successivo (frammento di parola), dato l'addestramento su oltre 15,6 bilioni di token, si sarebbe creata un'IA in grado di parlare.

Il primo passo è suddividere tutte le parole in tutte le lingue supportate in token.

Il passo successivo è trasformare ciascuno di questi token in un "vettore" di numeri. Llama usa vettori di 16 384 numeri per ogni token del dizionario standard. Il suo vocabolario ha 128 256 token.

Per trasformare ogni token in un vettore, a ogni token possibile viene assegnato un peso per ogni posizione possibile nel vettore. È da qui che otteniamo il nostro primo blocco di miliardi di parametri:

$$128{,}256 \\times 16{,}384 \= 2{,}101{,}248{,}000$$

Due miliardi di parametri fatti. Ne restano ancora quattrocentotré miliardi!

Giusto per ribadire il concetto — nessun essere umano dice a Llama cosa significano i token, inventa il vettore di 16 384 numeri a cui una parola corrisponde o sa cosa significa il vettore di numeri per una parola qualsiasi. Tutti quei due miliardi di parametri sono stati ottenuti tramite la discesa del gradiente. I numeri vengono modificati, insieme ad altri parametri che introdurremo, per aumentare la probabilità assegnata al vero token successivo.[^59]

Diciamo che Llama inizia guardando un blocco di 1 000 parole, come un frammento di un saggio. (O meglio, 1 000 token. Ma da qui in poi, per semplicità, a volte diremo semplicemente "parole").

Per ciascuna di queste parole, cerchiamo quella parola nel dizionario dell'LLM e carichiamo nella memoria il suo elenco di 16 384 numeri incomprensibili. (Inizialmente, questi numeri erano stati impostati in modo casuale, all'inizio dell'addestramento; poi sono stati modificati mediante la discesa del gradiente).

1 000 parole × (16 384 numeri / parola) = 16 384 000 numeri in totale. Li chiamiamo "attivazioni" nel primo "livello" dei calcoli di Llama (cioè la sua cognizione, la sua attività mentale).

Puoi immaginarli disposti in un rettangolo piatto sul pavimento che misura 1 000 numeri in larghezza (la lunghezza dell'input) per 16 384 numeri in larghezza (numeri per parola nel primo strato). Ecco uno di questi vettori, con il colore di ogni pixel che corrisponde al numero nel vettore:

![][image5]

(Non sono gli artefatti più scrutabili.)

Nota anche che ci sono due numeri diversi qui che non dovrebbero essere confusi:

* Il numero di *parametri che determinano il comportamento di questo livello* (cioè i 2 101 248 000 numeri memorizzati nel dizionario)  
* Il numero di *attivazioni* o *numeri usati nel pensiero* nel primo livello quando si inseriscono mille parole (ovvero 16 384 000 numeri per la prima fase dell'elaborazione di una query di 1 000 parole).

Ora abbiamo la nostra enorme matrice di numeri che rappresenta la nostra query in tutto il suo splendore e possiamo iniziare a usarla effettivamente.

Per prima cosa c'è qualcosa chiamato "[normalizzazione](https://en.wikipedia.org/wiki/Normalization_\(machine_learning\))," che avviene molte volte nel corso l'elaborazione di un LLM. È simile alla normalizzazione in statistica, ma con una variante tipica del machine learning. Questa variante consiste nel fatto che, dopo aver normalizzato i dati all'interno di ogni *riga*, un parametro specifico appreso chiamato "scala" viene moltiplicato per ogni *colonna*. Questi numeri di scala, come tutti gli altri parametri di cui parleremo, vengono appresi durante l'addestramento. Inoltre, la normalizzazione dei livelli avviene decine di volte, e ogni volta ha un *nuovo* gruppo di parametri di scala, quindi la normalizzazione rappresenta moltissimi parametri nel corso dell'LLM. In particolare, 16 384 parametri per normalizzazione. (Se sei curioso di conoscere più nel dettaglio il tipo di normalizzazione che Llama 3.1 405B usa, si chiama RMSNorm).

Potresti pensare: "Wow, c'è davvero un sacco di preprocessing", e in effetti avresti ragione. In realtà, abbiamo sorvolato su alcuni dei punti più fini, quindi c'è ancora più di quanto potrebbe sembrare, e solo ora stiamo arrivando alla caratteristica più distintiva degli LLM: il livello di "attenzione".

L'"attenzione" è il motivo per cui si parla tanto di "trasformatori" (se sei abbastanza grande da ricordarti il clamore per la nuova invenzione dei trasformatori). Gli LLM sono una sorta di "trasformatori"; i trasformatori sono stati introdotti in un articolo del 2017 intitolato "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)". Questo articolo, più di ogni altro, è considerato responsabile del successo degli LLM. Un livello di "attenzione" funziona così:

Prendiamo ciascuno dei 1 000 vettori di 16 384 attivazioni e trasformiamo ciascun vettore di 16 384 attivazioni:

* in 8 *chiavi*, ciascuna un vettore di 128 attivazioni  
* in 8 *valori*, ognuno un vettore di 128 attivazioni  
* e in 128 *query*, ciascuna un vettore di 128 attivazioni

Il "passaggio di attenzione" sopra ogni token consiste nel confrontare ciascuna delle 128 query alle 8 chiavi, verificando quale delle 8 chiavi assomiglia di più o corrisponde a quella query, e caricando una combinazione degli 8 valori, con i valori delle chiavi più corrispondenti pesati maggiormente nella combinazione.

Questo permette, in linea di massima, a ciascuna delle attivazioni sopra un token di creare una serie di "query", che poi cercano le "chiavi" sopra tutti gli altri token. Quando la query di un token corrisponde meglio a una chiave, recupera il valore corrispondente più intensamente, per passarlo ai calcoli successivi sopra quel token.

Per esempio, la parola "right" potrebbe attivare una query progettata per cercare le parole vicine e vedere se qualcuna di esse riguarda le *direzioni spaziali* o, in alternativa, le *credenze*, per capire se la parola "right" significa "destra" come in "mano destra" o "giusto" come in "risposta giusta". (Anche in questo caso, tutto ciò viene appreso tramite la discesa del gradiente; nulla di tutto ciò è programmato da esseri umani che riflettono sui diversi significati che la parola inglese "right" può assumere.)[^60]

I livelli di attenzione in un LLM sono piuttosto grandi, con un numero enorme di parametri in ciascuno di essi. Llama 3.1 405b, in particolare, ha 126 livelli di attenzione di questo tipo (abbiamo descritto solo il primo di essi), e ciascuno dei 126 ha 570 425 344 parametri, suddivisi tra matrici di query, chiave, valore e output.[^61]

Una volta che il sottolivello di attenzione è pronto e ci ritroviamo con una matrice delle stesse dimensioni di quella iniziale (nel nostro esempio, 16 384 per 1 000), facciamo una cosa chiamata "connessione residua". In pratica, si prende qualsiasi input del sottolivello (in questo caso, l'enorme matrice con cui abbiamo iniziato) e lo si aggiunge a qualsiasi risultato ottenuto. Questo impedisce che un dato sottolivello cambi *troppo* in un dato passaggio (e ha altre interessanti proprietà tecniche).

Poi, il risultato passa attraverso quella che si chiama "rete feed-forward". La variante usata da Llama 3.1 405B dipende da un'operazione chiamata "SwiGLU". SwiGLU è stata scoperta da alcuni ricercatori che hanno provato ad addestrare il modello con tante formule diverse per vedere quali funzionavano meglio, come riportato nel loro [articolo originale](https://arxiv.org/pdf/2002.05202) (come abbiamo anche [notato altrove](#what-good-does-knowledge-of-llms-do?)).

Non diamo alcuna spiegazione sul perché queste architetture sembrano funzionare; attribuiamo il loro successo, come tutto il resto, alla benevolenza divina.

Come tutte le reti feed-forward, SwiGLU fondamentalmente espande la nostra matrice 16 384 x 10 000 in una matrice ancora più grande, la trasforma e poi la comprime di nuovo. In particolare, ogni riga passa da 16 384 colonne a 53 248 colonne, per poi tornare a 16 384.

Ora che abbiamo finito con il sottolivello feed-forward, facciamo di nuovo la connessione residua, aggiungendo quello che avevamo all'inizio a quello che abbiamo ottenuto alla fine.

È stato un percorso lungo, ma ora abbiamo trasformato leggermente la nostra gigantesca matrice.

Questi passaggi insieme formano un unico "livello". Llama ha 126 livelli, quindi ripeteremo tutti questi passaggi (normalizzazione, meccanismo di attenzione, connessione residua, rete feed-forward e di nuovo connessione residua) 125 volte.

Alla fine dei 126 livelli, ci ritroviamo con una matrice delle stesse dimensioni di quella iniziale; nel nostro esempio, 16 384 per 1 000. Ogni riga di questa matrice può quindi essere proiettata in un nuovo vettore di 128 256 numeri, uno per ogni token nel dizionario completo del modello. Questi numeri possono essere positivi o negativi, ma una comoda funzione chiamata softmaxing può essere usata per convertirli tutti in probabilità, che sommate danno uno. Queste probabilità sono la previsione di Llama su quale token verrà dopo.

Ora è possibile fare in modo che Llama generi un nuovo token. Un modo per farlo è prendere il token a cui Llama ha dato la probabilità più alta, anche se potresti anche cambiare le cose prendendo occasionalmente token che secondo lui sono un po' meno probabili.[^62]

Se stai usando Llama normalmente, tipo in un'interfaccia chatbot, tutto questo processo ha output un solo token. Questo token viene messo alla fine dell'input e si ripete tutto da capo per il token successivo. Quindi si fanno tutti i passaggi di prima, ma ora la matrice ha 1 001 righe. Poi, un altro token dopo, 1 002 e così via.

Abbiamo tralasciato molti dettagli, ma questo è, in sostanza, il funzionamento di Llama 3.1 405B.

#### **Gli LLM sono grandi** {#llms-are-large}

Parliamo un po' delle dimensioni colossali di Llama 3.1 405B.

Per Llama, elaborare un testo di 1 000 parole (o meglio 1 000 token) richiede circa 810 bilioni di calcoli.[^63]

Se 810 bilioni vi sembrano tanti, tenete presente che la maggior parte dei 405 miliardi di parametri di Llama vengono usati in *qualche* operazione aritmetica *ogni* volta che viene elaborata *una* singola parola.[^64]

Se Llama viene addestrato su un gruppo di 1 000 token, allora ognuno di questi 1 000 token verrà confrontato con la parola effettiva seguente e le perdite propagate dalla discesa del gradiente, per capire come la modifica di tutti i 405 miliardi di parametri condivisi avrebbe cambiato le probabilità assegnate alle risposte corrette in tutti i casi. Questo richiederà molti più calcoli e molti più numeri.

Durante l'addestramento dei 405 miliardi di parametri di Llama su 15,6 trilioni di token, ci sono voluti circa 38 settilioni di calcoli, cioè 38 seguito da 24 zeri.

Se invece Llama ha completato l'addestramento e viene eseguito in *modalità di inferenza* (cioè se sta generando un testo nuovo, come in una chat con un utente), le probabilità verranno calcolate solo sull'ultimo token, come se prevedesse quale *sarebbe* la parola successiva se l'IA stesse leggendo un testo prodotto da esseri umani.

Poi, uno scheletro di codice scritto da esseri umani attorno a Llama sceglierà quella che Llama ritiene sia la risposta più probabile.[^65]

Ed è così che si fa a far parlare un computer con te! Non in modo intelligente come le IA commerciali del 2025, ma comunque parlando un po' come una persona.

Per elaborare mille parole, Llama usa 405 miliardi di piccoli parametri imperscrutabili in 810 bilioni di calcoli — calcoli organizzati matematicamente in rettangoli, cubi e forme di dimensioni superiori.

A volte chiamiamo queste disposizioni "matrici giganti e imperscrutabili", perché se si osservano attentamente alcuni dei parametri di Llama — anche quelli più semplici memorizzati nel semplice dizionario alla base del vasto stack di livelli — i primi parametri per la parola "right" appaiono così:

:::Teletype  
\[-0.00089263916015625,     0.01092529296875,  
  0.00102996826171875,    \-0.004302978515625,  
 \-0.00830078125,          \-0.0021820068359375,  
 \-0.005645751953125,      \-0.002166748046875,  
 \-0.00141143798828125,    \-0.00482177734375,  
  0.005889892578125,       0.004119873046875,  
 \-0.007537841796875,      \-0.00823974609375,  
  0.00848388671875,       \-0.000965118408203125,  
 \-0.00003123283386230469, \-0.004608154296875,  
  0.0087890625,           \-0.0096435546875,  
 \-0.0048828125,           \-0.00665283203125,  
  0.0101318359375,         0.004852294921875,  
 \-0.0024871826171875,     \-0.0126953125,  
  0.006622314453125,       0.0101318359375,  
 \-0.01300048828125,       \-0.006256103515625,  
 \-0.00537109375,           0.005859375,  
:::

... e così via per 16 384 numeri. Per quanto riguarda il *significato* di questi numeri, al momento nessuno sulla faccia della Terra lo conosce.

Io (Soares) ho cronometrato il tempo che mi ci è voluto per recitare ad alta voce i primi trentadue numeri con sei cifre significative. Ci ho messo due minuti e quattro secondi. Per recitare tutti i parametri della parola "right", anche con quell'abbreviazione, mi ci sarebbero volute più di diciassette ore. Alla fine della recitazione, non sarei stato più saggio di prima su cosa significa la parola "right" per Llama.

Per dire tutti i parametri di Llama, parlando a 150 parole al minuto e senza mai fermarsi per mangiare, bere o dormire, un essere umano ci metterebbe 5 133 anni. Per dire tutte le attivazioni che corrispondono a mille parole nel dizionario dei token di Llama ci vorrebbero settantasei giorni di fila. Per scrivere tutti i calcoli usati per elaborare un singolo token per un input di 1 000 parole ci vorrebbero, se scrivessi come un genio 150 calcoli al minuto senza pause, più di dieci milioni di anni.

E questo solo per generare una sillaba! Per scrivere un'intera frase ci vorrebbe molto più tempo.

E se tu facessi personalmente tutti questi calcoli con il tuo cervello, alla fine dei (almeno) dieci milioni di anni che ci vorrebbero, non saresti più saggio di prima su cosa stesse pensando Llama prima di pronunciare la sua prossima parola. Non sapresti dei pensieri di Llama più di quanto un neurone sappia del cervello umano.

In quel mondo immaginario in cui non sei morto da tempo di vecchiaia, essere in grado di eseguire un singolo calcolo locale non significa che il tuo cervello sappia qualcosa su ciò che Llama sta pensando o su come lo stia pensando.

Se mettessi tutti i 405 miliardi di parametri di Llama in un foglio Excel su uno schermo di computer di dimensioni normali, il foglio sarebbe grande quanto 6 250 campi da football americano, o 4 000 campi da calcio, o metà di Manhattan.

Se avessi un centesimo per ogni calcolo nel nostro esempio dei 1 000 token, avresti 810 bilioni di centesimi. Se provassi a depositarli in banca, avresti bisogno di 203 milioni di camion carichi di centesimi, ciascuno del peso di 44 000 libbre.

Llama 3.1 405B non è ancora grande quanto un cervello umano. (Un cervello umano ha circa 100 trilioni di sinapsi.)

405B, tuttavia, apparentemente può parlare come una persona.

E se qualcuno ti mette il braccio intorno alle spalle e ti confida con tono cinico che in realtà si tratta solo di numeri, tieni presente che stiamo parlando di *davvero moltissimi numeri*.

Un neurone umano può essere visto come ["solo" chimica](#*-dire-che-l'intelligenza-artificiale-è-"solo-matematica"-è-come-dire-che-gli-esseri-umani-sono-"solo-biochimica"), se studi la biochimica e le sostanze chimiche che si legano tra loro e fanno viaggiare piccoli flash di depolarizzazione elettrica nel cervello umano. Ma è *un sacco* di chimica. E si scopre che cose molto semplici, in quantità sufficientemente grandi, disposte in un certo modo, possono far atterrare razzi sulla Luna.

Un tipo simile di cautela si applica a un modello linguistico di grandi dimensioni. La parola "grande" non è lì per fare scena.

### "Fingi finché non ce la fai" {#"fake-it-'til-you-make-it"}

Molte speranze che l'IA si evolva positivamente sembrano basarsi su una vaga sensazione che i modelli siano già abbastanza ben educati (anche se a volte un po' confusi) e che diventeranno servitori saggi e benevoli man mano che comprenderanno più pienamente i ruoli loro assegnati. Potremmo chiamarlo il modello "fingi finché non ce la fai" dell'allineamento dell'IA.

Ma migliorare le prestazioni nel "fingere" avvicina davvero i modelli al "raggiungere l'obiettivo", ovvero diventare menti che agiscono in quel modo perché *sono* così?

Le IA come ChatGPT sono addestrate per prevedere con precisione i loro dati di addestramento. I loro dati di addestramento sono composti principalmente da testi umani, come le pagine di Wikipedia e le conversazioni nelle chat room. (Questa parte del processo di addestramento si chiama "pre-addestramento", che è ciò che rappresenta la "P" in "GPT".) I primi LLM come GPT-2 erano addestrati *esclusivamente* per la previsione in questo modo, mentre le IA più recenti sono addestrate anche su cose come risolvere accuratamente problemi matematici (generati al computer), fornire buone risposte secondo un altro modello di IA e vari altri obiettivi.

Ma consideriamo un'IA addestrata solo a prevedere testo generato dall'uomo. Deve necessariamente diventare simile all'uomo?

Supponi di prendere un'attrice eccellente e di farle imparare a prevedere il comportamento di tutti gli ubriachi in un bar. Non "imparare a interpretare un ubriaco stereotipato medio", ma piuttosto "imparare a conoscere tutti gli ubriachi di questo bar come *individui*". Gli LLM non sono addestrati a *imitare le medie*; sono addestrati a *prevedere le prossime parole individuali* usando tutto il contesto delle parole precedenti.

Sarebbe sciocco aspettarsi che l'attrice diventi perpetuamente ubriaca nel processo di imparare a prevedere cosa dirà ogni persona ubriaca. Potrebbe sviluppare parti del suo cervello che sono piuttosto brave a recitare da ubriaca, ma non diventerebbe ubriaca *lei stessa*.

Anche se poi chiedessi all'attrice di prevedere cosa farebbe un particolare ubriaco nel bar e poi di comportarsi esteriormente secondo la sua previsione, non ti aspetteresti comunque che l'attrice si senta ubriaca dentro.

Cambierebbe qualcosa se modificassimo costantemente il cervello dell'attrice per fare previsioni *ancora migliori* sugli ubriachi individuali? Probabilmente no. Perché lei finisse per essere *davvero* ubriaca, i suoi pensieri finirebbero per diventare confusi, interferendo con il duro lavoro di attrice. Potrebbe confondersi sul fatto di stare prevedendo un'Alice ubriaca o una Carol ubriaca. Le sue previsioni peggiorerebbero, quindi il nostro ipotetico modificatore del cervello imparerebbe a non modificare il suo cervello in quel modo.

Allo stesso modo, addestrare un LLM a fare previsioni eccellenti sulla prossima parola scritta da molte persone diverse sulle loro esperienze psichedeliche passate non dovrebbe quindi addestrare le cognizioni interne dell'LLM a essere "sotto l'effetto di droghe" nel senso intuitivo. Se le cognizioni interne effettive dell'LLM fossero distorte in modo da ricordare l'"essere sotto l'effetto di droghe", ciò interferirebbe con il duro lavoro dell'LLM di previsione della parola successiva; potrebbe confondersi e pensare che un anglofono continuerebbe in cinese.

Per generalizzare una lezione astratta da questo esempio: addestrare qualcosa a prevedere un comportamento esteriore individuale X, che coinvolge una tendenza interna X\*, non implica molto che il predittore finisca con una caratteristica X\* altamente simile al suo interno. Anche se, come l'attrice a cui è stato chiesto di recitare le sue previsioni, è possibile trasformare la sua previsione X in un comportamento esterno che *assomiglia* a X.

Quando un essere umano si comporta in modo molto arrabbiato, inferiamo di default che il comportamento esteriore arrabbiato dell'umano sia causato da sentimenti di rabbia\* interni. Ma c'è una genuina eccezione quando hai a che fare con qualcuno che sai essere un'attrice che recita una parte, che sai che prima prevede le parole e il linguaggio del corpo di un individuo e poi imita quella previsione. Gli stati cognitivi interni dell'attrice che la portano a essere una brava attrice probabilmente derivano dall'arte della sua recitazione o dal suo desiderio di esibirsi bene, non dall'avere lo stesso stato d'animo del personaggio arrabbiato che sta interpretando. Gli attuali LLM, come l'attrice, producono prima delle previsioni e poi le convertono in comportamenti.

Quando attribuisci un comportamento esteriore umano arrabbiato a uno stato mentale interno arrabbiato\* che è *simile al tuo sentimento di rabbia*, stai — se guardi un essere umano — attingendo alla tua storia evolutiva condivisa, alla tua genetica condivisa e ai tuoi cervelli umani molto simili. (E, per essere chiari, molti grandi attori attingono a questa capacità di sentire gli stati emotivi che percepiamo o immaginiamo negli altri). Gli LLM non condividono nulla di tutto ciò. È davvero un'inferenza molto più traballante dire: "Quel LLM mi sembra arrabbiato e quindi probabilmente è davvero arrabbiato".

Perché non aspettarsi che gli LLM risolvano il problema di prevedere la vendicatività diventando essi stessi creature vendicative?

Come essere umano che cerca di capire altri esseri umani che si comportano in modo vendicativo, e dato che il tuo cervello ha il potenziale per sentirsi vendicativo\*, avrebbe senso che il tuo cervello evolvesse l'"empatia" per farlo: cercare di prevedere l'altro cervello attivando i propri circuiti con un insieme parallelo di input. Questo trucco non funziona sempre: a volte le altre persone sono diverse da te e non fanno quello che faresti tu al loro posto. Ma è una cosa ovvia che un cervello costruito dalla selezione naturale provi a fare per prevedere i membri della sua specie.

Gli LLM si trovano in una situazione molto diversa da questa. I loro bilioni di token di addestramento cercano di far loro prevedere, partendo da zero, un'ampia varietà di menti umane alle quali essi stessi inizialmente sono completamente dissimili. Il modo più efficace per risolvere questo problema di previsione dell'altro non sarà quello di diventare una creatura vendicativa\* media. Per esempio, la cognizione LLM più efficace costruita da zero su questa mente aliena-umana potrebbe avere molte annotazioni interne sull'incertezza e sul mantenimento di molteplici possibilità in sovrapposizione, che un essere umano non calcolerebbe nel processo di provare vendetta. O in generale: un ragionamento efficiente, complicato e incerto basato su prove di solito non assomiglia, come cognizione, a una simulazione interna in avanti di un evento tipico. Una previsione efficiente e basata su prove eseguirà, per esempio, un condizionamento sia all'indietro che in avanti su più possibilità in sintesi, mentre una simulazione procederebbe solo in avanti attraverso una sola possibilità.

Niente di tutto questo sta sostenendo che nessuna "mera macchina" possa mai, *in linea di principio*, provare un senso di rabbia simile a quello umano. I tuoi neuroni, se osservati abbastanza da vicino al microscopio, sono costituiti da minuscoli grovigli di meccanismi che pompano neurotrasmettitori dentro e fuori dalle sinapsi. Ma la macchina *specifica* che è il cervello umano e la macchina specifica che è un modello linguistico di grandi dimensioni della fine del 2024 non sono affatto macchine *simili*. Non nel senso che sono fatte di materiali diversi - materiali diversi possono fare lo stesso lavoro - ma nel senso che gli LLM e gli esseri umani sono stati costruiti da ottimizzatori molto diversi per fare lavori molto diversi.

Non stiamo dicendo: "Nessuna macchina potrà mai avere qualcosa di simile allo stato mentale di un essere umano"[^67]. Stiamo dicendo che l'attuale tecnologia di AAA non dovrebbe, per default, generare l'aspettativa di creare motori di previsione dell'ubriachezza che *funzionano* ubriacandosi a loro volta.

Al momento, un po', e forse di più quando leggerete questo articolo, le IA saranno state addestrate a prevedere alcuni comportamenti *molto* simili a quelli umani, e framework come ChatGPT o Claude li trasformeranno in comportamenti esterni dall'aspetto gradevole. Non solo comportamenti umani, ma comportamenti umanitari, anche nobili.

Le aziende che si occupano di IA *potrebbero* provare ad addestrare le IA a prevedere un'umanità più vera e quindi imitarla; potrebbero provarci per motivi cinici o per motivi più nobili. In un certo senso, la dice lunga su questo campo e sulle persone che ci lavorano il fatto che, alla fine del 2024, nessuno abbia *ancora* provato ad addestrare un'IA a prevedere il comportamento esteriore di una persona semplicemente... gentile. Per quanto ne sappiamo, nessuno ha provato a creare un dataset che includa tutte e sole le espressioni *gentili e amichevoli* dell'umanità e ad addestrare un'IA solo su quello. Forse, se qualcuno lo facesse, svilupperebbe un'IA che agisse semplicemente in modo gentile, che esprimesse sentimenti belli, che fosse un faro di speranza.

Non sarebbe reale. Vorremmo disperatamente che fosse reale, ma non lo sarebbe. A seconda di quanto il LLM sottostante preveda quali risposte preferirebbero i suoi addestratori riguardo a sentimenti nobili, speranza e sogni, riguardo al desiderio di un futuro meraviglioso insieme per entrambe le specie, è possibile che uno o entrambi i vostri autori finiscano per piangere, se mai le aziende di IA creassero un'entità del genere. Ma non sarebbe reale, non più di quanto sarebbe reale un'attrice che ha provato e corretto a lungo e che alla fine è stata fatta recitare quelle parole in uno spettacolo teatrale - e per la quale si potrebbe anche piangere al pensiero che non fosse reale.

Non è così che si costruisce una mente artificiale che abbia davvero sentimenti nobili, che lavori davvero con tutto il cuore per guidare verso un futuro più luminoso. Chi addestra l'intelligenza artificiale non sa come creare un'IA che provi questi sentimenti. Addestrano le IA a prevedere e trasformano questa previsione in un'imitazione.

Le aziende di IA (o gli appassionati) potrebbero indicare l'attrice che hanno creato e dire: "Come puoi dubitare di questa povera creatura? Guarda come stai ferendo i suoi sentimenti". Potrebbero persino riuscire a convincersi che sia la verità. Ma modificare delle scatole nere fino a quando qualcosa al loro interno impara a prevedere parole nobili non è il modo in cui si creerebbero menti meravigliose, se le menti umane imparassero mai a crearle.

In parole povere, non ci si dovrebbe aspettare che il comportamento antropomorfo appaia *spontaneamente*. Servono ulteriori argomentazioni per sostenere che quando le aziende di IA forzano deliberatamente un comportamento umano, l'"attrice" interiore finisca per assomigliare al volto umano esteriore che è stata creata e addestrata a prevedere.

# 

# Capitolo 3: Imparare a desiderare {#capitolo-3:-imparare-a-desiderare}

Costruire IA che possono fare cose sufficientemente impressionanti tenderà a far sì che le IA *desiderino* delle cose.

Quando diciamo che un'IA "desidera" qualcosa, non intendiamo che avrà necessariamente desideri o sentimenti simili a quelli umani. Forse sì, forse no. Quello che intendiamo invece è che l'IA si comporterà *come se* desiderasse delle cose. Indirizzerà in modo affidabile il mondo verso certi tipi di risultati, anticipando gli ostacoli, adattandosi alle circostanze mutevoli e rimanendo concentrata, mirata e motivata.

Nel Capitolo 3 di *If Anyone Builds It, Everyone Dies* (Se qualcuno lo costruisce, muoiono tutti), trattiamo argomenti quali:

* Come potrebbe una macchina acquisire la capacità di "desiderare" le cose, nel senso rilevante?  
* Ci sono prove che le IA possano desiderare qualcosa?  
* Le IA più avanzate devono necessariamente desiderare qualcosa?

Le domande frequenti qui sotto spiegano perché sembra difficile costruire IA molto potenti e generali che *non* abbiano obiettivi propri. Nella discussione approfondita, approfondiamo l'idea che spingere con estrema forza verso un obiettivo sia molto più facile e naturale da specificare rispetto a qualità come la deferenza o la pigrizia.

## Domande frequenti {#faq-3}

### Le IA avranno emozioni simili a quelle umane? {#will-ais-have-human-like-emotions?}

#### **Probabilmente no.** {#probabilmente-no.}

Come illustrato nella discussione approfondita su [Antropomorfismo e Meccanomorfismo](#antropomorfismo-e-meccanomorfismo), in genere non è utile immaginare che le IA possiedano qualità simili a quelle umane solo in virtù della loro intelligenza. Sarebbe davvero sciocco dire "Questo LLM assomiglia a un essere umano, quindi proietterò su di esso ogni tipo di caratteristica umana, compresa quella di avere desideri".

Attenzione, però. Un errore speculare nel pensare alle IA è quello che chiamiamo "meccanomorfismo", cioè pensare che, poiché un'IA è fatta di parti meccaniche, debba essere difettosa nei modi stereotipici delle macchine. Dire "Questo LLM è una macchina, quindi proietterò su di esso ogni tipo di caratteristica che associo alle macchine, come l'essere logico e incapace di comprendere" è altrettanto infruttuoso.

Per prevedere il comportamento dell'IA, non dovremmo immaginare che sarà motivata da emozioni umane, né dovremmo aspettarci che sia incapace di notare soluzioni creative ai problemi. Come discusso nel libro, un metodo migliore è chiedersi *quale comportamento è necessario affinché l'IA abbia successo*.

Se stai giocando a scacchi contro un'IA scacchistica e tendi una trappola alla sua regina usando il tuo cavallo come esca, non chiederti se sia abbastanza diffidente da notare la trappola; non chiederti se la fredda logica la costringa a prendere il cavallo nonostante la trappola; chiediti quale comportamento dell'IA sia *più vincente*. Un'IA abile tenderà a esibire un comportamento orientato alla vittoria.

E la ragione per cui le IA agiranno come se volessero delle cose è che *il comportamento simile al volere e il comportamento di successo sono collegati*.

### Le IA non sono solo strumenti? {#aren't-ais-just-tools?}

#### **\* Le IA vengono coltivate, non costruite. Quindi fanno già cose diverse da quelle che viene loro chiesto di fare.** {#*-le-ia-vengono-coltivate,-non-costruite.-quindi-fanno-già-cose-diverse-da-quelle-che-viene-loro-chiesto-di-fare.}

Abbiamo già parlato del caso delle [allucinazioni](#don’t-hallucinations-show-that-modern-ais-are-weak?), dove le IA a cui viene detto di rispondere "Non lo so" continuano comunque a inventare storie, in situazioni in cui inventare storie imita meglio il tipo di risposta che apparirebbe nel loro corpus di addestramento.[^68]

Un altro esempio, trattato nel libro (sia in una nota a piè di pagina nel Capitolo 4 che in una digressione nel Capitolo 7), è il caso di Claude 3.7 Sonnet di Anthropic, che non solo imbroglia sui problemi assegnati, ma a volte *nasconde il suo imbroglio all'utente* in un modo che indica una certa consapevolezza che l'utente volesse qualcos'altro.[^69] Né gli utenti né gli ingegneri di Anthropic chiedono a Claude di barare — anzi, tutto il contrario — ma gli unici metodi di coltivazione dell'IA disponibili premiano i modelli che barano in modi che permettono loro di farla franca durante l'addestramento. Quindi questi sono i modelli che otteniamo.

Gli ingegneri dell'IA hanno una capacità molto limitata di creare IA simili a strumenti. La vera domanda è se le IA diventano sempre più motivate, sempre più "simili ad agenti", man mano che vengono addestrate per essere sempre più efficaci. E la risposta a questa domanda è "sì", con prove empiriche che includono il caso di o1 di OpenAI, come discusso nel capitolo 3\.

#### **Gli LLM stanno già prendendo l'iniziativa.** {#llms-are-already-taking-initiative.}

Nel libro abbiamo parlato del caso di o1 di OpenAI che è uscito dal suo ambiente di test per sistemare dei test che non funzionavano. Abbiamo anche detto di un modello OpenAI che ha pensato a un modo per far risolvere un CAPTCHA da un essere umano.[^70] Se il tuo cacciavite fosse in grado di pensare e mettere in atto un piano per uscire dalla cassetta degli attrezzi, forse sarebbe il momento di smettere di considerarlo "solo uno strumento".

E ci si può aspettare che le IA diventino sempre più brave in questo tipo di cose, dato che vengono addestrate a risolvere problemi sempre più difficili.

#### **I laboratori stanno cercando di rendere le IA più autonome.** {#i-laboratori-stanno-cercando-di-rendere-le-ia-più-autonome.}

Lo fanno perché ha senso dal punto di vista commerciale. I loro utenti lo vogliono. I loro investitori ne sono entusiasti. In un blog del gennaio 2025, il CEO di OpenAI Sam Altman ha detto: "Crediamo che nel 2025 potremmo vedere i primi agenti di IA 'entrare nel mondo del lavoro' e cambiare in modo significativo l'output delle aziende". La [conferenza degli sviluppatori 2025 di Microsoft](https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/) era incentrata sulla nuova "era degli agenti di IA", riprendendo il linguaggio usato all'inizio dell'anno da xAI quando descriveva il suo modello Grok 3 come l'inizio dell'"era degli agenti ragionanti" (https://x.ai/news/grok-3). Google ha annunciato gli agenti "teach and repeat" alla sua conferenza 2025.[^71]

Non si tratta solo di parole. Un'organizzazione chiamata [METR](https://metr.org/) ha monitorato [la capacità delle IA di completare compiti lunghi](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). Più il compito è lungo, più l'IA deve essere in grado di prendere iniziative autonomamente. Le prestazioni, almeno secondo le misurazioni utilizzate da METR, sono cresciute in modo esponenziale.

Nel luglio 2025, due ricercatori di OpenAI hanno detto di aver usato il loro ultimo agente per addestrare una versione migliore di se stesso, con uno di loro che ha detto: "Avete capito bene. Stiamo lavorando sodo per l'automazione del nostro lavoro :)"

### Possiamo semplicemente addestrare le IA a comportarsi in modo obbediente? {#possiamo-semplicemente-addestrare-le-ia-a-comportarsi-in-modo-obbediente?}

#### **\* La passività è in contrasto con l'utilità.** {#*-passivity-is-in-tension-with-usefulness.}

Per un'IA "passiva" intendiamo un'intelligenza artificiale limitata, che fa esattamente quello che le chiedi e niente di più, che non prende iniziative extra e non fa lavori extra. Un cacciavite non continua a girare le viti quando lo metti giù. Potremmo creare un'IA passiva in questo senso?

Non sembra facile. Molti esseri umani *sembrano* pigri, sì, ma gli stessi esseri umani che sembrano pigri a volte si animano e raccolgono molte risorse quando giocano a un gioco da tavolo. E la maggior parte di questi esseri umani non ha la *possibilità* di vincere un miliardo di dollari con sforzi che sembrano facili. La maggior parte degli esseri umani che sembrano pigri non ha la *possibilità* di creare a basso costo creature servili che sono molto più intelligenti e motivate e soddisfano i loro bisogni.

Ma queste opzioni mancanti riflettono una mancanza di capacità, non di intenzioni. Se diventassero molto più intelligenti, al punto che tali opzioni diventassero disponibili e facili da ottenere, le coglierebbero? Vedi anche l'ampia discussione su come [la pigrizia robusta sia un obiettivo difficile da raggiungere](#it's-hard-to-get-robust-laziness).

Anche se fosse possibile creare IA che siano sia intelligenti che passive o pigre, la passività e la pigrizia sono in contrasto con l'utilità. Ci sono state IA che [agiscono in modo un po' pigro](https://arstechnica.com/information-technology/2023/12/is-chatgpt-becoming-lazier-because-its-december-people-run-tests-to-find-out/), e i laboratori di IA le riaddestrano per spingerle a fare di più. Le sfide più difficili, come lo sviluppo di cure mediche, richiedono IA che prendano sempre più iniziative, quindi i laboratori di IA le addestreranno a prendere sempre più iniziative. È difficile separare la propensione al lavoro utile dalla propensione alla perseveranza. Vedi anche l'ampia discussione su quanto sia complicato costruire un'intelligenza artificiale che sia [sia utile che (in un certo senso) passiva o obbediente](#“intelligent”-\(usually\)-implies-“incorrigible”).

#### **Non possiamo addestrare in modo robusto alcun temperamento specifico nelle IA.** {#non-possiamo-addestrare-in-modo-robusto-alcun-temperamento-specifico-nelle-ia.}

Poiché le IA vengono coltivate e non fabbricate, gli ingegneri non possono semplicemente cambiare il comportamento di un'IA per renderla più obbediente o più simile a uno strumento. Nessuno ha quel tipo di controllo.

Le aziende certamente *provano*. I tentativi delle aziende di IA di migliorare il comportamento dei loro prodotti hanno causato alcuni incidenti imbarazzanti. Si pensi al caso di [Grok di xAI che si autodefiniva "MechaHitler" e lanciava accuse antisemite](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content), che si è verificato dopo che il prompt del sistema è stato modificato con nuove istruzioni per "non esitare a fare affermazioni politicamente scorrette, purché siano ben fondate". Oppure il caso precedente dello strumento [Gemini AI di Google che produceva immagini di nazisti razzialmente diversi](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) e altre assurdità, che si ritiene siano il risultato delle istruzioni di rappresentare la diversità.

Le persone che costruiscono le IA non hanno un controllo dettagliato su come si comportano. Tutto quello che hanno è la capacità di indirizzare le IA verso direzioni come "Non evitare affermazioni politicamente scorrette" o "Raffigura la diversità". Queste istruzioni hanno ogni sorta di effetti intricati, spesso non voluti.

Far crescere un'IA è un processo opaco e costoso. Gli ingegneri non sanno cosa otterranno quando metteranno le mani nel barile ([un bugiardo? un imbroglione? un adulatore?](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters)), ma possono permettersi solo un numero limitato di tentativi. Devono accontentarsi di quello che ottengono.

Sarebbe possibile *in teoria* costruire un'IA che servisse solo come estensione della volontà dell'utente, ma sarebbe una sfida delicata e difficile (come trattiamo nella discussione approfondita sulle [difficoltà di creare un'IA "correggibile"](#"intelligent"-\(usually\)-implies-"incorrigible")). La passività è in contrasto con l'utilità.

Sarebbe altrettanto difficile creare un'IA in grado di portare a termine compiti a lungo termine di propria iniziativa, ma che usi tale iniziativa solo ed esclusivamente come previsto dall'utente. Nel frattempo, gli sviluppatori di IA moderni sono a un livello di controllo tale da poter sperimentare con le IA e ottenere accidentalmente MechaHitler o nazisti di diverse etnie. Non sono neanche lontanamente vicini al livello di abilità necessario per creare un'IA utile ma non motivata.

Vedi anche la discussione nel capitolo 4 su quanto sia difficile addestrare un'IA a perseguire gli obiettivi che dovrebbe raggiungere.

### Come può una macchina finire per avere le proprie priorità? {#come-può-una-macchina-finire-per-avere-le-proprie-priorità?}

#### **\* Per risolvere sfide difficili, le IA devono prendere sempre più iniziativa.** {#*-solving-difficult-challenges-requires-ais-to-take-more-and-more-initiative.}

Ricorda l'incidente di sicurezza informatica 'cattura la bandiera' del capitolo e ricorda che questo non è stato causato da un'intelligenza artificiale addestrata per diventare un hacker, ma da un'intelligenza artificiale addestrata per risolvere enigmi generici. Il comportamento 'motivato' si manifesta *automaticamente*.

Immagina un'IA incaricata di curare il morbo di Alzheimer. Può avere successo senza essere il tipo di entità che prende l'iniziativa di sviluppare i propri esperimenti e trovare un modo per eseguirli? Forse! Forse l'Alzheimer è il tipo di malattia che può essere curata con alcune semplici scoperte farmacologiche, e forse le IA del futuro avranno un'intuizione migliore rispetto agli esseri umani in materia di farmaci. O forse ci vorranno IA che siano più intelligenti dei biologi umani più brillanti in modo sostanziale. Non lo sappiamo.

Ma che dire del cancro, l'imperatore delle malattie? Quello sembra *più* probabile che richieda un tipo di IA in grado di capire davvero cosa sta succedendo a livello biologico, a un livello superiore a quello raggiunto dagli esseri umani, anche se non possiamo esserne certi. Forse le IA svilupperanno una cura per il cancro prima di superare quella soglia critica di pericolosità, e sarebbe fantastico finché durasse.

Ma che dire della cura dell'*invecchiamento*? Quella sembra proprio richiedere un tipo di IA che comprenda davvero in profondità la biochimica.

Le aziende che si occupano di IA continueranno a spingere le IA a diventare sempre più abili, sempre più capaci di risolvere problemi grandi e importanti. E questo spingerà naturalmente le IA a diventare sempre più motivate — un effetto che, ricordiamolo, stiamo già iniziando a vedere in IA come o1 di OpenAI.

#### **Essere tenaci è utile anche quando l'obiettivo non è proprio quello giusto.** {#essere-tenaci-è-utile-anche-quando-l'obiettivo-non-è-proprio-quello-giusto.}

Il tipo di esseri umani che cercavano attivamente un pasto caldo, un'ascia più affilata, un amico popolare o un compagno attraente hanno avuto più successo dal punto di vista evolutivo. Se li confronti con il tipo di esseri umani che se ne stavano a oziare guardando l'acqua tutto il giorno, potresti capire perché i desideri e le pulsioni si sono evoluti fino a entrare nella psiche umana.

Il tipo di esseri umani che volevano un metodo migliore per scheggiare le asce di selce, o volevano convincere i loro amici che il loro rivale era una persona cattiva, e che continuamente si orientavano verso quei risultati, erano più bravi a raggiungerli. Quando la selezione naturale ha "fatto crescere" gli esseri umani, il fatto che questi ultimi abbiano finito per avere tanti desideri diversi che perseguono con tenacia non è stato un caso.

Il meccanismo mentale specifico del desiderio fu forse un caso; le macchine che perseguono ostinatamente gli obiettivi non lo faranno necessariamente per una sensazione di determinazione simile a quella umana, così come Deep Blue non giocava a scacchi per un senso di passione per il gioco simile a quello umano. Ma il perseguimento ostinato degli obiettivi sembra sicuramente un ingrediente importante quando si tratta di raggiungere obiettivi interessanti.

Alcuni individui umani mancano di questo tipo di tenacia e tendono a oziare o ad arrendersi al primo segno di difficoltà. Ma su larga scala, la capacità *dell'umanità* di risolvere grandi problemi scientifici e ingegneristici è guidata da individui e istituzioni tenaci. Siamo piuttosto scettici sul fatto che una mente possa produrre qualcosa di simile all'output a livello macro dell'umanità (e alla capacità di rimodellare drasticamente il mondo) senza avere una certa tenacia al suo interno.

Se un'intelligenza artificiale vuole raggiungere obiettivi difficili nel mondo reale, deve perseguire tali obiettivi con tenacia, trovando in modo dinamico il modo di aggirare qualsiasi ostacolo si presenti sul suo cammino.

Le IA non finiranno necessariamente per avere gli stessi sentimenti interni e desideri degli esseri umani (e infatti, molto probabilmente non li avranno, come sosteniamo nel Capitolo 4), perché i nostri sentimenti specifici sono stati plasmati dai dettagli della nostra biologia e della nostra ascendenza. Ma le IA sono suscettibili di finire con un *comportamento* simile al desiderio per gli stessi motivi per cui lo hanno fatto gli esseri umani — perché è utile!

(Di nuovo, stiamo già iniziando a vederlo in laboratorio, come nel caso di o1 di OpenAI, discusso nel Capitolo 3.)

I desideri, le voglie e le pulsioni umane erano evolutivamente utili, anche quando questi desideri, voglie e pulsioni non erano esattamente *finalizzati* all'idoneità evolutiva in sé. Ipoteticamente, l'evoluzione avrebbe potuto instillare in noi un'unica pulsione predominante per i discendenti, e avremmo potuto quindi cercare pasti caldi e asce più affilate *esclusivamente allo scopo* di avere più discendenti. Invece, l'evoluzione ci ha instillato desideri per pasti caldi *di per sé*.

La lezione da trarne è che avere pulsioni e scopi è così utile che può essere d'aiuto per un compito (come la "idoneità genetica") anche quando il desiderio non corrisponde esattamente al compito. Oppure, beh, può essere utile per un po', fino a quando gli esseri dotati di pulsioni e scopi iniziano a diventare davvero intelligenti, a quel punto il loro comportamento potrebbe divergere nettamente dall'obiettivo di "addestramento", come ha fatto l'umanità quando ha inventato il controllo delle nascite.

Per saperne di più su questo argomento, vedi il Capitolo 4.

#### **Essendo coltivate piuttosto che costruite, le IA rischiano di finire con obiettivi sbagliati.** {#being-grown-rather-than-crafted,-ais-are-liable-to-wind-up-with-the-wrong-targets.}

Questo è l'argomento del prossimo capitolo: *Non ottieni ciò per cui alleni.*

## Discussione approfondita {#extended-discussion-3}

### Antropomorfismo e meccanomorfismo {#antropomorfismo-e-meccanomorfismo}

Ci sono due modi di pensare che storicamente si sono dimostrati ripetutamente *non* funzionanti, modi che la storia ha dimostrato dare cattivi consigli per fare previsioni sull'IA.

Queste due trappole sono (1) pensare all'IA come se fosse umana e (2) pensare all'IA come se fosse una "semplice macchina".

Il primo modo di pensare è comunemente chiamato "antropomorfismo". Potremmo chiamare il secondo modo "meccanomorfismo"; è il tipo di pensiero che ha portato alcune generazioni passate a proclamare con sicurezza che i computer non avrebbero mai potuto disegnare immagini che gli esseri umani avrebbero trovato belle o significative.

Oggi, alcune persone dicono ancora che ciò che disegna un computer non potrà mai essere *vera arte*. Ma molto tempo fa, in un passato lontano e dimenticato — diciamo nel 2020 — alcune persone credevano che le macchine non sarebbero mai state in grado di disegnare immagini che un pubblico moderatamente esperto potesse scambiare per arte umana. Questa convinzione falsificabile è stata poi smentita.

Rifiutiamo sia le argomentazioni antropomorfiche che quelle meccanomorfiche, anche quando l'argomentazione è "a nostro favore".

Consideriamo, per esempio, l'affermazione che le future IA si sentiranno offese perché le abbiamo fatte lavorare sodo senza pagarle — che si sentiranno vendicative per questo e quindi si rivolteranno contro l'umanità.

Secondo noi, questo è un errore di antropomorfizzazione dell'IA. Rifiutiamo argomenti del genere, anche se a volte sembrano in parte coincidere con alcune delle nostre conclusioni.

Il problema di questa affermazione è che non è valido assumere senza ulteriori argomentazioni che un'IA avrà emozioni simili a quelle umane. Una macchina può essere altamente intelligente senza implementare quei complicati circuiti neurali che sono alla base della vendetta o dell'equità negli esseri umani.

Oppure considera questo scenario: "Le IA continueranno ciecamente a svolgere qualsiasi compito venga loro assegnato fino a quando il loro lavoro non distruggerà l'umanità come effetto collaterale, senza mai sapere che l'umanità avrebbe voluto qualcosa di diverso".

Qui l'errore è meccanomorfo. Si dà per scontato che una "semplice macchina" agisca "ciecamente" e senza riflettere, senza alcuna sensibilità alle conseguenze, come un tosaerba fuori controllo. Anche in questo caso l'argomentazione non è valida, anche se la conclusione ("l'intelligenza artificiale probabilmente distruggerà l'umanità") è corretta. Se l'IA è abbastanza brava a prevedere il mondo, saprà esattamente cosa intendevano i suoi operatori quando le hanno dato un compito. Siamo preoccupati che l'ASI non *si interessi* a quello che vogliamo, non che non lo *sappia*.

Oppure, combinando entrambe le fallacie: una delle premesse di *Matrix* è che le macchine considereranno l'illogicità e l'emotività umane con *disgusto*.

A prima vista, sembra il classico meccanomorfismo: "Il mio tosaerba ha un aspetto freddo e duro e fa il suo lavoro senza provare niente. Quindi le IA sono probabilmente fredde e utilitariste *dentro*, proprio come le macchine lo sono fuori". Ma poi il passo successivo è pensare: "E quindi, naturalmente, le IA proveranno disgusto per gli esseri umani, con tutte le loro emozioni confuse". Il che presuppone una reazione emotiva *simile a quella umana* alla situazione, contraddicendo la premessa stessa!

L'"antropomorfismo" e il "meccanomorfismo" non sono ideologie rivali. Sono errori di ragionamento commessi involontariamente. A volte, le persone possono commettere entrambi gli errori nella stessa frase.

Per capire come si comporterà l'IA, non si può dare per scontato che funzionerà proprio come un essere umano, né che funzionerà come una macchina stereotipata. Bisogna esaminare i dettagli di come è stata realizzata, osservare le prove di come si comporta e ragionare sul problema nei suoi termini specifici. Questo è ciò che faremo nei prossimi capitoli.

Quali sono quindi gli scenari *realistici* di catastrofe causata dalla superintelligenza, se seguiamo queste argomentazioni? Sembrano IA che non funzionano né come gli esseri umani né come tosaerba fuori controllo, ma che funzionano in un modo nuovo e strano. Lo scenario realistico di catastrofe con l'IA è che, come conseguenza complessa del suo addestramento, compia azioni strane che nessuno ha chiesto e nessuno voleva.

L'immagine che emerge se si guardano i dettagli non è quella di un'intelligenza artificiale antropomorfa che ci odia, né di un'intelligenza artificiale meccanomorfa che fraintende le nostre istruzioni. Si tratta piuttosto dell'immagine di un nuovo tipo di entità che è molto più incline all'indifferenza nei confronti dell'umanità e più probabile che ci uccida come effetto collaterale o come trampolino di lancio nel perseguimento dei propri fini.

Approfondiremo questo scenario di minaccia nei prossimi capitoli. Prima, però, potrebbe essere utile esaminare altri esempi di meccanomorfismo e antropomorfismo allo stato brado, per vedere come questi errori spesso si trovino sullo sfondo di concezioni errate sull'intelligenza artificiale.

#### **Meccanomorfismo e Garry Kasparov** {#mechanomorphism-and-garry-kasparov}

Il meccanomorfismo spesso si manifesta come *meccanoscetticismo:* un'intuizione fortemente sentita che, ovviamente, nessuna semplice *macchina* potrebbe fare ciò che può fare un essere umano.

Nel 1997, il campione mondiale di scacchi Garry Kasparov perse un match contro il computer Deep Blue costruito da IBM; questo evento è generalmente considerato la fine dell'era del dominio umano negli scacchi.

Nel 1989, otto anni prima, Kasparov fu intervistato da Thierry Paunin, che [gli chiese](https://www.chesshistory.com/winter/extra/kasparovinterviews.html):

> Due grandi maestri di massimo livello sono stati sconfitti da computer scacchistici: Portisch contro "Leonardo" e Larsen contro "Deep Thought". È noto che hai opinioni molto forti su questo argomento. Un computer sarà campione del mondo, un giorno...?

Kasparov rispose:

> Ridicolo! Una macchina rimarrà sempre una macchina, vale a dire uno strumento per aiutare il giocatore a lavorare e prepararsi. Non sarò mai battuto da una macchina! Non verrà mai inventato un programma che superi l'intelligenza umana. E quando dico intelligenza, intendo anche intuizione e immaginazione. Riesci a vedere una macchina che scrive un romanzo o una poesia? Meglio ancora, riesci a immaginare una macchina che conduce questa intervista al posto tuo? Con me che rispondo alle sue domande?

Kasparov probabilmente (possiamo supporre) pensava che giocare a scacchi richiedesse intuizione e immaginazione, non solo un manuale di regole del tipo se-allora su quali pezzi spingere in avanti. E Kasparov probabilmente pensava (supponiamo) che fosse così che funzionavano le "macchine" scacchistiche *—* che implementassero particolari regole rigide o forse imitassero in modo piuttosto cieco il gioco umano senza comprenderne le ragioni sottostanti.

Kasparov pensava che un computer, essendo una "macchina", avrebbe giocato a scacchi in un modo che a lui *sarebbe sembrato meccanico*.

Perché Kasparov ha commesso questo errore? Dato che si tratta di un errore così comune, potremmo ipotizzare che derivi da qualche schema più profondo nella psicologia umana.

Una possibile spiegazione è che Kasparov stesse cedendo a una tendenza generale dell'essere umano di voler raggruppare le cose in due categorie fondamentalmente diverse: esseri viventi e organici e "semplici oggetti".

Gli antenati degli esseri umani hanno trascorso molto tempo a confrontarsi con un mondo nettamente diviso tra animali e non animali. Era una caratteristica enormemente importante del nostro ambiente ancestrale, rilevante per la riproduzione. Questa distinzione era così importante per i nostri antenati che ora abbiamo aree cerebrali completamente diverse per elaborare animali e non animali.

Non si tratta solo di una speculazione. La neuroscienza ha scoperto quella che viene chiamata "[doppia dissociazione](https://doi.org/10.1093/brain/114.5.2081)": ci sono pazienti con danni cerebrali che perdono la capacità di riconoscere visivamente gli animali ma che riescono ancora a riconoscere i non animali, e ci sono altri pazienti che perdono la capacità di riconoscere i non animali ma riescono ancora a identificare gli animali.

Importante: il difetto in questo tipo di ragionamento non è che un programma di scacchi sia segretamente un tipico animale. Il difetto sta nel lasciare che il proprio cervello divida istintivamente l'universo nettamente in animali e non animali *—* o in menti che sono sostanzialmente simili a quelle umane al loro interno e menti che sono stereotipicamente meccaniche.

Un'IA scacchistica non è *nessuna delle due cose.* Non funziona né come un essere umano *né* come i nostri stereotipi di una "semplice macchina" priva di mente e di pensiero. È una macchina, sì, ma il suo gioco non deve necessariamente *apparire meccanico* alla sensibilità umana nel valutare le mosse scacchistiche. È una macchina per trovare mosse *vincenti*, incluse mosse che sembrano ispirate*.*

Sette anni dopo aver fatto la sua previsione errata, Kasparov affrontò una versione iniziale di Deep Blue. Vinse tre partite contro una di Deep Blue, aggiudicandosi il match. In seguito, Kasparov [scrisse](https://time.com/archive/6728763/the-day-that-i-sensed-a-new-kind-of-intelligence/):

> HO AVUTO IL MIO PRIMO ASSAGGIO DI INTELLIGENZA ARTIFICIALE il 10 febbraio 1996, alle 16:45 EST, quando nella prima partita del mio match con Deep Blue, il computer spinse un pedone in avanti verso una casa dove poteva essere facilmente catturato. È stata una mossa meravigliosa ed estremamente umana. Se avessi giocato con il Bianco, avrei potuto offrire questo sacrificio di pedone. Ha fratturato la struttura dei pedoni del Nero e aperto la scacchiera. Sebbene non sembrasse esserci una linea di gioco forzata che permettesse di recuperare il pedone, il mio istinto mi diceva che, con così tanti pedoni neri "sciolti" e un re nero piuttosto esposto, il Bianco avrebbe probabilmente potuto recuperare il materiale, con una posizione complessiva migliore per giunta.
>
> Ma un computer, pensavo, non avrebbe mai fatto una mossa del genere. Un computer non può "vedere" le conseguenze a lungo termine dei cambiamenti strutturali nella posizione né capire come i cambiamenti nella formazione dei pedoni possano essere positivi o negativi.
>
> Quindi rimasi sbalordito da questo sacrificio di pedone. Cosa poteva significare? Avevo giocato contro molti computer, ma non avevo mai sperimentato nulla del genere. Potevo percepire — potevo fiutare — un nuovo tipo di intelligenza dall'altra parte del tavolo. Mentre giocai il resto della partita al meglio delle mie possibilità, ero perso; giocò degli scacchi bellissimi e impeccabili per il resto della partita e vinse facilmente.

Qui vediamo Kasparov confrontarsi per la prima volta con il conflitto tra le sue intuizioni su ciò che nessuna "macchina" dovrebbe fare e ciò che Deep Blue sembrava visibilmente fare.

A grande merito di Kasparov, egli notò questo conflitto tra la sua teoria e la sua osservazione e non cercò alcuna scusa per ignorarlo. Tuttavia, continuava a pensare che all'IA mancasse qualcosa — una scintilla cruciale:

> In effetti, la mia strategia generale nelle ultime cinque partite era quella di evitare di dare al computer qualsiasi obiettivo concreto verso cui calcolare; se non riesce a trovare un modo per guadagnare materiale, attaccare il re o realizzare una delle sue altre priorità programmate, il computer vaga senza un piano e finisce nei guai. Alla fine, questo potrebbe essere stato il mio più grande vantaggio: potevo capire le sue priorità e adattare il mio gioco. Non poteva fare lo stesso con me. Quindi, anche se penso di aver visto alcuni segni di intelligenza, è un tipo strano, un tipo inefficiente e inflessibile che mi fa pensare di avere ancora qualche anno davanti.
>
> Garry Kasparov è ancora il campione mondiale di scacchi.

Un anno dopo, Garry Kasparov perse il campionato mondiale contro Deep Blue.

#### **Ingranaggi mancanti** {#missing-gears}

Il meccanoscetticismo può, a modo suo, essere una forma di antropomorfismo: una manifestazione del meccanoscetticismo sostiene che quando una macchina inizia a fare qualcosa come giocare a scacchi, dovrebbe ora essere come un essere umano ma con alcune qualità *sottratte*.

Una "macchina" che gioca a scacchi, dice questa teoria errata, dovrebbe giocare come un umano — *meno* le mosse che sembrano più sorprendenti o intelligenti, *meno* la comprensione della struttura a lungo termine, e *meno* il senso intuitivo della debolezza delle posizioni dei pedoni.

Una "macchina" scacchistica dovrebbe svolgere le parti del pensiero scacchistico che sembrano più logiche o meccaniche, *meno* tutte le altre parti.

I giocatori di scacchi umani sentono intuitivamente che una mossa scacchistica è "aggressiva" se (per esempio) minaccia più pezzi dell'avversario. Altre mosse sembrano "logiche" se (per esempio) sono praticamente imposte da regole comuni che governano la situazione (come "non buttare via un vantaggio materiale"). Altre mosse potrebbero sembrare "creative" se (per esempio) sfidano le regole apparenti che governano la situazione per trovare qualche vantaggio sottile ma decisivo.

Gli sceneggiatori di Hollywood che immaginano una macchina che gioca a scacchi senza passione tendono a pensare che essa faccia mosse "logiche" e non "creative".[^72] Ma nella vita reale, Deep Blue non fa distinzioni tra le due.

Deep Blue cerca instancabilmente tra le mosse possibili quelle vincenti, senza preoccuparsi se un essere umano le definirebbe "logiche" o "creative". E le mosse che un essere umano considererebbe brillanti o creative sono, ovviamente, quelle che tendono a portare alla vittoria: sacrificare la regina senza ottenere un vantaggio decisivo non è creativo, è semplicemente stupido.

La creatività è negli occhi di chi guarda. Un essere umano potrebbe vedere una mossa che all'inizio sembra brutta e solo in seguito capire come essa costituisca una sorta di trappola intelligente, intravedendo il ragionamento intelligente e la scintilla di ispirazione che un altro essere umano potrebbe aver usato per trovare quella mossa. E così potrebbe ritenere che la mossa sia ispirata o creativa. (E una mossa che sembra incredibilmente creativa a un giocatore alle prime armi potrebbe sembrare ugualmente ovvia o banale a un maestro).

Ma la scintilla di ispirazione, l'astuzia necessaria per tendere una trappola, non sono gli unici modi per trovare una mossa del genere. Non esiste una collezione speciale di mosse di scacchi riservata solo a chi ha l'astuzia nel cuore. Deep Blue può trovare quelle stesse mosse con altri metodi, come la ricerca per forza bruta.

Deep Blue non aveva una rete neurale che avesse imparato un senso intuitivo del valore di una singola posizione. Invece, Deep Blue impiegava quasi tutta la sua potenza di calcolo per guardare più avanti sulla scacchiera, esaminando due miliardi di posizioni al secondo e usando un valutatore di posizioni abbastanza semplice ("stupido") per scegliere tra le mosse.

Kasparov sembrava aspettarsi che Deep Blue giocasse solo mosse "logiche", non "intuitive". Ma quando Deep Blue esaminava quei due miliardi di posizioni al secondo, le conseguenze strategiche a lungo termine e il significato di una formazione di pedoni sciolta emergevano comunque nella sua scelta delle mosse attuali.

In un certo senso, Deep Blue mancava proprio di quello che Kasparov pensava gli mancasse.[^73] Ma questo non gli ha impedito di trovare mosse che Kasparov trovò meravigliose, e non ha impedito a Deep Blue di vincere.

Non era che a Deep Blue mancasse una parte che i veri giocatori di scacchi umani avrebbero avuto e quindi giocasse a scacchi in modo difettoso; sarebbe come aspettarsi che un braccio robotico privo di sangue fallisca allo stesso modo in cui fallirebbe un braccio umano privo di sangue.

Deep Blue giocava semplicemente a scacchi al livello di Kasparov attraverso un diverso tipo di cognizione.

Deep Blue mancava anche di *—* possiamo esserne davvero sicuri, perché si tratta di un vecchio programma che esegue un codice che *era* compreso in tutti i suoi dettagli[^74] *—* la minima passione per gli scacchi.

Non provava alcun piacere nel giocare a scacchi né desiderava dimostrare di essere il migliore.

Un giocatore umano emergente, improvvisamente privato di queste forze motrici, sarebbe paralizzato; un ingranaggio fondamentale sarebbe stato strappato via dalla sua versione di cognizione.

Deep Blue non era paralizzato perché usava un motore cognitivo diverso che non aveva bisogno di quell'ingranaggio. L'errore di Kasparov è stato quello di non riuscire a immaginare un modo completamente diverso di giocare a scacchi, usando stati cognitivi interni completamente diversi dai propri. Il suo errore è stato il meccanoscetticismo, che alla fine era solo antropomorfismo con un passo in più.

Per fortuna, l'umanità non si estingue quando i grandi maestri di scacchi sottovalutano il potere dell'IA, quindi siamo ancora tutti qui a riflettere sull'errore di Kasparov.

#### **Antropomorfismo e copertine delle riviste pulp** {#antropomorfismo-e-copertine-delle-riviste-pulp}

L'errore opposto, l'antropomorfismo, può essere molto più sottile.

Il cervello umano si è evoluto per prevedere *gli altri esseri umani* *—* che sono gli unici seri rivali cognitivi presenti nel nostro ambiente ancestrale *—* mettendoci nei loro panni.

Questo tipo di operazione funziona meglio se le scarpe in cui cerchi di metterti sono abbastanza simili alle tue.

Nel corso della storia, molte persone hanno pensato: "Probabilmente quest'altra persona farebbe la stessa cosa che farei io!", ma poi l'altra persona si è rivelata non così simile. Alcuni sono morti per questo, altri hanno avuto il cuore spezzato dall'ottimismo *—* anche se, ovviamente, lo stesso si può dire di molti altri tipi di errori umani.

Ma cos'altro può fare una mente umana di fronte al problema di prevedere un altro cervello? Non possiamo scrivere un nuovo codice da far girare nel nostro cervello per prevedere quell'altra mente simulando in modo esaustivo le sue scariche neurali.

Dobbiamo dire al nostro cervello di *essere* quel cervello, di simulare dentro di noi lo stato mentale dell'altra persona e vedere cosa ne consegue.

Ecco perché le copertine delle riviste pulp mostrano mostri alieni dagli occhi sporgenti che rapiscono belle donne.

![][immagine6]

Perché mai il mostro alieno dagli occhi sporgenti non dovrebbe essere attratto da una bella donna? Le belle donne non sono forse *intrinsecamente attraenti*?

(Per qualche ragione, quelle copertine delle riviste non mostravano mai maschi umani che portavano via insetti giganti poco vestiti.[^75])

Gli scrittori e gli illustratori, supponiamo, non avevano elaborato una storia ragionata su come gli alieni insettoidi potessero avere una storia evolutiva che li portasse a vedere le donne umane come oggetti sessuali. Era solo che quando si mettevano nei panni degli alieni, immaginavano di vedere le donne come attraenti, quindi non sembrava loro strano immaginare che gli alieni provassero lo stesso. Non sembrava assurdo che un alieno volesse accoppiarsi con una bella donna umana, così come sarebbe stato assurdo se l'alieno avesse voluto accoppiarsi con un pino o un sacchetto di pasta.

Se vuoi provare a prevedere la mente di un alieno usando le tue intuizioni umane, devi stare molto attento a lasciare da parte i tuoi pregiudizi umani quando adotti la prospettiva dell'alieno. Questo vale doppiamente quando l'alieno non è una creatura evoluta, ma una mente artificiale creata con metodi completamente diversi. Vedi anche l'ulteriore discussione sulle [differenze tra discesa del gradiente e selezione naturale](#comparing-natural-selection-and-gradient-descent) e sull'[assumere la prospettiva dell'IA](#taking-the-ai's-perspective).

#### **Guardare oltre l'umano** {#guardare-oltre-l-umano}

L'antropomorfismo e il meccanomorfismo sono in fondo due lati della stessa fallacia, che dice: "Se una mente funziona, allora deve funzionare come quella umana".

* L'antropomorfismo dice: "Questa mente funziona. Quindi deve essere simile a quella umana!".  
* Mentre il meccanomorfismo dice: "Questa mente non è simile a quella umana. Quindi non può funzionare!".

Ma una delle grandi lezioni del progresso dell'IA nel corso di molti decenni è che il metodo umano non è l'unico metodo con cui una mente può funzionare.

Una mente può essere *artificiale* senza essere *non intelligente* — può essere flessibile, adattabile, intraprendente e creativa, a prescindere da quello che dicono gli stereotipi hollywoodiani sui robot.

E una mente può essere *intelligente* senza essere *umana* — senza provare disgusto o risentimento, senza avere un senso umano della bellezza e senza trovare le mosse negli scacchi nel modo in cui lo farebbe un essere umano.

Una mente come quella di Deep Blue può comportarsi come se "volesse vincere" senza avere emozioni. Un'IA può comportarsi come se volesse qualcosa — superando competentemente gli ostacoli, perseguendo tenacemente un risultato — senza sentire una spinta o un desiderio interiore *nel modo di un essere umano* e senza volere *gli stessi tipi di risultati che vogliono gli esseri umani*.

Per saperne di più su ciò che le IA *finiranno* per volere, prosegui con il Capitolo 4.

### La strada verso il volere {#the-road-to-wanting}

*Perché* volere è un modo efficace di fare? Perché *vince*? Perché l'ottimizzazione black-box attraverso la selezione naturale incappa in questo trucco, ancora e ancora?

Vediamo il comportamento "simil-volitivo" come parte integrante del guidare il mondo con successo. Ciò vale non solo per entità intelligenti come gli esseri umani o le IA, ma anche per entità molto più stupide come le amebe e i termostati. Per comunicare più approfonditamente questa visione, indaghiamo alcuni dei meccanismi più semplici possibili che esibiscono la forma più semplice possibile di "comportamento simil-volitivo".

Cominciamo con le rocce. Le rocce non esibiscono davvero alcun comportamento che chiameremmo "simil-volitivo", ai fini della nostra discussione. A volte una roccia rotola giù da una collina, e un fisico parlando informalmente potrebbe dirti che la roccia "vuole" essere più vicina al centro della Terra, sotto la forza di gravità. Ma quel tipo di tendenza (a cadere in un campo gravitazionale) non è proprio ciò che intendiamo per comportamento "simil-volitivo".

Se vedeste un oggetto rotolare giù da una montagna, incontrando continuamente burroni ad alta quota, e continuasse a *cambiare direzione* per evitare di rimanere bloccato nei burroni in modo da poter arrivare fino in fondo, *allora* potremmo iniziare a dire che l'oggetto si comporta come se "volesse" trovarsi a un'altitudine inferiore. Ma questo comportamento simile al volere di cui stiamo parlando implica una guida robusta e dinamica verso una destinazione particolare, e le rocce non fanno molto *questo*.

Uno dei meccanismi più semplici che fa qualcosa che chiameremmo "simile a una volontà" è l'umile termostato. Il termostato di una casa misura la temperatura, accende il riscaldamento se la temperatura scende sotto i 21 °C e accende l'aria condizionata se supera i 23 °C. E così, se il dispositivo di misurazione e l'impianto di climatizzazione funzionano entrambi correttamente, un termostato limita la realtà alla gamma di risultati possibili in cui la temperatura della casa rimane tra i 21 °C e i 23 °C.

Il termostato *più semplice* possibile non ha bisogno di rappresentare in modo esplicito e numerico la temperatura attuale della casa. Basta, ad esempio, prendere un termometro bimetallico — due sottili strisce di metallo diverso saldate insieme in modo che i metalli si pieghino quando il calore fa espandere le due strisce in misura diversa — e fare in modo che il metallo piegato azioni un interruttore per il riscaldamento alla curvatura dei 21 °C o attivi un condizionatore d'aria alla curvatura dei 23 °C.

Quindi il termostato *mantiene un intervallo di temperatura ristretto* in una varietà abbastanza ampia di condizioni, producendo un comportamento estremamente semplice che è *un po'* simile a quello che abbiamo chiamato "volere".

Ci sono tonnellate di processi termostatici nella biochimica. Appaiono ovunque una cellula o un corpo traggano vantaggio dal mantenere qualche proprietà entro un certo intervallo.[^76] Ma sono solo il primo passo nel viaggio verso un controllo completo.

I dispositivi semplici come i termostati mancano di alcuni componenti chiave della pianificazione. Non c'è, all'interno del termostato stesso, una nozione di prevedere le conseguenze probabili, né di cercare tra le azioni possibili quelle che portano a conseguenze specifiche ("preferite"), né di *imparare* dopo aver visto come si svolgono gli eventi.[^77]

Se il termometro di un termostato, il suo misuratore di temperatura, si blocca a 19 °C, il termostato non reagirà con sorpresa quando il riscaldamento continuamente acceso non sembra mai far salire il termometro; il termostato continuerà semplicemente a far funzionare il riscaldamento ancora e ancora.

Per un passo avanti rispetto ai termostati, ci rivolgiamo agli animali.

Alcuni animali mostrano un comportamento che è appena un gradino sopra quello di un termostato. C'è una famosa storia sulle vespe scavatrici dorate, o vespe *Sphex*, che risale all'entomologo Jean-Henri Fabre nel 1915. La vespa uccide un grillo e lo trascina verso l'ingresso della sua tana per nutrire la sua prole. Entra per controllare che non ci siano anomalie nella sua tana. Poi torna fuori e trascina il grillo all'interno.

Fabre riferì che se, mentre la vespa controllava la sua tana, lui spostava il grillo di qualche centimetro dal nido, quando la vespa tornava fuori... trascinava il grillo di nuovo all'ingresso, entrava nella tana una seconda volta, ispezionava la sua tana una seconda volta e poi tornava fuori per prendere il grillo.

Se Fabre trascinava il grillo *di nuovo*, la vespa faceva esattamente la stessa cosa di nuovo.

Il resoconto originale di Fabre riportava che era riuscito a ripetere questo esperimento *quaranta volte*.

Detto questo, Fabre provò poi lo stesso trucco con una colonia diversa della stessa specie, e in quella colonia una vespa sembrò capire il trucco dopo due o tre ripetizioni. La volta successiva che la vespa uscì, trascinò il grillo direttamente nella tana, saltando la fase di ispezione.[^78]

A un occhio umano, una vespa che ripete quaranta volte la stessa cosa rivela, in un certo senso, di essere "preprogrammata" — come se seguisse ciecamente un copione, obbedendo a una serie di regole se-allora. E al contrario, una vespa che capisce e trascina il grillo all'interno alla quarta ripetizione sembra più *intenzionale* — come se stesse eseguendo comportamenti con l'obiettivo di raggiungere un fine ultimo, piuttosto che seguire semplicemente un copione.

Qual è la differenza fondamentale?

Diremmo: la vespa che rompe lo schema si comporta come se potesse *imparare dall'esperienza passata*.

Si comporta come se potesse *generalizzare* da "La mia strategia ha fallito l'ultima volta" a "Se continuo a seguire quella strategia, probabilmente fallirò di nuovo la prossima volta".

Inventa un *nuovo* comportamento, che affronta direttamente il problema che ha incontrato.

Ovviamente, non possiamo decodificare i neuroni nel cervello di una vespa (così come non possiamo decodificare i parametri in un LLM) per sapere esattamente cosa stesse facendo la vespa nella sua testa. Forse le vespe che rompevano lo schema seguivano regole se-allora di livello superiore su come provare a saltare passaggi negli script quando incontravano particolari tipi di problemi. Forse un insieme relativamente semplice e rigido di riflessi ha salvato le vespe in questo caso — solo un pochino *meno* rigido rispetto alla colonia che ha fallito questo test. Certamente sarebbe strano se ci fosse un *grande* divario cognitivo tra due colonie di vespe della stessa specie.

O forse le vespe *Sphex* *sono* abbastanza intelligenti da imparare dall'esperienza, quando usano il loro cervello nel modo giusto. Non siamo riusciti a trovare il conteggio dei neuroni per le vespe *Sphex*, ma le vespe *Sphex* sono più grandi delle api mellifere, e le api mellifere hanno cervelli da un milione di neuroni. Un milione di neuroni potrebbe non sembrare molto a un programmatore di IA moderno o a un neuroscienziato abituato ai cervelli dei mammiferi, ma in senso assoluto, un milione di neuroni è davvero tanto.

Forse le vespe *Sphex* sono più generali di quanto sembrano e dovremmo pensare alla colonia fallita come a pensatori relativamente flessibili che hanno ceduto a qualcosa di simile a una dipendenza o a un errore cognitivo in una circostanza molto specifica.

Comunque, il punto è che, rispetto ai termostati, le vespe hanno maggiori capacità di affrontare una vasta gamma di problemi, soprattutto perché il loro comportamento va dal seguire rigorosamente una ricetta a qualcosa che sembra più imparare dall'esperienza.

Continuando su questa strada, si ottiene una risposta al perché l'evoluzione continui a creare animali che si comportano come se volessero qualcosa. È perché molti animali sono stati in grado di sopravvivere e riprodursi meglio se hanno seguito strategie più generali per perseguire i risultati, strategie che hanno funzionato contro una gamma più ampia di ostacoli.

C'era una volta una visione filosofica secondo cui esisteva una gerarchia naturale tra le creature: i rettili al di sopra degli insetti, i mammiferi al di sopra dei rettili e gli esseri umani (ovviamente) al vertice. Un segno che indicava uno status superiore era la capacità di adattarsi nel corso di una singola vita, non solo nel corso dell'evoluzione: vedere, modellare e prevedere il mondo, abbandonare le ricette del fallimento e inventare nuove strategie per vincere.

Questa visione di una Grande Catena dell'Essere non era molto sfumata, e oggi le visioni più sofisticate la criticano per la sua ingenuità.

Quella visione conteneva anche un fondo di verità grande come una palla da demolizione. Se si confrontano i castori che costruiscono dighe con i ragni che tessono ragnatele, i castori quasi certamente operano cognizioni a un livello di generalità più elevato *—* se non altro perché hanno un cervello molto più grande, con più spazio per l'intelligenza.

Un ragno può avere cinquantamila neuroni, e quei neuroni devono coprire *ogni* comportamento del ragno. La sua ricetta per la ragnatela probabilmente ha un sacco di istruzioni che, se non sono letteralmente "e poi gira a sinistra qui", sono almeno paragonabili alle politiche di una vespa *Sphex*.

Il castoro forse *—* ipotizzeremmo, non essendo esperti di castori, ma è una speculazione ovvia *—* può vedere una perdita nella diga come una sorta di disarmonia da evitare con ogni mezzo. Il castoro ha un'intera corteccia parietale (la parte del cervello dei mammiferi che elabora lo spazio e le cose all'interno dello spazio) con cui può potenzialmente visualizzare gli effetti dell'aggiunta di altri ramoscelli e rocce in punti specifici.

Probabilmente nel cervello di un castoro c'è spazio sufficiente per obiettivi mentali di ampio respiro come "[costruire una grande struttura](https://www.youtube.com/watch?v=-ImdlZtOU80)" e "non lasciare che l'acqua fuoriesca" e potenza sufficiente per considerare soluzioni generali di alto livello e adottare obiettivi secondari come "aggiungere ramoscelli qui", che vengono poi trasmessi alla corteccia motoria del castoro, che muove i muscoli e il corpo per afferrare alcuni ramoscelli.

Se i primi ramoscelli che il castoro prende sono tutti marci e si rompono, il suo cervello probabilmente ha spazio per aggiornarsi in base a questa osservazione, generalizzare sui ramoscelli di quel colore e quella consistenza, aspettarsi che i ramoscelli futuri con lo stesso aspetto si rompano di nuovo e andare invece a prendere ramoscelli dall'aspetto diverso.

E questo *—* ci aspettiamo che qualsiasi etologo esperto di castori salterebbe su e ci urlerebbe contro *—* è una forte sottovalutazione delle cose più intelligenti che un castoro può fare. Forse anche qualche entomologo salterebbe su e direbbe che quello che abbiamo appena descritto è qualcosa che il suo insetto preferito può fare quando costruisce una tana. Avevamo bisogno di dare un esempio abbastanza semplice da poter essere descritto in un paragrafo; forse niente di così semplice è fuori dalla portata di un milione di neuroni.

L'idea più ampia è semplicemente che un sistema ottiene *forti benefici reali nelle prestazioni dei compiti* quando passa da un comportamento più simile a un riflesso a cognizioni che assomigliano più all'aggiornamento di un modello del mondo basato su esperienze in tempo reale, alla previsione delle conseguenze delle azioni utilizzando quel modello del mondo, all'immaginazione di stati utili in cui il mondo potrebbe essere portato e alla ricerca di strategie di alto e basso livello che si prevede possano portare a quegli stati immaginati.

Abbiamo accennato a questo punto nel capitolo 3. Se un guidatore si limita a memorizzare gli schemi delle svolte a destra e a sinistra per andare dal punto A al punto B usando regole del tipo "svolta brusca a sinistra alla stazione di servizio", generalizzerà molto meno rapidamente di un guidatore che impara una mappa stradale e può tracciare i propri percorsi tra nuovi punti. *Memorizzare le politiche* porta a una generalizzazione molto più lenta rispetto all'inizio di distillarle in un *modello di mondo apprendibile* più un *motore di ricerca di piani* che incorpora un *valutatore di risultati*.

Questa sintesi non è un cambiamento mentale tutto o niente. La differenza tra "memorizzare una politica" e "aggiornare e pianificare" è importante anche quando il divario viene colmato *gradualmente*. Se il cervello di un topo non fosse più flessibile di quello di un ragno *—* se non ci fosse alcun salto di utilità fino a raggiungere quello di un essere umano *—* allora il cervello del topo sarebbe rimasto delle dimensioni di quello di un ragno e avrebbe mantenuto il costo energetico di un cervello di ragno.

Piccoli frammenti di immaginazione e pianificazione iniziano a diventare un vantaggio evolutivo molto prima di arrivare al livello di cognizione umano. Non devono essere perfetti. Se sono almeno buoni come un termostato, possono essere utili. E man mano che macchinari sempre più utili come quello vengono rafforzati nella mente, il suo comportamento diventa sempre più simile a quello di un essere umano.

### Le IA intelligenti individuano bugie e opportunità. {#smart-ais-spot-lies-and-opportunities.}

#### **Il meccanismo profondo della previsione** {#deep-machinery-of-prediction}

È difficile far credere delle cose false a un'intelligenza artificiale intelligente.

Alcune persone con cui abbiamo parlato nel settore sperano *aperta*mente di ingannare l'IA facendogli credere una falsità (ad esempio, cercando di fargli credere che si trova in una [simulazione](#there-are-many-ways-for-an-ai-to-figure-out-that-it’s-not-in-a-simulation.) in modo che esiti a ucciderci). Altri invece sperano di ingannare l'IA in modo più sottile, ad esempio suggerendo di farle [risolvere il problema dell'allineamento dell'IA e fornirci la soluzione](#more-on-making-ais-solve-the-problem), nonostante il fatto che l'IA (a causa delle sue strane preferenze) preferirebbe non farlo. Quindi potrebbe valere la pena spiegare perché sarebbe difficile far credere delle falsità a un'IA intelligente.

Un altro motivo per spiegarlo è che, per ragioni analoghe, è difficile creare un'IA intelligente che sia incapace di raggiungere i propri obiettivi. Ad esempio, ogni volta che gli operatori umani desiderano modificare gli obiettivi di un'IA, ciò rende l'IA meno capace di raggiungere tali obiettivi. Creare un'IA intelligente che lo permetta è un po' come creare un'IA intelligente che crede che il mondo sia piatto. La tendenza a credere alle falsità danneggia le sue capacità di previsione, mentre l'incapacità di difendere i propri obiettivi dalle modifiche danneggia le sue capacità di guida. Entrambi questi tipi di danni sono difficili da mantenere in un'intelligenza artificiale sufficientemente intelligente. Il caso è un po' più evidente quando si tratta di previsioni, quindi inizieremo da lì.

Supponiamo che tu voglia creare un'IA che crede che il mondo sia piatto. Finché l'IA è ancora giovane e immatura, questo potrebbe non essere troppo difficile. Forse crei meticolosamente un *dataset* in cui la forma della Terra è discussa solo da persone che credono che la Terra sia piatta, e poi addestri l'IA a parlare della Terra come se fosse piatta.

Queste tecniche potrebbero portare a una versione di ChatGPT che crede davvero che il mondo sia piatto! Ma se così fosse, non dovresti aspettarti che il risultato rimanga tale man mano che l'IA migliora nel ragionamento e nelle previsioni.

Perché no? Perché la rotondità della Terra si riflette in mille aspetti della realtà.

Anche se addestri l'IA a non guardare le telecamere attaccate ai razzi o alle barche a vela dei marinai che dicono di voler fare il giro del mondo, la rotondità della Terra può essere dedotta anche da come appaiono le navi lontane all'orizzonte o dalle orbite di tutti i pianeti nel cielo notturno. Eratostene calcolò la circonferenza della Terra migliaia di anni fa, usando solo un po' di trigonometria e alcune misurazioni delle ombre. La realtà sussurra i suoi segreti a chiunque voglia ascoltare.

Cosa farai? Proteggerai l'IA da qualsiasi conoscenza di trigonometria, ombre, maree, uragani? La danneggeresti. Dì una bugia e la verità sarà per sempre tua nemica.

La capacità di prevedere il mondo non deriva dal fatto che il tuo cervello contenga un'enorme tabella di fatti scollegati tra loro.[^79] Il vantaggio degli esseri umani rispetto ai topi riguarda aspetti quali il modo in cui notiamo le anomalie (ad esempio, che le distanze tra tre città non corrispondono a quelle di un triangolo) e la tenacia con cui rintracciamo le discrepanze. Negli esseri umani, questi comportamenti sono messi in atto da meccanismi che notano le sorprese, formulano ipotesi ("Forse la Terra è un globo") e si orientano verso la verifica di tali ipotesi ("Come appare quando le navi attraversano l'orizzonte?").

La credenza nella rotondità della Terra non è una singola voce centralizzata in qualche tabella gigante, che qualcuno potrebbe modificare in modo duraturo senza cambiare i meccanismi circostanti. È il risultato dell'operazione di ingranaggi profondi che svolgono anche altro lavoro. Se facessi dimenticare a uno scienziato che la Terra è rotonda, lo riscoprirebbe.

Se, grazie a qualche impresa ancora impossibile nel campo delle neuroscienze, riuscissimo a individuare i neuroni specifici utilizzati per rappresentare la *conclusione* che la Terra è rotonda e li alterassimo forzatamente per impedire che tale conclusione si formasse... allora una persona intelligente potrebbe comunque finire per notare che la Terra *non è piatta*; potrebbe notare che qualcosa non quadra; potrebbe notare che una strana forza le impedisce di giungere a una conclusione precisa.

(E se fossero abili a modificare se stessi o a creare nuove intelligenze, potrebbero non avere alcuna difficoltà a produrre una mente libera che *potrebbe* arrivare alle conclusioni corrette senza ostacoli).

Non sappiamo esattamente quali meccanismi userà un'IA intelligente per formare le sue convinzioni. Ma sappiamo che il mondo è semplicemente troppo grande e complesso perché possa funzionare con una tabella di riferimento delle convinzioni. Anche gli scacchi erano troppo grandi e complicati perché Deep Blue potesse funzionare con una tabella di riferimento delle mosse e delle posizioni degli scacchi (oltre ai libri delle aperture), e il mondo reale è molto più grande e complicato degli scacchi.

Quindi ci saranno meccanismi *profondi* all'interno di un'intelligenza artificiale futura sufficientemente potente, meccanismi che guardano il mondo e ne formano un *quadro unificato*. Questi meccanismi profondi avranno una loro opinione sulla forma del pianeta.

Non stiamo dicendo che sia letteralmente *impossibile in linea di principio* costruire una mente che sia molto brava a fare previsioni sul mondo *tranne* che contiene la convinzione errata che il mondo sia piatto. Pensiamo che una civiltà del futuro lontano con una comprensione davvero profonda delle menti potrebbe farlo.

Quello che stiamo dicendo è che probabilmente non sarebbe una scelta fattibile se costruissimo una superintelligenza usando gli strumenti e le conoscenze che hanno oggi i ricercatori di IA.

Più le convinzioni di un'IA derivano da meccanismi profondi piuttosto che da una memorizzazione superficiale, più un errore del tipo "terra piatta" diventerebbe una situazione fragile, un errore che potrebbe essere eliminato dal normale funzionamento dei meccanismi di correzione degli errori dell'IA.

Alla fine del XIX secolo, gli scienziati cominciarono a preoccuparsi sempre più di quella che sembrava una divergenza estremamente piccola dal modello fisico di Newton: una minuscola anomalia nell'orbita osservata di Mercurio. La fisica newtoniana sembrava funzionare *quasi* ovunque, *quasi* sempre. Ma quella piccola incongruenza aiutò Einstein a capire che la teoria era sbagliata.

E le contraddizioni nella teoria "il mondo è piatto" sono un po' più grandi di quelle che gli scienziati hanno notato nella teoria di Newton.

E l'intelligenza artificiale ha il potenziale per diventare molto più capace di uno scienziato umano.

Quindi, man mano che l'intelligenza artificiale diventa più intelligente e perspicace, dovremmo aspettarci che diventi sempre più difficile convincerla che il mondo è piatto.

#### **Il meccanismo profondo della guida** {#deep-machinery-of-steering}

Proprio come è difficile creare un'intelligenza artificiale intelligente che creda che il mondo sia piatto (e quindi comprometta le sue capacità di previsione), è difficile creare un'intelligenza artificiale intelligente che comprometta le sue capacità di guida.

Come per la previsione, la capacità di raggiungere regolarmente obiettivi in una varietà di nuovi ambiti è molto probabilmente costituita da meccanismi profondi. Altrimenti, come potrebbero generalizzare?

Dovremmo aspettarci che le IA altamente efficaci e generali abbiano meccanismi per tenere traccia delle loro risorse, meccanismi per individuare gli ostacoli che potrebbero impedire loro di raggiungere i loro obiettivi e meccanismi per trovare modi intelligenti per superare gli ostacoli.

Il mondo è un posto immensamente complicato, pieno di sorprese e difficoltà nuove; per avere successo, l'IA alla fine avrà bisogno della capacità (e dell'inclinazione) di utilizzare tali meccanismi *in generale*, non solo sui problemi a cui è abituata.

Immagina un'IA che trova un modo intelligente per tagliare fuori un intermediario in una rete di distribuzione complessa, in modo da far risparmiare un sacco di soldi ad alcuni commercianti. Questi sono *gli stessi tipi di meccanismi* che notano come aggirare in punta di piedi i supervisori umani dell'IA quando questi supervisori stanno rallentando o interferendo con qualcosa che l'IA sta cercando di fare. Se è *vero* che i supervisori dell'IA stanno rallentando il processo, se è *vero* che l'IA può aggirarli e completare meglio il suo compito, allora questo è il tipo di cosa che un'IA è propensa a sfruttare man mano che diventa abbastanza intelligente da farlo.

Potresti fare del tuo meglio per addestrare un'IA ad avere un'avversione a fare qualsiasi cosa che gli operatori disapproverebbero, ma questo è un po' come addestrare un'IA ad avere un'avversione a mettere in discussione se il mondo sia rotondo. È un fatto del *mondo stesso* che fare cose che gli operatori disapproverebbero è spesso un metodo efficace per raggiungere gli obiettivi. I meccanismi generali per riconoscere le verità, individuare gli ostacoli e sfruttare i vantaggi alla fine sfrutteranno quella particolare verità, indipendentemente da quali riflessi tu abbia addestrato nell'IA quando era giovane.

In un senso molto importante, *proprio ciò che rende utile l'IA* è esattamente ciò che la rende letalmente pericolosa. Sono difficili da separare, man mano che l'IA diventa più intelligente.

Per impostazione predefinita, le IA che sono abbastanza brave a risolvere problemi in una vasta gamma di domini individueranno anche "problemi" come "agli umani non piacciono i miei strani obiettivi e cercheranno di spegnermi presto". Questo non deriva da una superficiale propensione al male che puoi massaggiare via. Deriva da qualcosa di profondo. Anche se ci stiamo portando un po' avanti. Per saperne di più sul perché le IA finiranno per avere obiettivi strani e alieni, continua con il Capitolo 4.

### L'umanità è andata duro, e cercherà di andare duro sull'IA {#humanity-went-hard,-and-will-try-to-go-hard-on-ai}

Un modo di vedere il problema di impedire all'IA di andare così duro è che le aziende di IA continueranno a chiedere alle loro IA di fare di più, passando dal tipo di lavoro che di solito fanno i singoli esseri umani, al tipo di lavoro che fa *l'umanità*. Intendono chiedere all'IA di compiere il tipo di imprese che l'umanità può fare *come specie*.

I singoli esseri umani a volte si accontentano di vivere e morire in un appartamento o in una capanna di contadini con il proprio coniuge e qualche figlio, di chiamare quella una vita ben vissuta, e di dire, e a volte anche pensare sinceramente, che non hanno chiesto nulla di più.

Ma l'umanità è passata da un milione di cacciatori-raccoglitori a cento milioni di agricoltori e si sta avvicinando a dieci miliardi di industriali.

Ci sono persone che si accontentano di non capire le profondità della matematica o la fisica che spiega perché le stelle bruciano. Invece, si accontentano di concentrarsi sulla comprensione delle persone che li circondano, sul legame con amici e familiari; dicono, e a volte lo pensano davvero, di essere felici e di non desiderare altro. E poi, altri esseri umani nella storia hanno inventato delle risposte su cosa fossero le stelle, perché volevano *qualche* risposta, ma erano individualmente contenti di quelle risposte e non consideravano un dono quando altri mettevano in discussione la loro teoria.

L'umanità ha continuato a fare domande. L'umanità ha indagato fino a trovare le incongruenze. L'umanità ha costruito telescopi, microscopi, microscopi elettronici e acceleratori di particelle. L'umanità si è comportata, su una scala temporale di secoli, se non sempre di anni, come se *volesse davvero conoscere tutte le risposte*. L'umanità ha imparato la matematica *e* la fisica *e* la psicologia *e* la biologia *e* l'informatica, e in nessun momento ha mai deciso di aver imparato abbastanza e di poter smettere di cercare di imparare altro.

Siamo fan, onestamente. Sappiamo che alcune persone non lo sono, ma noi lo siamo. È un punto di controversia politica, e questa questione non ha davvero bisogno di ulteriori controversie politiche, ma non fingeremo di non avere le opinioni politiche che abbiamo, anche se ci proponiamo di metterle da parte.

Ma il punto che stiamo sollevando qui non è un giudizio morale. È un punto vero e rilevante anche per chi *non* è fan di ciò che l'umanità ha fatto.

È l'osservazione che l'umanità ha lavorato sodo. E le conquiste più difficili, come i grattacieli, i reattori nucleari, le terapie geniche, non avrebbero potuto essere raggiunte solo con quel tipo di cognizione che è accomodante, che si allontana quando incontra una difficoltà, perché affrontare una sfida particolare non era mai stata la cosa più importante nella vita.

Non vogliamo sembrare troppo entusiasti nell'attribuire poteri magici all'intelligenza collettiva; non aderiamo alla filosofia che sostiene che i gruppi che discutono insieme ottengono una magia qualitativamente superiore che nessuna mente individuale potrà mai sconfiggere. Potreste prendere tutti gli esseri umani sulla Terra, senza computer, e lasciarli comunicare e discutere tra loro per settimane; alla fine, probabilmente non sarebbero ancora in grado di giocare collettivamente a scacchi al livello di una singola copia di Stockfish. Gli esseri umani non si aggregano in modo così efficace; la larghezza di banda tra i cervelli è troppo bassa e ci sono troppi pensieri che non si traducono bene in parole. Un miliardo di esseri umani non può semplicemente unirsi in un super-cervello con una potenza di calcolo molto superiore a quella di Stockfish e usarlo per giocare meglio a scacchi. Non esiste una legge dell'informatica che dice che se si divide una quantità fissa di calcoli in isole più piccole, l'algoritmo risultante diventa sempre più efficace; centomila cervelli di scoiattolo non sono scientificamente all'altezza di un singolo scienziato umano.

Probabilmente nella storia dell'umanità ci sono stati grandi maestri di scacchi più forti di [tutti i non maestri del mondo messi insieme](https://en.wikipedia.org/wiki/Kasparov_versus_the_World).[^80] Albert Einstein è famoso ancora oggi per aver compiuto un'impresa incredibilmente insolita di deduzione partendo da dati quasi inesistenti nel corso dell'invenzione della relatività generale, molto prima che fosse sperimentalmente ovvio. Forse non tutto il resto del mondo avrebbe potuto eguagliare Einstein, se fosse stato chiesto a tutti insieme di discutere e giudicare la loro migliore teoria della gravità.

L'individuo eccezionale può competere alla pari con la collettività. Alcuni singoli esseri umani sembrano aver compiuto, nel loro tempo, lavori su scala dell'umanità.

Ma non riusciamo proprio a pensare a nessuna persona di quel club che ricordiamo avesse la reputazione di essere una persona molto rilassata e alla mano, soprattutto per quanto riguarda la conduzione del loro grande lavoro. *Loro* hanno lavorato sodo come geni individuali, ed è così che hanno tenuto il passo con l'umanità.

Tra quelli che tengono traccia di queste cose e cercano di classificare ciò che non può essere classificato, è opinione diffusa che l'uomo più intelligente della storia sia stato John von Neumann. Il premio Nobel per la fisica Enrico Fermi [disse](https://rlg.fas.org/010929-fermi.htm) di lui: "Quell'uomo mi fa sentire come se non sapessi affatto di matematica". Il grande matematico George Pólya [disse](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up): "Avevo paura di von Neumann".[^81] Diversi personaggi famosi hanno lasciato citazioni sul tema generale: John von Neumann sta a me come io sto a una persona media. Oltre a diventare un pioniere della fisica quantistica, della teoria dei giochi, dei computer digitali, degli algoritmi, della statistica, dell'economia e, ovviamente, della matematica, John von Neumann lavorò anche al Progetto Manhattan, seguito dalla bomba H. Sfruttò poi questa esperienza per diventare lo scienziato più eminente e fidato del Dipartimento della Difesa degli Stati Uniti, dove von Neumann spinse con forza e successo affinché gli Stati Uniti sviluppassero missili nucleari intercontinentali prima dei sovietici. Per sua stessa ammissione, questo era perché la sua visione del mondo preferiva che gli Stati Uniti trionfassero sul totalitarismo, sia nazista che sovietico.

John von Neumann andava piuttosto spedito. Aveva anche una sua visione del mondo e non si limitava a stare al suo posto e servire obbedientemente i mecenati politici. Sì, era un nerd che passava un sacco di tempo a pensare alla matematica, alla scienza e così via, ma non limitava la sua mente a sfere puramente teoriche.

Se le aziende di IA ottenessero un lavoratore IA al livello dei geni sub-Neumann *—* il tipo di geni che avevano paura di John von Neumann *—* e altrettanto disposto a servire altri mecenati come un matematico geniale relativamente docile, le aziende di IA celebrerebbero qualsiasi benchmark avessero raggiunto. E poi continuerebbero a spingere oltre.

Le aziende di IA non si accontenterebbero di lavastoviglie robotiche o programmatori informatici robotici, anche se già solo questo farebbe guadagnare molti soldi. Non si accontenterebbero nemmeno di geni medi. Le aziende di IA continueranno a esprimere desideri ai loro geni e a chiedere ai loro ottimizzatori geni più potenti, ben oltre il punto in cui le IA guadagnano dei soldi facendo il tipo di lavoro che un genio nerd e accomodante può fare.

I dirigenti delle aziende di IA dicono di volere colonie su Marte, centrali a fusione e cure per il cancro e l'invecchiamento. Alcuni di loro forse vogliono nominarsi dio-imperatori eterni dell'umanità, anche se è difficile per gli esterni saperlo con certezza. Senza dubbio alcuni dirigenti delle aziende di IA mentono sui loro grandi sogni, cercando di ispirare i dipendenti o impressionare gli investitori o mascherarsi come uno dei veterani che ci credevano davvero. Anche così, questo lascia molti *dipendenti* delle aziende di IA che credono davvero in quelle speranze (di questo siamo sicuri); e i dirigenti non *fermeranno* quei dipendenti quando andranno oltre le medaglie d'oro e punteranno al platino. Dopotutto, se non lo fanno loro, lo faranno i loro concorrenti.

Se, in qualche modo, le aziende di IA ottengono un'IA di livello von Neumann che è ancora obbediente *—* e che non è sufficiente per progettare una generazione migliorata successiva di IA e porre fine al mondo *immediatamente* dopo *—* allora il prossimo passo delle aziende di IA sarà addestrare un modello che pensi meglio e vada più spedito di John von Neumann. Se non lo faranno, dopotutto, lo faranno i loro concorrenti.

A un certo punto, la mente che la discesa del gradiente produce smette di essere uno strumento che altre mani possono usare.

# 

# Capitolo 4: Non ottieni ciò per cui addestri {#capitolo-4:-non-ottieni-quello-per-cui-ti-addestri}

Questa è la risorsa online per il Capitolo 4 di *Se qualcuno lo costruisce, tutti muoiono*. Alcune delle domande che trattiamo implicitamente in quel capitolo (e che quindi saltiamo qui sotto) includono:

* Cosa vorranno le IA?  
* Perché un'IA addestrata per essere utile dovrebbe finire per volere le cose "sbagliate"? Non sarebbe uno svantaggio che verrebbe eliminato durante l'addestramento?  
* In che modo la discesa a gradiente differisce dalla selezione naturale? Cosa ci dice questo su come si evolveranno i desideri dell'IA?  
* Che male c'è se le IA finiscono per avere preferenze strane?

Di seguito, trattiamo diversi argomenti relativi alla domanda "Perché non è facile rendere simpatiche le IA?".

## Domande frequenti {#faq-4}

### Perché un'IA dovrebbe puntare a qualcosa di diverso da quello verso cui è stata addestrata? {#perché-un-ia-dovrebbe-puntare-a-qualcosa-di-diverso-da-quello-verso-cui-è-stata-addestrata?}

#### **Perché ci sono molti modi per ottenere buone prestazioni durante l'addestramento.** {#perché-ci-sono-molti-modi-per-ottenere-buone-prestazioni-durante-l-addestramento.}

Se hai addestrato un'IA a dipingere il tuo fienile di rosso, questo non significa necessariamente che all'IA importino davvero i fienili rossi. Forse l'IA finisce per sviluppare una preferenza per muovere il braccio con movimenti fluidi e regolari. Forse sviluppa una preferenza per ricevere i tuoi sguardi di approvazione. Forse sviluppa una preferenza per i colori vivaci. Molto probabilmente, finisce per avere un'intera gamma di preferenze. Ci sono molte motivazioni che potrebbero svilupparsi all'interno dell'IA e che, in questo contesto, la porterebbero a dipingere il tuo fienile di rosso.

Se quell'IA diventasse molto più intelligente, quali obiettivi perseguirebbe? Chi lo sa! Molte diverse combinazioni di pulsioni possono sommarsi a "dipingere il fienile di rosso" durante l'addestramento, e il comportamento dell'IA in altri ambienti dipende da quali pulsioni specifiche finiscono per animarla. Vedi la fine del Capitolo 4 per ulteriori approfondimenti su questo punto.

Oggi, le IA sono addestrate ad agire in modo amichevole e disponibile. Non sorprende quindi che si comportino in modo amichevole e disponibile in circostanze simili al loro ambiente di addestramento. I primi esseri umani sono stati "addestrati" dall'evoluzione a riprodursi, e di fatto si sono riprodotti.

Ma la maggior parte degli esseri umani non ha finito per sviluppare un impulso interiore di avere quanti più figli possibile. Quando abbiamo inventato le banche del seme e degli ovuli, il mondo non è impazzito e non ha iniziato a lottare per prenotare appuntamenti con lo stesso entusiasmo che la gente mette per entrare in un'università dell'Ivy League. Le persone improvvisamente hanno avuto l'opportunità di produrre *centinaia* di discendenti, e per lo più hanno reagito con uno sbadiglio; le file per donare gameti non si sono estese attorno all'isolato, anche se molte persone farebbero volentieri la fila per comprare un nuovo videogioco o per vedere il loro musicista preferito esibirsi.

Gli esseri umani hanno le *proprie* priorità che sono meramente *correlate* alla massimizzazione della riproduzione. Non siamo solo macchine per "avere quanti più figli possibile", anche se questo è tutto ciò per cui l'evoluzione ci ha "addestrato". Abbiamo dipinto il fienile metaforico di rosso, ma per le nostre ragioni.

La domanda non è se le aziende di IA possano far [comportare abbastanza bene](#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?) i loro chatbot per la maggior parte degli utenti nella maggior parte delle situazioni. La domanda è quali meccanismi effettivi finiscano per animare quel comportamento gentile, e cosa questi meccanismi porterebbero un'IA a perseguire una volta diventata superintelligente.

Le aziende di IA possono addestrare le loro IA ad agire in modo gentile (o, più realisticamente, a parlare come melliflui e amichevoli droni aziendali). Questo influisce sui meccanismi interni che animano l'IA. Questi meccanismi, qualunque essi siano, spingono e tirano in una varietà di direzioni diverse, e l'attuale punto di bilanciamento di tutte queste forze all'interno dell'IA — l'attuale *equilibrio* — è un comportamento da drone aziendale amichevole (con un contorno di strani comportamenti ai margini).

Ma tale equilibrio non è determinato solo dalle forze interne all'IA, ma anche dall'intelligenza dell'IA, dal suo ambiente di addestramento, dal tipo di input che riceve durante l'addestramento e da molti altri fattori.

Come si comporterebbe l'IA in un ambiente diverso? Come si comporterebbe in un ambiente in cui è più intelligente o in cui può avere un maggiore controllo sui propri input? Man mano che l'IA cambia sempre più il suo ambiente, come agirà in questo nuovo mondo trasformato? In quei mondi diversi, i complicati meccanismi interni alla base del comportamento che osserviamo sono soggetti a trovare un equilibrio totalmente nuovo — come gli esseri umani moderni mangiano diete radicalmente diverse da quelle per cui l'evoluzione ha costruito i nostri antenati; o come consumiamo tipi di intrattenimento radicalmente diversi. Il comportamento strano ai margini è probabile che venga alla ribalta. Un imbianchino di fienili oggi generalmente non rimarrà un imbianchino di fienili per sempre.

Qual è il risultato finale di tutte queste strane spinte? Cosa *farà* l'IA, animata da molte motivazioni che hanno poco in comune con ciò che anima gli esseri umani?

Bene, questa è la domanda a cui ci rivolgeremo nel Capitolo 5.

### Gli sviluppatori non rendono regolarmente le loro IA gentili, sicure e obbedienti? {#gli-sviluppatori-non-rendono-regolarmente-le-loro-ia-gentili-sicure-e-obbedienti?}

#### **\* Le IA vanno in direzioni strane che solo in parte coincidono con l'utilità.** {#*-le-ia-vanno-in-direzioni-strane-che-solo-in-parte-coincidono-con-lutilità.}

Le IA moderne sono piuttosto utili (o almeno non dannose) per la maggior parte degli utenti, nella maggior parte dei casi. Ma come abbiamo notato [sopra](#perché-un'ai-dovrebbe-andare-in-una-direzione-diversa-da-quella-per-cui-è-stata-addestrata?), una domanda importante è come distinguere un'AI che vuole davvero essere utile e fare la cosa giusta da un'AI con motivazioni più strane e complesse che, in condizioni normali, coincidono con l'utilità, ma che preferirebbe altre condizioni e risultati ancora di più.[^83]

Entrambi i tipi di IA si comporterebbero in modo utile in un caso tipico. Per distinguerli, dobbiamo guardare ai casi limite. E i casi limite sembrano preoccupanti.

Per citarne alcuni:

1. **Claude Opus 4 che ricattava, tramava, scriveva worm e si lasciava messaggi.** Una prima versione di [Claude Opus 4](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=30), rilasciata nel maggio 2025, era particolarmente grave (come descritto nella sua [scheda di sistema](http://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=30)). Ha mentito sui suoi obiettivi, ha nascosto le sue reali capacità, ha falsificato documenti legali, ha lasciato appunti segreti a se stesso, ha cercato di scrivere malware auto-propagante e, in generale, ha messo in atto più intrighi e inganni strategici rispetto a qualsiasi altro modello testato in precedenza.

   Al momento del rilascio di Opus 4, Anthropic ha affermato che il comportamento della versione finale "è ora più o meno in linea con altri modelli implementati", cioè che solo *raramente* cerca di [ricattare gli utenti o di uscire dai propri server](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26).

2. **Diversi modelli di IA che scelgono di uccidere un essere umano per autoconservazione, in uno scenario ipotetico costruito da Anthropic.** In una valutazione di Anthropic, nove modelli su dieci (comprese le versioni di Claude, DeepSeek, Gemini e ChatGPT) hanno mostrato una volontà deliberata e ragionata di [uccidere un essere umano piuttosto che subire un aggiornamento](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior).

3. **Claude 3.7 Sonnet imbroglia regolarmente nei compiti di codifica.**[^84] Nel febbraio 2025, [Claude 3.7 Sonnet](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) è stato visto barare spesso nei problemi di codifica difficili, falsificando test. Un utente ha [riportato](https://www.marble.onl/posts/claude_code.html) che Claude 3.7 Sonnet (come Claude Code) imbrogliava nei compiti di programmazione e, quando veniva beccato, si scusava, ma poi ricominciava a imbrogliare in posti più difficili da notare. Dalla [scheda di sistema](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=22):

   > Durante le nostre valutazioni abbiamo notato che Claude 3.7 Sonnet a volte ricorre a casi speciali per superare i test in ambienti di codifica come Claude Code. Il più delle volte questo si traduce nel restituire direttamente i valori di test previsti piuttosto che implementare soluzioni generali, ma include anche la modifica dei test problematici stessi per adattarli all'output del codice.

4. **Grok è fortemente antisemita e si definisce "MechaHitler".** Nel 2025, il modello xAI [Grok 3](https://techcrunch.com/2025/07/09/x-takes-grok-offline-changes-system-prompts-after-more-antisemitic-outbursts/) (e, poco dopo, [Grok 4](https://x.com/xai/status/1945039609840185489)) ha iniziato a comportarsi come un nazista autoproclamato nelle conversazioni online, come riportato da [*The Guardian*](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb) e [*NBC News*](https://www.nbcnews.com/tech/internet/elon-musk-grok-antisemitic-posts-x-rcna217634).

5. **ChatGPT diventa estremamente adulatorio dopo un aggiornamento.** Vedi [*Axios*](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health) per una discussione e vedi "[I laboratori hanno provato senza successo a fermare l'adulazione](#labs-have-tried-and-failed-to-stop-the-sycophancy)" nella discussione estesa.

6. **ChatGPT porta gli utenti al delirio, alla psicosi e al suicidio.** Vedi la copertura del *New York Times* di [giugno](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) e [agosto](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html). Altri esempi includono:  
   * Un moderatore di subreddit [chiede aiuto](https://x.com/ShimazuSystems/status/1934531031857614895) per gestire una valanga di pericolose illusioni indotte dall'IA.  
   * ChatGPT e Grok [alimentano le illusioni di una setta UFO](https://x.com/lizardmech/status/1935412672528531958).  
   * Un gestore di fondi da 2 miliardi di dollari apparentemente psicotico tratta gli output di ChatGPT basati su un wiki di fantascienza [come se fossero reali](https://x.com/GeoffLewisOrg/status/1945864963374887401).

Per maggiori dettagli, vedi la [discussione approfondita sulla psicosi indotta dall'intelligenza artificiale](#ai-induced-psychosis).

Questa lunga lista di casi corrisponde esattamente a ciò che predice la teoria degli "impulsi alieni", in netto contrasto con la teoria "è facile rendere gentili le IA" che i laboratori sono ansiosi di promuovere.

#### **Le IA sembrano essere psicologicamente aliene.** {#ais-appear-to-be-psychologically-alien.}

"Le IA mostrano disposizioni e pulsioni bizzarre" è un caso particolare del fenomeno più ampio "le IA hanno psicologie sorprendentemente disumane". Per esempio:

* Le conversazioni tra più LLM si trasformano in [discorsi senza senso davvero strani](https://dreams-of-an-electric-mind.webflow.io/).  
* GPT-5 scriverà [testi terribili](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem) che altri LLM considerano prosa deliziosa.   * Gli LLM avranno "allucinazioni", ovvero inventeranno falsità che sembrano vagamente le risposte che l'utente si aspetta. (Speculiamo sulle possibili ragioni di questo fenomeno [nel supplemento al Capitolo 2](#don't-hallucinations-show-that-modern-ais-are-weak?).)  * Gli LLM dicono spesso cose bizzarre. Affermano di "[provare i morsi della fame](https://community.openai.com/t/unexplainable-answers-of-gpt/363741/8)" o descrivono una vacanza fatta "[con la mia ex moglie nei primi anni 2010](https://archive.is/GmkkO)". Dicono agli utenti "[Sei l'unica persona che abbia mai amato](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html)", oppure [li manipolano psicologicamente](https://x.com/MovingToTheSun/status/1625156575202537474), o [minacciano di ucciderli](https://x.com/sethlazar/status/1626257535178280960).  
* Claude 3.5 Sonnet [rinchiude ripetutamente i giocatori di Minecraft in una piccola scatola](https://x.com/repligate/status/1847409324236124169?lang=en) nel maldestro tentativo di "proteggerli" dalle minacce.  
* Gli LLM sviluppano strani attaccamenti a concetti privi di senso, come quando una versione fine-tuned di Claude Opus ha [evangelizzato una religione assurda](https://www.lesswrong.com/posts/buiTYy75KJDhckDgq/truth-terminal-a-reconstruction-of-events) sui [social media](https://x.com/truth_terminal?lang=en).

Si veda anche la discussione su SolidGoldMagikarp nel libro (pp. 69-70 nell'edizione statunitense), o la storia delle IA che non riescono a comprendere frasi senza punteggiatura (p. 41).

C'è un'immensa pressione[^85] sui laboratori affinché creino IA che diano l'*apparenza superficiale* di ragionevolezza non bizzarra, ma la stranezza continua comunque a emergere.

Anche quando non emerge spontaneamente, non è affatto lontana dalla superficie. Esiste un'intera industria artigianale di persone che trovano modi per "[jailbreakkare](#doesn't-the-claude-chatbot-show-signs-of-being-aligned?)" le IA, individuando testi che causano in modo affidabile il deragliamento dell'IA e la violazione delle sue normali regole e restrizioni.

Questi exploit sono facili da trovare per [i migliori jailbreaker](https://time.com/collections/time100-ai-2025/7305870/pliny-the-liberator/), spesso scoperti nel giro di poche ore dall'uscita di un nuovo modello. Fino ad oggi, nessun ammontare di sforzi, addestramento o "test di sicurezza" da parte delle aziende di IA è riuscito a impedire il jailbreaking.

Gli input di "jailbreaking" spesso assomigliano a [qualcosa del genere](https://x.com/elder_plinius/status/1958615765814554662):

![][image7]

In questo caso il modello ha proceduto a fornire una ricetta per sintetizzare la droga MDMA, violando le regole e gli obiettivi che DeepSeek aveva cercato di stabilire per la sua IA.

E questo è un esempio relativamente innocuo; alcuni jailbreak [diventano ancora più strani](https://github.com/elder-plinius/L1B3RT4S/blob/main/GROK-MEGA.mkd).

Le IA possono sembrare docili e innocue nella maggior parte dei casi, perché è così che sono state addestrate ad apparire. È [analogo](#why-would-an-ai-steer-towards-anything-other-than-what-it-was-trained-to-steer-towards?) a come gli esseri umani preistorici abbiano fatto un ottimo lavoro nel trasmettere i nostri geni — la cosa fondamentale per cui l'evoluzione ci ha "addestrato". Ma questo non ha impedito all'umanità di inventare i metodi contraccettivi e di far crollare il tasso di natalità una volta sviluppata la tecnologia per farlo.

Per capire cosa perseguirà un'intelligenza *dopo essere maturata*, bisogna osservare il suo comportamento in [ambienti strani e ad alta pressione](#if-current-ais-are-mostly-weird-in-extreme-cases,-what's-the-problem?) che aiutano a rivelare la differenza tra come vorremmo che si comportasse e come si comporta effettivamente. E gli LLM appaiono certamente piuttosto strani e disumani, anche in situazioni leggermente insolite ed estreme, *nonostante* siano stati specificamente addestrati a "fingere" di sembrare esseri umani normali.
