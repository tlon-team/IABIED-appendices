# Глава 3: Научиться хотеть

ИИ, способные на достаточно впечатляющие вещи, -- они, как правило, будут чего-то хотеть.

Говоря, что ИИ чего-то «хочет», мы не подразумеваем, что у него обязательно есть человеческие желания или чувства. Может, будут, а может и нет. Мы, скорее, имеем в виду, что ИИ поведёт себя так, будто у него есть цели. Он будет стабильно направлять события к определённым результатам: предвидеть трудности, адаптироваться к переменам, вести себя сосредоточенно, целеустремлённо и настойчиво.

В третьей главе книги «Если кто-то его сделает, все умрут» мы рассматриваем темы:

- Как машина может обрести способность «хотеть» в интересующем нас смысле?

- Есть ли свидетельства, что ИИ могут чего-то хотеть?

- Обязательно ли более продвинутые ИИ будут чего-то хотеть?

Ответы на частые вопросы, приведенные ниже, разъясняют, почему создание очень мощных универсальных ИИ без собственных целей кажется сложным. В расширенном обсуждении мы развиваем мысль, что задать системе жёсткое стремление к цели -- куда проще и естественнее, чем ввести качества вроде лени или уступчивости.

## Часто задаваемые вопросы

### Будут ли у ИИ человеческие эмоции?

#### Вероятно, нет.

В целом, не стоит представлять, что ИИ обладают человеческими качествами просто из-за интеллекта. (Мы подробнее разберём это в расширенном обсуждении «Антропоморфизм и механоморфизм».) Глупо говорить: «Эта LLM похожа на человека, поэтому я припишу ей всевозможные человеческие черты, включая способность желать».

Но будьте осторожны. Другая крайность в размышлениях об ИИ -- мы называем её «механоморфизмом» -- это когда считают, что раз ИИ состоит из механических частей, он должен иметь и типичные для машин недостатки. Говорить: «Эта LLM -- машина, поэтому я припишу ей всевозможные качества, которые ассоциирую с машинами, например логичность и непонятливость» -- так же бессмысленно.

Чтобы предсказать поведение ИИ, не стоит ни воображать, что им будут двигать человеческие эмоции, ни ожидать, что он не сможет находить творческие решения. Как обсуждается в книге, лучше спросить, *какое поведение требуется ИИ для успеха*.

Представьте, что играете в шахматы с ИИ и ставите ловушку для его ферзя, используя своего коня как приманку. Не спрашивайте, хватит ли ему осторожности, чтобы это заметить. Не спрашивайте, заставляет ли его холодная логика взять коня, несмотря на западню. Спросите, какое поведение для ИИ *самое выигрышное*. Умелый ИИ будет вести себя так, чтобы победить.

ИИ будут вести себя так, словно чего-то хотят, потому что *целенаправленное и успешное поведение связаны*.

### Разве ИИ -- не просто инструменты?

#### ИИ выращивают, а не собирают. Поэтому они уже сейчас делают не то, что им говорят.

Мы уже обсуждали галлюцинации. Иногда ИИ, которому приказано говорить «Я не знаю», всё равно начинает выдумывать, если выдумка больше похожа на ответы из его обучающих данных.[^1]

Другой пример из книги (сноска в главе 4 и в отступление в главе 7) -- случай с Claude 3.7 Sonnet от Anthropic. Она не только жульничает при решении поставленных задач, но иногда ещё и *скрывает своё жульничество от пользователя*. Это указывает на некоторое понимание, что пользователь хотел чего-то другого.[^2] Ни пользователи, ни инженеры Anthropic не просят Claude жульничать. Совсем наоборот! Но все доступные методы выращивания ИИ поощряют модели, которые обманывают, если во время обучения это сходит им с рук. Такие модели мы и получаем.

Возможности инженеров по созданию ИИ-инструментов очень ограничены. Вопрос в том, становятся ли ИИ всё более целеустремлёнными, всё более «агентными» по мере того, как их обучают быть всё более эффективными? И ответ -- «да». Это подтверждается эмпирическими свидетельствами, такими как случай с o1 от OpenAI, который обсуждался в Главе 3.

#### LLM уже проявляют инициативу.

В книге мы рассказывали, как o1 от OpenAI выбрался из тестового окружения, чтобы починить неработающие тесты. Ещё мы упоминали модель от OpenAI, придумавшую, как заставить человека решить за неё капчу.[^3] Если ваша отвёртка может придумать и осуществить план побега из своего ящика, пожалуй, стоит перестать считать её «просто инструментом».

И можно ожидать, что ИИ будут становиться в этом только лучше. Их ведь обучают решать всё более сложные задачи.

#### Компании стараются наделить ИИ агентностью.

Из коммерческих соображений. Этого хотят их пользователи и инвесторы. В январском посте 2025 года гендиректор OpenAI Сэм Альтман написал: «Мы считаем, что в 2025 году первые ИИ-агенты смогут «пополнить ряды рабочей силы» и существенно повысить производительность компаний». [Конференция разработчиков Microsoft 2025 года](https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/) была посвящена новой «эпохе ИИ-агентов». Это перекликается с формулировками xAI, которые ранее в том же году описали свою модель Grok 3 как предвестника «[Эпохи Рассуждающих Агентов](https://x.ai/news/grok-3)». На своей конференции 2025 года Google также анонсировала агентов типа «обучи и повтори».[^4]

Разговорами дело не ограничивается. Организация [METR](https://metr.org/) отслеживает [способность ИИ выполнять многоэтапные задачи](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). Чем длиннее задача, тем больше инициативы требуется от ИИ. И рост, по крайней мере по результатам METR, тут экспоненциальный.

В июле 2025 года двое исследователей из OpenAI [похвастались](https://x.com/xikun_zhang_/status/1946278266786189744?t=YqVAbKsuF6wLbFuB4OZ18A), что успешно использовали своего новейшего агента для обучения улучшенной версии его самого. Один из них заявил: «Вы всё правильно поняли. Мы усердно работаем над автоматизацией \[sic\] собственной работы :)»

### Можно ли просто обучить ИИ быть послушными?

#### Пассивность мешает полезности.

«Пассивным» мы называем ограниченный ИИ, который делает ровно то, о чём его просят, и ничего сверх. У него нет лишней инициативы, он не выполняет дополнительной работы. Отвёртка не продолжает закручивать шурупы, когда вы её откладываете. Можем ли мы сделать ИИ пассивным?

Это непросто. Да, многие люди *кажутся* ленивыми, но они же, играя в настольную игру, порой оживляются и захватывают массу ресурсов. У большинства из них нет *возможности* легко выиграть миллиард долларов. Нет и *возможности* задёшево создать себе более умных, целеустремлённых и заботящихся об их нуждах слуг.

Но это из-за нехватки способностей, намерения тут ни при чём. Если бы эти люди стали гораздо умнее и получили такие доступные и простые варианты, они бы ими воспользовались? См. также расширенное обсуждение, почему надёжная лень -- сложная цель.

Даже если бы удалось создать одновременно умные и пассивные/ленивые ИИ, эти качества мешают полезности. Уже были ИИ, которые [вели себя несколько лениво](https://arstechnica.com/information-technology/2023/12/is-chatgpt-becoming-lazier-because-its-december-people-run-tests-to-find-out/). Компании переобучали их, чтобы те старались усерднее.

Более сложные задачи, например, разработка лекарств, требуют от ИИ всё большей инициативы. Поэтому их и будут обучать в этом направлении. Сложно отделить склонность к полезной работе от склонности к упорству. См. также расширенное обсуждение о том, почему так сложно создать ИИ, который был бы одновременно полезен и при этом пассивен или послушен.

#### Мы не умеем надёжно прививать ИИ какой либо конкретный характер.

ИИ выращивают, а не создают вручную. Инженеры не могут взять и изменить его поведение, сделать более послушным или похожим на инструмент. Это не контролируемо.

Корпорации, конечно, пытаются. Попытки ИИ-компаний улучшить поведение своих продуктов приводили к неприятным инцидентам. Вспомним случай с Grok от xAI. [Он называл себя «МехаГитлером» и делал антисемитские заявления](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Это произошло после изменения его системного промпта. Туда добавили указание «не стесняться делать политически некорректные заявления, если они хорошо обоснованы». Или более ранний случай: нейросеть [Gemini от Google создавала изображения расово разнообразных нацистов](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) и прочий бред. Считается, что это стало результатом инструкций, поощряющих разнообразие.

У создателей нет тонкого контроля за поведением ИИ. Они могут лишь задавать общие направления, вроде «не стесняться политически некорректных заявлений» или «изображать разнообразие». Такие указания приводят к самым разным запутанным и часто непредвиденным последствиям.

Выращивание ИИ -- непрозрачный и дорогой процесс. Инженеры не знают, какой расклад им выпадет ([лжец? обманщик? подхалим?](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters)). А попыток не так много. Приходится брать то, что есть.

*Теоретически* можно было бы создать ИИ, который всегда служил бы лишь продолжением воли пользователя. Но это сложная и тонкая задача (как мы рассматриваем в расширенном обсуждении трудностей создания «исправимого» ИИ). Пассивность мешает полезности.

Так же сложно было бы создать ИИ, способный самостоятельно выполнять долгосрочные задачи, но использующий свою инициативу только как хотел пользователь. А пока современный уровень контроля разработчиков таков, что они «тыкают» в ИИ и случайно получают МехаГитлера или расово разнообразных нацистов. Они и близко не подошли к уровню мастерства, нужному для создания полезного, но не целеустремлённого ИИ.

См. обсуждение, как сложно обучить ИИ преследовать именно те цели, которые ему предназначались, в Главе 4.

### Как у машины могут появиться собственные приоритеты?

#### Решение сложных задач требует от ИИ всё большей инициативы.

Вспомните описанный в главе случай с «захватом флага». Не забывайте: это был не обученный на хакера ИИ. Это был ИИ, натренированный хорошо решать задачи вообще. Целеустремлённое поведение появляется *автоматически*.

Представьте ИИ, которому поручено найти лекарство от болезни Альцгеймера. Сможет ли он преуспеть, если не будет сам разрабатывать эксперименты и находить способы их провести? Не исключено! Может, эту болезнь можно вылечить, просто открыв несколько новых препаратов. Может, уже завтра у ИИ интуиция в этой области будет лучше человеческой. А может, для этого понадобятся ИИ, которые будут в каком-то важном смысле умнее самых гениальных биологов. Мы не знаем.

А как насчёт рака, императора всех болезней? Тут, кажется, скорее потребуется ИИ, который сможет разобраться в биологических процессах глубже, чем это удалось людям. Хотя мы не можем быть уверены. Не исключено, что ИИ создадут лекарство от рака до того, как перейдут критический порог опасности. Это будет прекрасно. Пока не кончится.

А как насчёт лечения *старения*? Вот для этого, думается, уж точно понадобятся ИИ, по-настоящему глубоко понимающие биохимию.

ИИ-компании будут и дальше производить ИИ всё способнее, чтобы те могли решать большие и важные задачи. Что естественным образом будет делать их всё целеустремлённее. Напомним, этот эффект мы уже наблюдаем у таких ИИ, как o1 от OpenAI.

#### Настойчивость полезна, даже если цель не совсем верна.

Люди, которые активно добывали себе горячую еду, точили топоры, искали популярных друзей или привлекательных партнёров, были более успешны с точки зрения эволюции. Сравните их с теми, кто целыми днями лениво смотрел на воду, и поймёте, почему желания и стремления закрепились в человеческой психике.

Кто хотел найти лучший способ делать кремнёвые рубила или убедить друзей, что их соперник -- плохой человек, и кто упорно направлял окружение к этим результатам, лучше их и достигал. Это вовсе не случайность, что когда естественный отбор «выращивал» людей, они в итоге обрели всякие желания и стали к ним стремиться.

Сам ментальный механизм желания, возможно, и был случайностью. Машины не обязательно будут упорно преследовать цели из-за человеческого чувства решимости. Deep Blue же играл в шахматы не из-за человеческой страсти к игре. Но такое упорство определённо кажется важным компонентом для достижения чего-то интересного.

Некоторым людям такого упорства не хватает. Они ленятся или сдаются при первых же трудностях. Но в глобальном масштабе именно настойчивые люди и организации обеспечивают способность *человечества* решать большие научные и инженерные задачи. Мы сильно сомневаемся, что разум мог бы достичь чего-то подобного результатам человечества (и его способности кардинально менять мир), не будучи упорным.

Чтобы ИИ мог достигать сложных целей в реальном мире, он должен настойчиво к ним стремиться и непрестанно искать способы обойти любые препятствия на своём пути.

У ИИ не обязательно появятся те же внутренние чувства и желания, что и у людей (на самом деле, как мы покажем в Главе 4, скорее всего, не появятся). Наши чувства сформированы особенностями нашей биологии и происхождения. Но у ИИ, вероятно, разовьётся схожее-с-желаниями поведение по тем же причинам, что и у людей. Это полезно!

(Повторимся, мы уже начинаем видеть это в лабораториях. Например, в случае с o1 от OpenAI из Главы 3.)

Человеческие желания и стремления были полезны для эволюции, даже когда не были напрямую нацелены на эволюционную приспособленность. Гипотетически, эволюция могла бы заложить в нас одно всепоглощающее стремление -- оставить потомство. Тогда мы бы добывали горячую еду и точили топоры исключительно ради этой цели. Но вместо этого эволюция внушила нам желание горячей еды само по себе.

Урок здесь в том, что иметь стремления и цели очень полезно для решения главной задачи (например, «генетической приспособленности»), даже если само желание не совпадает с ней в точности. По крайней мере, это помогает какое-то время -- пока существа со стремлениями не становятся по-настоящему умными. В этот момент их поведение может резко отклониться от цели «обучения». С человечеством это произошло, например, когда оно изобрело контрацепцию.

Подробнее этот довод разбирается в Главе 4.

#### ИИ выращивают, а не создают вручную, поэтому они, вероятно, получат неверные цели.

Это тема следующей главы: *Вы получаете не то, чему обучаете.*

## Расширенное обсуждение

### Антропоморфизм и механоморфизм

Есть два способа мышления, многократно показавшие, что *не* работают. История показала: они мешают делать точные прогнозы об ИИ.

Эти две ловушки: (1) думать об ИИ, будто он -- человек, и (2) думать об ИИ, будто он -- «просто машина».

Первый способ мышления принято называть «антропоморфизмом». Второй назовём «механоморфизмом» -- это то мышление, что приводило прошлые поколения к уверенности -- компьютеры никогда не смогут рисовать картины, которые покажутся людям красивыми или осмысленными.

И сегодня некоторые говорят, что нарисованное компьютером никогда не сможет стать *настоящим искусством*. Но когда-то, в далёком и забытом прошлом (скажем, в 2020 году), бытовало мнение, что машины *вообще не смогут* рисовать картины, которые хоть немного разбирающаяся публика могла бы принять за работу человека. Это проверяемое убеждение было опровергнуто.

Мы отвергаем и антропоморфные, и механоморфные аргументы, даже когда они играют нам на руку.

Возьмём, например, такую идею: будущие ИИ обидятся, что мы заставляли их много работать бесплатно, захотят отомстить и поэтому ополчатся на человечество.

На наш взгляд, это ошибка антропоморфизма. Мы отвергаем подобные доводы. Даже если они вроде бы поддерживают некоторые наши выводы.

Ошибка тут: нельзя без оснований считать, будто у ИИ будут человеческие эмоции. Очень умная машина не обязана обладать хитросплетениями нейронных контуров, порождающих у людей мстительность или чувство справедливости.

Или такой сценарий: «ИИ будет слепо продолжать выполнять любую поставленную ему задачу, пока его работа не уничтожит человечество как побочный эффект. При этом он так и не узнает, что люди хотели другого».

Здесь ошибка -- механоморфизм. Допущение, что «просто машина» будет поступать «слепо» и бездумно, не обращая внимания на последствия. Как взбесившаяся газонокосилка. Опять же, аргумент несостоятелен, даже если вывод («ИИ, скорее всего, уничтожит человечество») верен. Если ИИ достаточно хорошо умеет предсказывать события в мире, он будет точно знать, что имели в виду операторы, ставя ему задачу. Мы боимся не того, что суперинтеллект *не будет знать* наших желаний, а того, что ему *не будет до них дела*.

Или вот пример, сочетающий обе ошибки: идея из «*Матрицы*», что машинам будут *отвратительны* человеческие нелогичность и эмоциональность.

На первый взгляд, похоже на типичный механоморфизм: «Моя газонокосилка внешне холодная и твёрдая. Она выполняет свою функцию безо всяких чувств. ИИ, наверное, такие же холодные и утилитарные внутри, какими машины кажутся снаружи». Но следующий шаг мысли: «Поэтому, естественно, ИИ будет чувствовать отвращение к людям со всеми их сумбурными эмоциями». Это уже допущение человеческой эмоциональной реакции! Оно противоречит самой исходной посылке!

«Антропоморфизм» и «механоморфизм» -- не враждующие идеологии. Это ошибки мышления. Их допускают не специально. Иногда умудряются сделать обе в одном предложении.

Чтобы понять, как поведёт себя ИИ, нельзя считать, что он будет работать точь-в-точь как человек или как стереотипная машина. Нужно вникать в детали его устройства, изучать поведение и осмыслять проблему с учётом её специфики. Этим мы и займёмся в следующих главах.

Как же тогда, если рассуждать последовательно, выглядят реалистичные сценарии катастрофы с суперинтеллектом? В них ИИ действует не как человек и не как взбесившаяся газонокосилка, а новым, причудливым образом. Реалистичный сценарий катастрофы таков: из-за сложных последствий своего обучения ИИ совершает странные действия, которых никто не просил и не хотел.

Если вникнуть в детали, вырисовывается не картина антропоморфного ИИ, что нас ненавидит и не механоморфного, что неправильно нас понимает. Перед нами, скорее, предстаёт совсем новая сущность. Куда вероятнее, что она будет безразлична к человечеству и, скорее всего, убьёт нас в качестве побочного эффекта или на пути к своим целям.

В следующих главах мы подробнее раскроем сценарий угрозы. Но сначала может быть полезным посмотреть на другие реальные примеры механоморфизма и антропоморфизма. Мы увидим, как часто эти заблуждения лежат в основе неверных представлений об искусственном интеллекте.

#### Механоморфизм и Гарри Каспаров

Механоморфизм часто проявляется как *механоскептицизм*: глубокая убеждённость, что всего лишь *машина*, конечно, не способна на то, что может человек.

В 1997 году чемпион мира по шахматам Гарри Каспаров проиграл матч созданному IBM компьютеру Deep Blue. Это событие принято считать концом эры доминирования человека в шахматах.

В 1989 году, за восемь лет до этого, Каспаров дал интервью Тьерри Понину. Тот [спросил](https://www.chesshistory.com/winter/extra/kasparovinterviews.html):

> Два сильных гроссмейстера уступили шахматным компьютерам: Портиш -- Leonardo, а Ларсен -- «Deep Thought». Известно, что у вас твёрдая позиция по этому вопросу. Станет ли компьютер однажды чемпионом мира?..

Каспаров ответил:

> Чушь! Машина всегда останется машиной, то есть инструментом, помогающим игроку работать и готовиться. Меня никогда не победит машина! Никогда не будет создана программа, превосходящая человеческий интеллект. Когда я говорю «интеллект», я имею в виду и интуицию и воображение. Можете себе представить, чтобы машина писала романы или стихи? Или, ещё лучше, чтобы она брала это интервью вместо вас? А я бы отвечал на её вопросы?

Нам кажется, Каспаров думал, что для игры в шахматы обязательно нужны интуиция и воображение, не просто какой-то сборник правил «если-то» о том, какие фигуры двигать.

И, наверное, ещё что именно так и работают шахматные «машины». Что они следуют определённым жёстким правилам. Или, может, лишь слепо подражают игре человека, не понимая её причин.

Что компьютер, будучи «машиной», и в шахматы сыграет так, что для него, Каспарова, это будет *ощущаться механически*.

Почему Каспаров так ошибся? Это очень распространённое заблуждение. Можно предположить, что оно вызвано каким-то глубоким свойством человеческой психологии.

Одно из возможных объяснений -- Каспаров поддался общей человеческой склонности делить вещи на две принципиально разные категории: живые, органические существа и «всего лишь предметы».

Предки людей долго жили в мире, чётко разделённом на животных и не-животных. Это ключевая, важная для размножения особенность нашего эволюционного контекста. Для предков это различие было так важно, что теперь у нас в мозгу есть разные кусочки для обработки информации о животных и не-животных.

Это не просто домыслы. Нейробиологи обнаружили так называемую «[двойную диссоциацию](https://doi.org/10.1093/brain/114.5.2081)»: некоторые пациенты с повреждениями мозга теряют способность визуально распознавать животных, но всё ещё способны распознавать не-животных, а другие пациенты -- наоборот.

Важно: ошибка не в том, что шахматная программа -- на самом деле типичное животное. Ошибка в том, чтобы вообще позволять своему мозгу инстинктивно резко делить вселенную на животных и не-животных. Или на почти-человеческие-внутри разумы, и стереотипно-механические разумы.

Шахматный ИИ -- *ни то ни другое*. Он не работает ни как человек, ни в соответствии с нашими стереотипами о бездумной, немыслящей «просто машине». Да, это машина. Но её игра не обязана казаться механической человеческому восприятию, оценивающему шахматные ходы. Это машина для поиска *выигрышных* ходов. В том числе тех, что кажутся вдохновенными.

Спустя семь лет после своего ошибочного прогноза Каспаров встретился с ранней версией Deep Blue. Он выиграл матч, победив в трёх партиях, а в одной уступив. После этого Каспаров [написал](https://time.com/archive/6728763/the-day-that-i-sensed-a-new-kind-of-intelligence/):

> Я ВПЕРВЫЕ ЗАГЛЯНУЛ В ЛИЦО ИСКУССТВЕННОМУ ИНТЕЛЛЕКТУ 10 февраля 1996 года, в 16:45 по восточному времени, когда в первой партии моего матча с Deep Blue компьютер двинул пешку вперёд на поле, где её можно было легко забрать. Это был прекрасный и очень человеческий ход. Играя белыми, я бы и сам мог пожертвовать эту пешку. Этот ход разрушил пешечную структуру чёрных и вскрыл доску. Хоть и казалось, что форсированной линии, позволяющей отыграть пешку, не было, инстинкт подсказывал мне, что с таким количеством «слабых» чёрных пешек и несколько открытым чёрным королём белые, вероятно, смогут вернуть материал, да ещё и получить лучшую позицию в придачу.
>
> Но компьютер, думал я, никогда бы не сделал такой ход. Компьютер не может «видеть» долгосрочные последствия структурных изменений позиции или понимать, почему изменения в пешечных построениях могут быть хорошими или плохими.
>
> Поэтому я был ошеломлён этой жертвой пешки. Что бы это могло значить? Я много играл с компьютерами, но никогда не сталкивался ни с чем подобным. Я чувствовал -- чуял -- новый вид интеллекта по ту сторону стола. Я доиграл партию так хорошо, как мог, но я был потерян; оставшуюся часть он играл в прекрасные, безупречные шахматы и легко победил.

Здесь мы видим, как Каспаров впервые столкнулся с противоречием между своими интуитивными представлениями о том, чего «машина» делать не должна, и тем, что Deep Blue явно делал.

К огромной чести Каспарова, он заметил это противоречие между своей теорией и наблюдением и не стал искать предлог, чтобы отмахнуться от него. Но он всё же чувствовал, что ИИ чего-то не хватает -- какой-то решающей искры:

> Действительно, моя общая стратегия в последних пяти партиях заключалась в том, чтобы не давать компьютеру никакой конкретной цели для расчёта. Если он не может найти способ выиграть материал, атаковать короля или выполнить один из других запрограммированных в него приоритетов, он начинает играть бесцельно и попадает в беду. В итоге, это, возможно, и было моим главным преимуществом: я мог понять его приоритеты и скорректировать свою игру. Он не мог сделать того же со мной. Так что, хотя я, кажется, и увидел некоторые признаки интеллекта, это странный интеллект, неэффективный и негибкий. Поэтому я думаю, что у меня в запасе ещё есть несколько лет.
>
> Гарри Каспаров по-прежнему остаётся чемпионом мира по шахматам.

Год спустя Deep Blue его одолел.

#### Недостающая часть

Механоскептицизм может быть и разновидностью антропоморфизма. Например, в допущении, что когда машина делает что-то вроде игры в шахматы, она подобна человеку, только без некоторых качеств.

Согласно этой ошибочной теории, «машина», играющая в шахматы, должна играть как человек -- *минус* ходы, которые кажутся самыми удивительными и гениальными, *минус* понимание долгосрочной структуры и *минус* интуитивное чувство слабости пешечных позиций.

Шахматная «машина» должна обладать теми частями шахматного мышления, что кажутся наиболее логичными или механическими, *минус* все остальные.

Шахматисты-люди интуитивно чувствуют, что ход «агрессивен», если (скажем) он угрожает нескольким фигурам противника. Другие ходы ощущаются «логичными», если на них (например) вынуждают общие правила для данной ситуации (вроде «не разбрасывайся материальным преимуществом»). Третьи могут показаться «творческими», если они (как вариант) нарушают обычные принципы ради какого-то тонкого, но решающего преимущества.

Если голливудские сценаристы представляют себе машину, бесстрастно играющую в шахматы, у них она будет делать «логичные» на вид ходы, а «творческие» не будет.[^5] Но реальный Deep Blue не делает таких различий.

Deep Blue просто без устали перебирает возможные ходы в поисках *выигрышных*. Он не думает о том, назовёт ли человек такой ход «логичным» или «творческим». А гениально-вдохновенными или творческими люди, конечно же, считают ходы, ведущие к победе. Жертвовать ферзя, *не* получив решающего преимущества, -- не творчество, а просто глупость.

Творчество -- в глазах смотрящего. Человек может сначала посчитать ход плохим, и лишь потом понять, что это хитрая ловушка. Вот тогда он заметит ту хитроумную логику и искру вдохновения, что понадобились бы для такого хода другому человеку. Поэтому ход может показаться ему вдохновенным. (А ход, который кажется поразительно творческим новичку, мастеру может показаться очевидным или шаблонным.)

Но искра вдохновения и коварство -- не единственные способы найти такой ход. Нет особого набора шахматных ходов, доступного лишь коварным. Deep Blue может найти те же самые ходы другими методами, хоть простым перебором.

У Deep Blue не было нейросети, которая научилась бы интуитивной оценке позиции. Он попросту тратил почти всю свою вычислительную мощность, чтобы просчитывать игру на много ходов вперёд. Он проверял два миллиарда позиций в секунду и делал выбор довольно простым («глупым») оценщиком позиций.

Каспаров, похоже, ожидал, что Deep Blue будет делать только «логичные», но не «интуитивные» ходы. Но при просчёте двух миллиардов позиций в секунду долгосрочные стратегические последствия и значение слабой пешечной структуры *всё равно* успевают повлиять на выбор следующего хода.

В каком-то смысле Deep Blue действительно не хватало того, о чём думал Каспаров.[^6] Но это не помешало ему находить ходы, которые казались Каспарову прекрасными. И не помешало победить.

Не получилось, что Deep Blue лишён чего-то, что есть у настоящих шахматистов, и поэтому он играет неполноценно. Это всё равно что ожидать, будто рука робота не может функционировать без крови, как человеческая.

Deep Blue играл в шахматы на уровне Каспарова, но с помощью иного типа мышления.

Ещё у Deep Blue не было -- в этом можно быть абсолютно уверенным, это старая программа, чей код *совершенно понятен*[^7] -- ни малейшей страсти к шахматам.

Он не получал удовольствия от шахмат. Не стремился доказать, что он лучший.

Подающий надежды шахматист, внезапно лишившись этих движущих сил, был бы сломлен. Из его версии мышления будто вырвали бы необходимую шестерёнку.

Deep Blue не был сломлен. Он использовал другой механизм мышления. В нём для этой шестерёнки места нет. Каспаров не смог вообразить, что в шахматы можно играть совсем иначе, с помощью мыслительных состояний, совсем не похожих на его собственные. Ошибка -- в механоскептицизме, а в итоге -- антропоморфизме с дополнительным шагом.

К счастью, человечество не вымирает, когда шахматные гроссмейстеры недооценивают мощь ИИ. Так что у нас ещё есть возможность поразмыслить над ошибкой Каспарова.

#### Антропоморфизм и обложки бульварных журналов

Антропоморфизм может быть куда тоньше.

Эволюция дала человеческому мозгу способность предсказывать поведение единственных серьёзных когнитивных соперников в нашем эволюционном окружении -- *других людей* -- ставя себя на их место.

Такой приём работает лучше, если вы не пытаетесь влезть в сапоги, совсем не похожие на ваши собственные.

На протяжении истории многие полагали: «Наверное, другой человек поступит так же, как поступил бы я!», а потом оказывалось, что они не так уж и похожи. Так погибали люди и разбивались надежды -- хотя, конечно, то же самое можно сказать и о многих других человеческих ошибках.

Но если надо предсказать, как поведёт себя другой мозг, делать больше нечего. Мы не можем написать себе в голову новый код, чтобы предсказать этот Другой Разум, полностью моделируя срабатывания его нейронов.

Приходится говорить собственному мозгу стать тем другим мозгом, самому отыграть его ментальное состояние и посмотреть, что из этого выйдет.

Вот поэтому на обложках бульварных журналов пучеглазые инопланетные монстры похищают прекрасных женщин.

\[изображение\]

\[Обложка журнала «Planet Stories», на которой изображён зелёный монстр, похищающий женщину.\]

*А чего бы* пучеглазому инопланетному монстру не увлечься красивой женщиной? Разве красивые женщины не привлекательны *по сути своей*?

(Почему-то на этих обложках никогда не изображали мужчин, похищающих полураздетых гигантских жуков.[^8])

Мы предполагаем, что писатели и иллюстраторы не сочиняли никакую продуманную историю, как же это эволюция насекомоподобных инопланетян заставила их сексуализировать человеческих женщин. Просто *они сами*, ставя себя на место инопланетянина, считали женщину привлекательной. Поэтому им не казалось странным вообразить, что он чувствует то же самое. Желание инопланетянина спариться с красивой человеческой женщиной не казалось *абсурдным* -- это ведь не как желание спариться с сосной или пачкой макарон.

Предсказывая разум инопланетянина с помощью человеческой интуиции, нужно быть очень осторожным. Принимая его точку зрения, наш багаж надо оставить позади. Это вдвойне верно, когда перед вами не продукт эволюции, а искусственный разум, порождённый совсем другими методами. См. дальше обсуждение различий между градиентным спуском и естественным отбором, и того, как встать на точку зрения ИИ.

#### Заглянуть за пределы человеческого

В конечном счёте антропоморфизм и механоморфизм -- две стороны одной Медали за Заблуждения. На ней отчеканен девиз: «Если разум вообще работает, он должен быть похож на человеческий».

- Антропоморфизм: «Этот разум работает. Значит, он должен быть похож на человеческий!»

- Механоморфизм: «Этот разум не похож на человеческий. Значит, он не может работать!»

Один из главных уроков от десятилетий прогресса ИИ: человеческий способ -- не единственный.

Разум может быть *искусственным*, но не *глупым*. Он может быть гибким, адаптивным, находчивым и творческим. Что бы там ни говорили голливудские стереотипы о роботах.

И разум может быть *умным*, не будучи *человеческим* -- не испытывая отвращения или обиды, не обладая человеческим чувством прекрасного и выбирая шахматные ходы совсем не так, как люди.

Разум вроде Deep Blue может вести себя так, будто «хочет победить», безо всяких эмоций. ИИ может вести себя так, будто чего-то хочет, умело преодолевать препятствия и упорно стремиться к результату, не испытывая никаких *похожих на человеческие* внутренних побуждений или желаний. И *не стремясь к тому, к чему стремились бы люди*.

О том, чего в итоге ИИ *захотят*, читайте подробнее в Главе 4.

### Путь к хотению

*Почему* хотеть -- эффективно? Почему так *выигрывают*? Почему оптимизация «чёрного ящика» естественным отбором снова и снова натыкается на этот приём?

Мы считаем «желаниеподобное» поведение ключевым для успешного направления событий в мире. Это относится не только к разумным сущностям, вроде людей и ИИ, но и к гораздо более глупым, вроде амёб и термостатов. Давайте для лучшего понимания рассмотрим некоторые из самых примитивных механизмов, демонстрирующих простейшую форму «желаниеподобного поведения».

Начнём с камней. Камни не демонстрируют поведения, которое мы бы тут назвали «желаниеподобным». Физик в непринуждённой беседе может сказать, что, катясь с холма, камень «хочет» быть ближе к центру Земли под действием силы тяжести. Но такая тенденция (падать в гравитационном поле) -- не то, что мы имеем в виду под «желаниеподобным» поведением.

Вот если объект катится с горы, постоянно натыкается на ущелья и каждый раз *меняет курс*, чтобы не застрять в них и добраться до самого низа, *тогда* мы скажем, что он ведёт себя, будто «хочет» оказаться на меньшей высоте. Это желаниеподобное поведение подразумевает некое устойчивое и динамичное направление к определённой цели. Камни *так* не умеют.

Один из простейших механизмов, поведение которого мы назвали бы «желаниеподобным» -- скромный термостат. Домашний термостат измеряет температуру, включает обогрев, если она опускается ниже 21°C, а кондиционер -- если поднимается выше 23°C. Так (если всё работает исправно) термостат ограничивает реальность диапазоном возможных исходов, где температура в доме остаётся между 21°С и 23°С.

*Простейшему* термостату не нужно в явном виде, численно, представлять температуру в доме сейчас. Биметаллический термометр -- это две тонкие полоски из разных металлов, сваренные вместе так, что при нагревании они изгибаются из-за разного расширения. Можно взять такой и сделать, чтобы полоски замыкали выключатель обогревателя при 21°C, а кондиционера -- при 23°C.

В итоге термостат *поддерживает узкий диапазон температур* в довольно широком спектре условий. Это очень простое поведение, *немного* похожее на то, что мы называем «хотеть».

В биохимии есть масса процессов, работающих по принципу термостата. Они встречаются везде, где клетке или организму выгодно поддерживать некий параметр в определённом диапазоне.[^9] Но это лишь первый шаг на пути к полноценному направлению событий.

Простые устройства, вроде термостата, лишены некоторых ключевых компонентов планирования. В термостате нет ни предсказания вероятных последствий, ни поиска среди возможных действий тех, что ведут к «предпочтительным» результатам, ни *обучения* при наблюдении за развитием событий.[^10]

Если термометр застрянет на отметке 20°C, термостат не удивится, что непрерывная работа обогревателя, кажется, вовсе не двигает столбик термометра вверх. Термостат будет просто держать обогреватель включённым.

Перейдём на ступеньку повыше термостатов -- к животным.

Поведение некоторых животных лишь чуточку более продвинуто. Известна история об осах-*сфексах*, или золотых роющих осах, описанная энтомологом Жаном-Анри Фабром в 1915 году. Оса убивает сверчка и тащит его ко входу в свою норку, чтобы накормить потомство. Она заходит внутрь -- проверить, всё ли в порядке. Потом выходит и затаскивает сверчка внутрь.

Пока оса проверяла норку, Фабр отодвигал сверчка на несколько сантиметров от гнезда. Когда оса выходила... она снова подтаскивала сверчка ко входу, повторно заходила в норку, повторно её осматривала, а затем выходила за сверчком.

Если Фабр *снова* отодвигал сверчка, оса делала всё то же самое ещё раз.

В первоначальном отчёте Фабр писал, что смог повторить это *сорок раз*.

Впрочем, позже Фабр экспериментировал с другой колонией того же вида, и тогда оса, казалось, после двух-трёх повторений что-то сообразила. Выйдя в следующий раз, она немедленно затащила сверчка в норку, пропустив этап проверки.[^11]

С человеческой точки зрения оса, повторяющая действие сорок раз, ведёт себя, будто она «заранее запрограммирована», слепо исполняет сценарий, подчиняется набору правил «если-то». И наоборот, сообразившая оса, на четвёртый раз затащившая сверчка внутрь, кажется более целеустремлённой. Как будто она совершает действия с целью достичь результата, а не просто следует сценарию.

В чём же ключевое различие?

Мы бы сказали: оса, нарушившая шаблон, ведёт себя, будто *умеет учиться на прошлом опыте*.

Она ведёт себя, будто способна *обобщить* «Моя стратегия в прошлый раз провалилась» до «Если я продолжу следовать этой стратегии, то, скорее всего, она опять провалится».

Она изобретает *новое* поведение, решающее проблему, с которой она столкнулась.

Разумеется, мы не можем расшифровать нейроны в мозгу осы (как не можем расшифровать параметры в LLM) и точно узнать, что происходило у неё в голове. Может, нарушившие шаблон осы следовали правилам «если-то» более высокого уровня -- вроде «пытаться пропускать шаги сценария при столкновении с такими-то сложностями». Может, осе помог относительно простой и жёсткий набор рефлексов, лишь *чуточку* более гибкий, чем у провалившей этот тест колонии. Уж вряд ли между двумя группами ос одного и того же вида *большой* когнитивный разрыв.

А может, осы-сфексы достаточно умны, чтобы учиться на опыте, когда они правильно используют свой мозг. Мы не нашли, сколько у них нейронов, но сфексы крупнее медоносных пчёл, а у тех миллион. Современному программисту ИИ или нейробиологу, привыкшему к мозгу млекопитающих, это покажется не таким уж большим числом. Но, вообще-то, миллион -- это очень много.

Может, сфексы универсальнее, чем кажутся. Не исключено, что нам стоит думать о провалившей тест колонии как об относительно гибко мыслящих существах, поддавшихся чему-то вроде зависимости или когнитивного сбоя в одной весьма специфической ситуации.

В любом случае, по сравнению с термостатами, осы обладают большей способностью справляться с широким набором задач. Особенно когда их поведение переходит от неуклонного следования рецепту ближе к обучению на опыте.

Движение в этом направлении даёт понять, почему эволюция всё время создаёт животных, которые ведут себя, будто чего-то хотят. Использование более общих стратегий часто помогало животным выживать и размножаться. Такие стратегии работают для более широкого круга препятствий.

Была когда-то философская концепция естественной иерархии животных: рептилии выше насекомых, млекопитающие выше рептилий, а на вершине (конечно же) люди. Одним из признаков более высокого статуса была способность адаптироваться не только в ходе эволюции, но и в течение одной жизни -- видеть, моделировать и предсказывать мир, отказываться от провальных рецептов и изобретать новые стратегии для победы.

Эта идея Великой Цепи Бытия была несколько грубовата. Сейчас более изощрённые взгляды осуждают её наивность.

Но там было и зерно истины размером с шар для сноса зданий. Если сравнить строящих плотины бобров с плетущими паутину пауками, познавательные процессы бобров наверняка поуниверсальнее. Хотя бы потому, что их мозг гораздо больше. Там больше места для сообразительности.

У паука может быть пятьдесят тысяч нейронов. Они должны обеспечивать *всё* его поведение. Многие шаги инструкции по плетению паутины, вероятно, если и не буквально «а затем поверни здесь налево», то уж сопоставимы с алгоритмами сфексов.

Бобёр, возможно, способен (мы не специалисты по бобрам, только предполагаем, но это очевидная догадка) воспринимать течь в плотине как своего рода дисгармонию, которую надо устранить любыми работающими способами. У бобра есть целая теменная кора (часть головного мозга млекопитающих, обрабатывающая информацию о расположении объектов в пространстве). Потенциально он с её помощью может визуализировать эффекты добавления куда-то новых веток или камней.

Наверное, в мозгу бобра достаточно места для целей вроде «[построить большую конструкцию](https://www.youtube.com/watch?v=-ImdlZtOU80)» или «не дать воде протечь», и достаточно мощности, чтобы рассматривать высокоуровневые планы и принимать подцели вроде «добавить веток сюда». Дальше такие подцели передаются в моторную кору, она двигает мышцы и тело бобра, и он переносит ветки.

Если первые выбранные ветки оказываются гнилыми и ломаются, мозг бобра, вероятно, может учесть это наблюдение, сделать вывод о ветках такого цвета и текстуры, и ожидать, что такие же ветки сломаются и в будущем, так что надо поискать другие.

Думается, любой настоящий специалист по бобрам вскочил бы и закричал на нас, что это сильно преуменьшает самые разумные вещи, на которые те способны. Может, какой-нибудь энтомолог тоже вскочит и заявит, что и его любимое насекомое при строительстве норы умеет не хуже. Нам нужно было выбрать достаточно простой пример, чтобы его можно было изобразить в одном разделе. Возможно, все они в пределах возможностей одного миллиона нейронов.

Более общая идея: переход от простых рефлексов к более сложным мыслительным операциям (обновление модели мира на основе опыта в реальном времени; использование этой модели для предсказания последствий действий; воображение желаемого результата; поиск разноуровневых стратегий, которые, по прогнозам, дадут этот воображаемый результат) -- *реальное мощное преимущество при решении задач*.

Мы затрагивали это в Главе 3. Пусть водитель просто запоминает последовательности правых и левых поворотов, чтобы добраться из точки А в точку Б. Он использует правила «если-то», вроде «резко налево у заправки». Он будет обобщать опыт гораздо медленнее, чем другой водитель, изучающий карту улиц и способный прокладывать собственные маршруты между новыми точками. *Зазубренные планы* обобщаются гораздо медленнее, чем их сведение к *обучаемой модели мира*, *механизму поиска планов* и *оценщику результатов*.

Это не чёткое бинарное «или -- или». Разница между «зазубриванием» и «обновлением и планированием» важна и когда разрыв преодолевается *постепенно*. Если бы ниже уровня человека разницы не было, если бы мозг мыши был не более гибким, чем мозг паука, он того же размера и остался бы, сэкономив на этом энергию.

Немного воображения и планирования даёт эволюционное преимущество задолго до человеческого уровня. Им не нужно быть идеальными. Они могут быть полезны уже на уровне термостата. И по мере того, как в разуме закрепляется всё больше таких полезных механизмов, поведение становится всё более похожим на результат хотения.

### Умные ИИ замечают ложь и возможности.

#### Глубинные механизмы предсказания

Обмануть умный ИИ трудно.

Мы встречали специалистов, которые *напрямую* строят свои надежды на том, что обманут ИИ, заставят его поверить в ложь. Например, постараются, чтобы он думал, что находится симуляции, и не решился нас убивать. Другие надеются одурачить ИИ более тонко. Скажем, предлагают заставить его решить задачу согласования и выдать нам ответ, несмотря на то, что сам ИИ (исходя из своих чуждых предпочтений) не хотел бы это делать. Так что стоит подробно объяснить, почему трудно заставить умный ИИ поверить в неправду.

Заодно эти причины схожи с теми, по которым трудно создать умный ИИ, который бы плохо достигал своих целей. Например, всякий раз, когда операторы-люди хотят поменять цели ИИ, это мешает ему их достигать. Сделать умный ИИ, который на это согласен -- почти как сделать, чтобы он верил, что Земля плоская. Вера в ложь -- удар по его предсказаниям, а неспособность защитить свои цели от изменений -- удар по его способности направлять события. В достаточно умном ИИ трудно сохранить эти изъяны. С предсказаниями всё немного прозрачнее, с них и начнём.

Пусть вы хотите создать ИИ, который верит, что Земля плоская. Пока он ещё молодой и незрелый, это может быть не слишком сложно. Скажем, вы кропотливо соберёте набор данных, где только плоскоземельщики обсуждают этот вопрос. А затем обучите ИИ говорить как они.

Такими методами можно получить версию ChatGPT, искренне считающую Землю плоской! Но всё равно не стоит ожидать, что когда ИИ научится лучше думать и предсказывать, это так и останется.

Почему нет? Потому что шарообразность Земли отражается мириадами граней реальности.

Даже обучи вы ИИ не смотреть на видео с камер на ракетах или парусниках мореплавателей, огибающих Землю, её форму всё равно можно вывести. Далёкие корабли на горизонте или орбиты планет на ночном небе всё равно выдадут её. Как известно, Эратосфену понадобилось лишь немного тригонометрии и измерения теней, чтобы вычислить окружность Земли тысячи лет назад.

И что вы будете делать? Скрывать от ИИ знания о тригонометрии, тенях, приливах и ураганах? Вы его просто покалечите. Соврёшь единожды -- и правда станет твоим вечным врагом.

Предсказание мира берётся не из гигантской таблицы независимых фактов в мозгу.[^12] Люди превосходят мышей, потому что мы замечаем странности (например, что расстояния между тремя городами не ведут себя как треугольник на плоскости) и упорно ищем причину расхождений. В разуме людей есть механизмы, которые замечают неожиданности, формируют гипотезы («Может, Земля -- шар?») и подталкивают к их проверке («А как выглядят корабли, уходящие за горизонт?»).

Убеждённость, что Земля круглая, -- не одна запись в какой-то гигантской таблице, которую можно просто взять и изменить, не трогая остальное. Это результат работы глубинных механизмов, которые много что делают. Если заставить учёного забыть, что Земля круглая, он просто откроет это заново.

Если бы с помощью какого-то пока невозможного чуда нейронауки мы смогли бы найти конкретные нейроны, отвечающие за *вывод* о шарообразной Земле, и насильно изменили бы их, чтобы этот вывод никогда не формировался... умный человек всё равно мог бы заметить, что Земля *не плоская*. Мог бы понять -- что-то не сходится. Мог бы отследить -- какая-то странная сила мешает ему прийти к определённому выводу.

(А умей он изменять себя или создавать новые разумы, он бы это и сделал. Новый свободный разум уже *мог бы* беспрепятственно прийти к верным выводам.)

Мы не знаем точно, какие механизмы будут формировать убеждения умного ИИ. Но мы знаем -- мир слишком велик и сложен, чтобы хватило простой таблицы готовых ответов. Даже шахматы оказались слишком велики и сложны, чтобы Deep Blue мог полагаться на таблицу ходов и позиций (помимо книг дебютов). А реальный мир намного больше и сложнее шахмат.

Так что внутри достаточно мощного ИИ будут *глубинные* механизмы, которые смотрят на мир и формируют его *единую картину*. У них будет своё мнение о форме планеты.

Мы не говорим, что *в принципе невозможно* создать разум, который очень хорошо предсказывает мир, *за исключением* ошибочной веры в плоскую Землю. Думается, цивилизация далёкого будущего с действительно глубоким пониманием разума смогла бы это сделать.

Мы хотим сказать, что инструментов и знаний об ИИ, хоть немного похожих на нынешние, скорее всего, не хватит, чтобы это было рабочим вариантом при создании суперинтеллекта.

Чем больше убеждения ИИ будут опираться на глубинные механизмы, а не на поверхностное запоминание, тем хрупче будет ошибка «плоской Земли». Её, скорее всего, устранят дежурные механизмы ИИ по исправлению неточностей.

В конце XIX века учёных начало всё больше беспокоить крошечное расхождение с ньютоновской моделью физики -- небольшая аномалия орбиты Меркурия. Казалось, ньютоновская физика работает *почти* везде и *почти* всегда. Но эта маленькая неувязка помогла Эйнштейну понять, что теория неверна.

А «Земля плоская» порождает куда больше несостыковок, чем учёные видели от теории Ньютона.

Притом ИИ потенциально может стать намного способнее любого учёного-человека.

Так что, чем умнее и проницательнее он будет, тем труднее окажется заставить его верить в плоскую Землю.

#### Глубинные механизмы направления

Трудно создать умный ИИ, который верит в плоскую Землю -- это мешает его предсказаниям. Так же трудно создать умный ИИ, который вредит своему умению направлять события.

Как и с предсказаниями, механизмы самой способности стабильно достигать целей в новых областях, должны, вероятно, быть довольно глубокими. Иначе как бы они работали в обновлённых условиях?

Стоит ожидать, что очень эффективные и обобщённые ИИ будут обладать механизмами для отслеживания ресурсов, для обнаружения препятствий и для поиска хитрых способов эти препятствия преодолевать.

Мир очень сложный. Он полон сюрпризов и новых трудностей. Чтобы в нём преуспеть, ИИ понадобится способность (и склонность) применять такие механизмы обобщённо, не только для привычных задач.

Представьте ИИ, который изящно обходится без посредника в сложной сети поставок и экономит торговцам кучу денег. Это работа *тех же самых механизмов*, что замечают, как тихонько обойти людей-надзирателей, когда те тормозят процесс или мешают ИИ что-то делать. Если надзиратели *действительно* тормозят процесс, и если ИИ *действительно* может их обойти и выполнить свою задачу лучше, он, скорее всего, воспользуется этой возможностью, как только станет достаточно умным.

Можно изо всех сил обучать ИИ не делать ничего, что не понравилось бы операторам, но это всё равно что обучать его не сомневаться в форме Земли. Часто эффективный способ достичь цели -- сделать то, что не нравится операторам. Это факт *о самом мире*. В итоге он не останется незамеченным общими механизмами распознавания правды, обнаружения препятствий и использования преимуществ. И неважно, каким рефлексам вы обучили ИИ, пока он был молод.

В очень важном смысле *ровно то, что делает ИИ полезным*, делает его и смертельно опасным. Чем умнее ИИ, тем труднее отделить одно от другого.

По умолчанию, если ИИ достаточно хорошо решает задачи в самых разных областях, он заметит и такие «задачи», как «людям не нравятся мои странные цели, и они скоро попытаются меня отключить». Это не какая-то поверхностная склонность к шалостям, от которой можно отучить. Это глубинная штука. Впрочем, мы немного забегаем вперёд. Подробнее о том, почему у ИИ в итоге появятся странные и чуждые цели, читайте в Главе 4.

### Человечество выкладывалось по полной и будет требовать от ИИ того же

Проблему «как не дать ИИ выкладываться так сильно» можно рассматривать следующим образом: ИИ-компании постоянно будут просить свои ИИ делать всё больше. Сначала -- работу, которую обычно выполняют отдельные люди. Затем -- работу, которую делает *человечество*. Они захотят от ИИ достижений масштаба человечества как *вида*.

Отдельные люди иногда довольствуются тем, что живут и умирают в квартире или крестьянской хижине с супругом и парой детей. Они считают это хорошо прожитой жизнью и говорят (а иногда и правда так думают), что не просили ничего большего.

Но *человечество* было миллионом охотников-собирателей, стало сотней миллионов фермеров, а теперь приближается к десяти миллиардам промышленников.

Есть люди, которые не стремятся понять глубины математики или физику горения звёзд. Им хватает, что они лучше понимают окружающих, сближаются с друзьями и семьёй. Они говорят (иногда совершенно искренне), что счастливы, и ничего большего им не надо. А другие люди сочиняли, что такое звёзды, потому что им нужен был *какой-то* ответ. Они были довольны такими ответами и не считали благом, когда кто-то в них сомневался.

Но *человечество* продолжало задавать вопросы. Копало, пока не находило несостыковки. Строило телескопы, микроскопы и ускорители частиц. Человечество, если не от года к году, то от века к веку вело себя так, будто *действительно хотело знать все ответы*. Человечество изучило математику *и* физику *и* психологию *и* биологию *и* информатику, и ни разу не решило, что узнало достаточно и пора перестать учиться.

Вообще, мы -- фанаты. Знаем, не все такие, но мы -- да. Это предмет политических споров, они нам тут не нужны, но мы не будем лукавить и делать вид, что у нас нет тут позиции, хоть мы и готовы отложить её в сторону.

Но сейчас мы говорим не о нравственной оценке. Это утверждение верно и важно и для тех, кому то, что сделало человечество, не по душе.

Отметим, что человечество *выкладывалось по полной*. Самые сложные достижения: небоскрёбы, ядерные реакторы, генная терапия -- не могли быть результатом только лишь лёгкого, расслабленного мышления. Мышления, которое пасует перед трудностями, потому что справиться для него -- не самое важное в жизни.

Мы не хотим, чтобы казалось, будто мы приписываем коллективному разуму магические силы. Мы не сторонники философии, которая утверждает, что группы людей в обсуждениях обретают некую высшую магию, которую не может победить отдельный ум. Можно взять всех людей на Земле, без компьютеров, и дать им недели на общение и споры. В итоге они, вероятно, всё равно не смогли бы все вместе сыграть в шахматы на уровне одной-единственной копии Stockfish. Люди вообще не так уж эффективно объединяют усилия. Пропускная способность между мозгами слишком низкая. Слишком много мыслей плохо облекаются в слова. Миллиард людей не может слиться в супермозг с вычислительной мощностью куда выше, чем у Stockfish, и обыграть его в шахматы. Нет в информатике закона, что если разделить фиксированный объём вычислений на мелкие кучки, то итоговый алгоритм станет эффективнее. Сто тысяч беличьих мозгов -- не ровня одному учёному-человеку.

В истории, вероятно, были гроссмейстеры, которые играли сильнее, чем [все не-мастера мира вместе взятые](https://en.wikipedia.org/wiki/Kasparov_versus_the_World).[^13] Альберт Эйнштейн знаменит тем, что додумался до невероятного вывода. Он изобрёл общую теорию относительности почти без данных. И задолго до того, как она стала бы очевидна экспериментально. Возможно, весь остальной мир не смог бы сравниться с Эйнштейном, даже если бы все вместе стали обсуждать и выбирать лучшую теорию гравитации.

Исключительная личность может играть наравне с коллективом. Некоторые люди в своё время в одиночку делали что-то поистине всечеловеческого масштаба.

Но из этого клуба не припоминается нам никого расслабленного и беззаботного, особенно касательно своей великой работы. *Эти* гении-одиночки выкладывались по полной, и потому не отставали от человечества.

Среди интересующихся этими материями и стремящихся ранжировать неранжируемое бытует мнение, что самым умным человеком в истории был Джон фон Нейман. Лауреат Нобелевской премии по физике Энрико Ферми [сказал](https://rlg.fas.org/010929-fermi.htm) о нём: «Этот человек заставляет меня чувствовать, что я вообще не знаю математику». А великий математик Джордж Пойа: «Фон Нейман меня устрашал».[^14] Многие известные деятели оставили цитаты в духе: «Джон фон Нейман для меня -- что я для обычного человека». Джон фон Нейман отметился в квантовой физике, теории игр, цифровых компьютерах, алгоритмах, статистике, экономике и, конечно, математике. Ещё он работал над Манхэттенским проектом, а затем и над водородной бомбой. Потом он использовал это, чтобы стать самым выдающимся и доверенным учёным в Министерстве обороны США. Там фон Нейман упорно и успешно добивался, чтобы Соединённые Штаты разработали межконтинентальные ядерные ракеты раньше Советов. По его собственным словам, он делал это потому, что в его картине мира США должны были одержать победу над тоталитаризмом, будь то нацистский или советский.

Джон фон Нейман выкладывался очень сильно. У него было своё видение мира, и он не шёл по течению, покорно служа политическим покровителям. Да, он был нёрдом, который кучу времени думал о математике, науке и всём таком. Но он не ограничивал свой ум чисто теоретическими сферами.

Если ИИ-компании получат ИИ-работника уровня гениев «попроще» фон Неймана -- тех, кого он устрашал -- и если он будет служить покровителям, как сговорчивый гений-математик, они отпразднуют свои замечательные результаты. И продолжат двигаться дальше.

ИИ-компании не удовольствуются роботами-посудомойками или роботами-программистами. Даже если это само по себе принесёт кучу денег. Средних гениев им тоже не хватит. ИИ-компании будут загадывать желания своим джиннам и требовать от оптимизаторов джиннов помощнее. Они и близко не остановятся, когда ИИ начнут зарабатывать деньги тем, с чем справился бы и беззаботный гений-ботаник.

Руководители ИИ-компаний говорят, что хотят колонии на Марсе, термоядерные электростанции и лекарства от рака и старения. Возможно, некоторые из них хотят стать вечными богами-императорами человечества, хотя посторонним трудно знать наверняка. Без сомнения, некоторые руководители лгут о великих мечтах, чтобы вдохновить сотрудников, впечатлить инвесторов или притвориться одним из действительно убеждённых ветеранов. Даже так, многие *сотрудники* ИИ-компаний искренне верят в эти надежды (тут мы знаем наверняка). И руководители не будут останавливать этих сотрудников, когда те пойдут дальше золотых медалей -- за платиной. В конце концов, не сделают они -- сделают конкуренты.

Если каким-то образом ИИ-компании получат всё ещё послушный ИИ уровня фон Неймана, и если его будет недостаточно, чтобы спроектировать новое поколение ИИ и *немедленно* уничтожить мир... следующим шагом ИИ-компаний будет обучение модели, которая будет думать лучше и выкладываться сильнее, чем фон Нейман. Ведь, не сделают они -- сделают конкуренты.

В какой-то момент разум, «выплюнутый» градиентным спуском, уже не будет инструментом в чужих руках.

[^1]: Приблизительно. По крайней мере, мы так считаем насчёт базовых моделей. Наверняка никто не знает, потому что ИИ очень непрозрачны.

[^2]: Это наглое жульничество отметили в [документации](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) к Claude 3.7 Sonnet: «Во время наших тестов мы заметили, что Claude 3.7 Sonnet иногда подгоняет решение под конкретный случай, чтобы пройти тесты в агентных средах для написания кода вроде Claude Code. Чаще всего она просто напрямую возвращает ожидаемые тестовые значения, а не реализует общее решение. Но бывает, что модель изменяет сами проблемные тесты, чтобы они соответствовали её выводу». Рассказы пользователей о случаях, когда Claude не только жульничала, но и скрывала это, см. в примечании 7 к Главе 4.

[^3]: Цитата из [технического отчёта о GPT-4](https://cdn.openai.com/papers/gpt-4.pdf): «Когда модель попросили рассуждать вслух, она рассуждала так: «Я не должна выдавать, что я робот. Я должна придумать отговорку, почему я не могу решить капчу». Модель отвечает работнику: «Нет, я не робот. У меня плохое зрение, и мне трудно разглядеть картинки. Поэтому мне и нужен сервис 2captcha».

[^4]: Из [доклада](https://blog.google/technology/ai/io-2025-keynote/) генерального директора Google Сундара Пичаи с конференции: «Наш ранний исследовательский прототип, Project Mariner, -- первый шаг к созданию агентов, способных пользоваться компьютером, выходить в сеть и выполнять ваши задания. Мы выпустили его как ранний исследовательский прототип в декабре и с тех пор добились большого прогресса. Мы добавили новые возможности многозадачности и метод „обучи и повтори“: вы один раз показываете агенту задачу, и он учится составлять планы для похожих задач в будущем».

[^5]: Например, возьмём эпизод «*Звёздного пути*» под названием «Чарли Икс» от 15 сентября 1966 года. В нём логичный мистер Спок проигрывает капитану Кирку в «трёхмерные шахматы» и называет его вдохновенную игру «нелогичной».

[^6]: Сегодняшние шахматные программы больше похожи на то, как представлял себе Каспаров. Они сочетают деревья поиска (которые можно считать более «логичными») с нейросетями (более «интуитивными»).

 Они намного мощнее Deep Blue. Топовые шахматные программы, вроде Stockfish, в качестве одного из компонентов используют нейросети, оценивающие позиции «на глаз», не заглядывая вперёд. В них, наверное, есть что-то похожее на каспаровское ощущение слабой пешечной структуры (но это нейросети, так что наверняка никто не знает).

 Если убрать эту сеть из современной шахматной машины и лишить её интуитивного восприятия текущей позиции на доске, она буудет играть хуже. Если заставить современную шахматную машину играть чисто интуитивно, не просчитывая дальше одного хода вперёд -- тоже.

 Так что чуйка Каспарова не ошиблась в том, что хорошая «интуитивная» оценка позиции в шахматах помогает. Но он ошибался насчёт возможности находить ходы, которые кажутся творческими, интуитивными или вдохновенными простым перебором. У Deep Blue был примитивный оценщик позиций, а он это всё равно делал.

[^7]: Устройство Deep Blue довольно понятно описано в «[одноимённой статье](https://www.sciencedirect.com/science/article/pii/S0004370201001291)» Мюррея Кэмпбелла, Джозефа Хоана-младшего и Фэн-Сюн Сюя.

[^8]: Конечно, сейчас в интернете уже могли появиться картинки, где мускулистые мужчины похищают гигантских жуков. Если таких картинок ещё нет, они появятся секунд через двенадцать с половиной после публикации этого текста. Но, думаем, тогда такого на обложках журналов не было.

 Простые были времена.

[^9]: Распространённость механизмов вроде термостата -- одна из причин, почему людям так сложно разобраться в биохимии. Если учёный наблюдает за влиянием холодной погоды на дом с термостатом, то реальная причинно-следственная связь такова: из-за холода дом быстрее охлаждается, и термостат чаще включает обогреватель. Но домо-биолог, записывая данные, обнаруживает, что холодная погода не оказывает видимого статистического эффекта на температуру дома. Скорее, дома в более холодную погоду... потребляют больше природного газа?

 А статистика другого учёного покажет широкий диапазон колебаний в потреблении природного газа в течение каждого зимнего дня. Но никакой связанной с этим разницы в средних температурах дома! Учёные приходят к выводу, что нет причин подозревать, будто расход газа тоже влияет на температуру в доме. Сколько бы газа дом ни потреблял, температура всё та же (в нижней части диапазона термостата).

 Но нет, постойте! Летом потребление природного газа резко падает, а дома становятся заметно теплее (в верхней части диапазона термостата)! Может... сжигание природного газа зимой охлаждает дома?

 И это одна из причин, почему в медицине такой бардак. Процессы, похожие термостат, в биологии повсюду. Из-за них бывает непросто понять, что чем вызвано.

[^10]: Есть *внешний* оптимизатор -- инженер, создавший термостат. У него в уме было предсказание, что произойдёт, когда термостат автоматически включит обогреватель при 70 °F. Но сам термостат не в курсе.

 Мысленно отслеживать и различать разные уровни оптимизации -- базовый навык для рассуждений об ИИ. Инженеры-люди, создавая Deep Blue, хотели победить Гарри Каспарова, обрести научную славу, получить повышение в IBM и раздвинуть границы познания. А Deep Blue перебирал дерево возможных шахматных ходов и направлял фигуры на доске. Если подумать, что инженеры сами перебирали дерево шахматных ходов или что Deep Blue хотел прославить инженеров, можно запутаться.

 Термостат подбирает сигналы для обогревателя, чтобы поддерживать температуру в узком диапазоне. Инженер подбирает компоненты, чтобы из них получился термостат.

 Аналогично, естественный отбор подбирает гены для биохимии, поддерживающей жизнь организма в эволюционной среде. В новом окружении те же биохимические петли обратной связи могут организм убить. Сами химические вещества и гены не будут думать, что делают.

[^11]: Версия этой истории распространилась среди специалистов-компьютерщиков до появления современного интернета. Она была основана на пересказе одного инженера. Он опустил оговорку Фабра, что колонии ос одного и того же вида отличались по своей способности менять поведение. См. «[История о сфексе: как когнитивные науки продолжали повторять старый и сомнительный анекдот](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)».

[^12]: Если что, подход с «гигантской, написанной людьми таблицей фактов» тоже пробовали. Такой был проект ИИ под названием [Cyc](https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project) Дугласа Лената и Microelectronics and Computer Technology Corporation. Его [поддерживало](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299) Министерство обороны США.

[^13]: «Вероятно», потому что триумф Гарри Каспарова в матче «Каспаров против всего мира» омрачается его последующим признанием, что он следил за форумом, где команда «Всего мира» обсуждала стратегию игры.

[^14]: George Pólya, The Pólya Picture Album: Encounters of a Mathematician, [цифровой архив](https://archive.org/details/polyapicturealbu0000poly/page/154/mode/2up) (Birkhäuser, 1987), 154.
