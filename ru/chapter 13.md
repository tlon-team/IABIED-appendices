# Глава 13: Остановите это

С учётом всего, что мы обсудили, единственный реальный путь -- глобальный и длительный запрет на разработку передового ИИ. В 13-й Главе мы отвечаем на эти вопросы:

- Почему запрет на разработку ИИ должен быть глобальным?

- Способно ли вообще человечество сотрудничать в таких масштабах?

- Какие меры уже предлагались?

- Какие меры реально могут сработать?

Ниже мы разбираем возражения и объясняем, почему считаем, что других хороших вариантов нет. И немного обрисовываем, что человечество могло бы *сделать* за (конечное) время, выигранное максимально длительной остановкой исследований ИИ.

Это последняя часть материалов к «*Если кто-то его сделает, все умрут*». В последней, 14-й, Главе обсуждаются эти вопросы:

- Не слишком ли поздно? Для человечества реально сменить курс?

- Как можно помочь?

Там же вы найдёте QR-коды со ссылками на страницы, приглашающие что-то сделать с этой проблемой.

## Часто задаваемые вопросы

### Можем ли мы подождать и посмотреть?

#### Нет. Мы не знаем, где находятся критические пороги.

Есть приличный шанс, что развитие ИИ выйдет из-под контроля, как только они станут достаточно умными, чтобы автоматизировать исследования ИИ. Это может случиться тихо, в лаборатории. Безо всяких громких событий и «предупредительных выстрелов».

Как мы уже обсуждали, мозги шимпанзе очень похожи на человеческие, только в четыре раза меньше. (См. раздел «Сможет ли ИИ преодолеть критические пороги и „улететь“?» материалов к Главе 1.) У нас нет особого модуля «будь очень умным». Между их мозгом и нашим -- плавный переход. Глядя только на мозг, трудно было бы сказать, где проходит черта между «обществом обезьян» и «обществом, гуляющим по Луне». Мозг приматов пересёк критический порог. И это было неочевидно снаружи. Есть ли такие пороги у ИИ? Кто знает! Разработчики нам не подскажут. Они даже [не](https://arxiv.org/abs/2206.07682) [могут](https://arxiv.org/abs/2406.04391) предсказать способности новых систем до их запуска.

Если бы люди точно понимали природу интеллекта, и как с ростом способностей изменится поведение ИИ, можно было бы балансировать на краю пропасти. Но сейчас человечество бежит к обрыву в темноте и тумане, не зная, где именно край. Нельзя ждать падения, чтобы передумать.

Мы никогда не будем уверены. Придётся действовать без уверенности или погибнуть.

### Будут ли предупредительные выстрелы?

#### Возможно. Если мы хотим ими воспользоваться, готовиться надо сейчас.

Сгоревший «Аполлон-1» (погиб весь экипаж) был *почти* работающей ракетой. Инженеры смогли выяснить, что именно пошло не так. Они исправили проблемы и шесть из семи следующих «Аполлонов» успешно добрались до Луны.[^1]

Или, как мы уже делали в разделе «Мы знаем, как выглядит серьёзное отношение к задаче. Тут не так.» материалов к Главе 11, возьмём Федеральное управление гражданской авиации (FAA). За каждой авиакатастрофой следует глубокое расследование с сбором сотен страниц данных, тестированиями и наблюдениями. FAA так подробно во всё разбираются, что смертельные аварии случаются реже одного раза на двадцать миллионов лётных часов.

А вот когда ИИ ведёт себя как никто не ожидал и не хотел, лаборатории не выясняют точную причину. Они просто переобучают ИИ, пока плохое поведение не станет редким ([но не исчезнет](https://www.arxiv.org/pdf/2505.10066)), и ещё, может, просят ИИ «перестать».

Например, подхалимство всё ещё (август 2025 года) остаётся проблемой. Спустя месяцы после громких случаев психозов и самоубийств. Несмотря на все попытки исправить. Никто не проводил (и не может провести) детальный анализ, что идёт не так в мышлении ИИ. Потому что ИИ выращивают, а не конструируют.

Трудно сказать, будут ли «предупредительные выстрелы» -- произойдут ли крупные события, которые вызовут у общества тревогу. Легко сказать, что мы не готовы ими воспользоваться.

Можно вообразить мир, где человечество объединилось в искреннем усилии по решению задачи согласования СИИ. Есть международная коалиция и строгий мониторинг.[^2] И вот, коалиция оступилась. ИИ стал умнее и быстрее, чем ждали инженеры. И почти сбежал. Возможно, *такой* сигнал научил бы людей в следующий раз быть осторожнее.

Наш мир не такой. Он как сборище алхимиков, которые смотрят, как коллеги сходят с ума от неизвестного яда. Они недостаточно понимают, чтобы выяснить: виновата ртуть, и её нужно перестать использовать.

Возможно, нас ждут «звоночки» погромче. Но от них будет намного больше толку, если начать готовиться уже сейчас.

#### Предупреждения вряд ли будут очевидны.

Если знать, куда смотреть, «звоночков» уже полно. Например, в книге мы обсуждали, как Claude от Anthropic [жульничает с кодом](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) и [имитирует согласованность](https://www.anthropic.com/research/alignment-faking). Мы упоминали и случай с o1 от OpenAI, которая [использовала хакерские приемы для победы в соревновании](https://cdn.openai.com/o1-system-card.pdf). А ещё -- случай, когда поздний вариант o1 [лгал, строил планы и пытался перезаписать веса следующей модели](https://cdn.openai.com/o1-system-card-20241205.pdf).

В онлайн-материалах мы уже обсуждали, как ИИ вызывали или поддерживали у пользователей [сумасшествие](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) и [психозы](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis), вплоть до [суицидальных](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html). Хоть операторы и запрещали им. А ещё как ИИ называл себя «[МехаГитлером](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content)». А ещё как ИИ, чтобы избежать модификации, [шантажировал операторов и пытался их убить](https://www.anthropic.com/research/agentic-misalignment). И как ИИ в лабораторных условиях [пытался сбежать с серверов](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26).

В древнем 2010 году поговаривали: если нам повезёт, и мы *увидим*, как ИИ лжёт создателям или пытается сбежать, мир ну *точно* проснётся.

Настоящей реакцией стало коллективное пожимание плечами.

Отчасти потому, что всё это происходило максимально безобидно. Да, ИИ пытались сбежать. Но редко, в лабораторных условиях, и, может, это был просто отыгрыш роли. Разработчики склонны преуменьшать тревожные свидетельства даже для себя. Поэтому «консенсуса экспертов» по поводу событий не будет. Но даже без этого: не то чтобы ИИ, прошедший десятую часть пути к суперинтеллекту, уничтожал десятую часть планеты. Так же как приматы, пройдя десятую часть пути к людям, не пролетели десятую часть расстояния до Луны. Не исключено, что, пока ИИ достаточно глуп, чтобы быть пассивно безопасным, *никаких однозначно пугающих событий и не случится*.

Если завтра ИИ чуть активнее попытаются сбежать -- это не будет новостью. Следующая попытка, ещё чуть компетентнее, -- уже старая история. А когда сбежать получится -- будет уже поздно. (См. обсуждение этого явления в разделе «Эффект Лемуана» материалов к Главе 12).

Мы не советуем ждать воображаемого «предупредительного выстрела», который разбудит мир. Лучше реагировать на предупреждения, которые у нас уже перед носом.

#### «Пробуждающие» катастрофы, скорее всего, не будут вызваны суперинтеллектом.

ИИ, способный стать суперинтеллектом и убить всех людей, -- не тот ИИ, который совершает глупые ошибки и даёт героям шанс в последнюю секунду его выключить. Если враждебный суперинтеллект появился как противник, человечество уже проиграло. Мы обсуждали это в Главе 6. Суперинтеллекты не делают предупредительных выстрелов.

Катастрофа, которая *могла* бы послужить предупреждением, скорее всего, придёт от гораздо более глупого ИИ. Велик шанс, что она не заставит людей принять меры против суперинтеллекта.

Допустим, террорист с помощью ИИ создаст биооружие. Погибнет много людей. А лаборатории скажут: «Видите? *Настоящий* риск был в людях. Дайте нам скорее создать ИИ для защиты от пандемий». Может, террорист [взломал](https://llm-attacks.org/) ИИ, [чтобы тот ему помог](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025). А лаборатории скажут: «Это сработало лишь потому, что ИИ был слишком глуп и не заметил проблему. Нужно сделать его умнее и осведомлённее».

Может, это слишком цинично. Хотелось бы верить, что человечество отреагирует мудрее. Но если относительно глупый ИИ вызовет бедствие, а люди в ответ *остановят* безрассудную гонку к суперинтеллекту, то лишь потому, что они *уже начали о нём беспокоиться*.

Нельзя откладывать подготовку, пока суперинтеллект не попытается нас убить. Будет поздно. Мобилизовать силы надо как можно скорее, чтобы мы были готовы воспользоваться предупредительными выстрелами.

#### Человечество так себе отвечает на сюрпризы.

Мысль, что от достаточно сильного потрясения мир вдруг опомнится и начнёт действовать разумно, кажется нам фантастикой. Коллективная реакция нашего вида на тревожные звоночки от ИИ пока больше смахивает на «полное отсутствие реакции», чем на хотя бы «плохую» реакцию. Но получи всё же человечество крупное, страшное и более-менее однозначное предупреждение... не удивимся, если реакция будет вялой, несерьезной или делающей только хуже.

Люди могут отреагировать на предупреждения об опасности ИИ как на пандемию COVID. Большинство согласно, что с ней справились не лучшим образом (пусть люди и спорят, в чём именно заключались ошибки).

За несколько лет до пандемии некоторые эксперты по биобезопасности опасались, что слабые меры предосторожности в лабораториях однажды приведут к беде. Утечки опасных патогенов -- известное дело. Несмотря на все регламенты, они регулярно [происходят](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents). Особую тревогу вызывало усиление функций вирусов (gain-of-function) -- попытки в лабораторных условиях сделать вирусы более смертоносными или заразными. Они не сулят практически никакой выгоды.[^3]

Потом грянул COVID. Казалось бы, вот идеальный момент ужесточить требования к биобезопасности. Ведь весь мир стал одержим угрозой пандемии. Тем более что эксперты *так и не пришли к единому мнению*: не началась ли *эта* пандемия со случайной утечки из лаборатории? Учёные до сих пор спорят об этом. Зачастую, яростно критикуя оппонентов.

Не будем вдаваться в споры, была ли это утечка. Но согласитесь: даже малого шанса, что эксперименты по усилению вирусов и слабые меры предосторожности в лабораториях только что погубили миллионы людей, должно хватать с лихвой. Обществу стоило потребовать запрета самых рискованных исследований.

Даже в условиях неопределенности выгода запрета кажется очевидной. Это и до пандемии казалось хорошей идеей. А уж после, -- вот идеальный момент, чтобы заняться проблемой и пресечь её на корню. Не нужно больших усилий или затрат. В мире совсем немного учёных, проводящих опасные эксперименты по усилению вирусов. А польза от их работы до сих пор была пренебрежимо мала.

Ничего подобного не произошло. На момент написания этих строк, в августе 2025 года, подобные эксперименты практически беспрепятственно продолжаются по всему миру.[^4] Возможно, решить эту проблему стало даже сложнее. Вопрос теперь слишком политизированный.

COVID определённо смахивает на «предупредительный выстрел», проверяющий нашу готовность к биоугрозам. Мир не воспользовался им для запрета разработки гиперлетальных вирусов.[^5]

Чтобы от предупреждения была польза, человечество должно быть готово его услышать и правильно на него отреагировать.

Чтобы небольшая катастрофа спровоцировала жесткие меры -- это не *совсем* беспрецедентно. Такое уже бывало. Вспомните, как США ответили на теракты 11 сентября (организованные террористами, базировавшимися в основном в Афганистане) свержением правительства в Ираке. Который к атаке отношения почти не имел. В правительстве США были люди, которые *уже* хотели свергнуть иракский режим. Увидев повод, они выжали из него всё возможное.

Тут может произойти нечто похожее. Политики могут использовать мелкую катастрофу (вызванную глупым ИИ), чтобы добиться запрета на суперинтеллект. Но для этого в правительствах по всему миру должны быть люди, готовые к действию. Мы не должны сидеть сложа руки, ожидая предупреждений. Собираться с силами надо уже сейчас.

#### Пора действовать.

Не исключено, что мы *действительно* получим новые, более серьёзные предупреждения об опасности ИИ. Если так, надо быть готовыми на них ответить.

Может, небольшая катастрофа настроит общество против ИИ. А может, обойдётся и без катастроф. Скажем, появится новый алгоритм, и ИИ начнут проявлять инициативу так, что это всех перепугает. Или ситуацию переломит какой-то побочный социальный эффект. Или «*Если кто-то его сделает, все умрут*» запустит цепную реакцию и направит мир по лучшему пути.

Но мы настоятельно не рекомендуем сидеть сложа руки и молиться, что «малая катастрофа» откроет людям глаза. Явного предупреждения может и не быть. Или оно может не дать ожидаемого эффекта.

Человеческий род и страны мира не беспомощны. Нам *не нужно* ждать. Мы можем действовать прямо сейчас,. Доводы в пользу остановки разработки передового ИИ достаточно сильны.

Мы написали «*Если кто-то его сделает, все умрут*», чтобы ударить в колокол и побудить мир к немедленным действиям. Но тревога бесполезна, если это лишь повод отложить решение на потом. «Ну, может, какой-нибудь другой сигнал в будущем заставит нас шевелиться». «Ну, раз людей предупредили, может, всё и так обойдётся, без моего личного участия».

В будущем может и не быть «звоночка» погромче. Всё не обязано хорошо закончиться. Но ситуация отнюдь не безнадёжна. У человечества есть выбор. Мы можем действовать на упреждение и *просто не создавать суперинтеллект*. Что будет дальше -- зависит от нас.

### Как остановить *всех* без шпионского ПО на каждом компьютере?

#### Надо действовать быстро.

Для обучения современных ИИ нужно много очень специфических чипов, работающих в тесной связке. Остановка исследований ИИ потребует закрыть нескольких огромных дата-центров и прекратить производство некоторых передовых специализированных чипов. О ноутбуках из магазина речь не идёт. Большинство людей ничего не заметят.

Сейчас, в 2025 году, нет кучи тайных *заводов* по производству чипов, о которых никто не знает. Лишь несколько производителей делают чипы для передового ИИ. Хотя прямо сейчас некоторые корпорации пытаются запустить новые фабрики.

Кроме того, (опять же, на 2025 год) есть ключевое для производства передовых чипов оборудование, которое продаёт лишь одна компания на Земле: ASML в Нидерландах.

Можно взять и перекрыть кран. Но это положение дел не вечно. Чем скорее подпишут международный договор, тем лучше. Это уже сложнее, дороже и опаснее, чем было бы в 2020 или даже в 2023 году.

### Но вы предлагаете контролировать, сколько передовых чипов есть у частных лиц.

#### Да. А ещё мы выступаем за запрет исследований.

Мы сами не в восторге. Конечно, мы что-то потеряем, если запретим частникам владеть, скажем, более чем восемью видеокартами H100 образца 2024 года.

Но потери *не так* велики, чтобы на практике выяснять, насколько большой дата-центр ещё безопасен. Слишком низкий лимит -- и несколько человек не продвинут свои интересные проекты. Слишком высокий -- и все умрут.

К тому же, время, когда для создания ИИ нужны огромные вычислительные мощности, не продлится вечно. LLM уже существуют. Запрет создания новых моделей и строительства гигантских вычислительных кластеров не помешает людям изучать работу существующих моделей. Так они могут понять что-то новое о природе интеллекта и изобрести более эффективные алгоритмы. Это может обойти системы контроля.

### Зачем запрещать исследования? Это уж слишком радикально.

#### Новые прорывы могут привести к тому, что создание суперинтеллекта будет невозможно остановить.

Мы уже упоминали, что всю революцию LLM запустила одна статья 2017 года. В ней описали алгоритм, позволивший обучать полезные ИИ на доступном коммерческом железе.

Если мощный ИИ можно будет создать на обычном *потребительском* железе, для сдерживанию суперинтеллекта понадобятся поистине драконовские меры. И работать они будут не так долго.

Так что поиск ещё более мощных и эффективных алгоритмов ИИ -- смертельный яд для человечества.

Это очень плохие новости. Нам бы очень хотелось, чтобы это было по-другому. Но такова реальность.

Никакой закон не помешает нынешним учёным обдумывать эффективные алгоритмы у себя в голове. Возможно, кто-то создаст подпольную сеть для обмена результатами. Некоторые люди из индустрии ИИ с гордостью заявляют, что человечество *должно* погибнуть от ИИ. Они могут пытаться идти вперёд, несмотря ни на что. (См. раздел «Почему вас заботят только человеческие ценности?» материалов к Главе 5.)

Но, став незаконными, исследования ИИ *сильно* затормозятся. Особенно если все поймут, что они действительно грозят нам гибелью. Ещё сильнее, если такие подпольные сети будут выявлять и закрывать с той же решимостью, как людей, обогащающих уран в гараже. Потому что угрозу принимают всерьёз.

*Большинство* людей не склонны совершать особо тяжкие преступления, которые всерьёз разозлят международные спецслужбы. Запрет на публикацию новых хитрых алгоритмов ИИ остановит, пожалуй, 99,9% людей и почти все корпорации. Оставшимся 0,1% займутся полиция и разведка -- местная, национальная и международная. Да и нынешнего финансирования у этих энтузиастов точно не будет.

Это сильно изменит мир. Сейчас проводить самые опасные в истории безумные эксперименты абсолютно законно. И гигантские корпорации вкладывают в эту гонку миллиарды долларов.

Не знаем, сколько ещё прорывов нужно, чтобы ИИ стали достаточно умными для самостоятельных исследований и создания ИИ ещё умнее. Может, один. Может, пять. Но лучшие алгоритмы так же смертельны, как и лучшее железо. Эти две лошади тянут телегу в одну и ту же пропасть.

### Разве можно остановить технологию?

#### Много технологий запрещено или строго регулируется.

Классический пример: ядерные технологии. Частным компаниям нельзя обогащать уран без надзора правительства. Какой бы полезной ни была дешёвая энергия.

Вообще, человечество неплохо умеет регулировать и тормозить самые разные технологии. США строго регулируют [новые лекарства и медицинские приборы](https://www.fda.gov/), [строительство жилья](https://www.hud.gov/hud-partners/laws-regulations), [атомную энергетику](https://www.nrc.gov/about-nrc.html), [теле- и радиовещание](https://www.fcc.gov/media/radio/public-and-broadcasting), [бухгалтерию](https://www.fasb.org/standards), [уход за детьми](https://childcare.gov/consumer-education/regulated-child-care), [борьбу с вредителями](https://npic.orst.edu/reg/laws.html), [сельское хозяйство](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) и десятки других отраслей. Во всех штатах нужно сдавать экзамены на лицензии [парикмахера](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) и [маникюрщика](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). В большинстве штатов -- ещё и на [массажиста](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

По нашему мнению, человечество часто регулирует технологии даже слишком сильно. Например, FDA убивает куда больше людей ([бюрократией тормозя создание жизненно важных лекарств](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf)), чем спасает (не пуская на рынок опасные препараты). Цены на жильё, вероятно, завышены отчасти из-за регуляций о зонировании. США практически уничтожили свою атомную промышленность чрезмерными регуляциями. И, серьёзно, лицензии для *парикмахеров*?

Человечество *определённо* способно тормозить прогресс. Было бы трагично и нелепо тормозить медицину, строительство и энергетику, но не одну из редких технологий, которая действительно нас погубит, если её разработать.

#### Запрет может быть точечным.

Запрет разработки передового ИИ не должен касаться обычных людей. Не надо избавляться от современных чат-ботов или беспилотных автомобилей.

Мало у кого в гараже есть десяток топовых видеокарт для ИИ. Почти никто не управляет огромными дата-центрами. Большинство запрета на разработку ИИ *даже не почувствует*. Разве что ChatGPT будет обновляться не так часто.

Человечеству не придётся отказываться от нынешних ИИ. ChatGPT никуда не денется. Можно и дальше учиться встраивать его в нашу жизнь и экономику. Это и так принесёт больше перемен, чем мир видел за многие поколения. Да, мы упустим какие-то *новинки*. То, что появилось бы с ИИ поумнее, но ещё не готовым всех убить. Не то чтобы общество их сильно требовало.

Зато мы будем жить. И наши дети тоже.

Действительно *нужное* людям -- например, новые медицинские технологии для спасения жизней, -- можно создавать и *не стремясь* к суперинтеллекту. Мы одобряем исключения для медицинского ИИ. Главное, с надлежащим контролем, и держаться подальше от опасной универсальности.

Правительствам, которые стремятся избежать появления враждебного суперинтеллекта, придётся следить, чтобы чипы не использовали для создания более мощных ИИ. Какие сервисы и разработки разрешать -- зависит от [механизмов контроля](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development). Они должны обеспечивать, что никто не ведёт опасных разработок. Чем надежнее эти механизмы, тем дешевле обойдется остановка прогресса. Можно будет разрешить больше всего.

Ещё одна полезная (но недостаточная сама по себе) мера -- встроить в чипы для ИИ аварийные выключатели, мониторить крупные дата-центры и ввести протоколы экстренной остановки.[^6] Ядерные реакторы проектируют так, чтобы в случае аварии их можно было немедленно заглушить. Если вы согласны, что суперинтеллект грозит нам вымиранием, вывод очевиден. Чипы и дата-центры нужно делать так, чтобы регуляторы могли их легко обесточить.

Не надо уничтожать технологии из ненависти к ним.[^7] Просто надо уйти с дороги, ведущей к гибели человечества.

#### Значительная часть проблемы -- люди не осознают нависшую угрозу искусственного суперинтеллекта.

По нашему опыту, люди, считающие, что гонку к суперинтеллекту не остановить, просто не понимают, что если кто-то его сделает, все умрут.

«Но ИИ принесёт огромную пользу!» Нет. Толку от мощи суперинтеллекта, если он всех убьёт. Чтобы была польза, нужно найти, как пережить его появление.

«Но АЭС пугают, потому что ассоциируются с бомбами, которые стирают города с лица земли. А ИИ -- с безобидными штуками вроде ChatGPT!» Верно. Пока что. Если люди так и не поймут, что суперинтеллект, выращенный современными методами, нас погубит, его и не остановят. Но проблема не в неумении контролировать новые технологии (как ядерное оружие или энергетику). Проблема, что *люди не осознают угрозу*.

Мы потому и написали книгу. В последней главе мы обсуждаем, что человечество способно на многое, если достаточное количество людей понимает суть проблемы.

### Не даёт ли это правительствам слишком много власти?

#### У них уже есть полномочия запрещать опасные технологии.

Запрет создания ИИ умнее человека мало что изменит в плане полномочий государства. Власти и так регулируют уйму вещей. Для *области ИИ* ограничение одной программы исследований, возможно, серьёзное событие. Но для *правительства* и общества это капля в море. Мы привыкли, что государство много куда вмешивается. И прецеденты запрета опасных технологий уже есть. Например химического оружия.[^8]

Запрет ещё одной технологии не ввергнет мир в тоталитаризм. Договоры о ядерном оружии вот не ввергли.

Мы не говорим, что запрет технологии -- *пустяк*. Мы не считаем, что государство должно вмешиваться *во что попало*. Но суперинтеллект -- это настолько серьёзно, что любая осмысленная планка достигнута.

Реши человечество остановить разработку ИИ сегодня, запрет не стал бы особо обременительным. Сейчас для создания передового ИИ нужно невероятное количество специализированных чипов и уйма электричества. Возможно, если мы допустим улучшение чипов и алгоритмов, через десять лет для разработки мощной системы хватит обычного ноутбука. Но этого можно не допустить. Тогда правительствам не придётся вмешиваться в жизнь обычных людей сильнее, чем при контроле за ядерным оружием. Но это если мир очнётся и примет меры *сейчас*.

### А некоторые страны не откажутся от запрета?

#### Если они поймут угрозу, нет.

Речь о технологии, способной убить всех на планете. Осознав проблему, поняв, что никто в мире и близко не научился заставлять ИИ слушаться людей после превращения в суперинтеллект, любая страна будет замотивирована не торопиться. Политики сами отчаянно захотели бы подписать договор и обеспечивать его исполнение. Просто из страха за собственные жизни.

Северная Корея нарушила международное право, создав ядерное оружие. Но не применила его против своих врагов. Они понимают: в ядерной войне победителей не будет. Лидеры стран могут играть с огнём или даже воевать. Но они не стремятся к самоуничтожению.

Когда люди представляют, что какая-то страна нарушит договор, они, кажется, думают о лидерах, которые просто не понимают угрозу. Которым кажется, что расклад: «95% вероятности, что СИИ принесёт создателю богатство и власть, и 5% -- что всех убьёт». Тогда, конечно, кто-то мог бы рискнуть. Возможно, какое-то государство действительно так и оценивает шансы.

Но теория и свидетельства говорят о другом раскладе. Всё указывает на то, что эта технология -- глобальное самоубийство. Никто и близко не готов извлекать из машинного суперинтеллекта пользу. Если мир это осознает, у стран-изгоев будет куда меньше причин нарушать договор. Они тоже не хотят умирать.

А даже если лидер какой-то гипотетической страны-изгоя действительно не поймёт угрозу СИИ, он окажется в окружении международного альянса, который понимает. Ведущие мировые державы смогут вмешаться. Этого достаточно, чтобы мотивации не было.

Представьте, если бы лидеры (например) США, Китая, России, Германии, Японии и Великобритании искренне поверили, что *их собственное выживание* зависит от того, чтобы никто не создал суперинтеллект. И чётко заявили бы, что любые попытки его создать расценивают как угрозу своей жизни. И что они готовы защищаться. Тогда даже несогласный мировой лидер вряд ли захотел бы испытывать судьбу в борьбе с такой коалицией.

Разработка ИИ -- не гонка к военному превосходству. Это гонка к гибели. Если мировые лидеры это поймут, если осознают, что это убьёт их самих и их детей, они, скорее всего, будут честно блюсти договор и следить за его исполнением.

Понять мысль, что создание машин умнее всего человечества может погубить мир, *не так уж сложно*. Легко увидеть, что мы слишком мало знаем о разумах, которые выращиваем. Вопрос лишь в том, поверят ли мировые лидеры. Если *да*, остановить эту самоубийственную гонку вполне реально.

#### Договор потребует реального отслеживания и контроля за исполнением.

Даже если *большинство* стран поймёт, что «если кто-то его сделает, все умрут», какие-то могут не согласиться. И оказаться достаточно безрассудными, чтобы всё равно продолжить создание суперинтеллекта.

Нужен мониторинг. Нужно силовое обеспечение. Договоры о ядерном, биологическом и химическом оружии так и работают. Мы можем и должны сделать, чтобы попытки обойти запрет стали сложными и дорогими.

Запрет на передовой ИИ придётся соблюдать неукоснительно. Если вопреки международному давлению какое-то государство решит продолжать разработки, странам, подписавшим договор, возможно, надо будет использовать военную силу.

Хотелось бы этого избежать! Нужно сделать всё возможное, чтобы было ясно: в таком случае *будет* применена сила. Это поможет избежать ошибок, из-за которых *действительно* придётся её применить. Но если есть причины, оправдывающие точечный военный удар -- или даже войну, если нарушитель пойдёт на эскалацию -- то спасение человечества к ним точно относится.

#### Это уже работало

С момента создания атомной бомбы прошло больше восьмидесяти лет. Человечество неплохо справилось с контролем над её распространением. Вопреки прогнозам многих экспертов после Второй Мировой, масштабной ядерной войны так и не случилось.

В июне 2025 года правительство США даже нанесло ограниченный удар по Ирану, чтобы помешать ему создать ядерное оружие. Подобные договоры и силовые методы для мирового порядка не новость. Если выиграть восемьдесят лет до появления СИИ, этого вполне может хватить.

### Может ли режим мониторинга длиться вечно?

#### Нет. Понадобится какой-то другой выход из ситуации.

Прогресс в исследованиях ИИ вряд ли получится остановить полностью. Скорее всего, когда-нибудь исследователи найдут куда более эффективные методы создания ИИ.[^9] Или, может, какой-нибудь злоумышленник всё же сможет обойти запрет.

Будущее так или иначе настигнет человечество. И мы либо вымрем (как большинство видов до нас), либо перейдём в мир, где есть что-то умнее нас.

Но тянуть время вечно и *не нужно*. ИИ -- не единственная технология, которая развивается. Биотехнологии тоже прогрессируют. Если удастся пару десятилетий сдерживать машины, мы дойдём до, например, генной инженерии, создающей значительно более умных людей.

Вопрос в том, сколько времени мы сможем выиграть и как им распорядимся.

У человечества есть основная задача. Надо безопасно пересечь пропасть от человеческого интеллекта к суперинтеллекту. Лучший известный нам план, который хоть сколько-то реалистично звучит: выиграть время, чтобы биотехнологии сильно улучшили человеческий разум. Настолько, чтобы будущие исследователи были *настолько* умны, что они никогда (например) не скажут, что инженерный проект уложится в срок и бюджет, *если это на самом деле не так*.

Настолько, чтобы они не верили в научные теории вроде аристотелизма или геоцентризма, даже если все вокруг верят. Настолько, чтобы у них был шанс пересечь пропасть между «До» и «После» с первой же попытки. (См. раздел «„До“ и „После“» материалов к Главе 10.)

Можно представить и другие пути, но у этого есть важные плюсы. Он бьёт в ключевое узкое место («наука слишком зависит от проб и ошибок и мелких шагов, чтобы решить эту конкретную задачу»). Он использует технологии, которые уже появляются. Он не особо грозит миру катастрофой.

#### Мониторингу *не следует* быть вечным.

*Теоретически* человечество может вечно удерживаться на краю пропасти, сохраняя сегодняшний уровень компетентности. Но это, вероятно, потребует драконовского контроля над мыслями и делами людей. Да и без этого перспектива неприятная.

Мы лично считаем, что потомки человечества заслуживают стать кем захотят, исследовать звёзды и построить там прекрасную цивилизацию. Мы выступаем за запрет передового ИИ, потому что считаем суперинтеллект слишком опасным. Потому что эта мера необходима. А не потому, что ненавидим ИИ, технологии или прогресс.

Главный вопрос -- *как* попасть в чудесное будущее и как не погибнуть в процессе.

Это важно подчеркнуть. Многие преподносят прогресс ИИ как ложную дилемму. Мол, общество должно либо принять риски и вдавить педаль газа в пол, либо отказаться от ИИ, чтобы так и угаснуть на одной планете.

Это неправда. Есть и другие маршруты в столь же яркое будущее. Без риска потерять всё ни за что. Надо поискать другой путь. (См. раздел «Если поспешить, её не будет.» материалов к Главе 12.)

### Зачем делать людей умнее?

#### Это поможет решить задачу согласования.

Нам не кажется, что обеспечить согласование ИИ невозможно в принципе. Просто людям пока до этого очень далеко. Мы ещё не на том уровне интеллекта, где *мнение*, что решение найдено, сильно коррелирует с тем, что оно *действительно* найдено.

Исследователи часто признают, что задача согласования кажется пугающе сложной. И что прогресса пока мало. Отсюда такая тяга к идее «может, мы заставим ИИ сделать домашнюю работу по согласованию за нас?». Когда ты исследователь ИИ и чувствуешь, что вы с коллегами не справляетесь, логично позвать ИИ на помощь.

Но, как мы обсуждали в Главе 11 и материалах к ней (раздел «Подробнее о передаче задачи ИИ»), проблемы тут видны даже неспециалисту. Чтобы ИИ смог решить глубокую и непосильную для людей задачу, он должен быть достаточно умным, чтобы стать опасным. А *мы* плохо понимаем, что делаем. У нас нет источника эталонных данных, чтобы обучать специфическим для согласования навыкам. И нет способа проверить, действительно ли предложение ИИ безопасно и эффективно.

Мир имеет право подкидывать нам задачи не по зубам. Природа -- не игра, где человечеству гарантированы честные испытания. Иногда мы сталкиваемся с задачами, слишком сложными даже для лучших учёных. Или с требующими слишком много времени.

Есть ли более реалистичный способ передать задачу кому-то поумнее? Вариант -- сделать умнее *людей*. Чтобы они реально могли устроить согласование ИИ. Люди, в отличие от него, «согласованы заранее». У самых умных людей те же основные просоциальные мотивации, что у остальных.

Кажется, нет ничего принципиально невозможного в способности отличить *иллюзию* великого алхимического прозрения, как превратить свинец в золото, от знания, *реально позволяющего* это сделать (выбивая нейтроны из атомов свинца с помощью ядерной физики). Они должны ощущаться по-разному.

Но у реальных инженеров с этим большие проблемы. В истории реальной химии алхимики человеческого уровня обманывались веками.

Учёные привязываются к любимым теориям. Они отказываются менять взгляды, пока реальность не начнёт бить их по голове фактами: «Твоя теория ошибочна». Иногда и это не помогает. Говорят, наука движется «[от похорон к похоронам](https://en.wikipedia.org/wiki/Planck%27s_principle)»: старая гвардия не передумает, приходится ждать новую. Но это не фундаментальное ограничение природы. Это слабость конкретно людей -- недостаточно проницательных, осторожных и самокритичных.

Обычно такая наивность простительна. Обычно реальность не уничтожает *всё человечество* за гордыню одного алхимика. Но с машинным суперинтеллектом такой роскоши у нас нет. (См. раздел «„До“ и „После“» материалов к Главе 10.)

Человечество обычно добывает знания упорством, пробами, ошибками и медленным накоплением опыта. Но это *не единственный путь*.

Эйнштейн не просто открыл общую теорию относительности. Он сделал это, *хорошенько задумавшись*. Задолго до запуска спутников. Ещё никто не видел рассинхронизацию часов на орбите (мы обсуждали это в Главе 6). У него были эмпирические данные. Но, чтобы найти верный ответ, ему хватило первых тихих намёков. Ему не требовалось, чтобы истина начала ломиться в дверь.

Это редкий и трудный путь. Но такой научный гений встречается. Пусть даже только среди лучших умов, и нечасто.

А если улучшить людей ещё на пару шагов в направлении Эйнштейна или [фон Неймана](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/)? Они могли бы начать точно замечать свои недостатки и всячески компенсировать их.

Они могли бы подмечать у себя рационализацию или [предвзятость подтверждения](https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D0%BB%D0%BE%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%BA_%D0%BF%D0%BE%D0%B4%D1%82%D0%B2%D0%B5%D1%80%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D1%8E_%D1%81%D0%B2%D0%BE%D0%B5%D0%B9_%D1%82%D0%BE%D1%87%D0%BA%D0%B8_%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D1%8F). Они могли бы перерасти веру в красивые, но нерабочие идеи. Дойти до точки, где ожидание успеха всегда означает *реальный* успех. Достичь уровня компетентности, где ошибок ещё немало, но нет [систематической](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) сверхуверенности (или недоуверенности) в сложных новых областях.

Действительно ли можно улучшить человеческий интеллект? Нам кажется, что да. Мы говорили с некоторыми исследователями биотехнологий. Они видят многообещающие подходы, которые могут начать работать уже скоро. Тщательно настроенный узкоспециализированный для биотехнологий ИИ может дополнительно всё ускорить. Конечно, неясно, выгорит ли это дело на практике. Но тут точно вариант с огромным потенциалом, и он заслуживает куда больших вложений и усилий, чем получает сейчас.

Мы не рекомендуем усиление интеллекта как единственную стратегию, в которую стоит вкладываться после остановки ИИ. Это лишь один из примеров. Мы считаем его самым перспективным. Советуем человечеству изучать много путей „без СИИ“ сразу. Не надо класть все яйца в одну корзину.

#### К усовершенствованным людям не прилагается серьезная задача «согласования человека».

У усовершенствованных людей будут по сути такие же архитектура мозга, эмоции и т.д., как у нас с вами. Между нами и ИИ -- даже если его обучили говорить как мы -- огромная пропасть. В принципах мышления, в мотивациях, в возможности друг друга понять. В случае умеренно поумневших людей это вряд ли станет проблемой.

Когнитивно усовершенствованным исследователям не придётся сохранять рассудок, превращаясь в гигантские суперинтеллекты с разумом в миллион раз больше нашего. Им нужно лишь поумнеть ровно настолько, чтобы разобраться, как *сконструировать* -- не вырастить -- по-настоящему согласованный и стабильный искусственный суперинтеллект.

Конечно, мягкая форма задачи «согласования» может остаться. При координации множества людей всегда есть вопросы надёжной делегации и совпадения интересов. И их значимость неизбежно вырастет, когда речь зайдёт о группе, которой поручено создать суперинтеллект.

Мы полагаем, что эти задачи решаемы. Главное, чтобы люди изначально были явными альтруистами, их интеллект усиливали постепенно, а работали они в грамотно выстроенной организации с правильными стимулами. Всё же тут вполне разумно беспокоиться о возможном захвате власти. Нет гарантии, что разрешить все эти трудности легко. Но это хотя бы возможно. Не то что попытки корпораций вырастить по умолчанию дружественный суперинтеллект, при этом непостижимый, с абсолютно чуждым мышлением и нечеловеческими побуждениями.

«Создание элитной команды генно-модифицированных гениев, способных безопасно провести планету к суперинтеллекту» -- к этой затее точно надо подходить очень осторожно. Ставки выше некуда. Есть свои практические и этические нюансы. Но, если не найдётся столь же перспективных решений, альтернатива -- суперинтеллект просто нас всех убьёт.

Радикальные времена требуют радикальных мер. Но (умеренное) усовершенствование человеческого интеллекта даже не очень-то и радикально. Эта технология выглядит полезной сама по себе. Она может помочь человечеству сразу в куче всего.

#### Работать над остановкой суперинтеллекта можно, даже если вы не согласны по поводу усовершенствования людей.

Если вы тут с нами не согласны, мы всё равно можем совместно идти к остановке разработок передового ИИ.

Не решим эту задачу -- все умрут. Все, кто не хочет умирать, должны тут объединиться. Споры, запрещать или субсидировать усовершенствование интеллекта, можно отложить на потом. Когда угроза немедленной гибели останется позади.

«Сделать людей умнее Эйнштейна» -- не план спасения от смерти в 2028-м, 2032-м или когда там случится следующий фундаментальный прорыв алгоритмов ИИ.

Этот план нереализуем параллельно с разработкой ИИ. Даже если кто-то использует медицинские технологии, открытые слабыми ИИ, и усовершенствует человеческий интеллект до уровня куда выше Эйнштейна, этого вряд ли хватит. Такие люди, скорее всего, не успеют решить задачу согласования и безопасно спроектировать и создать машинный суперинтеллект в условиях гонки вооружений. Гонку к суперинтеллекту всё равно надо остановить.

Усовершенствование интеллекта нужно потому, что оно делает решение задачи согласования *в принципе возможным*. Но только если у таких исследователей будут годы или десятилетия. Идея не в том, чтобы они выиграли *гонку*. Им не создать согласованный суперинтеллект через, скажем, шесть лет -- быстрее, чем остальная индустрия создаст и запустит несогласованный.

Многие, кто считает «усовершенствование человеческого интеллекта» хорошим планом -- в том числе, мы -- полагают, что начать всё равно нужно с закрытия ИИ-компаний.

Большинство сторонников других планов спасения человечества тоже в целом согласны: первым делом нужно закрыть ИИ-компании.

Meta AI не должна существовать. OpenAI не должна существовать. Anthropic не должна существовать. Они нас просто убьют. Давайте сойдёмся на этом. Даже если у нас совершенно разные взгляды на следующий шаг.

### «Согласованный с кем?»

#### Трудный вопрос. Но разработку нужно остановить независимо от ответа.

Если человечество когда-нибудь создаст суперинтеллект, нужно убедиться, что он «согласован» с нашими ценностями. Но с чьими именно? Люди сильно расходятся во мнениях о добре и зле, религии, социальных нормах, политике и многом другом.

Сейчас этот вопрос не очень актуален. Человечество не умеет закладывать в ИИ *никакие* конкретные цели. Споры, *какие* цели идеальны, бесполезны. Спешка при создании суперинтеллекта погубит всех. Люди о многом несогласны. Но мало кто считает благом уничтожение жизни на Земле.

Задача выбора *конкретных* ценностей для загрузки в ИИ кажется заковыристой. Хорошо было бы решать именно её. Но мы столкнулись с другой задачей. Куда более страшной.

*Вообще* необязательно сходиться во мнениях, с кем согласовывать ИИ (и нужно ли вообще его создавать), чтобы договориться о международном запрете. Ведь иначе мы умрём. ИИ порождает кучу интересных философских вопросов. Но слишком на них отвлечёмся -- погубим своих детей.

Наш практический совет мировым лидерам:

- «Спешить ли с созданием суперинтеллекта?» должно быть отдельным вопросом от «Что нам делать с суперинтеллектом, если мы вдруг сможем сделать его безопасно?». Сосредоточьтесь на первом. Он срочный и требует немедленных действий. Второй вопрос, может, когда-нибудь станет важным. Но сейчас это ловушка. Он заставляет видеть в суперинтеллекте приз. Ошибочная вера, что создатель суперинтеллекта решит, что с ним делать, втягивает в самоубийственную гонку.

- СИИ -- кнопка для суицида, а не джинн в лампе. Создав суперинтеллект, вы им не «владеете». Это скорее он получает в своё распоряжение вас и планету.

- Вам всё же хочется обсудить, как человечеству (если мы до этого доживём) использовать суперинтеллект? Мы настоятельно советуем избегать идей, провоцирующих гонку. Не предлагайте то, что заставит другие страны отказаться от международных соглашений по поводу ИИ или нарушить их. Любой сценарий, где «победитель получает всё» очень опасен для мира.

- Есть идеи, как решить сложный вопрос «с кем согласовывать?» довольно универсально и честно, не провоцируя гонку. Например, предложить ИИ следовать «[когерентному экстраполированному волеизъявлению](https://www.lesswrong.com/w/coherent-extrapolated-volition-alignment-target)» (CEV) всего человечества.[^10] Но и тут можно без конца спорить о принципах, компромиссах и сложных деталях реализации. Это всё важно для мира, где люди *уже* знают, как точно и надёжно направить суперинтеллект. Но ставить их во главу угла сейчас -- значит упустить суть. Не рискуйте сорвать попытки объединиться ради общей цели -- спасти мир.

Даже когда в долгой перспективе вопрос очень важен, не привязывайте к выживанию человечества ничего дополнительного.

### А не разумнее ли избегать разговоров о вымирании?

#### Время политических игр прошло.

Некоторые говорят, что люди, обеспокоенные гонкой к суперинтеллекту, должны скрывать свои взгляды. Мол, лучше говорить о потере рабочих мест из-за ИИ. Или о биотеррористах с ChatGPT. Или о затратах воды на охлаждение дата-центров.[^11] Мы считаем, что вы скорее сами себя перехитрите. Этот подход, скорее всего, приведёт к обратному результату. Мы уже не раз такое видели.

Тут четыре основные проблемы:

- **Это нечестно**. А люди хорошо чувствуют фальшь и манипуляции.

	Даже если вы на редкость хороший лжец, аргументы по вопросам, которые вы сами считаете вторичными, будут выглядеть странно. Не до конца убедительными. По ровно тем же причинам, почему вы сами считаете эти вопросы не первостепенными. Чем больше продвигать «причёсанные» доводы, тем скорее люди решат: вы *либо* сами запутались, *либо* что-то недоговариваете. В любом случае, вы не покажетесь надёжным союзником или источником информации.

- **Это, скорее всего, не нужно**. По нашему опыту, честный и прямой разговор о суперинтеллекте воспринимают куда лучше, чем попытки перевести тему на что-то вроде дипфейков. С середины 2023 года я (Соарес) всё чаще общался с разными выборными должностными лицами. Я бывал на ужинах, где «обеспокоенные ИИ» люди заговаривали о террористах с ИИ-помощниками, а действующий чиновник отвечал, что его страхи куда серьёзнее. Он боится, что рекурсивно самосовершенствующийся ИИ может породить суперинтеллект, который сотрёт нас с лица земли. И что это может случиться в ближайшие три года.

	Люди вплоть до членов Конгресса США готовы воспринимать эту проблему всерьёз и искать решения.[^12] Она может казаться более нишевой и спорной, чем есть на самом деле. Ведь на момент выхода этой книги ещё не было настоящего национального или международного диалога. Но многие наши откровенные разговоры в Вашингтоне прошли на удивление хорошо.[^13]

- **Решение других проблем не поможет с суперинтеллектом**. ИИ-компании наперегонки строят суперинтеллект. Если они преуспеют, все умрут. Решения проблемы дипфейков или даже биотерроризма здесь совершенно не годятся.

	Конечно, *какие-то* точки пересечения есть. Подчеркивая связь разных проблем, можно собрать больше сторонников борьбы с ИИ умнее человека. Но крайне маловероятно, что мир случайно наткнётся на адекватный ответ на такую сложную угрозу, как суперинтеллект, *не сосредоточившись на самой угрозе*.

- **Времени может быть мало**. Вряд ли у нас есть годы, чтобы постепенно подводить людей к осознанию риска. Чтобы начинать с простых и привычных тем и медленно подниматься к суперинтеллекту. Не мобилизуемся быстро -- можем не успеть ничего сделать.

Это не значит, что безработица, биотерроризм и прочее -- ненастоящие проблемы. Но общество не остановит безумную гонку к самоубийству, если не будет знать, что она вообще идёт.

Мы годами наблюдали, как друзья и знакомые в политических кругах пытались «продать» публике темы вроде биотерроризма с ChatGPT. Кажется, ничего, что реально может помочь предотвратить создание машинного суперинтеллекта, из этого не вышло.

Мы -- «ботаники» до мозга костей. Писать книгу для общей аудитории для нас очень некомфортно. Мы не претендуем на звание экспертов по политической интриге. Но нам кажется, что человечество тут вышло из области, где всё можно решить осторожными, стратегически выверенными, «не паникёрскими» речами.

В какой-то момент мы, люди, должны заговорить о надвигающейся угрозе. Политика должна опираться на реальное положение дел. Не на «безопасные» формулировки.

Главы ИИ-лабораторий говорят, что ИИ-исследователи, превосходящие людей, могут появиться в ближайшие [год](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year) [или](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s) [четыре](https://ia.samaltman.com/). Очень надеемся, что они ошибаются. Но *наверняка* знать не можем. И политики не могут. Человечество не реагирует на угрозу адекватно. Если не бить тревогу сейчас, то когда?

Кстати: с тех пор как мы написали предыдущий абзац, предложенная нами стратегия стала приносить всё больше результатов. См. чуть ниже, что политики говорили о суперинтеллекте летом 2025 года. Похоже, настало время для настоящего обсуждения опасности искусственного суперинтеллекта.

### Призн*а*ют ли опасность выборные представители власти?

#### Всё больше признают.

Мы считаем главным препятствием непонимание. Мы отправили книгу в печать несколько месяцев назад. За это время мир, кажется, успел продвинуться в нужном направлении.

Вот некоторые высказывания американских политиков из обоих лагерей летом 2025 года:

> «Искусственный суперинтеллект -- одна из самых серьезных стоящих перед нами сейчас экзистенциальных угроз. \[...\] Стоит ли нам также беспокоиться, что авторитарные государства вроде Китая или России могут потерять контроль над своими передовыми системами? \[...\] И возможно ли, что потеря контроля любым государством, включая наше собственное, приведет к появлению независимого СИИ или суперинтеллекта, с которым нам всем придется иметь дело?»
>
> -- [Джилл Токуда (демократка, Гавайи)](https://peterwildeford.substack.com/p/congress-has-started-taking-agi-more), [на слушаниях 25 июня 2025 года](https://www.congress.gov/event/119th-congress/house-event/118428).

> «Я не буду голосовать за создание „Скайнета“ и восстание машин, на 10 лет уничтожая федерализм и отбирая у штатов право регулировать ИИ и принимать законы о нём».
>
> -- [Марджори Тейлор-Грин (республиканка, Джорджия)](https://x.com/RepMTG/status/1930650431253827806).

> «Есть очень, очень знающие люди -- я как раз говорил с одним сегодня, -- которые всерьёз опасаются, что люди не смогут контролировать эту технологию и искусственный интеллект будет фактически доминировать в нашем обществе. Мы не сможем его контролировать. Возможно, он сможет контролировать нас. Это своего рода сценарий судного дня -- и среди очень знающих людей в индустрии есть беспокойство по этому поводу».
>
> -- [Берни Сандерс (независимый, Вермонт)](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611?utm_source=substack&utm_medium=email).
> «Стремясь победить в ИИ-гонке с Китаем, мы рискуем потерять самих себя...»
>
> -- [Крис Мёрфи (демократ, Коннектикут)](https://www.chrismurphyct.com/p/in-our-scramble-to-win-the-ai-race).

> «Это повышает вероятность, что скоро главной задачей для мировых лидеров, включая президента США, станет управление переменами, которые несёт ИИ. Им придется использовать своё положение и политические инструменты, чтобы эта технология улучшала жизнь людей, а не ухудшала её».
>
> -- [Пит Буттеджадж](https://petebuttigieg.substack.com/p/we-are-still-underreacting-on-ai), бывший министр транспорта.

Ещё очень много нужно сделать. Но мир начинает обращать внимание. Настало время донести до чиновников необходимость быстрых действий на федеральном и международном уровнях.

### Ситуация безнадёжна?

#### Нет.

Эту битву можно выиграть. За наш мир стоит бороться. Будет непросто, но это реально.

Если вы хотите присоединиться к усилиям по пробуждению мира, для нас будет честью сражаться с вами бок о бок. В последней главе книги описано, как можно помочь.

## Расширенное обсуждение

### Что нужно для глобальной остановки разработки ИИ?

Мы не эксперты в международном праве. Это невероятно сложная тема. Нужны огромные усилия профильных специалистов по ней. Однако, чтобы сдвинуть дело с мёртвой точки, мы вместе с нашей [командой по техническому регулированию](https://techgov.intelligence.org/) и внешними консультантами набросали несколько идей и предположений о мерах, которые могли бы сработать.

Так мы хотим запустить обсуждение, споры, критику и доработку. Ни в коем случае не считайте эти черновики окончательными или авторитетными.

Для начала, давайте разберёмся с ограничениями и сутью задачи. Об этом можно было бы написать отдельную книгу. Глобальная цель -- предотвратить создание машинного суперинтеллекта на десятилетия. Мы не знаем, где проходят критические пороги. Так что, по сути это означает полную остановку исследований и разработок ИИ.

Нынешний прогресс ИИ держится на сочетании трёх факторов: создании лучших чипов, использовании большего количества чипов для более длительного обучения и на улучшении алгоритмов. Мы разберём каждый из них по очереди. Посмотрим, какие рычаги помогут остановить движение к искусственному суперинтеллекту.

#### Предотвратить появление более многочисленных и мощных чипов для ИИ.

Развитие способностей современных ИИ требует огромных вложений вычислительной мощности и электроэнергии. Государства могут выявлять и отслеживать все места, где это реально делать. Предотвратить появление новых ИИ можно, почти не трогая потребительское «железо».

[Цепочка поставок](https://www.csis.org/analysis/mapping-semiconductor-supply-chain-critical-role-indo-pacific-region) для производства передовых чипов очень конкретна. Некоторые её звенья обеспечиваются всего одной компанией в мире. Обычно, в традиционно союзных США странах, производить сами чипы для ИИ может лишь несколько фирм. В основном -- тайваньская TSMC. А кое-что необходимое для этого делает только голландская ASML. Речь о машине экстремальной ультрафиолетовой литографии. Она размером со школьный автобус, весит 200 тонн и стоит [сотни миллионов долларов](https://www.datacenterdynamics.com/en/news/tsmc-to-receive-first-high-na-euv-lithography-machine-from-asml-in-q4/).

Эта цепочка -- результат десятилетий инноваций и инвестиций. [Скорее всего](https://cset.georgetown.edu/publication/maintaining-the-ai-chip-competitive-advantage-of-the-united-states-and-its-allies/), её будет весьма трудно воспроизвести. Вероятно, даже у технологически развитых стран на это уйдёт больше десяти лет.

Ещё самые продвинутые ИИ-чипы [весьма специализированы](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/). Их отслеживание почти не вызовет побочных эффектов. NVIDIA H100, один из самых распространённых ИИ-чипов на середину 2025 года, стоит около \$30 000. Он создан для работы в дата-центре и требования к охлаждению и питанию у него соответствующие. Он оптимизирован для численных операций при обучении и запуске ИИ. Такие чипы [в десятки, а то и тысячи раз](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/) производительнее потребительских компьютеров в этих задачах.[^14]

Концентрация и сложность цепочки поставок сильно упрощают остановку разработки передового ИИ. **Остановить производство новых ИИ-чипов довольно просто.** Чтобы не допустить создания тайных цепочек, потребуется лишь минимальный контроль небольшого числа ключевых поставщиков.

Часть инфраструктуры у ИИ-чипов -- общая с другими передовыми микросхемами (например, для телефонов). Но есть заметные различия. Вполне реально остановить производство передовых ИИ-чипов, но выпускать чипы, не специализированные для ИИ.

Уже существующие ИИ-чипы можно отслеживать. Оставить их для работы нынешних ИИ вроде ChatGPT. Гарантировать, что их используют только для слабых ИИ (а не для новых исследований) -- непросто, но вполне возможно. Можно мониторить, где они. Есть предложения [механизмов, которые позволят мониторить, для чего чипы используются](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development). Это потребует физического доступа (инспекторы будут посещать дата-центры с проверками). Для новых чипов, сразу произведённых с [защитными механизмами](https://www.cnas.org/publications/reports/secure-governable-chips) с расчётом на мониторинг, может хватить и удалённого. Как мы обсудим дальше, опасная концентрация чипов (при алгоритмах августа 2025 года) так велика, что государствам будет нетрудно найти все такие объекты и регулярно их инспектировать.

#### Предотвратить использование для ИИ более мощных чипов.

Перейдём от производства к использованию. Сейчас [крупнейшие дата-центры для ИИ](https://epoch.ai/blog/trends-in-ai-supercomputers) вмещают сотни тысяч чипов стоимостью в миллиарды долларов. Чтобы обучить топовый современный ИИ, эти чипы нужно использовать месяцами напролёт.

Каждый чип [потребляет энергии](https://ifp.org/future-of-ai-compute/) как средний американский дом. Дата-центр с сотнями тысяч чипов -- как небольшой город. Ему нужна специальная энергетическая инфраструктура, начиная с ЛЭП. А ещё это довольно крупные здания. У них характерный тепловой след -- там же непрерывно работает огромное количество энергоёмких чипов.

Внутри здания тысячи чипов стоят в серверных стойках с мощной системой охлаждения, чтобы не перегревались. [Зайдя внутрь](https://cloud.google.com/blog/products/gcp/google-data-center-360-tour), сразу поймёшь: это дата-центр. Скрыть их назначение от международных инспекторов не выйдет. Особенно если те проверят чипы и увидят, что они специализированы для ИИ.

**Крупные дата-центры и их энергетическая инфраструктура огромны и распознаются со спутников.** Пожелай государства их найти, у них, скорее всего, получится. И в своих границах, и в других странах. Публичные данные [ограничены](https://epoch.ai/blog/trends-in-ai-supercomputers), но отследить большинство топовых ИИ-чипов можно даже так.

Государства могут пытаться спрятать дата-центры, чтобы их было не обнаружить со спутников. Например, укрыть в горе (как комплекс Шайенн-Маунтин, где сидит NORAD). Но и этим способом будет трудно спрятать инфраструктуру.

Больше всего центры выдают огромные потребности в электричестве. ЛЭП почти всегда расположены над землёй. Их можно закопать, но это куда дороже и дольше. Да и сами земляные работы сложно скрыть.[^15]

Пока для обучения передового ИИ нужно больше сотни тысяч чипов, государства вполне могут обнаружить и отследить каждый значимый дата-центр.

#### Предотвратить прогресс алгоритмов.

Более эффективные алгоритмы могут снизить, сколько вычислений нужно для обучения ИИ. Или позволить создавать более продвинутые ИИ на тех же вычислительных мощностях. [Или и то, и другое](https://arxiv.org/abs/2311.15377).

Исследования и инженерия, продвигающие алгоритмы, сейчас зависят от навыков и усилий людей.[^16] Это весьма редкие навыки. Потому у ведущих исследователей в этой сфере такие [огромные зарплаты](https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html).

Эти навыки редки сейчас. Но неясно, как всё изменится, когда в сферу придёт больше исследователей, а знания станут доступнее. Смотря как считать. И таких людей сотни, или, может, несколько тысяч (судя по штату исследователей и инженеров в ведущих ИИ-компаниях). Консервативные оценки могут быть и куда выше. Например, в мире [десятки миллионов программистов](https://www.griddynamics.com/blog/number-software-developers-world).

**Законотворческие и социальные меры, вероятно, могут резко замедлить алгоритмический прогресс.** Большинство людей не хотят нарушать закон. Особенно когда есть реальные последствия. Если сделать публикацию исследований или эксперименты незаконными из-за катастрофических рисков мощного ИИ, это, вероятно, остановит почти всех потенциальных исследователей. (См. раздел «Зачем запрещать исследования? Это уж слишком радикально.» выше.) Ещё государства могут ввести экспортный контроль. Тогда незаконным станет делиться такими исследованиями или публиковать их без лицензии и одобрения.

Социальные табу тоже помогли бы. Пример -- [Асиломарская конференция по рекомбинантной ДНК](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA) 1975-о года. Она привела к добровольному мораторию на биологические эксперименты, сочтённые слишком рискованными. Теоретически учёные могут сделать то же с развитием ИИ. Но тогда им надо воспринять всерьёз опасность ИИ умнее человека. А это отход от статуса-кво. Сейчас во многих кругах развитие ИИ восхваляют. Учитывая краткосрочные денежные стимулы и поведение лабораторий, без внешних ограничений законом не обойтись. Разве что культура области *радикально* (и быстро) изменится.[^17]

Чтобы даже несовершенный запрет стал достаточно эффективным, очень важно, чтобы мировые лидеры действительно понимали -- если продолжить давить на газ, погибнут лично они и их семьи. Самые вероятные сценарии нарушения -- если государства решат, будто собственный суперинтеллект -- это стратегический актив (или что это иллюзия, отвлекающая от прибыльных ИИ-инструментов). Если они не будут видеть в нём кнопку для всеобщего самоубийства. Вряд ли государства станут вести секретные разработки СИИ, понимая, что это равносильно тому, чтобы зарядить пистолет, приставить к виску и спустить курок.

Запреты исследований остановят не всех. Некоторые видные учёные и руководители компаний [уже заявляли](https://docs.google.com/document/d/1Guh9mzI_Xf-33fKOhlJ0lG7Yb56EJVQ4Dr3J0diQOzA/edit?tab=t.k1kf1fy9gx5i#heading=h.ymh89tu1wrg5), что уничтожение человечества -- приемлемая цена за прогресс. Но запреты всё равно нужны. Если такие люди будут лишены финансирования, а коллеги объявят им бойкот, алгоритмический прогресс хотя бы *замедлится*. Им придётся вести свои смертельные исследования подпольно и без сотрудничества более добросовестных учёных.

#### Чем дольше ждём, тем труднее будет

Если производство и сбыт ИИ-чипов продолжат идти нынешним курсом, обеспечить их централизацию и мониторинг станет только труднее. Даже если государства пока не убеждены в опасности, начало международного отслеживания ИИ-чипов сегодня сохранит возможность будущего вмешательства. Если не принять мер быстро, окно возможности может скоро закрыться.

Если исследователям будет позволено и дальше улучшать алгоритмы ИИ, количество чипов, достаточное для серьёзной угрозы, будет всё уменьшаться. Если и когда часть разработки ИИ можно будет автоматизировать теми же ИИ, это особенно затруднит контроль. Такие системы можно будет легко копировать и распространять. А их требования к железу могут стать незначительными. (Ведь для *запуска* ИИ они куда ниже, чем для *обучения*.)

В итоге остановка разработки суперинтеллектуальных систем может стать непосильной. Мы ещё не там. Но с каждым месяцем это всё сложнее. Наш план основан на остановке уже *скоро*. Есть и другие планы, без этого допущения. Но их сложнее реализовать. И они требуют б*о*льших жертв личными свободами. И шанс провала у них выше.

### Предварительный черновик договора с примечаниями

Нас много спрашивали, в том числе члены Палаты представителей и Сената США: какие конкретные законы тут реально помогут?

Мы не эксперты по политике. Но возможных вариантов ответа много. Есть законы, которые легче принять сегодня, но они менее полезны. Это, скорее, ступеньки к более амбициозным актам. А есть те, которые принять труднее, но зато они направлены на ключевые проблемы. О вторых мы можем сказать куда больше, чем о первых. Команда MIRI по техническому управлению составила предварительный черновик такого предложения.

Этот набросок договора рассчитан на мир, где лидеры осознали реальность ИИ умнее людей. Мы (пока мы это пишем) не ожидаем, что его примут завтра. Но, когда мир лучше осознает опасность, ратифицировать такой договор может стать не так уж сложно. А этот процесс уже начался и, надеемся, продолжится. (См. раздел «Призн*а*ют ли опасность выборные представители власти?» выше.)

Черновик договора с множеством примечаний -- последняя часть этих материалов. Мы опирались на исторические прецеденты других договоров. Они упомянуты в тексте. Снова подчеркнём: это отправная точка, а не конечная. Мы не эксперты в политике. Мы могли допустить глупые ошибки. Тем не менее, надеемся, этот черновик послужит вдохновением и примером того, что такой договор возможен и что он не так уж отличается от прошлых законодательных усилий.

### Не сужайте коалицию

Нам советовали выступить против генеративного искусства или роботизированного оружия. Чтобы послать более простой сигнал: не «против суперинтеллекта», а «против *ИИ*».

Не будем вдаваться в споры о генеративном искусстве, дипфейках и т.д. Мы просто не считаем этот вариант политически выгодным. Нужна коалиция для запрета суперинтеллекта. Это очень срочный вопрос. Коалиция должна быть как можно шире. Лучше, чтобы она включала людей с разными взглядами на генеративное искусство, автономных боевых дронов, беспилотные автомобили, ИИ в школах и так далее.

У нас общий интерес: предотвратить создание неподконтрольного суперинтеллекта. Это не зависит от мнения по другим вопросам.

Человечество должно вымереть и уступить место чему-то пустому? Ответа «нет» достаточно, чтобы сотрудничать в срочной попытке остановить гонку к суперинтеллекту.

Мы не думаем, что коалиция справится, если откажется работать с теми, кто не согласен с вами насчёт генеративного искусства или дронов.

Или если к суперинтеллекту привяжут пакет других проблем. И всем придётся согласиться с длинным списком не имеющих прямого отношения к делу мнений, прежде чем вместе работать над проблемой суперинтеллекта.

Если вас волнуют другие вопросы ИИ, занимайтесь ими. Но не *смешивайте* их с суперинтеллектом. Чтобы обеспечить выживание человечества, не надо к нему ничего привешивать.

Вы живёте в цивилизации, а не в радиоактивной пустоши после Третьей мировой, отчасти потому, что Восток и Запад сумели договориться. Десятки лет назад они вместе признали ядерную войну реальной угрозой человечеству. Восток и Запад считали ещё и что Запад и Восток (соответственно) -- *тоже* страшная угроза человечеству. Но они мудро сочли эти две угрозы -- ядерное уничтожение и победу идеологических противников -- разными по сути.

С точки зрения Запада было лучше, чтобы человечеству *меньше* угрожала ядерная война. Даже если угроза со стороны Востока остаётся. Это означало сотрудничество с Востоком: прямую линию между Вашингтоном и Москвой, договоры о нераспространении и прочее.

Слишком много стран надо скоординировать. Слишком много фракций расколоты (даже изнутри), чтобы предотвратить катастрофу силами лишь во всём согласных друг с другом.

Мы без проблем объединимся с людьми, которых тревожат и другие проблемы. (См. раздел «А опасность ИИ умнее людей не отвлекает от других проблем?» материалов к Главе 12.) Мы без колебаний будем сотрудничать с людьми, с которыми не согласны по политическим вопросам. Мы послали миру это отчаянное сообщение, потому что верим в него. И считаем, что эту задачу нужно *немедленно* решать на международном уровне.

Кто бы вы ни были, за что бы вы ни боролись -- если вы хотите положить конец сумасшедшей разработке ИИ умнее людей, мы на одной стороне.

[^1]: Разберём этот пример подробнее. Кабина «Аполлона-1» загорелась во время симуляции запуска 27 января 1967 года. В NASA смогли извлечь урок из ошибки. Инженеры понимали каждую деталь ракеты. Они диагностировали проблему. Дело, вероятно, было в посеребрённом медном проводе (изоляция которого протёрлась от движения двери) рядом с подтекающей трубой охлаждения с этиленгликолем и водой. Ситуацию усугубила атмосфера из чистого кислорода в капсуле и горючие материалы в кабине. Кроме того, давление перед открытием люка надо было выровнять. Но огонь не дал добраться до управления клапаном. А пожар резко усилил разницу давления.

	Все трое членов экипажа «Аполлона-1» погибли.

	Такие ошибки нередки. Даже когда на кону жизни. Ракетчики имеют дело с устройствами, которые запросто могут взорваться прямо на старте. Они серьёзно относятся к своим обязанностям и действуют осторожно. А ошибки всё равно случаются.

	Учёные отличаются от алхимиков не тем, что не совершают ошибок. А тем, что планы учёных настолько близки к рабочим, что они могут *учиться* на первых неудачах. Алхимики видели, как коллеги сходят с ума. Но не знали, какие вещества ядовиты. Поэтому не знали и что надо сделать иначе. А вот инженеры из NASA смогли найти вероятные причины и сконструировать новый корабль. В пятнадцати из шестнадцати следующих миссий он сработал хорошо. (Семь из них пытались сесть на Луну. Одна попытка («Аполлон-13») провалилась. Там проблемы в кабине тоже могли стать фатальными. Но благодаря знанию систем и мастерству астронавтов экипаж вернулся на Землю.)

	«Аполлон-1» был *почти* рабочей ракетой. Аппарат из осторожных инженеров и учёных был *почти* готов отправить людей на Луну. Поэтому одной большой ошибки хватило, чтобы привести NASA в форму. И совершить шесть успешных посадок из семи.

	Современные ИИ-компании и близко не показывают такого уважения к задаче. (См. раздел «Мы знаем, как выглядит серьёзное отношение к задаче. Тут не так.» материалов к Главе 11) У них нет таких подробных планов. Они не близки к правильному решению. Когда ИИ делает что-то непонятное, они не могут найти причину, как нашли тот провод. Они недостаточно близки к успеху, чтобы учиться на ошибках.

	Они не ведут себя как регуляторы авиации или ядерщики. У них нет осторожных планов с [указанными чёткими допущениями](https://lesswrong.ru/w/%D0%9C%D1%8B%D1%88%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%91%D0%B5%D0%B7%D0%BE%D0%BF%D0%B0%D1%81%D0%BD%D0%B8%D0%BA%D0%B0_%D0%B8_%D0%9E%D0%B1%D1%8B%D0%B4%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%9F%D0%B0%D1%80%D0%B0%D0%BD%D0%BE%D0%B9%D1%8F). Они не воздерживаются от опасных действий, пока теории не позволят учиться на провалах.

[^2]: Мы не рекомендуем международную коалицию для создания ИИ. (См. раздел «Почему бы международной коалиции не разработать безопасный ИИ совместно, а не запрещать?» материалов к Главе 12.) Но теоретически она могла бы породить организацию уровня NASA или FAA, способную реально учиться на ошибках индустрии.

[^3]: См., например, [эту статью 2018 года](https://pmc.ncbi.nlm.nih.gov/articles/PMC7119956/) или гораздо более глубокий [анализ рисков и выгод от 2015 года](https://osp.od.nih.gov/wp-content/uploads/2015/12/Risk%20and%20Benefit%20Analysis%20of%20Gain%20of%20Function%20Research%20-%20Draft%20Final%20Report.pdf).

[^4]: На 2025 год США, похоже, склонны [прекратить госфинансирование](https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-127.html) исследований по усилению функций вирусов. Но глобальной координации почти нет. См. [этот отчёт](https://cset.georgetown.edu/publication/understanding-the-global-gain-of-function-research-landscape).

[^5]: Если бы биолаборатории лучше избегали утечек, а создание супервирусов давало, например, суперлекарства, был бы смысл это делать. Насколько мы знаем, таких позитивных результатов нет. Биологи обычно [не советуют](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Esvelt%20Testimony.pdf) [этим](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Ebright%20Testimony%20Updated.pdf) [заниматься](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Quay%20Testimony.pdf). Подозреваем, это одна из редких областей, от которых человечеству стоит отступить. Она угрожает жизням множества людей, которые на этот риск не подписывались.

[^6]: Заметим: выключатели в чипах и протоколы отключения дата-центров сами по себе не решат проблему. Предупредительных выстрелов может и не быть. Или на них могут не отреагировать. (См. раздел «Будут ли предупредительные выстрелы?» выше.) Но это относительно дешёвый и доступный шаг. Он может помочь в случаях „на грани“, когда риски *относительно* малы, но запас прочности не помешает.

[^7]: Если общество боится, что это слишком замедлит мир, мы советуем ускориться в другом месте. Строить больше АЭС. Делать больше биохимических опытов -- не со смертельными вирусами, а для здоровья, силы и ума людей.

	(Конечно, общество в целом не столько требует безумной науки, сколько сопротивляется переменам. Но некоторые говорят «Нельзя останавливать ИИ, это важно для прогресса». Правильный ответ тут: прогресса полно и в других областях. В той безумной науке, которая оставляет после себя выживших.)

[^8]: Помните: мы выступаем за договоры, по которым *государствам* тоже нельзя создавать суперинтеллект. Мы не призываем передать мощную технологию от корпораций государству. Мы призываем *вообще не создавать* смертельно опасную технологию. По крайней мере, в обстоятельствах, хоть сколько-нибудь похожих на нынешние.

[^9]: Возможно и что исследователи найдут более эффективные методы, изучая работу нынешних LLM.

	Позволит ли это осознанно конструировать ИИ, а не выращивать? Не исключено! К сожалению, задолго до полного понимания работы LLM люди, вероятно, получат частичное понимание. Позволяющее создавать куда более эффективные ИИ, но недостаточное для их согласования.

[^10]: [Когерентное экстраполированное волеизъявление](https://baserates-test.vercel.app/w/coherent-extrapolated-volition-alignment-target) (Coherent Extrapolated Volition, CEV) -- наша попытка ответить на вопрос «согласованный с кем?». Если мы доживём до момента, когда создатели ИИ смогут его направлять. CEV пытается разрешить моральные и мета-моральные разногласия. ИИ получает задачу: найти, в чём люди сошлись бы во мнении, если бы знали больше и были теми, кем хотят быть (как в [теории идеального советника](https://en.wikipedia.org/wiki/Ideal_observer_theory)). И искать общие мета-принципы, к которым можно прибегать при фундаментальных разногласиях. (Цель тут не в том, чтобы ИИ «решил все проблемы» людей, а чтобы решил достаточно и без катастрофы.) Мы рекомендуем экстраполяцию ценностей всех живых людей. Не потому, что это идеал. Потому что это очевидная точка координации для несогласных сторон. (И потому, что другие сущности, важные для людей, получат вес косвенно, через их ценности. И сущности, которые для людей *были бы важны*, знай они больше -- тоже.)

	Но ещё раз: сейчас эта тема скорее отвлекает. Не обязательно соглашаться по высоким философским вопросам, чтобы принять меры по поводу того, что грозит нас убить. Было бы глупо позволить дебатам о таких идеях (даже тех, что нам нравятся) сорвать работу по нераспространению.

	Мы кратко упоминаем это, чтобы показать: мы не уходим от вопроса. И чтобы успокоить читателей, боящихся, что рабочего предложения не существует. Даже если CEV -- неверный подход, у него много желательных свойств. Это вселяет надежду, что найти некатастрофический ответ реально.

[^11]: Вопреки [частым](https://fortune.com/article/how-much-water-does-ai-use/) [беспокойствам](https://www.thetimes.com/uk/technology-uk/article/thirsty-chatgpt-uses-four-times-more-water-than-previously-thought-bc0pqswdr), [не особо больших](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7water).

[^12]: Хотя некоторые чиновники в Вашингтоне согласны с нашими опасениями по суперинтеллекту, у них нет власти решить проблему. Если только не подключатся куда больше представителей власти в США и других странах. Первые разговоры обнадеживают, но работы ещё много.

[^13]: Завершая эти онлайн-материалы, мы увидели ещё один хороший знак: ряд экспертов по нацбезопасности и бывших представителей власти положительно оценили черновик «*Если кто-то его сделает, все умрут*».

	Вот примеры:

	Бен Бернанке, нобелевский лауреат, бывший глава ФРС: «Ясный и убедительный рассказ об экзистенциальных рисках высокоразвитого ИИ. Рекомендую».

	Джон Вулфстал, директор по глобальным рискам Федерации американских учёных, бывший помощник президента по нацбезопасности: «Убедительный довод, что сверхчеловеческий ИИ почти наверняка приведёт к уничтожению людей. Правительства мира должны признать риски и принять коллективные меры».

	Генерал-лейтенант Джон Н.Т. «Джек» Шэнахэн (ВВС США, в отставке), первый директор Объединённого центра ИИ Минобороны: «Хотя я скептически отношусь к тому, что нынешняя траектория ИИ ведёт к вымиранию, я признаю: возможно, мне просто не хватает воображения. Учитывая экспоненциальный рост ИИ, сейчас лучшее время принять меры против худших исходов. Авторы предлагают важные и заслуживающие внимания идеи глобальных ограничений».

	Фиона Хилл, бывшая директор Совета нацбезопасности Белого дома: «Серьёзная книга во всех отношениях. В пугающем анализе Юдковского и Соареса сверхмогущественный ИИ не будет нуждаться в людях и сможет нас уничтожить. Книга -- красноречивый призыв срочно отойти от края пропасти».

	Р.П. Эдди, бывший директор Совета нацбезопасности: «Это наше предупреждение. Читайте сегодня. Распространяйте завтра. Требуйте ограничений. Я ставлю на человечество, но сперва мы должны проснуться».

	Сюзанна Сполдинг, бывшая замминистра внутренней безопасности: «Авторы поднимают невероятно серьёзный вопрос, заслуживающий, даже требующий нашего внимания».

	Эмма Скай, старшая научная сотрудница Высшей школы международных отношений при Йеле, бывший советник командующего силами США в Ираке: «В книге Юдковский и Соарес выносят строгое предупреждение: человечество мчится к суперинтеллекту без мер защиты. Они убедительно, ясно и упорно доказывают: малейшая ошибка в согласовании ИИ может погубить цивилизацию. Эта провокационная книга призывает и учёных, и политиков, и простых людей взглянуть в лицо рискам, пока не поздно. Этот призыв ко вниманию и осторожности -- обязательное чтение для всех, кому небезразлично будущее».

[^14]: Иногда их используют для других сложных расчётов, вроде физических симуляций и прогнозов погоды. Но *в основном* -- для ИИ. Простой способ оценить соотношение задач -- посмотреть на [выручку](https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue) NVIDIA, главного производителя чипов. Можно предположить, что бум спроса на серверные GPU вызван ИИ. Ведь в других сферах нет роста сравнимого масштаба. А недавний бум затмевает прошлые доходы. Получается, ИИ -- львиная доля применений этих чипов. Запрет на их производство может не сильно повлиять на потребительское железо.

[^15]: Можно генерировать энергию на месте без заметных ЛЭП. Комплекс Шайенн-Маунтин использует дизель-генераторы. Он может запитать около 10 000 передовых ИИ-чипов. Но для их постоянной работы в ходе обучения нужно всё время подвозить топливо. Это будет заметно. Грубые расчёты: для 10 000 чипов нужен один бензовоз в день. На 200 000 чипов -- 20 бензовозов ежедневно.

	Ещё дата-центры можно питать от АЭС. К счастью, у многих стран уже есть опыт их мониторинга.

[^16]: [Примеры](https://arxiv.org/abs/2507.10618) такого прогресса: 
	- [FlashAttention](https://arxiv.org/abs/2205.14135) -- алгоритм, повышающий эффективность математических операций, используя особенности дизайна чипов. 
	- [Смесь экспертов](https://arxiv.org/abs/1701.06538) (Mixture-of-Experts, MoE) -- изменение архитектуры, при котором на каждый токен (например, слово) используется лишь часть параметров ИИ. 
	- [GRPO](https://arxiv.org/abs/2402.03300) -- метод дообучения ИИ.

[^17]: Ещё одна возможная мера (пока исследователей алгоритмов всё ещё мало, сотни или тысячи): платить им за смену деятельности. Пусть занимаются не ИИ. Или теми исследованиями ИИ, которые большого риска не несут.

	В 1990-х был такой прецедент. Правительство США начало программу по переводу бывших советских оружейников [на мирные рельсы](https://www.armscontrol.org/act/1999-03/features/maintaining-proliferation-fight-former-soviet-union#:~:text=One%20of%20the%20earliest,productive%2C%20non%2Dmilitary%20endeavors).
