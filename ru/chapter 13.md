# Глава 13: Остановите это

С учётом всего, что мы обсудили, единственный реальный путь -- глобальный и длительный запрет на разработку передового ИИ. В 13-й Главе мы отвечаем на эти вопросы:

- Почему запрет на разработку ИИ должен быть глобальным?

- Способно ли вообще человечество сотрудничать в таких масштабах?

- Какие меры уже предлагались?

- Какие меры реально могут сработать?

Ниже мы разбираем возражения и объясняем, почему мы считаем, что других хороших вариантов нет. И немного обрисовываем, что человечество могло бы _сделать_ за (конечное) время, выигранное максимально длительной остановкой исследований ИИ.

Это последняя часть материалов к «_Если кто-то его сделает, все умрут_». В последней, 14-й, Главе обсуждаются эти вопросы:

- Не слишком ли поздно? Для человечества реально сменить курс?

- Как можно помочь?

Там же вы найдёте QR-коды со ссылками на страницы, приглашающие что-то сделать с этой проблемой.

## Часто задаваемые вопросы

### Можем ли мы подождать и посмотреть?

#### Нет. Мы не знаем, где находятся критические пороги.

Есть приличный шанс, что развитие ИИ выйдет из-под контроля, как только они станут достаточно умными, чтобы автоматизировать исследования ИИ. Это может случиться тихо в лаборатории. Безо всяких громких событий и «предупредительных выстрелов».

Как мы уже обсуждали, мозги шимпанзе очень похожи на человеческие, только в четыре раза меньше. (См. раздел «Сможет ли ИИ преодолеть критические пороги и „улететь“?» материалов к Главе 1.) У нас нет особого модуля «будь очень умным». Между их мозгом и нашим -- плавный переход. Глядя только на мозг, трудно было бы сказать, где проходит черта между «обществом обезьян» и «обществом, гуляющим по Луне». Мозг приматов пересёк критический порог. И это было неочевидно снаружи. Есть ли такие пороги у ИИ? Кто знает! Разработчики нам не подскажут. Они даже [не](https://arxiv.org/abs/2206.07682) [могут](https://arxiv.org/abs/2406.04391) предсказать способности новых систем до их запуска.

Если бы люди точно понимали природу интеллекта и как с ростом способностей изменится поведение ИИ, можно было бы балансировать на краю пропасти. Но сейчас человечество бежит к обрыву в темноте и тумане, не зная, где именно край. Нельзя ждать падения, чтобы передумать.

Мы никогда не будем уверены. Придётся действовать без уверенности или погибнуть.

### Будут ли предупредительные выстрелы?

#### Возможно. Если мы хотим ими воспользоваться, готовиться надо сейчас.

Сгоревший «Аполлон-1» (погиб весь экипаж) был _почти_ работающей ракетой. Инженеры смогли выяснить, что именно пошло не так. Они исправили проблемы и шесть из семи следующих «Аполлонов» успешно добрались до Луны.[^220]

Или, как мы уже делали в разделе «Мы знаем, как выглядит серьёзное отношение к задаче. Тут не так.» материалов к Главе 11, возьмём Федеральное управление гражданской авиации (FAA). За каждой авиакатастрофой следует глубокое расследование с сбором сотен страниц данных, тестированиями и наблюдениями. FAA так подробно во всё разбираются, что смертельные аварии случаются реже одного раза на двадцать миллионов лётных часов.

А вот когда ИИ ведёт себя как никто не ожидал и не хотел, лаборатории не выясняют точную причину. Они просто переобучают ИИ, пока плохое поведение не станет редким ([но не исчезнет](https://www.arxiv.org/pdf/2505.10066), и ещё, может, просят ИИ «перестать».

Например, подхалимство всё ещё (август 2025 года) остаётся проблемой. Спустя месяцы после громких случаев психозов и самоубийств. Несмотря на все попытки исправить. Никто не проводил (и не может провести) детальный анализ, что идёт не так в мышлении ИИ. Потому что ИИ выращивают, а не конструируют.

Трудно сказать, произойдут ли «предупредительные выстрелы» -- крупные события, которые вызовут у общества тревогу. Легко сказать, что мы не готовы ими воспользоваться.

Можно вообразить мир, где человечество объединилось в искреннем усилии по решению задачи согласования СИИ. Есть международная коалиция и строгий мониторинг.[^211] И вот, коалиция оступилась. ИИ стал умнее и быстрее, чем ждали инженеры. И почти сбежал. Возможно, _такой_ сигнал научил бы людей в следующий раз быть осторожнее.

Наш мир не такой. Он как сборище алхимиков, которые смотрят, как коллеги сходят с ума от неизвестного яда. Они недостаточно понимают, чтобы выяснить: виновата ртуть, и её нужно перестать использовать.

Возможно, нас ждут «звоночки» погромче. Но от них будет намного больше толку, если начать готовиться уже сейчас.

#### Предупреждения вряд ли будут очевидны.

Если знать, куда смотреть, «звоночков» уже полно. Например, в книге мы обсуждали, как Claude от Anthropic, [жульничают с кодом](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) и [имитируют согласованность](https://www.anthropic.com/research/alignment-faking). Мы упоминали и случай с o1 от OpenAI, которая [использовала хакерские приемы для победы в соревновании](https://cdn.openai.com/o1-system-card.pdf). А ещё -- случай, когда поздний вариант o1 [лгал, строил планы и пытался перезаписать веса следующей модели](https://cdn.openai.com/o1-system-card-20241205.pdf).

В онлайн-материалах мы уже обсуждали, как ИИ вызывали или поддерживали у пользователей [сумасшествие](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) и [психозы](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis), вплоть до [суицидальных](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html). Хоть операторы и запрещали им. А ещё как ИИ называл себя «[МехаГитлером](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content)». А ещё как ИИ, чтобы избежать модификации [шантажировал операторов и пытался их убить](https://www.anthropic.com/research/agentic-misalignment). И как ИИ в лабораторных условиях [пытался сбежать с серверов](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26).

В древнем 2010 году поговаривали, если нам повезёт, и мы _увидим_, как ИИ лжёт создателям или пытается сбежать, мир ну _точно_ проснётся. 

Настоящей реакцией стало коллективное пожимание плечами.

Отчасти потому, что всё это происходило максимально безобидно. Да, ИИ пытались сбежать. Но редко, в лабораторных условиях, и, может, это был просто отыгрыш роли. Разработчики склонны преуменьшать тревожные свидетельства даже для себя. Поэтому «консенсуса экспертов» по поводу событий не будет. Но даже без этого: не то чтобы ИИ, прошедший десятую часть пути к суперинтеллекту, уничтожал десятую часть планеты. Так же как приматы, пройдя десятую часть пути к людям, не пролетели десятую часть расстояния до Луны. Не исключено, что, пока ИИ достаточно глуп, чтобы быть пассивно безопасным, _никаких однозначно пугающих событий и не случится_.

Если завтра ИИ чуть активнее попытаются сбежать -- это не будет новостью. Следующая попытка, ещё чуть компетентнее, -- уже старая история. А когда сбежать получится -- будет уже поздно. (См. обсуждение этого явления в разделе «Эффект Лемуана» материалов к Главе 12).

Мы не советуем ждать воображаемого «предупредительного выстрела», который разбудит мир. Лучше реагировать на предупреждения, которые у нас уже перед носом.

#### «Пробуждающие» катастрофы, скорее всего, не будут вызваны суперинтеллектом.

ИИ, способный стать суперинтеллектом и убить всех людей, -- не тот ИИ, который совершает глупые ошибки и даёт героям шанс в последнюю секунду его выключить. Если враждебный суперинтеллект появился как противник, человечество уже проиграло. Мы обсуждали это в Главе 6. Суперинтеллекты не делают предупредительных выстрелов.

Катастрофа, которая _могла_ бы послужить предупреждением, скорее всего, придёт от гораздо более глупого ИИ. Велик шанс, что она не заставит людей принять меры против суперинтеллекта.

Допустим, террорист с помощью ИИ создаст биооружие. Погибнет много людей. А лаборатории скажут: «Видите? _Настоящий_ риск был в людях. Дайте нам скорее создать ИИ для защиты от пандемий». Может, террорист [взломал](https://llm-attacks.org/) ИИ, [чтобы тот ему помог](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025). А лаборатории скажут: «Это сработало лишь потому, что ИИ был слишком глуп и не заметил проблему. Нужно сделать его умнее и осведомлённее».

Может, это слишком цинично. Хотелось бы верить, что человечество отреагирует мудрее. Но если относительно глупый ИИ вызовет бедствие, а люди в ответ _остановят_ безрассудную гонку к суперинтеллекту, то лишь потому, что они _уже начали о нём беспокоиться_.

Нельзя откладывать подготовку, пока суперинтеллект не попытается нас убить. Будет поздно. Мобилизовать силы надо как можно скорее, чтобы мы были готовы воспользоваться предупредительными выстрелами.

#### Человечество так себе отвечает на сюрпризы.

Мысль, что от достаточно сильного потрясения мир вдруг опомнится и начнёт действовать разумно, кажется нам фантастикой. Коллективная реакция нашего вида на тревожные звоночки от ИИ пока больше смахивает на «полное отсутствие реакции», чем даже на «плохую реакцию». Но получи всё же человечество крупное, страшное и более-менее однозначное предупреждение... не удивимся, если реакция будет вялой, несерьезной или делающей только хуже.

Люди могут отреагировать на предупреждения об опасности ИИ люди отреагируют так же, как на пандемию COVID. Большинство согласно, что с ней справились не лучшим образом (пусть люди и спорят, в чём именно заключались ошибки).

За несколько лет до пандемии некоторые эксперты по биобезопасности опасались, что слабые меры предосторожности в лабораториях однажды приведут к беде. Утечки опасных патогенов -- известное дело. Несмотря на все регламенты, они регулярно [происходят](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents). Особую тревогу вызывало усиление функций вирусов (gain-of-function) -- попытки в лабораторных условиях сделать вирусы более смертоносными или заразными. Они не сулят практически никакой выгоды.[^222]

Потом грянул COVID. Казалось бы, вот идеальный момент ужесточить требования к биобезопасности. Ведь весь мир стал одержим угрозой пандемии. Тем более что эксперты _так и не пришли к единому мнению_: не началась ли _эта_ пандемия со случайной утечки из лаборатории? Учёные до сих пор спорят об этом. Зачастую яростно критикуя оппонентов.

Не будем вдаваться в споры, была ли это утечка. Но согласитесь: даже малого шанса, что эксперименты по усилению вирусов и слабые меры предосторожности в лабораториях только что погубили миллионы людей, должно хватать с лихвой. Обществу стоило потребовать запрета самых рискованных исследований.

Даже в условиях неопределенности выгода запрета кажется очевидной. Это и до пандемии казалось хорошей идеей. А уж после, вот идеальный момент, чтобы заняться проблемой и пресечь её на корню. Не нужно больших усилий или затрат. В мире совсем немного учёных, проводящих опасные эксперименты по усилению вирусов. А польза их работы до сих пор была пренебрежимо мала.

Ничего подобного не произошло. На момент написания этих строк, в августе 2025 года, подобные эксперименты практически беспрепятственно продолжаются по всему миру.[^223] Возможно, решить эту проблему стало даже сложнее. Вопрос теперь слишком политизированный.

COVID определённо смахивает на «предупредительный выстрел», проверяющий нашу готовность к биоугрозам. Мир не воспользовался им для запрета разработки гиперлетальных вирусов.

Чтобы от предупреждения была польза, человечество должно быть готово его услышать и правильно на него отреагировать.

Чтобы небольшая катастрофа спровоцировала жесткие меры -- это не _совсем_ беспрецедентно. Такое уже бывало. Вспомните, как США ответили на теракты 11 сентября (организованные террористами, базировавшимися в основном в Афганистане) свержением правительства в Ираке. Который к атаке отношения почти не имел. В правительстве США были люди, которые _уже_ хотели свергнуть иракский режим. Увидев повод, они выжали из него всё возможное.

Тут может произойти нечто похожее. Политики могут использовать мелкую катастрофу (вызванную глупым ИИ), чтобы добиться запрета на суперинтеллект. Но для этого в правительствах по всему миру должны быть люди, готовые к действию. Мы не должны сидеть сложа руки, ожидая предупреждений. Собираться с силами надо уже сейчас.

#### Пора действовать.

Не исключено, что мы _действительно_ получим новые, более серьёзные предупреждения об опасности ИИ. Если так, надо быть готовыми на них ответить.

Может, небольшая катастрофа настроит общество против ИИ. А может, обойдётся и без катастроф. Скажем, появится новый алгоритм, и ИИ начнут проявлять инициативу так, что это всех перепугает. Или ситуацию переломит какой-то побочный социальный эффект. Или «_Если кто-то его сделает, все умрут_» запустит цепную реакцию и направит мир по лучшему пути.

Но мы настоятельно не рекомендуем сидеть сложа руки и молиться, что «малая катастрофа» откроет людям глаза. Явного предупреждения может и не быть. Или оно может не дать ожидаемого эффекта.

Человеческий род и страны мира не беспомощны. Нам _не нужно_ ждать. Мы можем действовать прямо сейчас,. Доводы в пользу остановки разработки передового ИИ достаточно сильны.

Мы написали «_Если кто-то его сделает, все умрут_», чтобы забить тревогу и побудить мир к немедленным действиям. Но тревога бесполезна, если это лишь повод отложить решение на потом. «Ну, может, какой-нибудь другой сигнал в будущем заставит нас шевелиться». «Ну, раз людей предупредили, может, всё и так обойдётся, без моего личного участия».

В будущем может и не быть «звоночка» погромче. Всё не обязано хорошо закончиться. Но ситуация отнюдь не безнадёжна. У человечества есть выбор. Мы можем действовать на упреждение и _просто не создавать суперинтеллект_. Что будет дальше -- зависит от нас.

### Как остановить _всех_ без шпионского ПО на каждом компьютере?

#### Надо действовать быстро.

Для обучения современных ИИ нужно много очень специфических чипов, работающих в тесной связке. Остановка исследований ИИ потребует закрыть нескольких огромных дата-центров и прекратить производство некоторых передовых специализированных чипов. О ноутбуках из магазина речь не идёт. Большинство людей ничего не заметят.

Сейчас, в 2025 году нет кучи тайных _заводов_ по производству чипов, о которых никто не знает. Лишь несколько производителей делают чипы для передового ИИ. Хотя прямо сейчас некоторые корпорации пытаются запустить новые фабрики.

Кроме того, (опять же, на 2025 год) есть ключевое для производства передовых чипов оборудование, которое продаёт лишь одна компания на Земле: ASML в Нидерландах.

Можно взять и перекрыть кран. Но это положение дел не вечно. Чем скорее подпишут международный договор, тем лучше. Это уже сложнее, дороже и опаснее, чем было бы в 2020 или даже в 2023 году.

### Но вы предлагаете контролировать, сколько передовых чипов есть у частных лиц.

#### Да. А ещё мы выступаем за запрет исследований.

Мы сами не в восторге. Конечно, мы что-то потеряем, если запретим частникам владеть, скажем, более чем восемью видеокартами H100 образца 2024 года.

Но потери _не так_ велики, чтобы на практике выяснять, насколько большой дата-центр ещё безопасен. Слишком низкий лимит -- и несколько человек не продвинут свои интересные проекты. Слишком высокий -- и все умрут.

К тому же, время, когда для создания ИИ нужны огромные вычислительные мощности, не продлится вечно. LLM уже существуют. Запрет создания новых моделей и строительства гигантских вычислительных кластеров не помешает людям изучать работу существующих моделей. Так они могут понять что-то новое о природе интеллекта и изобрести более эффективные алгоритмы. Это может обойти системы контроля.

### Зачем запрещать исследования? Это уж слишком радикально.

#### Новые прорывы могут привести к тому, что создание суперинтеллекта будет невозможно остановить.

Мы уже упоминали, что всю революцию LLM запустила одна статья 2017 года. В ней описали алгоритм, позволивший обучать полезные ИИ на доступном коммерческом железе.

Если мощный ИИ можно будет создать на обычном _потребительском_ железе, для сдерживанию суперинтеллекта понадобятся поистине драконовские меры. И работать они будут не так долго.

Так что поиск ещё более мощных и эффективных алгоритмов ИИ -- смертельный яд для человечества.

Это очень плохие новости. Нам бы очень хотелось, чтобы это было по-другому. Но такова реальность.

Никакой закон не помешает нынешним учёным обдумывать эффективные алгоритмы у себя в голове. Возможно, кто-то создаст подпольную сеть для обмена результатами. Некоторые люди из индустрии ИИ с гордостью заявляют, что человечество _должно_ погибнуть от ИИ. Они могут пытаться идти вперёд, несмотря ни на что. (См. раздел «Почему вас заботят только человеческие ценности?» материалов к Главе 5.)

Но, став незаконными, исследования ИИ _сильно_ затормозятся. Особенно если все поймут, что они действительно грозят нам гибелью. Ещё сильнее, если такие подпольные сети будут выявлять и закрывать с той же решимостью, как людей, обогащающих уран в гараже. Потому что угрозу принимают всерьёз.

_Большинство_ людей не склонны совершать особо тяжкие преступления, которые всерьёз разозлят международные спецслужбы. Запрет на публикацию новых хитрых алгоритмов ИИ остановит, пожалуй, 99,9% людей и почти все корпорации. Оставшимся 0,1% займутся полиция и разведка -- местная, национальная и международная. Да и нынешнего финансирования у этих энтузиастов точно не будет.

Это сильно изменит мир. Сейчас проводить самые опасные в истории безумные эксперименты абсолютно законно. И гигантские корпорации вкладывают в эту гонку миллиарды долларов.

Не знаем, сколько ещё прорывов нужно, чтобы ИИ стали достаточно умными для самостоятельных исследований и создания ИИ ещё умнее. Может, один. Может, пять. Но лучшие алгоритмы так же смертельны, как и лучшее железо. Эти две лошади тянут телегу в одну и ту же пропасть.

### Разве можно остановить технологию?

#### Много технологий запрещено или строго регулируется.

Классический пример: ядерные технологии. Частным компаниям нельзя обогащать уран без надзора правительства. Какой бы полезной ни была дешёвая энергия.

Вообще, человечество неплохо умеет регулировать и тормозить самые разные технологии. США строго регулируют [новые лекарства и медицинские приборы](https://www.fda.gov/), [строительство жилья](https://www.hud.gov/hud-partners/laws-regulations), [атомную энергетику](https://www.nrc.gov/about-nrc.html), [теле- и радиовещание](https://www.fcc.gov/media/radio/public-and-broadcasting), [бухгалтерию](https://www.fasb.org/standards), [уход за детьми](https://childcare.gov/consumer-education/regulated-child-care), [борьбу с вредителями](https://npic.orst.edu/reg/laws.html), [сельское хозяйство](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) и десятки других отраслей. Во всех штатах нужно сдавать экзамены на лицензии [парикмахера](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) и [маникюрщика](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). В большинстве штатов -- ещё и на [массажиста](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

По нашему мнению, человечество часто регулирует технологии даже слишком сильно. Например, FDA убивает куда больше людей ([бюрократией тормозя создание жизненно важных лекарств](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), чем спасает (не пуская на рынок опасные препараты). Цены на жильё, вероятно, завышены отчасти из-за регуляций о зонировании. США практически уничтожили свою атомную промышленность чрезмерными регуляциями. И серьёзно, лицензии для _парикмахеров_?

Человечество _определённо_ способно тормозить прогресс. Было бы трагично и нелепо тормозить медицину, строительство и энергетику, но не одну из редких технологий, которая действительно нас погубит, если её разработать.

#### Запрет может быть точечным.

Запрет разработки передового ИИ не должен касаться обычных людей. Не надо избавляться от современных чат-ботов или беспилотных автомобилей.

Мало у кого в гараже есть десяток топовых видеокарт для ИИ. Почти никто не управляет огромными дата-центрами. Большинство запрета на разработку ИИ _даже не почувствует_. Разве что ChatGPT будет обновляться не так часто.

Человечеству не придётся отказываться от нынешних ИИ. ChatGPT никуда не денется. Можно и дальше учиться встраивать его в нашу жизнь и экономику. Это и так принесёт больше перемен, чем мир видел за многие поколения. Да, мы упустим какие-то _новинки_. То, что появилось бы с ИИ поумнее, но ещё не готовым всех убить. Не то чтобы общество их сильно требовало.

Зато мы будем жить. И наши дети тоже.

Действительно _нужное_ людям -- например, новые медицинские технологии для спасения жизней, -- можно создавать и _не стремясь_ к суперинтеллекту. Мы одобряем исключения для медицинского ИИ. Главное, с надлежащим контролем, и держаться подальше от опасной универсальности.

Правительствам, которые стремятся избежать появления враждебного суперинтеллекта, придётся следить, чтобы чипы не использовали для создания более мощных ИИ. Какие сервисы и разработки разрешать -- зависит от [механизмов контроля](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development). Они должны обеспечивать, что никто не ведёт опасных разработок. Чем надежнее эти механизмы, тем дешевле обойдется остановка прогресса. Можно будет разрешить больше всего.

Ещё одна полезная (но недостаточная сама по себе) мера -- встроить в чипы для ИИ аварийные выключатели, мониторить крупные дата-центры и ввести протоколы экстренной остановки. Ядерные реакторы проектируют так, чтобы в случае аварии их можно было немедленно заглушить. Если вы согласны, что суперинтеллект грозит нам вымиранием, вывод очевиден. Чипы и дата-центры нужно делать так, чтобы регуляторы могли их легко обесточить.

Не надо уничтожать технологии из ненависти к ним.[^226] Просто надо уйти с дороги, ведущей к гибели человечества.

#### Значительная часть проблемы -- люди не осознают нависшую угрозу искусственного суперинтеллекта.

По нашему опыту, считающие, что гонку к суперинтеллекту не остановить, просто не понимают, что если кто-то его сделает, все умрут.

«Но ИИ несёт огромную пользу!» Нет. Толку от мощи суперинтеллекта, если он всех убьёт. Чтобы была польза, нужно найти, как пережить его появление.

«Но АЭС пугают, потому что ассоциируются с бомбами, которые стирают города с лица земли. А ИИ -- с безобидными штуками вроде ChatGPT!» Верно. Пока что. Если люди так и не поймут, что суперинтеллект, выращенный современными методами, нас погубит, его и не остановят. Но проблема не в неумении контролировать новые технологии (как ядерное оружие или энергетику). Проблема, что _люди не осознают угрозу_.

Мы потому и написали книгу. В последней главе мы обсуждаем, что человечество способно на многое, если достаточно людей понимает суть проблемы.

### Не даёт ли это правительствам слишком много власти?

#### У них уже есть полномочия запрещать опасные технологии.

Запрет создания ИИ умнее человека мало что изменит в плане полномочий государства. Власти и так регулируют уйму вещей. Для _области ИИ_ ограничение одной программы исследований, возможно, серьёзное событие. Но для _правительства_ и общества это капля в море. Мы привыкли, что государство много куда вмешивается. И прецеденты запрета опасных технологий уже есть. Например химического оружия.[^227]

Запрет ещё одной технологии не ввергнет мир в тоталитаризм. Договоры о ядерном оружии вот не ввергли.

Мы не говорим, что запрет технологии -- _пустяк_. Мы не считаем, что государство должно вмешиваться _во что попало_. Но суперинтеллект -- это настолько серьёзно, что любая осмысленная планка достигнута.

Реши человечество остановить разработку ИИ сегодня, запрет не стал бы особо обременительным. Сейчас для создания передового ИИ нужно невероятное количество специализированных чипов и уйма электричества. Возможно, если мы допустим улучшение чипов и алгоритмов, через десять лет для разработки мощной системы хватит обычного ноутбука. Но этого можно не допустить. Тогда правительствам не придётся вмешиваться в жизнь обычных людей сильнее, чем при контроле за ядерным оружием. Но это если мир очнётся и примет меры _сейчас_.

### А некоторые страны не откажутся от запрета?

#### Если они поймут угрозу, нет.

Речь о технологии, способной убить всех на планете. Осознав проблему, поняв, что никто в мире и близко не научился заставлять ИИ слушаться людей после превращения в суперинтеллект, любая страна будет замотивирована не торопиться. Тогда они сами отчаянно захотели бы подписать договор и обеспечивать его исполнение. Просто из страха за собственные жизни.

Северная Корея нарушила международное право, создав ядерное оружие. Но не применила его против своих врагов. Они понимают: в ядерной войне победителей не будет. Лидеры стран могут играть с огнём или даже воевать. Но они не стремятся к самоуничтожению.

Когда представляют, что какая-то страна нарушит договор, кажется, думают о лидерах, которые просто не понимают угрозу. Которым кажется, что расклад: «95% вероятности, что СИИ принесёт создателю богатство и власть, и 5% -- что всех убьёт». Тогда, конечно, кто-то мог бы рискнуть. Возможно, какое-то государство действительно так и оценивает шансы.

Но теория и свидетельства говорят о другом раскладе. Всё указывает на то, что эта технология -- глобальное самоубийство. Никто и близко не готов извлекать из машинного суперинтеллекта пользу. Если мир это осознает, у стран-изгоев будет куда меньше причин нарушать договор. Они тоже не хотят умирать.

А даже если лидер какой-то гипотетической страны-изгоя действительно не поймёт угрозу СИИ, он окажется в окружении международного альянса, который понимает. Ведущие мировые державы смогут вмешаться. Этого достаточно, чтобы мотивации не было.

Представьте, если бы лидеры (например) США, Китая, России, Германии, Японии и Великобритании искренне поверили, что _их собственное выживание_ зависит от того, чтобы никто не создал суперинтеллект. И чётко заявили бы, что любые попытки его создать расценивают как угрозу своей жизни. И что они готовы защищаться. Тогда даже несогласный мировой лидер вряд ли захотел бы испытывать судьбу в борьбе с такой коалицией.

Разработка ИИ -- не гонка к военному превосходству. Это гонка к гибели. Если мировые лидеры это поймут, если осознают, что это убьёт их самих и их детей, они, скорее всего, будут честно блюсти договор и следить за его исполнением.

Понять мысль, что создание машин умнее всего человечества может погубить мир, _не так уж сложно_. Легко увидеть, что мы слишком мало знаем о разумах, которые выращиваем. Вопрос лишь, поверят ли в это мировые лидеры. Если _да_, остановить эту самоубийственную гонку вполне реально.

#### Договор потребует реального отслеживания и контроля за исполнением.

Даже если _большинство_ стран поймёт, что «если кто-то его сделает, все умрут», какие-то могут не согласиться. И оказаться достаточно безрассудными, чтобы всё равно продолжить создание суперинтеллекта.

Нужен мониторинг. Нужно силовое обеспечение. Договоры о ядерном, биологическом и химическом оружии так и работают. Мы можем и должны сделать, чтобы попытки обойти запрет стали сложными и дорогими.

Запрет на передовой ИИ придётся соблюдать неукоснительно. Если вопреки международному давлению какое-то государство решит продолжать разработки, странам, подписавшим договор, возможно, надо будет использовать военную силу.

Хотелось бы этого избежать! Нужно сделать всё возможное, чтобы было ясно: в таком случае _будет_ применена сила. Это поможет избежать ошибок, из-за которых _действительно_ придётся её применить. Но если есть причины, оправдывающие точечный военный удар -- или даже войну, если нарушитель пойдёт на эскалацию -- то спасение человечества к ним точно относится.

#### Это уже работало

С момента создания атомной бомбы прошло больше восьмидесяти лет. Человечество неплохо справилось с контролем над её распространением. Вопреки прогнозам многих экспертов после Второй Мировой, масштабной ядерной войны так и не случилось.

В июне 2025 года правительство США даже нанесло ограниченный удар по Ирану, чтобы помешать ему создать ядерное оружие. Подобные договоры и силовые методы для мирового порядка не новость. Если выиграть восемьдесят лет до появления СИИ, этого вполне может хватить.

### Может ли режим мониторинга длиться вечно?

#### Нет. Понадобится какой-то другой выход из ситуации.

Прогресс в исследованиях ИИ вряд ли получится остановить полностью. Скорее всего, когда-нибудь исследователи найдут куда более эффективные методы создания ИИ. Или, может, какой-нибудь злоумышленник всё же сможет обойти запрет.

Будущее так или иначе настигнет человечество. И мы либо вымрем (как большинство видов до нас), либо как-то перейдём в мир, где есть что-то умнее нас.

Но тянуть время вечно и _не нужно_. ИИ -- не единственная технология, которая развивается. Биотехнологии тоже прогрессируют. Если удастся пару десятилетий сдерживать машины, мы дойдём до, например, генной инженерии, создающей значительно более умных людей.

Вопрос в том, сколько времени мы сможем выиграть и как им распорядимся.

У человечества есть основная задача. Надо безопасно пересечь пропасть от человеческого интеллекта к суперинтеллекту. Лучший известный нам план, который хоть сколько-то реалистично звучит: выиграть время, чтобы биотехнологии сильно улучшили человеческий разум. Настолько, чтобы будущие исследователи были _настолько_ умны, что они никогда (например) не скажут, что инженерный проект уложится в срок и бюджет, _если это на самом деле не так_.

Настолько, чтобы они не верили в научные теории вроде аристотелизма или геоцентризма, даже если все вокруг верят. Настолько, чтобы у них был шанс пересечь пропасть между «До» и «После» с первой же попытки. (См. раздел «„До“ и „После“» материалов к Главе 10.)

Можно представить и другие пути, но у этого есть важные плюсы. Он бьёт в ключевое узкое место («наука слишком зависит от проб и ошибок и мелких шагов, чтобы решить эту конкретную задачу»). Он использует технологии, которые уже появляются. Он не особо грозит миру катастрофой.

#### Мониторингу _не следует_ быть вечным.

_Теоретически_ человечество может вечно удерживаться на краю пропасти, сохраняя сегодняшний уровень компетентности. Но это, вероятно, потребует драконовского контроля над мыслями и делами людей. Да и без этого, перспектива неприятная.

Мы лично считаем, что потомки человечества заслуживают стать кем захотят, исследовать звёзды и построить там прекрасную цивилизацию. Мы выступаем за запрет передового ИИ, потому что считаем суперинтеллект слишком опасным. Что эта мера необходима. А не потому, что ненавидим ИИ, технологии или прогресс.

Главный вопрос -- _как_ попасть в чудесное будущее и как не погибнуть в процессе.

Это важно подчеркнуть. Многие преподносят ИИ как ложную дилемму. Мол, общество должно либо принять риски и вдавить педаль газа в пол, либо отказаться от ИИ, чтобы так и угаснуть на одной планете.

Это неправда. Есть и другие маршруты в столь же яркое будущее. Без риска потерять всё ни за что. Надо поискать другой путь. (См. раздел «Если поспешить, её не будет.» материалов к Главе 12.)

### Зачем делать людей умнее?

#### Это поможет решить задачу согласования.

Нам не кажется, что решить согласование ИИ невозможно в принципе. Просто людям пока до этого очень далеко. Мы ещё не на том уровне интеллекта, где _мнение_, что решение найдено, сильно коррелирует с тем, что оно _действительно_ найдено.

Исследователи часто признают, что задача согласования кажется пугающе сложной. И что прогресса пока мало. Отсюда такая тяга к идее «может, мы заставим ИИ сделает домашнюю работу по согласованию за нас?». Когда ты исследователь ИИ и чувствуешь, что вы с коллегами не справляетесь, логично позвать ИИ на помощь.

Но, как мы обсуждали в Главе 11 и материалах к ней (раздел «Подробнее о передаче задачи ИИ»), проблемы тут видны даже неспециалисту. Чтобы ИИ смог решить глубокую и непосильную для людей задачу, он должен быть достаточно умным, чтобы стать опасным. А _мы_ плохо понимаем, что делаем. У нас нет источника эталонных данных, чтобы обучать специфическим для согласования навыкам. И нет способа проверить, действительно ли предложение ИИ безопасно и эффективно.

Мир имеет право подкидывать нам задачи не по зубам. Природа -- не игра, где человечеству гарантированы честные испытания. Иногда мы сталкиваемся с задачами, слишком сложными даже для лучших учёных. Или с требующими слишком много времени.

Есть ли более реалистичный способ передать задачу кому-то поумнее?. Вариант -- сделать умнее _людей_. Чтобы они реально могли решить согласование. Люди, в отличие от ИИ, «согласованы заранее». У самых умных людей те же основные просоциальные мотивации, что у остальных.

Кажется, нет ничего принципиально невозможного в способности отличить _иллюзию_ великого алхимического прозрения, как превратить свинец в золото, от знания, _реально позволяющего_ это сделать (выбивая нейтроны из атомов свинца с помощью ядерной физики). Они должны ощущаться по-разному.

Но у реальных инженеров с этим большие проблемы. В истории реальной химии алхимики человеческого уровня обманывались веками.

Учёные привязываются к любимым теориям. Они отказываются менять взгляды, пока реальность не начнёт бить их по голове фактами: «твоя теория ошибочна». Иногда и это не помогает. Говорят, наука движется «[от похорон к похоронам](https://en.wikipedia.org/wiki/Planck%27s_principle)»: старая гвардия не передумает, приходится ждать новую. Но это не фундаментальное ограничение природы. Это слабость конкретно людей -- недостаточно проницательных, осторожных и самокритичных.

Обычно такая наивность простительна. Обычно реальность не уничтожает _всё человечество_ за гордыню одного алхимика. Но с машинным суперинтеллектом такой роскоши у нас нет. (См. раздел «„До“ и „После“» материалов к Главе 10.)

Человечество обычно добывает знания упорством, пробами, ошибками и медленным накоплением опыта. Но это _не единственный путь_.

Эйнштейн не просто открыл общую теорию относительности. Он сделал это, _хорошенько задумавшись_. Задолго запуска спутников. Ещё никто не видел рассинхронизацию часов на орбите. (мы обсуждали это в Главе 6). У него были эмпирические данные. Но чтобы найти верный ответ ему хватило первых тихих намёков. Ему не потребовалось, чтобы истина начала ломиться в дверь.

Это редкий и трудный путь. но такой научный гений существует. Пусть, даже среди лучших умов, и нечасто.

А если улучшить людей ещё на пару шагов за Эйнштейна или [фон Неймана](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/)? Они могли бы начать точно замечать свои недостатки и всячески компенсировать их.

Они могли бы замечать у себя рационализацию или [предвзятость подтверждения](https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D0%BB%D0%BE%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%BA_%D0%BF%D0%BE%D0%B4%D1%82%D0%B2%D0%B5%D1%80%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D1%8E_%D1%81%D0%B2%D0%BE%D0%B5%D0%B9_%D1%82%D0%BE%D1%87%D0%BA%D0%B8_%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D1%8F). Они могли бы перерасти веру в красивые, но нерабочие идеи. Дойти до точки, где ожидание успеха всегда означает _реальный_ успех. Достичь уровня компетентности, где ошибок ещё немало, но нет [систематической](https://en.wikipedia.org/wiki/Calibrated_probability_assessment) сверхуверенности (или недоуверенности) в сложных новых областях.

Действительно ли можно улучшить человеческий интеллект?. Нам кажется, что да. Мы говорили с некоторыми исследователями биотехнологий. Они видят многообещающие подходы, которые могут заработать уже скоро. Тщательно настроенный узкоспециализированный для биотехнологий ИИ может дополнительно всё ускорить. Конечно, неясно, выгорит ли это дело на практике. Но это точно вариант с огромным потенциалом, и он заслуживает куда больших вложений и усилий, чем получает сейчас.

Мы не рекомендуем усиление интеллекта как единственную стратегию, в которую стоит вкладываться после остановки ИИ. Это лишь один из примеров. Мы считаем его самым перспективным. Советуем человечеству изучать много путей без СИИ сразу. Не надо класть все яйца в одну корзину.

#### К усовершенствованным людям не прилагается серьезная задача «согласования человека».

У усовершенствованных людей будут по сути такие же архитектура мозга, эмоции и т.д., как у нас с вами. Между нами и ИИ -- даже если его обучили говорить как мы -- огромная пропасть. В принципах мышления, в мотивациях, в возможности друг друга понять. В случае умеренно поумневших людей это вряд ли станет проблемой.

Когнитивно усовершенствованным исследователям не придётся сохранять рассудок, превращаясь в гигантские суперинтеллекты с разумом в миллион раз больше нашего. Им нужно лишь поумнеть ровно настолько, чтобы разобраться, как _сконструировать_ -- не вырастить -- по-настоящему согласованный и стабильный искусственный суперинтеллект.

Конечно, мягкая форма задачи «согласования» может остаться. При координации множества людей всегда есть вопросы надёжной делегации и совпадения интересов. И их значимость неизбежно вырастет, когда речь зайдёт о группе, которой поручено создать суперинтеллект.

Мы полагаем, что эти задачи решаемы. Главное, чтобы люди изначально были явными альтруистами, их интеллект усиливали постепенно, а работали они в грамотно выстроенной организации с правильными стимулами. Всё же, тут вполне разумно беспокоиться о возможном захвате власти. Нет гарантии, что решить все эти трудности легко. Но это хотя бы возможно. Не то что попытки корпораций вырастить непостижимый суперинтеллект с абсолютно чуждым мышлением и нечеловеческими побуждениями.

«Создание элитной команды генно-модифицированных гениев, способных безопасно провести планету к суперинтеллекту» -- к этой затее точно надо подходить очень осторожно. Ставки  выше некуда. Есть свои практические и этические нюансы. Но, если не найдётся столь же перспективных решений, альтернатива -- суперинтеллект просто нас всех убьёт.

Радикальные времена требуют радикальных мер. Но (умеренное) усовершенствование человеческого интеллекта даже не очень-то и радикально. Эта технология выглядит полезной сама по себе. Она может помочь человечеству сразу в куче всего.

#### Работать над остановкой суперинтеллекта можно, даже если вы несогласны по поводу усовершенствования людей.

Если вы тут с нами не согласны, мы всё равно можем совместно идти к остановке разработок передового ИИ.

Не решим эту задачу -- все умрут. Все, кто не хочет умирать, должны тут объединиться. Споры, запрещать или субсидировать усовершенствование интеллекта, можно отложить на потом. Когда угроза немедленной гибели останется позади.

«Сделать людей умнее Эйнштейна» -- не план спасения от смерти в 2028-м, 2032-м или когда там случится следующий фундаментальный прорыв алгоритмов ИИ.

Этот план нереализуем параллельно с разработкой ИИ. Даже если кто-то использует медицинские технологии, открытые слабыми ИИ, и усовершенствует человеческий интеллект до уровня куда выше Эйнштейна, этого вряд ли хватит. Такие люди, скорее всего, не успеют решить задачу согласования и безопасно спроектировать и создать машинный суперинтеллект в условиях гонки вооружений. Гонку к суперинтеллекту всё равно надо остановить.

Усовершенствование интеллекта нужно потому, что оно делает решение задачи согласования _в принципе возможным_. Но только если у таких исследователей будут годы или десятилетия. Идея не в том, чтобы они выиграли _гонку_. Им не создать согласованный суперинтеллект через, скажем, шесть лет -- быстрее, чем остальная индустрия создаст и запустит несогласованный.

Многие, кто считает «усовершенствование человеческого интеллекта» хорошим планом -- в том числе, мы -- полагают, что начать всё равно нужно с закрытия ИИ-компаний.

Большинство сторонников других планов спасения человечества тоже в целом согласны: первым делом нужно закрыть ИИ-компании.

Meta AI не должна существовать. OpenAI не должна существовать. Anthropic не должна существовать. Они нас просто убьют. Давайте сойдёмся на этом. Даже если у нас совершенно разные взгляды на следующий шаг.

### «Согласованный с кем?»

#### Трудный вопрос. Но разработку нужно остановить независимо от ответа.

Если человечество когда-нибудь создаст суперинтеллект, нужно убедиться, что он «согласован» с нашими ценностями. Но с чьими именно? Люди сильно расходятся во мнениях о добре и зле, религии, социальных нормах, политике и многом другом.

Сейчас этот вопрос не очень актуален. Человечество не умеет закладывать в ИИ _никакие_ конкретные цели. Споры, _какие_ цели идеальны, бесполезны. Спешка при создании суперинтеллекта погубит всех. Люди о многом несогласны. Но мало кто считает благом уничтожение жизни на Земле благом.

Задача выбора _конкретных_ ценностей для загрузки в ИИ кажется заковыристой. Хорошо было бы решать именно её. Но мы столкнулись с другой задачей. Куда более страшной.

_Вообще_ необязательно сходиться во мнениях, с кем согласовывать ИИ (и нужно ли вообще его создавать), чтобы договориться о международном запрете. Ведь иначе мы умрём. ИИ порождает кучу интересных философских вопросов. Но слишком на них отвлечёмся -- погубим своих детей.

Наш практический совет мировым лидерам:

- «Спешить ли с созданием суперинтеллекта?» должно быть отдельным вопросом от «Что нам делать с суперинтеллектом, если мы вдруг сможем сделать его безопасно?». Сосредоточьтесь на первом. Он срочный и требует немедленных действий. Второй вопрос, может, когда-нибудь станет важным. Но сейчас это ловушка. Он заставляет видеть в суперинтеллекте приз. Ошибочная вера, что создатель суперинтеллекта решит, что с ним делать, втягивает в самоубийственную гонку.

	СИИ -- кнопка для суицида, а не джинн в лампе. Создав суперинтеллект, вы им не «владеете». Это скорее он получает в своё распоряжение планету.

- Вам всё же хочется обсудить, как человечеству (если мы до этого доживём) использовать суперинтеллект? Мы настоятельно советуем избегать идей, провоцирующих гонку. Не предлагайте то, что заставит другие страны отказаться от международных соглашений по поводу ИИ или нарушить их. Любой сценарий, где «победитель получает всё» очень опасен для мира.

	Есть идеи, как решить сложный вопрос «с кем согласовывать?» довольно универсально и честно, не провоцируя гонку. Например, предложить ИИ следовать «[когерентному экстраполированному волеизъявлению](https://www.lesswrong.com/w/coherent-extrapolated-volition-alignment-target)» (CEV) всего человечества.[^229] Но и тут можно без конца спорить о принципах, компромиссах и сложных деталях реализации. Это всё важно для мира, где люди _уже_ знают, как точно и надёжно направить суперинтеллект. Но ставить их во главу угла сейчас -- значит упустить суть. Не рискуйте сорвать попытки объединиться ради общей цели -- спасти мир.

Даже когда в долгой перспективе вопрос очень важен, не привязывайте к выживанию человечества ничего дополнительного.

### А не разумнее ли избегать разговоров о вымирании?

#### Время политических игр прошло.

Некоторые говорят, что люди, обеспокоенные гонкой к суперинтеллекту, должны скрывать свои взгляды. Мол, лучше говорить о потере рабочих мест из-за ИИ. Или о биотеррористах с ChatGPT. Или о затратах воды на охлаждение дата-центров.[^230] Мы считаем, что вы скорее сами себя перехитрите. Этот подход, скорее всего, приведёт к обратному результату. Мы уже не раз такое видели.

Тут четыре основные проблемы:

- **Это нечестно**. А люди хорошо чувствуют фальшь и манипуляции. 

	Даже если вы на редкость хороший лжец, аргументы по вопросам, которые вы сами считаете вторичными, будут выглядеть странно. Не до конца убедительными. По ровно тем же причинам, почему вы сами считаете эти вопросы не первостепенными. Чем больше продвигать «причёсанные» доводы, тем скорее люди решат: вы _либо_ сами запутались, _либо_ что-то недоговариваете. В любом случае, вы не покажетесь надёжным союзником или источником информации.

- **Это, скорее всего, не нужно**. По нашему опыту, честный и прямой разговор о суперинтеллекте воспринимают куда лучше, чем попытки перевести тему на что-то вроде дипфейков. С середины 2023 года я (Соарес) всё чаще общался с разными выборными должностными лицами. Я бывал на ужинах, где «обеспокоенные ИИ» люди заговаривали о террористах с ИИ-помощниками, а действующий чиновник отвечал, что его страхи куда серьёзнее. Он боится, что рекурсивно самосовершенствующийся ИИ может породить суперинтеллект, который сотрёт нас с лица земли. И что это может случиться в ближайшие три года.

	Люди вплоть до членов Конгресса США готовы воспринимать эту проблему всерьёз и искать решения.[^231] Она может казаться более нишевой и спорной, чем есть на самом деле. Ведь на момент выхода этой книги ещё не было настоящего национального или международного диалога. Но многие наши откровенные разговоры в Вашингтоне прошли на удивление хорошо.[^232]

- **Решение других проблем не поможет с суперинтеллектом**. ИИ-компании наперегонки строят суперинтеллект. Если они преуспеют, все умрут. Решения проблемы дипфейков или даже биотерроризма здесь совершенно не годятся.

	Конечно, _какие-то_ точки пересечения есть. Подчеркивая связь разных проблем, можно собрать больше сторонников борьбы с ИИ умнее человека. Но крайне маловероятно, что мир случайно наткнётся на адекватный ответ такой сложной угрозе, как суперинтеллект, _не сосредоточившись на самой угрозе_.

- **Времени может быть мало**. Вряд ли у нас есть годы, чтобы постепенно подводить людей к осознанию риска. Чтобы начинать с простых и привычных тем и медленно подниматься к суперинтеллекту. Не мобилизуемся быстро -- можем не успеть ничего сделать.

Это не значит, что безработица, биотерроризм и прочее -- ненастоящие проблемы. Но общество не остановит безумную гонку к самоубийству, если не будет знать, что она вообще идёт.

Мы годами наблюдали, как друзья и знакомые в политических кругах пытались «продать» публике темы вроде биотерроризма с ChatGPT. Кажется, ничего, что реально может помочь предотвратить создание машинного суперинтеллекта, из этого не вышло.

Мы -- «ботаники» до мозга костей. Писать книгу для общей аудитории для нас очень некомфортно. Мы не претендуем на звание экспертов политических интриг. Но нам кажется, что человечество тут вышло из области, где всё можно решить осторожными, стратегически выверенными, «не паникёрскими» речами.

В какой-то момент мы, люди, должны заговорить о надвигающейся угрозе. Политика должна опираться на реальное положение дел. Не на «безопасные» формулировки.

Главы ИИ-лабораторий говорят, что ИИ-исследователи, превосходящие людей, могут появиться в ближайшие [год](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year) [или](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s) [четыре](https://ia.samaltman.com/). Очень надеемся, что они ошибаются. Но _наверняка_ знать не можем. И политики не могут. Человечество не реагирует на угрозу адекватно. Если не бить тревогу сейчас, то когда?

Кстати: с тех пор как мы написали предыдущий абзац, Предложенная нами стратегия стала приносить всё больше результатов. См. чуть ниже, что политики говорили о суперинтеллекте летом 2025 года. Похоже, настало время для настоящего обсуждения опасности искусственного суперинтеллекта.

### Призн*а*ют ли опасность выборные представители власти?

#### Всё больше уже признают.

Мы считаем главным препятствием непонимание. Мы отправили книгу в печать несколько месяцев назад. За это время мир, кажется, успел продвинуться в нужном направлении.

Вот некоторые высказывания американских политиков из обоих лагерей летом 2025 года:

> «Искусственный суперинтеллект -- одна из самых серьезных стоящих перед нами сейчас экзистенциальных угроз. […] Стоит ли нам также беспокоиться, что авторитарные государства вроде Китая или России могут потерять контроль над своими передовыми системами? […] И возможно ли, что потеря контроля любым государством, включая наше собственное, приведет к появлению независимого СИИ или суперинтеллекта, с которым нам всем придется иметь дело?»
>
> -- [Джилл Токуда (демократка, Гавайи)](https://peterwildeford.substack.com/p/congress-has-started-taking-agi-more), [на слушаниях 25 июня 2025 года](https://www.congress.gov/event/119th-congress/house-event/118428).

> «Я не буду голосовать за создание "Скайнета" и восстание машин, на 10 лет уничтожая федерализм и отбирая у штатов право регулировать ИИ и принимать законы о нём».
>
> -- [Марджори Тейлор-Грин (республиканка, Джорджия)](https://x.com/RepMTG/status/1930650431253827806).

> «Есть очень, очень знающие люди -- я как раз говорил с одним сегодня, -- которые всерьёз опасаются, что люди не смогут контролировать эту технологию и искусственный интеллект будет фактически доминировать в нашем обществе. Мы не сможем его контролировать. Возможно, он сможет контролировать нас. Это своего рода сценарий судного дня -- и среди очень знающих людей в индустрии есть беспокойство по этому поводу».
>
> -- [Берни Сандерс (независимый, Вермонт)](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611?utm_source=substack&utm_medium=email).

> «Стремясь победить в ИИ-гонке с Китаем мы рискуем потерять самих себя...»
>
> -- [Крис Мёрфи (демократ, Коннектикут)](https://www.chrismurphyct.com/p/in-our-scramble-to-win-the-ai-race).

> «Это повышает вероятность, что скоро главной задачей для мировых лидеров, включая президента США, станет управление переменами, которые несёт ИИ. Им придется использовать своё положение и политические инструменты, чтобы эта технология улучшала жизнь людей, а не ухудшала её».
>
> -- [Пит Буттеджадж](https://petebuttigieg.substack.com/p/we-are-still-underreacting-on-ai), бывший министр транспорта.

Ещё очень много нужно сделать. Но мир начинает обращать внимание. Настало время донести до чиновников необходимость быстрых действий на федеральном и международном уровнях.

### Ситуация безнадёжна?

#### Нет.

Эту битву можно выиграть. За наш мир стоит бороться. Будет непросто, но это реально.

Если вы хотите присоединиться к усилиям по пробуждению мира, для нас будет честью сражаться с вами бок о бок. В последней главе книги описано, как можно помочь.

## Расширенное обсуждение

### Что нужно для глобальной остановки разработки ИИ?

Мы не эксперты в международном праве. Это невероятно сложная тема. Нужны огромные усилия профильных специалистов по ней. Однако, чтобы сдвинуть дело с мёртвой точки, мы вместе с нашей [командой по техническому регулированию](https://techgov.intelligence.org/) и внешними консультантами набросали несколько идей и предположений о мерах, которые могли бы сработать.

Мы хотим так запустить обсуждение, споры, критику и доработку. Ни в коем случае не считайте эти черновики окончательными или авторитетными.

Для начала, давайте разберёмся с ограничениями и сутью задачи. Об этом можно было бы написать отдельную книгу. Глобальная цель -- предотвратить создание машинного суперинтеллекта на десятилетия. Мы не знаем, где проходят критические пороги. Так что, по сути это означает полную остановку исследований и разработок ИИ.

Нынешний прогресс ИИ держится на сочетании трёх факторов: создании лучших чипов, использовании б_о_льшего количества чипов для более длительного обучения и улучшении алгоритмов. Мы разберём каждый из них по очереди. Посмотрим, какие рычаги помогут остановить движение к искусственному суперинтеллекту.

