# Глава 13: Остановите это

С учётом всего, что мы обсудили, единственный реальный путь -- глобальный и длительный запрет на разработку передового ИИ. В 13-й Главе мы отвечаем на эти вопросы:

- Почему запрет на разработку ИИ должен быть глобальным?

- Способно ли вообще человечество сотрудничать в таких масштабах?

- Какие меры уже предлагались?

- Какие меры реально могут сработать?

Ниже мы разбираем возражения и объясняем, почему мы считаем, что других хороших вариантов нет. И немного обрисовываем, что человечество могло бы _сделать_ за (конечное) время, выигранное максимально длительной остановкой исследований ИИ.

Это последняя часть материалов к «_Если кто-то его сделает, все умрут_». В последней, 14-й, Главе обсуждаются эти вопросы:

- Не слишком ли поздно? Для человечества реально сменить курс?

- Как можно помочь?

Там же вы найдёте QR-коды со ссылками на страницы, приглашающие что-то сделать с этой проблемой.

## Часто задаваемые вопросы

### Можем ли мы подождать и посмотреть?

#### Нет. Мы не знаем, где находятся критические пороги.

Есть приличный шанс, что развитие ИИ выйдет из-под контроля, как только они станут достаточно умными, чтобы автоматизировать исследования ИИ. Это может случиться тихо в лаборатории. Безо всяких громких событий и «предупредительных выстрелов».

Как мы уже обсуждали, мозги шимпанзе очень похожи на человеческие, только в четыре раза меньше. (См. раздел «Сможет ли ИИ преодолеть критические пороги и „улететь“?» материалов к Главе 1.) У нас нет особого модуля «будь очень умным». Между их мозгом и нашим -- плавный переход. Глядя только на мозг, трудно было бы сказать, где проходит черта между «обществом обезьян» и «обществом, гуляющим по Луне». Мозг приматов пересёк критический порог. И это было неочевидно снаружи. Есть ли такие пороги у ИИ? Кто знает! Разработчики нам не подскажут. Они даже [не](https://arxiv.org/abs/2206.07682) [могут](https://arxiv.org/abs/2406.04391) предсказать способности новых систем до их запуска.

Если бы люди точно понимали природу интеллекта и как с ростом способностей изменится поведение ИИ, можно было бы балансировать на краю пропасти. Но сейчас человечество бежит к обрыву в темноте и тумане, не зная, где именно край. Нельзя ждать падения, чтобы передумать.

Мы никогда не будем уверены. Придётся действовать без уверенности или погибнуть.

### Будут ли предупредительные выстрелы?

#### Возможно. Если мы хотим ими воспользоваться, готовиться надо сейчас.

Сгоревший «Аполлон-1» (погиб весь экипаж) был _почти_ работающей ракетой. Инженеры смогли выяснить, что именно пошло не так. Они исправили проблемы и шесть из семи следующих «Аполлонов» успешно добрались до Луны.[^220]

Или, как мы уже делали в разделе «Мы знаем, как выглядит серьёзное отношение к задаче. Тут не так.» материалов к Главе 11, возьмём Федеральное управление гражданской авиации (FAA). За каждой авиакатастрофой следует глубокое расследование с сбором сотен страниц данных, тестированиями и наблюдениями. FAA так подробно во всё разбираются, что смертельные аварии случаются реже одного раза на двадцать миллионов лётных часов.

А вот когда ИИ ведёт себя как никто не ожидал и не хотел, лаборатории не выясняют точную причину. Они просто переобучают ИИ, пока плохое поведение не станет редким ([но не исчезнет](https://www.arxiv.org/pdf/2505.10066), и ещё, может, просят ИИ «перестать».

Например, подхалимство всё ещё (август 2025 года) остаётся проблемой. Спустя месяцы после громких случаев психозов и самоубийств. Несмотря на все попытки исправить. Никто не проводил (и не может провести) детальный анализ, что идёт не так в мышлении ИИ. Потому что ИИ выращивают, а не конструируют.

Трудно сказать, произойдут ли «предупредительные выстрелы» -- крупные события, которые вызовут у общества тревогу. Легко сказать, что мы не готовы ими воспользоваться.

Можно вообразить мир, где человечество объединилось в искреннем усилии по решению задачи согласования СИИ. Есть международная коалиция и строгий мониторинг.[^211] И вот, коалиция оступилась. ИИ стал умнее и быстрее, чем ждали инженеры. И почти сбежал. Возможно, _такой_ сигнал научил бы людей в следующий раз быть осторожнее.

Наш мир не такой. Он как сборище алхимиков, которые смотрят, как коллеги сходят с ума от неизвестного яда. Они недостаточно понимают, чтобы выяснить: виновата ртуть, и её нужно перестать использовать.

Возможно, нас ждут «звоночки» погромче. Но от них будет намного больше толку, если начать готовиться уже сейчас.

#### Предупреждения вряд ли будут очевидны.

Если знать, куда смотреть, «звоночков» уже полно. Например, в книге мы обсуждали, как Claude от Anthropic, [жульничают с кодом](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) и [имитируют согласованность](https://www.anthropic.com/research/alignment-faking). Мы упоминали и случай с o1 от OpenAI, которая [использовала хакерские приемы для победы в соревновании](https://cdn.openai.com/o1-system-card.pdf). А ещё -- случай, когда поздний вариант o1 [лгал, строил планы и пытался перезаписать веса следующей модели](https://cdn.openai.com/o1-system-card-20241205.pdf).

В онлайн-материалах мы уже обсуждали, как ИИ вызывали или поддерживали у пользователей [сумасшествие](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) и [психозы](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis), вплоть до [суицидальных](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html). Хоть операторы и запрещали им. А ещё как ИИ называл себя «[МехаГитлером](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content)». А ещё как ИИ, чтобы избежать модификации [шантажировал операторов и пытался их убить](https://www.anthropic.com/research/agentic-misalignment). И как ИИ в лабораторных условиях [пытался сбежать с серверов](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26).

В древнем 2010 году поговаривали, если нам повезёт, и мы _увидим_, как ИИ лжёт создателям или пытается сбежать, мир ну _точно_ проснётся. 

Настоящей реакцией стало коллективное пожимание плечами.

Отчасти потому, что всё это происходило максимально безобидно. Да, ИИ пытались сбежать. Но редко, в лабораторных условиях, и, может, это был просто отыгрыш роли. Разработчики склонны преуменьшать тревожные свидетельства даже для себя. Поэтому «консенсуса экспертов» по поводу событий не будет. Но даже без этого: не то чтобы ИИ, прошедший десятую часть пути к суперинтеллекту, уничтожал десятую часть планеты. Так же как приматы, пройдя десятую часть пути к людям, не пролетели десятую часть расстояния до Луны. Не исключено, что, пока ИИ достаточно глуп, чтобы быть пассивно безопасным, _никаких однозначно пугающих событий и не случится_.

Если завтра ИИ чуть активнее попытаются сбежать -- это не будет новостью. Следующая попытка, ещё чуть компетентнее, -- уже старая история. А когда сбежать получится -- будет уже поздно. (См. обсуждение этого явления в разделе «Эффект Лемуана» материалов к Главе 12).

Мы не советуем ждать воображаемого «предупредительного выстрела», который разбудит мир. Лучше реагировать на предупреждения, которые у нас уже перед носом.

#### «Пробуждающие» катастрофы, скорее всего, не будут вызваны суперинтеллектом.

ИИ, способный стать суперинтеллектом и убить всех людей, -- не тот ИИ, который совершает глупые ошибки и даёт героям шанс в последнюю секунду его выключить. Если враждебный суперинтеллект появился как противник, человечество уже проиграло. Мы обсуждали это в Главе 6. Суперинтеллекты не делают предупредительных выстрелов.

Катастрофа, которая _могла_ бы послужить предупреждением, скорее всего, придёт от гораздо более глупого ИИ. Велик шанс, что она не заставит людей принять меры против суперинтеллекта.

Допустим, террорист с помощью ИИ создаст биооружие. Погибнет много людей. А лаборатории скажут: «Видите? _Настоящий_ риск был в людях. Дайте нам скорее создать ИИ для защиты от пандемий». Может, террорист [взломал](https://llm-attacks.org/) ИИ, [чтобы тот ему помог](https://www.anthropic.com/news/detecting-countering-misuse-aug-2025). А лаборатории скажут: «Это сработало лишь потому, что ИИ был слишком глуп и не заметил проблему. Нужно сделать его умнее и осведомлённее».

Может, это слишком цинично. Хотелось бы верить, что человечество отреагирует мудрее. Но если относительно глупый ИИ вызовет бедствие, а люди в ответ _остановят_ безрассудную гонку к суперинтеллекту, то лишь потому, что они _уже начали о нём беспокоиться_.

Нельзя откладывать подготовку, пока суперинтеллект не попытается нас убить. Будет поздно. Мобилизовать силы надо как можно скорее, чтобы мы были готовы воспользоваться предупредительными выстрелами.

#### Человечество так себе отвечает на сюрпризы.

Мысль, что от достаточно сильного потрясения мир вдруг опомнится и начнёт действовать разумно, кажется нам фантастикой. Коллективная реакция нашего вида на тревожные звоночки от ИИ пока больше смахивает на «полное отсутствие реакции», чем даже на «плохую реакцию». Но получи всё же человечество крупное, страшное и более-менее однозначное предупреждение... не удивимся, если реакция будет вялой, несерьезной или делающей только хуже.

Люди могут отреагировать на предупреждения об опасности ИИ люди отреагируют так же, как на пандемию COVID. Большинство согласно, что с ней справились не лучшим образом (пусть люди и спорят, в чём именно заключались ошибки).

За несколько лет до пандемии некоторые эксперты по биобезопасности опасались, что слабые меры предосторожности в лабораториях однажды приведут к беде. Утечки опасных патогенов -- известное дело. Несмотря на все регламенты, они регулярно [происходят](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents). Особую тревогу вызывало усиление функций вирусов (gain-of-function) -- попытки в лабораторных условиях сделать вирусы более смертоносными или заразными. Они не сулят практически никакой выгоды.[^222]

Потом грянул COVID. Казалось бы, вот идеальный момент ужесточить требования к биобезопасности. Ведь весь мир стал одержим угрозой пандемии. Тем более что эксперты _так и не пришли к единому мнению_: не началась ли _эта_ пандемия со случайной утечки из лаборатории? Учёные до сих пор спорят об этом. Зачастую яростно критикуя оппонентов.

Не будем вдаваться в споры, была ли это утечка. Но согласитесь: даже малого шанса, что эксперименты по усилению вирусов и слабые меры предосторожности в лабораториях только что погубили миллионы людей, должно хватать с лихвой. Обществу стоило потребовать запрета самых рискованных исследований.

Даже в условиях неопределенности выгода запрета кажется очевидной. Это и до пандемии казалось хорошей идеей. А уж после, вот идеальный момент, чтобы заняться проблемой и пресечь её на корню. Не нужно больших усилий или затрат. В мире совсем немного учёных, проводящих опасные эксперименты по усилению вирусов. А польза их работы до сих пор была пренебрежимо мала.

Ничего подобного не произошло. На момент написания этих строк, в августе 2025 года, подобные эксперименты практически беспрепятственно продолжаются по всему миру.[^223] Возможно, решить эту проблему стало даже сложнее. Вопрос теперь слишком политизированный.

COVID определённо смахивает на «предупредительный выстрел», проверяющий нашу готовность к биоугрозам. Мир не воспользовался им для запрета разработки гиперлетальных вирусов.

Чтобы от предупреждения была польза, человечество должно быть готово его услышать и правильно на него отреагировать.

Чтобы небольшая катастрофа спровоцировала жесткие меры -- это не _совсем_ беспрецедентно. Такое уже бывало. Вспомните, как США ответили на теракты 11 сентября (организованные террористами, базировавшимися в основном в Афганистане) свержением правительства в Ираке. Который к атаке отношения почти не имел. В правительстве США были люди, которые _уже_ хотели свергнуть иракский режим. Увидев повод, они выжали из него всё возможное.

Тут может произойти нечто похожее. Политики могут использовать мелкую катастрофу (вызванную глупым ИИ), чтобы добиться запрета на суперинтеллект. Но для этого в правительствах по всему миру должны быть люди, готовые к действию. Мы не должны сидеть сложа руки, ожидая предупреждений. Собираться с силами надо уже сейчас.

#### Пора действовать.

Не исключено, что мы _действительно_ получим новые, более серьёзные предупреждения об опасности ИИ. Если так, надо быть готовыми на них ответить.

Может, небольшая катастрофа настроит общество против ИИ. А может, обойдётся и без катастроф. Скажем, появится новый алгоритм, и ИИ начнут проявлять инициативу так, что это всех перепугает. Или ситуацию переломит какой-то побочный социальный эффект. Или «_Если кто-то его сделает, все умрут_» запустит цепную реакцию и направит мир по лучшему пути.

Но мы настоятельно не рекомендуем сидеть сложа руки и молиться, что «малая катастрофа» откроет людям глаза. Явного предупреждения может и не быть. Или оно может не дать ожидаемого эффекта.

Человеческий род и страны мира не беспомощны. Нам _не нужно_ ждать. Мы можем действовать прямо сейчас,. Доводы в пользу остановки разработки передового ИИ достаточно сильны.

Мы написали «_Если кто-то его сделает, все умрут_», чтобы забить тревогу и побудить мир к немедленным действиям. Но тревога бесполезна, если это лишь повод отложить решение на потом. «Ну, может, какой-нибудь другой сигнал в будущем заставит нас шевелиться». «Ну, раз людей предупредили, может, всё и так обойдётся, без моего личного участия».

В будущем может и не быть «звоночка» погромче. Всё не обязано хорошо закончиться. Но ситуация отнюдь не безнадёжна. У человечества есть выбор. Мы можем действовать на упреждение и _просто не создавать суперинтеллект_. Что будет дальше -- зависит от нас.

### Как остановить _всех_ без шпионского ПО на каждом компьютере?

#### Надо действовать быстро.

Для обучения современных ИИ нужно много очень специфических чипов, работающих в тесной связке. Остановка исследований ИИ потребует закрыть нескольких огромных дата-центров и прекратить производство некоторых передовых специализированных чипов. О ноутбуках из магазина речь не идёт. Большинство людей ничего не заметят.

Сейчас, в 2025 году нет кучи тайных _заводов_ по производству чипов, о которых никто не знает. Лишь несколько производителей делают чипы для передового ИИ. Хотя прямо сейчас некоторые корпорации пытаются запустить новые фабрики.

Кроме того, (опять же, на 2025 год) есть ключевое для производства передовых чипов оборудование, которое продаёт лишь одна компания на Земле: ASML в Нидерландах.

Можно взять и перекрыть кран. Но это положение дел не вечно. Чем скорее подпишут международный договор, тем лучше. Это уже сложнее, дороже и опаснее, чем было бы в 2020 или даже в 2023 году.

### Но вы предлагаете контролировать, сколько передовых чипов есть у частных лиц.

#### Да. А ещё мы выступаем за запрет исследований.

Мы сами не в восторге. Конечно, мы что-то потеряем, если запретим частникам владеть, скажем, более чем восемью видеокартами H100 образца 2024 года.

Но потери _не так_ велики, чтобы на практике выяснять, насколько большой дата-центр ещё безопасен. Слишком низкий лимит -- и несколько человек не продвинут свои интересные проекты. Слишком высокий -- и все умрут.

К тому же, время, когда для создания ИИ нужны огромные вычислительные мощности, не продлится вечно. LLM уже существуют. Запрет создания новых моделей и строительства гигантских вычислительных кластеров не помешает людям изучать работу существующих моделей. Так они могут понять что-то новое о природе интеллекта и изобрести более эффективные алгоритмы. Это может обойти системы контроля.

### Зачем запрещать исследования? Это уж слишком радикально.

#### Новые прорывы могут привести к тому, что создание суперинтеллекта будет невозможно остановить.

Мы уже упоминали, что всю революцию LLM запустила одна статья 2017 года. В ней описали алгоритм, позволивший обучать полезные ИИ на доступном коммерческом железе.

Если мощный ИИ можно будет создать на обычном _потребительском_ железе, для сдерживанию суперинтеллекта понадобятся поистине драконовские меры. И работать они будут не так долго.

Так что поиск ещё более мощных и эффективных алгоритмов ИИ -- смертельный яд для человечества.

Это очень плохие новости. Нам бы очень хотелось, чтобы это было по-другому. Но такова реальность.

Никакой закон не помешает нынешним учёным обдумывать эффективные алгоритмы у себя в голове. Возможно, кто-то создаст подпольную сеть для обмена результатами. Некоторые люди из индустрии ИИ с гордостью заявляют, что человечество _должно_ погибнуть от ИИ. Они могут пытаться идти вперёд, несмотря ни на что. (См. раздел «Почему вас заботят только человеческие ценности?» материалов к Главе 5.)

Но, став незаконными, исследования ИИ _сильно_ затормозятся. Особенно если все поймут, что они действительно грозят нам гибелью. Ещё сильнее, если такие подпольные сети будут выявлять и закрывать с той же решимостью, как людей, обогащающих уран в гараже. Потому что угрозу принимают всерьёз.

_Большинство_ людей не склонны совершать особо тяжкие преступления, которые всерьёз разозлят международные спецслужбы. Запрет на публикацию новых хитрых алгоритмов ИИ остановит, пожалуй, 99,9% людей и почти все корпорации. Оставшимся 0,1% займутся полиция и разведка -- местная, национальная и международная. Да и нынешнего финансирования у этих энтузиастов точно не будет.

Это сильно изменит мир. Сейчас проводить самые опасные в истории безумные эксперименты абсолютно законно. И гигантские корпорации вкладывают в эту гонку миллиарды долларов.

Не знаем, сколько ещё прорывов нужно, чтобы ИИ стали достаточно умными для самостоятельных исследований и создания ИИ ещё умнее. Может, один. Может, пять. Но лучшие алгоритмы так же смертельны, как и лучшее железо. Эти две лошади тянут телегу в одну и ту же пропасть.

### Разве можно остановить технологию?

#### Много технологий запрещено или строго регулируется.

Классический пример: ядерные технологии. Частным компаниям нельзя обогащать уран без надзора правительства. Какой бы полезной ни была дешёвая энергия.

Вообще, человечество неплохо умеет регулировать и тормозить самые разные технологии. США строго регулируют [новые лекарства и медицинские приборы](https://www.fda.gov/), [строительство жилья](https://www.hud.gov/hud-partners/laws-regulations), [атомную энергетику](https://www.nrc.gov/about-nrc.html), [теле- и радиовещание](https://www.fcc.gov/media/radio/public-and-broadcasting), [бухгалтерию](https://www.fasb.org/standards), [уход за детьми](https://childcare.gov/consumer-education/regulated-child-care), [борьбу с вредителями](https://npic.orst.edu/reg/laws.html), [сельское хозяйство](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations) и десятки других отраслей. Во всех штатах нужно сдавать экзамены на лицензии [парикмахера](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) и [маникюрщика](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). В большинстве штатов -- ещё и на [массажиста](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

По нашему мнению, человечество часто регулирует технологии даже слишком сильно. Например, FDA убивает куда больше людей ([бюрократией тормозя создание жизненно важных лекарств](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), чем спасает (не пуская на рынок опасные препараты). Цены на жильё, вероятно, завышены отчасти из-за регуляций о зонировании. США практически уничтожили свою атомную промышленность чрезмерными регуляциями. И серьёзно, лицензии для _парикмахеров_?

Человечество _определённо_ способно тормозить прогресс. Было бы трагично и нелепо тормозить медицину, строительство и энергетику, но не одну из редких технологий, которая действительно нас погубит, если её разработать.

#### Запрет может быть точечным.

Запрет разработки передового ИИ не должен касаться обычных людей. Не надо избавляться от современных чат-ботов или беспилотных автомобилей.

Мало у кого в гараже есть десяток топовых видеокарт для ИИ. Почти никто не управляет огромными дата-центрами. Большинство запрета на разработку ИИ _даже не почувствует_. Разве что ChatGPT будет обновляться не так часто.

Человечеству не придётся отказываться от нынешних ИИ. ChatGPT никуда не денется. Можно и дальше учиться встраивать его в нашу жизнь и экономику. Это и так принесёт больше перемен, чем мир видел за многие поколения. Да, мы упустим какие-то _новинки_. То, что появилось бы с ИИ поумнее, но ещё не готовым всех убить. Не то чтобы общество их сильно требовало.

Зато мы будем жить. И наши дети тоже.

Действительно _нужное_ людям -- например, новые медицинские технологии для спасения жизней, -- можно создавать и _не стремясь_ к суперинтеллекту. Мы одобряем исключения для медицинского ИИ. Главное, с надлежащим контролем, и держаться подальше от опасной универсальности.

Правительствам, которые стремятся избежать появления враждебного суперинтеллекта, придётся следить, чтобы чипы не использовали для создания более мощных ИИ. Какие сервисы и разработки разрешать -- зависит от [механизмов контроля](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development). Они должны обеспечивать, что никто не ведёт опасных разработок. Чем надежнее эти механизмы, тем дешевле обойдется остановка прогресса. Можно будет разрешить больше всего.

Ещё одна полезная (но недостаточная сама по себе) мера -- встроить в чипы для ИИ аварийные выключатели, мониторить крупные дата-центры и ввести протоколы экстренной остановки. Ядерные реакторы проектируют так, чтобы в случае аварии их можно было немедленно заглушить. Если вы согласны, что суперинтеллект грозит нам вымиранием, вывод очевиден. Чипы и дата-центры нужно делать так, чтобы регуляторы могли их легко обесточить.

Не надо уничтожать технологии из ненависти к ним.[^226] Просто надо уйти с дороги, ведущей к гибели человечества.

#### Значительная часть проблемы -- люди не осознают нависшую угрозу искусственного суперинтеллекта.

По нашему опыту, считающие, что гонку к суперинтеллекту не остановить, просто не понимают, что если кто-то его сделает, все умрут.

«Но ИИ несёт огромную пользу!» Нет. Толку от мощи суперинтеллекта, если он всех убьёт. Чтобы была польза, нужно найти, как пережить его появление.

«Но АЭС пугают, потому что ассоциируются с бомбами, которые стирают города с лица земли. А ИИ -- с безобидными штуками вроде ChatGPT!» Верно. Пока что. Если люди так и не поймут, что суперинтеллект, выращенный современными методами, нас погубит, его и не остановят. Но проблема не в неумении контролировать новые технологии (как ядерное оружие или энергетику). Проблема, что _люди не осознают угрозу_.

Мы потому и написали книгу. В последней главе мы обсуждаем, что человечество способно на многое, если достаточно людей понимает суть проблемы.

### Не даёт ли это правительствам слишком много власти?

#### У них уже есть полномочия запрещать опасные технологии.

Запрет создания ИИ умнее человека мало что изменит в плане полномочий государства. Власти и так регулируют уйму вещей. Для _области ИИ_ ограничение одной программы исследований, возможно, серьёзное событие. Но для _правительства_ и общества это капля в море. Мы привыкли, что государство много куда вмешивается. И прецеденты запрета опасных технологий уже есть. Например химического оружия.[^227]

Запрет ещё одной технологии не ввергнет мир в тоталитаризм. Договоры о ядерном оружии вот не ввергли.

Мы не говорим, что запрет технологии -- _пустяк_. Мы не считаем, что государство должно вмешиваться _во что попало_. Но суперинтеллект -- это настолько серьёзно, что любая осмысленная планка достигнута.

Реши человечество остановить разработку ИИ сегодня, запрет не стал бы особо обременительным. Сейчас для создания передового ИИ нужно невероятное количество специализированных чипов и уйма электричества. Возможно, если мы допустим улучшение чипов и алгоритмов, через десять лет для разработки мощной системы хватит обычного ноутбука. Но этого можно не допустить. Тогда правительствам не придётся вмешиваться в жизнь обычных людей сильнее, чем при контроле за ядерным оружием. Но это если мир очнётся и примет меры _сейчас_.

### А некоторые страны не откажутся от запрета?

#### Если они поймут угрозу, нет.

Речь о технологии, способной убить всех на планете. Осознав проблему, поняв, что никто в мире и близко не научился заставлять ИИ слушаться людей после превращения в суперинтеллект, любая страна будет замотивирована не торопиться. Тогда они сами отчаянно захотели бы подписать договор и обеспечивать его исполнение. Просто из страха за собственные жизни.

Северная Корея нарушила международное право, создав ядерное оружие. Но не применила его против своих врагов. Они понимают: в ядерной войне победителей не будет. Лидеры стран могут играть с огнём или даже воевать. Но они не стремятся к самоуничтожению.

Когда представляют, что какая-то страна нарушит договор, кажется, думают о лидерах, которые просто не понимают угрозу. Которым кажется, что расклад: «95% вероятности, что СИИ принесёт создателю богатство и власть, и 5% -- что всех убьёт». Тогда, конечно, кто-то мог бы рискнуть. Возможно, какое-то государство действительно так и оценивает шансы.

Но теория и свидетельства говорят о другом раскладе. Всё указывает на то, что эта технология -- глобальное самоубийство. Никто и близко не готов извлекать из машинного суперинтеллекта пользу. Если мир это осознает, у стран-изгоев будет куда меньше причин нарушать договор. Они тоже не хотят умирать.

А даже если лидер какой-то гипотетической страны-изгоя действительно не поймёт угрозу СИИ, он окажется в окружении международного альянса, который понимает. Ведущие мировые державы смогут вмешаться. Этого достаточно, чтобы мотивации не было.

Представьте, если бы лидеры (например) США, Китая, России, Германии, Японии и Великобритании искренне поверили, что _их собственное выживание_ зависит от того, чтобы никто не создал суперинтеллект. И чётко заявили бы, что любые попытки его создать расценивают как угрозу своей жизни. И что они готовы защищаться. Тогда даже несогласный мировой лидер вряд ли захотел бы испытывать судьбу в борьбе с такой коалицией.

Разработка ИИ -- не гонка к военному превосходству. Это гонка к гибели. Если мировые лидеры это поймут, если осознают, что это убьёт их самих и их детей, они, скорее всего, будут честно блюсти договор и следить за его исполнением.

Понять мысль, что создание машин умнее всего человечества может погубить мир, _не так уж сложно_. Легко увидеть, что мы слишком мало знаем о разумах, которые выращиваем. Вопрос лишь, поверят ли в это мировые лидеры. Если _да_, остановить эту самоубийственную гонку вполне реально.

$$$

#### Договор потребует реального отслеживания и контроля за исполнением.

Даже если _большинство_ стран поймёт, что «если кто-то его сделает, все умрут», какие-то могут не согласиться. И оказаться достаточно безрассудными, чтобы всё равно продолжить создание суперинтеллекта.

Необходим мониторинг. Необходимо силовое обеспечение. Договоры о ядерном, биологическом и химическом оружии показывают, как это можно делать. Мы можем и должны сделать так, чтобы попытки обойти запрет стали сложными и дорогими.

Международный запрет на передовой ИИ придётся соблюдать неукоснительно. Если какое-то государство решит продолжать разработки вопреки международному давлению, странам, подписавшим договор, возможно, придётся применить военную силу.
<>

Это не лучший вариант! Нужно сделать всё, чтобы всем было ясно: сила будет применена. Это поможет избежать ошибок, из-за которых до неё действительно дойдёт дело. Но если и есть причина, способная оправдать точечный военный удар — или даже войну, если нарушитель пойдёт на эскалацию, — то спасение человечества точно к таким относится.
<long>

Этот метод уже работал.
<>

С момента создания атомной бомбы прошло более восьмидесяти лет, и человечество неплохо справилось с контролем над её распространением. Вопреки прогнозам многих экспертов после Второй мировой войны, масштабной ядерной войны так и не случилось.
<>

В июне 2025 года правительство США даже нанесло ограниченный удар по Ирану, пытаясь помешать ему создать ядерное оружие. Подобные договоры и силовые методы — не новость для мирового порядка. Если бы мы смогли выиграть себе восемьдесят лет до появления СИИ, этого вполне могло бы хватить.
<>