# Глава 10: Проклятая задача

Задача согласования ИСИ: как получить от искусственного суперинтеллекта (ИСИ) полезную работу, причём надёжно и без катастроф. Много присущих этой задаче свойств делают её очень сложной.

Ниже -- ответы на вопросы читателей десятой главы «_Если кто-то его сделает, все умрут_». Мы разберем пользу исторических аналогий и обсудим планы, которые могут упростить задачу. Чтобы не пересказывать книгу, мы _пропустим_ эти вопросы:

- Что делает сложной инженерную задачу?
- С какими трудными задачами человечество уже сталкивалось? Какие уроки из них можно извлечь тут?
- Что делать, если заранее известно -- задача сложная? Что надо делать _по-другому_?

В «Расширенном обсуждении» мы разбираем, почему у нас есть лишь одна попытка согласования. А ещё о том, сколько теории и знаний потребовалось для обеспечения безопасности первого в мире ядерного реактора.

## Часто задаваемые вопросы

### А ИИ разве не будет отличаться от исторических примеров?

#### Будет.

Некоторые уникальные особенности согласования ИИ упростят задачу по сравнению, например, с проектированием АЭС. Другие -- усложнят. В целом, кажется, управлять ядерным оружием и электростанциями куда проще, чем ИИ умнее человека.

Специалисты индустрии часто говорят, что сам ИИ может помочь с собственным согласованием. Мы не думаем, что это сильно изменит ситуацию. Грубо говоря, любой ИИ, способный понять, как согласовать суперинтеллект, сам достаточно опасен. Он сам уже должен быть согласованным (подробнее об этом в Главе 11).

Другое возможное упрощение по сравнению с АЭС: у людей много контроля над тем, как работает создаваемый ими ИИ. Законы физики в реакторе не изменишь. Но при создании ИИ можно определять законы его мышления. Если, конечно, точно знать, что делаешь. (Но, как мы описывали в Главе 2, вообще-то никто и близко не подошёл к такому уровню понимания).

А теперь о том, почему ИИ, скорее всего, окажется _сложнее_ прошлых вызовов человечества. Сравним суперинтеллект с ядерным оружием. В [открытом письме](https://aistatement.com/), упомянутом в начале книги, сказано: «Снижение риска вымирания от ИИ должно стать глобальным приоритетом наряду с другими всеобщими рисками, такими как пандемии и ядерная война». Как же ИИ выглядит на их фоне?

Откровенно говоря, мы считаем, что такое сравнение недооценивает ИИ. Вот несколько причин:

1. Ядерное оружие не умнее человечества.
2. Ядерное оружие не самовоспроизводится.
3. Ядерное оружие не самосовершенствуется.
4. Большинство реалистичных сценариев ядерной войны не предполагают полного уничтожения человечества. Скорее всего, среди руин останутся люди, чтобы всё отстроить.
5. Никакие компании не увеличивают мировые запасы ядерного оружия в десять раз каждый год.
6. Наука о ядерном оружии хорошо изучена. Инженеры могут примерно рассчитать мощность бомбы до её постройки. Они точно знают, какая концентрация радиоактивного материала приведёт к цепной реакции и катастрофическому взрыву.
7. У ядерного оружия нет собственных планов. Страна создала бомбу -- страна владеет бомбой. Учёным не нужно беспокоиться, что бомба станет намного умнее их и решит, что не хочет быть чьей-то собственностью.
8. Мир в целом согласен: если ядерное оружие взорвётся, погибнут люди. Нет идеологических фракций физиков со странными позициями вроде «Если дать каждому свою бомбу, мы не будем зависеть от плохих людей с бомбами», или «Всё нормально, люди просто сольются с ядерным оружием», или «Ядерная война неизбежна, так что пытаться остановить её -- глупо и по-детски».
9. Ядерное оружие трудно воспроизвести. Никто не прикладывает огромных усилий на разработку технологии, которая позволит кому угодно сделать бомбу. Создание в лаборатории одной боеголовки не позволяет через неделю развернуть сто тысяч.
10. Великие державы считают ядерную войну реальной возможностью и неприемлемым исходом. Мировые лидеры искренне считают её злом и стараются избежать. Даже самые эгоистичные из них знают, что война может убить их самих и их семьи, и уничтожить всё, что им дорого. Граждане и избиратели не хотят войны. Человечество едино в неприятии ядерной войны настолько, насколько это вообще возможно.

Так что да, подобрать аналогию для сложности ИИ трудно. Он принесет свои, новые проблемы. Для начала стоит понять, что проблемы вообще будут. Потом -- добавить к этому факт, что у человечества только одна попытка (как мы обсуждали в книге и разделе ниже). И вот -- ситуация выглядит совсем скверно.

#### С ИИ всё иначе, потому что второго шанса не будет.

Ключевое отличие этой ситуации: когда первопроходцы ошибаются (а в науке это обычное дело) -- все умирают. Второго шанса нет. Это делает задачу качественно иной.

История великих достижений человеческого гения -- это история ошибок и обучения на них. Люди рисковали лишь собой, а выигрывало всё человечество. Поэтому они были безусловными героями. Иногда безрассудными, но героями. Если и был способ подняться, не ломая хребты таким смельчакам, если можно было согреться не у их погребальных костров -- мы этого способа не знаем.

Суперинтеллект сюда не вписывается. Представьте: вы досконально изучили «зародыш» ИИ. Полностью расшифровали его мышление. Создали отличную теорию его работы. Оттестировали её на куче примеров. Вы используете эту теорию, чтобы предсказать изменения в разуме ИИ при превращении в суперинтеллект, когда у него впервые появится реальная возможность захватить мир. И вы всё равно полагаетесь на новую, Но даже в этом случае вы, по сути, используете новую, непроверенную теорию. Вы пытаетесь предсказать результаты ещё не проведенного эксперимента: что ИИ сделает, когда действительно появится настоящий шанс отнять у людей власть.

Первые версии научных теорий часто ошибочны. Чем менее ваши наблюдения точны, чем больше у вас алхимии вместо науки, тем вероятнее, что ранние теории неверны.

Даже отличные теории могут ошибаться в экстремальных случаях. Вспомните теорию гравитации Ньютона. Она блестяще предсказывала события. Она помогла открыть новые планеты. Но она оказалась неверной на больших скоростях и расстояниях, и уступила теории Эйнштейна. Что, если наша первая теория о развитии мышления ИИ при переходе к суперинтеллекту окажется слегка неточной в таких «предельных режимах»? Если созданный на её основе ИИ станет суперинтеллектом и его цели будут отличаться от «добра», нам конец. Он воспользуется шансом, сотрёт человечество с лица Земли и построит пустое, бессмысленное будущее. Переиграть не получится.

И это при наличии у нас полноценной теории интеллекта. С кучей подтверждающих экспериментальных данных.

Для реального шанса пережить такое испытание, цивилизация должна сказать: «Постойте! Давайте проработаем теорию для высоких скоростей и больших расстояний. Давайте проверим, где она ошибается в граничных случаях». Даже вопреки массе подтверждающих данных. Такая цивилизация помнит ошибку Ньютона. И она понимает: тут второго шанса не будет.

Наша цивилизация не такая. Нам до этого далеко. Мы генерируем кучу бестолковых идей. Ответственные за них люди «увольняются по личным причинам». А мир этого почти не замечает. Никто не формулирует допущения, на которых строится безопасность, чтобы можно было вовремя заметить их нарушение. Никто не пишет детальных планов: что делать, что для этого нужно, как сложно это получить.

Профессионалы из нормальной, разумной цивилизации, посмотрев на Землю, зарыдали бы.

### Сколько времени займёт решение задачи согласования ИСИ?

#### Проблема не только в нехватке времени. Проблема -- что ошибки смертельны.

К 500 году н.э. мир сошелся во мнении, что Солнце вращается вокруг Земли. Позже возникла теория Коперника. Её рассмотрели и в основном отвергли. И лишь когда Галилей с помощью телескопа увидел луны Юпитера -- вращающиеся вокруг него, а не Земли, -- молодая наука пришла к верному выводу. Земля всё же вращается вокруг Солнца.

Со временем человечество постигло правильную теорию небесной механики. Но сперва люди пришли к ложному согласию. И они яростно цеплялись за него. До тех пор, пока реальность не ошарашила Галилея по голове тем фактом, что Земля -- не центр мироздания.

Обычно путь науки к истине так и выглядит так. Учёные ошибаются, а реальность бьёт их по голове свидетельствами, пока они не обновят свои модели.

Проблема согласования ИСИ не только в сложности самой задачи. Беда в том, как здесь выглядит «удар реальности по голове» за ошибочную теорию. Тут это значит, что недружественный ИСИ поглотит планету. И не останется в живых никого, кто мог бы разработать теорию получше.

Будь у нас сто лет _и бесконечные попытки_, мы бы, наверное, решили задачу согласования ИСИ без особых проблем.

Но будь у нас даже триста лет. Мы создадим теорию интеллекта, изучим, как меняется умнеющий ИИ и как задать ему устойчивую цель. Но без возможности _взять и посмотреть_, что бывает, когда ИИ становится радикально умнее, мы всё равно, скорее всего, придём к неверному ответу. У нас не будет жизненно важных свидетельств. Люди склонны останавливаться на таких ошибочных выводах.

### А что, если разрабатывать ИИ неспешно и так же плавно внедрять его в общество?

#### Скорее всего, это закончится катастрофой.

Мы предсказываем конечный итог, а не путь к нему. Мы не знаем, что будет происходить с ИИ до того момента, как он станет по-настоящему опасен.

Кто знает, может, всё случится за полгода. Выяснится, что «глупые» ИИ при долгом размышлении неплохо умеют исследовать самих себя. Начнётся цепная реакция. А может, прогресс встанет. Лет шесть уйдёт на ожидание важной идеи. Ещё лет шесть -- на её доработку. За эти двенадцать лет ИИ успеет удивительно перекроить образование и рынок труда.

(Да, мы видим ваш скептический прищур. Мы в курсе, что «[заблуждение о неизменном объёме работ](https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq)» -- действительно заблуждение. Суть не в этом. Наши догадки о влиянии ИИ на рынок труда в ближайшем будущем не так уж важны на фоне того, что случится потом.)

В дополнительных материалах к шестой главе (раздел «А нельзя улучшить людей, чтобы они не отставали от ИИ?») мы говорили, что люди вряд ли угонятся за развитием ИИ. Даже если срочно займутся усилением собственного интеллекта. (В Главе 13 мы всё равно призываем к такому усилению. Но это не позволит идти в ногу с машинами, если не остановить их разработку.)

Так что, каким бы странным и интересным ни было то будущее, власти у машин там будет всё больше. Как только группа ИИ сможет забрать себе ресурсы планеты, мы пройдём точку невозврата. Если никому в этой группе не будут дороги счастливые, здоровые и свободные люди, -- плохи наши дела.

### А если ИИ будет много и все разные?

#### Это не поможет, если мы не научим хотя бы одного из них ценить добро.

Как мы писали в Главе 4, путей, ведущих ИИ к странным, чуждым и незапланированным целям -- масса. Мы можем создать хоть миллиард ИИ. Это не поможет, если все они будут стремиться к разным, но одинаково нелепым вещам. Нам не выжить, если мы не создадим хотя бы один ИИ, которому по-настоящему важны здоровье, счастье и свобода людей. И важны не на словах, пока он ещё «молод», а на деле. Забота о людях должна быть самым эффективным решением тех задач, которые он (или его потомки) будет выполнять. См. Главу 5.

Умей мы сделать как надо хотя бы один ИИ из десяти, можно было бы наклепать их побольше в надежде, что хорошие экземпляры выторгуют для нас десятую долю Вселенной. Но (см. Главы 2–4) при нынешнем подходе, когда мы «выращиваем» разум, получить ИИ с нужной заботой о людях невероятно сложно. Это не шанс один к десяти. Это «такое не происходит, если точно не знать, как это сделать». Мы и близко не подошли. Даже с миллиардами ИИ, если им всем на нас наплевать, и миллиардной доли Вселенной нам не достанется.

## Расширенное обсуждение

### «До» и «После»

Повторим сказанное в главе. Главная сложность с ИИ:

Нужно согласовать ИИ __До__ того, как он наберет силу, чтобы вас убить (или помешать согласованию). И это согласование должно _перенестись на совсем другие условия_ -- __После__. Когда суперинтеллект[^187] (или их группа) уже сможет вас уничтожить, если захочет.

Иначе говоря: создавая суперинтеллект, вы должны согласовать его без единой возможности проверить свои методы в тех условиях, где это действительно имеет значение. И неважно, насколько «эмпирической» кажется работа с системами, которые пока не могут вас убить.

Ни исследователи ИИ, ни инженеры из почти всех других областей не привыкли к таким требованиям.

Нам часто жалуются: мол, мы требуем чего-то ненаучного, оторванного от опыта. В ответ советуем поговорить с создателями космических зондов.

Природа несправедлива. Иногда критически важная среда -- не та, где можно провести тесты. И всё же, бывает, что инженеры хорошо подготавливаются и справляются с первого раза. В таких случаях у них есть _глубокое понимание, что они делают_ -- надежные инструменты и мощные предсказательные теории. То, чего в области ИИ явно не хватает.

Суть проблемы: ИИ, _который безопасно тестировать, неудача с которым вас не убьёт_, работает не в том режиме, что ИИ (или экосистема из ИИ), которую _надо протестировать, потому что если он несогласованный, все умрут_. Первый ИИ не наблюдает реальную возможность всех убить, даже если хочет. А вот второй эту возможность видит.[^188]

Представьте, что вы хотите назначить своего коллегу Боба диктатором страны. Сначала вы назначаете его «потешным» диктатором города, чтобы проверить, не будет ли он злоупотреблять властью. Увы, проверка так себе. Опция «приказать армии запугать парламент и „присмотреть“ за выборами» -- принципиально отличается от «поиграться с понарошковой властью на глазах у горожан (которые могут мне навалять и уволить)».

При наличии хорошей теории мышления можно попытаться «прочитать мысли» ИИ. Предсказать, что он будет думать, когда решит, что возможность захватить власть действительно есть.

Можно использовать симуляции (и подделывать «ощущения» ИИ и всякое такое). Ваша теория подскажет, как сделать их похожими на реальную ситуацию с опцией предательства.

Но связь между лабораторными тестами и реальной ситуацией _принципиально полагается на вашу непроверенную теорию мышления_. А разум ИИ может сильно измениться, когда он станет суперинтеллектом!

Если ИИ создаст более умных преемников, их «начинка», вероятно, будет устроена по-другому. Изучив разум До, вы применяете знания к разуму После. Вы зависите от гипотезы о том, как мышление меняется от До к После. _Непроверенной_.

Позволить ИИ работать, пока он _действительно_ не получит возможность вас предать (которую сложно подделать), -- это эмпирический тест. Он принципиально отличается от всего, что вы можете сделать в лабораторной среде.

Многие учёные (и программисты) знают: теории о работе сложных систем в принципиально новой среде _редко срабатывают с первого раза_.[^189] У такой задачи «несправедливые» требования к предсказуемости, контролю и пониманию слабо исследованной территории. А если надежды инженеров не оправдаются -- мы все погибнем.

Так что мы считаем: причин, почему исследователям сломя голову гнаться за прогрессом ИИ, не просто достаточно, а _с огромным запасом_. Это чистое безумие. И дозволять это -- такое же безумие для любого правительства.

### История «Чикагской поленницы-1»

В 1942 году под руководством Энрико Ферми построили «Чикагскую поленницу-1». 45 000 графитовых блоков общим весом 330 тонн, 4,9 тонны металлического урана и 41 тонна оксида урана. Разместили всё это под трибунами кортов на стадионе Чикагского университета. В зависимости от определения, что считается ядерным реактором, она была или не была первым. Энергию для промышленности она не давала. Но это было первое устройство, где шла устойчивая цепная реакция.

По нынешним меркам безопасностью там пренебрегли. Например, построив реактор под спортивными трибунами университета в центре крупного города.

Генерал Гровс, глава Манхэттенского проекта, хотел устроить эксперимент _рядом_ с Чикаго, а не _в самом городе_. Он даже распорядился построить для этого здание. Но стройка затянулась. Артур Комптон, нобелевский лауреат и профессор Чикагского университета, приютившего проект, не стал просить разрешения у ректора. Позже Комптон объяснил: ректор был бы обязан отказать, а это было неправильным ответом.

Блоки укладывали вчерашние школьники, решившие подзаработать в ожидании призыва в армию.

Уран заключили в семиметровый резиновый куб, а не в стальной корпус. Конечно, никакого гигантского бетонного саркофага вокруг не было.

Позже, узнав всё это, Джеймс Конант, председатель Национального комитета оборонных исследований, побледнел. Даже для сороковых годов такой подход к науке считался не совсем нормальным.

Читая об этом в учебнике истории и не зная финала, можно подумать, что это пролог к грандиозной катастрофе. Здесь нет массы вещей, которые в культуре 2025 года считаются стандартом безопасности. Где инспекторы с планшетами? Где толстые тома регламентов? Где степенные дискуссии комитетов? Где оценка рисков? Где правила, разрешающие укладывать урановые кирпичи только Людям С Дипломами? Где вся _бюрократия_?

Но куча урановых и графитовых кирпичей не расплавилась.

Причина проста: Ферми знал, что делает. Он заранее просчитал правила игры.

Ферми не просто складывал загадочные кирпичи, которые нагревались от близости друг к другу. Он знал, что некоторые атомы урана будут спонтанно распадаться. Знал, что это деление породит нейтроны. Знал, что эти нейтроны иногда будут врезаться в другие атомы урана и вызывать новые распады.

Ферми всё понимал _заранее_. Ему не пришлось на горьком опыте узнавать, что он имеет дело с экспоненциальным процессом. И не как современные СМИ называют «экспонентой» всё большое или быстрое. Процессом, где скорость роста пропорциональна текущему уровню. С _математической_ экспонентой.

Ферми знал: добавляя урановые и графитовые блоки, он увеличивает _основание степени_. Мы уже это обсуждали. Есть огромная разница между коэффициентом размножения нейтронов ниже 100% и выше.[^190] Ниже -- у вас просто тёплая куча кирпичей. Выше -- радиоактивность сборки растёт. И растёт. И растёт.

Она ведёт себя не так, как предыдущие кучки урана поменьше. Если не понимать всё достаточно хорошо, чтобы правильно настроить замедлитель (притушивающий цепную реакцию при перегреве), реактор не стабилизирует сам себя, как малые сборки. Оставь его работать на ночь -- и на утро получишь вовсе не полезную промышленную мощность.

Куча просто становилась бы всё более радиоактивной. Пока не загорелся бы графит или не расплавился бы уран.

Приехали бы пожарные. А пожар странный: не перестаёт выделять жар, даже когда его заливают водой.

1942-й стал бы не лучшим годом для учёбы в Чикагском университете.

Но Ферми всё это знал! Так что всё обошлось. Когда 2 декабря 1942 года он приказал вытянуть управляющий стержень (деревянную планку, обитую кадмием) ещё на двенадцать дюймов, он заранее объявил: это именно тот шаг, после которого уровень радиации «будет расти дальше и дальше... он не стабилизируется».

И радиоактивность удвоилась за две минуты. И ещё раз удвоилась. Реакции дали поработать двадцать восемь минут. Мощность удваивалась каждые две и выросла примерно в 16 000 раз.

Рост радиоактивности в 16 000 раз был ожидаемым поведением. Его верно предсказали и детально поняли заранее. Это не было внезапным сюрпризом для кого-то, кому просто велели навалить в десять раз больше урановых кирпичей, чем в прошлый раз, и поглядеть -- не случится ли чего интересного и прибыльного.

Как мы обсуждали в книге, между ядерным реактором и ядерным взрывом очень тонкая грань. Если точно -- чуть больше половины процента. Такова разница между реактором, дающим промышленную энергию, и реактором, который взрывается.

Иначе говоря: чтобы ядерная реакция вообще начала работать, нужно её нехило разогнать. Но как только она наберёт эту силу, стоит добавить ещё хоть _чуточку_, зайти ещё на 0,65 процента дальше -- и прогремит взрыв.

Реальность имеет право иногда подкидывать вам такие задачи. Бывает.

Но Ферми, Силард и их команда просчитали все эти правила до того, как столкнуться с ними. Они знали о запаздывающих и мгновенных нейтронах (см. Главу 10). Поэтому Ферми довёл коэффициент размножения до 100,06 процента, и _не стал_ вытягивать стержень дальше, чтобы посмотреть, что получится. Он дошёл до критического состояния, но не шагнул на ещё 0,65 процента к надкритичности на мгновенных нейтронах. Ферми получил предсказанный результат. Он _знал_, что произойдёт за чертой. И поэтому не переступил её.

Двадцать восемь минут спустя, когда радиоактивность выросла в 16 000 раз, Ферми заглушил первый в мире ядерный реактор -- сложенные урановые кирпичи под трибунами университетского стадиона в центре мегаполиса.

Уточним: мы не утверждаем, что Ферми вёл себя абсолютно ответственно только потому, что у него была вроде бы непротиворечивая модель физики низких энергий. Ферми мог ошибаться. В истории ядерной инженерии бывали сюрпризы.

Мощность взрыва при испытании термоядерной бомбы[^191] «Касл Браво» в три раза превысила расчётную. Причиной стала смесь лития-6 и лития-7 в топливе. Создатели оружия знали, что литий-6 даёт мощный выброс энергии при синтезе, а от лития-7 ничего такого не ждали. Оказалось, что литий-7 _не_ инертен.

Ферми запустил реакцию на низкой мощности. Не на уровне, нужном для промышленности. Так он избежал многих проблем мощных, полезных реакторов. Но представьте, что существовали бы неизвестные ускоряющие реакцию факторы, о которых Ферми не знал. Нечто вроде сюрприза на испытаниях «Касл Браво». Если бы при росте потока нейтронов в 16 000 раз коэффициент размножения скакнул с 1,0006 до 1,02 быстрее, чем человек успеет сбросить аварийный кадмий, -- была бы сегодня в Америке Чикагская зона отчуждения.

И всё же мы не говорим, что Ферми точно зря провёл этот эксперимент. Он не грозил гибелью всему человечеству. Может, цель и оправдывала риск, что при неполном понимании получается Зона отчуждения. Да, к 1945 году нацистская Германия так и не приблизилась к созданию бомбы. Но в 1942-м никто этого не знал. Делать такие прогнозы трудно. Собирать реактор _не в городе_ было неудобно. На войне неудобства обходятся дорого.

Мы рассказываем эту историю без оценочного суждения. Длч этого пришлось бы закопаться в исторические детали. Какие у этих людей были варианты? Не упустили ли они пути получше?

Наш урок -- о разнице между стереотипной «безопасностью» и реально нужным, чтобы выжить.

Бюрократы очень любят внешние, показные меры. У «Чикагской поленницы-1» они напрочь отсутствовали. Катастрофу предотвратило понимание. Не «театр безопасности». Знаний Ферми оказалось достаточно. Могло и не хватить, но хватило. Реальность требовала именно глубокого понимания, а не бюрократической показухи.

Мало было бы толку от инспекторов в строгих костюмах, если бы ни у кого не было глубокого понимания процессов внутри груды странных металлических кирпичей. Не помогли бы и красивые официальные инструкции. И требования допускать к укладке только Сертифицированных Операторов.

Представим мир, где «Чикагскую поленницу» строили _без_ Энрико Ферми. Мир, где никто не знал истинных законов, управляющих загадочными самонагревающимися кирпичами.

Возможно, даже там другой учёный смог бы разглядеть смертельную угрозу, пока не стало слишком поздно. Мог бы произойти диалог вроде этого:

> __Сальвиати__: Мощность кирпичей при сближении резко растёт. Это явно указывает на самоусиливающийся процесс. Математические модели таких процессов обычно имеют одну особенность: если зайти слишком далеко, происходит взрыв.
>
> __Симпличио__: Чушь! По-научному -- верить, что в реальности у любого такого процесса есть предел. Они не могут расти до бесконечности! Так что складывать уран и графит совершенно безопасно. Процесс упрётся в потолок и вреда не будет.
>
> __Сальвиати__: Это всё равно что считать сверхновую безопасной, ведь она не может нагреться до бесконечности. Или называть искусственный суперинтеллект безобидным, ведь он не может стать бесконечно умным. Или утверждать, что пуля не пробьёт кожу, ведь её скорость ограничена. То, что предел существует, не значит, что он безопасен. Все наши модели самонагрева кирпичей указывают: где-то есть критический порог. Если его перейти, куча взорвётся и поубивает всех вокруг.
> 
> __Симпличио__: Но учёные не могут даже договориться, где этот порог! Скажи наука твёрдо, что класть ещё один кирпич опасно, я бы остановился. Но раз учёные спорят, зачем волноваться?
>
> __Сальвиати__: Когда [многие](https://youtu.be/KcbTbTxPMLc?feature=shared&t=1580) [ведущие](https://www.youtube.com/watch?v=PTF5Up1hMhw&t=2283s) [учёные](https://aistatement.com/) предупреждают о риске смертельного взрыва, невозможность точно рассчитать, когда он произойдёт, должна пугать ещё _больше_. Знай мы точно, как работают кирпичи, нашли бы узкую зону, где можно безопасно извлекать энергию. Но учёные всё ещё спорят. Значит, мы _не ведаем_, что творим! Значит, не время играть с цепной реакцией, которая сегодня греет кирпичи, а завтра взорвёт их вместе с нами. Сначала разберитесь с _наукой_.

С ИИ мы и близко не подошли до малой доли уровня понимания, который был у Ферми с ядерными реакциями.

Если мы продолжим этот путь, то в какой-то неизвестный момент на полной скорости влетим в ситуацию куда страшнее радиоактивного заражения Чикаго.

[^187]: Иногда нам говорят, что всё хорошо, ведь можно создать несколько суперинтеллектов, чтобы они следили друг за другом. Есть много причин, почему эти предложения кажутся нам совершенно неправдоподобными. Но подчеркнём всё ту же главная проблема: _у нас лишь одна попытка, чтобы эта хитрая схема сработала._

	Можно тестировать и наблюдать До. _Не_ ставя на кон жизнь всех людей. Но решающий случай будет другим. (И схема должна быть очень умной. Мы ведь понятия не имеем, как заставить хоть один ИИ из этой группы заботиться о нас -- см. раздел «А если ИИ будет много и все разные?» выше).

[^188]: Можно попытаться обмануть слабый ИИ. Заставить его _ошибочно_ считать, что он может получить решающее преимущество. И обучать этим не пользоваться. Но вы будете обучать систему, достаточно глупую, чтобы дать себя провести (см. раздел «Умные ИИ замечают ложь и возможности» в материалах к Главе 3). Она будет видеть фальшивое оружие вместо настоящего. Так что потенциально смертельная ситуация будет заметно отличаться от учебной. Есть большая разница: вам _сказали_, что у вас есть оружие против операторов, или вы сами создали его (или путь к побегу) и детально понимаете, как оно работает. ИИ, который ведется на обман, -- не тот же ИИ, который видит реальные возможности.

	Механизм согласования, работающий на ИИ, которых можно обдурить, проверен только До. Но работать он должен После.

[^189]: Например: механика Ньютона давала потрясающе точные прогнозы. Это простая, сжатая математическая теория с огромной объяснительной силой. Она разбила все прошлые идеи в пух и прах. Но попробуйте с ней отправить груз к далеким планетам на релятивистских скоростях. Вам крышка! Ньютоновская механика не учитывает релятивистские эффекты.

	Предупреждали бы лишь мелкие намеки. Свет движется с одинаковой скоростью во всех направлениях круглый год. Свет огибает Солнце во время затмений. Перигелий Меркурия чуть смещается относительно расчетов Ньютона. Мелкие аномалии против огромной кучи успешных предсказаний из самых разных областей.

	Представьте: до открытия механики Ньютона странные пришельцы предложили Земле сделку. Мы получим огромные богатства за межзвездную доставку, но если провалим -- нас уничтожат. И вот ученые открывают механику Ньютона. И уговаривают, что ну _теперь-то_ можно согласиться. На их стороне -- горы эмпирических свидетельств. Их новые знания позволяют изобретать мощные технологии.

	Представьте, какая _твердость духа_ нужна тем, кто принимает это решение, чтобы сказать: «И все же вы не можете объяснить смещение перигелия Меркурия. Так что ответ -- "нет"».

	Ученым это показалось бы жуткой несправедливостью! У них ведь столько доказательств!

	(Вообще, скорее всего человечество _не поняло_ бы, что надо отказать. Поэтому мы не надеемся на международную коалицию и считаем, что Земле нужно полностью отступить от этой задачи. См. раздел «Почему бы международной коалиции не разработать безопасный ИИ совместно, а не запрещать?» в материалах к Главе 12.)

	Природе плевать на горы свидетельств и предсказаний физики Ньютона. Когда мы переходим к энергиям и масштабам, далеким от наших наблюдений, теория все равно рушится. Она просто не работает на высоких энергиях и больших расстояниях.

	Заставить научную теорию сработать с первой же критически важной попытки -- трудно.

[^190]: Как отмечено в примечании 6 к Главе 10, физики обычно не указывают коэффициенты размножения нейтронов в процентах. Там же отмечено, что мы делаем так для ясности.

[^191]: «Касл Браво» не был первым взрывом термоядерного (водородного) устройства. Первенство принадлежит устройству «Майк» размером с дом из испытания «[Иви Майк](https://ru.wikipedia.org/wiki/%D0%98%D0%B2%D0%B8_%D0%9C%D0%B0%D0%B9%D0%BA)». Литий там не использовался.