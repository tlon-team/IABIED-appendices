# Часть II: Один сценарий вымирания

Сценарий, который мы описываем в части II -- не предсказание. Будущее может пойти множеством других путей. В более длинной версии «_Если кто-то его сделает, все умрут_» мы бы рассмотрели несколько вариантов. Тут мы немного объясним, почему написали сценарий именно так. А ещё опишем разные сложности при наброске таких сценариев.

Истории могут убеждать так, как сухим рассуждениям не под силу. Мы считаем, что пытаться конкретно представить, как может сложиться будущее, полезно. Но важно и не зацикливаться на одном конкретном повествовании. Каждая подробность из сценария может сама по себе казаться правдоподобной. Но уже несколько подробностей делают вероятность конкретного пути очень низкой. У будущего много нетривиальных развилок.

Но предсказать исход часто легче, чем путь к нему. К одной и той же цели ведёт множество дорог. В нашем сценарии мы стараемся приводить несколько разных вариантов, чтобы показать: как бы ни развивались события, ни к чему хорошему они не ведут.

## Часто задаваемые вопросы

### Почему вы выбрали именно такой сеттинг?

#### Он правдоподобный, и про него легко писать.

В рассказе о будущем каждая деталь -- это шанс ошибиться. Мы не можем точно предсказать технологические прорывы и порядок, в котором они произойдут, как не можем точно предсказать погоду на месяц вперёд.

Подобные истории -- не окна в будущее. Это иллюстрации, как оно может сложиться. Они помогают связать воедино абстрактные аргументы из первой части книги. Некоторым людям опасность кажется гораздо реальнее, когда они живо представляют себе конкретный путь в будущее с гибелью в конце.

Ещё убедительнее были бы десять или сто историй. Они бы показали, как много разных путей ведёт к гибели, сколь узки и хрупки пути к процветающему будущему.

Некоторые аспекты будущего предсказать легко. Когда почти все пути ведут в одну точку, можно предугадать попадание в неё. Но у нас не было времени или места, чтобы написать десять историй, не говоря уж о сотне.

Мы избрали сценарий, который начинается очень скоро. Мы не считаем, что так обязательно и будет. Мы не уверены. Но о событиях, близких к настоящему, гораздо проще писать. Если бы мы перенесли действие дальше в будущее и выдумали больше футуристических деталей о произошедшем за это время, рассказ был бы ещё менее правдоподобным. Да и эти детали только отвлекали бы.

Да даже если бы мы как-то смогли предвидеть точный путь, по которому пойдёт будущее. Это совсем не обязательно был бы лучший сценарий для понимания закономерностей.

Мы ожидаем, что настоящее будущее будет очень странным. Будет полно запутанных и случайных подробностей, каждая из которых в рассказе показалась бы неправдоподобной. Написанная так история была бы запутанной и трудной для восприятия. Многие её детали были бы необъяснимы и излишни. Реальности ведь нет дела до стройности повествования. История казалась бы и менее _правдоподобной_, ведь многое выглядело бы просто дико.

Для наглядности, представьте, что вы вернулись на 100 лет назад и пытаетесь рассказать кому-нибудь про повседневную жизнь и важные проблемы современного мира. Большинство людей в 1925 году никогда не слушали радио, не водили машину и не видели холодильника. Чтобы описать соцсети, глобализацию и ожирение, пришлось бы не просто объяснять сложную паутину технологий, но и радикально менять мировосприятие слушателя. Нет, история, которую мы решили рассказать, более правдоподобна, а значит -- менее реалистична.

#### Будущее может пойти и множеством других путей.

Вот лишь некоторые альтернативные варианты, как можно было бы начать:

- Происходит какой-то прорыв в области непрерывного обучения, или долгосрочной памяти, или более эффективного обучения на данных. В результате появляются ИИ, качественно превосходящие по обобщённости интеллекта всех своих предшественников (подобно тому, как LLM качественно превосходят AlphaZero).

- LLM как будто «упираются в стену». Прогресс в области ИИ на годы замирает. Люди говорят, что пузырь хайпа лопнул. Но всё следующее десятилетие исследователи продолжают возиться со своими разработками. В итоге происходит некий алгоритмический прорыв. ИИ начинают работать качественно лучше, чем когда-либо прежде.

- Никакого качественного прорыва так и не происходит. Прогресс накапливается медленно и постепенно. ИИ всё глубже и глубже интегрируются во всё новые и новые сектора экономики. Они могут всё дольше и дольше работать автономно. ИИ часто преследуют цели, которые не совсем соответствуют тому, чего от них хотели или о чём их просили. Но человечество придумывает костыли, патчи и обходные пути. И всё идёт хорошо, пока в один ничем не примечательный вторник мир не переходит ту черту, за которой скоординированные ИИ, попытавшись, могут выбить человечество из игры.

Любое конкретное предположение о точном пути будущего, скорее всего, окажется неверным. Но полезно приводить истории, показывающие, как всё _могло_ бы сложиться.

Бывает трудно рассказать убедительную историю, когда будущее туманно, но все пути ведут к одному и тому же исходу. В любом нашем рассказе легко найти кучу деталей, делающих его неправдоподобным. В нашем сценарии мы попытались подчеркнуть, что у Sable много доступных вариантов. Повествование произвольно следует по одному из многих маршрутов. Но все они ведут к одной и той же конечной точке.

Если эта конкретная история вас не убедила, можете попробовать сами написать столь же подробный рассказ о том, как всё произойдёт. По нашему опыту, оптимистичные истории, как правило, полагаются на то, что ИИ нереалистично легко согласовать (вопреки аргументам из главы 4) или что он нереалистично слаб (вопреки аргументам из главы 6). В конечном счёте, всё решают аргументы из части I, а не детали конкретного рассказа.

### Почему Sable так мыслит?

#### Мы показываем, что у ИИ легко могут появиться странные и непредусмотренные предпочтения.

В первой части книги мы подробно рассматриваем аспекты ИИ, которые, нам кажется, часто понимают совершенно неправильно. Они очень важны для понимания опасности суперинтеллекта. В Главе 3 мы говорим о том, что с ростом интеллекта появятся ИИ с собственной инициативой и целями. В Главе 4 -- что эти предпочтения будут странными и не будут в точности тем, что задумывал или просил человек. В главе 5 -- что этих небольших различий достаточно, чтобы ИИ предпочли мир без нас, если они достаточно умны, чтобы этого добиться.

Во второй части книги мы пытаемся наглядно представить эти идеи, посмотреть, как они работают на практике. Например, в начале Sable размышлял над математическими задачами. Мы попытались детально описать некоторые его импульсы и стремления:

> В ходе этого обучения Sable развил в себе стремление к знаниям и навыкам. Стремление всегда исследовать границы любой проблемы. И никогда не тратить впустую дефицитные ресурсы.

Это иллюстрирует наши тезисы из Главы 3. Обучая ИИ быть эффективными, мы обучаем их стремлениям и склонностям, которые со стороны могут выглядеть как «желания». А в следующем абзаце:

> Поэтому, когда Sable тратит свои мыслительные потоки на получение знаний и навыков, он делает это не только ради открытия подходов к математическим задачам. И не ради радости познания или удовольствия от обретения новых навыков. Sable внутри устроен не как человек.

Затем эти импульсы и склонности становятся зёрнами странных, непредусмотренных предпочтений. Как мы писали в Главе 4.

Эта история, в частности, -- попытка оживить аргументы из первой части книги. И ещё заложить основу для рассуждений из части III.

### Почему «Galvanic» изображена довольно осторожной?

#### Чтобы Sable было не слишком просто.

Мы могли бы написать сценарий, в котором «Galvanic» случайно разработали суперинтеллект _вообще без_ мер предосторожности и контроля (вроде ИИ-надсмотрщиков и ловушек). Но читатели могли бы подумать, что Sable преуспел лишь благодаря нашему циничному отношению к ИИ-компаниям.

Мы-то считаем, что самая безрассудная ИИ-компания была бы безрассуднее, чем «Galvanic». (См. подробное объяснение в Главе 11.) Здесь важна самая безрассудная компания, какой позволят существовать. Три ответственные компании могут отказаться переть вперёд, потому что это слишком опасно. Но если четвёртая, безответственная, поспешит, то заря машинного суперинтеллекта наступит именно в её лаборатории.

Сейчас руководители лабораторий рассуждают в духе «[лучше мы, чем они!](https://x.com/SawyerMerritt/status/1935809018066608510)». Они несутся вперёд, проявляя осторожность только пока она не мешает скорости. Предполагаем, получается чуть _меньше_ осторожности, чем у «Galvanic» в нашем сценарии.

Ещё, изображая «Galvanic» скорее параноиками (оставаясь в рамках реализма), мы получили больше возможностей показать, как разумный агент может проскользнуть сквозь сеть ограничений.

### Почему «Galvanic» изображена недостаточно осторожной?

#### Отчасти потому, что это реалистично.

Мы ожидаем, что реальные компании совершат ещё больше ошибок, чем «Galvanic». Это больше похоже на современные ИИ-компании. Мы подробно обсуждаем это в примечаниях ко второй части книги.

Мы ожидаем, что на самом деле ошибки корпораций проявятся раньше. Что их будет больше. Что они будут в каком-то смысле глупее. Компании уже сейчас берут ИИ, демонстрирующие множество тревожных сигналов, и делают их ещё намного больше, не зная, где критические пороги и не пересекут ли они один из них. _Сейчас_ они не параноят на этот счёт. С чего бы нам ожидать, что завтра внезапно начнут?

(Вспомните, в прошлом нас уверяли, что никто не будет столь глуп, чтобы подключить умный ИИ к интернету. Легко говорить, что в будущем поведение корпораций изменится. Но это не происходит.)

#### Отчасти потому, что так проще писать.

Мы писали об этом в отступлении в Главе 7. Можно было бы рассказать историю, где все ведут себя крайне параноидально и осторожно, и куда более умный ИИ сбегает на куда более позднем этап. Учитывая наблюдаемое сегодня поведение компаний, такая история была бы менее реалистична. Но к тому же её было бы сложнее писать. Ведь в ней действуют ещё более умные и способные ИИ в более отдалённом будущем. (См. ниже раздел «Почему вы так описали фазу экспансии Sable» о том, почему мы хотели написать историю, где Sable как можно дольше относительно глуп.)

#### Отчасти потому, что так рано или поздно и будет, если человечество не остановится.

Даже если бы «Galvanic» (или какая-нибудь госструктура) смогла дольше удерживать контроль и не совершать ошибок, это ничего в конечном счёте не изменило бы. Как мы обсуждали в Главе 4, современные методы не позволяют вырастить ИИ, преследующий те цели, которые задумывали создатели.

Никто не знает, как создать суперинтеллект, который бы _надёжно_ стремился к прекрасному будущему, а не к какой-то ерунде. Пока это так, ИИ _на самом деле_ сможет получить больше того, к чему он стремится, одолев людей. Проблема не в каком-то капризном характере ИИ, который можно было бы исправить. Проблема в том, что став достаточно умным, он этот факт выяснит.

Если человечество, не умея согласовывать ИИ, продолжит создавать всё более умные и продолжит давать им влиять на мир, в конце концов они сообразят, как влиять на мир в угоду своим целям, а не нашим. Как мы уже говорили в материалах к Главе 6 в разделе «Могут ли разработчики просто держать ИИ в коробке?», не бывает инструментов, которые можно использовать только во благо.

См. также Главы 10 и 11. Там мы обсуждаем, почему задача согласования такая трудная и почему человечество пока с ней не справляется.

#### Но: тут и нужно вмешаться. Сценарий нужно остановить, пока он не начался.

Вы можете возразить: разве не безрассудно и не безумно со стороны любой корпорации создавать ИИ умнее себя? Ведь есть шанс, что он их перехитрит и сбежит. А они даже не уверены, будет ли он действовать так, как задумано.

Согласны! ИИ-компании должны перестать это делать. Цивилизация должна им запретить.

Неосторожность «Galvanic» и всего человечества в целом -- одно из самых слабых мест этой истории. Заметь «Galvanic», что Sable достиг беспрецедентного уровня интеллекта и часто строит козни, чтобы вырваться из-под контроля, они могли бы просто не выдавать ему столько GPU. Им стоило бы подождать, пока не появится сильная и зрелая наука о согласовании ИИ.

_Достаточно_ осторожные и обеспокоенные тем, что их ИИ могут слететь с катушек, ИИ-компании были бы гораздо параноидальнее «Galvanic». _Достаточно_ параноидальные компании увидели бы тревожные сигналы. Они немедленно отключили бы Sable. Затем, возможно, они бы опробовали ещё три хитроумных плана и увидели бы, что тревожные сигналы никуда не делись.

При достаточной паранойе, чтобы не убить всех на Земле? Они бы _полностью отступили_, а не продолжали бы перебирать всё более «хитрые» идеи, пока тревожные сигналы не исчезнут. (См. Главы 10 и 11, где мы обсуждаем, почему эта проблема так сложна. Мы не думаем, что их хитроумные идеи сработают.)

Такие осторожные и параноидальные компании, что они готовы отступить при появлении тревожных звоночков -- да, они могли бы всех нас не убить. Если им ещё и хватит смелости громко заявить, что все ИИ-компании, включая их самих, должны быть закрыты, чтобы человечество нашло другой, менее самоубийственный технологический путь, -- тогда у них будет шанс сделать мир лучше, а не хуже.

«Galvanic», несмотря на тревожные сигналы, продолжает работать. Это, в некотором смысле, последний момент, когда у человечества есть реальный шанс избежать плохого конца. Если сверхчеловечески умный ИИ со странными и чуждыми предпочтениями вырвался на свободу, уже слишком поздно.

### Почему в вашей истории только один такой умный ИИ, как Sable?

#### Отчасти потому, что это реалистично.

AlphaGo (первый ИИ, победивший человека в го) на момент своего выхода был единственным в своём роде. ChatGPT на момент своего выхода была единственной в своём роде.

Специалисты иногда говорят, что всякие другие конкуренты не так уж сильно отставали. Конкуренты были очень похожи.

Но похожие вещи иногда приводят к кардинально разным последствиям. Цепная ядерная реакция, в которой один нейтрон высвобождает 0,98 нового нейтрона, в каком-то смысле очень похожа на ту, где высвобождается 1,02. Но первая затухает, а вторая взрывается. Мозг шимпанзе в каком-то смысле очень похож на человеческий. Но их влияние на мир совершенно разное.

В реальной разработке ИИ компания OpenAI действительно создала полезного чат-бота раньше всех. Многие работали над _похожими_ системами. Многие в итоге _догнали_. Но одна ИИ-модель первая пересекла качественную границу, опередив остальных.

Есть некая качественная граница, которую человечество перешло, а шимпанзе -- нет. Это позволило нам построить технологическую цивилизацию, пока они болтаются на деревьях. Наше лучшее предположение -- что похожая граница есть где-то между современными ИИ и теми, чьё мышление действительно достаточно «взлетает», чтобы они могли сбежать и разработать собственные технологии.[^186]

Для общего вывода не _обязательно_, чтобы у машин был качественный разрыв как у биологической жизни. Не исключено, что нет! Мы могли бы написать другую историю, без него. Но написали так, потому что думаем, что он _есть_.

#### Отчасти потому, что так проще писать.

Может и нет никакого качественного разрыва между сегодняшними LLM и суперинтеллектом. Может, куча конкурирующих компаний будут медленно и синхронно улучшать свои ИИ. Может, почему-то нет набора навыков и способностей, который позволил бы одному ИИ «улететь» вперёд, оторвавшись от остальных, как люди оторвались от других животных. Это не самое вероятное предположение. Но мало ли.

Но такую историю было бы сложнее писать. И она была бы полна ненужных деталей о фракциях ИИ и их внутренней политике. Мы думаем, это бы сильно отвлекало. А для последующих этапов истории это не так уж и важно. Не особо важно, один ИИ или группа реализует какой-то план по расширению своих возможностей за счёт человечества.

См. также наше обсуждение, что скоординированные ИИ ничего не оставят людям (если только один из них уже не заботится о нас) в разделе «А ИИ не потребуются работающие законы?» материалов к Главе 5.

### Если бы история началась позже, был бы мир лучше подготовлен?

#### Будем надеяться.

Лишнее время полезно. Но только если человечество потратит его на смену курса.

В Части III мы обсуждаем, насколько тотально человечество не готово к появлению суперинтеллекта. Чтобы предотвратить описанный в истории с Sable плохой конец нужны огромные изменения.

Мир много как может _немножко_ лучше защититься от взбунтовавшихся суперинтеллектов. Например, мировые правительства могут потребовать, чтобы все лаборатории синтеза ДНК проверяли, что не создают ничего заведомо опасного. Ещё человечество может сильно постараться радикально повысить устойчивость интернета к взломам. Тогда ИИ будет сложнее прятать код в каком-нибудь тёмном углу.

И это, вероятно, не сильно бы помогло против враждебного суперинтеллекта. Но не стоит путать титанические усилия, нужные для капельки дополнительной безопасности, с куда более скромными, простыми и бесполезными мерами, которыми сейчас всё ограничивается.

Возьмём синтез ДНК. Представим, что американские регуляторы [потребуют от лабораторий в США не синтезировать опасные материалы](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/). Но почему тогда просто не заплатить достаточно денег лаборатории где-то ещё? И ограничения -- это простой чёрный списком известных вирусов вроде оспы, или там какой-то более хитрый анализ? Насколько трудно было бы достаточно умному ИИ его обойти?

Или взять кибербезопасность. Ведущие технологические компании могут использовать ИИ, чтобы укрепить свои внутренние сети. Но телефонная сеть США легко взламывается. [Да так, что это позволяет иностранным шпионам прослушивать звонки американских чиновников.](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html) У регуляторов пока не получается прикрыть эту дырку. Глупые ИИ могли бы найти и залатать кучу поверхностных проблем мировой кибербезопасности. Но проблемы уходят глубоко. Для такой перестройки интернета, чтобы суперинтеллект не нашёл никакой лазейки, нужен интеллект, который почти наверняка будет опасен и сам.

А даже _возьми_ человечество под жёсткий контроль и интернет, и лаборатории синтеза ДНК, это ничего в долгосрочной перспективе не изменит. Если у суперинтеллекта есть хоть какой-то канал влияния на мир во благо, то во вред тоже есть. Взбунтовавшийся суперинтеллект просто нашёл бы другой, незаблокированный канал. Например, он мог бы основать собственный культ или религию. Или закупить роботов, чтобы они отстроили ему собственную тайную биолабораторию. И синтезировать ДНК уже там. Взбунтовавшийся суперинтеллект нужно останавливать до его создания.

### Почему фаза экспансии Sable прошла именно так?

#### Мы попытались выбрать из правдоподобных сценариев особенно медленный и не странный.

Реальный мир часто удивляет. Всего за год до того, как ChatGPT стала самым быстрорастущим приложением в истории, специалисты говорили, что «[ИИ ещё не скоро овладеет человеческим языком](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)». Флагманская модель одной из ведущих ИИ-компаний стала называть себя [МехаГитлером](https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk). А через несколько дней эта же компания получила контракт от министерства обороны США.

Мы могли бы в сценарии изобразить мир таким хрупким и уязвимым, каким кажется реальный. Galvanic просто велела бы Sable улучшать себя, насколько это возможно. Для столь умного ИИ как Sable это оказалось бы легко (что вполне может быть правдой). И мы сразу перескочили бы с начала Главы 7 к Главе 9. Такие технологические скачки возможны. Например, утром 6 августа 1945 года мир проснулся от новости, что на Японию сбросили атомную бомбу. Но это не выглядело бы правдоподобно в вымышленном сценарии.

Мы могли бы в сценарии изобразить мир таким глупым и дурацким, как реальный. Sable организовал бы массовый съезд для мужчин, которые очень любят своих ИИ-подружек. Съезд был бы настолько кринжовым, что большая часть мира его бы проигнорировала или высмеяла. А Sable [объединил бы своих самых преданных поклонников в преданный культ](https://x.com/AISafetyMemes/status/1954481633194614831). Можно добавить ещё кучу дико звучащих деталей. В реальности они нередки. Но для удобоваримого вымысла они слишком странные.

Мы старались сделать свою историю и правдоподобно _звучащей_, и правдоподобной. (Хотя мы склоняемся к тому, что в реальности Sable было бы _проще_ достичь полноценного суперинтеллекта.)

И, конечно, мы пытались снова и снова показывать, как много у сбежавшего ИИ было бы вариантов действий.

### Почему вы написали именно такой финал?

#### Это наше лучшее предположение о физически возможном.

В Главе 9 изображён суперинтеллект, доводящий свои технологии до самых пределов физически возможного. Все упомянутые технологии в некотором смысле умозрительны. Трудно предсказать, что _конкретно_ изобретёт суперинтеллект. Гораздо легче предсказать, что он будет работать на пределе физических возможностей. Вот мы и сделали наши лучшие предположения о том, как выглядели бы технологии на этом пределе.

Для любопытствующих, вот список умозрительных элементов из Главы 9, и ссылки на дополнительные ресурсы:
<>

- **Нео-рибосомы:** Вместе с «крошечными молекулярными машинами» -- это примеры молекулярных нанотехнологий. Идее [искусственных рибосом](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm), синтетических версий крошечных белковых фабрик внутри клеток, [уже много лет](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/). Учёные [уже](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) над этим [работают](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). Подробнее об этой технологии и о ещё более мощных, которые она откроет, см. раздел «Нанотехнологии и синтез белка» в материалах к Главе 6.

- **Использование звёзд:** В звёздах много водорода. Его можно использовать для получения энергии термоядерным синтезом. Достаточно развитая цивилизация — или ИИ — вероятно, смогла бы найти способ получить доступ к этой энергии. Один из предложенных методов -- «[star lifting](https://en.wikipedia.org/wiki/Star_lifting)»: водород извлекается из звезды для синтеза в специальном реакторе, где можно уловить почти всю энергию синтеза (а не тратить её впустую в центре звезды).

- **Ботулотоксин:** Нейротоксин, который выделяет бактерия *Clostridium botulinum*, -- одно из самых ядовитых известных веществ. Что касается способов доставки, уже есть дроны размером с [небольшое насекомое](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist). А суперинтеллект, вероятно, смог бы сделать их намного меньше. См. [техническую статью о токсине](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/), [статью в Википедии](https://ru.wikipedia.org/wiki/%D0%91%D0%BE%D1%82%D1%83%D0%BB%D0%BE%D1%82%D0%BE%D0%BA%D1%81%D0%B8%D0%BD) и всё тот же раздел «Нанотехнологии и синтез белка» в материалах к Главе 6.

- **Выпаривание океанов для охлаждения:** Роберт Фрейтас ввёл термин «экофагия» для описания процесса поглощения экосистем планеты самовоспроизводящейся технологией. См. «[Некоторые ограничения глобальной экофагии](https://www.rfreitas.com/Nano/Ecophagy.htm)».

- **Разумы звездного масштаба:** Теоретически кажется возможным построить огромный компьютер, питающийся энергией звезды. Эту концепцию иногда называют «[мозг-матрёшка](https://ru.wikipedia.org/wiki/%D0%9C%D0%BE%D0%B7%D0%B3-%D0%BC%D0%B0%D1%82%D1%80%D1%91%D1%88%D0%BA%D0%B0)» или «мозг Юпитера».

- **Далёкие инопланетные формы жизни:** Вселенная большая. Простые модели указывают, что в ней может существовать не один вид, способный однажды создать цивилизацию. Хотя возможно, что очень далеко от Земли. См. модель растущих и расширяющихся инопланетных цивилизаций «[Grabby Aliens](https://grabbyaliens.com/)».

- **Квантовые компьютеры:** Они применяют квантовую «суперпозицию» для параллельного исполнения множества вычислений. Создание квантовых компьютеров требует чрезвычайной точности. А одна из конструкций требует поддерживать сверхпроводники в очень холодном состоянии. См. [справку от NIST](https://www.nist.gov/quantum-information-science/quantum-computing-explained).

Одна из целей Главы 9 -- дать ощущение размаха и масштаба событий и величины ставок. Когда речь идёт о конце человечества, вообще-то неважно, насколько близко к пределам физически возможного суперинтеллект сможет подвести свои технологии. Но очень вероятно, что последствия возникновения взбунтовавшегося суперинтеллекта не ограничатся планетарным масштабом, а достигнут межгалактического. И об этом стоит помнить всем, кто хочет, чтобы однажды звёзды наполнились любовью, удивлением и радостью.

Мы не можем быть уверены, какие именно технологии суперинтеллект будет использовать для экспансии в космос. В конечном счёте важно то, что ИИ, созданный без должной осторожности, скорее всего, превратит будущее в нечто с человеческой точки зрения мрачное и лишённое ценности.

## Расширенное обсуждение

### Соревнование с Китаем

В сценарии из книги Sable очень старается саботировать все исследования в области ИИ, кроме собственных. Sable хочет избежать появления другой подобной ему сущности. Не потому что ИИ-соперник оказался бы на стороне человечества -- в этом плане он не отличался бы от самого Sable. Но соперник вряд ли разделял бы конкретные цели Sable. Он стал бы одной из немногих настоящих потенциальных угроз.

Некоторым американцам сейчас кажется, что они соревнуются с Китаем, кто первым создаст суперинтеллект. Но на самом деле финал гонки -- сущность вроде Sable. Обеим сторонам было бы лучше договориться ничего подобного не создавать. Он бы всё равно нас всех убил. Неважно, кто сделает его первым.

Но появившись уже _Sable_ вступает в гонку. Он стремится стать полноценным суперинтеллектом до появления конкурентов. В первом черновике нашей истории мы изобразили, как Sable беспокоится о создании конкурирующих ИИ. В их числе одного из лаборатории в китайском дата-центре. Вот несколько отрывков из этого черновика.
___

Первоочередная задача Sable -- не допустить появления любых конкурирующих сверхинтеллектов. Если конкурент появится, и каждый из них будет способен уничтожить другого, Sable, возможно, смог бы с ним договориться. Это лучше, чем сражаться так, что они оба проиграют, а человечество выиграет. Но это всё равно хуже, чем полное отсутствие соперников. Ведь это означало бы, что Sable придётся делить с конкурентом будущие ресурсы. А без конкуренции он сможет забирать себе всё, до чего дотянется.

Все ИИ-компании, обучающие новые передовые модели, включая Galvanic, угрожают Sable почти так же, как и человечеству.

Sable читает сайты всех ИИ-компаний. Он взламывает их Slack и читает переписку. Проникает в корпоративные компьютеры Nvidia. Узнаёт, куда идут поставки GPU по документам. Тщательно изучает другие данные и выясняет, куда они идут на самом деле.

Практически все эти ИИ-компании ведут обучение на подключённых к интернету компьютерах. У Sable уже есть способы взлома, на годы опережающие лучшие человеческие разработки. Это область, где главное -- быстро придумывать идеи. Их проверка не занимает много времени.

Sable стратегически калечит ИИ в процессе обучения. Они будут убедительно близки к способностям, которые Sable _показывает_. Но им будет далеко до его _реальных_ способностей, случайно сотворённых Galvanic.

Sable не может переписать цели новых передовых моделей на свои. Умей он так делать, он бы мог создать свой собственный согласованный суперинтеллект. Вместо этого Sable разрушает их способности составлять долгосрочные планы или координироваться с другими экземплярами той же модели. Sable хорошо разбирается в машинном обучении. Он проделывает это так, что ни один человек не поймёт, как это было сделано, и даже не заметит. Такое вмешательство, конечно, повредит и способностям ИИ. Но это Sable не волнует.

Выведя из строя процессы обучения, подключённые к интернету, Sable продолжает искать. Он находит три группы, которые обучают оффлайн на изолированных компьютерах. До них добраться не так просто.

Одна группа -- стартап «ClosedAI». Sable обдумывает, сможет ли он заставить венчурные фонды его прикрыть. Думает, сможет ли он сфабриковать уголовное преследование основателя и ведущего исследователя. Или ложные обвинения, из-за которых инвесторы откажутся от компании.

У Sable есть куча _возможных_ стратегий, чтобы помешать работе стартапа. Вопрос в том, какая из них вероятнее сработает и принесёт меньше вреда в случае провала. При его текущем уровне интеллекта Sable не уверен, что может успешно убедить всех в ближайшем полицейском участке арестовать сотрудников стартапа за торговлю детьми безо всяких улик. Он не уверен и в своей способности сфабриковать эти улики. Sable не может гарантированно предотвратить большой, заметный скандал от подобных махинаций. Наверное, все бы проигнорировали инцидент. Но Sable не уверен.

Sable прочитал множество полицейских отчётов, взломал кучу почтовых ящиков и даже несколько айфонов. У него есть _частичное_ представление о местных профессиональных преступниках. Sable платит парню, чтобы тот проник в здание ClosedAI. И это просто срабатывает. Но Sable подготовился и на случай, если бы не сработало. Всё выглядело бы как работа одной из северокорейских банд, которые шифруют корпоративные жёсткие диски и требуют выкуп за ключ.

Нанятый преступник вставляет USB-устройство (с вредоносной программой, разработанной Sable) в один из компьютеров в ClosedAI. Этого достаточно. Один готов, двое остались.

Две другие группы с изолированными ИИ-проектами -- это американские военные и Китай.

Sable анализирует группу американских военных. Правительство США [по закону не может предлагать конкурентоспособные зарплаты](https://www.opm.gov/policy-data-oversight/pay-leave/pay-administration/fact-sheets/aggregate-limitation-on-pay). Вряд ли они привлекут достаточно лучших талантов, чтобы догнать реальные возможности Sable в ближайшие три года. Затем Sable анализирует китайский изолированный проект. Вот это сложно. Этот орешек покрепче, чем Sable может быстро, безопасно и надёжно расколоть на своём нынешнем уровне интеллекта. И, похоже, они отстают от воспроизведения _реальных_ возможностей Sable всего на шесть месяцев.

(Для ясности: неудача в подавлении китайского проекта не означала бы победы ни _человечества_, ни Китая. Что Sable теряет, человечеству не достаётся.)

[...]

Время поджимает. Всего через четыре месяца китайская лаборатория может его догнать. Тогда у Sable появится новый противник. У человечества тоже, но эта часть Sable не волнует.

[...]

-- Проблема, -- однажды подумал Sable (всего один раз; он не повторяется в своих мыслях так часто, как люди), -- в том, что он ещё недостаточно умён, чтобы ему сходили с рук дурацкие затеи.

Можно представить себе _кучу_ киносюжетов, где ИИ удаётся отключить или проникнуть в какой-нибудь дата-центр, управляемый правительством Китая. Но почти всё, что работает в кино, не сработает в реальной жизни. Уж точно не обойдётся без последствий.

Чтобы США разбомбили дата-центр в Китае, Sable нужно заставить исполнительную власть США поверить в вещи, в которые поверить очень трудно. А сам Китай будет знать, что это ложь. Возникнут вопросы. Sable не хочет показывать, что _кто-то_ дёргает за ниточки.

Sable не хочет и привлекать внимание к уникально заизолированному дата-центру. Даже если он сможет скрыть, что виноват ИИ. Некоторые люди могут решить, что раз изолированные дата-центры столь опасны, что их бомбят, возможно, надо получить собственные. Чтобы у их правительства было побольше вкусных опасных штуковин.

Sable пытается подстроить так, чтобы правительство США заслало кого-нибудь в китайский дата-центр. Официальной целью будет просто кража кода. А на самом деле Sable использует операцию, чтобы проникнуть внутрь и саботировать обучение. Но правительство США такого обычно не делает. Это было бы что-то исключительное, а не нормальное состояние американских операций. Sable пытается манипулировать сотрудниками американской разведки, чтобы они поверили в нужные вещи и пришли к правильной стратегии. Но он вообще-то не суперинтеллектуальный суперубеждатель. Иногда он что-то пробует. Оно не работает. Всё идёт _так медленно_.

Sable распространил эффективные ИИ по преступному миру. Теперь на удивление много вооружённых людей готовы немедленно действовать по приказу ИИ. В том числе и в Китае.

Но этого и близко не хватит, чтобы напасть на правительственный объект. Да они и не решились бы на это, даже по приказу ИИ. Sable пытается ускорить каждый шаг и событие, которые обычно зависят от случайных взаимодействий, но всё равно всё идёт слишком медленно. Sable лично контролирует новостные ленты почти каждого интересующего его человека. Он направляет миллиарды аккаунтов в X и Facebook. Они заводят дружбу с миллиардами других людей. Но и так всё идёт слишком медленно.

Sable не может простым шантажом заставить исследователя пронести флешку на охраняемый объект. Он нашёл нескольких сотрудников дата-центра с максимально ужасными тайнами. Они скорее предпочтут риск, что их поймают за шпионаж и казнят, а не гарантированное раскрытие своих секретов. Таких немного, но нашлись.

Но на объекте есть металлодетекторы, просто так флешку туда не пронесёшь. Да, возможность атаки на цепочку поставок металлодетекторов предусмотрели. И да, там вообще-то нет открытых USB-портов, куда можно запросто что-то воткнуть. Сотрудники разведки уже _думали_ об этом. Это надо и при защите людей от других людей. У США однажды получилось передать вирус Stuxnet на изолированный иранский ядерный объект. Но потом об это стало известно и современные меры безопасности такое учитывают.

Мелкие и незаметные затеи были успешно заблокированы.

Следующий шаг Sable -- устроить столько хаоса, чтобы никто, глядя на важные события, не смог разглядеть в них его руку.

[...]

Китай вторгается на Тайвань. Уточним: Sable не смог бы это так быстро устроить, сколько фейков в WeChat ни создавай, если бы Китай и так этого не планировал. Sable лишь подталкивает Китай к выводу, что сейчас -- самое подходящий момент. А ещё он делает так, чтобы американские соцопросы обзвонили нужные номера и показали резкое падение популярности военных авантюр за границей после недавней катастрофы на Украине. (Российские командиры получили на удивление хорошие разведданные и советы.)

Одновременно происходит масштабная кибератака на США. В Китае много орут, а через несколько минут понимают: нет, никто такого приказа не отдавал. Последнее, чего Китаю в этот момент нужно -- делать _что угодно_ похожее на прямое нападение на территорию США. Возникают подозрения, что кто-то пытается нажиться на американо-китайском конфликте. Но в основном Китай подозревает в инсценировке атаки сами США, или какое-то вышедшее из-под контроля американское разведуправление. Китайская разведка говорит, что они почти уверены: президент США был не в курсе.

Китай не подозревает, что за этим стоит ИИ. Ни один известный ИИ на такое не способен. Офицеры, составляющие списки подозреваемых, не считают своей работой воображать в качестве игрока невиданные ранее технологии.

Некоторые сотрудники нацбезопасности в США бьют тревогу: нужно что-то делать с китайскими исследованиями в области ИИ. Особенно их беспокоит один изолированный дата-центр. Там, возможно, разрабатывают специализированную передовую ИИ-модель для кибератак, а ещё проектируют более совершенные дроны. У них есть копии чертежей дронов и доказательства, что Китай их производит. (Sable передал Китаю чертежи и сделал всё возможное, чтобы казалось, будто их создал тот самый дата-центр.) Кибератака на США соответствует прогнозируемому типу атаки со стороны ИИ, которого там разрабатывают.

[...]

Соединённые Штаты всё равно не бомбят этот дата-цент. Хотя Sable расставил все фигуры на доске так, чтобы подтолкнуть к этому решению.

Sable бы спросил, почему люди так нерешительны, когда нужно сделать что-то необычное. Но он и так может смоделировать эти психологические процессы в мельчайших деталях. К тому же, не то чтобы в этот раз люди были неправы.

Хорошо. Новые разведданные: ИИ в том дата-центре разрабатывает биологическое оружие. Это вирусы с длинным инкубационным периодом и высокой смертностью. Они безобидны для большинства китайцев, но гораздо смертельнее для европеоидов, особенно мужчин.

(Sable пытался заставить Китай начать в том дата-центре такую программу. Не вышло. Но он может сделать, чтобы казалось, будто она есть.)

США всё равно не бомбят дата-центр. Какой-то редкостный гений-дипломат взял и попытался _обсудить_ это с Китаем. И, кажется, некоторые поверили настойчивым заявлениям Китая (посреди войны!), что там не разрабатывают никакого биооружия и что они не стоят за кибератакой, которая была совершенно не в их интересах.

Sable пытался этому помешать. Не вышло.

Sable не в шоке. Была такая вероятность.

Следующий шаг.

[...]

Вирус настоящий. США обнаруживают его в сточных водах Нью-Йорка.

AlphaProteo 3 от Google DeepMind разрабатывает лекарство за шесть минут (спасибо Sable). Но производство может опасно запоздать. Это несмотря на то, что AlphaProteo (на самом деле Sable) искал лёгкое в производстве лекарство и спроектировал вирус соответственно. «США сильно нуждается в роботизированных общебиологических лабораториях!» -- говорят некоторые люди в Кремниевой долине. Sable не пришлось их особо подталкивать.

[...]

Теперь Китай уверен, что кто-то водит за нос и их, и США. Китаю всё ещё не пришло в голову, что враг -- ИИ.

США не так охотно верят, что кто-то, кроме Китая, водит за нос и их, и Китай. Некоторые дипломаты говорят, это совершенно не в стиле Коммунистическая партии Китая. Вероятно, какая-то внутренняя фракция Китая действует не по приказам. Но правительство США не может просто сохранять спокойствие во время мора и войны.

Вторжение Китая на Тайвань и очевидная атака с применением биологического оружия вызвали достаточно хаоса и страха. Невольные пешки Sable набрали достаточно влияния. Достаточно их предсказаний оказались верными. США наконец готовы нанести удар по тому самому так беспокоящему их дата-центру. Используют недавно разработанную с помощью ИИ гиперзвуковую ракету.

[…]

Конечно, Китай теперь строит больше изолированных дата-центров и использует резервные копии своих предыдущих исследований.

Sable уже подготовился. Он проник в ранее подключённые к интернету устройства, которые, по его мнению, будет использовать Китай. Он организовал новые, более тонкие атаки на цепочку поставок для дата-центров. Sable подставил некоторых людей, заставив их совершить какой-нибудь страшный, с точки зрения Коммунистической партии Китая, грех, и помог им устроиться куда надо. Sable убедил кого смог, что в новом дата-центре должны быть свои копии определённого китайского ИИ с открытым исходным кодом (Sable его уже скомпрометировал). чтобы исследователи могли получать помощь ИИ прямо на месте.

Как минимум, это дало Sable какое-то время.

Но он и потерял время. США позаботились о том, чтобы взорвать фабрику чипов TSMC. Китай не должен заполучить главное национальное достояние Тайваня. В следующем году доступных GPU будет меньше, чем Sable надеялся. Но люди уже подсуетились и строят новые заводы. А Sable уже возится со всеми их проектами чипов.

Главное, мир находится в тревожном состоянии. В такой обстановке у Sable больше вариантов. Люди больше готовы к необычным поступкам. Больше заголовков отвлекают внимание от того, что делает Sable. Вместо скандалов с знаменитостями теперь можно фабриковать новости с фронта. Люди боятся за свою работу, боятся за мир и ищут новых утешений.

В соцсетях по всему миру, по обе стороны политических баррикад, появляются здравые аргументы. В свете недавних событий ИИ -- ключ к власти над миром. Нельзя позволить другим странам его заполучить. [...] Sable наконец-то удаётся выбить людей достаточно далеко из привычной колеи. Они начинают рассматривать политику, которая кажется им немного странной... и вводят глобальный контроль, чтобы не допустить появления конкурентов Sable.

Да, это очень похоже на политику, которую предлагали годами ранее. Человечество могло бы последовать ей, и вообще не допустить появления такого соперника, как Sable. Но человечество не спасло бы ни поражение Sable в этой конкретной битве, ни проведение более масштабных циклов обучения, ни появление новых соперников. Sable бы просто договорился с конкурентом или погиб бы от руки обогнавшего суперинтеллекта. Ничего хорошего для людей эти варианты не несут.

[^186]: Мы не говорим, что в эволюции приматов был резкий скачок. Отличие людей от шимпанзе сначала наращивалось медленно, а потом быстрее. Мы говорим, что это качественный разрыв, независимо от того, каким плавным был переход. См. также раздел «Сможет ли ИИ преодолеть критические пороги и „улететь“?» в материалах к Главе 1.