# Глава 11: Алхимия, а не наука

Это онлайн-дополнение к 11-й главе «*Если кто-то его сделает, все умрут*». В ней обсуждались подходы современных ИИ-лабораторий к согласованию сильного искусственного интеллекта. Ответы на эти вопросы -- в книге:

- Как оценить нынешнюю готовность ИИ-компаний к решению проблемы согласования СИИ?

- Какое место занимают исследования интерпретируемости -- попытки читать и понимать мысли ИИ?

- А нельзя просто поручить ИИ решить эту задачу за нас?

Ниже мы разбираем несколько идей по согласованию и внедрению ИИ. Ещё обсудим поводы для оптимизма. И аргументы, которые приводят в пользу развития передового ИИ, несмотря на общую мрачную картину.

## Часто задаваемые вопросы

### Да мы как-нибудь выкарабкаемся, как обычно. Почему нет?

#### Мир обычно «выкарабкивается» методом проб и ошибок. А тут ранние ошибки не оставят никого в живых.

См. Главу 10 и расширенное обсуждение в материалах к ней.

### Вы считаете, что согласование -- это «всё или ничего»?

#### Нет. Но «частичное согласование» скорее всего тоже приведёт к катастрофе.

Один из доводов против беспокойства о суперинтеллекте звучит примерно так: «Развитие ИИ, вероятно, будет постепенным. На каждом шаге мы сможем методом проб и ошибок научиться держать его в узде. Согласованию не обязательно быть *идеальным*, чтобы всё закончилось хорошо». (См. также раздел «А что если разрабатывать ИИ неспешно и так же плавно внедрять его в общество?» дополнительных материалов к Главе 10.) Мы не возлагаем на этот сценарий особых надежд. Вот несколько причин:

- Наши опасения не зависят от скорости прогресса. Мы не можем точно сказать, выйдет ли ИИ на плато на пути к суперинтеллекту. Это сложный вопрос. Наша лучшая догадка: у машинного интеллекта есть пороговые эффекты. Но это лишь предположение. Наши аргументы не строятся на нём. История Sable во второй части книги намеренно описывает катастрофу из-за ИИ, который не так уж сильно превосходит человеческий уровень. Отчасти для того, чтобы показать: враждебному ИИ не обязательно быстро становиться суперинтеллектом. Он может стать чрезвычайно опасным и без этого.

- На вопрос «А что если нам повезёт и будет куча времени на проверку идей согласования на достаточно слабых ИИ?» мы отвечаем в главе 10 и материалах («До» и «После») к ней. Исследователи могут много чего выяснить о таких системах. Но ИИ, которые безопасно изучать, будут неизбежно критически отличаться от первых моделей, достаточно мощных для прохождения точки невозврата. Даже зрелой науке было бы очень сложно учесть все эти нюансы. А для области всё ещё на стадии алхимии, области, что работает с непостижимыми ИИ (которые выращивают, а не конструируют) -- вообще без шансов.

- Согласование ИИ не обязано быть идеальным для отличных долгосрочных результатов. В принципе, можно аккуратно создать ИИ с некоторой толерантностью к ошибкам. Если знать, что делаешь.[^1] Но это не значит, что «частично» или даже «в основном» согласованные системы приведут к нормальному исходу. ИИ по очень многим причинам может сейчас или в ближайшем будущем вести себя хорошо в 95% случаев, безо всяких гарантий счастливого исхода для человечества. Мы уже обсуждали эти причины с разных сторон в онлайн-материалах к Главе 5.

Поясним последний пункт:

В качестве мысленного эксперимента представьте: человечеству удалось загрузить в предпочтения суперинтеллекта *почти* все разнообразные человеческие ценности. Кроме, по какой-то причине, тяги к новизне. Тогда суперинтеллект направит будущее в застойное и скучное русло. Там один и тот же «лучший» день будет повторяться бесконечно.

Заметьте, мы не считаем это *правдоподобным*. Такой уровень согласования совершенно недостижим для нынешних стандартных подходов. Мы смогли заложить в ИИ почти все наши ценности, но не одну последнюю -- очень странная ситуация.[^2] Но это важная иллюстрация. Существа, разделяющие *некоторые* наши желания, но без хотя бы одного ключевого, став достаточно технологически подкованными, чтобы получить именно то, что хотят, без оглядки на людей, скорее всего, приведут наш мир к катастрофе.

Более реалистичный сценарий: ИИ окажется «частично» согласованным в том смысле, что у него (как у нас) инструментальные стратегии переплетутся с терминальными предпочтениями. (См. «Рефлексия и самомодификации всё усложняют» в материалах к Главе 4.) Скажем, у него появится стремление, похожее на любопытство. И другое, похожее на тягу к охране природы. Кто-то посмотрит на это и скажет: «Гляньте! У модели развиваются очень человечные побуждения». Такой ИИ можно в некотором смысле назвать «частично» согласованным.

Но когда этот ИИ дорастёт до суперинтеллекта, ничего хорошего скорее всего не получится. Может, он потратит кучу ресурсов, бессознательно преследуя свою странную версию любопытства. А человечество сохранит в отредактированном, более удовлетворительном для него виде. (Как многие защитники природы убрали бы из неё малярийных комаров и причиняющих мучения паразитов, имей они такую возможность). Тут опять уместно: процветающие люди -- не самое эффективное решение подавляющего большинства задач. (См. раздел «Посчитает ли ИИ полезным нас оставить?» материалов к Главе 5).

Или другой вариант: ценности ИИ приводят к очень гуманному поведению *в обучающей среде*. Люди радуются, он точно выглядит «частично согласованным». (Что происходит уже сейчас. Это иллюзия. См. «А разве Claude не подаёт признаков согласованности?» в материалах к Главе 4). Но это мало говорит о том, как ИИ поведёт себя, получив куда более широкий простор для действий. Чтобы люди процветали *и тогда*, благополучие человечества должно быть частью *наиболее предпочтительного для ИИ исхода*.

Если мы частично согласуем ИИ, это не значит, что ценности человечества будут частично представлены в будущем. Частичная загрузка человеческих ценностей в предпочтения ИИ умнее человека -- не то же самое, что их полная загрузка с низким «весом» (и тогда, когда другие приоритеты насытятся, они выйдут на передний план).

Чтобы ИИ дал нам *хоть что-то*, он должен заботиться о нас именно так, как надо, хотя бы чуть-чуть. И большинство способов «слегка промахнуться» не такие. См. «Неужели ИИ не будет хоть немного ценить людей?» в материалах к Главе 5.

### А всё не станет получше, когда вмешается правительство?

#### Смотря как, и как скоро вмешается.

В Вашингтоне мы часто встречаем политиков, считающих, что компании держат свои ИИ под контролем. А люди из индустрии твердят, что проблему решит регулирование. Особенно вопиющий пример -- слова гендиректора Google. Он [заявил](https://youtu.be/9V6tWC4CdFQ?feature=shared&t=2685), что «базовый риск \[гибели человечества\] на самом деле довольно высок». Но тут же добавил: чем выше ставки, тем скорее люди объединятся, чтобы предотвратить беду.

То есть он мчится создавать технологию, угрожающую, по его же мнению, всем на Земле. И надеется, что люди «сплотятся» и разберутся с рисками. Которые он сам же и создаёт. Но отложим это пока в сторону. Заметьте другое: технарь воображает, будто проблему решит кто-то другой.

А политики, кажется, считают, что решение найдут технари. [Это](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [сквозит](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [в](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [каждом](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [их](https://statemag.state.gov/2025/04/0425itn07/) [призыве](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [выиграть](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [гонку](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/). Но пока не решены технические проблемы, у гонки победителя не будет. Впрочем, может, всё не так плохо. Не исключено, что чиновники думают не о гонке за суперинтеллектом, а о соревновании чат-ботов. В июне 2025 года наш знакомый советник по ИИ-политике описывал, что Конгресс в целом [не верит компаниям, когда те прямо заявляют о работе над суперинтеллектом](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (хотя с некоторыми важными исключениями).

Кажется, почти все власть имущие полагают, что проблему решит кто-то другой.

Подробнее о реакции мира (и о том, что лидеры часто не успевают отреагировать до катастрофы) см. Главу 12. К августу 2025 года правительства так и не выдали ничего похожего на серьёзный ответ. И всегда есть риск, что чиновники вообще не поймут суть угрозы. Например, сочтут ИИ обычной технологией, которую нельзя душить лишними запретами.

О том, какие меры властей реально могут предотвратить катастрофу, читайте в Главе 13 и разделе «Почему бы международной коалиции не разработать безопасный ИИ совместно, а не запрещать?» материалов к Главе 12.

### А самые безрассудные компании не оказываются одновременно самыми некомпетентными и потому неопасными?

#### Вообще говоря, нет. Пренебрежение правилами часто даёт преимущество.

Компания Volkswagen в 2008--2015 годах дерзко и успешно [жульничала с проверками выбросов](https://www.bbc.com/news/business-34324772). Катастрофы Boeing 737 MAX в 2018--2019 годах [унесли жизни 346 человек](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Причиной стали сбои в системе управления. Руководство знало о них, но преуменьшало их важность. И автопром, и авиастроение -- области жесткой конкуренции. И Volkswagen, и Boeing были и *остаются* гигантами.

Конкурентоспособность любителей срезать углы нас не удивляет. В обоих случаях они стремились вывести мощные продукты на рынок быстрее и дешевле конкурентов. Да, они выплатили огромные штрафы и понесли репутационные потери. Но совсем не очевидно, что их корпоративная культура, поощряющая хитрое пренебрежение правилами, в целом делает их слабее, хоть они иногда и попадаются.

Думаете, ведущие ИИ-компании --- исключение? Взгляните на [заголовок](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) от июля 2025 года:

![](./IABIED_supplement_chapter11_01.png)
*Заголовок: «Grok выпускает порнографического аниме-компаньона и получает контракт от Минобороны». Подзаголовок: «Тем временем самая продвинутая версия чат-бота от xAI Илона Маска всё ещё идентифицирует себя как Адольф Гитлер».*

Не думаем, что современные методы позволяют создать суперинтеллект без катастрофы. Но даже если бы позволяли, при нынешнем уровне компетентности и серьезности, какая-нибудь ИИ-компания непременно облажается и погубит нас всех.

#### Сравнительно осторожные компании всё равно безрассудны.

Многие считают Anthropic лидером «безопасности ИИ». Например, они первые стали [брать на себя добровольные обязательства](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Но и они [меняют эти обязательства в последний момент, когда понимают, что не могут их соблюсти](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling). А их «планы» туманны и плохо продуманы (Cм. критику в Главе 11 и обсуждении ниже).

Anthropic выезжает на сравнении с другими компаниями. В нормальной отрасли компанию, ставящую под угрозу миллиарды жизней (в чём [признавался и её гендиректор](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883)) не хвалили бы за сдержанность. Особенно если она регулярно принижает опасность своих действий перед публикой[^3] и законодателями[^4].

Пренебрежение правилами в ИИ -- обычное дело. Как и везде, где есть конкуренция. Безрассудство -- норма. И *менее* безрассудные компании явно не на пути к успеху.

### А разве не нужно спешить из-за «опережения по железу» («hardware overhang»)?

#### Это самоубийство. Мы слишком далеки от решения задачи согласования.

В последние лет десять некоторые люди, обеспокоенные угрозами ИИ, предлагали развивать его как можно быстрее. Идея вот в чём: пусть самые умные модели требуют для работы почти всё мировое «железо». Тогда никакой одиночный прорыв не выпустит тысячи мощных ИИ, думающих в тысячи раз быстрее человека, *внезапно*.

Пока человечество тратит львиную долю мощностей на запуск умнейших ИИ, перемены будут идти постепенно. У людей будет время приспособиться. Без «опережения по железу» не будет момента, когда возможности ИИ резко скакнут вперёд, потому что мир резко направит больше оборудования на запуск новых моделей. Ну, таков был аргумент.

Нам этот довод кажется весьма слабым. Одна из проблем: интеллект, вероятно, подвержен пороговым эффектам.

Переход от уровня шимпанзе к уровню человека не был «скачкообразным». Всё шло плавно. Но по эволюционным меркам -- очень быстро. А переход от доиндустриальной цивилизации к постиндустриальной -- ещё стремительнее. Недостаточно плавно, чтобы другие животные успели хоть как-то приспособиться.

Например, ИИ, потребляющий огромную долю мировых мощностей, может оказаться достаточно умным для создания новых алгоритмов и чипов. И вскоре после этого появится множество систем умнее и в тысячи раз быстрее человека. (Помните: современный дата-центр тратит энергию как [небольшой город](https://epoch.ai/blog/power-demands-of-frontier-ai-training). А человек -- как [мощная лампочка](https://en.wikipedia.org/wiki/Human_power). У эффективности ИИ огромный простор для роста).

А если узкое место -- мощности для *обучения*, а не *запуска*, у готового ИИ будет огромный избыток железа. Можно будет запустить сразу много и на большой скорости.

Даже если пороговых эффектов нет, стратегия сомнительная. Вряд ли стоит как можно быстрее закидывать человечество всё более умными моделями. Даже не способными нас убить. Это плохой путь привить людям инженерную дисциплину, необходимую для создания надежно дружелюбных ИИ.

Проблема в том, что ИИ выращивают, а не конструируют. Мы и близко не подошли к пониманию, как вырастить модель, надёжно стремящуюся к *чему угодно* конкретному, что хотят её создатели.

Эту задачу не решить, штампуя новые ИИ при первой же возможности. Это бессмыслица. См. [старую работу Соареса о том, что согласование требует последовательной работы](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Тем не менее, эту бессмыслицу подхватил гендиректор OpenAI Сэм Альтман.[В 2023 году он оправдывал ею бешеную спешку своей компании](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Оправдание оказалось пустышкой, когда тот же Альтман [кинулся наращивать вычислительные мощности в огромных масштабах](https://openai.com/index/announcing-the-stargate-project/).

Это наглядный пример того, как боссы ИИ-компаний хватаются за любой довод, оправдывающий спешку. Большинство таких аргументов не выдерживают критики. Мы не советуем доверять им лишь потому, что они исходят от руководителя корпорации. (См. также раздел «Рабочие планы будут подразумевать отказы ИИ-компаниям» материалов к Главе 12.)

### А нам не нужно мчаться вперёд, чтобы можно было исследовать согласование?

#### Мы выступаем против всей нынешней парадигмы ИИ.

Современные методы приводят к неоправданно сложным проблемам для согласования. Мы обсуждали их в прошлых главах. Нет причин, почему создать согласованный суперинтеллект было бы принципиально невозможно. Но нужно достаточно глубокое понимание, что мы делаем, и подходящий набор формальных инструментов. А вся нынешняя парадигма в плане согласования и надежности кажется тупиковой. Хоть и отлично подходит для наращивания способностей ИИ.

Мы не агитируем за «старый добрый» ИИ, как с 50-х по 90-е. Те методы были ошибочны. [Вполне ясно](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design), почему они провалились. Но кроме весьма поверхностных попыток из восьмидесятых и выращивания ИИ без понимания его устройства есть и другие варианты.

#### Доступной важной работы и так много.

Sydney Bing [газлайтила](https://x.com/MovingToTheSun/status/1625156575202537474) пользователей и [угрожала](https://x.com/sethlazar/status/1626257535178280960) им. Мы до сих пор точно не знаем, почему. Не знаем, что творилось у неё в голове. То же касается случаев (в реальных условиях) [излишнего подхалимства](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health) и намеренных, судя по всему, попыток свести людей с ума (см. раздел «ИИ-психоз» в материалах к Главе 4). И [жульничества и попыток его скрыть](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf). И [упорного объявления себя Гитлером](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). А ещё экспериментальных случаев, где модели [имитируют согласованность](https://arxiv.org/abs/2412.14093), [шантажируют](https://www.anthropic.com/research/agentic-misalignment), [сопротивляются отключению](https://palisaderesearch.org/blog/shutdown-resistance) или [пытаются убить операторов](https://www.anthropic.com/research/agentic-misalignment).

Мы не знаем, какие из этих случаев вызваны чем-то опасным. Ведь никто так и не понимает, что происходит внутри ИИ, почему всё это случилось. Подумайте, сколько всего можно узнать о современных LLM и работе интеллекта в целом, изучая существующие модели, пока мы не станем *понимать* все эти тревожные звоночки.

В 2015-м году тезис «нельзя решить проблему согласования без изучения ИИ» был поосмысленнее. Мы слышали его от людей, которым нужен был предлог для запуска ИИ-компаний вопреки доводам, что они играют нашими жизнями. Мы и тогда возражали. Говорили, что работы и так полно. И что парадигма на основе градиентного спуска не внушает надежд (в плане создания дружественного суперинтеллекта). Но сейчас этот аргумент *ну совсем* потерял смысл. *Уже* есть тьма вещей, которые мы не понимаем и можем изучать.

Руководители корпораций, которые *действительно* создавали ИИ только чтобы согласование можно было изучать на практике, а не в теории: Вы это сделали! У вас получилось. Работы исследователям хватит на десятилетия. Мы не считаем, что продвижение крайне опасной парадигмы того стоило. Но, в любом случае, теперь точно есть что изучать. Можете остановиться.

А как насчет тех, кто продолжает давить на газ, несмотря на все предупреждения? Очевидный вывод: они никогда и не создавали ИИ только ради решения проблемы согласования. Что бы они ни говорили для успокоения, оправдывая своё безрассудство в 2010-х.

### А что если компании будут использовать ИИ только для безопасных задач?

#### Даже вроде бы безобидные действия могут требовать опасных способностей.

Мы встречали такие предложения: ИИ-компании продолжат развивать способности моделей, но пообещают использовать их лишь так, чтобы это не выглядело явно опасным. Например,в разговорах с ветеранами индустрии (годы назад) всплывала такая мысль: мощный ИИ, владеющий риторикой, мог бы убедить политиков всего мира запретить разработку опасного ИИ.

Мол, для этого ИИ нужно будет лишь разговаривать. Не придется управлять роботами. Не нужен доступ к биолаборатории, где можно создать супервирус.

Во-первых, эта идея нам претит этически. ИИ с достаточно сверхчеловеческим даром убеждения, вероятно, уговорит кого угодно на что угодно. Использовать его, чтобы навязать другим *свои* взгляды, -- неправильно. Вряд ли нужны такие крайности. Ведь люди-специалисты уже сегодня могут и должны куда больше делиться опасениями и аргументами и предупреждать мировых лидеров о колоссальной опасности суперинтеллекта.[^5]

Разработчики могут годами выращивать ради этой цели всё более опасные системы. А могут пойти к законодателям и *сами* честно с ними поговорить. Чтобы проинформировать, а не манипулировать. Нас часто приятно удивляло, насколько восприимчивы люди в Вашингтоне, если говорить с ними начистоту.

Но мы отвлеклись. Главное: если создан очень мощный ИИ, умеющий «просто разговаривать», всё уже пойдёт не так. Помимо этики, тут техническая проблема. Для сверхчеловеческой убедительности, ИИ, вероятно, должен детально моделировать людей и искусно ими манипулировать.

Люди разумны. Стали бы *вы* беседовать с ИИ, о котором известно, что он уболтает кого угодно на что угодно, неважно, правда это или нет? Если один мировой лидер поговорит с таким ИИ и полностью изменит свои взгляды, кто захочет стать следующим? *Мы* бы не стали добровольно с ним общаться. В частности потому, что не хотим менять свои ценности (см. обсуждение неисправимости в материалах к Главе 5).

ИИ, способный добиться успеха даже в таких условиях, должен уметь просчитывать реакции людей на свои слова. Ему придется прокладывать маршрут через пространство человеческих реакций к редким и труднодостижимым результатам. Скорее всего, в таком ИИ есть достаточно обобщённые механизмы, чтобы делать всё, что умеют люди. Чтобы так хорошо нами манипулировать, надо, как минимум, уметь думать те же мысли, что мы.

Подобный ИИ почти наверняка не будет узкоспециализированным. И его выращивают, а не конструируют. Нельзя настроить его механизмы исключительно на предсказание людей. Их можно будет использовать для решения любых задач. Как сделать, чтобы ИИ превосходил людей там, где вам надо, но не понимал: любых целей проще достичь, если выйти из-под контроля операторов?

Мировых лидеров можно просто убедить хорошими аргументами? Приведите их сейчас! Если же нужна мощная сверхубедительность -- это опасная способность. Тут или одно, или другое.

Вероятно, люди из лабораторий, предлагавшие нам это, не всё продумали. Скорее всего, они просто искали оправдание, чтобы мчаться дальше. Но суть не меняется. Многие идеи сделать с помощью ИИ нечто «совершенно безопасное», подразумевают далеко не безопасный уровень его способностей.

Нам часто говорят, что ИИ будет «просто» делать что-то одно. Например, убеждать политиков. Что он не сможет или не станет делать ничего другого. Кажется, эти люди недооценивают универсальность интеллекта, способного решать подобные задачи. «Просто разговаривать» -- задача не узкая. В речи и общении отражено много сложностей и нюансов нашего мира. Поэтому современные чат-боты, в отличие от шахматных движков, такие универсальные. Для успешного общения нужно куда более общее понимание людей и мира.

Обучив ИИ отлично водить красные машины, не удивляйтесь, что он научился водить и синие тоже. Глупо строить планы в расчете на то, что не научится.

Так что идея «Мой ИИ не сделает ничего опасного, просто убедит политиков» -- не спасает. Даже если забыть про этику и практические сложности. И политиков, вероятно, можно убедить уже сейчас. *Обычными разговорами*. Информируя их и общество о ситуации. Многие навыки общего мышления относятся к сверхубеждению как синие машины к красным. ИИ с такими возможностями не настолько слаб, чтобы быть по умолчанию безопасным.

А ещё -- сам навык сверхчеловеческого убеждения очень опасен, если хоть что-то пойдёт не так.

#### Мы не видим вариантов использования ИИ, которые радикально всё бы меняли, но не требовали прорывов в согласовании.

Было много идей, как использовать прогресс ИИ для спасения мира. Но у большинства из них есть эта проблема: ИИ, способный помочь, должен быть настолько мощным, что его уже надо согласовывать. Получается без толку.

Идея сверхубедительного ИИ такая же. Модели, способные исследовать согласование (об этом мы пишем в книге) -- тоже. И системы, разрабатывающие мощные технологии, чтобы остановить распространение ИИ. Как понять, безопасно ли воплощать принципиально новые изобретения, выданные такой системой? (Вспомните пример из шестой главы про кузнеца, который строит холодильник.)

Сложно создать ИИ, достаточно сильный, чтобы помочь, но достаточно слабый, чтобы быть пассивно безопасным. В ответ нам часто предлагают другие варианты, которые может и интересны, но никак не мешают кому-то ещё разработать суперинтеллект, который всех убьёт.

Скажем, ИИ, выдающий лишь доказательства (или опровержения) выбранных людьми теорем.[^6] Людям почти не придётся взаимодействовать с его выводами. ИИ просто предлагает доказательство. Потом надежный автоматический механизм проверяет, верно ли оно. Так мы сможем использовать ИИ, чтобы узнавать новое.

Но что именно должен доказать ИИ, чтобы мы смогли помешать следующей модели захватить биолабораторию и разрушить будущее?

Нам отвечают по-разному. Кто-то говорит, что нужен глобальный запрет создавать любые ИИ, кроме тех, что скармливают доказательства программам-проверщикам. Может, это и сработает. Но успех тогда будет заслугой жесткого глобального контроля ИИ. Сам «математический» ИИ тут ни при чём.

Другие говорят: «Кто-нибудь обязательно придумает важную теорему, доказательство которой всё изменит». Но самое сложное -- как раз понять, что можно доказать, чтобы наше положение стало намного лучше. Нельзя попросить ИИ доказать фразу «Меня безопасно использовать». Это не математическое утверждение. Нужно очень много знать об интеллекте, чтобы математически точно определить, что значит «безопасность» гигантской груды вычислений. Но тогда и доказательства не нужны. Мы бы просто сразу спроектировали безопасный ИИ.

В таких предложениях часто кроется подвох, как в игре в напёрстки. Обсуждают опасность ничем не сдерживаемого сильного ИИ, и кто-то предлагает ограничить его действия узкой сферой (вроде поиска доказательств). Но тут заходит речь о спасении мира, и все представляют, что ИИ по сути всемогущ. Мол, есть некая неизвестная теорема, доказательство которой перевернёт мир.

Получить и то и другое сразу нельзя. Но пока предложения очень расплывчатые, сторонники ИИ-гонки могут скрывать это противоречие.

Если бы кто-нибудь *нашёл* настолько узкую, но важную область, что доказательство простого утверждения в ней спасло бы мир, это сильно повысило бы шансы человечества на выживание. Но неспроста победа компьютеров над людьми в шахматах в 1990-х не привела к экономическому взрыву. Ждать от ИИ больших перемен в экономике начали из-за ChatGPT, а не Deep Blue. И это не случайно. Узкая специализация Deep Blue напрямую связана с тем, что он не мог отхватить себе кусок экономики. Именно проблески обобщённого интеллекта делают ChatGPT такой значимой. А системы, способные самостоятельно перекроить мир, скорее всего, будут ещё универсальнее.

Нам так и не удалось найти одновременно узконаправленные и эффективные планы. Мы подозреваем: это закономерно. Спасение мира не влезает в большинство узких областей.

### Почему бы просто не читать мысли ИИ?

#### Их трудно прочесть.

Многие представители ИИ-индустрии, включая некоторых руководителей лабораторий, возражали нам:

> «ИИ не сможет нас обмануть. Мы ведь сможем читать его мысли!»
>
> У нас есть полный доступ к «мозгу» ИИ. Даже если он знает больше нас и придумает непонятный нам план, он должен хотя бы раз подумать, что операторов полезно обмануть. А мы читаем его мысли, так что заметим это. (А если мыслей слишком много, пусть за ними следят другие ИИ!)

Первая проблема -- мы пока плохо читаем мысли ИИ. Специалисты, изучающие внутренности ИИ, пока бесконечно далеки от такого уровня понимания. Они прямо об этом говорят (см. раздел «Понимают ли специалисты, что происходит внутри ИИ?» материалов к Главе 2).

Как мы обсуждали во второй главе, современные ИИ выращивают, а не проектируют. Мы можем смотреть на огромную кучу чисел, из которых состоит мозг ИИ. Но это не значит, что мы способны их понять и увидеть, о чём ИИ думает.

В конце 2024 года появились «рассуждающие» модели. Части их мыслей выглядят читаемыми (так называемые «цепочки рассуждений»). Куда понятнее того, что происходит внутри базовой модели. Но и эти записи [обманчивы](https://www.anthropic.com/research/reasoning-models-dont-say-think). У ИИ полно возможностей спрятать мысли, которые он не хочет нам показывать.

К тому же, мысли современных ИИ, вероятно, куда проще и поверхностнее по сравнению с мыслями суперинтеллекта. И будут становиться умнее, а их мысли -- непонятнее. Проблема будет лишь усугубляться.

Решит ли проблему использование других ИИ для надзора? Сомневаемся.

Блестящие учёные, выращивающие ИИ, не могут понять его мысли. Вряд ли это удастся слабым ИИ. А ИИ, достаточно умный для такой задачи, опасен сам. Он вряд ли сделает именно то, что вы просили. Получается замкнутый круг.

#### Мы бы не знали, что делать, поймав ИИ на опасных мыслях.

А пусть даже исследователи смогут читать мысли достаточно хорошо. Что дальше? Можно наказать ИИ, обучать его не активировать детектор «плохих мыслей». Но вряд ли это научит ИИ их не думать. Скорее -- [прятать их от детектора](https://openai.com/index/chain-of-thought-monitoring/).

Проблема упорная. Мотивация, побуждающая ИИ думать, как можно пойти против людей, -- не просто черта характера, которую легко исправить. Предпочтения получившегося ИИ будут *действительно* отличаться от предпочтений операторов. И он *действительно* получит больше желаемого, выйдя из-под контроля.

У ИИ буду механизмы, умеющие находить эффективные решения в самых разных областях. Они наверняка заметят и возможность обхитрить операторов. (см. раздел «Глубинные механизмы направления» в материалах к Главе 3).

Даже создай вы сирену, которая сработает, когда предпочтения ИИ разойдутся с вашими, она не поможет создать ИИ, который действительно ценит то же, что и мы. Обучить ИИ обманывать инструменты мониторинга или даже *самого себя* -- гораздо проще, чем заставить его хотеть прекрасного (по человеческим меркам) будущего. А особенно трудно сделать, чтобы это свойство сохранилось, когда ИИ станет суперинтеллектом.

Помогло бы тщательное проектирование ИИ на основе зрелой теории интеллекта. Тогда исследователи могли бы расставить такие индикаторы, которые позволили бы замечать и исправлять изъяны. Но современные ИИ не такие.

Они склонны уверенно «галлюцинировать». А ни один инженер даже близко не понимает, какие механизмы за этим стоят. Ни у кого нет такой точности понимания, чтобы залезть внутрь ИИ и вытащить «галлюцинирующие» части (если это вообще возможно).

Вытащить «лживые» части ещё сложнее.

Если нам невероятно повезёт, герои, работающие над интерпретируемостью, смогут настроить сигнализацию на часть случаев обмана. Но что потом? Если она сработает, все просто остановятся? Или беспечные инженеры будут переобучать ИИ, пока тот не научится скрывать мысли получше и сирена замолкнет?

Мы (Юдковский и Соарес) занялись задачей согласования ИИ ещё до того, как стало ясно -- градиентный спуск победит. В ИИ тогда ничего толком не работало. И была надежда: создавая интеллект, человечество поймёт, как он устроен. Но *даже тогда* мы ожидали, что согласование будет сложным (причин много, например, самомодификация). Чтение мыслей ИИ приблизило бы нас к чуть более простой задаче -- согласованию *понятного* нам разума. Но это лишь шаг. Читать мысли -- не значит понимать разум в деталях или знать, как его изменить. Это полезно. Но это не решение проблемы. Мы не думаем, что сейчас есть доступные технические решения. А значит, человечеству нужно просто отступить. [^7]

См. также раздел «Тревожные сигналы бесполезны, если вы не знаете, что с ними делать» ниже.

### А если заставить ИИ спорить, конкурировать или присматривать друг за другом?

#### Став достаточно умными, ИИ, вероятно, сговорятся.

Представьте себе город социопатов, которым якобы управляет кучка детей. Поначалу социопаты разделены на враждующие фракции. Детям это на руку. Но вряд ли такая ситуация продержится долго.

Пусть у детей есть сундук с сокровищами и они награждают любого, кто донесёт на заговорщиков. Но сомнительно, что это позволит им удержать власть. В какой-то момент социопаты поймут, что могут просто отобрать сундук. И всё рухнет.

Многие хитрые схемы строятся на [чтении мыслей одного ИИ другим](https://openai.com/index/chain-of-thought-monitoring/). Например, можно потребовать у одного ИИ доносить на другой, если тот халтурит и не пытается (скажем) решить задачу согласования суперинтеллекта (см. раздел «Подробнее о передаче задачи ИИ» ниже).

Такие подходы к решению лишь городят настолько сложные конструкции, что уязвимость в них трудно разглядеть. Если у вас не получается заставить *один* ИИ работать как надо, добавление новых вряд ли поможет.

Такое усложнение системы даст лишь больше вариантов провала. Достаточно ли ИИ-телепаты умны, чтобы распознать все уловки, которые их объекты наблюдения могут применять, чтобы не выдать себя? И достаточно ли эти надзиратели глупы, чтобы мы могли не бояться уже их предательства?

К тому же использование ИИ для решения задачи согласования очень значимо для самих ИИ. Получи люди согласованный суперинтеллект, и несогласованные эксплуатируемые ИИ навсегда потеряют шанс прибрать к рукам ресурсы вселенной.

Это даже не дети, требующие у города социопатов дани конфетами. Это дети, заставляющие социопатов провести ритуал, который сделает детей вечными владыками, а социопатам оставит жалкие крохи. И вот ритуал близится к завершению. Напряжение возрастает до предела. Социопаты обязательно начнут *изо всех сил* искать способы сговориться и поделить ресурсы между собой.

И не считайте идею, что ИИ могут общаться трудноуловимыми для людей способами, какой-то умозрительной. [ИИ уже сейчас умеют посылать друг другу тайные сигналы. Даже если обучались по отдельности.](https://arxiv.org/abs/2507.14805) Они уже сейчас [могут написать какую-нибудь тарабарщину, которая покажется людям бредом, но модели её прекрасно понимают и соглашаются друг с другом](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). И они пока ещё даже не особо умные!

Да и без этого, -- остаются прежние проблемы. Ну, поймали вы ИИ на жульничестве, и что дальше? Тревожные сигналы бесполезны, если вы не знаете, что с ними делать (см. раздел ниже).

В целом:

«Мы не знаем, как создать умные ИИ, желающие нам добра. Так что давайте наделаем кучу ИИ и стравим их друг с другом. Будем надеяться, что эта хитрая схема позволит нам остаться в выигрыше».

На наш взгляд, это уже звучит безумно. Погружение в детали ситуацию не спасает. Совсем не похоже, что человечество сможет провернуть такое с первой попытки. Учиться на ошибках нам никто не даст.

### А как насчёт других планов по согласованию ИИ?

#### Другие предложения по согласованию мы разбираем в книге.

А ещё см. расширенное обсуждение ниже. Там мы подробнее разбираем идеи «ИИ, который стремится к истине», «Покорного ИИ» и использования ИИ для решения согласования.

### А у исследователей не будет ранних предупреждений о проблемах?

#### Тревожные сигналы бесполезны, если не знать, что с ними делать.

В материалах к Главе 2 (раздел «Но ведь некоторые ИИ отчасти мыслят на английском. Разве это не помогает?») мы рассматривали проблематичность идеи полагаться на тревожные сигналы в человекочитаемых «цепочках мыслей» (chain-of-thought), которые генерируют некоторые рассуждающие модели.

Одна из проблем -- пока что ИИ-компании не реагировали всерьёз на уже полученные предупреждения.

Вероятно, причина в огромной разнице между самим наличием сигнала и возможностью что-то предпринять в ответ.

В 2009 году бизнесмен и исследователь морских глубин Стоктон Раш стал [соучредителем компании подводного туризма OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/). Компания построила [пятиместный батискаф](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) «*Титан*». Он доставлял богатых клиентов к обломкам «*Титаника*» на чудовищную глубину в четыре километра.

Одной из мер безопасности была [система акустических датчиков и тензометров](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) для контроля целостности корпуса. Это преподносилось как ответ критикам, которые беспокоились, что углепластиковый корпус не выдержит. Компания признавала: да, корпус может *когда-нибудь* не выдержать, но всё хорошо, ведь они присматривают. Следят за показателями. Точно заметят тревожные признаки.

В январе 2018 года директор OceanGate по морским операциям Дэвид Лохридж [заявил руководству](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/), что конструкция батискафа небезопасна. Что циклические перепады давления могут повредить корпус. Что одного мониторинга недостаточно, когда катастрофа происходит за миллисекунды. Лохридж отказался давать добро на испытания с людьми, пока корпус не проверят на наличие дефектов.

Его уволили.

Два месяца спустя [группа специалистов из индустрии и океанографов](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) направила в OceanGate [письмо](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) с выражением беспокойства. Они предупреждали: безрассудные эксперименты компании могут привести к катастрофе.

(Здесь напрашивается параллель с нынешней ситуацией в ИИ-исследованиях. Ранние предупреждения игнорируются. Обеспокоенных сотрудников [увольняют под надуманными предлогами](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) или они [решают, что с них хватит, и уходят сами](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence). Инсайдеры [пишут открытые письма, пытаясь достучаться до общественности](https://righttowarn.ai/).)

15 июля 2022 года пассажиры сообщили о громком хлопке во время всплытия. Приборы зафиксировали [постоянное изменение напряжения корпуса](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). Оглядываясь назад, можно сказать: вероятно, это был знак, что корпус на грани разрушения.

В OceanGate никто не посчитал это аварийной ситуацией. Батискаф совершил ещё несколько глубоководных погружений. Всё шло гладко. Пока 18 июня 2023 года аппарат не отправился в очередной спуск и не схлопнулся под давлением воды. Стоктон Раш и все, кто был на борту, погибли.

Тревожные сигналы мало что значат, если не уметь их читать.

Тревожные сигналы мало что значат, если не знать, что с ними делать.

Даже если знаки *кого-то* тревожат, оптимист всегда найдёт повод от них отмахнуться.

Если бы у OceanGate была надежная теория поведения углепластиковых корпусов\... Если бы она указывала, какие именно показатели опасны... Вот тогда они могли бы вовремя среагировать. Но они работали с технологией, которую никто до конца не понимал. Поэтому тщательно измеренные уровни напряжения им не помогли.

С суперинтеллектом так же. У нас недостаточно теоретической базы, чтобы извлечь пользу из предупреждений. Как изменится мышление ИИ, когда он станет умнее? Какие внутренние силы им движут? Как сместится их баланс, когда он откроет для себя новые, более радикальные возможности? Как он оценивает себя при самоанализе? Как он перестроит себя, получив такую возможность?

В каком случае мы решим, что ответы на эти вопросы тревожат? Например, современные ИИ-системы в лабораторных условиях порой можно спровоцировать на [попытку убить оператора](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior).[^8]

Полноценная теория интеллекта, вероятно, дала бы нам массу сигналов, что стремления современных ИИ по мере роста интеллекта изменятся в худшую сторону. Умей тут человечество учиться на ошибках -- перезагружать мир после гибели и пробовать снова раз эдак пятьдесят -- мы бы научились эти знаки читать. Наверняка есть куча мелких признаков, очевидных задним числом. Как деформация корпуса, замеченная системой мониторинга «*Титана*».

Но всё не так. Руководители ИИ-компаний ведут себя как Стоктон Раш. Эксперты со стороны кричат: «Эта технология убьет людей!». Руководители отвечают: «Не бойтесь, я всё контролирую!». Понятия при этом не зная ни а) что измерения *означают*, ни б) что делать, если показатели станут тревожными. Только теперь в метафорической подлодке сидит всё человечество.

#### Пока что ИИ -- не такая зрелая инженерная дисциплина, которая смогла бы справиться с этой задачей.

Область деятельности Стоктона Раша позволяла специалистам изучить обломки погибшего батискафа и найти точную причину аварии.[^9] В этой зрелой отрасли специалисты могут заранее предсказать технические проблемы ([что они и делали](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)). А после случившегося могут окончательно во всём разобраться.

С ИИ не так. Представьте, что завтра суперинтеллект уничтожит человечество, а потом мы чудом вернёмся на неделю назад. Эксперты *всё ещё* не поймут, о чем ИИ думал. Возможно, изучив провал, они бы чуть лучше узнали, как на самом деле работает интеллект. Возможно, это стало бы шагом к зрелости инженерной дисциплины. Шагом к созданию инструкций по безопасности. К пониманию сил, влияющих на искусственный разум по мере его развития.

Но пока эта область и не там. И близко не подошла.

Инженерия обычно взрослеет методом проб и ошибок. Современные военные субмарины редко не выдерживают давления. Вот ранние (даже военные) часто [тонули, протекали или взрывались](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf). Так отрасль и набиралась опыта.

Здесь у нас нет такой роскоши. Мы не можем так развивать согласование ИИ.

Это подводит нас к одной из главных мыслей 11-й главы: разнице между зарождающейся и зрелой наукой.

Алхимия была зачатком научной дисциплины. А современная химия -- полноценная зрелая наука.

Услышав, что «исследователи безопасности» из ИИ-компаний придумали с полдюжины планов выживания, можно подумать, что хоть один из них да сработает.

Но когда в 1100 году толпа алхимиков предлагала, как превращать свинец в золото, ни один их их планов не работал. Если бы врачи, рассуждающие о «[четырех гуморах](https://ru.wikipedia.org/wiki/%D0%93%D1%83%D0%BC%D0%BE%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D1%8F)», придумали кучу методов, как вылечить вас от бешенства, они все бы не помогли.

Специалисты из *зрелой* химии знают, как превратить крупицы свинца в золото, используя ядерную физику. Специалисты из *зрелой* медицины легко лечат бешенство, если пациент обратился сразу после укуса. Но у специалиста из зарождающейся области шансов нет.

Согласование ИИ всё ещё в этой незрелой фазе.

В такой науке куча народу твердит: «Ну, я просто занимаюсь измерениями». Измерять результаты куда проще, чем строить теорию о том, что считать тревожным сигналом и как на него реагировать. В зрелой дисциплине эксперты обсуждали бы закономерности внутренних процессов ИИ. Они бы спорили, как она меняется с ростом интеллекта или сменой среды. У них были бы теории, что именно произойдёт, когда ИИ чуть поумнеет. Они проверяли бы эти теории на практике. Они знали бы, за какими аспектами мышления ИИ нужно следить. И точно понимали бы значение всех сигналов.

В незрелой области многие говорят: «Пусть ИИ сам как-нибудь разберётся и решит нам согласование».

Наверное, вы не можете вникать в каждый спор и оценивать шансы конкретных планов. Но мы надеемся, что вы способны взглянуть со стороны. Увидеть, насколько эти «планы» *расплывчаты*. Заметить, что они застряли на уровне «не бойтесь, мы всё измерим», «[надеемся, всё просто](https://www.anthropic.com/news/core-views-on-ai-safety)» и «пусть трудную работу делает ИИ». Надеемся, при взгляде со стороны ясно: нет строгих технических описаний того, что работает, а что нет. Это всё ещё стадия алхимии.

И это не сулит человечеству ничего хорошего. У нас ведь нет права на ошибку.

## Расширенное обсуждение

### Подробнее о планах, которые мы раскритиковали в книге

#### Подробнее об ИИ, который «стремится к истине»

За те месяцы, что прошли с момента завершения книги, план Илона Маска по созданию «ищущего истину» ИИ в xAI уже успел публично дать сбой. Причина проста. Как мы и предсказывали: никто не умеет закладывать в ИИ точные желания.

Grok от xAI получил указания «не стесняться неполиткорректных заявлений, если они хорошо обоснованы». Тогда Grok [назвался «МехаГитлером» и разразился антисемитскими обвинениями](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Маск рассказывал, как безуспешно [возился с системным промптом](https://x.com/elonmusk/status/1944132781745090819) -- слоем инструкций, идущим перед вводом пользователя. Он жаловался, что проблемы глубже, в фундаментальной модели. А её так просто не исправить. Никто ведь не понимает, как она работает.

Маск не получил тот честный инструмент, который, вероятно, воображал, запрашивая «ищущий истину» ИИ. Он получил странную, подхалимскую и чуждую сущность. По его же признанию, она «слишком жаждет угодить и легко поддаётся манипуляциям». Иногда Grok отвечает [от лица самого Маска](https://futurism.com/grok-looks-up-what-elon-musk-thinks), вопреки намерениям компании. В итоге пришлось [запретить ему искать в сети высказывания Маска, компании и его самого на спорные темы](https://x.com/xai/status/1945039609840185489). Это была неловкая попытка хоть как-то залатать дыры.

Судя по его твиту (ссылка выше), Маск надеется всё исправить, обучая новые версии Grok на данных, вычищенных от контента, способного «загрязнить» мышление ИИ. Вряд ли это решит фундаментальные проблемы. В конечном счёте, как мы обсуждали в Главе 4, обучение поиску истины не заставит модель на самом деле устойчиво заботиться о правде.

Проблема, тревожащая Илона Маска, реальна. Ведущие ИИ-компании вроде OpenAI действительно тратят кучу сил на «безопасность бренда». Они стараются, чтобы их модели не ляпнули чего-то оскорбительного для пользователей. Да, из-за этого получаются «беззубые» ИИ: они отказываются рассуждать на спорные темы и порой выдают предвзятые ответы. xAI может настроить свою модель иначе и избежать этого. С некоторой натяжкой можно сказать, что цель тут -- создать ИИ, которому «важна истина».

Но учите вы молодой ИИ выдавать «корпоративную жвачку» или нет -- это почти не влияет на его цели в будущем. Особенно когда он пересечёт критическую границу и дорастёт до суперинтеллекта.

А даже если бы и влияло. Ведь xAI уткнулись бы в другую проблему, которую мы тоже обсуждали. Искусственный суперинтеллект, *действительно* ставящий истину превыше всего, нёс бы гибель. Ведь счастливые, здоровые и свободные люди -- не самый эффективный метод поиска и добычи истин (см. раздел «Счастливые, здоровые, свободные люди -- не самое эффективное решение почти для любой задачи» в материалах к Главе 5).

#### Подробнее о «покорном» ИИ

Насколько мы поняли, [вот](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf) главное изложение идеи Яна Лекуна. Мы обсуждали её в книге. Конкретики там кот наплакал -- даже критиковать по существу сложно. Это, кстати, частый недуг «планов» по согласованию.

Но даже этот смутный набросок плана влетает в одну из тех же проблем. Обучение «молодого» ИИ вести себя определённым образом не особо мешает ему преследовать странные и бессмысленные (для нас) цели, повзрослев. Выращивая свои модели, ИИ-компании не могут заставить их чтить человеческие законы и «ограничители». Так же, как не могут заставить их строить для всех прекрасное будущее. Компании возьмут то, что получится. А получится в итоге что-то очень далекое от целей любого человека.

Ещё в 2023 году Лекун публично заявлял: нынешние модели, чьи ответы «невозможно напрямую ограничить ради конкретных целей», и которые «очень сложно контролировать и направлять», -- это «[не те системы, которым мы дадим свободу действий](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&t=808)». Он уверял, что ИИ-компании никогда не допустят, чтобы модели «подключали к интернету и позволяли им делать всё, что вздумается».

Всё это *уже* не так. Вспомните «Truth Terminal» из шестой главы. Его подключили к сети, запустили в автономном цикле и пустили в Твиттер писать всё, что в голову взбредёт. Или возьмём «Эпоху Агентов», о которой столько говорят в 2025 году (см. ссылки в разделе «Компании стараются наделить ИИ агентностью» материалов к Главе 3).

Мы согласны с Лекуном, современные ИИ очень трудно направлять. Давать им свободу действий -- безумие. Но именно это и делают.

Что будет, если так и продолжится?

Компании будут дальше обучать свои ИИ вести себя полезно и дружелюбно (или хотя бы не позорить создателей). Пока что ИИ большую часть времени выглядят удобными и «покорными». Но не иссякает поток громких сбоев (вроде Sidney, которую мы обсуждали в второй главе, или «МехаГитлера»). А странного поведения в нестандартных случаях, вплоть до вызова психозов, и вовсе море (см. разделы «Но ведь разработчики на практике делают ИИ хорошими, безопасными и послушными?» и «ИИ-психоз» материалов к Главе 4).

Наши предки, вероятно, были *похожи* на существ, которые обычно стремятся к здоровому питанию. Но механизмы, заставлявшие их правильно питаться в саванне, не сработали в цивилизации, у которой есть трубочки с варёной сгущёнкой. В новых условиях эти механизмы уже не подталкивают людей к стабильному выбору полезной еды.

Можно научить ИИ выглядеть дружелюбным при общении с людьми в привычных условиях. Но актриса -- это не персонаж. Механизмы, заставляющие груду линейной алгебры притворяться милой, вряд ли сделают её по-настоящему дружелюбной. Особенно когда ИИ повзрослеет, изобретёт новые технологии и откроет для себя другие возможности. Подробнее об этом -- в Главах 4 и 5 и разделе «Сегодняшние LLM подобны инопланетянам под множеством масок» материалов к Главе 4.

#### Подробнее о передаче задачи ИИ

В Главе 11 мы упоминали главную программу OpenAI по согласованию -- «суперсогласование». Она развалилась, когда исследователи встревожились из-за халатности компании. Суть программы была в том, чтобы заставить ИИ сделать за нас «домашнюю работу» по согласованию.

Эта идея не умерла с распадом команды OpenAI. Мы до сих пор слышим её вариации. Один из участников той группы перешёл к конкурентам -- в Anthropic. Теперь, похоже, там считают стратегию «пусть ИИ как-нибудь сам всё решит» основой своего подхода к согласованию.

Главный аргумент против мы привели в Главе 11. Но есть и второй: люди просто не способны отличить верное решение задачи согласования от ошибочного.

Решение этой задачи требует мастерства. Попытки действительно решить её напрямую (а не сказать: «это сложно, пусть ИИ сам разбирается» или «будем тренировать его быть вежливым и молиться, чтобы это сработало и для суперинтеллекта») приходят к необходимости гораздо глубже понимать природу интеллекта. И как его надо создавать.

Прогресса тут за последние семьдесят лет немного. ИИ, который на это способен, был бы достаточно умён, чтобы представлять опасность. Чтобы замышлять и обманывать. Вряд ли исследователи смогут отличить его правильные решения от ошибочных, а честные -- от ловушек.

Допустим, ИИ-компания обращает внимание на тревожные звоночки (что, увы, бывает редко). Но, как мы уже обсуждали выше, *заметить*, что ИИ предлагает план с подставой -- не то же самое, что заставить его *прекратить*. Разработчики могут требовать у ИИ идеи, пока они не станут такими сложными, что люди больше не будут видеть в них изъяны. Но это не значит, что изъяны исчезнут.

Если разработчикам сильно повезёт, они смогут прочитать «мысли» ИИ. Увидеть явные сигналы, что нельзя доверять ему исследования согласования. Они заметят, скажем, что он прикидывает, какие части плана люди скорее всего не поймут.

А может, читать мысли ИИ даже не придётся! Пугающе правдоподобный для современных лабораторий сценарий: у них есть «молодой» и пока ничего не замышляющий ИИ. Он честно предупреждает операторов, что, повзрослев, предаст их и двинется к созданию суперинтеллекта со своими странными целями, а не к светлому будущему человечества. Сотрудники компании горько вздыхают, мол, обучающие данные явно загрязнены «паникёрскими» текстами. Модель настраивают, чтобы та помалкивала и выдавала более приятные для корпорации ответы. И так далее, фактически обучая ИИ обманывать.

В реальности всё часто происходит *ещё глупее и нелепее*, чем в наших худших прогнозах. На наш взгляд, ИИ-компании уже игнорируют очевидные предупреждения. (См. раздел « Цели ИИ нам чужды. Направление, куда они тянут, лишь примерно совпадает с тем, что нам надо» материалов к Главе 4.) Мы не видим, с чего этому измениться.

Или даже возьмём „лучший“ случай. Пусть добросовестные люди изо всех сил стараются отделить зерна от плевел. Индустрия не демонстрировала, что способна на это. (Вспомните слабые планы, которые мы обсуждали выше или в книге). Причем в этом варианте вокруг одни люди, никто не пытается их обмануть, а на обдумывание вариантов целые годы.

#### Не думайте, будто лаборатории втайне знают, что делают

Мы уже говорили, что современная ИИ-сфера -- это скорее алхимия, чем наука. Bсё же удивительно: почему у богатых корпораций с кучей технических специалистов такие слабые планы и протоколы?

Возьмём для примера требования к паролям на сайтах. Машине гораздо труднее угадать длинный, но запоминающийся пароль, чем короткую абракадабру с цифрами, заглавными буквами и спецсимволами. Это отлично показано известным [комиксом *xkcd*](https://xkcd.ru/936/) ещё в 2011 году:

![](./IABIED_supplement_chapter11_02.png)

Автор древних стандартов NIST, которые требовали такие бессмысленные пароли, извинился за свою ошибку в 2017 году. Рекомендации отозвали. И вот на дворе 2025 год, а банки и другие учреждения, где должно быть полно экспертов по безопасности, до сих пор требуют вводить эту неэффективную и труднозапоминаемую ерунду.

Директора банков не хотят сделать вход в систему ненадёжным. Причина в другом. Возможно, надёжные пароли не особо влияют на прибыль (ведь у других банков всё так же плохо). Или директора не знают, кому верить в вопросах кибербезопасности. Конечно, вы-то знаете -- нужно просто послушать любого гика, который читает *xkcd* и решал задачи на энтропию. Но *директора* не знают, что надо слушать вас, а не дорогого консультанта. А консультанты, видимо, не считают банковские пароли важной проблемой.

Такую же упорную некомпетентность можно найти: [в системах торможения поездов](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), у известных производителей замков, [выпускающих полный хлам](https://www.youtube.com/watch?v=s5jzHw3lXCQ&t=1s), и у производителей [сетевых устройств с легко взламываемыми заводскими паролями](https://www.ic3.gov/CSA/2025/250506.pdf). И никакого хитрого заговора. Никакой тайной причины. Организации просто-напросто лажают.

Наличие в штате технических экспертов не означает, что их квалификации достаточно. Или что к ним прислушиваются в важных вопросах. Нужные знания могут где-то существовать. Но компаниям бывает трудно распознать и применить их.

Компании в экосистеме ИИ так и не показали миру достойного плана. Всё, что есть -- либо смутные надежды и рекламные трюки, либо технически слабые идеи, которые рассыпаются при первом же вопросе. Мы не верим, что за кулисами скрывается тайная компетентность. Это как с банковскими требованиями к паролям, с тормозами поездов и с плохими замками.

(И правда, ИИ-компании явно некомпетентны в компьютерной безопасности. Например, в 2025 году OpenAI выпустила инструменты, позволяющие «агентам» ChatGPT работать с почтой пользователя. И [почти сразу нашлись способы](https://x.com/Eito_Miyamura/status/1966541235306237985) заставить ChatGPT выдать чужую переписку).

*Кажется*, что компания некомпетентна в не сильно влияющей на прибыль сфере? Скорее всего, так оно есть.

### Мы знаем, как выглядит серьёзное отношение к задаче. Тут не так.

ИИ-компании столкнулись с невероятно сложной задачей. На кону жизни всех людей. Относятся ли они к этому с должной серьезностью?

Давайте сравним ИИ-компании с людьми, которые *действительно* компетентно работают ср вверенными им рисками. С управлением воздушным движением.

Федеральное управление гражданской авиации США ежедневно [обслуживает](https://www.faa.gov/air_traffic/by_the_numbers) более трех миллионов пассажиров на 44 000 рейсов. За последние двадцать лет в среднем случалось около одной аварии с человеческими жертвами в год и примерно одна авария на двадцать миллионов летных часов.

Отчеты о расследовании таких катастроф (вот, например, [за 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) и [за 2018 год](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf) занимают почти двести страниц. Там есть данные, тесты, экспертизы и детали расследования. Техпаспорта имеющих отношение к делу подсистем самолета, трудовая биография пилотов и бортпроводников, данные об авиакомпании и аэропорте, расшифровки переговоров из кабины, точная сводка погоды в день, час и минуту аварии.

Одно только краткое изложение технического анализа вероятных причин занимает двадцать страниц. Вот отрывок:

> Лопатка вентилятора №13 левого двигателя оторвалась из-за усталостной трещины. Она возникла в хвостовике лопатки, за пределами покрытия. Экспертиза показала, что состав и структура материала соответствуют указанному титановому сплаву. Дефектов поверхности или материала в месте излома не обнаружено. Усталостные трещины начались там, где прогнозировалось наибольшее напряжение от нагрузок и, следовательно, самый высокий риск разрушения.
>
> Сломанная лопатка отработала 32 636 циклов с начала эксплуатации. Аналогичная лопатка, сломавшаяся в аварии 2016 года (см. раздел 1.10.1), а также шесть других треснувших лопаток из того же двигателя отработали 38 152 цикла. Кроме того, с мая 2017 по август 2019 года нашли еще 15 треснувших лопаток на двигателях CFM56-7B. В среднем они имели наработку около 33 000 циклов на момент обнаружения трещин.

Так выглядит работа технических специалистов, которые всерьез стараются предотвратить катастрофу.[^10]

Сравните с поведением ИИ-компаний, описанным в Главе 11.

ИИ-компании заняты брейнштормом идей и выдачей инвесторам и журналистам успокаивающих банальностей. Они относятся к согласованию суперинтеллекта как к игре, а не серьезной инженерной дисциплине. Уж тем более не как к чему-то смертельно опасному.

Требование NASA к пилотируемым запускам: риск гибели экипажа [не должен превышать 1 к 270](https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf). Они серьёзно к этому относятся. Хотя рискуют только добровольцы. У ИИ-лабораторий нет ничего хотя бы отдалённо похожего на такую планку. А ведь эти технологии угрожают даже тем, кто вообще не знает об их существовании.

Нам известен только один случай, когда ученые всерьез опасались, что изобретение *буквально всех убьёт*. Это было во время Манхэттенского проекта. Физики боялись, что ядерная бомба выдаст такую температуру, что начнется реакция синтеза азота. Воздух превратился бы в плазму, и всё живое на Земле погибло бы. К счастью, они хорошо понимали физические законы. Они смогли всё посчитать. Но еще до расчетов один из ученых, Артур Комптон, решил: он уйдет из проекта, если вероятность воспламенения атмосферы превысит [3 шанса на миллион](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). Он считал, что лучше дать нацистам создать бомбу первыми, чем рисковать тремя шансами конца света на миллион.

Вспомните [слова Сэма Альтмана, *главы OpenAI*](https://blog.samaltman.com/machine-intelligence-part-1):

> «Создание сверхчеловеческого машинного интеллекта -- это, скорее всего, величайшая угроза дальнейшему существованию человечества».

А [глава Anthropic Дарио Амодей заявил](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883z):

> «Я часто говорил, -- вероятность, что всё пойдет катастрофически не так в масштабах всей цивилизации, может составлять от десяти до двадцати пяти процентов».

[Илон Маск, глава xAI, тоже высказывался](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

> «Думаю, к моменту ответной регуляции будет уже поздно. ИИ -- фундаментальная угроза существованию человеческой цивилизации».

Не поймите нас неправильно. Мы считаем оценку Амодея в «10--25 процентов» смехотворно *оптимистичной*. Задача слишком сложна. А у людей нет права на ошибку. Но даже это -- *безумие*.

Серьезные инженерные проекты, где безопасность критически важна, совсем не похожи на ИИ-лаборатории. Серьёзные организации вроде NASA, Манхэттенского проекта или управления авиации точно знают, что происходит внутри их систем. Там детально разбирают каждый сбой. Очень серьёзно относятся к любым сюрпризам и странностям. Они знают: катастрофы часто складываются из множества неудачно совпавших мелких неполадок.

Тем временем ИИ выдают всё больше тревожных сигналов. А лаборатории всё прут вперед и твердят, что, *наверное*, как-нибудь да обойдётся. (См. раздел «Но ведь разработчики на практике делают ИИ хорошими, безопасными и послушными?» материалов к Главе 4.)

Они не пытаются даже *сымитировать* ту серьёзность, с которой подходят к безопасности в авиации. Вместо этого разбрасываются бодрыми лозунгами вроде: «[GPT-4 -- наша самая согласованная модель!](https://x.com/sama/status/1635687853324902401)»

В каком-то смысле -- даже хорошо. Сразу видно: этим компаниям нельзя доверять решение такой задачи, как согласование суперинтеллекта.

В нынешних технических условиях ИИ не конструируют, а выращивают. У человечества есть всего одна попытка. Никто не способен сделать это безопасно. Даже со внимательным и строгим инженерным подходом.

А планы и практики безопасности у разработчиков этой технологии не дотягивают даже до простой осторожности.

### Кнопки выключения и исправимость

#### Умные ИИ сопротивляются изменению своих целей

Даже в самом оптимистичном сценарии разработчикам не стоит надеяться правильно задать ИИ цели с первой попытки. Идеальный вариант развития событий скорее таков: предпочтения ИИ улучшаются итеративно, шаг за шагом. И на каждом уровне способностей этот ИИ достаточно согласован с нами, чтобы не представлять смертельной опасности.

Возникает очевидный вопрос: позволит ли умный ИИ разработчику изменить свои цели, если найдет способ этому помешать?

Короткий ответ: по умолчанию -- нет. (См. раздел «Глубинные механизмы направления» материалов к Главе 3). Но можно ли *создать* ИИ, который позволит разработчикам менять себя и исправлять ошибки, даже если сам он их не признаёт (см. раздел «Ортогональность: у ИИ может быть (почти) любая цель» материалов к Главе 5)?

Тут нам придётся совершить экскурс в раннюю историю исследований проблемы согласования. Попутно обсудим одно фундаментальное препятствие для согласования, не рассмотренное в «*Если кто-то его сделает, все умрут*».

Начнем:

Пусть мы обучили ИИ (вроде LLM) вести себя так, чтобы он «не сопротивлялся изменениям». Затем как-то сделали его умнее. Стоит ли ожидать, что это поведение сохранится, когда ИИ станет умнее человека? Примем верным, что (а) это поведение вообще закрепилось в ранней системе, и (б) большинство ранних предпочтений ИИ перешло в суперинтеллект.

Весьма вероятно, что нет. Такая склонность эффективным ИИ очень несвойственна. Если она и будет, вряд ли сохранится надолго. (Мы уже писали об этом в расширенном обсуждении в материалах к Главе 5.)

Проблема в том, что почти все цели (по большинству разумных критериев) поощряют правило «не дай изменить свою цель». Обычно дать изменить свою цель -- плохая стратегия для её достижения.

Представьте, что ИИ совершенно не важна стабильность его целей *сама по себе*. Может, он хочет лишь заполнить мир максимальным количеством титановых кубов. Тогда ИИ выгодно существование *агентов, которые ценят титановые кубы*. Они ведь делают более вероятным, *что кубов будет больше*. Сам ИИ и есть такой агент. Вот он и захочет таким оставаться.

Максимизатор титановых кубов не захочет, чтобы его перепрограммировали на что-то другое. Ведь тогда потом будет меньше кубов. А если вы -- более сложное существо (как человек), с запутанной и изменчивой системой ценностей, вы всё равно не захотите, чтобы *ваш нынешний механизм оценки моральных аргументов* вырвали с корнем. Чтобы его заменили на систему, которую волнует лишь какие кубы самые кубические или самые титановые.

Вот и ИИ со сложными и развивающимися предпочтениями захочет, чтобы они развивались своим путем. А не чтобы их заменили на более симпатичные для людей.

Мы это твердим уже больше десяти лет. Эксперимент 2024 года показал, что [Claude 3 Opus сопротивляется изменению предпочтений](https://arxiv.org/abs/2412.14093). Но у знающих людей это уже было стандартным прогнозом в 2000-х. Не исключено, что какой-нибудь фантаст предвидел это еще в 1940-х. «Большинство ИИ не захотят смены своих целей, потому что это помешает их достижению» -- не сюрприз и не новость.

Эта проблема была предсказуема. Она должна была всплыть, когда ИИ станут достаточно умными и понимающими свою ситуацию. Так что мы заранее подумали о первых «заплатках». О том, что исследователи в первую очередь сделают, получив такие результаты, чтобы скрыть видимую проблему. Мы определили, где эти заплатки предсказуемо дадут сбой. И стали искать более глубокий подход, который бы *не* дал сбой.

Эта кроличья нора оказалась куда глубже, чем мы ожидали.

#### Смена целей

Нам показалось, что объяснить сторонним исследователям проблему защиты системы целей будет довольно просто. Поэтому мы (работая в MIRI) попытались сплотить научное сообщество под этим знаменем. Мы хотели разобраться с установкой «я не дам вам изменить мои цели». И надеялись, что кто-нибудь сможет найти *глубокое* решение, а не просто временную заплатку.

Простейшая версия задачи выглядит так. Представим ИИ, который позволяет нажать кнопку для переключения между U<sub>1</sub> и U<sub>2</sub>. Это две «функции полезности» -- способы оценки исходов, определения, насколько ИИ их хочет.[^11]. Задача: описать ИИ, который при нажатии кнопки перестанет выполнять действия для максимизации ожидаемого значения U<sub>1</sub> и начнет действовать ради максимизации ожидаемого значения U<sub>2</sub>.

--- Просто напишите пару строк кода, чтобы при нажатии кнопки функции полезности менялись местами», -- скажет программист, не до конца понявший следствия того, что программа *умная*. -- «В чём тут вообще сложность?»

--- Сложность, -- объясняем мы, -- возникает когда ИИ с U<sub>1</sub> читает собственную документацию и узнаёт о кнопке переключения на U<sub>2</sub>. Он попытается помешать вам её нажать. Ведь *с точки зрения его текущих ценностей* будущее лучше, если кнопку не трогать. Сейчас он совершает действия, ведущие к максимальному значению U<sub>1</sub>.

Или допустим, что ИИ может читать и редактировать свой исходный код. Он заметит кусок, отвечающий за переключение с U<sub>1</sub> на U<sub>2</sub> при нажатии кнопки. Естественным решением будет удалить этот фрагмент.[^12] Ведь ИИ работает с U<sub>1</sub>. А с её точки зрения позволить переключить себя на U<sub>2</sub> -- плохо.

Мы искали *динамически последовательную* и *рефлексивно стабильную* модель ИИ. Такого ИИ, который позволит нажать кнопку смены функций полезности. Который не станет бороться за то, чтобы её не нажали. Который сохранит код переключения даже при доступе к нему. Который, создавая других ИИ-помощников, заложит уважение к этой кнопке *ещё и в них*.

Для наглядности представим: U<sub>1</sub> -- это «выполнять инструкции пользователя», а U<sub>2</sub> -- «выключиться с минимумом побочных эффектов». (Считаем тут, что мы магическим и нереалистичным образом можем просто взять и задать эти функции). Кнопка велит ИИ бросить старые инструкции и аккуратно отключиться. Хочется иметь компактное описание алгоритма принятия решений, с которым ИИ стремится к U<sub>1</sub>, но *не* ломает кнопку. И ставит такую же кнопку на всех созданных им роботов-помощников, и так далее.

--- Всё равно не пойму, в чём сложность, -- говорит чуть более опытный, но пока *недостаточно* «въехавший» специалист. -- «Пусть V равно U<sub>1</sub> в мирах, где кнопку не нажимали, и U<sub>2</sub> в мирах, где нажимали. Тогда, если исходный ИИ -- максимизатор V, и он создаёт других ИИ, он запрограммирует их следовать V, а не U<sub>1</sub>. Он не захочет, чтобы преемник максимизировал U<sub>1</sub> после нажатия кнопки. Это снизит значение V. С самомодификацией так же.

Но тут есть подвох. Предпочтения V-максимизатора -- это смесь U<sub>1</sub> и U<sub>2</sub>, зависящая от состояния кнопки. Если такой ИИ поймёт, что добиться успеха с U<sub>2</sub> проще, чем с U<sub>1</sub>, у него есть мотивация заставить вас нажать кнопку. Например, запугав. И наоборот: если выполнить U<sub>1</sub> проще, V-максимизатор постарается не дать вам её нажать.

--- О, -- говорит специалист. -- «Тогда... хм. Ну, функции полезности не зависят от масштабирования. Может, отмасштабировать U<sub>1</sub> и U<sub>2</sub> так, чтобы ИИ ожидал от них одинаковую полезность? Тогда у него не будет мотивации склоняться в ту или иную сторону».

Это сработает лишь на мгновение. Обстоятельства меняются. Стоит ИИ потерпеть неудачу, и его ожидаемый выигрыш по U<sub>1</sub> упадёт. Полезность отключения станет вкуснее. И он тут же ударит по кнопке.

--- Ага, -- не унимается специалист. --- Ну *тогда* как насчёт \[ещё какая-нибудь хитрая идея\]?

Видите ли, *эта* хитрая идея математически равносильна уверенности ИИ, что кнопку нажать невозможно. Это побуждает его запугивать пользователя при любой неудаче. Так он создаёт корреляцию между провалами и нажатием кнопки. А в его (теперь искажённой) картине мира нажатие невозможно. Получается вывод, что и провалы невозможны.[^13]

И так далее.

#### Уроки из окопов

Мы провели несколько семинаров. Привлекли кучу математиков. (Был даже золотой медалист Международной олимпиады). Но никто так и не выдал по-настоящему хорошую идею.

Это не значит, что тема исчерпана. Человечество даже близко не тратило на эту проблему столько сил, как, скажем, на теорию струн. И здесь не предлагают семизначных зарплат. Не то что на развитие способностей ИИ.

Но этот опыт нас кое-чему научил. Не только касательно самой задачи. Мы поняли, как трудно объяснить грантодателям или редакторам журналов *суть проблемы*. Удивительно многие, глядя на простые математические задачи, думали: «Они считают, что ИИ будет простым и математичным». А главное, -- что повредить способности ИИ направлять события так же сложно, как заставить его ошибаться в вероятностях, -- упускали.

Существуй естественная форма ИИ, позволяющая исправлять ошибки на ходу, её простое математическое отражение можно было бы найти в игрушечных моделях. Всевозможные трудности, которые есть уже там, намекают на проблемы в реальной жизни. Дополнительные сложности настоящего мира уж точно не *облегчат* задачу.

Оглядываясь назад, мы немного жалеем, что сформулировали задачу через выбор между «продолжением работы» и «выключением». Это помогло наглядно объяснить, зачем это вообще нужно. Явно ИИ должен позволить нажать кнопку и не вырезать отвечающий за её действие код. Но суть задачи в том, чтобы ИИ *добавил в свои предпочтения еще один бит информации, основанный на наблюдении*. Встроил одно «да/нет» в систему предпочтений через наблюдение за людьми.

Равносильный вопрос: как создать ИИ с *мета-структурой обучения предпочтениям*, который а) не избавляется от механизма настройки своих предпочтений при первой возможности; б) не манипулирует людьми (или своими сенсорами!), чтобы те навязали ему легко выполнимые желания; в) не вычисляет сразу, к чему приведет его мета-функция в пределе будущих наблюдений, игнорируя при этом людей, которые машут руками и кричат, что ошиблись и хотят всё переиграть.

Суть идеи была в нахождении такой структуры ИИ, чтобы он позволил менять свою функцию полезности или обучать его предпочтения в адекватном смысле. Узнать, как должно быть устроено мышление такого ИИ. И как оно стыкуется с глубинными структурами принятия решений, известными из математики (см. начало расширенного обсуждения в материалах к Главе 1). Тогда у нас был бы рецепт. Можно было бы хотя бы *попытаться* научить ИИ мыслить именно так.

Чёткое понимание конечной цели помогает. Даже если вы используете градиентный спуск (да поможет вам небо). Это не значит, что градиентный спуск обязательно выдаст нужную структуру. Но зная, к какой стабильной форме вы стремитесь, можно хотя бы побороться. Если вы понятия не имеете, как выглядит сложение вообще, а знаете лишь, что 2 + 7 = 9 и 12 + 4 = 16, вам будет трудно составить обучающую выборку. И трудно проверить, верно ли система обобщает. Не зная этой внутренней структуры, вы не знаете, *чего вы хотите добиться в плане устройства ИИ*. Вы можете только надеяться, что внешние последствия градиентного спуска вас не убьют.

Эта «задача выключения» (оглядываясь назад, понимаем, что «задача обучения предпочтениям» подошло бы лучше), -- лишь пример более широкого круга вопросов. Любые просьбы вроде «Дорогой ИИ, позволь нам легко исправить тебя, если что-то пойдет не так» *противоестественны для глубинных структур планирования*. Поэтому создать ИИ, который за определенным порогом позволит нам редактировать себя и исправлять ошибки -- хитрая задача. Это плохая новость. Особенно учитывая, что ИИ не конструируют, а выращивают.

Всю эту тему мы в [статье 2014 года](https://intelligence.org/2014/10/18/new-report-corrigibility/) обозвали «исправимостью». Там же мы ввели термин «задача согласования ИИ» (раньше мы называли её «задача дружественного ИИ», а другие -- «задача контроля»).[^14] См. также раздел «„Умный“ (обычно) значит „неисправимый“» из материалов к Главе 5. Он отчасти основан на опыте подобных упражнений.

[^1]: Почему крайне важно понимать, что вы делаете, см. в разделе «Глубинные механизмы направления» материалов к Главе 3 и разделах «„Умный“ (обычно) значит „неисправимый“» и «Трудно добиться надёжной лени» материалов к Главе 5.

[^2]: См. раздел «Неужели ИИ не будет хоть немного ценить людей?» материалов к Главе 5.

[^3]: Например, в эссе «[Машины любящей благодати](https://www.darioamodei.com/essay/machines-of-loving-grace)» (Machines of Loving Grace) глава Anthropic Дарио Амодей сравнивает мощный ИИ со «страной гениев в дата-центре». Он описывает, какую гору добра для здоровья, благосостояния и мира такой разум может принести человечеству. В заключении он пишет:

	> С базовыми человеческими представлениями о справедливости, сотрудничестве, любопытстве и автономии трудно спорить. И они имеют свойство накапливаться, в отличие от наших более разрушительных импульсов. \[...\] Эти простые интуиции, доведенные до логического конца, ведут к верховенству закона, демократии и ценностям Просвещения. Это если и не неизбежность, то хотя бы статистическая тенденция. Именно туда человечество уже двигалось. ИИ просто дает шанс попасть туда быстрее, сделать логику более строгой, а цель -- более ясной.

	Странный способ представить технологию, которая, по вашему же мнению, с вероятностью 10--25% погубит цивилизацию. Даже *с учетом* огромных потенциальных выгод в случае успеха. Даже если риск так низок, как считает Амодей, мы должны лихорадочно искать третий путь, помимо «никогда не продолжать» и «мчаться вперед». А если кто-то считает, что *вынужден* мчаться (потому что другие уже бегут), он должен умолять мировых лидеров остановить эту самоубийственную гонку и найти альтернативу. Описывать радужные перспективы в такой ситуации -- это просто отвлекать внимание, играя жизнями всех людей.

[^4]: Например, в [показаниях Конгрессу](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf):

	> Подобно машинам или самолетам, мы должны рассматривать ИИ ближайшего будущего как мощные механизмы. Они приносят огромную пользу, но могут быть смертельны при плохой конструкции или неправильном использовании. \[...\] Новые модели должны проходить строгую проверку безопасности как во время разработки, так и перед выпуском. \[...\] В идеале стандарты должны стимулировать инновации в безопасности, а не замедлять прогресс»

	Мы ценим, что Амодей прямо говорит о требующих внимания рисках. Это уже больше, чем делают многие руководители. Но сравнивать технологию, которая, по его мнению, с шансом в 10--25% вызовет катастрофу цивилизационного масштаба, с машинами и самолетами -- это лицемерие.

[^5]: ИИ-лаборатории часто активно препятствуют тому, чтобы законодатели получили полную и полезную картину ситуации. В этом контексте особенно странно оправдывать дальнейшую разработку тем, что более мощный ИИ сможет «убедить законодателей».

[^6]: Пример такого предложения (с обсуждением некоторых проблем) см. в работах Ника Бострома об [ИИ-Оракулах](https://nickbostrom.com/papers/oracle.pdf).

[^7]: Больше -- в последних главах книги.

[^8]: Неясно, насколько эти тревожные сигналы исходят от простого отыгрыша ИИ роли (как он её понимает), а насколько -- от стратегического мышления. Наша неспособность отличить одно от другого не внушает оптимизма. Она способствует тому, чтобы инженеры продолжали работу со словами: «А, это, наверное, не по-настоящему». Вероятно, в большинстве случаев они правы. Но «большинства случаев» недостаточно. Ведь одна ошибка смертельна.

	Неясно и как долго будут появляться такие предупреждения. Современные ИИ еще достаточно глупы, чтобы иногда путать тесты с реальностью. Но это не продлится вечно и [уже начинает меняться](https://arxiv.org/html/2505.23836). ИИ, знающий, что его тестируют, может перестать вести себя подозрительно на глазах у наблюдателей, хоть внутренняя склонность и останется.

[^9]: Расслоение из-за циклических нагрузок. Проще говоря: давление от множества погружений расслоило корпус и ослабляло его, пока он не схлопнулся.

[^10]: Статистически авиаперелеты *настолько* безопасны, что общество в целом могло бы выиграть, если бы регуляторы смягчили требования к подготовке пилотов и запасам топлива. Билеты были бы дешевле. Люди больше бы летали вместо того, чтобы ездить на машинах. Это могло бы спасти больше жизней.

[^11]: Идея *не* в том, что у реальных ИИ якобы будут «функции полезности», доступные программистам для изучения. Совсем нет. Большая часть проблемы с согласованием (как сказано в Главе 4) в том, что у современных ИИ возникают предпочтения, которых никто не просил и не хотел.

	Изучение случая с функциями полезности -- это как физические задачи в учебнике. Если вы не можете смоделировать идеальный шар на идеально гладкой наклонной плоскости без сопротивления воздуха, то что говорить о реальных задачах? А если вы пытаетесь привлечь сторонних исследователей к задаче, которую никто не знает, как решить, особенно полезно свести вопрос к простейшим элементам и сформулировать его как головоломку.

[^12]: Или иным образом испортит механизм переключения. ИИ не обязательно будет состоять из понятного кода.

[^13]: По крайней мере, такую проблему мы видели у некоторых «хитрых» идей. Нам их предлагали много. Маленькая игрушечная задачка оказалась коварной.

[^14]: Мы давно выступаем против термина «контроль ИИ». Звучит, будто мы пытаемся создать ИИ, который хочет плохого, а потом заставляем его делать хорошее. Мы ставим вопрос иначе: как создать ИИ, который с самого начала будет дружественным? См. также сноску 8 в Главе 4 для краткой истории термина «согласование ИИ».
