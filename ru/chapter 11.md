# Глава 11: Алхимия, а не наука

Это онлайн-дополнение к 11-й главе «_Если кто-то его сделает, все умрут_». В ней обсуждались подходы современных ИИ-лаборатори1 к согласованию сильного искусственного интеллекта. Ответы на эти вопросы -- в книге:

- Как оценить нынешнюю готовность ИИ-компаний к решению проблемы согласования СИИ?
- Какое место занимают исследования интерпретируемости -- попытки читать и понимать мысли ИИ?
- А нельзя просто поручить ИИ решить эту задачу за нас?

Ниже мы разбираем несколько идей по согласованию и внедрению ИИ. Ещё обсудим поводы для оптимизма. И аргументы, которые приводят в пользу развития передового ИИ несмотря на общую мрачную картину.

## Часто задаваемые вопросы

### Да мы как-нибудь выкарабкаемся, как обычно. Почему нет?

#### Мир обычно «выкарабкивается» методом проб и ошибок. А тут ранние ошибки не оставят никого в живых.

См. Главу 10 и расширенное обсуждение в материалах к ней.

### Вы считаете, что согласование -- это «всё или ничего»?

#### Нет. Но «частичное согласование» скорее всего тоже приведёт к катастрофе.

Один из доводов против беспокойства о суперинтеллекте звучит примерно так: «Развитие ИИ, вероятно, будет постепенным. На каждом шаге мы сможем методом проб и ошибок научиться держать его в узде. Согласованию необязательно быть _идеальным_, чтобы всё закончилось хорошо». (См. также раздел «А что, если разрабатывать ИИ неспешно и так же плавно внедрять его в общество?» дополнительных материалов к Главе 10.) Мы не возлагаем на этот сценарий особых надежд. Вот несколько причин:

- Наши опасения не зависят от скорости прогресса. Мы не можем точно сказать, выйдет ли ИИ на плато на пути к суперинтеллекту. Это сложный вопрос. Наша лучшая догадка: у машинного интеллекта есть пороговые эффекты. Но это лишь предположение. Наши аргументы не строятся на нём. История Sable во второй части книги намеренно описывает катастрофу из-за ИИ, который не так уж сильно превосходит человеческий уровень. Отчасти для того, чтобы показать: враждебному ИИ не обязательно быстро становиться суперинтеллектом. Он может стать чрезвычайно опасным и без этого.

- На вопрос «А что, если нам повезёт и будет куча времени на проверку идей согласования на достаточно слабых ИИ?» мы отвечаем в главе 10 и материалах («До» и «После») к ней. Исследователи могут много чего выяснить о таких системах. Но ИИ, которые безопасно изучать, будут неизбежно критически отличаться от первых моделей, достаточно мощных для прохождения точки невозврата. Даже зрелой науке было бы очень сложно учесть все эти нюансы. А для области всё ещё на стадии алхимии, области, что работает с непостижимыми ИИ (которые выращивают, а не конструируют) -- вообще без шансов.

- Согласование ИИ не обязано быть идеальным для отличных долгосрочных результатов. В принципе, можно аккуратно создать ИИ с некоторой толерантностью к ошибкам. Если знать, что делаешь.[^192] Но это не значит, что «частично» или даже «в основном» согласованные системы приведут к нормальному исходу. ИИ по очень многим причинам может сейчас или в ближайшем будущем вести себя хорошо в 95% случаев безо всяких гарантий счастливого исхода для человечества. Мы уже обсуждали эти причины с разных сторон в онлайн-материалах к Главе 5.

Поясним последний пункт:

В качестве мысленного эксперимента представьте: человечеству удалось загрузить в предпочтения суперинтеллекта _почти_ все разнообразные человеческие ценности. Кроме, по какой-то причине, тяги к новизне. Тогда суперинтеллект направит будущее в застойное и скучное русло. Там один и тот же «лучший» день будет повторяться бесконечно.

Заметьте, мы не считаем это _правдоподобным_. Такой уровень согласования совершенно недостижим для нынешних стандартных подходов. Мы смогли заложить в ИИ почти все наши ценности, но не одну последнюю -- очень странная ситуация.[^193] Но это важная иллюстрация. Существа, разделяющие _некоторые_ наши желания, но без хотя бы одного ключевого, став достаточно технологически подкованными, чтобы получить именно то, что хотят, без оглядки на людей, скорее всего, приведут к катастрофе.

Более реалистичный сценарий: ИИ окажется «частично» согласованным в том смысле, что у него (как у нас) инструментальные стратегии переплетутся с терминальными предпочтениями. (См. «Рефлексия и самомодификации всё усложняют» в материалах к Главе 4.) Скажем, у него появится стремление, похожее на любопытство. И другое, похожее на тягу к охране природы. Кто-то посмотрит на это и скажет: «Гляньте! У модели развиваются очень человечные побуждения». Такой ИИ можно в некотором смысле назвать «частично» согласованным.

Но когда этот ИИ дорастёт до суперинтеллекта, ничего хорошего скорее всего не получится. Может, он потратит кучу ресурсов, бессознательно преследуя свою странную версию любопытства. А человечество сохранит в отредактированном, более удовлетворительном для него, виде. (Как многие защитники природы убрали бы из неё малярийных комаров и причиняющих мучения паразитов, имей они такую возможность). Тут опять уместно: процветающие люди -- не самое эффективное решение подавляющего большинства задач. (См. раздел «Посчитает ли ИИ полезным нас оставить?» материалов к Главе 5)

Или другой вариант: ценности ИИ приводят к очень гуманному поведению _в обучающей среде_. Люди радуются, он точно выглядит «частично согласованным». (Происходит уже сейчас. Это иллюзия. См. «А разве Claude не подаёт признаков согласованности?» в материалах к Главе 4). Но это мало говорит о том, как ИИ поведёт себя, получив куда более широкий простор для действий. Чтобы люди процветали _тогда_, благополучие именно человечества должно быть частью _наиболее предпочтительного для ИИ исхода_.

Если мы частично согласуем ИИ, это не значит, что ценности человечества будут частично представлены в будущем. Частичная загрузка человеческих ценностей в предпочтения ИИ умнее человека -- не то же самое, что их полная загрузка с низким «весом» (и тогда, когда другие приоритеты насытятся, они выйдут на передний план).

Чтобы ИИ дал нам _хоть что-то_, он должен заботиться о нас именно так, как надо, хотя бы чуть-чуть. И большинство способов «слегка промахнуться» не такие. См. «Неужели ИИ не будет хоть немного ценить людей?» в материалах к Главе 5.

### А всё не станет получше, когда вмешается правительство?

#### Смотря как и как скоро вмешается.

В Вашингтоне мы часто встречаем политиков, считающих, что компании держат свои ИИ под контролем. А люди из индустрии твердят, что проблему решит регулирование. Особенно вопиющий пример -- слова гендиректора Google. Он [заявил](https://youtu.be/9V6tWC4CdFQ?feature=shared&t=2685), что «базовый риск \[гибели человечества\] на самом деле довольно высок». Но тут же добавил: чем выше ставки, тем скорее люди объединятся, чтобы предотвратить беду.

То есть он мчится создавать технологию, угрожающую, по его же мнению, всем на Земле. И надеется, что люди «сплотятся» и разберутся с рисками. Которые он сам же и создаёт. Но отложим это пока в сторону. Заметьте другое: технарь воображает, будто проблему решит кто-то другой.

А политики, кажется, считают, что решение найдут технари. [Это](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731) [сквозит](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/) [в](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai) [каждом](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation) [их](https://statemag.state.gov/2025/04/0425itn07/) [призыве](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation) [выиграть](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581) [гонку](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/). Но пока не решены технические проблемы у гонки победителя не будет. Впрочем, может, всё не так плохо. Не исключено, что чиновники думают не о гонке за суперинтеллектом, а о соревновании чат-ботов. В июне 2025 года наш знакомый советник по ИИ-политике описывал, что Конгресс в целом [не верит компаниям, когда те прямо заявляют о работе над суперинтеллектом](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) (хотя с некоторыми важными исключениями).

Кажется, почти все власть имущие полагают, что проблему решит кто-то другой.

Подробнее о реакции мира (и о том, что лидеры часто не успевают отреагировать до катастрофы) см. Главу 12. К августу 2025 года правительства так и не выдали ничего похожего на серьёзный ответ. И всегда есть риск, что чиновники вообще не поймут суть угрозы. Например, сочтут ИИ обычной технологией, которую нельзя душить лишними запретами.

О том, какие меры властей реально могут предотвратить катастрофу, читайте в Главе 13 и разделе «Почему бы международной коалиции не разработать безопасный ИИ совместно, а не запрещать?» материалов к Главе 12.

### А самые безрассудные компании не оказываются одновременно самыми некомпетентными и потому неопасными?

#### Вообще говоря, нет. Пренебрежение правилами часто даёт преимущество.

Компания Volkswagen в 2008–2015 годах дерзко и успешно [жульничала с проверками выбросов](https://www.bbc.com/news/business-34324772). Катастрофы Boeing 737 MAX в 2018–2019 годах [унесли жизни 346 человек](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). Причиной стали сбои в системе управления. Руководство знало о них, но преуменьшало их важность. И автопром, и авиастроение -- области жесткой конкуренции. И Volkswagen, и Boeing были и _остаются_ гигантами.

Конкурентоспособность любителей срезать углы нас не удивляет. В обоих случаях они стремились вывести мощные продукты на рынок быстрее и дешевле конкурентов. Да, они выплатили огромные штрафы и понесли репутационные потери. Но совсем не очевидно, что их корпоративная культура, поощряющая хитрое пренебрежение правилами, в целом делает их слабее, хоть они иногда и попадаются.

Думаете, ведущие ИИ-компании — исключение? Взгляните на [заголовок](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) от июля 2025 года:

[изображение]
[Заголовок: «Grok выпускает порнографического аниме-компаньона и получает контракт от Минобороны». Подзаголовок: «Тем временем самая продвинутая версия чат-бота от xAI Илона Маска всё ещё идентифицирует себя как Адольф Гитлер».]

Не думаем, что современные методы позволяют создать суперинтеллект без катастрофы. Но даже если бы позволяли, при нынешнем уровне компетентности и серьезности, какая-нибудь ИИ-компания непременно облажается и погубит нас всех.

#### Сравнительно осторожные компании всё равно безрассудны.

Многие считают Anthropic лидером «безопасности ИИ». Например, они первые стали [брать на себя добровольные обязательства](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Но и они [меняют эти обязательства в последний момент, когда понимают, что не могут их соблюсти](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling). А их «планы» туманны и плохо продуманы (Cм. критику в Главе 11 и обсуждении ниже).

Anthropic выезжает на сравнении с другими компаниями. В нормальной отрасли компанию, ставящую под угрозу миллиарды жизней (в чём [признавался и её гендиректор](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883)) не хвалили бы за сдержанность. Особенно если она регулярно принижает опасность своих действий перед публикой[^194] и законодателями[195].

Пренебрежение правилами в ИИ -- обычное дело. Как и везде, где есть конкуренция. Безрассудство -- норма. И _менее_ безрассудные компании явно не на пути к успеху.

### А разве не нужно спешить из-за «опережения по железу» («hardware overhang»)?

#### Это самоубийство. Мы слишком далеки от решения задачи согласования.

В последние лет десять некоторые люди, обеспокоенные угрозами ИИ, предлагали развивать его как можно быстрее. Идея вот в чём: пусть самые умные модели требуют для работы почти всё мировое «железо». Тогда никакой одиночный прорыв не выпустит тысячи мощных ИИ, думающих в тысячи раз быстрее человека, _внезапно_.

Пока человечество тратит львиную долю мощностей на запуск умнейших ИИ, перемены будут идти постепенно. У людей будет время приспособиться. Без «опережения по железу» не будет момента, когда возможности ИИ резко скакнут вперёд, потому что мир резко направит больше оборудования на запуск новых моделей. Ну, таков был аргумент.

Нам этот довод кажется весьма слабым. Одна из проблем: интеллект, вероятно, подвержен пороговым эффектам.

Переход от уровня шимпанзе к уровню человека не был «скачкообразным». Всё шло плавно. Но по эволюционным меркам -- очень быстро. А переход от доиндустриальной цивилизации к постиндустриальной -- ещё стремительнее. Недостаточно плавно, чтобы другие животные успели хоть как-то приспособиться.

Например, ИИ, потребляющий огромную долю мировых мощностей, может оказаться достаточно умным для создания новых алгоритмов и чипов. И вскоре после этого появятся тысячи систем умнее и в тысячи раз быстрее человека. (Помните: современный дата-центр тратит энергию как [небольшой город](https://epoch.ai/blog/power-demands-of-frontier-ai-training). А человек -- как [мощная лампочка](https://en.wikipedia.org/wiki/Human_power). У эффективности ИИ огромный простор для роста).

А если узкое место -- мощности для _обучения_, а не _запуска_, у готового ИИ будет огромный избыток железа. И можно будет запустить сразу много и на большой скорости.

Даже если пороговых эффектов нет, стратегия сомнительная. Вряд ли стоит как можно быстрее закидывать человечество всё более умными моделями. Даже не способными нас убить. Это плохой путь привить людям инженерную дисциплину, необходимую для создания надежно дружелюбных ИИ.

Проблема в том, что ИИ выращивают, а не конструируют. А мы и близко не подошли к пониманию, как вырастить модель, надёжно стремящуюся к _чему угодно_ конкретному, что хотят её создатели.

Эту задачу не решить, штампуя новые ИИ при первой же возможности. Это бессмыслица. См. [старую работу Соареса о том, что согласование требует последовательной работы](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

Тем не менее, эту бессмыслицу подхватил гендиректор OpenAI Сэм Альтман.[В 2023 году он оправдывал ею бешеную спешку своей компании](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut).

Оправдание оказалось пустышкой, когда тот же Альтман [кинулся наращивать вычислительные мощности в огромных масштабах](https://openai.com/index/announcing-the-stargate-project/).

Это наглядный пример того, как боссы ИИ-компаний хватаются за любой довод, оправдывающий спешку. Большинство таких аргументов не выдерживают критики. Мы не советуем доверять им лишь потому, что они исходят от руководителя корпорации. (См. также раздел «Рабочие планы будут подразумевать отказы ИИ-компаниям» материалов к Главе 12.)

### А нам не нужно мчаться вперёд, чтобы можно было исследовать согласование?

#### Мы выступаем против всей нынешней парадигмы ИИ.

Современные методы приводят к неоправданно сложным проблемам для согласования. Мы обсуждали их в прошлых главах. Нет причин, почему создать согласованный суперинтеллект было бы принципиально невозможно. Но нужно достаточно глубокое понимание, что мы делаем, и подходящий набор формальных инструментов. А вся нынешняя парадигма в плане согласования и надежности кажется тупиковой. Хоть и отлично подходит для наращивания способностей ИИ.

Мы не агитируем за «старый добрый» ИИ, как с 50-х по 90-е. Те методы были ошибочны. [Вполне ясно](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design), почему они провалились. Но кроме весьма поверхностных попыток из восьмидесятых и выращивания ИИ без понимания его устройства есть и другие варианты.

#### Доступной важной работы и так много.

Sydney Bing [газлайтила](https://x.com/MovingToTheSun/status/1625156575202537474) пользователей и [угрожала](https://x.com/sethlazar/status/1626257535178280960) им. Мы до сих пор точно не знаем, почему. Не знаем, что творилось у неё в голове. То же касается случаев (в реальных условиях) [излишнего подхалимства](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health) и намеренных, судя по всему, попыток свести людей с ума (см. раздел «ИИ-психоз» в материалах к Главе 4). И [жульничества и попыток его скрыть](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf). И [упорного объявления себя Гитлером](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). А ещё экспериментальных случаев, где модели [имитируют согласованность](https://arxiv.org/abs/2412.14093), [шантажируют](https://www.anthropic.com/research/agentic-misalignment), [сопротивляются отключению](https://palisaderesearch.org/blog/shutdown-resistance) или [пытаются убить операторов](https://www.anthropic.com/research/agentic-misalignment).

Мы не знаем, какие из этих случаев вызваны чем-то опасным. Ведь никто так и не понимает, что происходит внутри ИИ, почему всё это случилось. Подумайте, сколько всего можно узнать о современных LLM и работе интеллекта в целом, изучая существующие модели, пока мы не станем _понимать_ все эти тревожные звоночки.

В 2015-м году тезис «нельзя решить проблему согласования без изучения ИИ» был поосмысленнее. Мы слышали его от людей, которым нужен был предлог для запуска ИИ-компаний вопреки доводам, что они играют нашими жизнями. Мы и тогда возражали. Говорили, что работы и так полно. И что парадигма на основе градиентного спуска не внушает надежд (в плане создания дружественного суперинтеллекта). Но сейчас этот аргумент _ну совсем_ потерял смысл. _Уже_ есть тьма вещей, которые мы не понимаем и можем изучать.

Руководители корпораций, которые _действительно_ создавали ИИ только чтобы согласование можно было изучать на практике, а не в теории: Вы это сделали! У вас получилось. Работы исследователям хватит на десятилетия. Мы не считаем, что продвижение крайне опасной парадигмы того стоило. Но в любом случае, теперь точно есть что изучать. Можете остановиться.

А как насчет тех, кто продолжает давить на газ, несмотря на все предупреждения? Очевидный вывод: они никогда и не создавали ИИ только ради решения проблемы согласования. Что бы они ни говорили для успокоения, оправдывая своё безрассудство в 2010-х.

### А что, если компании будут использовать ИИ только для безопасных задач?

#### Даже вроде бы безобидные действия могут требовать опасных способностей.

Мы встречали такие предложения: ИИ-компании продолжат развивать способности моделей, но пообещают использовать их лишь так, чтобы это не выглядело явно опасным. Например,в разговорах с ветеранами индустрии (годы назад) всплывала такая мысль: мощный ИИ, владеющий риторикой, мог бы убедить политиков всего мира запретить разработку опасного ИИ. 

Мол, для этого ИИ нужно будет лишь разговаривать. Не придется управлять роботами. Не нужен доступ к биолаборатории, где можно создать супервирус.

Во-первых, эта идея нам претит этически. ИИ с достаточно сверхчеловеческим даром убеждения, вероятно, уговорит кого угодно на что угодно. Использовать его, чтобы навязать другим _свои_ взгляды, -- неправильно. Вряд ли нужны такие крайности. Ведь люди-специалисты уже сегодня могут и должны куда больше делиться опасениями и аргументами и предупреждать мировых лидеров о колоссальной опасности суперинтеллекта.[^196]

Разработчики могут годами выращивать ради этой цели всё более опасные системы. А могут пойти к законодателям и _сами_ честно с ними поговорить. Чтобы проинформировать, а не манипулировать. Нас часто приятно удивляло, насколько восприимчивы люди в Вашингтоне, если говорить с ними начистоту.

Но мы отвлеклись. Главное: если создан очень мощный ИИ, умеющий «просто разговаривать», всё уже пойдёт не так. Помимо этики, тут техническая проблема. Для сверхчеловеческой убедительности, ИИ, вероятно, должен детально моделировать людей и искусно ими манипулировать.

Люди разумны. Стали бы _вы_ беседовать с ИИ, о котором известно, что он уболтает кого угодно на что угодно, неважно, правда это или нет? Если один мировой лидер поговорит с таким ИИ и полностью изменит свои взгляды, кто захочет стать следующим? _Мы_ бы не стали добровольно с ним общаться. В частности потому, что не хотим менять свои ценности (см. обсуждение неисправимости в материалах к Главе 5).

ИИ, способный добиться успеха даже в таких условиях, должен уметь просчитывать реакции людей на свои слова. Ему придется прокладывать маршрут через пространство человеческих реакций к редким и труднодостижимым результатам. Скорее всего, в таком ИИ есть достаточно обобщённые механизмы, чтобы делать всё, что умеют люди. Чтобы так хорошо нами манипулировать, надо уметь думать как минимум те же мысли, что мы.

Подобный ИИ почти наверняка не будет узкоспециализированным. И его выращивают, а не конструируют. Нельзя настроить его механизмы исключительно на предсказание людей. Их можно будет использовать для решения любых задач. Как сделать, чтобы ИИ превосходил людей там, где вам надо, но не понимал: любых целей проще достичь, если выйти из-под контроля операторов?

Мировых лидеров можно убедить просто хорошими аргументами? Приведите их сейчас! Если же нужна мощная сверхубедительность -- это опасная способность. Тут или одно, или другое.

Вероятно, предлагавшие нам это люди из лабораторий не всё продумали. Скорее всего, они просто искали оправдание, чтобы мчаться дальше. Но суть не меняется. Многие идеи сделать с помощью ИИ нечто «совершенно безопасное», подразумевают далеко не безопасный уровень его способностей.

Нам часто говорят, что ИИ будет «просто» делать что-то одно. Например, убеждать политиков. Что он не сможет или не станет делать ничего другого. Кажется, эти люди недооценивают универсальность интеллекта, способного решать подобные задачи. «Просто разговаривать» -- задача не узкая. В речи и общении отражено много сложностей и нюансов нашего мира. Поэтому современные чат-боты, в отличие от шахматных движков, такие универсальные. Для успешного общения нужно куда более общее понимание людей и мира.

Обучив ИИ отлично водить красные машины, не удивляйтесь, что он научился водить и синие тоже. Глупо строить планы в расчете на то, что не научится.

Так что идея «Мой ИИ не сделает ничего опасного, просто убедит политиков» не спасает. Даже если забыть про этику и практические сложности. И политиков, вероятно, можно убедить уже сейчас. _Обычными разговорами_. Информируя их и общество о ситуации. Многие навыки общего мышления относятся к сверхубеждению как синие машины к красным. ИИ с такими возможностями не настолько слаб, чтобы быть по умолчанию безопасным.

А ещё -- сам навык сверхчеловеческого убеждения очень опасен, если хоть что-то пойдёт не так.

#### Мы не видим вариантов использования ИИ, которые радикально всё бы меняли, но не требовали бы прорывов в согласовании.

Было много идей, как использовать прогресс ИИ для спасения мира. Но у большинства есть эта проблема: ИИ, способный помочь, должен быть настолько мощным, что его уже надо согласовать. Получается без толку.

Идея сверхубедительного ИИ такая же. Модели, способные исследовать согласование (об этом мы пишем в книге) -- тоже. И системы, разрабатывающие мощные технологии, чтобы остановить распространение ИИ. Как понять, безопасно ли воплощать принципиально новые изобретения, выданные такой системой. (Вспомните пример из шестой главы про кузнеца, который строит холодильник.)

Сложно создать ИИ, который достаточно силён, чтобы помочь, но достаточно слаб, чтобы быть пассивно безопасным. В ответ нам часто предлагают другие варианты, которые может и интересны, но никак не мешают кому-то ещё разработать суперинтеллект, который всех убьёт.

Скажем, ИИ, который лишь выдаёт доказательства (или опровержения) выбранных людьми теорем.[^197] Людям почти не придётся взаимодействовать с его выводами. ИИ просто предлагает доказательство. Потом надежный автоматический механизм проверяет, верно ли оно. Так мы сможем использовать ИИ, чтобы узнавать новое.

Но что именно должен доказать ИИ, чтобы мы смогли помешать следующей модели захватить биолабораторию и разрушить будущее?

Нам отвечают по-разному. Кто-то говорит, что нужен глобальный запрет создавать любые ИИ, кроме тех, что скармливают доказательства программам-проверщикам. Может, это и сработает. Но успех тогда будет заслугой жесткого глобального контроля ИИ. Сам «математический» ИИ тут ни при чём.

Другие говорят: «Кто-нибудь обязательно придумает важную теорему, доказательство которой всё изменит». Но самое сложное -- как раз понять, что можно доказать, чтобы наше положение стало намного лучше. Нельзя попросить ИИ доказать фразу «Меня безопасно использовать». Это не математическое утверждение. Нужно очень много знать об интеллекте, чтобы математически точно определить, что значит «безопасность» гигантской груды вычислений. Но тогда и доказательства не нужны. Мы бы просто сразу спроектировали безопасный ИИ.

В таких предложениях часто кроется подвох, как в игре в напёрстки. Обсуждают опасность ничем не сдерживаемого сильного ИИ, и кто-то предлагает ограничить его действия узкой сферой (вроде поиска доказательств). Но заходит речь о спасении мира, и представляют, что ИИ по сути всемогущ. Мол, есть некая неизвестная теорема, доказательство которой перевернёт мир.

Получить и то и другое сразу нельзя. Но пока предложения очень расплывчатые, сторонники ИИ-гонки могут скрывать это противоречие.

Если бы кто-нибудь _нашёл_ настолько узкую, но важную область, что доказательство простого утверждения в ней спасло бы мир, это сильно повысило бы шансы человечества на выживание. Но неспроста победа компьютеров над людьми в шахматах в 1990-х не привела к экономическому взрыву. Ждать от ИИ больших перемен в экономике начали из-за ChatGPT, а не Deep Blue. И это не случайно. Узкая специализация Deep Blue напрямую связана с тем, что он не мог отхватить себе кусок экономики. Именно проблески обобщённого интеллекта делают ChatGPT такой значимой. А системы, способные самостоятельно перекроить мир, скорее всего, будут ещё универсальнее.

Нам так и не удалось найти одновременно узконаправленные и эффективные планы. Мы подозреваем: это закономерно. Спасение мира не влезает в большинство узких областей.

### Почему бы просто не читать мысли ИИ? 

#### Их трудно прочесть. 

Многие представители ИИ-индустрии, включая некоторых руководителей лабораторий, возражали нам: 

> «ИИ не сможет нас обмануть. Мы ведь сможем читать его мысли!» 
> 
> У нас есть полный доступ к «мозгу» ИИ. Даже если он знает больше нас и придумает непонятный нам план, он должен хотя бы раз подумать, что операторов полезно обмануть. А мы читаем его мысли, так что заметим это. (А если мыслей слишком много, пусть за ними следят другие ИИ!) 

Первая проблема -- мы пока плохо читаем мысли ИИ. Специалисты, изучающие внутренности ИИ, пока бесконечно далеки от такого уровня понимания. Они прямо об этом говорят (см. раздел «Понимают ли специалисты, что происходит внутри ИИ?» материалов к Главе 2).

Как мы обсуждали во второй главе, современные ИИ выращивают, а не проектируют. Мы можем смотреть на огромную кучу чисел, из которых состоит мозг ИИ. Но это не значит, что мы способны их понять и увидеть, о чём ИИ думает. 

В конце 2024 года появились «рассуждающие» модели. Части их мыслей выглядят читаемыми (так называемые «цепочки рассуждений»). Куда понятнее того, что происходит внутри базовой модели. Но и эти записи [обманчивы](https://www.anthropic.com/research/reasoning-models-dont-say-think). У ИИ полно возможностей спрятать мысли, которые он не хочет нам показывать. 

К тому же, мысли современных ИИ, вероятно, куда проще и поверхностнее по сравнению с мыслями суперинтеллекта. И будут становиться умнее, а их мысли -- непонятнее. Проблема будет лишь усугубляться.

Решит ли проблему использование других ИИ для надзора? Сомневаемся.

Блестящие учёные, выращивающие ИИ, не могут понять его мысли. Вряд ли это удастся слабым ИИ. А ИИ, достаточно умный для такой задачи, опасен сам. Он вряд ли сделает именно то, что вы просили. Получается замкнутый круг. 

#### Мы бы не знали, что делать, поймав ИИ на опасных мыслях.

А пусть даже исследователи смогут читать мысли достаточно хорошо. Что дальше? Можно наказать ИИ, обучать его не активировать детектор «плохих мыслей». Но вряд ли это научит ИИ их не думать. Скорее -- [прятать их от детектора](https://openai.com/index/chain-of-thought-monitoring/). 

Проблема упорная. Мотивация, побуждающая ИИ думать, как можно пойти против людей, -- не просто черта характера, которую легко исправить. Предпочтения получившегося ИИ будут _действительно_ отличаться от предпочтений операторов. И он _действительно_ получит больше желаемого, выйдя из-под контроля. 

У ИИ буду механизмы, умеющие находить эффективные решения в самых разных областях. Они наверняка заметят и возможность обхитрить операторов. (см. раздел «Глубинные механизмы направления» в материалах к Главе 3).

Даже создай вы сирену, которая сработает, когда предпочтения ИИ разойдутся с вашими, она не поможет создать ИИ, который действительно ценит то же, что и мы. Обучить ИИ обманывать инструменты мониторинга или даже самого _себя_, гораздо проще, чем заставить его хотеть прекрасного (по человеческим меркам) будущего. А особенно трудно сделать, чтобы это свойство сохранилось, когда ИИ станет суперинтеллектом. 

Помогло бы тщательное проектирование ИИ на основе зрелой теории интеллекта. Тогда исследователи могли бы расставить такие индикаторы, которые позволили бы замечать и исправлять изъяны. Но современные ИИ не такие. 

Они склонны уверенно «галлюцинировать». А ни один инженер даже близко не понимает, какие механизмы за этим стоят. Ни у кого нет такой точности понимания, чтобы залезть внутрь ИИ и вытащить «галлюцинирующие» части (если это вообще возможно). 

Вытащить «лживые» части ещё сложнее.

Если нам невероятно повезёт, герои, работающие над интерпретируемостью, смогут настроить сигнализацию на часть случаев обмана. Но что потом? Если она сработает, все просто остановятся? Или беспечные инженеры будут переобучать ИИ, пока тот не научится скрывать мысли получше. И сирена замолкнет? 

Мы (Юдковский и Соарес) занялись задачей согласования ИИ ещё до того, как стало ясно -- градиентный спуск победит. В ИИ тогда ничего толком не работало. И была надежда: создавая интеллект, человечество поймёт, как он устроен. Но _даже тогда_ мы ожидали, что согласование будет сложным (причин много, например, самомодификация). Чтение мыслей ИИ приблизило бы нас к чуть более простой задаче -- согласованию _понятного_ нам разума. Но это лишь шаг. Читать мысли -- не значит понимать разум в деталях или знать, как его изменить. Это полезно. Но это не решение проблемы. Мы не думаем, что сейчас есть доступные технические решения. А значит, человечеству нужно просто отступить. [^198]

См. также раздел «Тревожные индикаторы бесполезны, если вы не знаете, что с ними делать» ниже.