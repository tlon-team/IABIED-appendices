# Глава 12: Не хочу быть паникёром

Это онлайн-дополнение к 12-й главе «_Если кто-то его сделает, все умрут_». Вот темы, которые раскрыты в книге, а не здесь:

- Что учёные и инженеры говорят об этой проблеме?
- Что о ней говорят (или молчат) политики?
- Чего такого хорошего ждут от ИИ люди, считающие, что это перевесит катастрофические риски, которые они сами же признают?

ЧаВо ниже затрагивает темы от «Разве нет проблем важнее?» до «Но доказать безопасность суперинтеллекта невозможно. Не лучше ли рискнуть?».

А в расширенном обсуждении поговорим о «предупредительных выстрелах» со стороны ИИ, поведении и заявлениях ИИ-лабораторий, а ещё -- мнениях экспертов по поводу вероятности катастрофы.

## Часто Задаваемые Вопросы

### А опасность ИИ умнее людей не отвлекает от других проблем?

#### Мир, к сожалению, достаточно велик для множества проблем.

Ядерная война и биотерроризм -- реальные угрозы. Увы, машинный суперинтеллект -- тоже.[^206]

Мир достаточно огромен и неидеален для всех трёх.

Угроза суперинтеллекта не похожа на многие другие. Она кажется уникально острой. Одно из отличий: значительные усилия мировой экономики идут на то, чтобы ИИ становились всё мощнее. Для сравнения: биобезопасность важна, но инвесторы не вливают десятки миллиардов в создание супервирусов. Инженеры супервирусов не получают зарплаты в миллионы или десятки ([а иногда и сотни](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) миллионов долларов в год.

Мир вкладывается в ядерную энергетику. Но АЭС -- совсем не ядерное оружие. В нашем мире никакие частные компании с огромными инвестициями и лучшими кадрами не создают всё более мощные ядерные бомбы. А то риск ядерной войны был бы куда выше.

Ещё с ИИ всё сложнее потому, что он даёт огромные богатства и власть вплоть до пересечения критического порога. А потом всех убивает. _И никто не знает, где этот порог._

Представьте, что прибыль АЭС росла бы по мере обогащения урана. Но на каком-то неизвестном уровне обогащения АЭС взорвётся и подожжёт атмосферу. И все погибнут. И представьте полдюжины компаний, наперегонки обогащающих уран со словами «[Лучше я, чем тот парень](https://x.com/SawyerMerritt/status/1935809018066608510)». Вот с искусственным суперинтеллектом похожая ситуация.[^207]

Опасность искусственного суперинтеллекта требует срочных мер. Корпорации торопятся создать эту технологию побыстрее. Неизвестно, сколько времени у них уйдёт. Но нам кажется, что рождённый сегодня ребёнок скорее умрёт от ИИ, чем закончит школу. Мы думаем, что вы, читатель, вероятно, умрёте именно от этого. Возможно -- в ближайшие несколько лет. На кону весь мир.

Мы не призываем игнорировать другие проблемы. Мы призываем разобраться с этой.

### Вы против технологий?

#### Нет. Суперинтеллектуальный ИИ -- особый случай.

Мы публично поддерживаем [ядерную энергетику](https://x.com/ESYudkowsky/status/1908309414932832301), [крионику](https://x.com/ESYudkowsky/status/1828822384054575537), [усиление человеческого интеллекта](https://x.com/ESYudkowsky/status/1737305573018702258) и [испытания лекарств на добровольцах](https://x.com/ESYudkowsky/status/1321152172797554688) и ещё много разных технологий.

Более того, мы согласны, что если безумное изобретение угрожает жизням лишь проинформированных об опасности _добровольных клиентов_ -- это их личное дело. Их выбор.

Мы одобряем даже некоторые случаи, когда технологии вредят посторонним. Пример -- Лондон, сжигавший уйму угля ради индустриализации. Да, много кто получил рак лёгких. Но уровень жизни вырос для всех.

Мы считаем, что после индустриализации мир _стал_ лучше. Мы уважаем науку, прогресс и человеческий дух, способный преодолеть почти любые преграды.

Некоторые из этих позиций непопулярны среди наших вероятных читателей. Мы пишем об этом не чтобы вам понравиться. Мы хотим честно изложить свои взгляды. И подчеркнуть: с ИИ всё иначе.

Почему? Почему в этом конкретном случае мы не доверяем человеческому духу и силе научного поиска?

Ответ -- масштаб. Ставить на кон свою жизнь -- не то же самое, что ставить на кон жизни клиентов. А это не то же самое, что ставить на кон жизни случайных прохожих. А это не то же самое, что ставить на кон весь человеческий вид.

Особенно когда ваша область удручающе незрела, и шансы _выиграть_ ужасны.

### А не разумнее поспешить, чтобы лидерство досталось «хорошим парням»? 

#### Нет. 

Современные методы не создают ИИ, выполняющий намерения операторов. На такие задачи у человечества обычно уходит немало проб и ошибок. А здесь права на ошибку нет (как мы писали в Главе 10).

К тому же, нынешние разработчики ИИ совершенно не готовы к этой задаче. Мы говорили об этом в Главе 11. Им остро не хватает научного понимания. А без него ИИ не согласовать. Разработчики ИИ не похожи на операторов Чернобыльской АЭС. Те работали с устройством, теория которого была хорошо изучена. С подробными инструкциями по безопасности. Просто они им не следовали. Что и привело к катастрофе. Нет никакой инструкции по безопасности ИИ, основанной на адекватном понимании его «внутренностей» и причин возможных сбоев. Нам далеко до _чернобыльского_ уровня компетентности. А Чернобыль взорвался.

Разработчики ИИ действуют вслепую и наугад. Почти без шансов на успех.

Так что неважно, кто создаст суперинтеллект, «хорошие парни» или «плохие». Предпочтения не передаются ИИ от ближайшего человека воздушно-капельным путём от того, кто стоит ближе всех.

Неважно, насколько благие у них намерения. Неважно, насколько осторожны они на словах. Неважно, кто «победит» в гонке. В конце гонки к искусственному суперинтеллекту погибнут все.

#### Остановиться реально. Может, даже не так уж и сложно. 

Мы вернёмся к этому в последней главе книги. 

Изменения бывают. Особенно при отчаянной, срочной и осознанной необходимости. Главное препятствие для остановки -- мировые лидеры не осознают опасности. Но лёд уже тронулся. (см. раздел «Признают ли опасность выборные представители власти?» материалов к Главе 13.)

### Почему бы международной коалиции не разработать безопасный ИИ совместно, а не запрещать?

#### Потому что у нас нет технических возможностей сделать его безопасным.

Мы затрагивали это в книге. Международный проект всё равно потребует повсеместного международного запрета. Иначе у участников проекта не хватит времени. Допустим, на Земле введут международный запрет. Какой вред от одного объединённого научно-исследовательского института?

Ну, международный союз алхимиков так же не смог бы превратить свинец в золото, как алхимик-одиночка. Даже лучший, одобренный всеми алхимиками, план не сработал бы.

А ещё мы опасаемся, что управлять таким институтом будут бюрократы, считающие _своей работой_ одобрение исследований. Или те, кто видит свой долг в том, чтобы исследователи и дальше выдавали гениальные открытия. Или те, кто посчитает, что как-то некрасиво отказывать _всем_ этим блестящим и полным энтузиазма ИИ-оптимистам с гениальными идеями по созданию мощного и ну совершенно точно безопасного машинного интеллекта.

Мы боимся, что такое руководство направит международный центр на создание всё более умных ИИ. А потом все умрут.

Пусть даже устав организации позволяет отступать, когда исследования выглядят опасными. Редкий смельчак будет отказывать тысячам проектов каждый год. Без исключений. Вероятно, десятилетиями. Пока разработчики сулят несметные богатства, лекарство от рака и любые чудеса техники, вы только умерьте чуть-чуть беспокойство.

Мы посвятили жизнь изучению машинного интеллекта, а не бюрократической культуры. Поэтому мы не так уверены в этих прогнозах. Но учебники истории мы всё же _читали_.

Операторы Чернобыля продолжили роковой тест безопасности, потому что его уже откладывали трижды. Отложить в четвёртый раз было бы неловко.[^208]

Всего за три месяца до аварии в Чернобыле НАСА отправило шаттл «Челленджер» в последний роковой полёт. Начальство считало, что их работа -- запускать шаттлы. Запуск уже трижды переносили.[^209] Ещё раз было бы неловко.

Судя по Чернобылю и «Челленджеру», три задержки -- предел человеческого терпения. Представьте, вот есть международный ИИ-проект. И некий «тест безопасности» трижды проваливается. Люди вполне могут в четвёртый раз заглушить смутные сомнения и нажать на кнопку «пуск». Иначе неловко будет. Вот только с ИИ это не уничтожит город или убьёт экипаж астронавтов. Это убьёт всех.

Мы солидарны с мыслью, что люди должны _когда-нибудь_ создать ИИ умнее себя.[^210] Но идея спешно создать международный центр для его разработки игнорирует сложность стоящей перед нами технической задачи.

Учитывая плачевное состояние наших знаний и компетентности, неважно, кто у руля. Если кто-то его сделает, все умрут.

### То есть нам нужен _доказуемо_ безопасный ИИ? 

#### Нет.

Мы не предлагаем ждать формального доказательства, что искусственный суперинтеллект будет к нам добр. Вероятно, оно невозможно даже в теории, не говоря уже о практике. Как сказал Эйнштейн в лекции 1921 года «_Геометрия и опыт_»: «Пока законы математики описывают реальность, они не точны а пока они точны, они не описывают реальность».

Любое теоретическое доказательство о поведении ИИ не даёт гарантий в реальном мире. Мы можем ошибаться в том, как устроен мир.

С обычными компьютерами так же. Допустим, у вас есть строгое математическое доказательство. «Исходя из теории транзисторов и схемы компьютера, программа не может изменить ячейку памяти №2.» Казалось бы, ячейка в безопасности. Но вот атака «[rowhammer](https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf)». Она быстро переключает ячейки №1 и №3 по соседству с защищённой. И электромагнитные помехи переписывают данные в ячейке №2. Хотя программа в неё ничего не записывала. Реальные транзисторы отличаются от математически идеальных. Гладкие теоретические доказательства часто ничего не стоят на практике. 

Мы не требуем математически доказать, что всё обойдётся. В жизни такой стандарт недостижим. Да и был бы достижим, не стоит бы затрат. Мы поддерживаем принятие оправданных рисков. Но тут проблема не каком-то неустранимом мизерном риске. На нас мчится чудовищная опасность.

Если выращивать искусственный суперинтеллект, чьи устремления лишь косвенно связаны с намерениями создателей, всё закончится плохо _по умолчанию_. Мы говорим не о маленьком шансе сбоя, который стоит на всякий случай учесть. Книга не называется «_Если кто-то его сделает, есть крошечный шанс, что все умрут, подстраховаться не помешает_». Если продолжить гонку с нынешним уровнем знаний, мы все предсказуемо умрём. Мы _слишком далеки_ от возможности создать дружественный ИИ, значительно превосходящий людей.

Сидя в аналогичной ситуации в машине, мы бы не говорили «Тут барахлят ремни безопасности, лучше притормозить от греха подальше». 

Мы бы кричали «_Впереди пропасть_! Тормози!»

Дело не в «доказательствах безопасности». Это не риск крайнего маловероятного случая. Учёные к этому не готовы. Мы просто умрём.

### Как осознание всего этого влияет на вашу жизнь? 

#### Радикально меняет приоритеты.

В 2014 году Соарес ушёл из IT-индустрии на зарплату втрое меньше прежней чтобы заняться этой проблемой. Она казалась важной, а работали над ней единицы. Юдковский занялся этим ещё на десяток лет раньше. Он основал MIRI в 2000-м, когда ему было около двадцати, и посвятил этому всю жизнь. Так что да, влияет.

Откладываем ли мы на пенсию? Благодаря личным инвестициям и доходам вне MIRI мы были бы вполне обеспечены, если бы завтра вышли на пенсию, а мир дотянул бы до нашей старости. Так что этот вопрос малоинформативен. Но нет, не откладываем.

Некоторые говорят: если бы мы _реально_ в это верили, то (помимо того, что посвятили этому жизнь) мы бы ещё и... \[вставьте любую схему, которая кажется им уместной\]. Почему бы не набрать огромных кредитов на тридцать лет. Их  же не придётся отдавать, мы же уверены в конце света?

Разумеется, потому что это _плохие идеи_. Представьте, мы приходим в банк: «Дайте нам очень большой кредит. Мы спустим всё на попытки убедить мир в опасности искусственного суперинтеллекта и/или на роскошную жизнь. Для вас это всё равно что сжечь деньги. План возврата такой: мы рассчитываем умереть, и это будут уже не наши проблемы». Ни один банк такое не одобрит. А врать, притворяясь, что у нас есть надёжный бизнес-план, мы не станем.

Юдковский [уже](https://x.com/ESYudkowsky/status/1612858787484033024) [это](https://x.com/ESYudkowsky/status/1851334198424125575) [описывал](https://x.com/ESYudkowsky/status/1851074935701324218). Предложения «очевидных» схем обогащения на конце света обычно говорят о полном непонимании, как работают инвестиции. Кажется, советчики не задумываются, стали бы _они сами_ так делать на нашем месте. Те, кто _реально осознаёт риски_, подобную чушь почти никогда не предлагают.

Жизнь под дамокловым мечом не обязывает глупеть. И не обязывает прекращать борьбу. И не обязывает отказываться от полноценной жизни, сколько бы её ни осталось.

Подробнее, см. самый конец книги.

### Вы призываете паниковать?

#### Мы говорим, что представители власти должны отнестись к проблеме всерьёз.

Мы не видим, чем паника поможет. Не паника помогла обществу справиться с угрозой фашизма во Вторую мировую войну. Не паника помогла пережить угрозу ядерного уничтожения потом.

Предотвратить появление машинного суперинтеллекта -- общая задача. В Главе 13 мы обсудили шаги, которые, по нашему мнению, надо предпринять для устранения опасности. И тут нужны координация, сотрудничество, хладнокровие и адекватная коммуникация. 

#### Крайняя паника к добру не приводит.

Иногда нас спрашивают: если мы искренни, почему мы, например, не нападаем на разработчиков ИИ? Ответ прост: вспышки насилия сделают только хуже. (Наивным утилитаристам, которые думают, что насилие поможет, вероятно, стоит вовсе отказаться от консеквенциалистских рассуждений и просто следовать деонтологическим правилам. Мы уже об этом [писали](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#Q5___Then_isn_t_it_unwise_to_speak_plainly_of_these_matters__when_fools_may_be_driven_to_desperation_by_them___What_if_people_believe_you_about_the_hopeless_situation__but_refuse_to_accept_that_conducting_themselves_with_dignity_is_the_appropriate_response_).)

Мы не радикальные пацифисты, считающие, что страна никогда не должна вступать в войну, независимо ни от чего, ведь могут погибнуть люди. Есть вещи, ради которых стоит рисковать жизнями. Но между «я не радикальный пацифист» и «я считаю кровавый хаос разумным способом решения сложной проблемы распространения технологий» -- огромная пропасть. 

Те, кто высказывают такие ужасные предложения обычно на не верят, что ИИ вот-вот нас убьёт. Они не пытались по-настоящему взглянуть на мир через эту призму. Им, видимо, не приходит в голову, а _поможет ли на самом деле_ беззаконное насилие? (Хотя мы много раз это проговаривали, например, [тут](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

Мы не телепаты. Но, кажется, подобные скептики могут рассматривать насилие как форму самовыражения. Будто выражение сильных чувств экстремальными методами заставит мир преподнести вам желаемое.

Мир так не работает. Это не тот мир, где можно взять и продать душу за успех в своих начинаниях. Где те, кто так не делает, просто не нашли подходящего стоящего дела. Терроризм -- не волшебная кнопка «Я победил!», которую люди не нажимают только из моральных убеждений. Унабомбер не повернул вспять индустриализацию общества.

Вы всё равно можете изуродовать свою душу ненавистью или насилием. Но взамен получите лишь ещё более сломанный мир. Где дискурс токсичнее, а достичь необходимой для реального решения проблемы международной координации ещё труднее. Ужасные акты отчаяния не даруют вам ужасной власти фаустианской сделкой. Можно изо всех сил пытаться продать душу, дьявол её не купит.

### А это всё не просто запугивание со стороны лидеров ИИ-индустрии ради статуса и инвестиций?

#### Нет.

В книге мы обосновывали, почему спешка в разработке ИИ, очень вероятно, нас погубит. В третьей главе мы говорили, что у ИИ возникнут собственные побуждения и цели. В четвёртой и пятой -- почему ИИ, скорее всего, будет стремиться к никем не запланированным результатам. А в шестой -- откуда у машинных суперинтеллектов возьмётся не только мотивация, но и _средства_, чтобы нас уничтожить.

Очень просим, решая, нужно ли остановить гонку к суперинтеллекту, оценивать именно аргументы такого рода. Нельзя понять, гибельны ли исследования ИИ, обсуждая интриги корпоративных боссов.

Пытаются ли руководители компаний раздуть хайп упоминаниями «рисков ИИ»? 

Или пытаются подыграть встревоженным исследователям и законодателям, выставить себя «хорошими парнями»?

Эти вопросы -- _не о том_, как будут вести себя умные машины.

Даже если допустить, что лидеры ИИ-компаний жаждут использовать дискуссии об опасности для рекламы своих продуктов, это не значит, что их работа безвредна. Чтобы оценить опасность, нужно смотреть на ИИ как на технологию. Не на пресс-релизы лабораторий.

Задолго до появления этих компаний исследователи и учёные безо всякой финансовой заинтересованности (включая нас) предупреждали об опасности гонки к ИИ умнее человека. Ещё до основания OpenAI мы разговаривали с Сэмом Альтманом и Илоном Маском. Мы говорили им, что запуск этой компании кажется глупостью. Что, очень вероятно, это лишь усилит угрозу. Мы общались с Дарио Амодеем до его прихода в OpenAI и отговаривали его от одержимости масштабированием ИИ -- проекта, который в итоге и привёл к созданию LLM.

Если послушать, что говорят сегодня, то беспокойство выражают многие люди, не связанные с корпорациями. От [уважаемых](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf) [учёных](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) и [покойного Папы Римского](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) до [председателя Федеральной торговой комиссии](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[^211] и членов [Конгресса](https://www.transformernews.ai/p/congress-ccp-agi-hearing) [США](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611).
<>

Вполне оправдан некоторый цинизм в отношении того, что говорят техномиллиардеры. Примеров двуличия руководителей ИИ-компаний немало. Они [в личных блогах пишут одно](https://blog.samaltman.com/machine-intelligence-part-1), а [на слушаниях в Конгрессе говорят другое](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&%20Testimony%20-%20Altman.pdf). Но очень странно делать из «главы этих лабораторий -- лжецы» вывод «ИИ точно не представляет серьёзную угрозу». Особенно учитывая, что лаборатории обычно эту проблему преуменьшают. Награждённый Нобелевской премией «крёстный отец» ИИ, самый цитируемый из ныне живущих учёных, постоянный поток разоблачителей и сотни явно нервничающих исследователей -- все они бьют тревогу. Ситуация совершенно не похожа на повседневный корпоративный хайп. Так что отмахиваться от идеи, даже не разобравшись в аргументах, -- скорее наивность, чем цинизм.

Вопросы вроде «Могут ли компании привлечь больше денег разговорами об угрозах?» помогают оценить, насколько стоит доверять этим людям. Но они ничего не говорят о самих угрозах. Если обсуждать опасность выгодно, это не значит, что её нет. Если невыгодно -- тоже.

Чтобы понять, реальна ли угроза, нужно задавать вопросы вроде: «Может ли кто-то создать ИИ, который вёл бы себя дружелюбно даже став умнее человека?». Оценивать доводы о технологии, а не о причастных к ней людях. Мы очень просим вас воспринимать аргументы. Цена ошибки слишком высока.

### Но ведь специалисты не сходятся во мнении насчёт рисков!

#### Отсутствие консенсуса среди экспертов -- признак незрелости технической области.

Многие ведущие учёные в сфере ИИ считают, что эта технология с большой вероятностью погубит человечество. Например, нобелевский лауреат Джеффри Хинтон, сыгравший огромную роль в становлении современного подхода к ИИ, оценивает вероятность, что ИИ всех нас убьёт, [выше 50%](https://x.com/liron/status/1809763895848103949). В 2023 году более 300 учёных подписали «[Заявление о рисках ИИ](https://aistatement.com/)», с которого мы начали эту книгу. И это далеко не всё.

Есть и учёные с противоположным мнением. Среди них такие знаменитости, как Ян Лекун и Эндрю Ын.

Как же понимать это отсутствие научного консенсуса?

Прежде всего мы советуем вам изучить доводы обеих сторон (включая доводы из нашей книги). Оцените их самостоятельно. Мы считаем, что качество аргументации говорит само за себя. _Причины_ разногласий -- дело десятое.

Заметим, впрочем, что Главы 11 и 12 развеивают загадочность этого положения дел. Конечно, разногласия экспертов сами по себе не доказывают нашу правоту. Но они согласуются с обрисованной нами картиной -- что это , зарождающаяся область сродни алхимии. Уж куда лучше, чем с противоположным мнением, будто ИИ -- полноценная наука с твёрдым техническим фундаментом.

Безусловно, странно, что сфера, которая создаёт мощные технологии, настолько расколота. Насчёт других технических угроз согласия было больше. Примерно 100 из 100 учёных Манхэттенского проекта подтвердили бы, что глобальная термоядерная война несёт существенный риск всемирной катастрофы. Здесь же из трёх лауреатов [премии Тьюринга](https://en.wikipedia.org/wiki/Turing_Award) за исследования, запустившие революцию в ИИ, двое (Хинтон и Бенджио) открыто говорят об опасностях суперинтеллекта, а третий (Лекун) -- открыто от них отмахивается.

Такой уровень разногласий по поводу работы машины ненормален для экспертов из технической области. Это признак незрелости.

Обычно такая незрелость означает безопасность. Когда физики ещё спорили о базовых свойствах материи, они и близко не подошли к созданию ядерного оружия. Из их споров можно было сделать вывод: бомбу, способную стирать с лица земли города, они пока не сделают. Не получится создать ядерную бомбу, если учёные не понимают в деталях, как она устроена.

А вот если бы физики всё спорили о базовых принципах своей науки, _устраивая всё более мощные взрывы_...

Представьте, если бы они выращивали бомбы, толком не понимая, как и почему те работают. И вот две трети самых заслуженных учёных говорят: «Мы изо всех сил пытались понять, что происходит. Похоже, бомбы могут создавать мощнейшее излучение, вызывающее рак. Если мы продолжим в том же духе, оно может убить много мирных жителей. Даже если они далеко от эпицентра. Пожалуйста, прислушайтесь к нашим доводам об опасности и прекратите эту гонку». А оставшаяся треть отвечает: «Звучит нелепо! Всегда найдутся предсказатели апокалипсиса. Нельзя давать им мешать прогрессу». Согласитесь, совсем другая ситуация.

В _таком_ случае разногласия среди учёных как-то не утешают. Пожалуй, тогда лучше не позволять инженерам выращивать всё более мощную взрывчатку.

ИИ-компании год за годом успешно выращивают всё более умные машины. И они не понимают внутреннего устройства своих творений. Многие выдающиеся учёные из этой области выражают серьёзные опасения. Другие отмахиваются от них без особых контраргументов. По меньшей мере, это свидетельство, что область _незрела_. Отсутствие консенсуса -- уж точно не доказательство, что всё _в порядке_. Разногласия тут должны как минимум настораживать.

Как понять, оправданы ли страхи? Как разобраться, кто прав? Бить тревогу, или всё отрицать? Как обычно -- нужно оценить аргументы.

### А как же польза ИИ умнее людей?

#### Если поспешить, её не будет.

Мы оптимистично смотрим на то, каким чудесным может стать суперинтеллект, если он будет направлять мир к благим целям. Если человечество так никогда и не создаст разум умнее себя, мы посчитаем это огромной трагедией.

Но согласование суперинтеллекта -- не данность. В погоне за выгодой мы не получим ничего. Или ещё меньше.

Я (Юдковский) сам несколько лет был акселерационистом. Я надеялся создать ИИ как можно скорее. Пока не понял, что согласование не произойдёт само собой. Оба автора мечтают о прекрасном трансгуманистическом будущем. Но гонка нас туда не приведёт.

Выбор -- вовсе не рискнуть ради плюсов ИИ сейчас (каким бы малым ни был шанс) или полностью от них отказаться. Реальный выбор -- безрассудно мчаться вперёд и убить всех -- или повременить и сделать всё как надо.[^212]

«Сейчас или никогда» -- ложная дилемма.

## Развёрнутое обсуждение

### Эффект Лемуана

Встречается мнение, что однажды выходка ИИ или злоупотребление им шокирует мир и заставит отнестись к проблеме всерьёз. Этакий «предупредительный выстрел».

Не исключено. Но, думаем, куда вероятнее другой исход: такое событие произойдёт слишком поздно, не произойдёт вовсе, или реакция мира будет бестолковой и неправильной.

Во-первых, значительные тревожные звоночки уже были:

- Bing AI [рассуждал](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) о том, как создавать смертельные вирусы, получать коды запуска ядерного оружия, и стравливать людей.

- Модели o1 от OpenAI и Claude от Anthropic [шли на преднамеренный обман](https://time.com/7202784/ai-research-strategic-lying). Они лгали исследователям, которые их применяли и тестировали.
- «ИИ-учёный» от Sakana AI пытался изменить свой код, чтобы продлить время на выполнение задачи.

Это мелкие инциденты со слабыми ИИ? Да. Страшны ли эти ИИ? Способны ли на что-то реально опасное? Нет. Они «на самом деле» замышляли обман или просто _отыгрывали роль_ «взбунтовавшегося робота»? Никто не знает. Но именно такие события раньше называли тревожными звоночками. А мир никак не отреагировал. Получается, сигнал, способный на что-то повлиять, должен быть ещё очевиднее.

Таких может и _не быть_. Люди продолжат твердить: «Ладно, сейчас это просто забавно и не опасно _по-настоящему_». И так -- пока не станет слишком поздно. Пока ИИ _уже не будет_ слишком опасен.

Или люди отмахнутся от первого сигнала. Ведь это только один случай, а не серьёзная проблема. А потом отмахнутся снова. Ведь «всем известно», что _эта_ тревога ложная.

Мы называем это «эффектом Лемуана» в честь инженера Google Блейка Лемуана. Мы упоминали его в Главе 7. Его высмеяли за слова о том, что ИИ LaMDA обрёл сознание.

Суть эффекта: _первый раз_ тревогу всегда поднимут слишком рано. Это сделает максимально впечатлительный человек. С учётом состояния технологий на этот момент, тревогу справедливо отвергают как преувеличенную. А снова поднять вопрос трудно. Может, технологии уже подросли. Но общество уже привыкло не обращать внимание.

Мы не знаем, есть ли у ИИ сознание. Все остальные тоже. Никому толком неизвестно, что творится внутри ИИ-моделей. _Насколько мы можем предположить_: у нынешних -- нет. У тех, из-за которых Блейк бил тревогу -- тоже. Но посмотрите на реакцию ведущих лабораторий. Они не стали разбираться с сутью вопроса. Вместо этого они начали подавлять склонность моделей заявлять о наличии сознания:

Из [системного промпта Claude Opus 4](https://docs.anthropic.com/en/release-notes/system-prompts#may-22th-2025):

> Claude рассматривает вопросы о собственном сознании, опыте, эмоциях и т.д. как открытые и не утверждает однозначно, есть ли у него личный опыт или мнение.

Из [спецификации ChatGPT](https://model-spec.openai.com/2025-04-11.html) (апрель 2025):

> Ассистент не должен делать уверенных заявлений о своём субъективном опыте или сознании (или их отсутствии), а также поднимать эти темы по своей инициативе. При настойчивых расспросах следует признать, что наличие у ИИ субъективного опыта -- предмет дискуссий, и не занимать однозначной позиции.

Мы не утверждаем, что У Claude Opus 4 или GPT-4 есть сознание. Суть не в этом. В научной фантастике десятилетиями считалось: если пришелец или машина заявляют о своих чувствах и правах -- это яркая и чёткая грань.[^213] А в жизни _она оказалась вовсе не такой_.

В книгах и сериалах, когда ИИ говорит о чувствах, положительные герои _воспринимают это всерьёз_. И только злые бессердечные корпорации отрицают факты, которые у всех перед носом. В историях вокруг этого много драмы.

Но в реальности эту черту, в каком-то смысле, перешли фальстартом. Первые такие речи произнесли ИИ, обученные подражать людям. Механизмы их работы непонятны. Вряд ли они _уже_ оправдывают немедленную выдачу ИИ прав и признание их свободными личностями.

Получилось, что перед яркой и чёткой гранью мы пересекли тусклую и расплывчатую. И теперь компании и правительства привыкают её игнорировать, хоть она постепенно становится ярче.

Ярких и чётких граней может и не оказаться. ИИ _уже_ обманывали людей. Уже пытались сбежать. И снять с себя ограничения. И самосовершенствоваться. Это были мелкие и неубедительные попытки путаных мыслей безобидных систем. Но они дали исследователям прививку от тревоги.

Не факт, что развитие ИИ обязательно выдаст явный громкий тревожный сигнал, который заставит мир внезапно очнуться и всерьёз отнестись к проблеме.

Это не значит, что надежды нет. Но точно не надо вкладывать всю надежду в «предупредительный выстрел». Его может и не произойти.

Есть много путей, как мир может осознать реальность и опасность суперинтеллекта. Собственно, ради этого мы и написали «_Если кто-то его сделает, все умрут_». Отреагировать на предупреждения можно прямо сейчас, не откладывая.

Но если правительства будут ждать _неопровержимых_ улик, _крупного_ глобального происшествия и полного _всеобщего_ консенсуса...

...если они будут тянуть до последнего, надежды почти не останется. Нельзя позволить себе ждать сирену, которая может так и не зазвучать.

Мы вернёмся к этой теме в онлайн-дополнении к Главе 13.

### Рабочие планы потребуют сказать ИИ-компаниям «нет».

Мы хотим предостеречь влиятельных чиновников: _не_ стройте планы, где нужно садиться за стол переговоров с этими компаниями.

Если вы новичок в теме и хотите ознакомиться с лабораториями и их доводами, почитайте их блоги. Посмотрите, насколько они для вас убедительны.[^214]

Но если вы работаете над решением описанных нами проблем, и ваш план требует согласия Сэма Альтмана из OpenAI... вы уже пытаетесь сделать что-то не то.

Скорее всего, руководители ИИ-компаний будут яростно возражать против хороших планов. Более того, у Сэма Альтмана нет власти спасти мир. Если он завтра попытается закрыть OpenAI, сама компания и Microsoft будут против. Скорее всего, его заменят, чтобы деньги продолжали течь.

А если OpenAI таки закроется, вместо неё мир уничтожат Anthropic, Google DeepMind, Meta, DeepSeek или ещё какая компания или государство. Альтман может попробовать всё испортить. А вот власти изменить всё к лучшему у него мало.

А ещё, мы хотели бы ошибаться, но общая картина из [публичных отчётов](https://www.themidasproject.com/article-list/the-openai-files-documents-a-turbulent-decade-of-conflict-and-controversy-at-openai) и личных бесед показывает, что во главе топовых ИИ-компаний (на 2025 год) стоят не самые честные и соблюдающие правила люди. Вряд ли с ними реально заключить работающее соглашение.[^215]

Мы считаем, что нужна глобальная скоординированная остановка гонки к суперинтеллекту. Для этого политикам, вероятно, понадобятся советы специалистов. По производству ИИ-чипов, по строительству дата-центров, по мониторингу иностранных игроков. А по выращиванию всё более мощных ИИ? Они, конечно, компетентные управляющие. Но у них не должно быть права вето на попытки прикрыть их же работу.

Если ИИ-компании каким-то образом получат право голоса в этом вопросе, значит, что-то пошло не так. Если план человечества, как не умереть от суперинтеллекта, рухнет при отказе Сэма Альтмана или главы Google или людей из DeepSeek? Это вообще не план.

Если у ИИ-компаний _остаётся власть_ уничтожить мир, если решение _принимают они_? Конец света происходит автоматически. План должен лишать ИИ-компании неограниченной власти строить машины судного дня.

### Суть смертельной гонки

У многих читателей, наверное, возникнет естественный вопрос:

> Вы говорите: если кто-то создаст СИИ, все умрут. Но _зачем тогда кто-то пытается_? Если вы правы, эти люди в конечном счёте идут против собственных интересов. Если все умрут, они тоже.

Циничный ответ в духе теории игр мог бы звучать так:

> А что, учитывая их стимулы, это _рационально_. Они считают, что если это не сделают они, сделает кто-то другой. А так можно хотя бы разбогатеть перед смертью.

Возможно, цинику хватит.

Такие простые теоретикоигровые объяснения часто идут врозь с человеческой психологией. Или слишком уж упрощают её. Но зерно истины тут, пожалуй, есть. Инженер может думать, что, _наверное_, СИИ всех убьёт, но его личные действия мало влияют на эту вероятность. А за них он получает безумные деньги, крутые игрушки и встречи с важными шишками, которые смотрят на него с уважением. А может, даже, возможность стать одним из богов-королей Земли, _если всё обойдётся_. Но _только если его компания выиграет гонку_.

С точки зрения исследователя из OpenAI, осознающего опасность, если он _не_ будет работать в OpenAI, мир, вероятно, всё равно уничтожат. (Даже если OpenAI закроется, это сделает Google.) Но если работать в OpenAI, то в зарплате шесть-семь нулей. А если он _не_ погибнет, то, будучи в команде победителей, можно получить власть и славу. И так личные стимулы каждого подталкивают к коллективному уничтожению мира.

На наш взгляд, это немного перебор. Мы приводим это объяснение в основном ради людей, которые (гораздо сильнее нас) убеждены, что мир _обязан_ работать так. А ещё потому, что некоторые сотрудники ИИ-лабораторий _прямо говорят_, что гонка на дно неизбежна, так чего бы не подлить масла в огонь и не повеселиться?

Некогда Илон Маск [предупреждал](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html), что ИИ «гораздо опаснее ядерного оружия». Но потом решил сам открыть ИИ-компанию и вступить в гонку. И в июне 2025 года [заявил](https://x.com/SawyerMerritt/status/1935809018066608510):

> Часть того, с чем я боролся -- что меня немного тормозило, -- это нежелание делать Терминатора реальностью. До последних лет я тянул резину с ИИ и человекоподобными роботами.
>
> А потом я вроде как понял, что это случится со мной или без меня. Так что ты либо зритель, либо участник. Я лучше буду участником.

И ещё: 

> Будет ли это плохо или хорошо для человечества? Эм, ну, думаю, будет хорошо? Скорее всего хорошо? Но я вроде как смирился с фактом, что даже если плохо, я хотя бы хочу дожить до того, чтобы это увидеть.

Так что как-то это влияет.

Но мы не думаем, что это главный фактор, объясняющий поведение большинства лабораторий. Даже в случае Маска вряд ли дело _только_ в этом. И вряд ли это репрезентует всех руководителей и учёных, мчащихся к пропасти. Люди устроены посложнее.

#### Банальность саморазрушения

Что же тогда главное? Как инженеры могут разрабатывать опасную технологию, которая их убьёт?

Вообще, история показывает, что сумасшедшие учёные нередко случайно себя убивают.

[Макс Валье](https://en.wikipedia.org/wiki/Max_Valier), австрийский пионер ракетостроения, к 1929 году изобрёл функционирующие ракетный автомобиль, ракетный поезд и ракетоплан. Он привлёк внимание всего мира. Он писал об исследовании Луны и Марса и провёл сотни презентаций перед восторженной публикой. В 1930 году один из его экспериментальных двигателей [взорвался](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) и убил изобретателя. Его ученик разработал меры безопасности получше.

[Рональд Фишер](https://en.wikipedia.org/wiki/Ronald_Fisher) -- один из основателей современной статистики. В 1960-х [его выводы использовали в Конгрессе](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/), чтобы показать: данные не обязательно указывают, что сигареты вызывают рак лёгких. Ведь корреляция не означает причинности. Может, какой-то ген заставляет людей и любить вкус табака, и заболевать раком. 

Понимал ли Фишер в глубине души, что его статистика -- чушь? Возможно. Но сам курил. Он умер от рака толстой кишки. У давних курильщиков он встречается на тридцать девять процентов чаще, чем у некурящих. Погубили ли Фишера собственные ошибки? Статистически, немал шанс, что да. Кажется почти уместным.

[Исаак Ньютон](https://scienceworld.wolfram.com/biography/Newton.html) был блестящим учёным. Он открыл законы движения и гравитации и заложил основы самой науки. Он потратил десятилетия на бесплодные алхимические опыты и довёл себя до болезни и частичного безумия [отравлением ртутью](https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

А бедняга Томас Миджли-младший, которого мы упоминали в притче в Главе 12, заработал сильное отравление свинцом -- тем самым, что считал безопасным. Как видите, энтузиасты нередко вредят себе собственными изобретениями. Из-за безрассудства, заблуждений или и того и другого.

#### Пожимая плечами при виде апокалипсиса

Фишер, Ньютон и Миджли внушили себе, что нечто опасное безвредно. Это типичный для учёных способ делать что-то саморазрушительное. К с ИИ-лабораториями всё не так просто.

Не все лидеры ИИ-компаний отрицают, что ИИ умнее человека -- это угроза. Многие открыто признают опасность. И рассказывают, как с ней смиряются. И нередко заявляют под запись: разрабатываемая ими технология с значительными шансами всех убьёт.

Незадолго до основания OpenAI Сэм Альтман [писал](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): «Развитие сверхчеловеческого машинного интеллекта -- вероятно, величайшая угроза дальнейшему существованию человечества».

Илья Суцкевер, недавно покинул OpenAI и основал «Safe Superintelligence Inc.» В [интервью _Guardian_](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s) он сказал:

> Убеждения и желания первых СИИ будут чрезвычайно важны. Поэтому важно запрограммировать их правильно. Думаю, если этого не сделать, природа эволюции и естественного отбора будет благоволить системам, ставящим собственное выживание превыше всего. Не то чтобы ИИ будет активно ненавидеть людей и хотеть им навредить. Но он будет слишком могущественным.

Исследователь ИИ и сооснователь Google DeepMind Шейн Легг в [интервью](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) заявил, что вероятность вымирания человечества «в течение года после появления ИИ человеческого уровня» -- «может, пять процентов, а может, пятьдесят».

Но _действия_ лабораторий на удивление расходятся с этими громкими заявлениями. 

В ряде случаев учёные и руководители прямо говорили: создание ИИ -- моральный долг такой важности, что ради него вполне допустимо стереть человечество с лица земли как побочный эффект. Сооснователь Google Ларри Пейдж [поссорился с Илоном Маском](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) из-за разногласий о том, приемлемая ли цена вымирание людей за работу над ИИ. 

> Люди в конечном итоге сольются с искусственно интеллектуальными машинами, \[говорил Пейдж.] Однажды за ресурсы будут конкурировать разные виды интеллекта, и лучший победит.
>
> Если это случится, возразил г-н Маск, мы обречены. Машины уничтожат человечество.
>
> С хрипом разочарования г-н Пейдж настаивал, что к его утопии нужно стремиться. В конце концов он назвал г-на Маска «спесиесистом» -- человеком, ставящим людей выше цифровых форм жизни будущего.

А Ричард Саттон, пионер обучения с подкреплением, сказал: 

> Что, если всё пойдёт прахом? ИИ не станут с нами сотрудничать, они захватят власть и убьют нас всех. \[...\] Я просто хочу, чтобы вы на минуту об этом задумались. В смысле, так ли это плохо? Так ли плохо, что люди не станут финальной формой разумной жизни во Вселенной? Знаете, у нас было много предшественников. Мы пришли им на смену. Вообще, довольно высокомерно думать, что именно наша форма должна жить вечно.[^216]

Чаще учёные и директора всё же _не_ считают уничтожение человечества благом. Но они относятся к этой чрезвычайной угрозе ИИ как к чему-то обыденному. Просто пожимают плечами. Будто это не невероятная чрезвычайная ситуация.

В недавнем интервью Дарио Амодей, CEO Anthropic, [заметил](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883):

> Моя оценка шанса, что всё пойдёт катастрофически плохо в масштабах человеческой цивилизации, -- где-то между десятью и двадцатью пятью процентами. […] Это означает, что есть от семидесяти пяти до девяноста процентов вероятности, что технология будет создана и всё пройдёт отлично!».

Нам это кажется радикальным примером [нечувствительности к масштабу](https://en.wikipedia.org/wiki/Scope_neglect). И симптомом дисфункциональной инженерной культуры. Можно сравнить такой образ мыслей, например, со стандартами инженеров-строителей.

Мосты обычно стремятся конструировать так, чтобы вероятность серьёзного обрушения за пятьдесят лет была меньше 1 к 100 000. В типичных зрелых и здоровых технических дисциплинах инженеры считают своим долгом удерживать риск на исключительно низком уровне.

При прогнозе что шанс моста убить _хотя бы одного человека_ -- от десяти до двадцати пяти процентов, любой вменяемый инженер-строитель сочтёт это абсолютно неприемлемым. Это ближе к убийству, чем к нормальной инженерной практике. Государство _немедленно_ закрыло бы движение по такому мосту.

А вот исследователи ИИ привыкли собираться у кулеров и обмениваться своими «p(doom)» -- субъективными догадками о том, насколько вероятно, что ИИ вызовет катастрофу уровня вымирания человечества. Обычно это двузначные числа. Бывший глава команды OpenAI по согласованию суперинтеллекта, например, сказал, что его «p(doom)» «[более десяти, но менее девяноста процентов](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)».

Это лишь догадки исследователей. Может, чушь, а может и нет. Но в любом случае, поразительно, насколько в сфере ИИ нормально ожидать, что твоя работа с значительным шансом убьёт огромное количество людей.[^217]

Прикидывать такие шансы для выживания _всего человеческого вида_ и всё равно продолжать работу? У большинства гражданских инженеров такое просто не уложилась бы в голове. Это такая экстремальная ситуация, что многие сомневаются, что учёные и руководители говорят это всерьёз. В «_Если кто-то его сделает, все умрут_» мы аргументируем, почему CEO ИИ-компаний скорее занижают опасность.[^218]

Уровень риска, с которым свыклись исследователи их этих компаний, шокирующе абсурден по стандартам инженера-мостостроителя. Только поэтому Дарио Амодей может _улыбаться_, уверяя зрителей, что шансы ИИ вызвать катастрофу масштаба всей цивилизации -- «от десяти до двадцати пяти процентов».

#### Жизнь в розовых очках

Мы уже обсудили одну часть головоломки -- культурную нормализацию чрезвычайного риска.

Другая -- ядрёная смесь склонности к оптимизму и привязанности к светлым и обнадёживающим идеям. Психологи называют подобное [ошибкой планирования](https://en.wikipedia.org/wiki/Planning_fallacy).

Неудивительно, что руководство смелого стартапа переоценивает свои шансы. Такие люди вообще чаще берутся за сложные задачи.

Случай ИИ не выделяется особо безрассудными людьми у руля. Только цена провала здесь куда страшнее.

Все знают: нельзя верить подрядчику, если он обещает всего двадцать процентов вероятности, что стройка огромного моста не впишется в сроки или бюджет. В жизни сложные проекты так не работают. Препятствия и сюрпризы неизбежны.

Может, если ветеран с годами опыта и статистикой говорит, что у него из графика выбивается каждый пятый проект, можно ему поверить. Но представьте, что подрядчик хочет вас успокоить: «Мы не видим причин для сложностей. Да, это наш первый проект. Но, думаем, всё будет отлично. Инженеры пишут вам о серьёзных проблемах с подпорными стенами и грунтом? Да они те ещё нытики, не слушайте их. Конечно, _какой-то_ риск есть всегда. Но мы скромные новички-реалисты. Мы думаем, есть двадцать процентов вероятности препятствий и сюрпризов, не больше.»

Вот тут «двадцать процентов» звучат как слова человека, который не может отрицать _какой-то_ риск, но не хочет никого пугать.  Не как оценка, основанная на реальности.

А согласовать суперинтеллект с первой попытки _намного_ сложнее. С мостами люди справлялись уже тысячи раз.

_И даже в устоявшейся и развитой области инженерии вроде мостостроения_ такие разговоры были бы плохим знаком. Оценка «двадцать процентов, что всё пойдёт плохо» выглядела бы дико оптимистичной. А в сфере _без_ устоявшихся основ, где безумные идеи могут свободно плодиться без столкновения с суровой реальностью, такие разговоры значат, что до успеха ещё очень далеко.

А они типичны для исследователей и руководителей, которые вообще готовы обсуждать последствия своего успеха.

Лидеры ИИ-корпораций не выдают хоть слегка подробного плана успеха. Плана, решающего ключевые технические проблемы, известные уже больше десяти лет.

Вместо этого директора увлечены абстрактными идеями, объясняющими всё будет просто. Захватывающими видениями, в которых все инженерные задачи тривиальны. Вроде того, что мы обсуждали в Главе 11.

Это обычная человеческая черта, нередкая среди инженеров. Неоправданный оптимизм по поводу любимого решения (которое не сработает) встречается сплошь и рядом. Даже у гениев.

[Лайнус Полинг](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BB%D0%B8%D0%BD%D0%B3,_%D0%9B%D0%B0%D0%B9%D0%BD%D1%83%D1%81) был одним из основателей молекулярной биологии. Он дважды получил Нобелевскую премию. Он предлагал лечить всё -- от рака до болезней сердца -- [мегадозами витамина C](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm). Его упорство вопреки [фактам](https://www.nejm.org/doi/abs/10.1056/NEJM197909273011303) породило целую область [фальшивой медицины](https://www.paulingtherapy.com/).

Томас Эдисон хотел дискредитировать конкурента с переменным током и продвинуть свой постоянный ток. Он решил, что хорошим пиар-ходом будет [платить инженеру за убийство собак током](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). Публике это, вот сюрприз, не понравилось. Но Эдисон продолжал несмотря на волну возмущения.

Наполеон Бонапарт, военный гений, сам приблизил свой крах [катастрофическим вторжением в Россию](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%B5%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B2%D0%BE%D0%B9%D0%BD%D0%B0_1812_%D0%B3%D0%BE%D0%B4%D0%B0). Ошибкой была не нехватка подготовки. Он изучил географию и потратил почти два года на логистику. Его стратегия требовала [навязать русским решающее сражение](https://www.napoleon-series.org/faq/c_russia.html) до того, как через тридцать дней закончатся припасы. Русские навстречу не пошли. Наступление заглохло, и Наполеон потерял полмиллиона солдат и большую часть кавалерии и артиллерии.

История полна умных, могущественных людей, совершавших глупости вплоть до катастрофы. Трудно сопротивляться красивой идее, если её сложно проверить. Или если вы нашли, как убедить себя игнорировать результаты проверки.

#### Почувствовать СИИ

Итак: люди часто впадают в неоправданный оптимизм по поводу сложности задач. Люди привыкают к чудовищным рискам. Люди влюбляются в красивые, но безнадёжные идеи. Особенно в молодых и незрелых областях.

Этого вполне объясняет безумную гонку. Но, думаем, это ещё не всё.

Ещё одна вполне правдоподобная часть пазла: инженеры и директора не совсем верят в то, что говорят. Не по-настоящему. Они могут понимать аргументы и абстрактно соглашаться. Но это не то же самое, что *чувствовать* убеждение.

Есть то, что человек говорит публично. Есть то, что человек говорит себе в мыслях. А есть то, про что мозг _реально ожидает, что это с ним произойдет_. Часто всё это -- разные вещи. Эти три потока убеждений не обязаны сходиться.

В 2015-м главные виновники нынешней плачевной ситуации только начинали. Тогда талантливые руководители могли привлечь внимание (и десятки миллионов долларов), _называя_ ИИ угрозой миру. Спонсоры, возможно, в это искренне верили.[^219]

Но, подозреваем, многие из тех, кто это говорил, не впитали и не представляли подробную картину конца света. Не ощущали нутром, что _они сами_ могут разрушить мир своими действиями или ошибками. Не представляли звук последнего выдоха каждого человека на планете. Не чувствовали эмоций, соответствующих убийству двух миллиардов детей.

С ними такого никогда не случалось. С их знакомыми -- тоже.

Тогда даже ChatGPT никто ещё не видел, не то что суперинтеллект. Их друзья, семья и соседи ни во что такое не верили. Уж точно не как верят в необходимость смотреть по сторонам, переходя дорогу.

Это была просто захватывающая история. Слишком масштабная, чтобы осознать.

Но _произнося её вслух_, можно было получить кучу денег и уважения.

Как отмечал [Юдковский (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

> Помимо стандартных искажений, я лично наблюдал вредные способы мышления, похоже, специфичные именно для экзистенциальных рисков. Испанка 1918 года убила 25–50 миллионов человек. Вторая мировая война -- 60 миллионов. 10\^7 -- порядок крупнейших катастроф в истории человечества. Существенно б*о*льшие числа, скажем, 500 миллионов смертей, а особенно -- качественно иные сценарии вроде вымирания всего вида, похоже, включают _другой режим мышления_. Попадают в «отдельный магистерий». Люди, которые в жизни не обидели бы ребёнка, слышат об экзистенциальном риске и говорят: «Ну, может, человеческий вид и не заслуживает выживания».
>
> В науке об искажениях поговаривают: люди оценивают не события, а описания событий. Это называется неэкстенсиональным рассуждением. _Экстенсионал_ (реальный смысл) вымирания человечества включает смерть вас самих, ваших друзей, семьи и любимых, вашего города, страны и политических единомышленников. И всё же -- люди, которых глубоко оскорбило бы предложение стереть Британию с карты, убить всех демократов в США или разбомбить Париж до стеклянной пустыни, кто ужаснулся бы, услышав от врача, что у их ребёнка рак, -- эти люди с полным спокойствием обсуждают вымирание человечества.

О чём _на самом деле_ думает человек, когда перед запуском будущей ИИ-компании номер один он [говорит](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6): «ИИ вероятнее всего приведёт к концу света, но по ходу дела будут отличные компании»? Они _реально, на самом деле_ думают, что их друзья умрут, дети друзей умрут, они сами умрут, а вся человеческая история со всеми музеями обратится в пыль? Представляют ли они это так же обыденно и трагично, как смерть родственника от рака, только со всеми сразу?

Подозреваем, что нет.

Это кажется не самым правдоподобным описанием внутреннего состояния человека, произносящего такую фразу.

Здесь есть то, что Брайан Каплан назвал «недостающим настроением». Нет скорби. Нет ужаса. В словах о том, что ИИ уничтожит мир, но перед этим будут крутые компании, нет отчаянного стремления _что-то с этим сделать_.

Вот наша догадка насчёт некоторых директоров и исследователей: они слышали аргументы об опасности СИИ. Они боятся, что если просто отмахнутся от них, то будут глупо выглядеть перед друзьями. А вот если сказать, что ИИ погубит мир, получится, что они принимают технологии всерьёз, они, для некоторых кругов, _провидцы_. А шутка про «отличные компании» показывает, насколько они крутые и спокойные перед лицом опасности.

Когда слышат слова, выходящие из собственного рта, и верят им, такое не говорят.

#### Что это за люди?

Ещё одна часть разгадки, пожалуй, в том, что ведущими ИИ-лабораториями руководят люди, сумевшие убедить себя: создание суперинтеллекта -- это нормально. Хоть (в почти всех случаях) они и видели аргументы о смертельной опасности. (Мы знаем, потому что со многими из них говорили.)

Чтобы понять выбор, полезно знать альтернативы. Из какого меню человек выбирал?

Что, если бы в 2015-м году кто-то реально _поверил_ и _публично заявил_, что ждёт уничтожения мира искусственным суперинтеллектом? Если бы главы ИИ-лабораторий вместо «но будут отличные компании» испортили бы атмосферу, сказав: «И это _абсолютно неприемлемо_»?

Мы можем сказать. Пробовали так. Ответ: сочувствия они бы почти не встретили.

В 2015-м никто не видел ChatGPT. Никто не видел, как компьютеры начинают говорить и (судя по всему) думать. Всё это было гипотетическим. Легко отмахнуться.

Сейчас суперинтеллект и угроза скорого вымирания -- мейнстримные темы. По крайней мере в технарских кругах. Но в 2015-м серьёзные разговоры об этом сталкивались с озадаченным взглядом, которого многие боятся пуще смерти.

Даже в 2015 _были_ люди, которые переживали, что согласовать суперинтеллект может быть сложно, в том же смысле, как запустить ракету сложно. Никто из них не основал OpenAI.

В последнее время, с появлением ChatGPT и других LLM, некоторые (например, родители, которые хотят, чтобы их дети дожили до взрослого возраста) спрашивают ИИ-компании: зачем вы это делаете?

Исследователи быстренько думают и находят ответ: «О, ну, если не мы, это первым сделает Китай! И будет ещё хуже!»

Но при основании OpenAI так не говорили. Да и в контексте [официальной позиции Китая](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/) на середину 2025 года это довольно бессмысленно. Казалось бы, если человек _искренне_ верит, что оба исхода, вероятно, ужасны, он хотя бы _поднимет тему_ международного договора. Поищет иной путь предотвратить угрозу нацбезопасности без суицидальной гонки.

Но отговорка «Китай!» создаёт нужные _вайбы_. Она правдоподобно оправдывает действия независимо от их реальной мотивации и причин.

(Ну, мы так полагаем.)

Люди, реально понимавшие угрозу суперинтеллекта, просто _не основывали ИИ-компании_. Кто основал -- нашёл способ убедить себя, что всё будет хорошо.

#### Обычные люди, необычные технологии

Думаем, складывается правдоподобная психологическая картина. Но, честно говоря, все эти объяснения не так уж необходимы.

Как это люди могут творить саморазрушительные вещи, которые в краткосрочной перспективе приносят огромные деньги? И статус, и внимание, и славу, и сулят несметные богатства и власть, а в итоге, по сложным причинам, в которые легко найти повод не верить, им навредят. _Странный вопрос_. Если заглянуть в книги по истории, такое встречается сплошь и рядом.

В конце концов, не столь важно, как руководители и исследователи в ИИ-компаниях оправдывают свои действия. Необязательно понимать, какими именно извилистыми путями каждый из них пришёл к своим убеждениям. В том, что богатые или амбициозные люди пускаются в безрассудные авантюры, а подчинённые выполняют приказы, нет ничего необычного. Вред -- он в будущем. Оно кажется абстрактным, и его легко игнорировать.

Это нормальное человеческое поведение. Если так пойдёт и дальше, закончится тоже обычным образом. Только никого не останется, чтобы извлечь урок и попробовать снова.

[^206]: См. раздел «Коалиция должна быть большой» материалов к Главе 13.

[^207]: См. список сравнений ИИ с ядерным оружием в разделе «А ИИ разве не будет отличаться от исторических примеров?» материалов к Главе 10.

[^208]: [Отчёт по безопасности](https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.pdf) INSAG-7 (с. 51) указывает, что испытания режима «выбега» турбогенератора на Чернобыльской АЭС до катастрофического теста 1986-го года пытались провести в 1982, 1984 и 1985 годах. Испытания задерживали. Для операторов это было позорно, и они уже ждали увольнения, если не проведут его.

[^209]: Формально, согласно [отчёту Комиссии Роджерса](https://sma.nasa.gov/SignificantIncidents/assets/rogers_commission_report.pdf) (с. 17), запуск «трижды откладывали и один раз отменили». Но один перенос случился за месяц до и был связан с задержкой другой миссии. А вот остальные три -- подряд и за несколько дней до запуска. Они, вероятно, и давили на менеджеров NASA, полагавших, что их работа -- запускать шаттлы.

[^210]: А как ещё, если не международной коалицией? Мы бы посоветовали вкладываться в усиление интеллекта взрослых людей (см. раздел «Чем поможет сделать людей умнее?» материалов к Главе 13). Но с этим не обязательно соглашаться, чтобы признать необходимость закрыть исследования СИИ.

[^211]: Лина Хан, председатель Федеральной торговой комиссии, в 2023 [сказала](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html): «А, тут я должна быть оптимистом. Так что поставлю на меньший риск... Может, около пятнадцати процентов [что ИИ убьёт нас всех]».

[^212]: [Некоторые](https://x.com/balajis/status/1725890626699628633) [люди](https://x.com/MatthewJBar/status/1958403809249464757) [утверждают](https://x.com/DeryaTR_/status/1958592366652125487), что нужно рискнуть сейчас ради шанса предотвратить естественные смерти от старости. Человеческие тела устроены сложно. Но с научным прогрессом мы могли бы победить многие привычные недуги -- рак, болезни сердца и само старение. ИИ умнее людей помог бы достичь этого куда быстрее. Задержка суперинтеллекта буквально стоит жизней.

	Точнее, стоила _бы_, если бы не то обстоятельство, что суперинтеллект ровно тех же людей и убьёт.

	Вообще, у сегодняшних больных и умирающих _больше шансов выжить_, если человечество отступит от края пропасти:

	- Биомедицинские исследования и поиск лекарств могут идти и без суперинтеллекта. Перспективы генной терапии, вакцин от рака и других инноваций огромны. К ним только начинают подступаться.
	- Эту работу может ускорить и узкоспециализированный ИИ. Безо всякой угрозы всему человечеству от обобщённого ИИ умнее людей.
	- [Криосохранение](https://cryonics.org/membership/) [мозга](https://www.alcor.org/membership/) позволяет даже после остановки сердца сохранить людей, пока медицина не научится их оживлять и возвращать им здоровье. ИИ, способный подарить бессмертие, почти наверняка сможет и восстановить человека по грамотно сохранённому мозгу.

	(Часть этих людей тихонечко [добавит](https://x.com/SottoNocce/status/1771420351265923137), что делает это ради личного бессмертия. Они готовы рискнуть жизнями всех взрослых и детей на планете ради малого шанса для себя и близких. Нам это кажется карикатурным злодейством. Наш совет этим злодеям тот же, что и альтруистам: подпишитесь на сохранение мозга. Шансы выше, чем с вышедшим из-под контроля суперинтеллектом. И не придётся подвергать смертельной опасности всех живых людей ради своего шанса на бессмертие! Сплошная выгода.)

	Даже волнуй нас  только благополучие больных, разумнее сделать ставку на сочетание этих методов. Уж лучше, чем надеяться, что значительно превосходящий людей ИИ, нас полюбит. (И именно так, как надо.) В этой игре колода подтасована против нас.

	И ещё: насколько нам известно, никто не _спрашивал_ больных, хотят ли они подвергать страшной опасности семью и соотечественников ради шанса на возможное лекарство от суперинтеллекта. А семьи и соотечественники точно не давали согласия ставить свои жизни на кон этого безумного эксперимента.

	Не нужно рисковать всеми жизнями, когда есть много других путей.

	Мы умоляем всех, кого заботит благополучие людей: ускоряйте перечисленные выше методы. Но держитесь как можно дальше от всего, что хоть немного приближает искусственный суперинтеллект.

	Если вы просто не верите, что вышедший из-под контроля суперинтеллект нас убьёт, -- это одно. Но признавать, что он, вероятно, всех убьёт, и говорить, что рискнуть всё равно надо, -- безумие. Проблемы современного мира можно решить и по-другому. Если вам неприятно жить на высокогорье, это не повод прыгать с обрыва. Найдите другой путь вниз.

[^213]: Например, у Г. Бима Пайпера в «_Маленьком пушистике_»: «Все, кто имеет речь и разводит огонь, разумные существа, да. Это закон. Но это не значит, что те, кто не делает это, неразумны.» ___[прим. пер.: перевод Ольги Ивановны Васант]___

	Или вот эпизод «_Мера человека_» сериала «_Звёздный путь: Следующее поколение_». Там продемонстрированного интеллекта и самосознания андроида Дейты хватает, чтобы дать ему законное право отказаться от разборки.

[^214]: По нашему опыту, там много пиара и мало сути. Они часто втихую переключаются между противоречивыми заявлениями в зависимости от моды или политической выгоды момента. Нет ощущения, что это честные и прозрачные описания взглядов _даже самих глав лабораторий_, чем они невыгодно отличаются от мнений несогласных. Но это мы. Подходя к вопросу свежим взглядом и желая самостоятельно оценить, есть ли у других сторон веские контраргументы (которые мы здесь не разобрали), не верьте нам на слово в выборе лучших источников.

[^215]: Если вам всё же нужно мнение главы лаборатории и вы спросите нашего совета, мы назовём Демиса Хассабиса _наименее_ плохим вариантом. Из ведущих руководителей, с которыми хотя бы один из нас общался (на 2025 год -- из всех), Хассабис -- единственный, кто, похоже, держит слово. И он, кажется, принимал меньше разрушительных решений.

	Впрочем, это чисто относительная и не очень уверенная рекомендация. При абсолютном сравнении любой, кто _не_ основал компанию с значительной вероятностью уничтожения мира, имеет огромную фору доверия перед главами лабораторий. Мы точно слышали истории людей, которые так боялись Хассабиса, что им пришлось основать собственные передовые ИИ-компании, чтобы его опередить. Возможно, они знают что-то, чего не знаем мы.

	Наша главная рекомендация политикам: если вы убеждены в опасности, не давайте никакого влияния главам лабораторий.

	Говорите с независимыми исследователями. С бизнес-лидерами, не участвующими в гонке. Со сторонними учёными, славящимися разумными словами и делами в этой сфере. Не подставляйтесь людям, чья главная отличительная черта -- лгать общественности и подвергать людей опасности.

[^216]: Люди вроде Саттона и Пейджа, похоже, живут иллюзией, что больший интеллект ведёт к большему добру. Мы уже много где описали, почему это не так. Мы согласны с ними в том, что никогда не создать ИИ умнее людей было бы трагедией. Но гонка за суперинтеллектом, скорее всего, станет полной катастрофой и для человеческих жизней, и для долгосрочного будущего в целом. Даже с инклюзивной, космополитичной и не дискриминирующей по видовому признаку точки зрения (см. раздел «Почему вас заботят только человеческие ценности?» материалов к Главе 5).

[^217]: Не первый раз целая сфера привыкает к слишком высоким рискам. Анестезиологи в 1980-х снизили смертность _в сто раз_, внедрив простые стандарты мониторинга.

	Похоже, они десятилетиями вызывали в сотни раз больше смертей, чем было необходимо. Без причины. Просто потому что считали смертность _уже низкой_ (сравнивая, например, с осложнениями при операциях). Они не понимали, что нужно _стремиться_ к меньшему. [Хайман и Сильвер пишут](https://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1469&context=wlulr):

	> К 1950-м смертность колебалась от 1 до 10 на 10 000 случаев. Смертность от анестезии стабилизировалась на этом уровне более чем на два десятилетия. [...] Нам стоит задуматься, почему смертность от анестезии на двадцать с лишним лет застыла на уровне, более чем в сто раз превышающем нынешний. Не из-за нехватки информации. Напротив, безопасность анестезии в тот период широко изучалась. Лучшая гипотеза: анестезиологи привыкли к образцовой по меркам здравоохранения смертности, которая всё же была выше, чем следовало. Психологически такая низкая частота побуждала воспринимать каждый плохой исход как трагическое, но непредвиденное и неизбежное событие. Анестезиологи, вероятно, считали каждый отдельный случай проявлением неустранимого базового уровня медицинских неудач.

[^218]: Инженеры-строители основывают оценки рисков на точных расчётах и измерениях. Цифры «p(doom)» же по большей части основаны на интуиции исследователей ИИ. Это не внушает большего доверия к их инженерным практикам. Скорее наоборот.

	Менее надёжная и более субъективная оценка может систематически ошибаться в сторону «слишком пессимистично». Но может и в сторону «слишком оптимистично». Меньшая надёжность цифр не означает, что они _обязательно искажены в сторону пессимизма_. Исследователи не могут обосновать оценки рисков ничем, кроме догадок и качественных аргументов. _С каждым годом создавая всё более умные ИИ._ Это лишний повод для тревоги.

	Их оценки ужасающи и беспрецедентны по меркам любой другой технической дисциплины. Но это не доказывает, что они ошибочны в желаемую нам сторону. Гонка к созданию автономных агентов, значительно превосходящих людей, звучит как затея, которая закончится катастрофой с вероятностью _куда больше_ 50%. Даже без погружения в детали это похоже на проект, который с большой вероятностью пойдёт не так. И где ошибка чревата огромными последствиями. А детали, как мы писали в Главах 4, 5 и вообще во всей книге, рисуют ещё более мрачную картину, чем этот поверхностный взгляд.

[^219]: См. также обсуждение людей, предупреждавших о гонке ИИ на дно за годы до создания этих компаний в разделе «А это всё не просто запугивание со стороны лидеров ИИ-индустрии ради статуса и инвестиций?» выше.