# Введение: Что сложно, а что просто

Это первое онлайн-дополнение к книге Юдковского и Соареса «[Если кто-то его сделает, все умрут](https://www.amazon.com/gp/product/0316595640)». В нём часто задаваемые вопросы по каждой главе. А ещё углублённые разборы неключевых для книги деталей и исторической информации.

Тут много материала. Не стоит читать всё от начала до конца. Мы написали книгу так, чтобы она была самодостаточна и доносила всю основную мысль. Но если у вас есть конкретный вопрос, контраргумент или любопытство, и книга не смогла отдать ему должное, очень вероятно, вам поможет дополнение. Если мы упускаем что-то важное, запросите это [здесь](http://ifanyonebuildsit.com/submit-question).

## Часто задаваемые вопросы

### Зачем писать книгу об угрозе вымирания от сверхчеловеческого ИИ?

#### Потому что ситуация действительно кажется серьезной и неотложной.

Если присмотреться к какой-то области, иногда можно увидеть приближение поворота истории.

В 1933 году физик Лео Силард первым понял, что цепные ядерные реакции возможны[^1]. Так он сумел предсказать один из поворотов раньше остальных.

По нашему мнению, уже сейчас видно -- ИИ ведёт нас к следующему повороту. И мы думаем, что если человечество не изменит курс, всё закончится плохо.

ИИ-лаборатории соревнуются, чтобы создать машины умнее любого человека раньше остальных. И у них очень значительный прогресс. Как мы обсудим в следующих главах, современные ИИ скорее *выращиваются*, чем конструируются. Они ведут себя так, как никто не просил и не хотел. И приближаются к тому, чтобы стать способнее любого человека. Нам это кажется крайне опасной ситуацией.

Ведущие ученые в области ИИ вместе подписали открытое [письмо](https://aistatement.com/). Они предупреждают общественность, что эту угрозу следует рассматривать как «глобальный приоритет, наряду с другими глобальными рисками, такими как пандемии и ядерная война». Такое беспокойство --- не редкость. Его разделяет почти половина специалистов (см. ниже "Эксперты по ИИ о катастрофических сценариях"). Мы надеемся, что даже если вы изначально настроены скептически, то такие заявления со стороны экспертов и высокие ставки, если их опасения верны, ясно показывают, что тема заслуживает серьезного обсуждения.

Тут лучше взвешивать аргументы, а не слепо доверять интуиции. Если специалисты не ошибаются, --- мир в невероятно опасном положении. Остаток книги мы будем излагать аргументы и доказательства, стоящие за этими предупреждениями.

Мы не считаем ситуацию безнадежной. Мы написали эту книгу в попытке изменить траекторию, по которой, судя по всему, движется человечество. Мы думаем, что есть надежда решить проблему.

И первый шаг к решению --- понять ее.

### Вы думаете, что ChatGPT может нас всех убить?

#### Нет. Мы беспокоимся о будущих достижениях в области ИИ.

Сейчас вы читаете эту книгу, в частности, потому, что такие разработки, как ChatGPT, сделали ИИ заметной новостью. Мир начинает обсуждать прогресс ИИ и то, как он влияет на общество. Это позволяет нам говорить об ИИ умнее человека, и о том, что нынешняя ситуация выглядит неблагоприятно.

Мы, авторы, давно работаем в этой области. Прогресс ИИ за последние годы повлиял на наши взгляды, но опасения зародились ещё до ChatGPT и других больших языковых моделей. Мы уже десятилетиями (Соарес с 2013 года, Юдковский с 2001) занимаемся техническими исследованиями и пытаемся сделать так, чтобы создание ИИ умнее человека закончилось хорошо. По некоторым признакам мир может быть готов к такому разговору. Правдоподобно, что разговор этот *необходим* сейчас. А иначе человечество может упустить окно возможности для реакции.

Область ИИ развивается. В какой-то момент (мы не знаем, когда) она дойдёт до создания ИИ умнее нас. Это открытая цель всех ведущих ИИ-компаний:

> Сейчас мы уверены, что знаем, как создать CИИ \[сильный искусственный интеллект\] в традиционном понимании \[...\] Мы начинаем направлять наши усилия дальше, к суперинтеллекту в истинном смысле слова. Нам нравятся наши нынешние продукты, но мы тут собрались во имя славного будущего. С суперинтеллектом всё остальное возможно.
>
> --- [Сэм Альтман](https://blog.samaltman.com/reflections), генеральный директор OpenAI
>
> Я думаю, что \[мощный ИИ\] может появиться уже в 2026 году. \[...\] Под мощным ИИ я имею в виду ИИ-модель \[...\] с такими свойствами: По чистому интеллекту она умнее лауреата Нобелевской премии в большинстве важных областей --- биологии, программировании, математике, инженерии, писательстве, и так далее. Это значит, что она может доказывать новые математические теоремы, писать очень хорошие романы, с нуля программировать сложные кодовые базы, и так далее.
>
> --- [Дарио Амодей](https://www.darioamodei.com/essay/machines-of-loving-grace), генеральный директор Anthropic
>
> В целом мы сосредоточены на создании полноценного общего интеллекта. Все возможности, которые я сегодня обсуждал -- следствия эффективного выполнения задачи создания общего интеллекта.
>
> --- [Марк Цукерберг](https://www.facebook.com/share/p/16STVBshtn/), генеральный директор Meta (незадолго до того, как компания [объявила](https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26) о [проекте «суперинтеллекта»](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta) на 14,3 миллиарда долларов)
>
> Я думаю, что в следующие пять-десять лет есть, вероятно, 50-процентный шанс на то, что мы бы определили как СИИ.
>
> --- [Демис Хассабис](https://youtu.be/CRraHg4Ks_g?feature=shared&t=41), генеральный директор Google DeepMind
>
> Уэс: Итак, Демис, вы пытаетесь вызвать взрыв интеллекта?\ Демис: Нет, ну, не неконтролируемый...
>
> --- [Уэс Рот (интервьюер) и Хассабис](https://x.com/WesRothMoney/status/1926669591163621789)

Их дела и вложения не расходятся со словами. [Microsoft](https://www.reuters.com/technology/artificial-intelligence/microsoft-plans-spend-80-bln-ai-enabled-data-centers-fiscal-2025-cnbc-reports-2025-01-03/), [Amazon](https://www.datacenterdynamics.com/en/news/amazon-2025-capex-to-reach-100bn-aws-revenue-hit-100bn-in-2024/) и [Google](https://www.datacenterdynamics.com/en/news/google-expects-2025-capex-to-surge-to-75bn-on-ai-data-center-buildout/) объявили о планах потратить на ИИ-дата-центры от 75 до 100 миллиардов долларов за 2025 год. xAI, [выкупивший X (бывший Twitter)](https://www.reuters.com/markets/deals/musks-xai-buys-social-media-platform-x-45-billion-2025-03-28/), оценивается на 80 миллиардов долларов -- примерно в два раза выше, чем сам X. Вскоре после покупки они [собрали 10 миллиардов долларов](https://www.cnbc.com/2025/07/01/elon-musk-xai-raises-10-billion-in-debt-and-equity.html) на огромный дата-центр и развитие своего ИИ -- Grok. OpenAI в партнёрстве с Microsoft и другими объявили о [проекте Stargate](https://openai.com/index/announcing-the-stargate-project/) стоимостью в 500 миллиардов долларов.

Генеральный директор Meta Марк Цукерберг [заявил](https://www.datacenterdynamics.com/en/news/zuckerberg-says-meta-will-spend-hundreds-of-billions-of-dollars-on-ai-infrastructure-over-the-long-term/), что его компания [планирует потратить 65 миллиардов долларов](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/) на ИИ-инфраструктуру в этом году и «сотни миллиардов» на ИИ-проекты в ближайшие годы. Meta уже инвестировала 14,3 миллиарда долларов в ScaleAI и наняла его генерального директора для управления свежесозданными [Meta Superintelligence Labs](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires). Ещё они переманили больше дюжины ведущих исследователей из конкурирующих лабораторий[^2] предложениями до 200 миллионов долларов в год каждому.

Это всё не означает, что до ИИ умнее человека уже совсем близко. Но показывает, что все крупные компании очень, очень стараются его создать. И такие ИИ, как ChatGPT -- результаты этой исследовательской программы. Компании не собираются ограничиваться чат-ботами. Их цель -- получить суперинтеллект, а эти боты --- лишь промежуточный пункт.

После десятилетий попыток лучше понять задачу и серьёзно обдумать будущее развитие, наше мнение таково: нет принципиального барьера для того, чтобы исследователи хоть завтра добились прорыва и успешно создали ИИ умнее человека.

Мы не знаем, будет ли этот порог действительно преодолён скоро или через десятилетие. История показывает -- предсказать время появления новой технологии гораздо сложнее, чем тот факт, что она вообще будет разработана. Но мы считаем, что аргументы в пользу опасности с лихвой оправдывают агрессивную международную реакцию уже сегодня. Эти аргументы, конечно, есть в книге.

### Но люди же постоянно паникуют и слишком остро реагируют на происходящее?

#### Да. Но это не значит, что опасности нет.

Порой люди остро реагируют на проблемы. Некоторые просто фаталисты. Иногда паника беспочвенна. Всё это не означает, что мы живём в абсолютно безопасном мире.

Германия в 1935 году была неподходящим местом для евреев, цыган и других групп людей. Некоторые разглядели это вовремя и уехали. Другие решили, что тут какое-то паникёрство, и погибли.

Угроза ядерного уничтожения была реальной, но человечество справилось с вызовом, и холодная война не перешла в горячую.

Хлорфторуглероды действительно прожигали дыру в озоновом слое, пока их успешно не запретили международным договором. Потом озоновый слой восстановился.

Иногда люди предупреждают о выдуманных опасностях. А иногда -- о реальных.

Человечество не всегда слишком остро реагирует на вызовы. Не всегда и недооценивает их. Порой люди умудряются делать и то, и другое одновременно --- например, в случае, когда страны готовили для следующей войны огромные линкоры, хотя должны были строить авианосцы. Простого решения вроде «игнорируй все предполагаемые технологические риски» или «считай все технологические риски реальными» не существует. Чтобы понять, что же правда, нужно изучить каждый случай подробно.

(Больше на эту тему читайте во введении к книге.)

### Когда будут разработаны эти тревожащие ИИ?

#### Знание о том, что технология грядёт, не говорит, когда именно она появится.

Многое, что люди просят нас предсказать, мы на самом деле знать не можем. Когда Лео Силард в 1939 году написал послание с предупреждением США о ядерном оружии, он не мог включить и не включил туда никакого утверждения вроде «Первая ядерная бомба будет готова к испытательному взрыву через шесть лет».

Это была бы очень ценная информация! Но даже когда ты, как Силард, первым правильно предсказываешь ядерные цепные реакции, даже когда ты самый первый видишь, что технология возможна и значима -- ты не можешь точно предсказать, когда эта технология появится.

Есть простые прогнозы и сложные. Мы не претендуем на то, что можем делать сложные -- например, точно предсказать, когда появятся опасные ИИ.

#### Экспертов постоянно удивляет, как быстро ИИ развивается.

Незнание того, когда ИИ появится, не равно знанию, что времени ещё много.

В 2021 году прогностическое сообщество на сайте Metaculus [оценивало](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/), что первый «по-настоящему сильный ИИ» появится в 2049 году. Через год, в 2022, этот коллективный прогноз сдвинулся на двенадцать лет назад --- к 2037. Ещё через год, в 2023, он сдвинулся ещё на четыре года назад --- к 2033. [Снова](https://x.com/slow_developer/status/1947248501743599705) и [снова](https://forecastingresearch.org/near-term-xpt-accuracy) быстрые темпы развития ИИ удивляли прогнозистов. Предсказания кардинально менялись из года в год.

Это явление не ограничивается Metaculus. Организация 80,000 Hours [задокументировала](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/) много случаев, когда у экспертов-прогнозистов быстро сокращались оценки времени. Даже суперпрогнозисты, которые постоянно выигрывают турниры по прогнозированию и часто превосходят в способности к предсказанию будущего специалистов по релевантной области, давали лишь [2,3% вероятности](https://forecastingresearch.org/near-term-xpt-accuracy) того, что ИИ получит золотую медаль Международной математической олимпиады к концу 2025 года. ИИ [получил](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) золотую медаль Международной математической олимпиады в июле 2025 года.

Интуитивно может казаться, что до ИИ умнее человека ещё десятилетия. Но ведь в 2021 году казалось, что до ИИ уровня ChatGPT ещё десятилетия, а потом он внезапно появился. Кто знает, когда так же внезапно появятся новые принципиальные улучшения ИИ? Возможно, ещё через десять лет. Или завтра. Мы не знаем, сколько времени это займёт, но некоторые исследователи всё сильнее тревожатся, что немного. Мы не претендуем на особые знания в этом вопросе, но считаем, -- человечеству следует действовать поскорее. Неизвестно, сколько ещё у нас будет предупреждений.

Подробное описание, как возможности ИИ могут лавинообразно нарастать почти без предупреждения см. в главе 1. А описание современных парадигм ИИ и того, «дойдут ли они до конца» --- в главе 2.

#### К заявлениям СМИ о том, что может и не может скоро произойти, лучше относиться скептически. (Возможно, это уже произошло!)

Через два года после того, как Уилбур Райт [с унынием предсказал](https://www.wright-brothers.org/History_Wing/Wright_Story/Inventing_the_Airplane/Not_Within_A_Thousand_Years/Not_Within_A_Thousand_Years.htm), что до механического полёта ещё тысяча лет, New York Times уверенно заявила, что миллион[^3]. Через два месяца и восемь дней братья Райт полетели.

Скептики часто очень уверенно говорят, что ИИ никогда не сможет соперничать с людьми в чём-то конкретном. Несмотря на то, что недавний прогресс машинного обучения показал, --- по всё большему числу тестов-бенчмарков ИИ сравнялись с человеческими результатами (или превзошли их). Например, как минимум с конца 2024 года известно, что современные ИИ часто способны распознать сарказм и иронию из [текста](https://www.yomu.ai/resources/can-ai-essay-writers-understand-satire-irony-or-sarcasm-in-essays) или даже [невербальных сигналов](https://dl.acm.org/doi/10.1145/3678957.3685723). Это не помешало New York Times в мае 2025 года [повторить заявление](https://www.nytimes.com/2025/05/16/technology/what-is-agi.html): «...У учёных нет твёрдых доказательств того, что сегодняшние технологии способны хотя бы на некоторые из простейших вещей, которые делает мозг, --- например, распознавать иронию»[^4].

Общий вывод: многие будут утверждать, что знают -- ИИ умнее человека вот-вот появится, или наоборот, что до него ещё невообразимо далеко. Но неуютная реальность такова, что никто сейчас этого не знает.

Хуже того. Велики шансы, что и не узнает, пока для международного сообщества не станет слишком поздно что-то предпринимать.

Предсказать время следующего технологического прорыва ужасно трудно. Мы знаем, что ИИ умнее человека смертельно опасен. Но если нам нужно знать, в какой день недели он появится, то увы. Надо уметь действовать в условиях неопределённости, иначе мы не будем действовать вообще.

### Можно ли по прошлому экстраполировать, когда мы создадим ИИ умнее человека?

#### Для этого у нас недостаточно понимания интеллекта.

Иногда успешные предсказания делаются так: берём на графике прямую линию, которая была стабильной много лет, и говорим, что она так и продолжится как минимум ещё год-два.

Так получается не всегда. Тренды меняются. Но это часто работает неплохо. Так люди делают успешные предсказания.

Недостаток этого метода в том, что часто мы хотим знать не «насколько высоко поднимется эта линия на графике к 2027 году?», а скорее «Что изменится качественно, если эта линия продолжит подниматься?». Какой её уровень соответствует каким важным реальным результатам?

В случае с ИИ мы просто не знаем. Довольно легко найти какой-нибудь параметр искусственного интеллекта, образующий на графике прямую (например, [«перплексию»](https://en.wikipedia.org/wiki/Perplexity), и провести эту линию дальше. Но никто не знает, какая будущая «перплексия» соответствует какой качественной способности играть в шахматы. Люди не могут предсказать это заранее -- приходится запустить ИИ и выяснить так.

Никто не знает, где на этом графике проходит линия «Теперь он способен убить всех».Можно только запустить ИИ и выяснить. Поэтому экстраполяция прямой линии на графике нам не поможет. (Даже до того, как график потеряет актуальность из-за прогресса алгоритмов.)

Поэтому в книге мы не занимается экстраполяцией линий на графиках, чтобы точно сказать, когда кто-то применит 10²⁷ операций с плавающей точкой для обучения ИИ, или к чему это приведёт. Такое предсказать сложно. Книга сосредоточена на том, что, как нам кажется, предсказать просто. Это довольно узкая область. В ней мы можем совершить небольшое число важных предсказаний, но это не даёт нам права делать уверенные предположения про всё что угодно в будущем.

### Чем мотивированы авторы? Нет ли у них конфликта интересов?

#### В среднем мы не ожидаем заработать на книге. И ещё -- мы будем рады, если её тезис ошибочен.

Мы (Соарес и Юдковский) получаем зарплату в Институте исследований машинного интеллекта (MIRI). Он финансируется пожертвованиями людей, считающих эту тему важной. Возможно, книга привлечёт пожертвования.

Но у нас есть и другие возможности зарабатывать. Мы занимаемся написанием книг не ради денег. Наш аванс за эту книгу целиком пошёл на её рекламу, а все гонорары достанутся MIRI, чтобы возместить рабочее время и усилия сотрудников.[^5]

И конечно, оба автора были бы в восторге, если бы пришли к выводу, что наша цивилизация в безопасности. Мы бы с удовольствием вышли на пенсию или стали зарабатывать как-то иначе.

Мы не думаем, что нам было бы трудно изменить мнение, если бы доказательства это оправдывали. Уже меняли. MIRI был основан (под названием «Институт сингулярности») как проект по *созданию* суперинтеллекта. Юдковскому понадобился год, чтобы понять -- само по себе это ничем хорошим не кончится. И ещё пара лет, чтобы понять -- довольно сложно заставить это дело кончиться хорошо.

Мы уже меняли своё мнение, и были бы рады сделать это снова. Просто мы не считаем, что это оправдано свидетельствами.

Мы не думаем, что ситуация безнадёжна. Но мы действительно считаем проблему реальной, и думаем, что если мир *не* примется за неё всерьёз, ему грозит ужасная опасность.

Подчеркнём: для понимания, находится ли ИИ на пути к тому, чтобы убить нас всех, нужно думать про ИИ. Если думать только о людях, можно найти причины отвергнуть любой источник. Академики оторваны от жизни; корпорации пытаются раздуть ажиотаж; некоммерческие организации хотят собрать деньги; любители не знают, о чём говорят.

Но если пойти этим путём, то финальные убеждения будут определяться тем, кого вы решили отвергнуть. Вы не будете давать аргументам и свидетельствам шанса изменить ваше мнение, если оно неверно. Чтобы понять, где правда, не обойтись без оценки аргументов и их собственной осмысленности, независимо от того, кто их выдвинул.

Наша книга не начинается с дешёвого аргумента, что корпоративные руководители лабораторий ИИ заинтересованы убедить население, мол, ИИ безопасны. Она начинается с обсуждения ИИ. Позже в книге мы немного касаемся исторических случаев, *когда учёные были слишком оптимистичны*. Но мы никогда не говорим, что какие-то аргументы лучше игнорировать, потому что их авторы из ИИ-лаборатории. Мы обсуждаем некоторые *реальные планы* разработчиков и то, почему они по объективным причинам не сработали бы. Мы изо всех сил стараемся поговорить о реальных аргументах, потому что значимы именно они.

Если вы считаете нас неправыми, приглашаем вас показать, где конкретно. Мы считаем это более надёжным способом найти правду, чем переходить на личности и мотивации. Даже если самый предвзятый человек в мире говорит, что идёт дождь, это не значит, что вокруг солнечно.

### Разве всё это про ИИ -- не просто научная фантастика?

#### Распространённость темы в художественной литературе не особо о чём-то говорит.

ИИ умнее человека ещё не создан, но его изображали в фантастике. Мы не рекомендуем опираться на эти представления. Настоящий ИИ, скорее всего, будет мало похож на фантастический -- в главе 4 мы разберём, почему.

ИИ -- не первая технология, которую предвосхитила фантастика. [Летательные аппараты тяжелее воздуха](https://www.weslpress.org/9780819577269/robur-the-conqueror/) и [полёты на Луну](https://www.imdb.com/title/tt0000417/) описали до их появления. Общую идею ядерного оружия предугадал Г. Уэллс, один из первых фантастов, в романе 1914 года [«Освобождённый мир»](https://ahf.nuclearmuseum.org/ahf/key-documents/hg-wells-world-set-free/). Он ошибся в деталях: у Уэллса была бомба, и она мощно горела много дней, а не мгновенно взрывалась, оставляя смертельное излучение. Но у него была общая идея бомбы на ядерной, а не химической энергии.

В 1939 году Альберт Эйнштейн и Лео Силард отправили письмо президенту Рузвельту с призывом опередить Германию в создании атомной бомбы. Можно представить мир, где Рузвельт впервые узнал об идее ядерных бомб из романа Уэллса и отверг её как научную фантастику.

В реальности Рузвельт отнёсся к идее серьёзно -- по крайней мере, достаточно серьёзно, чтобы создать «Урановый комитет». Это показывает, как опасно отвергать идеи лишь из-за того, что нечто похожее описал фантаст.

Научная фантастика может ввести в заблуждение и если считать её правдой, и если считать её *ложью*. Авторы-фантасты -- не пророки, но и не анти-пророки, чьи слова гарантированно неверны. Обычно лучше игнорировать фантастику и оценивать технологии и сценарии сами по себе.

Чтобы предсказать, что на самом деле произойдёт, придётся честно обдумать аргументы и взвесить свидетельства.

#### У ИИ точно будут странные последствия.

Мы одобряем мысль, что ИИ *странный*, что он нарушит статус-кво и изменит мир. Наши интуитивные соображения в некоторой степени адаптированы к миру, где люди -- единственный вид, способный, например, на строительство электростанции. Где всю человеческую историю машины всегда были неразумными инструментами. Но мы можем быть очень уверены, как минимум, в том, что будущее с ИИ умнее человека не будет таким.

Крупные и долгосрочные изменения мира случаются нечасто. Эвристика «ничего никогда не происходит»[^6] обычно работает прекрасно, но случаи, когда она терпит неудачу -- на них-то важнее всего обратить внимание. Весь смысл раздумий о будущем -- предвосхитить что-то большое до того, как оно всё же произойдёт, чтобы можно было подготовиться.

Как обсуждалось во введении, один из способов преодолеть уклон в сторону статуса-кво -- вспомнить историю.

Иногда отдельные изобретения кардинально меняют мир. Взять хоть паровой двигатель и другие технологии, появлению которых он поспособствовал во время Промышленной революции. Они быстро преобразили человеческую жизнь:

\[изображение\]

Будет ли появление действительно сильного ИИ аналогично влиятельным событием? Кажется, что искусственный интеллект будет ну хотя бы так же важен, как Промышленная революция. Среди прочего:

- ИИ, вероятнее всего, сильно ускорит технологический прогресс. Как мы обсудим в главе 1, машины способны работать намного быстрее человеческого мозга. Люди могут совершенствовать ИИ (а потом ИИ сможет улучшать себя сам), пока машины не опередят людей в совершении научных открытий, изобретении новых технологий и подобном.

<!-- -->

- Всю историю человечества механизмы нашего мозга принципиально не менялись, даже при создании всё более впечатляющих инженерных достижений. Стоит ожидать, что многое изменится очень быстро, когда сам механизм познания начнёт совершенствоваться и станет способен улучшать себя.

<!-- -->

- Кроме того, как мы обсудим в главе 3, достаточно способные ИИ, скорее всего, будут иметь собственные личные цели. Будь ИИ просто людьми, только побыстрее и поумнее, их появление уже было бы грандиозным событием. Но ИИ будут, по сути, совершенно новым видом разумной жизни на Земле. Видом со своими целями, которые, вероятно (как мы обсудим в главах 4 и 5), существенно отклонятся от человеческих.

Было бы удивительно, если бы эти два мощных прорыва могли не перевернуть существующий мировой порядок. Для веры в «нормальное» будущее, кажется, надо думать, что машинный интеллект вообще никогда не превзойдёт человеческий. Что и раньше не казалось возможным. А в 2025 году в это гораздо труднее поверить, чем в 2015 или 2005.

#### Долгосрочные прогнозы и технологические изменения

Далёкое будущее тоже будет странным.

Если заглянуть в будущее достаточно далеко, оно обязательно будет странным. XXI век совершенно невероятен с точки зрения XIX века, а тот казался бы удивительным из XVII. ИИ ускоряет этот процесс и вводит нового игрока.

Один аспект будущего кажется сегодня предсказуемым: развитые технологические виды не останутся навечно привязанными к планете. Сейчас ночное небо полно звёзд, прожигающих свою энергию. Ничто не мешает жизни научиться путешествовать к ним и использовать эту энергию в своих целях.

Есть физические ограничения скорости таких путешествий, но не похоже, чтобы были ограничения на их осуществление вообще[^7]. Ничто не мешает нам в итоге разработать межзвёздные зонды, способные добывать из вселенной ресурсы и создавать новые процветающие цивилизации, а заодно ещё больше самовоспроизводящихся зондов для колонизации других областей космоса. Если нас заменят ИИ, ничто не помешает им делать то же самое, подставьте только вместо «процветающих цивилизаций» их цели, какими бы они ни были.

Жизнь распространялась по мёртвым камням, пока не заселила всю Землю. Так же мы можем ожидать, что жизнь (или созданные ею машины) в конце концов заселит и необитаемые части вселенной. Тогда найти безжизненную звёздную систему будет так же странно, как сегодня найти на Земле безжизненный остров, лишённый даже бактерий.

Сейчас большая часть материи во вселенной, включая звёзды, расположена случайно. Но в достаточно далёком будущем основная часть материи почти наверняка будет расположена по определённому замыслу -- согласно предпочтениям тех, кто сможет собрать и переделать звёзды.

Даже если ничто с Земли никогда не распространится по космосу, и даже если большинство разумных форм жизни в далёких галактиках никогда не покинет родную планету, одного овладевшего космическими путешествиями разума где угодно во вселенной хватит, чтобы зажечь искру и начать распространение по космосу. Он будет путешествовать к новым звёздным системам и использовать местные ресурсы для создания новых зондов -- точно так же, как потребовался лишь один самовоспроизводящийся микроорганизм (плюс немного экспоненциального роста), чтобы превратить безжизненную планету в мир, где каждый остров наполнен жизнью.

Так что будущее не похоже на день сегодняшний. Более того, мы можем ожидать кардинальных отличий. Любые биологические или искусственные виды в поисках ресурсов неизбежно преобразят сами звёзды -- хоть мы и мало что можем сказать о том, как выглядел бы такой вид и на какие цели пошли бы ресурсы вселенной.

Предсказать детали кажется трудной, почти невозможной задачей. Это сложно. Но предсказать преобразование вселенной в место, где большая часть материи собрана и направлена на некую цель -- какой бы она ни была? Это проще, хоть и может показаться контринтуитивным с точки зрения цивилизации, едва начавшей извлекать ресурсы из звёзд.

Не стоит ожидать, что будущее через миллион лет будет похоже на 2025 год со стаей безволосых обезьян, копошащихся по поверхности Земли. Задолго до этого либо мы уничтожим себя, либо наши потомки отправятся исследовать космос[^8].

Человечество точно ждут странные времена. Вопрос только -- когда.

#### Будущее настигнет нас быстро.

Технологии вроде ИИ означают, что будущее может постучаться в нашу дверь уже скоро, и очень громко.

По меркам истории до Нового времени Промышленная революция преобразила мир очень быстро. По меркам эволюционных процессов *человек разумный* преобразил мир очень быстро. По меркам космологических и геологических процессов жизнь преобразила мир очень быстро. Новые изменения тоже могут оказаться очень быстрыми по старым меркам.

Похоже, человечество приближается к следующему радикальному преобразованию, когда машины смогут начать переделывать мир на своих скоростях, намного превышающих биологические. В главах 1 и 6 мы ещё поговорим о том, насколько хорошо машинный интеллект мог бы сравниться с человеческим. Но, как минимум, нам нужно серьёзно рассмотреть возможность, что разработка машин умнее человека кардинально и очень быстро изменит мир. Подобное уже случалось, и не раз.

## Расширенное обсуждение

### Эксперты по ИИ о катастрофических сценариях

В [опросе 2022 года](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) среди 738 участников академических конференций по ИИ NeurIPS и ICML сорок восемь процентов посчитали: есть как минимум десятипроцентная вероятность, что результат развития ИИ будет «крайне плохим (например, вымирание человечества)». Опасения, что ИИ может вызвать беспрецедентную катастрофу, широко распространены в этой области.

Ниже мы собрали комментарии известных учёных и инженеров в области ИИ о его катастрофических последствиях. Некоторые из этих учёных приводят свою «p(doom)» -- вероятность, что ИИ вызовет вымирание человечества или столь же катастрофические последствия[^9].

**Джеффри Хинтон** ([2024](https://youtu.be/PTF5Up1hMhw?t=2285), лауреат Нобелевской премии и премии Тьюринга за то, что запустил революцию глубокого обучения в ИИ), сказал о своих личных оценках[^10]:

> Я на самом деле думаю, что риск \[экзистенциальной угрозы\] составляет более пятидесяти процентов.

**Йошуа Бенджио** ([2023](https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340), лауреат премии Тьюринга (вместе с Хинтоном и Яном ЛеКуном) и самый цитируемый из живущих учёных):

> Мы не знаем, сколько у нас времени, прежде чем это станет действительно опасным. Я уже несколько недель говорю: «Пожалуйста, приведите мне аргументы, убедите меня, что нам не стоит волноваться, я буду намного счастливее». Пока этого не случилось. \[...\] У меня примерно двадцать процентов вероятности, что всё обернётся катастрофой.

**Илья Суцкевер** ([2023](https://openai.com/index/introducing-superalignment/), соавтор изобретения AlexNet, бывший главный научный сотрудник OpenAI и (вместе с Хинтоном и Бенджио) один из трёх наиболее цитируемых учёных в области ИИ):

> Огромная мощь суперинтеллекта также может быть очень опасной и может привести к перехвату власти у человечества, или даже его вымиранию. Хотя суперинтеллект сейчас кажется далёким, мы считаем, что он может появиться в это десятилетие. \[...\] Сейчас нас нет решения, как управлять или контролировать потенциальный суперинтеллект, как предотвратить его выход из-под контроля. Наши нынешние методы согласования ИИ, такие как обучение с подкреплением на основе человеческой обратной связи, полагаются на способность людей контролировать ИИ. Но люди не смогут надёжно контролировать ИИ-системы намного умнее нас, поэтому наши нынешние методы согласования не масштабируются для суперинтеллекта. Нам нужны новые научные и технические прорывы.

**Ян Лейке** ([2023](https://80000hours.org/podcast/episodes/jan-leike-superalignment/), соруководитель научного направления по согласованию в Anthropic и бывший соруководитель команды суперсогласования в OpenAI):

> \[интервьюер: «Я не тратил много времени на точное определение моей личной p(doom). Думаю, больше десяти процентов и меньше девяноста процентов».\] \[Лейк:\] Наверное, я назвал бы тот же диапазон.

**Пол Кристиано** ([2023](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom), руководитель отдела безопасности Института безопасности ИИ США (на базе NIST) и изобретатель обучения с подкреплением на основе человеческой обратной связи (RLHF):

> Вероятность, что большинство людей погибнет в течение 10 лет после создания мощного (достаточно мощного, чтобы сделать человеческий труд устаревшим) ИИ: **20%** \[...\]
>
> Вероятность, что будущее человечества каким-то образом необратимо испортится в течение 10 лет после создания мощного ИИ: **46%**

**Стюарт Рассел** ([2025](https://www.newsweek.com/deepseek-openai-race-human-extinction-2023482), заведующий инженерной кафедрой имени Смита-Заде в Калифорнийском университете в Беркли и соавтор ведущего учебника по ИИ для студентов «Искусственный интеллект: современный подход»):

> «Гонка к СИИ» между компаниями и между нациями в некотором роде похожа \[на гонку времён холодной войны по созданию всё более мощных ядерных бомб\], только хуже. Даже генеральные директора компаний, участвующих в гонке, заявляли, что у победителя есть значительная вероятность вызвать вымирание человечества, потому что мы понятия не имеем, как контролировать системы умнее нас самих. Иными словами, гонка к СИИ -- это гонка к краю пропасти.

**Виктория Краковна** ([2023](https://theinsideview.ai/victoria), научный сотрудник Google DeepMind и соучредитель Future of Life Institute):

> \[интервьюер: «Об этом не очень приятно думать, но какова, по вашему мнению, вероятность того, что Виктория Краковна умрёт от ИИ до 2100 года?»\] \[Краковна:\] Ну, 2100 год очень далеко, особенно учитывая, как быстро развивается технология прямо сейчас. Навскидку я бы сказала процентов двадцать или что-то в этом роде.

**Шейн Легг** ([2011](https://baserates-test.vercel.app/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai), соучредитель и главный учёный по СИИ в Google DeepMind):

> \[интервьюер: «Какую вероятность вы приписываете возможности плохих/очень плохих последствий в результате неправильно сделанного ИИ? \[...\] Где «плохие» = вымирание человечества; «очень плохие» = всех людей пытают»\]
>
> \[Легг:\] В течение года после появления чего-то вроде ИИ человеческого уровня \[...\] я не знаю. Может быть, пять процентов, может быть, пятьдесят процентов. Не думаю, что у кого-то есть хорошая оценка. Если под страданиями вы имеете в виду длительные страдания, то считаю это довольно маловероятным. Если бы сверхразумная машина (или любой другой сверхразумный агент) решила избавиться от нас, думаю, она сделала бы это довольно эффективно.

**Эмад Мостак** ([2024](https://x.com/EMostaque/status/1864266899170767105), основатель Stability AI, компании, создавшей Stable Diffusion):

> Моя p(doom) составляет 50%. Без указания периода времени вероятность, что системы способнее людей, вероятно, в итоге управляющие всей нашей критической инфраструктурой, нас всех уничтожат -- как подбрасывание монетки. Особенно учитывая подход, которого мы сейчас придерживаемся.

**Дэниел Кокотайло** ([2023](https://www.lesswrong.com/posts/xDkdR6JcQsCdnFpaQ/adumbrations-on-agi-from-an-outsider?commentId=sHnfPe5pHJhjJuCWW), специалист по регуляции ИИ, информатор из OpenAI и исполнительный директор AI Futures Project):

> Думаю, гибель от ИИ вероятна на 70%, и считаю, что те, кто думает, что меньше, скажем, 20%, -- очень неразумны.

**Дэн Хендрикс** ([2023](https://x.com/DanHendrycks/status/1642394635657162753), исследователь машинного обучения и директор Center for AI Safety):

> Моя p(doom) \> 80%, но раньше она была ниже. Два года назад она была \~20%.

Все перечисленные исследователи подписали [заявление о рисках ИИ](https://aistatement.com/), которым мы открыли книгу:

> Снижение риска вымирания из-за ИИ должно быть глобальным приоритетом подобно другим всеобщим рискам, таким как пандемии или ядерная война.

Некоторые другие известные исследователи, подписавшие заявление: архитектор ChatGPT Джон Шульман; бывший директор исследований Google Питер Норвиг; главный научный сотрудник Microsoft Эрик Хорвиц; руководитель исследований AlphaGo Дэвид Сильвер; один из изобретателей AutoML Франк Хуттер; один из изобретателей обучения с подкреплением Эндрю Барто; изобретатель GAN Ян Гудфеллоу; бывший президент Baidu Я-Цинь Чжан; изобретатель криптографии с открытым ключом Мартин Хеллман; руководитель исследований Vision Transformer Алексей Досовицкий. Список продолжается другими подписантами: Дон Сон, Яша Соль-Дикштейн, Дэвид МакАллестер, Крис Ола, Бин Ким, Филип Торр и сотни других.

### Когда Лео Силард увидел будущее

В сентябре 1933 года физик Лео Силард переходил дорогу на пересечении Саутгемптон-роу с Рассел-сквер[^11], и ему пришла в голову идея цепной ядерной реакции -- ключевая идея атомных бомб.

В этот момент началось целое приключение. Силард пытался понять, что делать с этой важной идеей. Он пошёл к более уважаемому физику Исидору Раби, а Раби обратился к ещё более уважаемому Энрико Ферми. и спросил у того, считает ли он цепные ядерные реакции реальной вещью, и Ферми прислал ответ:

> Чушь!

Раби спросил у Ферми, что означает «Чушь!». Тот ответил, что это отдалённая возможность.

Раби спросил, что Ферми имеет в виду под «отдалённой возможностью». «Десять процентов».

На что Раби ответил: «Десять процентов -- это не отдалённая возможность, если это означает, что мы можем так умереть».

И Ферми пересмотрел свою позицию.

Из этой истории можно извлечь несколько разных уроков. Урок, который мы *не извлекаем*: «Любая отдалённая возможность заслуживает беспокойства, если мы можем от неё умереть». В десяти процентах нет ничего «отдалённого», но о достаточно отдалённой возможности не стоило бы думать.

Урок, который мы *извлекаем* из этой истории: иногда можно понять, что технология вроде цепной ядерной реакции возможна, и так узнать (раньше всех остальных), что мир ждут кардинальные перемены.

Ещё один урок, который мы извлекаем из этой истории: первые интуитивные прикидки часто плохо помогают в предвидении и осмыслении кардинальных перемен. Даже если ты -- известный эксперт в соответствующей области, как Энрико Ферми.

Подумайте: откуда вообще у Ферми взялись эти «отдалённая возможность» и «десять процентов»?

Почему Ферми считал, что нельзя заставить радиоактивность вызывать больше радиоактивности в цепной реакции? Неужели лишь потому, что большинство крупных идей не срабатывают?

Ответ «Чушь!», кажется, говорит что-то более сильное. Он отражает ощущение, что эта конкретная большая идея чрезвычайно неправдоподобна. Но почему? На основе какого физического аргумента?

Это просто *казалось* безумным? Да, возможность ядерного оружия имела бы радикальные последствия для мира. Но реальность иногда допускает события с крупными последствиями.[^12]

Когда Ферми впервые услышал идею Силарда, он предложил Силарду опубликовать её и рассказать всему миру -- включая Германию и её нового канцлера Адольфа Гитлера.

Ферми проиграл этот спор. И хорошо, что так случилось, ведь ядерное оружие в итоге оказалось возможным. Ферми в конце концов присоединился к крошечному заговору Силарда, хотя оставался скептиком почти что до момента, когда уже сам наблюдал за созданием первого ядерного реактора -- Чикагской батареи-1.

Иногда технологии переворачивают мир. Если принимать как должное, что радикальные новые технологии -- это «чушь», прогресс может застать врасплох. Даже если ты один из умнейших учёных в мире. Большая заслуга Ферми в том, что он сел и поспорил с Силардом. И даже б*о*льшая заслуга, что он дал убедить себя и изменил поведение *до* появления технологии. До возможности собственными глазами её увидеть. Когда ещё было не поздно что-то с этим сделать.

За всю историю человечества произошло много ужасного. Но кое-что ужасное не произошло как раз потому, что кто-то сел и поговорил. В некоторых случаях -- заставил поговорить, как сделал Силард с Ферми.

Общество может решить, что p(doom) высок из-за беспомощности человечества, тогда как на самом деле он высок из-за бездействия. Разговоры о p(doom) могут превратиться в самосбывающееся пророчество и направят нас к катастрофе, которой можно было избежать.

Ещё у нас сложилось впечатление, что в Кремниевой долине люди обмениваются своими «p(doom)» как бейсбольными карточками, в отрыве от реальности. Если обратить внимание на эту переменную, то даже вероятность всего лишь в 5% *убить каждого человека на планете* должна быть очевидным поводом для крайней тревоги.

Это куда выше уровня, достаточного, чтобы оправдать немедленное закрытие всей области ИИ. Люди, кажется, удивительно быстро теряют из это из виду, как только у них появляется стрёмная привычка обмениваться значениями p(doom) на вечеринках, будто это забавная научно-фантастическая история, а не утверждение о том, *что действительно со всеми нами произойдёт*.

Мы тут не говорим, что названные p(doom) близки к реальности. Но их стоит рассматривать как сообщения отраслевых экспертов, что ситуация критическая.

Подобные аргументы, конечно, указывают на ложный вывод. Ферми *ошибался* насчёт цепных ядерных реакций. С учётом этого, мы бы сказали, что из существования таких аргументов реально извлечь урок:

«Всегда можно придумать как минимум настолько же правдоподобные аргументы против истины».

То, что Земля ещё не взорвалась -- не сильное свидетельство в пользу невозможности ядерных реакторов. Инженеры могут специально тщательно расположить атомы, чтобы они распадались. Поэтому такие аргументы не оправдывают столь ошибочных выводов как «Чушь!».

[^1]: Мы рассказываем часть истории Лео Сциларда в расширенном обсуждении ниже.

[^2]: [Bloomberg](https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million), июль 2025 года: «Генеральный директор Meta Марк Цукерберг успешно нанял более десяти исследователей из OpenAI, а также ведущих исследователей и инженеров из Anthropic, Google и других стартапов».

[^3]: Cтатья 1903 года [«Летающие машины, которые не летают»](https://www.nytimes.com/1903/10/09/archives/flying-machines-which-do-not-fly.html): «Машина делает лишь то, что должна делать в подчинении естественным законам, действующим на пассивную материю. Следовательно, если требуется, скажем, тысяча лет, чтобы приспособить для лёгкого полёта птицу, которая начала с зачаточными крыльями, или десять тысяч для той, что начала вовсе без крыльев и должна была их прорастить ab initio, можно предположить, что летающая машина, которая действительно полетит, может быть создана объединёнными и непрерывными усилиями математиков и механиков за период от одного миллиона до десяти миллионов лет --- при условии, конечно, что мы сможем тем временем устранить такие мелкие недостатки и затруднения, как существующее соотношение между весом и прочностью в неорганических материалах. Без сомнения, эта проблема притягивает интересующихся, но обычному человеку кажется, что усилия можно было бы направить и на более выгодное дело.»

[^4]: Да, ИИ могут даже [распознать иронию](https://x.com/AnthonyNAguirre/status/1923535891781517355) того, что New York Times сообщает, будто они не могут распознать иронию. (Отдадим должное New York Times, некоторые из их журналистов освещают ИИ с несколько [лучшим пониманием](https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html).)

[^5]: Если книга окажется столь успешной, что окупит все эти инвестиции, в нашем контракте есть пункт, говорящий, что авторы в итоге получат от MIRI долю прибыли. Но уже после того как MIRI получит за свои усилия хорошую компенсацию. Но MIRI вкладывает в помощь с книгой столько сил, что, если она не превзойдёт наши ожидания кардинально, мы никогда не получим ни копейки.

[^6]: Фраза «ничего никогда не происходит» распространена среди людей, участвующих в рынках предсказаний. Про неё писал, например, блогер Скотт Александер в эссе [«Эвристики, которые почти всегда работают»](https://www.astralcodexten.com/p/heuristics-that-almost-always-work).

[^7]: См., например, работу [«Вечность за шесть часов»](https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148?via%3Dihub), которая обсуждает пределы возможной межгалактической колонизации с учётом известных физических законов.

[^8]: А может, создадут для этого инструменты или преемников. Любым удобным способом, со всеми преимуществами более развитой науки.

[^9]: У нас есть опасения касательно практики называть «p(doom)». Назначение единственной вероятности (в противоположность отдельным вероятностям для разных реакций общества) кажется нам пораженчеством. Есть огромная разница между тем, чья p(doom) высока из-за мнения, что мир скорее всего *не способен* предотвратить катастрофу, и тем, у кого p(doom) высока из-за мнения, что мир *может* предотвратить катастрофу, но *не будет*.

[^10]: Вопреки тому, что Хинтон говорит в видео до этого, уверенность Юдковского в опасности составляет не «99,999» процента. Пять девяток были бы безумной степенью уверенности.

[^11]: Более полные описание и хронология собраны [Фондом атомного наследия](https://ahf.nuclearmuseum.org/ahf/profile/leo-szilard/).

[^12]: Мы видели людей, которые, услышав эту критику Ферми, его защищали, изобретая, почему весьма правдоподобно, что Ферми много думал, прежде чем сказать «Чушь!». Например, что Ферми знал -- каскады индуцированной радиоактивности до сих пор не уничтожили Землю. А можно подумать, что уже должны были, если бы были физически возможны.
